<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:03:16 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-26269] YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource</title>
                <link>https://issues.apache.org/jira/browse/SPARK-26269</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Currently, YarnAllocator may put a node with a completed container whose exit status is not one of SUCCESS,&#160;PREEMPTED,&#160;KILLED_EXCEEDED_VMEM,&#160;KILLED_EXCEEDED_PMEM into blacklist. Howerver, for other exit status, e.g.&#160;KILLED_BY_RESOURCEMANAGER, Yarn do not consider its related nodes shoule be added into blacklist(see YARN&apos;s explaination for detail &lt;a href=&quot;https://github.com/apache/hadoop/blob/228156cfd1b474988bc4fedfbf7edddc87db41e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/Apps.java#L273&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hadoop/blob/228156cfd1b474988bc4fedfbf7edddc87db41e3/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/Apps.java#L273&lt;/a&gt;). So, relaxing the current blacklist rule and having the same blacklist behaviour with YARN would maxmize use of cluster resources.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13202435">SPARK-26269</key>
            <summary>YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="Ngone51">wuyi</assignee>
                                    <reporter username="Ngone51">wuyi</reporter>
                        <labels>
                    </labels>
                <created>Wed, 5 Dec 2018 06:09:58 +0000</created>
                <updated>Sun, 17 May 2020 18:13:38 +0000</updated>
                            <resolved>Fri, 21 Dec 2018 19:28:29 +0000</resolved>
                                    <version>2.3.1</version>
                    <version>2.3.2</version>
                    <version>2.4.0</version>
                                    <fixVersion>2.4.1</fixVersion>
                    <fixVersion>3.0.0</fixVersion>
                                    <component>Spark Core</component>
                    <component>YARN</component>
                        <due>Mon, 10 Dec 2018 00:00:00 +0000</due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="16709652" author="apachespark" created="Wed, 5 Dec 2018 06:22:36 +0000"  >&lt;p&gt;User &apos;Ngone51&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/23223&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23223&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16709653" author="apachespark" created="Wed, 5 Dec 2018 06:23:11 +0000"  >&lt;p&gt;User &apos;Ngone51&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/23223&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23223&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16715447" author="githubbot" created="Mon, 10 Dec 2018 19:34:54 +0000"  >&lt;p&gt;tgravescs commented on issue #23223: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; title=&quot;YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26269&quot;&gt;&lt;del&gt;SPARK-26269&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;YARN&amp;#93;&lt;/span&gt;Yarnallocator should have same blacklist behaviour with yarn to maxmize use of cluster resource&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23223#issuecomment-445943499&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23223#issuecomment-445943499&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   ok thanks for trying.  if I get a chance I can try later in the week, but that doesn&apos;t have to block this now if someone else has time to review before I get to it. We can always pull it back later.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16726995" author="githubbot" created="Fri, 21 Dec 2018 19:27:45 +0000"  >&lt;p&gt;asfgit closed pull request #23223: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; title=&quot;YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26269&quot;&gt;&lt;del&gt;SPARK-26269&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;YARN&amp;#93;&lt;/span&gt;Yarnallocator should have same blacklist behaviour with yarn to maxmize use of cluster resource&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23223&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23223&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala&lt;br/&gt;
index 9497530805c1a..e158d96149622 100644&lt;br/&gt;
&amp;#8212; a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala&lt;br/&gt;
+++ b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocator.scala&lt;br/&gt;
@@ -612,13 +612,23 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;yarn&amp;#93;&lt;/span&gt; class YarnAllocator(&lt;br/&gt;
             val message = &quot;Container killed by YARN for exceeding physical memory limits. &quot; +&lt;br/&gt;
               s&quot;$diag Consider boosting ${EXECUTOR_MEMORY_OVERHEAD.key}.&quot;&lt;br/&gt;
             (true, message)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case _ =&amp;gt;&lt;/li&gt;
	&lt;li&gt;// all the failures which not covered above, like:&lt;/li&gt;
	&lt;li&gt;// disk failure, kill by app master or resource manager, ...&lt;/li&gt;
	&lt;li&gt;allocatorBlacklistTracker.handleResourceAllocationFailure(hostOpt)&lt;/li&gt;
	&lt;li&gt;(true, &quot;Container marked as failed: &quot; + containerId + onHostStr +&lt;/li&gt;
	&lt;li&gt;&quot;. Exit status: &quot; + completedContainer.getExitStatus +&lt;/li&gt;
	&lt;li&gt;&quot;. Diagnostics: &quot; + completedContainer.getDiagnostics)&lt;br/&gt;
+          case other_exit_status =&amp;gt;&lt;br/&gt;
+            // &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; title=&quot;YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26269&quot;&gt;&lt;del&gt;SPARK-26269&lt;/del&gt;&lt;/a&gt;: follow YARN&apos;s blacklisting behaviour(see &lt;a href=&quot;https://github&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github&lt;/a&gt;&lt;br/&gt;
+            // .com/apache/hadoop/blob/228156cfd1b474988bc4fedfbf7edddc87db41e3/had&lt;br/&gt;
+            // oop-yarn-project/hadoop-yarn/hadoop-yarn-common/src/main/java/org/ap&lt;br/&gt;
+            // ache/hadoop/yarn/util/Apps.java#L273 for details)&lt;br/&gt;
+            if (NOT_APP_AND_SYSTEM_FAULT_EXIT_STATUS.contains(other_exit_status)) {&lt;br/&gt;
+              (false, s&quot;Container marked as failed: $containerId$onHostStr&quot; +&lt;br/&gt;
+                s&quot;. Exit status: ${completedContainer.getExitStatus}&quot; +&lt;br/&gt;
+                s&quot;. Diagnostics: ${completedContainer.getDiagnostics}.&quot;)&lt;br/&gt;
+            } else {&lt;br/&gt;
+              // completed container from a bad node&lt;br/&gt;
+              allocatorBlacklistTracker.handleResourceAllocationFailure(hostOpt)&lt;br/&gt;
+              (true, s&quot;Container from a bad node: $containerId$onHostStr&quot; +&lt;br/&gt;
+                s&quot;. Exit status: ${completedContainer.getExitStatus}&quot; +&lt;br/&gt;
+                s&quot;. Diagnostics: ${completedContainer.getDiagnostics}.&quot;)&lt;br/&gt;
+            }&lt;br/&gt;
+&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         }&lt;br/&gt;
         if (exitCausedByApp) {&lt;br/&gt;
@@ -744,4 +754,12 @@ private object YarnAllocator &lt;/p&gt;
{
   val MEM_REGEX = &quot;[0-9.]+ [KMG]B&quot;
   val VMEM_EXCEEDED_EXIT_CODE = -103
   val PMEM_EXCEEDED_EXIT_CODE = -104
+
+  val NOT_APP_AND_SYSTEM_FAULT_EXIT_STATUS = Set(
+    ContainerExitStatus.KILLED_BY_RESOURCEMANAGER,
+    ContainerExitStatus.KILLED_BY_APPMASTER,
+    ContainerExitStatus.KILLED_AFTER_APP_COMPLETION,
+    ContainerExitStatus.ABORTED,
+    ContainerExitStatus.DISKS_FAILED
+  )
 }
&lt;p&gt;diff --git a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTracker.scala b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTracker.scala&lt;br/&gt;
index ceac7cda5f8be..268976b629507 100644&lt;br/&gt;
&amp;#8212; a/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTracker.scala&lt;br/&gt;
+++ b/resource-managers/yarn/src/main/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTracker.scala&lt;br/&gt;
@@ -120,7 +120,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class YarnAllocatorBlacklistTracker(&lt;br/&gt;
     if (removals.nonEmpty) &lt;/p&gt;
{
       logInfo(s&quot;removing nodes from YARN application master&apos;s blacklist: $removals&quot;)
     }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;amClient.updateBlacklist(additions.asJava, removals.asJava)&lt;br/&gt;
+    if (additions.nonEmpty || removals.nonEmpty) 
{
+      amClient.updateBlacklist(additions.asJava, removals.asJava)
+    }
&lt;p&gt;     currentBlacklistedYarnNodes = nodesToBlacklist&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTrackerSuite.scala b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTrackerSuite.scala&lt;br/&gt;
index aeac68e6ed330..201910731e934 100644&lt;br/&gt;
&amp;#8212; a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTrackerSuite.scala&lt;br/&gt;
+++ b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorBlacklistTrackerSuite.scala&lt;br/&gt;
@@ -87,7 +87,7 @@ class YarnAllocatorBlacklistTrackerSuite extends SparkFunSuite with Matchers&lt;br/&gt;
     // expired blacklisted nodes (simulating a resource request)&lt;br/&gt;
     yarnBlacklistTracker.setSchedulerBlacklistedNodes(Set(&quot;host1&quot;, &quot;host2&quot;))&lt;br/&gt;
     // no change is communicated to YARN regarding the blacklisting&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;verify(amClientMock).updateBlacklist(Collections.emptyList(), Collections.emptyList())&lt;br/&gt;
+    verify(amClientMock, times(0)).updateBlacklist(Collections.emptyList(), Collections.emptyList())&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   test(&quot;combining scheduler and allocation blacklist&quot;) &lt;/p&gt;
{
diff --git a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala
index b61e7df4420ef..53a538dc1de29 100644
--- a/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala
+++ b/resource-managers/yarn/src/test/scala/org/apache/spark/deploy/yarn/YarnAllocatorSuite.scala
@@ -17,6 +17,8 @@
 
 package org.apache.spark.deploy.yarn
 
+import java.util.Collections
+
 import scala.collection.JavaConverters._
 
 import org.apache.hadoop.conf.Configuration
@@ -114,13 +116,29 @@ class YarnAllocatorSuite extends SparkFunSuite with Matchers with BeforeAndAfter
       clock)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def createContainer(host: String, resource: Resource = containerResource): Container = {&lt;/li&gt;
	&lt;li&gt;val containerId = ContainerId.newContainerId(appAttemptId, containerNum)&lt;br/&gt;
+  def createContainer(&lt;br/&gt;
+      host: String,&lt;br/&gt;
+      containerNumber: Int = containerNum,&lt;br/&gt;
+      resource: Resource = containerResource): Container = 
{
+    val  containerId: ContainerId = ContainerId.newContainerId(appAttemptId, containerNum)
     containerNum += 1
     val nodeId = NodeId.newInstance(host, 1000)
     Container.newInstance(containerId, nodeId, &quot;&quot;, resource, RM_REQUEST_PRIORITY, null)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  def createContainers(hosts: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;, containerIds: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Container&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    hosts.zip(containerIds).map&lt;/p&gt;
{case (host, id) =&amp;gt; createContainer(host, id)}
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
+  def createContainerStatus(&lt;br/&gt;
+      containerId: ContainerId,&lt;br/&gt;
+      exitStatus: Int,&lt;br/&gt;
+      containerState: ContainerState = ContainerState.COMPLETE,&lt;br/&gt;
+      diagnostics: String = &quot;diagnostics&quot;): ContainerStatus = &lt;/p&gt;
{
+    ContainerStatus.newInstance(containerId, containerState, diagnostics, exitStatus)
+  }
&lt;p&gt;+&lt;br/&gt;
+&lt;br/&gt;
   test(&quot;single container allocated&quot;) &lt;/p&gt;
{
     // request a single container and receive it
     val handler = createAllocator(1)
@@ -148,7 +166,7 @@ class YarnAllocatorSuite extends SparkFunSuite with Matchers with BeforeAndAfter
       Map(YARN_EXECUTOR_RESOURCE_TYPES_PREFIX + &quot;gpu&quot; -&amp;gt; &quot;2G&quot;))
 
     handler.updateResourceRequests()
-    val container = createContainer(&quot;host1&quot;, handler.resource)
+    val container = createContainer(&quot;host1&quot;, resource = handler.resource)
     handler.handleAllocatedContainers(Array(container))
 
     // get amount of memory and vcores from resource, so effectively skipping their validation
@@ -417,4 +435,55 @@ class YarnAllocatorSuite extends SparkFunSuite with Matchers with BeforeAndAfter
     clock.advance(50 * 1000L)
     handler.getNumExecutorsFailed should be (0)
   }
&lt;p&gt;+&lt;br/&gt;
+  test(&quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; title=&quot;YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26269&quot;&gt;&lt;del&gt;SPARK-26269&lt;/del&gt;&lt;/a&gt;: YarnAllocator should have same blacklist behaviour with YARN&quot;) {&lt;br/&gt;
+    val rmClientSpy = spy(rmClient)&lt;br/&gt;
+    val maxExecutors = 11&lt;br/&gt;
+&lt;br/&gt;
+    val handler = createAllocator(&lt;br/&gt;
+      maxExecutors,&lt;br/&gt;
+      rmClientSpy,&lt;br/&gt;
+      Map(&lt;br/&gt;
+        &quot;spark.yarn.blacklist.executor.launch.blacklisting.enabled&quot; -&amp;gt; &quot;true&quot;,&lt;br/&gt;
+        &quot;spark.blacklist.application.maxFailedExecutorsPerNode&quot; -&amp;gt; &quot;0&quot;))&lt;br/&gt;
+    handler.updateResourceRequests()&lt;br/&gt;
+&lt;br/&gt;
+    val hosts = (0 until maxExecutors).map(i =&amp;gt; s&quot;host$i&quot;)&lt;br/&gt;
+    val ids = 0 to maxExecutors&lt;br/&gt;
+    val containers = createContainers(hosts, ids)&lt;br/&gt;
+&lt;br/&gt;
+    val nonBlacklistedStatuses = Seq(&lt;br/&gt;
+      ContainerExitStatus.SUCCESS,&lt;br/&gt;
+      ContainerExitStatus.PREEMPTED,&lt;br/&gt;
+      ContainerExitStatus.KILLED_EXCEEDED_VMEM,&lt;br/&gt;
+      ContainerExitStatus.KILLED_EXCEEDED_PMEM,&lt;br/&gt;
+      ContainerExitStatus.KILLED_BY_RESOURCEMANAGER,&lt;br/&gt;
+      ContainerExitStatus.KILLED_BY_APPMASTER,&lt;br/&gt;
+      ContainerExitStatus.KILLED_AFTER_APP_COMPLETION,&lt;br/&gt;
+      ContainerExitStatus.ABORTED,&lt;br/&gt;
+      ContainerExitStatus.DISKS_FAILED)&lt;br/&gt;
+&lt;br/&gt;
+    val nonBlacklistedContainerStatuses = nonBlacklistedStatuses.zipWithIndex.map &lt;/p&gt;
{
+      case (exitStatus, idx) =&amp;gt; createContainerStatus(containers(idx).getId, exitStatus)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    val BLACKLISTED_EXIT_CODE = 1&lt;br/&gt;
+    val blacklistedStatuses = Seq(ContainerExitStatus.INVALID, BLACKLISTED_EXIT_CODE)&lt;br/&gt;
+&lt;br/&gt;
+    val blacklistedContainerStatuses = blacklistedStatuses.zip(9 until maxExecutors).map {+      case (exitStatus, idx) =&amp;gt; createContainerStatus(containers(idx).getId, exitStatus)+    }
&lt;p&gt;+&lt;br/&gt;
+    handler.handleAllocatedContainers(containers.slice(0, 9))&lt;br/&gt;
+    handler.processCompletedContainers(nonBlacklistedContainerStatuses)&lt;br/&gt;
+    verify(rmClientSpy, never())&lt;br/&gt;
+      .updateBlacklist(hosts.slice(0, 9).asJava, Collections.emptyList())&lt;br/&gt;
+&lt;br/&gt;
+    handler.handleAllocatedContainers(containers.slice(9, 11))&lt;br/&gt;
+    handler.processCompletedContainers(blacklistedContainerStatuses)&lt;br/&gt;
+    verify(rmClientSpy)&lt;br/&gt;
+      .updateBlacklist(hosts.slice(9, 10).asJava, Collections.emptyList())&lt;br/&gt;
+    verify(rmClientSpy)&lt;br/&gt;
+      .updateBlacklist(hosts.slice(10, 11).asJava, Collections.emptyList())&lt;br/&gt;
+  }&lt;br/&gt;
 }&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16727167" author="githubbot" created="Sat, 22 Dec 2018 01:29:09 +0000"  >&lt;p&gt;Ngone51 opened a new pull request #23366: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; title=&quot;YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26269&quot;&gt;&lt;del&gt;SPARK-26269&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;YARN&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;BRANCH-2.4&amp;#93;&lt;/span&gt;Yarnallocator should have same blacklist behaviour with yarn to maxmize use of cluster resource&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23366&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23366&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What changes were proposed in this pull request?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   As I mentioned in jira &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; title=&quot;YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26269&quot;&gt;&lt;del&gt;SPARK-26269&lt;/del&gt;&lt;/a&gt;(&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-26269&lt;/a&gt;), in order to maxmize the use of cluster resource,  this pr try to make `YarnAllocator` have the same blacklist behaviour with YARN.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;How was this patch tested?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Added.&lt;/p&gt;


&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16727169" author="githubbot" created="Sat, 22 Dec 2018 01:42:31 +0000"  >&lt;p&gt;Ngone51 closed pull request #23366: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; title=&quot;YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26269&quot;&gt;&lt;del&gt;SPARK-26269&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;YARN&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;BRANCH-2.4&amp;#93;&lt;/span&gt;Yarnallocator should have same blacklist behaviour with yarn to maxmize use of cluster resource&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23366&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23366&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/.gitignore b/.gitignore&lt;br/&gt;
index 19db7ac277944..e4c44d0590d59 100644&lt;br/&gt;
&amp;#8212; a/.gitignore&lt;br/&gt;
+++ b/.gitignore&lt;br/&gt;
@@ -77,7 +77,6 @@ target/&lt;br/&gt;
 unit-tests.log&lt;br/&gt;
 work/&lt;br/&gt;
 docs/.jekyll-metadata&lt;br/&gt;
-*.crc&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;For Hive&lt;br/&gt;
 TempStatsStore/&lt;br/&gt;
diff --git a/.travis.yml b/.travis.yml&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 05b94adeeb93b..0000000000000
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/.travis.yml&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,50 +0,0 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;ol&gt;
		&lt;li&gt;Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
		&lt;li&gt;contributor license agreements. See the NOTICE file distributed with&lt;/li&gt;
		&lt;li&gt;this work for additional information regarding copyright ownership.&lt;/li&gt;
		&lt;li&gt;The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
		&lt;li&gt;(the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
		&lt;li&gt;the License. You may obtain a copy of the License at&lt;br/&gt;
-#&lt;/li&gt;
		&lt;li&gt;&lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
-#&lt;/li&gt;
		&lt;li&gt;Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
		&lt;li&gt;distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
		&lt;li&gt;WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
		&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
		&lt;li&gt;limitations under the License.&lt;br/&gt;
-&lt;/li&gt;
		&lt;li&gt;Spark provides this Travis CI configuration file to help contributors&lt;/li&gt;
		&lt;li&gt;check Scala/Java style conformance and JDK7/8 compilation easily&lt;/li&gt;
		&lt;li&gt;during their preparing pull requests.&lt;/li&gt;
		&lt;li&gt;- Scalastyle is executed during `maven install` implicitly.&lt;/li&gt;
		&lt;li&gt;- Java Checkstyle is executed by `lint-java`.&lt;/li&gt;
		&lt;li&gt;See the related discussion here.&lt;/li&gt;
		&lt;li&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/12980&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/12980&lt;/a&gt;&lt;br/&gt;
-&lt;/li&gt;
		&lt;li&gt;1. Choose OS (Ubuntu 14.04.3 LTS Server Edition 64bit, ~2 CORE, 7.5GB RAM)&lt;br/&gt;
-sudo: required&lt;br/&gt;
-dist: trusty&lt;br/&gt;
-&lt;/li&gt;
		&lt;li&gt;2. Choose language and target JDKs for parallel builds.&lt;br/&gt;
-language: java&lt;br/&gt;
-jdk:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;- oraclejdk8&lt;br/&gt;
-
	&lt;ol&gt;
		&lt;li&gt;3. Setup cache directory for SBT and Maven.&lt;br/&gt;
-cache:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;directories:&lt;/li&gt;
	&lt;li&gt;- $HOME/.sbt&lt;/li&gt;
	&lt;li&gt;- $HOME/.m2&lt;br/&gt;
-
	&lt;ol&gt;
		&lt;li&gt;4. Turn off notifications.&lt;br/&gt;
-notifications:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;email: false&lt;br/&gt;
-
	&lt;ol&gt;
		&lt;li&gt;5. Run maven install before running lint-java.&lt;br/&gt;
-install:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;- export MAVEN_SKIP_RC=1&lt;/li&gt;
	&lt;li&gt;- build/mvn -T 4 -q -DskipTests -Pkubernetes -Pmesos -Pyarn -Pkinesis-asl -Phive -Phive-thriftserver install&lt;br/&gt;
-
	&lt;ol&gt;
		&lt;li&gt;6. Run lint-java.&lt;br/&gt;
-script:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;- dev/lint-java&lt;br/&gt;
diff --git a/R/WINDOWS.md b/R/WINDOWS.md&lt;br/&gt;
index da668a69b8679..33a4c850cfdac 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/R/WINDOWS.md&lt;br/&gt;
+++ b/R/WINDOWS.md&lt;br/&gt;
@@ -3,7 +3,7 @@&lt;br/&gt;
 To build SparkR on Windows, the following steps are required&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; 1. Install R (&amp;gt;= 3.1) and &lt;span class=&quot;error&quot;&gt;&amp;#91;Rtools&amp;#93;&lt;/span&gt;(&lt;a href=&quot;http://cran.r-project.org/bin/windows/Rtools/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cran.r-project.org/bin/windows/Rtools/&lt;/a&gt;). Make sure to&lt;br/&gt;
-include Rtools and R in `PATH`.&lt;br/&gt;
+include Rtools and R in `PATH`. Note that support for R prior to version 3.4 is deprecated as of Spark 3.0.0.&lt;/p&gt;

&lt;p&gt; 2. Install&lt;br/&gt;
 &lt;span class=&quot;error&quot;&gt;&amp;#91;JDK8&amp;#93;&lt;/span&gt;(&lt;a href=&quot;http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html&lt;/a&gt;) and set&lt;br/&gt;
diff --git a/R/pkg/DESCRIPTION b/R/pkg/DESCRIPTION&lt;br/&gt;
index f52d785e05cdd..736da46eaa8d3 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/DESCRIPTION&lt;br/&gt;
+++ b/R/pkg/DESCRIPTION&lt;br/&gt;
@@ -1,6 +1,6 @@&lt;br/&gt;
 Package: SparkR&lt;br/&gt;
 Type: Package&lt;br/&gt;
-Version: 2.4.0&lt;br/&gt;
+Version: 3.0.0&lt;br/&gt;
 Title: R Frontend for Apache Spark&lt;br/&gt;
 Description: Provides an R Frontend for Apache Spark.&lt;br/&gt;
 Authors@R: c(person(&quot;Shivaram&quot;, &quot;Venkataraman&quot;, role = c(&quot;aut&quot;, &quot;cre&quot;),&lt;br/&gt;
@@ -15,7 +15,7 @@ URL: &lt;a href=&quot;http://www.apache.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/&lt;/a&gt; &lt;a href=&quot;http://spark.apache.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/&lt;/a&gt;&lt;br/&gt;
 BugReports: &lt;a href=&quot;http://spark.apache.org/contributing.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/contributing.html&lt;/a&gt;&lt;br/&gt;
 SystemRequirements: Java (== 8)&lt;br/&gt;
 Depends:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;R (&amp;gt;= 3.0),&lt;br/&gt;
+    R (&amp;gt;= 3.1),&lt;br/&gt;
     methods&lt;br/&gt;
 Suggests:&lt;br/&gt;
     knitr,&lt;br/&gt;
diff --git a/R/pkg/NAMESPACE b/R/pkg/NAMESPACE&lt;br/&gt;
index 96ff389faf4a0..1f8ba0bcf1cf5 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/R/pkg/NAMESPACE&lt;br/&gt;
+++ b/R/pkg/NAMESPACE&lt;br/&gt;
@@ -28,9 +28,8 @@ importFrom(&quot;utils&quot;, &quot;download.file&quot;, &quot;object.size&quot;, &quot;packageVersion&quot;, &quot;tail&quot;, &quot;u&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;S3 methods exported&lt;br/&gt;
 export(&quot;sparkR.session&quot;)&lt;br/&gt;
-export(&quot;sparkR.init&quot;)&lt;br/&gt;
-export(&quot;sparkR.stop&quot;)&lt;br/&gt;
 export(&quot;sparkR.session.stop&quot;)&lt;br/&gt;
+export(&quot;sparkR.stop&quot;)&lt;br/&gt;
 export(&quot;sparkR.conf&quot;)&lt;br/&gt;
 export(&quot;sparkR.version&quot;)&lt;br/&gt;
 export(&quot;sparkR.uiWebUrl&quot;)&lt;br/&gt;
@@ -42,9 +41,6 @@ export(&quot;sparkR.callJStatic&quot;)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; export(&quot;install.spark&quot;)&lt;/p&gt;

&lt;p&gt;-export(&quot;sparkRSQL.init&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&quot;sparkRHive.init&quot;)&lt;br/&gt;
-&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;MLlib integration&lt;br/&gt;
 exportMethods(&quot;glm&quot;,&lt;br/&gt;
               &quot;spark.glm&quot;,&lt;br/&gt;
@@ -70,7 +66,8 @@ exportMethods(&quot;glm&quot;,&lt;br/&gt;
               &quot;spark.svmLinear&quot;,&lt;br/&gt;
               &quot;spark.fpGrowth&quot;,&lt;br/&gt;
               &quot;spark.freqItemsets&quot;,&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&quot;spark.associationRules&quot;)&lt;br/&gt;
+              &quot;spark.associationRules&quot;,&lt;br/&gt;
+              &quot;spark.findFrequentSequentialPatterns&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;Job group lifecycle management methods&lt;br/&gt;
 export(&quot;setJobGroup&quot;,&lt;br/&gt;
@@ -150,7 +147,6 @@ exportMethods(&quot;arrange&quot;,&lt;br/&gt;
               &quot;printSchema&quot;,&lt;br/&gt;
               &quot;randomSplit&quot;,&lt;br/&gt;
               &quot;rbind&quot;,&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&quot;registerTempTable&quot;,&lt;br/&gt;
               &quot;rename&quot;,&lt;br/&gt;
               &quot;repartition&quot;,&lt;br/&gt;
               &quot;repartitionByRange&quot;,&lt;br/&gt;
@@ -158,7 +154,6 @@ exportMethods(&quot;arrange&quot;,&lt;br/&gt;
               &quot;sample&quot;,&lt;br/&gt;
               &quot;sample_frac&quot;,&lt;br/&gt;
               &quot;sampleBy&quot;,&lt;/li&gt;
	&lt;li&gt;&quot;saveAsParquetFile&quot;,&lt;br/&gt;
               &quot;saveAsTable&quot;,&lt;br/&gt;
               &quot;saveDF&quot;,&lt;br/&gt;
               &quot;schema&quot;,&lt;br/&gt;
@@ -200,6 +195,7 @@ exportMethods(&quot;%&amp;lt;=&amp;gt;%&quot;,&lt;br/&gt;
               &quot;acos&quot;,&lt;br/&gt;
               &quot;add_months&quot;,&lt;br/&gt;
               &quot;alias&quot;,&lt;br/&gt;
+              &quot;approx_count_distinct&quot;,&lt;br/&gt;
               &quot;approxCountDistinct&quot;,&lt;br/&gt;
               &quot;approxQuantile&quot;,&lt;br/&gt;
               &quot;array_contains&quot;,&lt;br/&gt;
@@ -258,6 +254,7 @@ exportMethods(&quot;%&amp;lt;=&amp;gt;%&quot;,&lt;br/&gt;
               &quot;dayofweek&quot;,&lt;br/&gt;
               &quot;dayofyear&quot;,&lt;br/&gt;
               &quot;decode&quot;,&lt;br/&gt;
+              &quot;degrees&quot;,&lt;br/&gt;
               &quot;dense_rank&quot;,&lt;br/&gt;
               &quot;desc&quot;,&lt;br/&gt;
               &quot;element_at&quot;,&lt;br/&gt;
@@ -274,6 +271,7 @@ exportMethods(&quot;%&amp;lt;=&amp;gt;%&quot;,&lt;br/&gt;
               &quot;floor&quot;,&lt;br/&gt;
               &quot;format_number&quot;,&lt;br/&gt;
               &quot;format_string&quot;,&lt;br/&gt;
+              &quot;from_csv&quot;,&lt;br/&gt;
               &quot;from_json&quot;,&lt;br/&gt;
               &quot;from_unixtime&quot;,&lt;br/&gt;
               &quot;from_utc_timestamp&quot;,&lt;br/&gt;
@@ -339,6 +337,7 @@ exportMethods(&quot;%&amp;lt;=&amp;gt;%&quot;,&lt;br/&gt;
               &quot;posexplode&quot;,&lt;br/&gt;
               &quot;posexplode_outer&quot;,&lt;br/&gt;
               &quot;quarter&quot;,&lt;br/&gt;
+              &quot;radians&quot;,&lt;br/&gt;
               &quot;rand&quot;,&lt;br/&gt;
               &quot;randn&quot;,&lt;br/&gt;
               &quot;rank&quot;,&lt;br/&gt;
@@ -352,6 +351,8 @@ exportMethods(&quot;%&amp;lt;=&amp;gt;%&quot;,&lt;br/&gt;
               &quot;row_number&quot;,&lt;br/&gt;
               &quot;rpad&quot;,&lt;br/&gt;
               &quot;rtrim&quot;,&lt;br/&gt;
+              &quot;schema_of_csv&quot;,&lt;br/&gt;
+              &quot;schema_of_json&quot;,&lt;br/&gt;
               &quot;second&quot;,&lt;br/&gt;
               &quot;sha1&quot;,&lt;br/&gt;
               &quot;sha2&quot;,&lt;br/&gt;
@@ -385,6 +386,7 @@ exportMethods(&quot;%&amp;lt;=&amp;gt;%&quot;,&lt;br/&gt;
               &quot;tanh&quot;,&lt;br/&gt;
               &quot;toDegrees&quot;,&lt;br/&gt;
               &quot;toRadians&quot;,&lt;br/&gt;
+              &quot;to_csv&quot;,&lt;br/&gt;
               &quot;to_date&quot;,&lt;br/&gt;
               &quot;to_json&quot;,&lt;br/&gt;
               &quot;to_timestamp&quot;,&lt;br/&gt;
@@ -413,18 +415,14 @@ export(&quot;as.DataFrame&quot;,&lt;br/&gt;
        &quot;cacheTable&quot;,&lt;br/&gt;
        &quot;clearCache&quot;,&lt;br/&gt;
        &quot;createDataFrame&quot;,&lt;/li&gt;
	&lt;li&gt;&quot;createExternalTable&quot;,&lt;br/&gt;
        &quot;createTable&quot;,&lt;br/&gt;
        &quot;currentDatabase&quot;,&lt;/li&gt;
	&lt;li&gt;&quot;dropTempTable&quot;,&lt;br/&gt;
        &quot;dropTempView&quot;,&lt;/li&gt;
	&lt;li&gt;&quot;jsonFile&quot;,&lt;br/&gt;
        &quot;listColumns&quot;,&lt;br/&gt;
        &quot;listDatabases&quot;,&lt;br/&gt;
        &quot;listFunctions&quot;,&lt;br/&gt;
        &quot;listTables&quot;,&lt;br/&gt;
        &quot;loadDF&quot;,&lt;/li&gt;
	&lt;li&gt;&quot;parquetFile&quot;,&lt;br/&gt;
        &quot;read.df&quot;,&lt;br/&gt;
        &quot;read.jdbc&quot;,&lt;br/&gt;
        &quot;read.json&quot;,&lt;br/&gt;
diff --git a/R/pkg/R/DataFrame.R b/R/pkg/R/DataFrame.R&lt;br/&gt;
index 4f2d4c7c002d4..24ed449f2a7d1 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/R/pkg/R/DataFrame.R&lt;br/&gt;
+++ b/R/pkg/R/DataFrame.R&lt;br/&gt;
@@ -226,7 +226,9 @@ setMethod(&quot;showDF&quot;,&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; #&apos; show&lt;br/&gt;
 #&apos;&lt;br/&gt;
-#&apos; Print class and type information of a Spark object.&lt;br/&gt;
+#&apos; If eager evaluation is enabled and the Spark object is a SparkDataFrame, evaluate the&lt;br/&gt;
+#&apos; SparkDataFrame and print top rows of the SparkDataFrame, otherwise, print the class&lt;br/&gt;
+#&apos; and type information of the Spark object.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @param object a Spark object. Can be a SparkDataFrame, Column, GroupedData, WindowSpec.&lt;br/&gt;
 #&apos;&lt;br/&gt;
@@ -244,11 +246,33 @@ setMethod(&quot;showDF&quot;,&lt;br/&gt;
 #&apos; @note show(SparkDataFrame) since 1.4.0&lt;br/&gt;
 setMethod(&quot;show&quot;, &quot;SparkDataFrame&quot;,&lt;br/&gt;
           function(object) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cols &amp;lt;- lapply(dtypes(object), function(l) 
{
-              paste(l, collapse = &quot;:&quot;)
-            }
&lt;p&gt;)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;s &amp;lt;- paste(cols, collapse = &quot;, &quot;)&lt;/li&gt;
	&lt;li&gt;cat(paste(class(object), &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;, s, &amp;quot;&amp;#93;&lt;/span&gt;\n&quot;, sep = &quot;&quot;))&lt;br/&gt;
+            allConf &amp;lt;- sparkR.conf()&lt;br/&gt;
+            prop &amp;lt;- allConf[&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;spark.sql.repl.eagerEval.enabled&amp;quot;&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+            if (!is.null(prop) &amp;amp;&amp;amp; identical(prop, &quot;true&quot;)) {&lt;br/&gt;
+              argsList &amp;lt;- list()&lt;br/&gt;
+              argsList$x &amp;lt;- object&lt;br/&gt;
+              prop &amp;lt;- allConf[&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;spark.sql.repl.eagerEval.maxNumRows&amp;quot;&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+              if (!is.null(prop)) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                numRows &amp;lt;- as.integer(prop)+                if (numRows &amp;gt; 0) {
+                  argsList$numRows &amp;lt;- numRows
+                }+              }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+              prop &amp;lt;- allConf[&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;spark.sql.repl.eagerEval.truncate&amp;quot;&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+              if (!is.null(prop)) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                truncate &amp;lt;- as.integer(prop)+                if (truncate &amp;gt; 0) {
+                  argsList$truncate &amp;lt;- truncate
+                }+              }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+              do.call(showDF, argsList)&lt;br/&gt;
+            } else &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+              cols &amp;lt;- lapply(dtypes(object), function(l) {
+                paste(l, collapse = &quot;:&quot;)
+              })+              s &amp;lt;- paste(cols, collapse = &amp;quot;, &amp;quot;)+              cat(paste(class(object), &amp;quot;[&amp;quot;, s, &amp;quot;]n&amp;quot;, sep = &amp;quot;&amp;quot;))+            }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;           })&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; #&apos; DataTypes&lt;br/&gt;
@@ -497,33 +521,6 @@ setMethod(&quot;createOrReplaceTempView&quot;,&lt;br/&gt;
               invisible(callJMethod(x@sdf, &quot;createOrReplaceTempView&quot;, viewName))&lt;br/&gt;
           })&lt;/p&gt;

&lt;p&gt;-#&apos; (Deprecated) Register Temporary Table&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; Registers a SparkDataFrame as a Temporary Table in the SparkSession&lt;br/&gt;
-#&apos; @param x A SparkDataFrame&lt;br/&gt;
-#&apos; @param tableName A character vector containing the name of the table&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; @family SparkDataFrame functions&lt;br/&gt;
-#&apos; @seealso \link&lt;/p&gt;
{createOrReplaceTempView}
&lt;p&gt;-#&apos; @rdname registerTempTable-deprecated&lt;br/&gt;
-#&apos; @name registerTempTable&lt;br/&gt;
-#&apos; @aliases registerTempTable,SparkDataFrame,character-method&lt;br/&gt;
-#&apos; @examples&lt;br/&gt;
-#&apos;\dontrun&lt;/p&gt;
{
-#&apos; sparkR.session()
-#&apos; path &amp;lt;- &quot;path/to/file.json&quot;
-#&apos; df &amp;lt;- read.json(path)
-#&apos; registerTempTable(df, &quot;json_df&quot;)
-#&apos; new_df &amp;lt;- sql(&quot;SELECT * FROM json_df&quot;)
-#&apos;}
&lt;p&gt;-#&apos; @note registerTempTable since 1.4.0&lt;br/&gt;
-setMethod(&quot;registerTempTable&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;signature(x = &quot;SparkDataFrame&quot;, tableName = &quot;character&quot;),&lt;/li&gt;
	&lt;li&gt;function(x, tableName) 
{
-              .Deprecated(&quot;createOrReplaceTempView&quot;)
-              invisible(callJMethod(x@sdf, &quot;createOrReplaceTempView&quot;, tableName))
-          }
&lt;p&gt;)&lt;br/&gt;
-&lt;br/&gt;
 #&apos; insertInto&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Insert the contents of a SparkDataFrame into a table registered in the current SparkSession.&lt;br/&gt;
@@ -769,6 +766,13 @@ setMethod(&quot;repartition&quot;,&lt;br/&gt;
 #&apos;  \item&lt;/p&gt;
{2.}
&lt;p&gt; &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {Return a new SparkDataFrame range partitioned by the given column(s), #&amp;#39;                      using code{spark.sql.shuffle.partitions} as number of partitions.}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; #&apos;}&lt;br/&gt;
+#&apos; At least one partition-by expression must be specified.&lt;br/&gt;
+#&apos; When no explicit sort order is specified, &quot;ascending nulls first&quot; is assumed.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; Note that due to performance reasons this method uses sampling to estimate the ranges.&lt;br/&gt;
+#&apos; Hence, the output may not be consistent, since sampling can return different values.&lt;br/&gt;
+#&apos; The sample size can be controlled by the config&lt;br/&gt;
+#&apos; \code&lt;/p&gt;
{spark.sql.execution.rangeExchange.sampleSizePerPartition}
&lt;p&gt;.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @param x a SparkDataFrame.&lt;br/&gt;
 #&apos; @param numPartitions the number of partitions to use.&lt;br/&gt;
@@ -823,7 +827,6 @@ setMethod(&quot;repartitionByRange&quot;,&lt;br/&gt;
 #&apos; toJSON&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Converts a SparkDataFrame into a SparkDataFrame of JSON string.&lt;br/&gt;
-#&apos;&lt;br/&gt;
 #&apos; Each row is turned into a JSON document with columns as different fields.&lt;br/&gt;
 #&apos; The returned SparkDataFrame has a single character column with the name \code&lt;/p&gt;
{value}
&lt;p&gt; #&apos;&lt;br/&gt;
@@ -933,7 +936,6 @@ setMethod(&quot;write.orc&quot;,&lt;br/&gt;
 #&apos; path &amp;lt;- &quot;path/to/file.json&quot;&lt;br/&gt;
 #&apos; df &amp;lt;- read.json(path)&lt;br/&gt;
 #&apos; write.parquet(df, &quot;/tmp/sparkr-tmp1/&quot;)&lt;br/&gt;
-#&apos; saveAsParquetFile(df, &quot;/tmp/sparkr-tmp2/&quot;)&lt;br/&gt;
 #&apos;}&lt;br/&gt;
 #&apos; @note write.parquet since 1.6.0&lt;br/&gt;
 setMethod(&quot;write.parquet&quot;,&lt;br/&gt;
@@ -944,17 +946,6 @@ setMethod(&quot;write.parquet&quot;,&lt;br/&gt;
             invisible(handledCallJMethod(write, &quot;parquet&quot;, path))&lt;br/&gt;
           })&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-#&apos; @rdname write.parquet&lt;br/&gt;
-#&apos; @name saveAsParquetFile&lt;br/&gt;
-#&apos; @aliases saveAsParquetFile,SparkDataFrame,character-method&lt;br/&gt;
-#&apos; @note saveAsParquetFile since 1.4.0&lt;br/&gt;
-setMethod(&quot;saveAsParquetFile&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;signature(x = &quot;SparkDataFrame&quot;, path = &quot;character&quot;),&lt;/li&gt;
	&lt;li&gt;function(x, path) 
{
-            .Deprecated(&quot;write.parquet&quot;)
-            write.parquet(x, path)
-          }
&lt;p&gt;)&lt;br/&gt;
-&lt;br/&gt;
 #&apos; Save the content of SparkDataFrame in a text file at the specified path.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Save the content of the SparkDataFrame in a text file at the specified path.&lt;br/&gt;
@@ -2739,15 +2730,29 @@ setMethod(&quot;union&quot;,&lt;br/&gt;
             dataFrame(unioned)&lt;br/&gt;
           })&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-#&apos; unionAll is deprecated - use union instead&lt;br/&gt;
-#&apos; @rdname union&lt;br/&gt;
-#&apos; @name unionAll&lt;br/&gt;
+#&apos; Return a new SparkDataFrame containing the union of rows.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; This is an alias for \code&lt;/p&gt;
{union}.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; @param x a SparkDataFrame.&lt;br/&gt;
+#&apos; @param y a SparkDataFrame.&lt;br/&gt;
+#&apos; @return A SparkDataFrame containing the result of the unionAll operation.&lt;br/&gt;
+#&apos; @family SparkDataFrame functions&lt;br/&gt;
 #&apos; @aliases unionAll,SparkDataFrame,SparkDataFrame-method&lt;br/&gt;
+#&apos; @rdname unionAll&lt;br/&gt;
+#&apos; @name unionAll&lt;br/&gt;
+#&apos; @seealso \link{union}
&lt;p&gt;+#&apos; @examples&lt;br/&gt;
+#&apos;\dontrun&lt;/p&gt;
{
+#&apos; sparkR.session()
+#&apos; df1 &amp;lt;- read.json(path)
+#&apos; df2 &amp;lt;- read.json(path2)
+#&apos; unionAllDF &amp;lt;- unionAll(df1, df2)
+#&apos; }
&lt;p&gt; #&apos; @note unionAll since 1.4.0&lt;br/&gt;
 setMethod(&quot;unionAll&quot;,&lt;br/&gt;
           signature(x = &quot;SparkDataFrame&quot;, y = &quot;SparkDataFrame&quot;),&lt;br/&gt;
           function(x, y) &lt;/p&gt;
{
-            .Deprecated(&quot;union&quot;)
             union(x, y)
           }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;@@ -2955,6 +2960,9 @@ setMethod(&quot;exceptAll&quot;,&lt;br/&gt;
 #&apos; @param source a name for external data source.&lt;br/&gt;
 #&apos; @param mode one of &apos;append&apos;, &apos;overwrite&apos;, &apos;error&apos;, &apos;errorifexists&apos;, &apos;ignore&apos;&lt;br/&gt;
 #&apos;             save mode (it is &apos;error&apos; by default)&lt;br/&gt;
+#&apos; @param partitionBy a name or a list of names of columns to partition the output by on the file&lt;br/&gt;
+#&apos;                    system. If specified, the output is laid out on the file system similar&lt;br/&gt;
+#&apos;                    to Hive&apos;s partitioning scheme.&lt;br/&gt;
 #&apos; @param ... additional argument(s) passed to the method.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @family SparkDataFrame functions&lt;br/&gt;
@@ -2966,13 +2974,13 @@ setMethod(&quot;exceptAll&quot;,&lt;br/&gt;
 #&apos; sparkR.session()&lt;br/&gt;
 #&apos; path &amp;lt;- &quot;path/to/file.json&quot;&lt;br/&gt;
 #&apos; df &amp;lt;- read.json(path)&lt;br/&gt;
-#&apos; write.df(df, &quot;myfile&quot;, &quot;parquet&quot;, &quot;overwrite&quot;)&lt;br/&gt;
+#&apos; write.df(df, &quot;myfile&quot;, &quot;parquet&quot;, &quot;overwrite&quot;, partitionBy = c(&quot;col1&quot;, &quot;col2&quot;))&lt;br/&gt;
 #&apos; saveDF(df, parquetPath2, &quot;parquet&quot;, mode = &quot;append&quot;, mergeSchema = TRUE)&lt;br/&gt;
 #&apos; }&lt;br/&gt;
 #&apos; @note write.df since 1.4.0&lt;br/&gt;
 setMethod(&quot;write.df&quot;,&lt;br/&gt;
           signature(df = &quot;SparkDataFrame&quot;),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;function(df, path = NULL, source = NULL, mode = &quot;error&quot;, ...) {&lt;br/&gt;
+          function(df, path = NULL, source = NULL, mode = &quot;error&quot;, partitionBy = NULL, ...) {&lt;br/&gt;
             if (!is.null(path) &amp;amp;&amp;amp; !is.character(path)) 
{
               stop(&quot;path should be character, NULL or omitted.&quot;)
             }
&lt;p&gt;@@ -2986,8 +2994,18 @@ setMethod(&quot;write.df&quot;,&lt;br/&gt;
             if (is.null(source)) &lt;/p&gt;
{
               source &amp;lt;- getDefaultSqlSource()
             }
&lt;p&gt;+            cols &amp;lt;- NULL&lt;br/&gt;
+            if (!is.null(partitionBy)) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+              if (!all(sapply(partitionBy, function(c) is.character(c)))) {
+                stop(&quot;All partitionBy column names should be characters.&quot;)
+              }+              cols &amp;lt;- as.list(partitionBy)+            }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;             write &amp;lt;- callJMethod(df@sdf, &quot;write&quot;)&lt;br/&gt;
             write &amp;lt;- callJMethod(write, &quot;format&quot;, source)&lt;br/&gt;
+            if (!is.null(cols)) &lt;/p&gt;
{
+              write &amp;lt;- callJMethod(write, &quot;partitionBy&quot;, cols)
+            }
&lt;p&gt;             write &amp;lt;- setWriteOptions(write, path = path, mode = mode, ...)&lt;br/&gt;
             write &amp;lt;- handledCallJMethod(write, &quot;save&quot;)&lt;br/&gt;
           })&lt;br/&gt;
@@ -3986,7 +4004,17 @@ setMethod(&quot;hint&quot;,&lt;br/&gt;
           signature(x = &quot;SparkDataFrame&quot;, name = &quot;character&quot;),&lt;br/&gt;
           function(x, name, ...) {&lt;br/&gt;
             parameters &amp;lt;- list(...)&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;stopifnot(all(sapply(parameters, is.character)))&lt;br/&gt;
+            if (!all(sapply(parameters, function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_up.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; {&lt;br/&gt;
+              if (is.character&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_up.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; || is.numeric&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_up.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;) 
{
+                TRUE
+              }
&lt;p&gt; else if (is.list&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_up.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+                all(sapply(y, function(z) { is.character(z) || is.numeric(z) }))+              }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; else &lt;/p&gt;
{
+                FALSE
+              }
&lt;p&gt;+            }))) &lt;/p&gt;
{
+              stop(&quot;sql hint should be character, numeric, or list with character or numeric.&quot;)
+            }
&lt;p&gt;             jdf &amp;lt;- callJMethod(x@sdf, &quot;hint&quot;, name, parameters)&lt;br/&gt;
             dataFrame(jdf)&lt;br/&gt;
           })&lt;br/&gt;
diff --git a/R/pkg/R/SQLContext.R b/R/pkg/R/SQLContext.R&lt;br/&gt;
index c819a7d14ae98..afcdd6faa849d 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/R/pkg/R/SQLContext.R&lt;br/&gt;
+++ b/R/pkg/R/SQLContext.R&lt;br/&gt;
@@ &lt;del&gt;37,37 +37,6 @@ getInternalType &amp;lt;&lt;/del&gt; function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; 
{
          stop(paste(&quot;Unsupported type for SparkDataFrame:&quot;, class(x))))
 }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-#&apos; Temporary function to reroute old S3 Method call to new&lt;br/&gt;
-#&apos; This function is specifically implemented to remove SQLContext from the parameter list.&lt;br/&gt;
-#&apos; It determines the target to route the call by checking the parent of this callsite (say &apos;func&apos;).&lt;br/&gt;
-#&apos; The target should be called &apos;func.default&apos;.&lt;br/&gt;
-#&apos; We need to check the class of x to ensure it is SQLContext/HiveContext before dispatching.&lt;br/&gt;
-#&apos; @param newFuncSig name of the function the user should call instead in the deprecation message&lt;br/&gt;
-#&apos; @param x the first parameter of the original call&lt;br/&gt;
-#&apos; @param ... the rest of parameter to pass along&lt;br/&gt;
-#&apos; @return whatever the target returns&lt;br/&gt;
-#&apos; @noRd&lt;br/&gt;
&lt;del&gt;dispatchFunc &amp;lt;&lt;/del&gt; function(newFuncSig, x, ...) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# When called with SparkR::createDataFrame, sys.call()[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;] returns c(::, SparkR, createDataFrame)&lt;/li&gt;
	&lt;li&gt;callsite &amp;lt;- as.character(sys.call(sys.parent())[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;])&lt;/li&gt;
	&lt;li&gt;funcName &amp;lt;- callsite[&lt;span class=&quot;error&quot;&gt;&amp;#91;length(callsite)&amp;#93;&lt;/span&gt;]&lt;/li&gt;
	&lt;li&gt;f &amp;lt;- get(paste0(funcName, &quot;.default&quot;))&lt;/li&gt;
	&lt;li&gt;# Strip sqlContext from list of parameters and then pass the rest along.&lt;/li&gt;
	&lt;li&gt;contextNames &amp;lt;- c(&quot;org.apache.spark.sql.SQLContext&quot;,&lt;/li&gt;
	&lt;li&gt;&quot;org.apache.spark.sql.hive.HiveContext&quot;,&lt;/li&gt;
	&lt;li&gt;&quot;org.apache.spark.sql.hive.test.TestHiveContext&quot;,&lt;/li&gt;
	&lt;li&gt;&quot;org.apache.spark.sql.SparkSession&quot;)&lt;/li&gt;
	&lt;li&gt;if (missing&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &amp;amp;&amp;amp; length(list(...)) == 0) 
{
-    f()
-  }
&lt;p&gt; else if (class&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; == &quot;jobj&quot; &amp;amp;&amp;amp;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;any(grepl(paste(contextNames, collapse = &quot;|&quot;), getClassName.jobj&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;))) 
{
-    .Deprecated(newFuncSig, old = paste0(funcName, &quot;(sqlContext...)&quot;))
-    f(...)
-  }
&lt;p&gt; else &lt;/p&gt;
{
-    f(x, ...)
-  }
&lt;p&gt;-}&lt;br/&gt;
-&lt;br/&gt;
 #&apos; return the SparkSession&lt;br/&gt;
 #&apos; @noRd&lt;br/&gt;
 getSparkSession &amp;lt;- function() {&lt;br/&gt;
@@ &lt;del&gt;198,11 +167,10 @@ getDefaultSqlSource &amp;lt;&lt;/del&gt; function() &lt;/p&gt;
{
 #&apos; df4 &amp;lt;- createDataFrame(cars, numPartitions = 2)
 #&apos; }
&lt;p&gt; #&apos; @name createDataFrame&lt;br/&gt;
-#&apos; @method createDataFrame default&lt;br/&gt;
 #&apos; @note createDataFrame since 1.4.0&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;TODO(davies): support sampling and infer type from NA&lt;br/&gt;
&lt;del&gt;createDataFrame.default &amp;lt;&lt;/del&gt; function(data, schema = NULL, samplingRatio = 1.0,&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;numPartitions = NULL) {&lt;br/&gt;
+createDataFrame &amp;lt;- function(data, schema = NULL, samplingRatio = 1.0,&lt;br/&gt;
+                            numPartitions = NULL) {&lt;br/&gt;
   sparkSession &amp;lt;- getSparkSession()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   if (is.data.frame(data)) &lt;/p&gt;
{
@@ -285,31 +253,18 @@ createDataFrame.default &amp;lt;- function(data, schema = NULL, samplingRatio = 1.0,
   dataFrame(sdf)
 }

&lt;p&gt;&lt;del&gt;createDataFrame &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;createDataFrame(data, schema = NULL)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; @rdname createDataFrame&lt;br/&gt;
 #&apos; @aliases createDataFrame&lt;br/&gt;
-#&apos; @method as.DataFrame default&lt;br/&gt;
 #&apos; @note as.DataFrame since 1.6.0&lt;br/&gt;
&lt;del&gt;as.DataFrame.default &amp;lt;&lt;/del&gt; function(data, schema = NULL, samplingRatio = 1.0, numPartitions = NULL) {&lt;br/&gt;
+as.DataFrame &amp;lt;- function(data, schema = NULL, samplingRatio = 1.0, numPartitions = NULL) &lt;/p&gt;
{
   createDataFrame(data, schema, samplingRatio, numPartitions)
 }

&lt;p&gt;-#&apos; @param ... additional argument(s).&lt;br/&gt;
-#&apos; @rdname createDataFrame&lt;br/&gt;
-#&apos; @aliases as.DataFrame&lt;br/&gt;
&lt;del&gt;as.DataFrame &amp;lt;&lt;/del&gt; function(data, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;as.DataFrame(data, schema = NULL)&quot;, data, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; toDF&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Converts an RDD to a SparkDataFrame by infer the types.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @param x An RDD&lt;br/&gt;
-#&apos;&lt;br/&gt;
 #&apos; @rdname SparkDataFrame&lt;br/&gt;
 #&apos; @noRd&lt;br/&gt;
 #&apos; @examples&lt;br/&gt;
@@ -343,12 +298,10 @@ setMethod(&quot;toDF&quot;, signature(x = &quot;RDD&quot;),&lt;br/&gt;
 #&apos; path &amp;lt;- &quot;path/to/file.json&quot;&lt;br/&gt;
 #&apos; df &amp;lt;- read.json(path)&lt;br/&gt;
 #&apos; df &amp;lt;- read.json(path, multiLine = TRUE)&lt;br/&gt;
&lt;del&gt;#&apos; df &amp;lt;&lt;/del&gt; jsonFile(path)&lt;br/&gt;
 #&apos; }&lt;br/&gt;
 #&apos; @name read.json&lt;br/&gt;
-#&apos; @method read.json default&lt;br/&gt;
 #&apos; @note read.json since 1.6.0&lt;br/&gt;
&lt;del&gt;read.json.default &amp;lt;&lt;/del&gt; function(path, ...) {&lt;br/&gt;
+read.json &amp;lt;- function(path, ...) {&lt;br/&gt;
   sparkSession &amp;lt;- getSparkSession()&lt;br/&gt;
   options &amp;lt;- varargsToStrEnv(...)&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Allow the user to have a more flexible definition of the text file path&lt;br/&gt;
@@ &lt;del&gt;359,55 +312,6 @@ read.json.default &amp;lt;&lt;/del&gt; function(path, ...) 
{
   dataFrame(sdf)
 }&lt;br/&gt;
 &lt;br/&gt;
&lt;del&gt;read.json &amp;lt;&lt;/del&gt; function(x, ...) {
-  dispatchFunc(&quot;read.json(path)&quot;, x, ...)
-}&lt;br/&gt;
-&lt;br/&gt;
-#&apos; @rdname read.json&lt;br/&gt;
-#&apos; @name jsonFile&lt;br/&gt;
-#&apos; @method jsonFile default&lt;br/&gt;
-#&apos; @note jsonFile since 1.4.0&lt;br/&gt;
&lt;del&gt;jsonFile.default &amp;lt;&lt;/del&gt; function(path) {
-  .Deprecated(&quot;read.json&quot;)
-  read.json(path)
-}&lt;br/&gt;
-&lt;br/&gt;
&lt;del&gt;jsonFile &amp;lt;&lt;/del&gt; function(x, ...) {
-  dispatchFunc(&quot;jsonFile(path)&quot;, x, ...)
-}&lt;br/&gt;
-&lt;br/&gt;
-#&apos; JSON RDD&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; Loads an RDD storing one JSON object per string as a SparkDataFrame.&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; @param sqlContext SQLContext to use&lt;br/&gt;
-#&apos; @param rdd An RDD of JSON string&lt;br/&gt;
-#&apos; @param schema A StructType object to use as schema&lt;br/&gt;
-#&apos; @param samplingRatio The ratio of simpling used to infer the schema&lt;br/&gt;
-#&apos; @return A SparkDataFrame&lt;br/&gt;
-#&apos; @noRd&lt;br/&gt;
-#&apos; @examples&lt;br/&gt;
-#&apos;\dontrun{
-#&apos; sparkR.session()
-#&apos; rdd &amp;lt;- texFile(sc, &quot;path/to/json&quot;)
-#&apos; df &amp;lt;- jsonRDD(sqlContext, rdd)
-#&apos;}&lt;br/&gt;
-&lt;br/&gt;
-# TODO: remove - this method is no longer exported&lt;br/&gt;
-# TODO: support schema&lt;br/&gt;
&lt;del&gt;jsonRDD &amp;lt;&lt;/del&gt; function(sqlContext, rdd, schema = NULL, samplingRatio = 1.0) {&lt;br/&gt;
-  .Deprecated(&quot;read.json&quot;)&lt;br/&gt;
-  rdd &amp;lt;- serializeToString(rdd)&lt;br/&gt;
-  if (is.null(schema)) {
-    read &amp;lt;- callJMethod(sqlContext, &quot;read&quot;)
-    # samplingRatio is deprecated
-    sdf &amp;lt;- callJMethod(read, &quot;json&quot;, callJMethod(getJRDD(rdd), &quot;rdd&quot;))
-    dataFrame(sdf)
-  } else {
-    stop(&quot;not implemented&quot;)
-  }&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
 #&apos; Create a SparkDataFrame from an ORC file.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Loads an ORC file, returning the result as a SparkDataFrame.&lt;br/&gt;
@@ &lt;del&gt;434,12 +338,12 @@ read.orc &amp;lt;&lt;/del&gt; function(path, ...) {&lt;br/&gt;
 #&apos; Loads a Parquet file, returning the result as a SparkDataFrame.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @param path path of file to read. A vector of multiple paths is allowed.&lt;br/&gt;
+#&apos; @param ... additional data source specific named properties.&lt;br/&gt;
 #&apos; @return SparkDataFrame&lt;br/&gt;
 #&apos; @rdname read.parquet&lt;br/&gt;
 #&apos; @name read.parquet&lt;br/&gt;
-#&apos; @method read.parquet default&lt;br/&gt;
 #&apos; @note read.parquet since 1.6.0&lt;br/&gt;
&lt;del&gt;read.parquet.default &amp;lt;&lt;/del&gt; function(path, ...) {&lt;br/&gt;
+read.parquet &amp;lt;- function(path, ...) {&lt;br/&gt;
   sparkSession &amp;lt;- getSparkSession()&lt;br/&gt;
   options &amp;lt;- varargsToStrEnv(...)&lt;br/&gt;
   # Allow the user to have a more flexible definition of the Parquet file path&lt;br/&gt;
@@ &lt;del&gt;450,24 +354,6 @@ read.parquet.default &amp;lt;&lt;/del&gt; function(path, ...) {   dataFrame(sdf) }&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;del&gt;read.parquet &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;read.parquet(...)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
-#&apos; @param ... argument(s) passed to the method.&lt;br/&gt;
-#&apos; @rdname read.parquet&lt;br/&gt;
-#&apos; @name parquetFile&lt;br/&gt;
-#&apos; @method parquetFile default&lt;br/&gt;
-#&apos; @note parquetFile since 1.4.0&lt;br/&gt;
&lt;del&gt;parquetFile.default &amp;lt;&lt;/del&gt; function(...) &lt;/p&gt;
{
-  .Deprecated(&quot;read.parquet&quot;)
-  read.parquet(unlist(list(...)))
-}
&lt;p&gt;-&lt;br/&gt;
&lt;del&gt;parquetFile &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;parquetFile(...)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; Create a SparkDataFrame from a text file.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Loads text files and returns a SparkDataFrame whose schema starts with&lt;br/&gt;
@@ &lt;del&gt;487,9 +373,8 @@ parquetFile &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
 #&apos; df &amp;lt;- read.text(path)
 #&apos; }
&lt;p&gt; #&apos; @name read.text&lt;br/&gt;
-#&apos; @method read.text default&lt;br/&gt;
 #&apos; @note read.text since 1.6.1&lt;br/&gt;
&lt;del&gt;read.text.default &amp;lt;&lt;/del&gt; function(path, ...) {&lt;br/&gt;
+read.text &amp;lt;- function(path, ...) {&lt;br/&gt;
   sparkSession &amp;lt;- getSparkSession()&lt;br/&gt;
   options &amp;lt;- varargsToStrEnv(...)&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Allow the user to have a more flexible definition of the text file path&lt;br/&gt;
@@ &lt;del&gt;500,10 +385,6 @@ read.text.default &amp;lt;&lt;/del&gt; function(path, ...) 
{
   dataFrame(sdf)
 }&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;del&gt;read.text &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;read.text(path)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; SQL Query&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Executes a SQL query using Spark, returning the result as a SparkDataFrame.&lt;br/&gt;
@@ &lt;del&gt;520,18 +401,13 @@ read.text &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
 #&apos; new_df &amp;lt;- sql(&quot;SELECT * FROM table&quot;)
 #&apos; }
&lt;p&gt; #&apos; @name sql&lt;br/&gt;
-#&apos; @method sql default&lt;br/&gt;
 #&apos; @note sql since 1.4.0&lt;br/&gt;
&lt;del&gt;sql.default &amp;lt;&lt;/del&gt; function(sqlQuery) {&lt;br/&gt;
+sql &amp;lt;- function(sqlQuery) &lt;/p&gt;
{
   sparkSession &amp;lt;- getSparkSession()
   sdf &amp;lt;- callJMethod(sparkSession, &quot;sql&quot;, sqlQuery)
   dataFrame(sdf)
 }

&lt;p&gt;&lt;del&gt;sql &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;sql(sqlQuery)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; Create a SparkDataFrame from a SparkSQL table or view&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Returns the specified table or view as a SparkDataFrame. The table or view must already exist or&lt;br/&gt;
@@ &lt;del&gt;590,9 +466,8 @@ tableToDF &amp;lt;&lt;/del&gt; function(tableName) &lt;/p&gt;
{
 #&apos; df4 &amp;lt;- read.df(mapTypeJsonPath, &quot;json&quot;, stringSchema, multiLine = TRUE)
 #&apos; }
&lt;p&gt; #&apos; @name read.df&lt;br/&gt;
-#&apos; @method read.df default&lt;br/&gt;
 #&apos; @note read.df since 1.4.0&lt;br/&gt;
&lt;del&gt;read.df.default &amp;lt;&lt;/del&gt; function(path = NULL, source = NULL, schema = NULL, na.strings = &quot;NA&quot;, ...) {&lt;br/&gt;
+read.df &amp;lt;- function(path = NULL, source = NULL, schema = NULL, na.strings = &quot;NA&quot;, ...) {&lt;br/&gt;
   if (!is.null(path) &amp;amp;&amp;amp; !is.character(path)) &lt;/p&gt;
{
     stop(&quot;path should be character, NULL or omitted.&quot;)
   }
&lt;p&gt;@@ &lt;del&gt;627,22 +502,13 @@ read.df.default &amp;lt;&lt;/del&gt; function(path = NULL, source = NULL, schema = NULL, na.string&lt;br/&gt;
   dataFrame(sdf)&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt;&lt;del&gt;read.df &amp;lt;&lt;/del&gt; function(x = NULL, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;read.df(path = NULL, source = NULL, schema = NULL, ...)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; @rdname read.df&lt;br/&gt;
 #&apos; @name loadDF&lt;br/&gt;
-#&apos; @method loadDF default&lt;br/&gt;
 #&apos; @note loadDF since 1.6.0&lt;br/&gt;
&lt;del&gt;loadDF.default &amp;lt;&lt;/del&gt; function(path = NULL, source = NULL, schema = NULL, ...) {&lt;br/&gt;
+loadDF &amp;lt;- function(path = NULL, source = NULL, schema = NULL, ...) &lt;/p&gt;
{
   read.df(path, source, schema, ...)
 }

&lt;p&gt;&lt;del&gt;loadDF &amp;lt;&lt;/del&gt; function(x = NULL, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;loadDF(path = NULL, source = NULL, schema = NULL, ...)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; Create a SparkDataFrame representing the database table accessible via JDBC URL&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Additional JDBC database connection properties can be set (...)&lt;br/&gt;
diff --git a/R/pkg/R/catalog.R b/R/pkg/R/catalog.R&lt;br/&gt;
index baf4d861fcf86..7641f8a7a0432 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/R/catalog.R&lt;br/&gt;
+++ b/R/pkg/R/catalog.R&lt;br/&gt;
@@ -17,40 +17,6 @@&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;catalog.R: SparkSession catalog functions&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;-#&apos; (Deprecated) Create an external table&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; Creates an external table based on the dataset in a data source,&lt;br/&gt;
-#&apos; Returns a SparkDataFrame associated with the external table.&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; The data source is specified by the \code&lt;/p&gt;
{source} and a set of options(...).&lt;br/&gt;
-#&apos; If \code{source}
&lt;p&gt; is not specified, the default data source configured by&lt;br/&gt;
-#&apos; &quot;spark.sql.sources.default&quot; will be used.&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; @param tableName a name of the table.&lt;br/&gt;
-#&apos; @param path the path of files to load.&lt;br/&gt;
-#&apos; @param source the name of external data source.&lt;br/&gt;
-#&apos; @param schema the schema of the data required for some data sources.&lt;br/&gt;
-#&apos; @param ... additional argument(s) passed to the method.&lt;br/&gt;
-#&apos; @return A SparkDataFrame.&lt;br/&gt;
-#&apos; @rdname createExternalTable-deprecated&lt;br/&gt;
-#&apos; @seealso \link&lt;/p&gt;
{createTable}
&lt;p&gt;-#&apos; @examples&lt;br/&gt;
-#&apos;\dontrun&lt;/p&gt;
{
-#&apos; sparkR.session()
-#&apos; df &amp;lt;- createExternalTable(&quot;myjson&quot;, path=&quot;path/to/json&quot;, source=&quot;json&quot;, schema)
-#&apos; }
&lt;p&gt;-#&apos; @name createExternalTable&lt;br/&gt;
-#&apos; @method createExternalTable default&lt;br/&gt;
-#&apos; @note createExternalTable since 1.4.0&lt;br/&gt;
&lt;del&gt;createExternalTable.default &amp;lt;&lt;/del&gt; function(tableName, path = NULL, source = NULL, schema = NULL, ...) &lt;/p&gt;
{
-  .Deprecated(&quot;createTable&quot;, old = &quot;createExternalTable&quot;)
-  createTable(tableName, path, source, schema, ...)
-}
&lt;p&gt;-&lt;br/&gt;
&lt;del&gt;createExternalTable &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;createExternalTable(tableName, path = NULL, source = NULL, ...)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; Creates a table based on the dataset in a data source&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Creates a table based on the dataset in a data source. Returns a SparkDataFrame associated with&lt;br/&gt;
@@ &lt;del&gt;69,7 +35,6 @@ createExternalTable &amp;lt;&lt;/del&gt; function(x, ...) {&lt;br/&gt;
 #&apos; @param ... additional named parameters as options for the data source.&lt;br/&gt;
 #&apos; @return A SparkDataFrame.&lt;br/&gt;
 #&apos; @rdname createTable&lt;br/&gt;
-#&apos; @seealso \link&lt;/p&gt;
{createExternalTable}
&lt;p&gt; #&apos; @examples&lt;br/&gt;
 #&apos;\dontrun&lt;/p&gt;
{
 #&apos; sparkR.session()
@@ -117,18 +82,13 @@ createTable &amp;lt;- function(tableName, path = NULL, source = NULL, schema = NULL, ..
 #&apos; cacheTable(&quot;table&quot;)
 #&apos; }
&lt;p&gt; #&apos; @name cacheTable&lt;br/&gt;
-#&apos; @method cacheTable default&lt;br/&gt;
 #&apos; @note cacheTable since 1.4.0&lt;br/&gt;
&lt;del&gt;cacheTable.default &amp;lt;&lt;/del&gt; function(tableName) {&lt;br/&gt;
+cacheTable &amp;lt;- function(tableName) &lt;/p&gt;
{
   sparkSession &amp;lt;- getSparkSession()
   catalog &amp;lt;- callJMethod(sparkSession, &quot;catalog&quot;)
   invisible(handledCallJMethod(catalog, &quot;cacheTable&quot;, tableName))
 }

&lt;p&gt;&lt;del&gt;cacheTable &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;cacheTable(tableName)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; Uncache Table&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Removes the specified table from the in-memory cache.&lt;br/&gt;
@@ &lt;del&gt;146,18 +106,13 @@ cacheTable &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
 #&apos; uncacheTable(&quot;table&quot;)
 #&apos; }
&lt;p&gt; #&apos; @name uncacheTable&lt;br/&gt;
-#&apos; @method uncacheTable default&lt;br/&gt;
 #&apos; @note uncacheTable since 1.4.0&lt;br/&gt;
&lt;del&gt;uncacheTable.default &amp;lt;&lt;/del&gt; function(tableName) {&lt;br/&gt;
+uncacheTable &amp;lt;- function(tableName) &lt;/p&gt;
{
   sparkSession &amp;lt;- getSparkSession()
   catalog &amp;lt;- callJMethod(sparkSession, &quot;catalog&quot;)
   invisible(handledCallJMethod(catalog, &quot;uncacheTable&quot;, tableName))
 }

&lt;p&gt;&lt;del&gt;uncacheTable &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;uncacheTable(tableName)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; Clear Cache&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Removes all cached tables from the in-memory cache.&lt;br/&gt;
@@ &lt;del&gt;168,48 +123,13 @@ uncacheTable &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
 #&apos; clearCache()
 #&apos; }
&lt;p&gt; #&apos; @name clearCache&lt;br/&gt;
-#&apos; @method clearCache default&lt;br/&gt;
 #&apos; @note clearCache since 1.4.0&lt;br/&gt;
&lt;del&gt;clearCache.default &amp;lt;&lt;/del&gt; function() {&lt;br/&gt;
+clearCache &amp;lt;- function() &lt;/p&gt;
{
   sparkSession &amp;lt;- getSparkSession()
   catalog &amp;lt;- callJMethod(sparkSession, &quot;catalog&quot;)
   invisible(callJMethod(catalog, &quot;clearCache&quot;))
 }

&lt;p&gt;&lt;del&gt;clearCache &amp;lt;&lt;/del&gt; function() &lt;/p&gt;
{
-  dispatchFunc(&quot;clearCache()&quot;)
-}
&lt;p&gt;-&lt;br/&gt;
-#&apos; (Deprecated) Drop Temporary Table&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; Drops the temporary table with the given table name in the catalog.&lt;br/&gt;
-#&apos; If the table has been cached/persisted before, it&apos;s also unpersisted.&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; @param tableName The name of the SparkSQL table to be dropped.&lt;br/&gt;
-#&apos; @seealso \link&lt;/p&gt;
{dropTempView}
&lt;p&gt;-#&apos; @rdname dropTempTable-deprecated&lt;br/&gt;
-#&apos; @examples&lt;br/&gt;
-#&apos; \dontrun&lt;/p&gt;
{
-#&apos; sparkR.session()
-#&apos; df &amp;lt;- read.df(path, &quot;parquet&quot;)
-#&apos; createOrReplaceTempView(df, &quot;table&quot;)
-#&apos; dropTempTable(&quot;table&quot;)
-#&apos; }
&lt;p&gt;-#&apos; @name dropTempTable&lt;br/&gt;
-#&apos; @method dropTempTable default&lt;br/&gt;
-#&apos; @note dropTempTable since 1.4.0&lt;br/&gt;
&lt;del&gt;dropTempTable.default &amp;lt;&lt;/del&gt; function(tableName) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.Deprecated(&quot;dropTempView&quot;, old = &quot;dropTempTable&quot;)&lt;/li&gt;
	&lt;li&gt;if (class(tableName) != &quot;character&quot;) 
{
-    stop(&quot;tableName must be a string.&quot;)
-  }&lt;/li&gt;
	&lt;li&gt;dropTempView(tableName)&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
&lt;del&gt;dropTempTable &amp;lt;&lt;/del&gt; function(x, ...) 
{
-  dispatchFunc(&quot;dropTempView(viewName)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; Drops the temporary view with the given view name in the catalog.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Drops the temporary view with the given view name in the catalog.&lt;br/&gt;
@@ &lt;del&gt;250,17 +170,12 @@ dropTempView &amp;lt;&lt;/del&gt; function(viewName) &lt;/p&gt;
{
 #&apos; tables(&quot;hive&quot;)
 #&apos; }
&lt;p&gt; #&apos; @name tables&lt;br/&gt;
-#&apos; @method tables default&lt;br/&gt;
 #&apos; @note tables since 1.4.0&lt;br/&gt;
&lt;del&gt;tables.default &amp;lt;&lt;/del&gt; function(databaseName = NULL) {&lt;br/&gt;
+tables &amp;lt;- function(databaseName = NULL) &lt;/p&gt;
{
   # rename column to match previous output schema
   withColumnRenamed(listTables(databaseName), &quot;name&quot;, &quot;tableName&quot;)
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;del&gt;tables &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;tables(databaseName = NULL)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; Table Names&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Returns the names of tables in the given database as an array.&lt;br/&gt;
@@ &lt;del&gt;274,9 +189,8 @@ tables &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
 #&apos; tableNames(&quot;hive&quot;)
 #&apos; }
&lt;p&gt; #&apos; @name tableNames&lt;br/&gt;
-#&apos; @method tableNames default&lt;br/&gt;
 #&apos; @note tableNames since 1.4.0&lt;br/&gt;
&lt;del&gt;tableNames.default &amp;lt;&lt;/del&gt; function(databaseName = NULL) {&lt;br/&gt;
+tableNames &amp;lt;- function(databaseName = NULL) {&lt;br/&gt;
   sparkSession &amp;lt;- getSparkSession()&lt;br/&gt;
   callJStatic(&quot;org.apache.spark.sql.api.r.SQLUtils&quot;,&lt;br/&gt;
               &quot;getTableNames&quot;,&lt;br/&gt;
@@ &lt;del&gt;284,10 +198,6 @@ tableNames.default &amp;lt;&lt;/del&gt; function(databaseName = NULL) &lt;/p&gt;
{
               databaseName)
 }

&lt;p&gt;&lt;del&gt;tableNames &amp;lt;&lt;/del&gt; function(x, ...) &lt;/p&gt;
{
-  dispatchFunc(&quot;tableNames(databaseName = NULL)&quot;, x, ...)
-}
&lt;p&gt;-&lt;br/&gt;
 #&apos; Returns the current default database&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Returns the current default database.&lt;br/&gt;
diff --git a/R/pkg/R/context.R b/R/pkg/R/context.R&lt;br/&gt;
index f168ca76b6007..e99136723f65b 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/R/context.R&lt;br/&gt;
+++ b/R/pkg/R/context.R&lt;br/&gt;
@@ &lt;del&gt;167,18 +167,30 @@ parallelize &amp;lt;&lt;/del&gt; function(sc, coll, numSlices = 1) {&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;2-tuples of raws&lt;br/&gt;
   serializedSlices &amp;lt;- lapply(slices, serialize, connection = NULL)&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# The PRC backend cannot handle arguments larger than 2GB (INT_MAX)&lt;br/&gt;
+  # The RPC backend cannot handle arguments larger than 2GB (INT_MAX)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;If serialized data is safely less than that threshold we send it over the PRC channel.&lt;/li&gt;
	&lt;li&gt;Otherwise, we write it to a file and send the file name&lt;br/&gt;
   if (objectSize &amp;lt; sizeLimit) 
{
     jrdd &amp;lt;- callJStatic(&quot;org.apache.spark.api.r.RRDD&quot;, &quot;createRDDFromArray&quot;, sc, serializedSlices)
   }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;fileName &amp;lt;- writeToTempFile(serializedSlices)&lt;/li&gt;
	&lt;li&gt;jrdd &amp;lt;- tryCatch(callJStatic(&lt;/li&gt;
	&lt;li&gt;&quot;org.apache.spark.api.r.RRDD&quot;, &quot;createRDDFromFile&quot;, sc, fileName, as.integer(numSlices)),&lt;/li&gt;
	&lt;li&gt;finally = 
{
-        file.remove(fileName)
-    }
&lt;p&gt;)&lt;br/&gt;
+    if (callJStatic(&quot;org.apache.spark.api.r.RUtils&quot;, &quot;getEncryptionEnabled&quot;, sc)) &lt;/p&gt;
{
+      # the length of slices here is the parallelism to use in the jvm&apos;s sc.parallelize()
+      parallelism &amp;lt;- as.integer(numSlices)
+      jserver &amp;lt;- newJObject(&quot;org.apache.spark.api.r.RParallelizeServer&quot;, sc, parallelism)
+      authSecret &amp;lt;- callJMethod(jserver, &quot;secret&quot;)
+      port &amp;lt;- callJMethod(jserver, &quot;port&quot;)
+      conn &amp;lt;- socketConnection(port = port, blocking = TRUE, open = &quot;wb&quot;, timeout = 1500)
+      doServerAuth(conn, authSecret)
+      writeToConnection(serializedSlices, conn)
+      jrdd &amp;lt;- callJMethod(jserver, &quot;getResult&quot;)
+    }
&lt;p&gt; else &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+      fileName &amp;lt;- writeToTempFile(serializedSlices)+      jrdd &amp;lt;- tryCatch(callJStatic(+          &amp;quot;org.apache.spark.api.r.RRDD&amp;quot;, &amp;quot;createRDDFromFile&amp;quot;, sc, fileName, as.integer(numSlices)),+        finally = {
+          file.remove(fileName)
+      })+    }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   RDD(jrdd, &quot;byte&quot;)&lt;br/&gt;
@@ &lt;del&gt;194,14 +206,21 @@ getMaxAllocationLimit &amp;lt;&lt;/del&gt; function(sc) &lt;/p&gt;
{
   ))
 }

&lt;p&gt;+writeToConnection &amp;lt;- function(serializedSlices, conn) {&lt;br/&gt;
+  tryCatch({&lt;br/&gt;
+    for (slice in serializedSlices) &lt;/p&gt;
{
+      writeBin(as.integer(length(slice)), conn, endian = &quot;big&quot;)
+      writeBin(slice, conn, endian = &quot;big&quot;)
+    }
&lt;p&gt;+  }, finally = &lt;/p&gt;
{
+    close(conn)
+  }
&lt;p&gt;)&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
 writeToTempFile &amp;lt;- function(serializedSlices) {&lt;br/&gt;
   fileName &amp;lt;- tempfile()&lt;br/&gt;
   conn &amp;lt;- file(fileName, &quot;wb&quot;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (slice in serializedSlices) 
{
-    writeBin(as.integer(length(slice)), conn, endian = &quot;big&quot;)
-    writeBin(slice, conn, endian = &quot;big&quot;)
-  }&lt;/li&gt;
	&lt;li&gt;close(conn)&lt;br/&gt;
+  writeToConnection(serializedSlices, conn)&lt;br/&gt;
   fileName&lt;br/&gt;
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/R/pkg/R/functions.R b/R/pkg/R/functions.R&lt;br/&gt;
index 572dee50127b8..f568a931ae1fe 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/R/functions.R&lt;br/&gt;
+++ b/R/pkg/R/functions.R&lt;br/&gt;
@@ -112,7 +112,7 @@ NULL&lt;br/&gt;
 #&apos; df &amp;lt;- createDataFrame(cbind(model = rownames(mtcars), mtcars))&lt;br/&gt;
 #&apos; tmp &amp;lt;- mutate(df, v1 = log(df$mpg), v2 = cbrt(df$disp),&lt;br/&gt;
 #&apos;                   v3 = bround(df$wt, 1), v4 = bin(df$cyl),&lt;br/&gt;
-#&apos;                   v5 = hex(df$wt), v6 = toDegrees(df$gear),&lt;br/&gt;
+#&apos;                   v5 = hex(df$wt), v6 = degrees(df$gear),&lt;br/&gt;
 #&apos;                   v7 = atan2(df$cyl, df$am), v8 = hypot(df$cyl, df$am),&lt;br/&gt;
 #&apos;                   v9 = pmod(df$hp, df$cyl), v10 = shiftLeft(df$disp, 1),&lt;br/&gt;
 #&apos;                   v11 = conv(df$hp, 10, 16), v12 = sign(df$vs - 0.5),&lt;br/&gt;
@@ -187,7 +187,9 @@ NULL&lt;br/&gt;
 #&apos;          \itemize{&lt;br/&gt;
 #&apos;          \item \code&lt;/p&gt;
{to_json}: it is the column containing the struct, array of the structs,&lt;br/&gt;
 #&apos;              the map or array of maps.&lt;br/&gt;
+#&apos;          \item \code{to_csv}: it is the column containing the struct.&lt;br/&gt;
 #&apos;          \item \code{from_json}: it is the column containing the JSON string.&lt;br/&gt;
+#&apos;          \item \code{from_csv}: it is the column containing the CSV string.&lt;br/&gt;
 #&apos;          }&lt;br/&gt;
 #&apos; @param y Column to compute on.&lt;br/&gt;
 #&apos; @param value A value to compute on.&lt;br/&gt;
@@ -196,10 +198,25 @@ NULL&lt;br/&gt;
 #&apos;          \item \code{array_position}: a value to locate in the given array.&lt;br/&gt;
 #&apos;          \item \code{array_remove}: a value to remove in the given array.&lt;br/&gt;
 #&apos;          }&lt;br/&gt;
-#&apos; @param ... additional argument(s). In \code{to_json}
&lt;p&gt; and \code&lt;/p&gt;
{from_json}, this contains&lt;br/&gt;
-#&apos;            additional named properties to control how it is converted, accepts the same&lt;br/&gt;
-#&apos;            options as the JSON data source.  In \code{arrays_zip}, this contains additional&lt;br/&gt;
-#&apos;            Columns of arrays to be merged.&lt;br/&gt;
+#&apos; @param schema&lt;br/&gt;
+#&apos;          \itemize{&lt;br/&gt;
+#&apos;          \item \code{from_json}
&lt;p&gt;: a structType object to use as the schema to use&lt;br/&gt;
+#&apos;              when parsing the JSON string. Since Spark 2.3, the DDL-formatted string is&lt;br/&gt;
+#&apos;              also supported for the schema.&lt;br/&gt;
+#&apos;          \item \code&lt;/p&gt;
{from_csv}: a DDL-formatted string&lt;br/&gt;
+#&apos;          }&lt;br/&gt;
+#&apos; @param ... additional argument(s).&lt;br/&gt;
+#&apos;          \itemize{&lt;br/&gt;
+#&apos;          \item \code{to_json}, \code{from_json} and \code{schema_of_json}: this contains&lt;br/&gt;
+#&apos;              additional named properties to control how it is converted and accepts the&lt;br/&gt;
+#&apos;              same options as the JSON data source.&lt;br/&gt;
+#&apos;          \item \code{to_json}: it supports the &quot;pretty&quot; option which enables pretty&lt;br/&gt;
+#&apos;              JSON generation.&lt;br/&gt;
+#&apos;          \item \code{to_csv}, \code{from_csv}
&lt;p&gt; and \code&lt;/p&gt;
{schema_of_csv}: this contains&lt;br/&gt;
+#&apos;              additional named properties to control how it is converted and accepts the&lt;br/&gt;
+#&apos;              same options as the CSV data source.&lt;br/&gt;
+#&apos;          \item \code{arrays_zip}, this contains additional Columns of arrays to be merged.&lt;br/&gt;
+#&apos;          }&lt;br/&gt;
 #&apos; @name column_collection_functions&lt;br/&gt;
 #&apos; @rdname column_collection_functions&lt;br/&gt;
 #&apos; @family collection functions&lt;br/&gt;
@@ -310,23 +327,37 @@ setMethod(&quot;acos&quot;,&lt;br/&gt;
           })&lt;br/&gt;
 &lt;br/&gt;
 #&apos; @details&lt;br/&gt;
-#&apos; \code{approxCountDistinct}: Returns the approximate number of distinct items in a group.&lt;br/&gt;
+#&apos; \code{approx_count_distinct}: Returns the approximate number of distinct items in a group.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @rdname column_aggregate_functions&lt;br/&gt;
-#&apos; @aliases approxCountDistinct approxCountDistinct,Column-method&lt;br/&gt;
+#&apos; @aliases approx_count_distinct approx_count_distinct,Column-method&lt;br/&gt;
 #&apos; @examples&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; \dontrun{
-#&apos; head(select(df, approxCountDistinct(df$gear)))
-#&apos; head(select(df, approxCountDistinct(df$gear, 0.02)))
+#&apos; head(select(df, approx_count_distinct(df$gear)))
+#&apos; head(select(df, approx_count_distinct(df$gear, 0.02)))
 #&apos; head(select(df, countDistinct(df$gear, df$cyl)))
 #&apos; head(select(df, n_distinct(df$gear)))
 #&apos; head(distinct(select(df, &quot;gear&quot;)))}&lt;br/&gt;
+#&apos; @note approx_count_distinct(Column) since 3.0.0&lt;br/&gt;
+setMethod(&quot;approx_count_distinct&quot;,&lt;br/&gt;
+          signature(x = &quot;Column&quot;),&lt;br/&gt;
+          function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; {
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;approx_count_distinct&quot;, x@jc)
+            column(jc)
+          })&lt;br/&gt;
+&lt;br/&gt;
+#&apos; @details&lt;br/&gt;
+#&apos; \code{approxCountDistinct}: Returns the approximate number of distinct items in a group.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; @rdname column_aggregate_functions&lt;br/&gt;
+#&apos; @aliases approxCountDistinct approxCountDistinct,Column-method&lt;br/&gt;
 #&apos; @note approxCountDistinct(Column) since 1.4.0&lt;br/&gt;
 setMethod(&quot;approxCountDistinct&quot;,&lt;br/&gt;
           signature(x = &quot;Column&quot;),&lt;br/&gt;
           function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; {
-            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;approxCountDistinct&quot;, x@jc)
+            .Deprecated(&quot;approx_count_distinct&quot;)
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;approx_count_distinct&quot;, x@jc)
             column(jc)
           })&lt;br/&gt;
 &lt;br/&gt;
@@ -1641,7 +1672,22 @@ setMethod(&quot;tanh&quot;,&lt;br/&gt;
 setMethod(&quot;toDegrees&quot;,&lt;br/&gt;
           signature(x = &quot;Column&quot;),&lt;br/&gt;
           function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; {
-            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;toDegrees&quot;, x@jc)
+            .Deprecated(&quot;degrees&quot;)
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;degrees&quot;, x@jc)
+            column(jc)
+          })&lt;br/&gt;
+&lt;br/&gt;
+#&apos; @details&lt;br/&gt;
+#&apos; \code{degrees}: Converts an angle measured in radians to an approximately equivalent angle&lt;br/&gt;
+#&apos; measured in degrees.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; @rdname column_math_functions&lt;br/&gt;
+#&apos; @aliases degrees degrees,Column-method&lt;br/&gt;
+#&apos; @note degrees since 3.0.0&lt;br/&gt;
+setMethod(&quot;degrees&quot;,&lt;br/&gt;
+          signature(x = &quot;Column&quot;),&lt;br/&gt;
+          function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; {
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;degrees&quot;, x@jc)
             column(jc)
           })&lt;br/&gt;
 &lt;br/&gt;
@@ -1655,7 +1701,22 @@ setMethod(&quot;toDegrees&quot;,&lt;br/&gt;
 setMethod(&quot;toRadians&quot;,&lt;br/&gt;
           signature(x = &quot;Column&quot;),&lt;br/&gt;
           function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; {
-            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;toRadians&quot;, x@jc)
+            .Deprecated(&quot;radians&quot;)
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;radians&quot;, x@jc)
+            column(jc)
+          })&lt;br/&gt;
+&lt;br/&gt;
+#&apos; @details&lt;br/&gt;
+#&apos; \code{radians}: Converts an angle measured in degrees to an approximately equivalent angle&lt;br/&gt;
+#&apos; measured in radians.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; @rdname column_math_functions&lt;br/&gt;
+#&apos; @aliases radians radians,Column-method&lt;br/&gt;
+#&apos; @note radians since 3.0.0&lt;br/&gt;
+setMethod(&quot;radians&quot;,&lt;br/&gt;
+          signature(x = &quot;Column&quot;),&lt;br/&gt;
+          function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; {
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;radians&quot;, x@jc)
             column(jc)
           })&lt;br/&gt;
 &lt;br/&gt;
@@ -1717,12 +1778,16 @@ setMethod(&quot;to_date&quot;,&lt;br/&gt;
 #&apos; df2 &amp;lt;- mutate(df2, people_json = to_json(df2$people))&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; # Converts a map into a JSON object&lt;br/&gt;
&lt;del&gt;#&apos; df2 &amp;lt;&lt;/del&gt; sql(&quot;SELECT map(&apos;name&apos;, &apos;Bob&apos;)) as people&quot;)&lt;br/&gt;
+#&apos; df2 &amp;lt;- sql(&quot;SELECT map(&apos;name&apos;, &apos;Bob&apos;) as people&quot;)&lt;br/&gt;
 #&apos; df2 &amp;lt;- mutate(df2, people_json = to_json(df2$people))&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; # Converts an array of maps into a JSON array&lt;br/&gt;
 #&apos; df2 &amp;lt;- sql(&quot;SELECT array(map(&apos;name&apos;, &apos;Bob&apos;), map(&apos;name&apos;, &apos;Alice&apos;)) as people&quot;)&lt;br/&gt;
&lt;del&gt;#&apos; df2 &amp;lt;&lt;/del&gt; mutate(df2, people_json = to_json(df2$people))}&lt;br/&gt;
+#&apos; df2 &amp;lt;- mutate(df2, people_json = to_json(df2$people))&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; # Converts a map into a pretty JSON object&lt;br/&gt;
+#&apos; df2 &amp;lt;- sql(&quot;SELECT map(&apos;name&apos;, &apos;Bob&apos;) as people&quot;)&lt;br/&gt;
+#&apos; df2 &amp;lt;- mutate(df2, people_json = to_json(df2$people, pretty = TRUE))}&lt;br/&gt;
 #&apos; @note to_json since 2.2.0&lt;br/&gt;
 setMethod(&quot;to_json&quot;, signature(x = &quot;Column&quot;),&lt;br/&gt;
           function(x, ...) {
@@ -1731,6 +1796,26 @@ setMethod(&quot;to_json&quot;, signature(x = &quot;Column&quot;),
             column(jc)
           })&lt;br/&gt;
 &lt;br/&gt;
+#&apos; @details&lt;br/&gt;
+#&apos; \code{to_csv}: Converts a column containing a \code{structType} into a Column of CSV string.&lt;br/&gt;
+#&apos; Resolving the Column can fail if an unsupported type is encountered.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; @rdname column_collection_functions&lt;br/&gt;
+#&apos; @aliases to_csv to_csv,Column-method&lt;br/&gt;
+#&apos; @examples&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; \dontrun{
+#&apos; # Converts a struct into a CSV string
+#&apos; df2 &amp;lt;- sql(&quot;SELECT named_struct(&apos;date&apos;, cast(&apos;2000-01-01&apos; as date)) as d&quot;)
+#&apos; select(df2, to_csv(df2$d, dateFormat = &apos;dd/MM/yyyy&apos;))}&lt;br/&gt;
+#&apos; @note to_csv since 3.0.0&lt;br/&gt;
+setMethod(&quot;to_csv&quot;, signature(x = &quot;Column&quot;),&lt;br/&gt;
+          function(x, ...) {
+            options &amp;lt;- varargsToStrEnv(...)
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;to_csv&quot;, x@jc, options)
+            column(jc)
+          })&lt;br/&gt;
+&lt;br/&gt;
 #&apos; @details&lt;br/&gt;
 #&apos; \code{to_timestamp}: Converts the column into a TimestampType. You may optionally specify&lt;br/&gt;
 #&apos; a format according to the rules in:&lt;br/&gt;
@@ -2035,13 +2120,24 @@ setMethod(&quot;pmod&quot;, signature(y = &quot;Column&quot;),&lt;br/&gt;
 &lt;br/&gt;
 #&apos; @param rsd maximum estimation error allowed (default = 0.05).&lt;br/&gt;
 #&apos;&lt;br/&gt;
+#&apos; @rdname column_aggregate_functions&lt;br/&gt;
+#&apos; @aliases approx_count_distinct,Column-method&lt;br/&gt;
+#&apos; @note approx_count_distinct(Column, numeric) since 3.0.0&lt;br/&gt;
+setMethod(&quot;approx_count_distinct&quot;,&lt;br/&gt;
+          signature(x = &quot;Column&quot;),&lt;br/&gt;
+          function(x, rsd = 0.05) {
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;approx_count_distinct&quot;, x@jc, rsd)
+            column(jc)
+          })&lt;br/&gt;
+&lt;br/&gt;
 #&apos; @rdname column_aggregate_functions&lt;br/&gt;
 #&apos; @aliases approxCountDistinct,Column-method&lt;br/&gt;
 #&apos; @note approxCountDistinct(Column, numeric) since 1.4.0&lt;br/&gt;
 setMethod(&quot;approxCountDistinct&quot;,&lt;br/&gt;
           signature(x = &quot;Column&quot;),&lt;br/&gt;
           function(x, rsd = 0.05) {
-            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;approxCountDistinct&quot;, x@jc, rsd)
+            .Deprecated(&quot;approx_count_distinct&quot;)
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;approx_count_distinct&quot;, x@jc, rsd)
             column(jc)
           })&lt;br/&gt;
 &lt;br/&gt;
@@ -2164,8 +2260,6 @@ setMethod(&quot;date_format&quot;, signature(y = &quot;Column&quot;, x = &quot;character&quot;),&lt;br/&gt;
 #&apos; to \code{TRUE}. If the string is unparseable, the Column will contain the value NA.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @rdname column_collection_functions&lt;br/&gt;
-#&apos; @param schema a structType object to use as the schema to use when parsing the JSON string.&lt;br/&gt;
-#&apos;               Since Spark 2.3, the DDL-formatted string is also supported for the schema.&lt;br/&gt;
 #&apos; @param as.json.array indicating if input string is JSON array of objects or a single object.&lt;br/&gt;
 #&apos; @aliases from_json from_json,Column,characterOrstructType-method&lt;br/&gt;
 #&apos; @examples&lt;br/&gt;
@@ -2203,9 +2297,98 @@ setMethod(&quot;from_json&quot;, signature(x = &quot;Column&quot;, schema = &quot;characterOrstructType&quot;)&lt;br/&gt;
           })&lt;br/&gt;
 &lt;br/&gt;
 #&apos; @details&lt;br/&gt;
-#&apos; \code{from_utc_timestamp}: Given a timestamp like &apos;2017-07-14 02:40:00.0&apos;, interprets it as a&lt;br/&gt;
-#&apos; time in UTC, and renders that time as a timestamp in the given time zone. For example, &apos;GMT+1&apos;&lt;br/&gt;
-#&apos; would yield &apos;2017-07-14 03:40:00.0&apos;.&lt;br/&gt;
+#&apos; \code{schema_of_json}: Parses a JSON string and infers its schema in DDL format.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; @rdname column_collection_functions&lt;br/&gt;
+#&apos; @aliases schema_of_json schema_of_json,characterOrColumn-method&lt;br/&gt;
+#&apos; @examples&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; \dontrun{&lt;br/&gt;
+#&apos; json &amp;lt;- &quot;{\&quot;name\&quot;:\&quot;Bob\&quot;}&quot;&lt;br/&gt;
+#&apos; df &amp;lt;- sql(&quot;SELECT * FROM range(1)&quot;)&lt;br/&gt;
+#&apos; head(select(df, schema_of_json(json)))}&lt;br/&gt;
+#&apos; @note schema_of_json since 3.0.0&lt;br/&gt;
+setMethod(&quot;schema_of_json&quot;, signature(x = &quot;characterOrColumn&quot;),&lt;br/&gt;
+          function(x, ...) {&lt;br/&gt;
+            if (class&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; == &quot;character&quot;) {
+              col &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;lit&quot;, x)
+            } else {
+              col &amp;lt;- x@jc
+            }&lt;br/&gt;
+            options &amp;lt;- varargsToStrEnv(...)&lt;br/&gt;
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;,&lt;br/&gt;
+                              &quot;schema_of_json&quot;,&lt;br/&gt;
+                              col, options)&lt;br/&gt;
+            column(jc)&lt;br/&gt;
+          })&lt;br/&gt;
+&lt;br/&gt;
+#&apos; @details&lt;br/&gt;
+#&apos; \code{from_csv}: Parses a column containing a CSV string into a Column of \code{structType}&lt;br/&gt;
+#&apos; with the specified \code{schema}.&lt;br/&gt;
+#&apos; If the string is unparseable, the Column will contain the value NA.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; @rdname column_collection_functions&lt;br/&gt;
+#&apos; @aliases from_csv from_csv,Column,character-method&lt;br/&gt;
+#&apos; @examples&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; \dontrun{
+#&apos; df &amp;lt;- sql(&quot;SELECT &apos;Amsterdam,2018&apos; as csv&quot;)
+#&apos; schema &amp;lt;- &quot;city STRING, year INT&quot;
+#&apos; head(select(df, from_csv(df$csv, schema)))}&lt;br/&gt;
+#&apos; @note from_csv since 3.0.0&lt;br/&gt;
+setMethod(&quot;from_csv&quot;, signature(x = &quot;Column&quot;, schema = &quot;characterOrColumn&quot;),&lt;br/&gt;
+          function(x, schema, ...) {&lt;br/&gt;
+            if (class(schema) == &quot;Column&quot;) {
+              jschema &amp;lt;- schema@jc
+            } else if (is.character(schema)) {
+              jschema &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;lit&quot;, schema)
+            } else {
+              stop(&quot;schema argument should be a column or character&quot;)
+            }&lt;br/&gt;
+            options &amp;lt;- varargsToStrEnv(...)&lt;br/&gt;
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;,&lt;br/&gt;
+                              &quot;from_csv&quot;,&lt;br/&gt;
+                              x@jc, jschema, options)&lt;br/&gt;
+            column(jc)&lt;br/&gt;
+          })&lt;br/&gt;
+&lt;br/&gt;
+#&apos; @details&lt;br/&gt;
+#&apos; \code{schema_of_csv}
&lt;p&gt;: Parses a CSV string and infers its schema in DDL format.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; @rdname column_collection_functions&lt;br/&gt;
+#&apos; @aliases schema_of_csv schema_of_csv,characterOrColumn-method&lt;br/&gt;
+#&apos; @examples&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; \dontrun&lt;/p&gt;
{
+#&apos; csv &amp;lt;- &quot;Amsterdam,2018&quot;
+#&apos; df &amp;lt;- sql(&quot;SELECT * FROM range(1)&quot;)
+#&apos; head(select(df, schema_of_csv(csv)))}
&lt;p&gt;+#&apos; @note schema_of_csv since 3.0.0&lt;br/&gt;
+setMethod(&quot;schema_of_csv&quot;, signature(x = &quot;characterOrColumn&quot;),&lt;br/&gt;
+          function(x, ...) {&lt;br/&gt;
+            if (class&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; == &quot;character&quot;) &lt;/p&gt;
{
+              col &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;lit&quot;, x)
+            }
&lt;p&gt; else &lt;/p&gt;
{
+              col &amp;lt;- x@jc
+            }
&lt;p&gt;+            options &amp;lt;- varargsToStrEnv(...)&lt;br/&gt;
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;,&lt;br/&gt;
+                              &quot;schema_of_csv&quot;,&lt;br/&gt;
+                              col, options)&lt;br/&gt;
+            column(jc)&lt;br/&gt;
+          })&lt;br/&gt;
+&lt;br/&gt;
+#&apos; @details&lt;br/&gt;
+#&apos; \code&lt;/p&gt;
{from_utc_timestamp}
&lt;p&gt;: This is a common function for databases supporting TIMESTAMP WITHOUT&lt;br/&gt;
+#&apos; TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a&lt;br/&gt;
+#&apos; timestamp in UTC, and renders that timestamp as a timestamp in the given time zone.&lt;br/&gt;
+#&apos; However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not&lt;br/&gt;
+#&apos; timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to&lt;br/&gt;
+#&apos; the given timezone.&lt;br/&gt;
+#&apos; This function may return confusing result if the input is a string with timezone, e.g.&lt;br/&gt;
+#&apos; (\code&lt;/p&gt;
{2018-03-13T06:18:23+00:00}
&lt;p&gt;). The reason is that, Spark firstly cast the string to&lt;br/&gt;
+#&apos; timestamp according to the timezone in the string, and finally display the result by converting&lt;br/&gt;
+#&apos; the timestamp to string according to the session local timezone.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @rdname column_datetime_diff_functions&lt;br/&gt;
 #&apos;&lt;br/&gt;
@@ -2261,9 +2444,16 @@ setMethod(&quot;next_day&quot;, signature(y = &quot;Column&quot;, x = &quot;character&quot;),&lt;br/&gt;
           })&lt;/p&gt;

&lt;p&gt; #&apos; @details&lt;br/&gt;
-#&apos; \code&lt;/p&gt;
{to_utc_timestamp}: Given a timestamp like &apos;2017-07-14 02:40:00.0&apos;, interprets it as a&lt;br/&gt;
-#&apos; time in the given time zone, and renders that time as a timestamp in UTC. For example, &apos;GMT+1&apos;&lt;br/&gt;
-#&apos; would yield &apos;2017-07-14 01:40:00.0&apos;.&lt;br/&gt;
+#&apos; \code{to_utc_timestamp}
&lt;p&gt;: This is a common function for databases supporting TIMESTAMP WITHOUT&lt;br/&gt;
+#&apos; TIMEZONE. This function takes a timestamp which is timezone-agnostic, and interprets it as a&lt;br/&gt;
+#&apos; timestamp in the given timezone, and renders that timestamp as a timestamp in UTC.&lt;br/&gt;
+#&apos; However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not&lt;br/&gt;
+#&apos; timezone-agnostic. So in Spark this function just shift the timestamp value from the given&lt;br/&gt;
+#&apos; timezone to UTC timezone.&lt;br/&gt;
+#&apos; This function may return confusing result if the input is a string with timezone, e.g.&lt;br/&gt;
+#&apos; (\code&lt;/p&gt;
{2018-03-13T06:18:23+00:00}
&lt;p&gt;). The reason is that, Spark firstly cast the string to&lt;br/&gt;
+#&apos; timestamp according to the timezone in the string, and finally display the result by converting&lt;br/&gt;
+#&apos; the timestamp to string according to the session local timezone.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @rdname column_datetime_diff_functions&lt;br/&gt;
 #&apos; @aliases to_utc_timestamp to_utc_timestamp,Column,character-method&lt;br/&gt;
@@ -3243,7 +3433,7 @@ setMethod(&quot;flatten&quot;,&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @rdname column_collection_functions&lt;br/&gt;
 #&apos; @aliases map_entries map_entries,Column-method&lt;br/&gt;
-#&apos; @note map_entries since 2.4.0&lt;br/&gt;
+#&apos; @note map_entries since 3.0.0&lt;br/&gt;
 setMethod(&quot;map_entries&quot;,&lt;br/&gt;
           signature(x = &quot;Column&quot;),&lt;br/&gt;
           function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; {&lt;br/&gt;
@@ -3458,13 +3648,21 @@ setMethod(&quot;collect_set&quot;,&lt;/p&gt;

&lt;p&gt; #&apos; @details&lt;br/&gt;
 #&apos; \code&lt;/p&gt;
{split_string}
&lt;p&gt;: Splits string on regular expression.&lt;br/&gt;
-#&apos; Equivalent to \code&lt;/p&gt;
{split} SQL function.&lt;br/&gt;
+#&apos; Equivalent to \code{split}
&lt;p&gt; SQL function. Optionally a&lt;br/&gt;
+#&apos; \code&lt;/p&gt;
{limit} can be specified&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @rdname column_string_functions&lt;br/&gt;
+#&apos; @param limit determines the length of the returned array.&lt;br/&gt;
+#&apos;              \itemize{&lt;br/&gt;
+#&apos;              \item \code{limit &amp;gt; 0}: length of the array will be at most \code{limit}
&lt;p&gt;+#&apos;              \item \code&lt;/p&gt;
{limit &amp;lt;= 0}
&lt;p&gt;: the returned array can have any length&lt;br/&gt;
+#&apos;              }&lt;br/&gt;
+#&apos;&lt;br/&gt;
 #&apos; @aliases split_string split_string,Column-method&lt;br/&gt;
 #&apos; @examples&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; \dontrun{&lt;br/&gt;
+#&apos; head(select(df, split_string(df$Class, &quot;&lt;br class=&quot;atl-forced-newline&quot; /&gt;d&quot;, 2)))&lt;br/&gt;
 #&apos; head(select(df, split_string(df$Sex, &quot;a&quot;)))&lt;br/&gt;
 #&apos; head(select(df, split_string(df$Class, &quot;&lt;br class=&quot;atl-forced-newline&quot; /&gt;d&quot;)))&lt;br/&gt;
 #&apos; # This is equivalent to the following SQL expression&lt;br/&gt;
@@ -3472,8 +3670,9 @@ setMethod(&quot;collect_set&quot;,&lt;br/&gt;
 #&apos; @note split_string 2.3.0&lt;br/&gt;
 setMethod(&quot;split_string&quot;,&lt;br/&gt;
           signature(x = &quot;Column&quot;, pattern = &quot;character&quot;),&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;function(x, pattern) {&lt;/li&gt;
	&lt;li&gt;jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;, &quot;split&quot;, x@jc, pattern)&lt;br/&gt;
+          function(x, pattern, limit = -1) 
{
+            jc &amp;lt;- callJStatic(&quot;org.apache.spark.sql.functions&quot;,
+                              &quot;split&quot;, x@jc, pattern, as.integer(limit))
             column(jc)
           }
&lt;p&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/R/pkg/R/generics.R b/R/pkg/R/generics.R&lt;br/&gt;
index 27c1b312d645c..eed76465221c6 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/R/generics.R&lt;br/&gt;
+++ b/R/pkg/R/generics.R&lt;br/&gt;
@@ -528,9 +528,6 @@ setGeneric(&quot;persist&quot;, function(x, newLevel) &lt;/p&gt;
{ standardGeneric(&quot;persist&quot;) }
&lt;p&gt;)&lt;br/&gt;
 #&apos; @rdname printSchema&lt;br/&gt;
 setGeneric(&quot;printSchema&quot;, function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;
{ standardGeneric(&quot;printSchema&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;-#&apos; @rdname registerTempTable-deprecated&lt;br/&gt;
-setGeneric(&quot;registerTempTable&quot;, function(x, tableName) &lt;/p&gt;
{ standardGeneric(&quot;registerTempTable&quot;) }
&lt;p&gt;)&lt;br/&gt;
-&lt;br/&gt;
 #&apos; @rdname rename&lt;br/&gt;
 setGeneric(&quot;rename&quot;, function(x, ...) &lt;/p&gt;
{ standardGeneric(&quot;rename&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;@@ -595,9 +592,6 @@ setGeneric(&quot;write.parquet&quot;, function(x, path, ...) &lt;/p&gt;
{
   standardGeneric(&quot;write.parquet&quot;)
 }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;-#&apos; @rdname write.parquet&lt;br/&gt;
-setGeneric(&quot;saveAsParquetFile&quot;, function(x, path) &lt;/p&gt;
{ standardGeneric(&quot;saveAsParquetFile&quot;) }
&lt;p&gt;)&lt;br/&gt;
-&lt;br/&gt;
 #&apos; @rdname write.stream&lt;br/&gt;
 setGeneric(&quot;write.stream&quot;, function(df, source = NULL, outputMode = NULL, ...) {&lt;br/&gt;
   standardGeneric(&quot;write.stream&quot;)&lt;br/&gt;
@@ -637,7 +631,7 @@ setGeneric(&quot;toRDD&quot;, function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;
{ standardGeneric(&quot;toRDD&quot;) }
&lt;p&gt;)&lt;br/&gt;
 #&apos; @rdname union&lt;br/&gt;
 setGeneric(&quot;union&quot;, function(x, y) &lt;/p&gt;
{ standardGeneric(&quot;union&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;-#&apos; @rdname union&lt;br/&gt;
+#&apos; @rdname unionAll&lt;br/&gt;
 setGeneric(&quot;unionAll&quot;, function(x, y) &lt;/p&gt;
{ standardGeneric(&quot;unionAll&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt; #&apos; @rdname unionByName&lt;br/&gt;
@@ -755,6 +749,10 @@ setGeneric(&quot;windowOrderBy&quot;, function(col, ...) { standardGeneric(&quot;windowOrderBy&quot;&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;add_months&quot;, function(y, x) &lt;/p&gt;
{ standardGeneric(&quot;add_months&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;+#&apos; @rdname column_aggregate_functions&lt;br/&gt;
+#&apos; @name NULL&lt;br/&gt;
+setGeneric(&quot;approx_count_distinct&quot;, function(x, ...) &lt;/p&gt;
{ standardGeneric(&quot;approx_count_distinct&quot;) }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
 #&apos; @rdname column_aggregate_functions&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;approxCountDistinct&quot;, function(x, ...) &lt;/p&gt;
{ standardGeneric(&quot;approxCountDistinct&quot;) }
&lt;p&gt;)&lt;br/&gt;
@@ -984,6 +982,10 @@ setGeneric(&quot;format_string&quot;, function(format, x, ...) { standardGeneric(&quot;format_s&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;from_json&quot;, function(x, schema, ...) &lt;/p&gt;
{ standardGeneric(&quot;from_json&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;+#&apos; @rdname column_collection_functions&lt;br/&gt;
+#&apos; @name NULL&lt;br/&gt;
+setGeneric(&quot;from_csv&quot;, function(x, schema, ...) &lt;/p&gt;
{ standardGeneric(&quot;from_csv&quot;) }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
 #&apos; @rdname column_datetime_functions&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;from_unixtime&quot;, function(x, ...) &lt;/p&gt;
{ standardGeneric(&quot;from_unixtime&quot;) }
&lt;p&gt;)&lt;br/&gt;
@@ -1204,6 +1206,14 @@ setGeneric(&quot;rpad&quot;, function(x, len, pad) &lt;/p&gt;
{ standardGeneric(&quot;rpad&quot;) }
&lt;p&gt;)&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;rtrim&quot;, function(x, trimString) &lt;/p&gt;
{ standardGeneric(&quot;rtrim&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;+#&apos; @rdname column_collection_functions&lt;br/&gt;
+#&apos; @name NULL&lt;br/&gt;
+setGeneric(&quot;schema_of_csv&quot;, function(x, ...) &lt;/p&gt;
{ standardGeneric(&quot;schema_of_csv&quot;) }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
+#&apos; @rdname column_collection_functions&lt;br/&gt;
+#&apos; @name NULL&lt;br/&gt;
+setGeneric(&quot;schema_of_json&quot;, function(x, ...) &lt;/p&gt;
{ standardGeneric(&quot;schema_of_json&quot;) }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
 #&apos; @rdname column_aggregate_functions&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;sd&quot;, function(x, na.rm = FALSE) &lt;/p&gt;
{ standardGeneric(&quot;sd&quot;) }
&lt;p&gt;)&lt;br/&gt;
@@ -1258,7 +1268,7 @@ setGeneric(&quot;sort_array&quot;, function(x, asc = TRUE) { standardGeneric(&quot;sort_array&quot;)&lt;/p&gt;

&lt;p&gt; #&apos; @rdname column_string_functions&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
-setGeneric(&quot;split_string&quot;, function(x, pattern) &lt;/p&gt;
{ standardGeneric(&quot;split_string&quot;) })&lt;br/&gt;
+setGeneric(&quot;split_string&quot;, function(x, pattern, ...) { standardGeneric(&quot;split_string&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt; #&apos; @rdname column_string_functions&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
@@ -1292,10 +1302,18 @@ setGeneric(&quot;substring_index&quot;, function(x, delim, count) { standardGeneric(&quot;subst&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;sumDistinct&quot;, function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;
{ standardGeneric(&quot;sumDistinct&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;+#&apos; @rdname column_math_functions&lt;br/&gt;
+#&apos; @name NULL&lt;br/&gt;
+setGeneric(&quot;degrees&quot;, function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;
{ standardGeneric(&quot;degrees&quot;) }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
 #&apos; @rdname column_math_functions&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;toDegrees&quot;, function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;
{ standardGeneric(&quot;toDegrees&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;+#&apos; @rdname column_math_functions&lt;br/&gt;
+#&apos; @name NULL&lt;br/&gt;
+setGeneric(&quot;radians&quot;, function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;
{ standardGeneric(&quot;radians&quot;) }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
 #&apos; @rdname column_math_functions&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;toRadians&quot;, function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;
{ standardGeneric(&quot;toRadians&quot;) }
&lt;p&gt;)&lt;br/&gt;
@@ -1308,6 +1326,10 @@ setGeneric(&quot;to_date&quot;, function(x, format) &lt;/p&gt;
{ standardGeneric(&quot;to_date&quot;) }
&lt;p&gt;)&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;to_json&quot;, function(x, ...) &lt;/p&gt;
{ standardGeneric(&quot;to_json&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;+#&apos; @rdname column_collection_functions&lt;br/&gt;
+#&apos; @name NULL&lt;br/&gt;
+setGeneric(&quot;to_csv&quot;, function(x, ...) &lt;/p&gt;
{ standardGeneric(&quot;to_csv&quot;) }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
 #&apos; @rdname column_datetime_functions&lt;br/&gt;
 #&apos; @name NULL&lt;br/&gt;
 setGeneric(&quot;to_timestamp&quot;, function(x, format) &lt;/p&gt;
{ standardGeneric(&quot;to_timestamp&quot;) }
&lt;p&gt;)&lt;br/&gt;
@@ -1453,6 +1475,10 @@ setGeneric(&quot;spark.freqItemsets&quot;, function(object) { standardGeneric(&quot;spark.freqI&lt;br/&gt;
 #&apos; @rdname spark.fpGrowth&lt;br/&gt;
 setGeneric(&quot;spark.associationRules&quot;, function(object) &lt;/p&gt;
{ standardGeneric(&quot;spark.associationRules&quot;) }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;+#&apos; @rdname spark.prefixSpan&lt;br/&gt;
+setGeneric(&quot;spark.findFrequentSequentialPatterns&quot;,&lt;br/&gt;
+            function(data, ...) &lt;/p&gt;
{ standardGeneric(&quot;spark.findFrequentSequentialPatterns&quot;) }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
 #&apos; @param object a fitted ML model object.&lt;br/&gt;
 #&apos; @param path the directory where the model is saved.&lt;br/&gt;
 #&apos; @param ... additional argument(s) passed to the method.&lt;br/&gt;
diff --git a/R/pkg/R/mllib_fpm.R b/R/pkg/R/mllib_fpm.R&lt;br/&gt;
index 4ad34fe82328f..ac37580c6b373 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/R/mllib_fpm.R&lt;br/&gt;
+++ b/R/pkg/R/mllib_fpm.R&lt;br/&gt;
@@ -23,6 +23,12 @@&lt;br/&gt;
 #&apos; @note FPGrowthModel since 2.2.0&lt;br/&gt;
 setClass(&quot;FPGrowthModel&quot;, slots = list(jobj = &quot;jobj&quot;))&lt;/p&gt;

&lt;p&gt;+#&apos; S4 class that represents a PrefixSpan&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; @param jobj a Java object reference to the backing Scala PrefixSpan&lt;br/&gt;
+#&apos; @note PrefixSpan since 3.0.0&lt;br/&gt;
+setClass(&quot;PrefixSpan&quot;, slots = list(jobj = &quot;jobj&quot;))&lt;br/&gt;
+&lt;br/&gt;
 #&apos; FP-growth&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; A parallel FP-growth algorithm to mine frequent itemsets.&lt;br/&gt;
@@ -155,3 +161,61 @@ setMethod(&quot;write.ml&quot;, signature(object = &quot;FPGrowthModel&quot;, path = &quot;character&quot;),&lt;br/&gt;
           function(object, path, overwrite = FALSE) &lt;/p&gt;
{
             write_internal(object, path, overwrite)
           }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
+#&apos; PrefixSpan&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#&apos; A parallel PrefixSpan algorithm to mine frequent sequential patterns.&lt;br/&gt;
+#&apos; \code&lt;/p&gt;
{spark.findFrequentSequentialPatterns}
&lt;p&gt; returns a complete set of frequent sequential&lt;br/&gt;
+#&apos; patterns.&lt;br/&gt;
+#&apos; For more details, see&lt;br/&gt;
+#&apos; \href&lt;/p&gt;
{https://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html#prefixspan}
{
+#&apos; PrefixSpan}
&lt;p&gt;.&lt;br/&gt;
+#&apos;&lt;br/&gt;
+#  Find frequent sequential patterns.&lt;br/&gt;
+#&apos; @param data A SparkDataFrame.&lt;br/&gt;
+#&apos; @param minSupport Minimal support level.&lt;br/&gt;
+#&apos; @param maxPatternLength Maximal pattern length.&lt;br/&gt;
+#&apos; @param maxLocalProjDBSize Maximum number of items (including delimiters used in the internal&lt;br/&gt;
+#&apos;                           storage format) allowed in a projected database before local&lt;br/&gt;
+#&apos;                           processing.&lt;br/&gt;
+#&apos; @param sequenceCol name of the sequence column in dataset.&lt;br/&gt;
+#&apos; @param ... additional argument(s) passed to the method.&lt;br/&gt;
+#&apos; @return A complete set of frequent sequential patterns in the input sequences of itemsets.&lt;br/&gt;
+#&apos;         The returned \code&lt;/p&gt;
{SparkDataFrame}
&lt;p&gt; contains columns of sequence and corresponding&lt;br/&gt;
+#&apos;         frequency. The schema of it will be:&lt;br/&gt;
+#&apos;         \code&lt;/p&gt;
{sequence: ArrayType(ArrayType(T))}
&lt;p&gt; (T is the item type)&lt;br/&gt;
+#&apos;         \code&lt;/p&gt;
{freq: Long}
&lt;p&gt;+#&apos; @rdname spark.prefixSpan&lt;br/&gt;
+#&apos; @aliases findFrequentSequentialPatterns,PrefixSpan,SparkDataFrame-method&lt;br/&gt;
+#&apos; @examples&lt;br/&gt;
+#&apos; \dontrun&lt;/p&gt;
{
+#&apos; df &amp;lt;- createDataFrame(list(list(list(list(1L, 2L), list(3L))),
+#&apos;                       list(list(list(1L), list(3L, 2L), list(1L, 2L))),
+#&apos;                       list(list(list(1L, 2L), list(5L))),
+#&apos;                       list(list(list(6L)))), schema = c(&quot;sequence&quot;))
+#&apos; frequency &amp;lt;- spark.findFrequentSequentialPatterns(df, minSupport = 0.5, maxPatternLength = 5L,
+#&apos;                                                   maxLocalProjDBSize = 32000000L)
+#&apos; showDF(frequency)
+#&apos; }
&lt;p&gt;+#&apos; @note spark.findFrequentSequentialPatterns(SparkDataFrame) since 3.0.0&lt;br/&gt;
+setMethod(&quot;spark.findFrequentSequentialPatterns&quot;,&lt;br/&gt;
+          signature(data = &quot;SparkDataFrame&quot;),&lt;br/&gt;
+          function(data, minSupport = 0.1, maxPatternLength = 10L,&lt;br/&gt;
+            maxLocalProjDBSize = 32000000L, sequenceCol = &quot;sequence&quot;) {&lt;br/&gt;
+              if (!is.numeric(minSupport) || minSupport &amp;lt; 0) &lt;/p&gt;
{
+                stop(&quot;minSupport should be a number with value &amp;gt;= 0.&quot;)
+              }
&lt;p&gt;+              if (!is.integer(maxPatternLength) || maxPatternLength &amp;lt;= 0) &lt;/p&gt;
{
+                stop(&quot;maxPatternLength should be a number with value &amp;gt; 0.&quot;)
+              }
&lt;p&gt;+              if (!is.numeric(maxLocalProjDBSize) || maxLocalProjDBSize &amp;lt;= 0) &lt;/p&gt;
{
+                stop(&quot;maxLocalProjDBSize should be a number with value &amp;gt; 0.&quot;)
+              }
&lt;p&gt;+&lt;br/&gt;
+              jobj &amp;lt;- callJStatic(&quot;org.apache.spark.ml.r.PrefixSpanWrapper&quot;, &quot;getPrefixSpan&quot;,&lt;br/&gt;
+                                  as.numeric(minSupport), as.integer(maxPatternLength),&lt;br/&gt;
+                                  as.numeric(maxLocalProjDBSize), as.character(sequenceCol))&lt;br/&gt;
+              object &amp;lt;- new(&quot;PrefixSpan&quot;, jobj = jobj)&lt;br/&gt;
+              dataFrame(callJMethod(object@jobj, &quot;findFrequentSequentialPatterns&quot;, data@sdf))&lt;br/&gt;
+            }&lt;br/&gt;
+          )&lt;br/&gt;
diff --git a/R/pkg/R/sparkR.R b/R/pkg/R/sparkR.R&lt;br/&gt;
index d3a9cbae7d808..ac289d38d01bd 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/R/sparkR.R&lt;br/&gt;
+++ b/R/pkg/R/sparkR.R&lt;br/&gt;
@@ &lt;del&gt;88,49 +88,6 @@ sparkR.stop &amp;lt;&lt;/del&gt; function() &lt;/p&gt;
{
   sparkR.session.stop()
 }

&lt;p&gt;-#&apos; (Deprecated) Initialize a new Spark Context&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; This function initializes a new SparkContext.&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; @param master The Spark master URL&lt;br/&gt;
-#&apos; @param appName Application name to register with cluster manager&lt;br/&gt;
-#&apos; @param sparkHome Spark Home directory&lt;br/&gt;
-#&apos; @param sparkEnvir Named list of environment variables to set on worker nodes&lt;br/&gt;
-#&apos; @param sparkExecutorEnv Named list of environment variables to be used when launching executors&lt;br/&gt;
-#&apos; @param sparkJars Character vector of jar files to pass to the worker nodes&lt;br/&gt;
-#&apos; @param sparkPackages Character vector of package coordinates&lt;br/&gt;
-#&apos; @seealso \link&lt;/p&gt;
{sparkR.session}&lt;br/&gt;
-#&apos; @rdname sparkR.init-deprecated&lt;br/&gt;
-#&apos; @examples&lt;br/&gt;
-#&apos;\dontrun{
-#&apos; sc &amp;lt;- sparkR.init(&quot;local[2]&quot;, &quot;SparkR&quot;, &quot;/home/spark&quot;)
-#&apos; sc &amp;lt;- sparkR.init(&quot;local[2]&quot;, &quot;SparkR&quot;, &quot;/home/spark&quot;,
-#&apos;                  list(spark.executor.memory=&quot;1g&quot;))
-#&apos; sc &amp;lt;- sparkR.init(&quot;yarn-client&quot;, &quot;SparkR&quot;, &quot;/home/spark&quot;,
-#&apos;                  list(spark.executor.memory=&quot;4g&quot;),
-#&apos;                  list(LD_LIBRARY_PATH=&quot;/directory of JVM libraries (libjvm.so) on workers/&quot;),
-#&apos;                  c(&quot;one.jar&quot;, &quot;two.jar&quot;, &quot;three.jar&quot;),
-#&apos;                  c(&quot;com.databricks:spark-avro_2.11:2.0.1&quot;))
-#&apos;}&lt;br/&gt;
-#&apos; @note sparkR.init since 1.4.0&lt;br/&gt;
&lt;del&gt;sparkR.init &amp;lt;&lt;/del&gt; function(&lt;br/&gt;
-  master = &quot;&quot;,&lt;br/&gt;
-  appName = &quot;SparkR&quot;,&lt;br/&gt;
-  sparkHome = Sys.getenv(&quot;SPARK_HOME&quot;),&lt;br/&gt;
-  sparkEnvir = list(),&lt;br/&gt;
-  sparkExecutorEnv = list(),&lt;br/&gt;
-  sparkJars = &quot;&quot;,&lt;br/&gt;
-  sparkPackages = &quot;&quot;) {
-  .Deprecated(&quot;sparkR.session&quot;)
-  sparkR.sparkContext(master,
-     appName,
-     sparkHome,
-     convertNamedListToEnv(sparkEnvir),
-     convertNamedListToEnv(sparkExecutorEnv),
-     sparkJars,
-     sparkPackages)
-}&lt;br/&gt;
-&lt;br/&gt;
 # Internal function to handle creating the SparkContext.&lt;br/&gt;
 sparkR.sparkContext &amp;lt;- function(&lt;br/&gt;
   master = &quot;&quot;,&lt;br/&gt;
@@ &lt;del&gt;272,61 +229,6 @@ sparkR.sparkContext &amp;lt;&lt;/del&gt; function(&lt;br/&gt;
   sc&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
-#&apos; (Deprecated) Initialize a new SQLContext&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; This function creates a SparkContext from an existing JavaSparkContext and&lt;br/&gt;
-#&apos; then uses it to initialize a new SQLContext&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; Starting SparkR 2.0, a SparkSession is initialized and returned instead.&lt;br/&gt;
-#&apos; This API is deprecated and kept for backward compatibility only.&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; @param jsc The existing JavaSparkContext created with SparkR.init()&lt;br/&gt;
-#&apos; @seealso \link{sparkR.session}
&lt;p&gt;-#&apos; @rdname sparkRSQL.init-deprecated&lt;br/&gt;
-#&apos; @examples&lt;br/&gt;
-#&apos;\dontrun&lt;/p&gt;
{
-#&apos; sc &amp;lt;- sparkR.init()
-#&apos; sqlContext &amp;lt;- sparkRSQL.init(sc)
-#&apos;}
&lt;p&gt;-#&apos; @note sparkRSQL.init since 1.4.0&lt;br/&gt;
&lt;del&gt;sparkRSQL.init &amp;lt;&lt;/del&gt; function(jsc = NULL) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.Deprecated(&quot;sparkR.session&quot;)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;if (exists(&quot;.sparkRsession&quot;, envir = .sparkREnv)) 
{
-    return(get(&quot;.sparkRsession&quot;, envir = .sparkREnv))
-  }&lt;br/&gt;
-&lt;br/&gt;
-  # Default to without Hive support for backward compatibility.&lt;br/&gt;
-  sparkR.session(enableHiveSupport = FALSE)&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
-#&apos; (Deprecated) Initialize a new HiveContext&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; This function creates a HiveContext from an existing JavaSparkContext&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; Starting SparkR 2.0, a SparkSession is initialized and returned instead.&lt;br/&gt;
-#&apos; This API is deprecated and kept for backward compatibility only.&lt;br/&gt;
-#&apos;&lt;br/&gt;
-#&apos; @param jsc The existing JavaSparkContext created with SparkR.init()&lt;br/&gt;
-#&apos; @seealso \link{sparkR.session}&lt;br/&gt;
-#&apos; @rdname sparkRHive.init-deprecated&lt;br/&gt;
-#&apos; @examples&lt;br/&gt;
-#&apos;\dontrun{
-#&apos; sc &amp;lt;- sparkR.init()
-#&apos; sqlContext &amp;lt;- sparkRHive.init(sc)
-#&apos;}&lt;br/&gt;
-#&apos; @note sparkRHive.init since 1.4.0&lt;br/&gt;
&lt;del&gt;sparkRHive.init &amp;lt;&lt;/del&gt; function(jsc = NULL) {&lt;br/&gt;
-  .Deprecated(&quot;sparkR.session&quot;)&lt;br/&gt;
-&lt;br/&gt;
-  if (exists(&quot;.sparkRsession&quot;, envir = .sparkREnv)) {-    return(get(&quot;.sparkRsession&quot;, envir = .sparkREnv))-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;# Default to without Hive support for backward compatibility.&lt;/li&gt;
	&lt;li&gt;sparkR.session(enableHiveSupport = TRUE)&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
 #&apos; Get the existing SparkSession or initialize a new SparkSession.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; SparkSession is the entry point into SparkR. \code
{sparkR.session}
&lt;p&gt; gets the existing&lt;br/&gt;
@@ &lt;del&gt;482,26 +384,11 @@ sparkR.uiWebUrl &amp;lt;&lt;/del&gt; function() &lt;/p&gt;
{
 #&apos; setJobGroup(&quot;myJobGroup&quot;, &quot;My job group description&quot;, TRUE)
 #&apos;}
&lt;p&gt; #&apos; @note setJobGroup since 1.5.0&lt;br/&gt;
-#&apos; @method setJobGroup default&lt;br/&gt;
&lt;del&gt;setJobGroup.default &amp;lt;&lt;/del&gt; function(groupId, description, interruptOnCancel) {&lt;br/&gt;
+setJobGroup &amp;lt;- function(groupId, description, interruptOnCancel) &lt;/p&gt;
{
   sc &amp;lt;- getSparkContext()
   invisible(callJMethod(sc, &quot;setJobGroup&quot;, groupId, description, interruptOnCancel))
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;del&gt;setJobGroup &amp;lt;&lt;/del&gt; function(sc, groupId, description, interruptOnCancel) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (class(sc) == &quot;jobj&quot; &amp;amp;&amp;amp; any(grepl(&quot;JavaSparkContext&quot;, getClassName.jobj(sc)))) 
{
-    .Deprecated(&quot;setJobGroup(groupId, description, interruptOnCancel)&quot;,
-                old = &quot;setJobGroup(sc, groupId, description, interruptOnCancel)&quot;)
-    setJobGroup.default(groupId, description, interruptOnCancel)
-  }
&lt;p&gt; else &lt;/p&gt;
{
-    # Parameter order is shifted
-    groupIdToUse &amp;lt;- sc
-    descriptionToUse &amp;lt;- groupId
-    interruptOnCancelToUse &amp;lt;- description
-    setJobGroup.default(groupIdToUse, descriptionToUse, interruptOnCancelToUse)
-  }
&lt;p&gt;-}&lt;br/&gt;
-&lt;br/&gt;
 #&apos; Clear current job group ID and its description&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @rdname clearJobGroup&lt;br/&gt;
@@ &lt;del&gt;512,22 +399,11 @@ setJobGroup &amp;lt;&lt;/del&gt; function(sc, groupId, description, interruptOnCancel) &lt;/p&gt;
{
 #&apos; clearJobGroup()
 #&apos;}
&lt;p&gt; #&apos; @note clearJobGroup since 1.5.0&lt;br/&gt;
-#&apos; @method clearJobGroup default&lt;br/&gt;
&lt;del&gt;clearJobGroup.default &amp;lt;&lt;/del&gt; function() {&lt;br/&gt;
+clearJobGroup &amp;lt;- function() &lt;/p&gt;
{
   sc &amp;lt;- getSparkContext()
   invisible(callJMethod(sc, &quot;clearJobGroup&quot;))
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;del&gt;clearJobGroup &amp;lt;&lt;/del&gt; function(sc) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (!missing(sc) &amp;amp;&amp;amp;&lt;/li&gt;
	&lt;li&gt;class(sc) == &quot;jobj&quot; &amp;amp;&amp;amp;&lt;/li&gt;
	&lt;li&gt;any(grepl(&quot;JavaSparkContext&quot;, getClassName.jobj(sc)))) 
{
-    .Deprecated(&quot;clearJobGroup()&quot;, old = &quot;clearJobGroup(sc)&quot;)
-  }&lt;/li&gt;
	&lt;li&gt;clearJobGroup.default()&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
-&lt;br/&gt;
 #&apos; Cancel active jobs for the specified group&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @param groupId the ID of job group to be cancelled&lt;br/&gt;
@@ &lt;del&gt;539,23 +415,11 @@ clearJobGroup &amp;lt;&lt;/del&gt; function(sc) 
{
 #&apos; cancelJobGroup(&quot;myJobGroup&quot;)
 #&apos;}
&lt;p&gt; #&apos; @note cancelJobGroup since 1.5.0&lt;br/&gt;
-#&apos; @method cancelJobGroup default&lt;br/&gt;
&lt;del&gt;cancelJobGroup.default &amp;lt;&lt;/del&gt; function(groupId) {&lt;br/&gt;
+cancelJobGroup &amp;lt;- function(groupId) &lt;/p&gt;
{
   sc &amp;lt;- getSparkContext()
   invisible(callJMethod(sc, &quot;cancelJobGroup&quot;, groupId))
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;del&gt;cancelJobGroup &amp;lt;&lt;/del&gt; function(sc, groupId) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (class(sc) == &quot;jobj&quot; &amp;amp;&amp;amp; any(grepl(&quot;JavaSparkContext&quot;, getClassName.jobj(sc)))) 
{
-    .Deprecated(&quot;cancelJobGroup(groupId)&quot;, old = &quot;cancelJobGroup(sc, groupId)&quot;)
-    cancelJobGroup.default(groupId)
-  }
&lt;p&gt; else &lt;/p&gt;
{
-    # Parameter order is shifted
-    groupIdToUse &amp;lt;- sc
-    cancelJobGroup.default(groupIdToUse)
-  }
&lt;p&gt;-}&lt;br/&gt;
-&lt;br/&gt;
 #&apos; Set a human readable description of the current job.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Set a description that is shown as a job description in UI.&lt;br/&gt;
@@ &lt;del&gt;626,6 +490,8 @@ sparkConfToSubmitOps[&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;spark.driver.extraLibraryPath&amp;quot;&amp;#93;&lt;/span&gt;] &amp;lt;&lt;/del&gt; &quot;--driver-library-pat&lt;br/&gt;
 sparkConfToSubmitOps[&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;spark.master&amp;quot;&amp;#93;&lt;/span&gt;] &amp;lt;- &quot;--master&quot;&lt;br/&gt;
 sparkConfToSubmitOps[&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;spark.yarn.keytab&amp;quot;&amp;#93;&lt;/span&gt;] &amp;lt;- &quot;--keytab&quot;&lt;br/&gt;
 sparkConfToSubmitOps[&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;spark.yarn.principal&amp;quot;&amp;#93;&lt;/span&gt;] &amp;lt;- &quot;--principal&quot;&lt;br/&gt;
+sparkConfToSubmitOps[&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;spark.kerberos.keytab&amp;quot;&amp;#93;&lt;/span&gt;] &amp;lt;- &quot;--keytab&quot;&lt;br/&gt;
+sparkConfToSubmitOps[&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;spark.kerberos.principal&amp;quot;&amp;#93;&lt;/span&gt;] &amp;lt;- &quot;--principal&quot;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;ol&gt;
	&lt;li&gt;Utility function that returns Spark Submit arguments as a string&lt;br/&gt;
diff --git a/R/pkg/R/stats.R b/R/pkg/R/stats.R&lt;br/&gt;
index 497f18c763048..7252351ebebb2 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/R/pkg/R/stats.R&lt;br/&gt;
+++ b/R/pkg/R/stats.R&lt;br/&gt;
@@ -109,7 +109,7 @@ setMethod(&quot;corr&quot;,&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; Finding frequent items for columns, possibly with false positives.&lt;br/&gt;
 #&apos; Using the frequent element count algorithm described in&lt;br/&gt;
-#&apos; \url
{http://dx.doi.org/10.1145/762471.762473}
&lt;p&gt;, proposed by Karp, Schenker, and Papadimitriou.&lt;br/&gt;
+#&apos; \url&lt;/p&gt;
{https://doi.org/10.1145/762471.762473}
&lt;p&gt;, proposed by Karp, Schenker, and Papadimitriou.&lt;br/&gt;
 #&apos;&lt;br/&gt;
 #&apos; @param x A SparkDataFrame.&lt;br/&gt;
 #&apos; @param cols A vector column names to search frequent items in.&lt;br/&gt;
@@ -143,7 +143,7 @@ setMethod(&quot;freqItems&quot;, signature(x = &quot;SparkDataFrame&quot;, cols = &quot;character&quot;),&lt;br/&gt;
 #&apos; &lt;b&gt;exact&lt;/b&gt; rank of x is close to (p * N). More precisely,&lt;br/&gt;
 #&apos;   floor((p - err) * N) &amp;lt;= rank&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &amp;lt;= ceil((p + err) * N).&lt;br/&gt;
 #&apos; This method implements a variation of the Greenwald-Khanna algorithm (with some speed&lt;br/&gt;
-#&apos; optimizations). The algorithm was first present in [[http://dx.doi.org/10.1145/375663.375670&lt;br/&gt;
+#&apos; optimizations). The algorithm was first present in [[https://doi.org/10.1145/375663.375670&lt;br/&gt;
 #&apos; Space-efficient Online Computation of Quantile Summaries]] by Greenwald and Khanna.&lt;br/&gt;
 #&apos; Note that NA values will be ignored in numerical columns before calculation. For&lt;br/&gt;
 #&apos;   columns only containing NA values, an empty list is returned.&lt;br/&gt;
diff --git a/R/pkg/inst/profile/general.R b/R/pkg/inst/profile/general.R&lt;br/&gt;
index 8c75c19ca7ac3..3efb460846fc2 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/R/pkg/inst/profile/general.R&lt;br/&gt;
+++ b/R/pkg/inst/profile/general.R&lt;br/&gt;
@@ -16,6 +16,10 @@&lt;br/&gt;
 #&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; .First &amp;lt;- function() {&lt;br/&gt;
+  if (utils::compareVersion(paste0(R.version$major, &quot;.&quot;, R.version$minor), &quot;3.4.0&quot;) == -1) &lt;/p&gt;
{
+    warning(&quot;Support for R prior to version 3.4 is deprecated since Spark 3.0.0&quot;)
+  }&lt;br/&gt;
+&lt;br/&gt;
   packageDir &amp;lt;- Sys.getenv(&quot;SPARKR_PACKAGE_DIR&quot;)&lt;br/&gt;
   dirs &amp;lt;- strsplit(packageDir, &quot;,&quot;)[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;]&lt;br/&gt;
   .libPaths(c(dirs, .libPaths()))&lt;br/&gt;
diff --git a/R/pkg/inst/profile/shell.R b/R/pkg/inst/profile/shell.R&lt;br/&gt;
index 8a8111a8c5419..32eb3671b5941 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/inst/profile/shell.R&lt;br/&gt;
+++ b/R/pkg/inst/profile/shell.R&lt;br/&gt;
@@ -16,6 +16,10 @@&lt;br/&gt;
 #&lt;br/&gt;
 &lt;br/&gt;
 .First &amp;lt;- function() {&lt;br/&gt;
+  if (utils::compareVersion(paste0(R.version$major, &quot;.&quot;, R.version$minor), &quot;3.4.0&quot;) == -1) {+    warning(&quot;Support for R prior to version 3.4 is deprecated since Spark 3.0.0&quot;)+  }
&lt;p&gt;+&lt;br/&gt;
   home &amp;lt;- Sys.getenv(&quot;SPARK_HOME&quot;)&lt;br/&gt;
   .libPaths(c(file.path(home, &quot;R&quot;, &quot;lib&quot;), .libPaths()))&lt;br/&gt;
   Sys.setenv(NOAWT = 1)&lt;br/&gt;
diff --git a/R/pkg/tests/fulltests/test_Serde.R b/R/pkg/tests/fulltests/test_Serde.R&lt;br/&gt;
index 3577929323b8b..1525bdb2f5c8b 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/tests/fulltests/test_Serde.R&lt;br/&gt;
+++ b/R/pkg/tests/fulltests/test_Serde.R&lt;br/&gt;
@@ -124,3 +124,35 @@ test_that(&quot;SerDe of list of lists&quot;, {&lt;br/&gt;
 })&lt;/p&gt;

&lt;p&gt; sparkR.session.stop()&lt;br/&gt;
+&lt;br/&gt;
+# Note that this test should be at the end of tests since the configruations used here are not&lt;br/&gt;
+# specific to sessions, and the Spark context is restarted.&lt;br/&gt;
+test_that(&quot;createDataFrame large objects&quot;, {&lt;br/&gt;
+  for (encryptionEnabled in list(&quot;true&quot;, &quot;false&quot;)) {&lt;br/&gt;
+    # To simulate a large object scenario, we set spark.r.maxAllocationLimit to a smaller value&lt;br/&gt;
+    conf &amp;lt;- list(spark.r.maxAllocationLimit = &quot;100&quot;,&lt;br/&gt;
+                 spark.io.encryption.enabled = encryptionEnabled)&lt;br/&gt;
+&lt;br/&gt;
+    suppressWarnings(sparkR.session(master = sparkRTestMaster,&lt;br/&gt;
+                                    sparkConfig = conf,&lt;br/&gt;
+                                    enableHiveSupport = FALSE))&lt;br/&gt;
+&lt;br/&gt;
+    sc &amp;lt;- getSparkContext()&lt;br/&gt;
+    actual &amp;lt;- callJStatic(&quot;org.apache.spark.api.r.RUtils&quot;, &quot;getEncryptionEnabled&quot;, sc)&lt;br/&gt;
+    expected &amp;lt;- as.logical(encryptionEnabled)&lt;br/&gt;
+    expect_equal(actual, expected)&lt;br/&gt;
+&lt;br/&gt;
+    tryCatch(&lt;/p&gt;
{
+      # suppress warnings from dot in the field names. See also SPARK-21536.
+      df &amp;lt;- suppressWarnings(createDataFrame(iris, numPartitions = 3))
+      expect_equal(getNumPartitions(df), 3)
+      expect_equal(dim(df), dim(iris))
+
+      df &amp;lt;- createDataFrame(cars, numPartitions = 3)
+      expect_equal(collect(df), cars)
+    }
&lt;p&gt;,&lt;br/&gt;
+    finally = &lt;/p&gt;
{
+      sparkR.stop()
+    }
&lt;p&gt;)&lt;br/&gt;
+  }&lt;br/&gt;
+})&lt;br/&gt;
diff --git a/R/pkg/tests/fulltests/test_context.R b/R/pkg/tests/fulltests/test_context.R&lt;br/&gt;
index 288a2714a554e..eb8d2a700e1ea 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/tests/fulltests/test_context.R&lt;br/&gt;
+++ b/R/pkg/tests/fulltests/test_context.R&lt;br/&gt;
@@ -54,15 +54,6 @@ test_that(&quot;Check masked functions&quot;, &lt;/p&gt;
{
                sort(namesOfMaskedCompletely, na.last = TRUE))
 }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;-test_that(&quot;repeatedly starting and stopping SparkR&quot;, {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (i in 1:4) 
{
-    sc &amp;lt;- suppressWarnings(sparkR.init(master = sparkRTestMaster))
-    rdd &amp;lt;- parallelize(sc, 1:20, 2L)
-    expect_equal(countRDD(rdd), 20)
-    suppressWarnings(sparkR.stop())
-  }
&lt;p&gt;-})&lt;br/&gt;
-&lt;br/&gt;
 test_that(&quot;repeatedly starting and stopping SparkSession&quot;, {&lt;br/&gt;
   for (i in 1:4) {&lt;br/&gt;
     sparkR.session(master = sparkRTestMaster, enableHiveSupport = FALSE)&lt;br/&gt;
@@ -101,9 +92,6 @@ test_that(&quot;job group functions can be called&quot;, &lt;/p&gt;
{
   cancelJobGroup(&quot;groupId&quot;)
   clearJobGroup()
 
-  suppressWarnings(setJobGroup(sc, &quot;groupId&quot;, &quot;job description&quot;, TRUE))
-  suppressWarnings(cancelJobGroup(sc, &quot;groupId&quot;))
-  suppressWarnings(clearJobGroup(sc))
   sparkR.session.stop()
 }
&lt;p&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/R/pkg/tests/fulltests/test_mllib_fpm.R b/R/pkg/tests/fulltests/test_mllib_fpm.R&lt;br/&gt;
index d80f66a25de1c..daf9ff97a8216 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/tests/fulltests/test_mllib_fpm.R&lt;br/&gt;
+++ b/R/pkg/tests/fulltests/test_mllib_fpm.R&lt;br/&gt;
@@ -83,4 +83,20 @@ test_that(&quot;spark.fpGrowth&quot;, {&lt;/p&gt;

&lt;p&gt; })&lt;/p&gt;

&lt;p&gt;+test_that(&quot;spark.prefixSpan&quot;, &lt;/p&gt;
{
+    df &amp;lt;- createDataFrame(list(list(list(list(1L, 2L), list(3L))),
+                          list(list(list(1L), list(3L, 2L), list(1L, 2L))),
+                          list(list(list(1L, 2L), list(5L))),
+                          list(list(list(6L)))), schema = c(&quot;sequence&quot;))
+    result1 &amp;lt;- spark.findFrequentSequentialPatterns(df, minSupport = 0.5, maxPatternLength = 5L,
+                                                    maxLocalProjDBSize = 32000000L)
+
+    expected_result &amp;lt;- createDataFrame(list(list(list(list(1L)), 3L),
+                                            list(list(list(3L)), 2L),
+                                            list(list(list(2L)), 3L),
+                                            list(list(list(1L, 2L)), 3L),
+                                            list(list(list(1L), list(3L)), 2L)),
+                                            schema = c(&quot;sequence&quot;, &quot;freq&quot;))
+  }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
 sparkR.session.stop()&lt;br/&gt;
diff --git a/R/pkg/tests/fulltests/test_sparkSQL.R b/R/pkg/tests/fulltests/test_sparkSQL.R&lt;br/&gt;
index 0c4bdb31b027b..0d5118c127f2b 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/tests/fulltests/test_sparkSQL.R&lt;br/&gt;
+++ b/R/pkg/tests/fulltests/test_sparkSQL.R&lt;br/&gt;
@@ -106,15 +106,6 @@ if (is_windows()) &lt;/p&gt;
{
   Sys.setenv(TZ = &quot;GMT&quot;)
 }

&lt;p&gt;-test_that(&quot;calling sparkRSQL.init returns existing SQL context&quot;, &lt;/p&gt;
{
-  sqlContext &amp;lt;- suppressWarnings(sparkRSQL.init(sc))
-  expect_equal(suppressWarnings(sparkRSQL.init(sc)), sqlContext)
-}
&lt;p&gt;)&lt;br/&gt;
-&lt;br/&gt;
-test_that(&quot;calling sparkRSQL.init returns existing SparkSession&quot;, &lt;/p&gt;
{
-  expect_equal(suppressWarnings(sparkRSQL.init(sc)), sparkSession)
-}
&lt;p&gt;)&lt;br/&gt;
-&lt;br/&gt;
 test_that(&quot;calling sparkR.session returns existing SparkSession&quot;, &lt;/p&gt;
{
   expect_equal(sparkR.session(), sparkSession)
 }
&lt;p&gt;)&lt;br/&gt;
@@ -221,7 +212,7 @@ test_that(&quot;structField type strings&quot;, {&lt;/p&gt;

&lt;p&gt; test_that(&quot;create DataFrame from RDD&quot;, {&lt;br/&gt;
   rdd &amp;lt;- lapply(parallelize(sc, 1:10), function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;
{ list(x, as.character(x)) }
&lt;p&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;df &amp;lt;- createDataFrame(rdd, list(&quot;a&quot;, &quot;b&quot;))&lt;br/&gt;
+  df &amp;lt;- SparkR::createDataFrame(rdd, list(&quot;a&quot;, &quot;b&quot;))&lt;br/&gt;
   dfAsDF &amp;lt;- as.DataFrame(rdd, list(&quot;a&quot;, &quot;b&quot;))&lt;br/&gt;
   expect_is(df, &quot;SparkDataFrame&quot;)&lt;br/&gt;
   expect_is(dfAsDF, &quot;SparkDataFrame&quot;)&lt;br/&gt;
@@ -287,7 +278,7 @@ test_that(&quot;create DataFrame from RDD&quot;, {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   df &amp;lt;- as.DataFrame(cars, numPartitions = 2)&lt;br/&gt;
   expect_equal(getNumPartitions(df), 2)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;df &amp;lt;- createDataFrame(cars, numPartitions = 3)&lt;br/&gt;
+  df &amp;lt;- SparkR::createDataFrame(cars, numPartitions = 3)&lt;br/&gt;
   expect_equal(getNumPartitions(df), 3)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;validate limit by num of rows&lt;br/&gt;
   df &amp;lt;- createDataFrame(cars, numPartitions = 60)&lt;br/&gt;
@@ -308,7 +299,7 @@ test_that(&quot;create DataFrame from RDD&quot;, {&lt;br/&gt;
   sql(&quot;CREATE TABLE people (name string, age double, height float)&quot;)&lt;br/&gt;
   df &amp;lt;- read.df(jsonPathNa, &quot;json&quot;, schema)&lt;br/&gt;
   insertInto(df, &quot;people&quot;)&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect_equal(collect(sql(&quot;SELECT age from people WHERE name = &apos;Bob&apos;&quot;))$age,&lt;br/&gt;
+  expect_equal(collect(SparkR::sql(&quot;SELECT age from people WHERE name = &apos;Bob&apos;&quot;))$age,&lt;br/&gt;
                c(16))&lt;br/&gt;
   expect_equal(collect(sql(&quot;SELECT height from people WHERE name =&apos;Bob&apos;&quot;))$height,&lt;br/&gt;
                c(176.5))&lt;br/&gt;
@@ -316,18 +307,6 @@ test_that(&quot;create DataFrame from RDD&quot;, 
{
   unsetHiveContext()
 }
&lt;p&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-test_that(&quot;createDataFrame uses files for large objects&quot;, &lt;/p&gt;
{
-  # To simulate a large file scenario, we set spark.r.maxAllocationLimit to a smaller value
-  conf &amp;lt;- callJMethod(sparkSession, &quot;conf&quot;)
-  callJMethod(conf, &quot;set&quot;, &quot;spark.r.maxAllocationLimit&quot;, &quot;100&quot;)
-  df &amp;lt;- suppressWarnings(createDataFrame(iris, numPartitions = 3))
-  expect_equal(getNumPartitions(df), 3)
-
-  # Resetting the conf back to default value
-  callJMethod(conf, &quot;set&quot;, &quot;spark.r.maxAllocationLimit&quot;, toString(.Machine$integer.max / 10))
-  expect_equal(dim(df), dim(iris))
-}
&lt;p&gt;)&lt;br/&gt;
-&lt;br/&gt;
 test_that(&quot;read/write csv as DataFrame&quot;, {&lt;br/&gt;
   if (windows_with_hadoop()) {&lt;br/&gt;
     csvPath &amp;lt;- tempfile(pattern = &quot;sparkr-test&quot;, fileext = &quot;.csv&quot;)&lt;br/&gt;
@@ -640,14 +619,10 @@ test_that(&quot;read/write json files&quot;, {&lt;br/&gt;
     jsonPath3 &amp;lt;- tempfile(pattern = &quot;jsonPath3&quot;, fileext = &quot;.json&quot;)&lt;br/&gt;
     write.json(df, jsonPath3)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# Test read.json()/jsonFile() works with multiple input paths&lt;br/&gt;
+    # Test read.json() works with multiple input paths&lt;br/&gt;
     jsonDF1 &amp;lt;- read.json(c(jsonPath2, jsonPath3))&lt;br/&gt;
     expect_is(jsonDF1, &quot;SparkDataFrame&quot;)&lt;br/&gt;
     expect_equal(count(jsonDF1), 6)&lt;/li&gt;
	&lt;li&gt;# Suppress warnings because jsonFile is deprecated&lt;/li&gt;
	&lt;li&gt;jsonDF2 &amp;lt;- suppressWarnings(jsonFile(c(jsonPath2, jsonPath3)))&lt;/li&gt;
	&lt;li&gt;expect_is(jsonDF2, &quot;SparkDataFrame&quot;)&lt;/li&gt;
	&lt;li&gt;expect_equal(count(jsonDF2), 6)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     unlink(jsonPath2)&lt;br/&gt;
     unlink(jsonPath3)&lt;br/&gt;
@@ -667,20 +642,6 @@ test_that(&quot;read/write json files - compression option&quot;, &lt;/p&gt;
{
   unlink(jsonPath)
 }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;-test_that(&quot;jsonRDD() on a RDD with json string&quot;, &lt;/p&gt;
{
-  sqlContext &amp;lt;- suppressWarnings(sparkRSQL.init(sc))
-  rdd &amp;lt;- parallelize(sc, mockLines)
-  expect_equal(countRDD(rdd), 3)
-  df &amp;lt;- suppressWarnings(jsonRDD(sqlContext, rdd))
-  expect_is(df, &quot;SparkDataFrame&quot;)
-  expect_equal(count(df), 3)
-
-  rdd2 &amp;lt;- flatMap(rdd, function(x) c(x, x))
-  df &amp;lt;- suppressWarnings(jsonRDD(sqlContext, rdd2))
-  expect_is(df, &quot;SparkDataFrame&quot;)
-  expect_equal(count(df), 6)
-}
&lt;p&gt;)&lt;br/&gt;
-&lt;br/&gt;
 test_that(&quot;test tableNames and tables&quot;, {&lt;br/&gt;
   count &amp;lt;- count(listTables())&lt;/p&gt;

&lt;p&gt;@@ -695,10 +656,10 @@ test_that(&quot;test tableNames and tables&quot;, {&lt;br/&gt;
   expect_true(&quot;tableName&quot; %in% colnames(tables()))&lt;br/&gt;
   expect_true(all(c(&quot;tableName&quot;, &quot;database&quot;, &quot;isTemporary&quot;) %in% colnames(tables())))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;suppressWarnings(registerTempTable(df, &quot;table2&quot;))&lt;br/&gt;
+  createOrReplaceTempView(df, &quot;table2&quot;)&lt;br/&gt;
   tables &amp;lt;- listTables()&lt;br/&gt;
   expect_equal(count(tables), count + 2)&lt;/li&gt;
	&lt;li&gt;suppressWarnings(dropTempTable(&quot;table1&quot;))&lt;br/&gt;
+  dropTempView(&quot;table1&quot;)&lt;br/&gt;
   expect_true(dropTempView(&quot;table2&quot;))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   tables &amp;lt;- listTables()&lt;br/&gt;
@@ -1418,7 +1379,7 @@ test_that(&quot;column operators&quot;, {&lt;/p&gt;

&lt;p&gt; test_that(&quot;column functions&quot;, {&lt;br/&gt;
   c &amp;lt;- column(&quot;a&quot;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;c1 &amp;lt;- abs(c) + acos(c) + approxCountDistinct(c) + ascii(c) + asin(c) + atan(c)&lt;br/&gt;
+  c1 &amp;lt;- abs(c) + acos(c) + approx_count_distinct(c) + ascii(c) + asin(c) + atan(c)&lt;br/&gt;
   c2 &amp;lt;- avg(c) + base64(c) + bin(c) + bitwiseNOT(c) + cbrt(c) + ceil(c) + cos(c)&lt;br/&gt;
   c3 &amp;lt;- cosh(c) + count(c) + crc32(c) + hash(c) + exp(c)&lt;br/&gt;
   c4 &amp;lt;- explode(c) + expm1(c) + factorial(c) + first(c) + floor(c) + hex(c)&lt;br/&gt;
@@ -1427,7 +1388,7 @@ test_that(&quot;column functions&quot;, {&lt;br/&gt;
   c7 &amp;lt;- mean(c) + min(c) + month(c) + negate(c) + posexplode(c) + quarter(c)&lt;br/&gt;
   c8 &amp;lt;- reverse(c) + rint(c) + round(c) + rtrim(c) + sha1(c) + monotonically_increasing_id()&lt;br/&gt;
   c9 &amp;lt;- signum(c) + sin(c) + sinh(c) + size(c) + stddev(c) + soundex(c) + sqrt(c) + sum(c)&lt;/li&gt;
	&lt;li&gt;c10 &amp;lt;- sumDistinct(c) + tan(c) + tanh(c) + toDegrees(c) + toRadians(c)&lt;br/&gt;
+  c10 &amp;lt;- sumDistinct(c) + tan(c) + tanh(c) + degrees(c) + radians(c)&lt;br/&gt;
   c11 &amp;lt;- to_date(c) + trim(c) + unbase64(c) + unhex(c) + upper(c)&lt;br/&gt;
   c12 &amp;lt;- variance(c) + ltrim(c, &quot;a&quot;) + rtrim(c, &quot;b&quot;) + trim(c, &quot;c&quot;)&lt;br/&gt;
   c13 &amp;lt;- lead(&quot;col&quot;, 1) + lead(c, 1) + lag(&quot;col&quot;, 1) + lag(c, 1)&lt;br/&gt;
@@ -1659,7 +1620,20 @@ test_that(&quot;column functions&quot;, {&lt;br/&gt;
   expect_equal(collect(select(df, bround(df$x, 0)))[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;]&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, 2)&lt;br/&gt;
   expect_equal(collect(select(df, bround(df$x, 0)))[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;]&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;, 4)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# Test to_json(), from_json()&lt;br/&gt;
+  # Test from_csv(), schema_of_csv()&lt;br/&gt;
+  df &amp;lt;- as.DataFrame(list(list(&quot;col&quot; = &quot;1&quot;)))&lt;br/&gt;
+  c &amp;lt;- collect(select(df, alias(from_csv(df$col, &quot;a INT&quot;), &quot;csv&quot;)))&lt;br/&gt;
+  expect_equal(c[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;][&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;]$a, 1)&lt;br/&gt;
+  c &amp;lt;- collect(select(df, alias(from_csv(df$col, lit(&quot;a INT&quot;)), &quot;csv&quot;)))&lt;br/&gt;
+  expect_equal(c[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;][&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;]$a, 1)&lt;br/&gt;
+&lt;br/&gt;
+  df &amp;lt;- as.DataFrame(list(list(&quot;col&quot; = &quot;1&quot;)))&lt;br/&gt;
+  c &amp;lt;- collect(select(df, schema_of_csv(&quot;Amsterdam,2018&quot;)))&lt;br/&gt;
+  expect_equal(c[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;], &quot;struct&amp;lt;_c0:string,_c1:int&amp;gt;&quot;)&lt;br/&gt;
+  c &amp;lt;- collect(select(df, schema_of_csv(lit(&quot;Amsterdam,2018&quot;))))&lt;br/&gt;
+  expect_equal(c[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;], &quot;struct&amp;lt;_c0:string,_c1:int&amp;gt;&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  # Test to_json(), from_json(), schema_of_json()&lt;br/&gt;
   df &amp;lt;- sql(&quot;SELECT array(named_struct(&apos;name&apos;, &apos;Bob&apos;), named_struct(&apos;name&apos;, &apos;Alice&apos;)) as people&quot;)&lt;br/&gt;
   j &amp;lt;- collect(select(df, alias(to_json(df$people), &quot;json&quot;)))&lt;br/&gt;
   expect_equal(j&lt;span class=&quot;error&quot;&gt;&amp;#91;order(j$json), &amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, &quot;[
{\&quot;name\&quot;:\&quot;Bob\&quot;}
&lt;p&gt;,&lt;/p&gt;
{\&quot;name\&quot;:\&quot;Alice\&quot;}
&lt;p&gt;]&quot;)&lt;br/&gt;
@@ -1686,6 +1660,12 @@ test_that(&quot;column functions&quot;, &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     expect_true(any(apply(s, 1, function(x) { x[[1]]$age == 16 })))   }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  df &amp;lt;- as.DataFrame(list(list(&quot;col&quot; = &quot;1&quot;)))&lt;br/&gt;
+  c &amp;lt;- collect(select(df, schema_of_json(&apos;&lt;/p&gt;
{&quot;name&quot;:&quot;Bob&quot;}
&lt;p&gt;&apos;)))&lt;br/&gt;
+  expect_equal(c[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;], &quot;struct&amp;lt;name:string&amp;gt;&quot;)&lt;br/&gt;
+  c &amp;lt;- collect(select(df, schema_of_json(lit(&apos;&lt;/p&gt;
{&quot;name&quot;:&quot;Bob&quot;}
&lt;p&gt;&apos;))))&lt;br/&gt;
+  expect_equal(c[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;], &quot;struct&amp;lt;name:string&amp;gt;&quot;)&lt;br/&gt;
+&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Test to_json() supports arrays of primitive types and arrays&lt;br/&gt;
   df &amp;lt;- sql(&quot;SELECT array(19, 42, 70) as age&quot;)&lt;br/&gt;
   j &amp;lt;- collect(select(df, alias(to_json(df$age), &quot;json&quot;)))&lt;br/&gt;
@@ -1699,14 +1679,14 @@ test_that(&quot;column functions&quot;, {&lt;br/&gt;
   df &amp;lt;- as.DataFrame(list(list(&quot;col&quot; = &quot;
{\&quot;date\&quot;:\&quot;21/10/2014\&quot;}
&lt;p&gt;&quot;)))&lt;br/&gt;
   schema2 &amp;lt;- structType(structField(&quot;date&quot;, &quot;date&quot;))&lt;br/&gt;
   s &amp;lt;- collect(select(df, from_json(df$col, schema2)))&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect_equal(s[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;][&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;], NA)&lt;br/&gt;
+  expect_equal(s[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;][&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;]$date, NA)&lt;br/&gt;
   s &amp;lt;- collect(select(df, from_json(df$col, schema2, dateFormat = &quot;dd/MM/yyyy&quot;)))&lt;br/&gt;
   expect_is(s[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;][&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;]$date, &quot;Date&quot;)&lt;br/&gt;
   expect_equal(as.character(s[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;][&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;]$date), &quot;2014-10-21&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;check for unparseable&lt;br/&gt;
   df &amp;lt;- as.DataFrame(list(list(&quot;a&quot; = &quot;&quot;)))&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expect_equal(collect(select(df, from_json(df$a, schema)))[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;][&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;], NA)&lt;br/&gt;
+  expect_equal(collect(select(df, from_json(df$a, schema)))[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;][&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;]$a, NA)&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;check if array type in string is correctly supported.&lt;br/&gt;
   jsonArr &amp;lt;- &quot;[
{\&quot;name\&quot;:\&quot;Bob\&quot;}
&lt;p&gt;, &lt;/p&gt;
{\&quot;name\&quot;:\&quot;Alice\&quot;}
&lt;p&gt;]&quot;&lt;br/&gt;
@@ -1721,6 +1701,11 @@ test_that(&quot;column functions&quot;, &lt;/p&gt;
{
     expect_equal(arr$arrcol[[1]][[2]]$name, &quot;Alice&quot;)
   }&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;+  # Test to_csv()&lt;br/&gt;
+  df &amp;lt;- sql(&quot;SELECT named_struct(&apos;name&apos;, &apos;Bob&apos;) as people&quot;)&lt;br/&gt;
+  j &amp;lt;- collect(select(df, alias(to_csv(df$people), &quot;csv&quot;)))&lt;br/&gt;
+  expect_equal(j&lt;span class=&quot;error&quot;&gt;&amp;#91;order(j$csv), &amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, &quot;Bob&quot;)&lt;br/&gt;
+&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Test create_array() and create_map()&lt;br/&gt;
   df &amp;lt;- as.DataFrame(data.frame(&lt;br/&gt;
     x = c(1.0, 2.0), y = c(-1.0, 3.0), z = c(-2.0, 5.0)&lt;br/&gt;
@@ -1831,6 +1816,14 @@ test_that(&quot;string operators&quot;, {&lt;br/&gt;
     collect(select(df4, split_string(df4$a, &quot;\\\\&quot;)))&lt;span class=&quot;error&quot;&gt;&amp;#91;1, 1&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     list(list(&quot;a.b@c.d   1&quot;, &quot;b&quot;))&lt;br/&gt;
   )&lt;br/&gt;
+  expect_equal(&lt;br/&gt;
+    collect(select(df4, split_string(df4$a, &quot;&lt;br class=&quot;atl-forced-newline&quot; /&gt;.&quot;, 2)))&lt;span class=&quot;error&quot;&gt;&amp;#91;1, 1&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    list(list(&quot;a&quot;, &quot;b@c.d   1&lt;br class=&quot;atl-forced-newline&quot; /&gt;b&quot;))&lt;br/&gt;
+  )&lt;br/&gt;
+  expect_equal(&lt;br/&gt;
+    collect(select(df4, split_string(df4$a, &quot;b&quot;, 0)))&lt;span class=&quot;error&quot;&gt;&amp;#91;1, 1&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    list(list(&quot;a.&quot;, &quot;@c.d   1&lt;br class=&quot;atl-forced-newline&quot; /&gt;&quot;, &quot;&quot;))&lt;br/&gt;
+  )&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   l5 &amp;lt;- list(list(a = &quot;abc&quot;))&lt;br/&gt;
   df5 &amp;lt;- createDataFrame(l5)&lt;br/&gt;
@@ -2419,6 +2412,15 @@ test_that(&quot;join(), crossJoin() and merge() on a DataFrame&quot;, &lt;/p&gt;
{
   expect_true(any(grepl(&quot;BroadcastHashJoin&quot;, execution_plan_broadcast)))
 }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;+test_that(&quot;test hint&quot;, &lt;/p&gt;
{
+  df &amp;lt;- sql(&quot;SELECT * FROM range(10e10)&quot;)
+  hintList &amp;lt;- list(&quot;hint2&quot;, &quot;hint3&quot;, &quot;hint4&quot;)
+  execution_plan_hint &amp;lt;- capture.output(
+    explain(hint(df, &quot;hint1&quot;, 1.23456, &quot;aaaaaaaaaa&quot;, hintList), TRUE)
+  )
+  expect_true(any(grepl(&quot;1.23456, aaaaaaaaaa&quot;, execution_plan_hint)))
+}
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
 test_that(&quot;toJSON() on DataFrame&quot;, {&lt;br/&gt;
   df &amp;lt;- as.DataFrame(cars)&lt;br/&gt;
   df_json &amp;lt;- toJSON(df)&lt;br/&gt;
@@ -2467,6 +2469,7 @@ test_that(&quot;union(), unionByName(), rbind(), except(), and intersect() on a DataF&lt;br/&gt;
   expect_is(unioned, &quot;SparkDataFrame&quot;)&lt;br/&gt;
   expect_equal(count(unioned), 6)&lt;br/&gt;
   expect_equal(first(unioned)$name, &quot;Michael&quot;)&lt;br/&gt;
+  expect_equal(count(arrange(suppressWarnings(union(df, df2)), df$age)), 6)&lt;br/&gt;
   expect_equal(count(arrange(suppressWarnings(unionAll(df, df2)), df$age)), 6)&lt;/p&gt;

&lt;p&gt;   df1 &amp;lt;- select(df2, &quot;age&quot;, &quot;name&quot;)&lt;br/&gt;
@@ -2646,17 +2649,14 @@ test_that(&quot;read/write Parquet files&quot;, {&lt;br/&gt;
     expect_is(df2, &quot;SparkDataFrame&quot;)&lt;br/&gt;
     expect_equal(count(df2), 3)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;# Test write.parquet/saveAsParquetFile and read.parquet/parquetFile&lt;br/&gt;
+    # Test write.parquet and read.parquet&lt;br/&gt;
     parquetPath2 &amp;lt;- tempfile(pattern = &quot;parquetPath2&quot;, fileext = &quot;.parquet&quot;)&lt;br/&gt;
     write.parquet(df, parquetPath2)&lt;br/&gt;
     parquetPath3 &amp;lt;- tempfile(pattern = &quot;parquetPath3&quot;, fileext = &quot;.parquet&quot;)&lt;/li&gt;
	&lt;li&gt;suppressWarnings(saveAsParquetFile(df, parquetPath3))&lt;br/&gt;
+    write.parquet(df, parquetPath3)&lt;br/&gt;
     parquetDF &amp;lt;- read.parquet(c(parquetPath2, parquetPath3))&lt;br/&gt;
     expect_is(parquetDF, &quot;SparkDataFrame&quot;)&lt;br/&gt;
     expect_equal(count(parquetDF), count(df) * 2)&lt;/li&gt;
	&lt;li&gt;parquetDF2 &amp;lt;- suppressWarnings(parquetFile(parquetPath2, parquetPath3))&lt;/li&gt;
	&lt;li&gt;expect_is(parquetDF2, &quot;SparkDataFrame&quot;)&lt;/li&gt;
	&lt;li&gt;expect_equal(count(parquetDF2), count(df) * 2)&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;Test if varargs works with variables&lt;br/&gt;
     saveMode &amp;lt;- &quot;overwrite&quot;&lt;br/&gt;
@@ -2704,8 +2704,16 @@ test_that(&quot;read/write text files&quot;, 
{
   expect_equal(colnames(df2), c(&quot;value&quot;))
   expect_equal(count(df2), count(df) * 2)
 
+  df3 &amp;lt;- createDataFrame(list(list(1L, &quot;1&quot;), list(2L, &quot;2&quot;), list(1L, &quot;1&quot;), list(2L, &quot;2&quot;)),
+                         schema = c(&quot;key&quot;, &quot;value&quot;))
+  textPath3 &amp;lt;- tempfile(pattern = &quot;textPath3&quot;, fileext = &quot;.txt&quot;)
+  write.df(df3, textPath3, &quot;text&quot;, mode = &quot;overwrite&quot;, partitionBy = &quot;key&quot;)
+  df4 &amp;lt;- read.df(textPath3, &quot;text&quot;)
+  expect_equal(count(df3), count(df4))
+
   unlink(textPath)
   unlink(textPath2)
+  unlink(textPath3)
 }
&lt;p&gt;)&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; test_that(&quot;read/write text files - compression option&quot;, {&lt;br/&gt;
@@ -3457,39 +3465,6 @@ test_that(&quot;Window functions on a DataFrame&quot;, &lt;/p&gt;
{
   expect_equal(result, expected)
 }
&lt;p&gt;)&lt;/p&gt;

&lt;p&gt;-test_that(&quot;createDataFrame sqlContext parameter backward compatibility&quot;, &lt;/p&gt;
{
-  sqlContext &amp;lt;- suppressWarnings(sparkRSQL.init(sc))
-  a &amp;lt;- 1:3
-  b &amp;lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)
-  ldf &amp;lt;- data.frame(a, b)
-  # Call function with namespace :: operator - SPARK-16538
-  df &amp;lt;- suppressWarnings(SparkR::createDataFrame(sqlContext, ldf))
-  expect_equal(columns(df), c(&quot;a&quot;, &quot;b&quot;))
-  expect_equal(dtypes(df), list(c(&quot;a&quot;, &quot;int&quot;), c(&quot;b&quot;, &quot;string&quot;)))
-  expect_equal(count(df), 3)
-  ldf2 &amp;lt;- collect(df)
-  expect_equal(ldf$a, ldf2$a)
-
-  df2 &amp;lt;- suppressWarnings(createDataFrame(sqlContext, iris))
-  expect_equal(count(df2), 150)
-  expect_equal(ncol(df2), 5)
-
-  df3 &amp;lt;- suppressWarnings(read.df(sqlContext, jsonPath, &quot;json&quot;))
-  expect_is(df3, &quot;SparkDataFrame&quot;)
-  expect_equal(count(df3), 3)
-
-  before &amp;lt;- suppressWarnings(createDataFrame(sqlContext, iris))
-  after &amp;lt;- suppressWarnings(createDataFrame(iris))
-  expect_equal(collect(before), collect(after))
-
-  # more tests for SPARK-16538
-  createOrReplaceTempView(df, &quot;table&quot;)
-  SparkR::listTables()
-  SparkR::sql(&quot;SELECT 1&quot;)
-  suppressWarnings(SparkR::sql(sqlContext, &quot;SELECT * FROM table&quot;))
-  suppressWarnings(SparkR::dropTempTable(sqlContext, &quot;table&quot;))
-}
&lt;p&gt;)&lt;br/&gt;
-&lt;br/&gt;
 test_that(&quot;randomSplit&quot;, {&lt;br/&gt;
   num &amp;lt;- 4000&lt;br/&gt;
   df &amp;lt;- createDataFrame(data.frame(id = 1:num))&lt;br/&gt;
@@ -3676,7 +3651,7 @@ test_that(&quot;catalog APIs, listTables, listColumns, listFunctions&quot;, {&lt;/p&gt;

&lt;p&gt;   createOrReplaceTempView(as.DataFrame(cars), &quot;cars&quot;)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;tb &amp;lt;- listTables()&lt;br/&gt;
+  tb &amp;lt;- SparkR::listTables()&lt;br/&gt;
   expect_equal(nrow(tb), count + 1)&lt;br/&gt;
   tbs &amp;lt;- collect(tb)&lt;br/&gt;
   expect_true(nrow(tbs&lt;span class=&quot;error&quot;&gt;&amp;#91;tbs$name == &amp;quot;cars&amp;quot;, &amp;#93;&lt;/span&gt;) &amp;gt; 0)&lt;br/&gt;
diff --git a/R/pkg/tests/fulltests/test_sparkSQL_eager.R b/R/pkg/tests/fulltests/test_sparkSQL_eager.R&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..9b4489a47b655
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/R/pkg/tests/fulltests/test_sparkSQL_eager.R&lt;br/&gt;
@@ -0,0 +1,72 @@&lt;br/&gt;
+#&lt;br/&gt;
+# Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+# contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+# this work for additional information regarding copyright ownership.&lt;br/&gt;
+# The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+# (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+# the License.  You may obtain a copy of the License at&lt;br/&gt;
+#&lt;br/&gt;
+#    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+#&lt;br/&gt;
+# Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+# distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+# See the License for the specific language governing permissions and&lt;br/&gt;
+# limitations under the License.&lt;br/&gt;
+#&lt;br/&gt;
+&lt;br/&gt;
+library(testthat)&lt;br/&gt;
+&lt;br/&gt;
+context(&quot;test show SparkDataFrame when eager execution is enabled.&quot;)&lt;br/&gt;
+&lt;br/&gt;
+test_that(&quot;eager execution is not enabled&quot;, 
{
+  # Start Spark session without eager execution enabled
+  sparkR.session(master = sparkRTestMaster, enableHiveSupport = FALSE)
+
+  df &amp;lt;- createDataFrame(faithful)
+  expect_is(df, &quot;SparkDataFrame&quot;)
+  expected &amp;lt;- &quot;eruptions:double, waiting:double&quot;
+  expect_output(show(df), expected)
+
+  # Stop Spark session
+  sparkR.session.stop()
+}
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
+test_that(&quot;eager execution is enabled&quot;, &lt;/p&gt;
{
+  # Start Spark session with eager execution enabled
+  sparkConfig &amp;lt;- list(spark.sql.repl.eagerEval.enabled = &quot;true&quot;)
+
+  sparkR.session(master = sparkRTestMaster, enableHiveSupport = FALSE, sparkConfig = sparkConfig)
+
+  df &amp;lt;- createDataFrame(faithful)
+  expect_is(df, &quot;SparkDataFrame&quot;)
+  expected &amp;lt;- paste0(&quot;(+---------+-------+\n&quot;,
+                     &quot;|eruptions|waiting|\n&quot;,
+                     &quot;+---------+-------+\n)*&quot;,
+                     &quot;(only showing top 20 rows)&quot;)
+  expect_output(show(df), expected)
+
+  # Stop Spark session
+  sparkR.session.stop()
+}
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
+test_that(&quot;eager execution is enabled with maxNumRows and truncate set&quot;, &lt;/p&gt;
{
+  # Start Spark session with eager execution enabled
+  sparkConfig &amp;lt;- list(spark.sql.repl.eagerEval.enabled = &quot;true&quot;,
+                      spark.sql.repl.eagerEval.maxNumRows = as.integer(5),
+                      spark.sql.repl.eagerEval.truncate = as.integer(2))
+
+  sparkR.session(master = sparkRTestMaster, enableHiveSupport = FALSE, sparkConfig = sparkConfig)
+
+  df &amp;lt;- arrange(createDataFrame(faithful), &quot;waiting&quot;)
+  expect_is(df, &quot;SparkDataFrame&quot;)
+  expected &amp;lt;- paste0(&quot;(+---------+-------+\n&quot;,
+                     &quot;|eruptions|waiting|\n&quot;,
+                     &quot;+---------+-------+\n&quot;,
+                     &quot;|       1.|     43|\n)*&quot;,
+                     &quot;(only showing top 5 rows)&quot;)
+  expect_output(show(df), expected)
+
+  # Stop Spark session
+  sparkR.session.stop()
+}
&lt;p&gt;)&lt;br/&gt;
diff --git a/R/pkg/tests/fulltests/test_streaming.R b/R/pkg/tests/fulltests/test_streaming.R&lt;br/&gt;
index bfb1a046490ec..6f0d2aefee886 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/R/pkg/tests/fulltests/test_streaming.R&lt;br/&gt;
+++ b/R/pkg/tests/fulltests/test_streaming.R&lt;br/&gt;
@@ -127,6 +127,7 @@ test_that(&quot;Specify a schema by using a DDL-formatted string when reading&quot;, {&lt;br/&gt;
   expect_false(awaitTermination(q, 5 * 1000))&lt;br/&gt;
   callJMethod(q@ssq, &quot;processAllAvailable&quot;)&lt;br/&gt;
   expect_equal(head(sql(&quot;SELECT count&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/star_yellow.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; FROM people3&quot;))[&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;], 3)&lt;br/&gt;
+  stopQuery(q)&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   expect_error(read.stream(path = parquetPath, schema = &quot;name stri&quot;),&lt;br/&gt;
                &quot;DataType stri is not supported.&quot;)&lt;br/&gt;
diff --git a/R/pkg/tests/run-all.R b/R/pkg/tests/run-all.R&lt;br/&gt;
index 94d75188fb948..1e96418558883 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/tests/run-all.R&lt;br/&gt;
+++ b/R/pkg/tests/run-all.R&lt;br/&gt;
@@ -18,50 +18,55 @@&lt;br/&gt;
 library(testthat)&lt;br/&gt;
 library(SparkR)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Turn all warnings into errors&lt;br/&gt;
-options(&quot;warn&quot; = 2)&lt;br/&gt;
+# &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25572&quot; title=&quot;SparkR tests failed on CRAN on Java 10&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25572&quot;&gt;&lt;del&gt;SPARK-25572&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
+if (identical(Sys.getenv(&quot;NOT_CRAN&quot;), &quot;true&quot;)) {&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-if (.Platform$OS.type == &quot;windows&quot;) &lt;/p&gt;
{
-  Sys.setenv(TZ = &quot;GMT&quot;)
-}
&lt;p&gt;+  # Turn all warnings into errors&lt;br/&gt;
+  options(&quot;warn&quot; = 2)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Setup global test environment&lt;/li&gt;
		&lt;li&gt;Install Spark first to set SPARK_HOME&lt;br/&gt;
+  if (.Platform$OS.type == &quot;windows&quot;) 
{
+    Sys.setenv(TZ = &quot;GMT&quot;)
+  }&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;NOTE(shivaram): We set overwrite to handle any old tar.gz files or directories left behind on&lt;/li&gt;
		&lt;li&gt;CRAN machines. For Jenkins we should already have SPARK_HOME set.&lt;br/&gt;
-install.spark(overwrite = TRUE)&lt;br/&gt;
+  # Setup global test environment&lt;br/&gt;
+  # Install Spark first to set SPARK_HOME&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;del&gt;sparkRDir &amp;lt;&lt;/del&gt; file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;)&lt;br/&gt;
&lt;del&gt;sparkRWhitelistSQLDirs &amp;lt;&lt;/del&gt; c(&quot;spark-warehouse&quot;, &quot;metastore_db&quot;)&lt;br/&gt;
-invisible(lapply(sparkRWhitelistSQLDirs,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; 
{ unlink(file.path(sparkRDir, x), recursive = TRUE, force = TRUE)}))&lt;br/&gt;
&lt;del&gt;sparkRFilesBefore &amp;lt;&lt;/del&gt; list.files(path = sparkRDir, all.files = TRUE)&lt;br/&gt;
+  # NOTE(shivaram): We set overwrite to handle any old tar.gz files or directories left behind on&lt;br/&gt;
+  # CRAN machines. For Jenkins we should already have SPARK_HOME set.&lt;br/&gt;
+  install.spark(overwrite = TRUE)&lt;br/&gt;
 &lt;br/&gt;
&lt;del&gt;sparkRTestMaster &amp;lt;&lt;/del&gt; &quot;local&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&quot;&lt;br/&gt;
&lt;del&gt;sparkRTestConfig &amp;lt;&lt;/del&gt; list()&lt;br/&gt;
-if (identical(Sys.getenv(&quot;NOT_CRAN&quot;), &quot;true&quot;)) {
-  sparkRTestMaster &amp;lt;- &quot;&quot;
-} else {
-  # Disable hsperfdata on CRAN
-  old_java_opt &amp;lt;- Sys.getenv(&quot;_JAVA_OPTIONS&quot;)
-  Sys.setenv(&quot;_JAVA_OPTIONS&quot; = paste(&quot;-XX:-UsePerfData&quot;, old_java_opt))
-  tmpDir &amp;lt;- tempdir()
-  tmpArg &amp;lt;- paste0(&quot;-Djava.io.tmpdir=&quot;, tmpDir)
-  sparkRTestConfig &amp;lt;- list(spark.driver.extraJavaOptions = tmpArg,
-                           spark.executor.extraJavaOptions = tmpArg)
-}&lt;br/&gt;
+  sparkRDir &amp;lt;- file.path(Sys.getenv(&quot;SPARK_HOME&quot;), &quot;R&quot;)&lt;br/&gt;
+  sparkRWhitelistSQLDirs &amp;lt;- c(&quot;spark-warehouse&quot;, &quot;metastore_db&quot;)&lt;br/&gt;
+  invisible(lapply(sparkRWhitelistSQLDirs,&lt;br/&gt;
+                   function&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; { unlink(file.path(sparkRDir, x), recursive = TRUE, force = TRUE)}
&lt;p&gt;))&lt;br/&gt;
+  sparkRFilesBefore &amp;lt;- list.files(path = sparkRDir, all.files = TRUE)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-test_package(&quot;SparkR&quot;)&lt;br/&gt;
+  sparkRTestMaster &amp;lt;- &quot;local&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&quot;&lt;br/&gt;
+  sparkRTestConfig &amp;lt;- list()&lt;br/&gt;
+  if (identical(Sys.getenv(&quot;NOT_CRAN&quot;), &quot;true&quot;)) &lt;/p&gt;
{
+    sparkRTestMaster &amp;lt;- &quot;&quot;
+  }
&lt;p&gt; else &lt;/p&gt;
{
+    # Disable hsperfdata on CRAN
+    old_java_opt &amp;lt;- Sys.getenv(&quot;_JAVA_OPTIONS&quot;)
+    Sys.setenv(&quot;_JAVA_OPTIONS&quot; = paste(&quot;-XX:-UsePerfData&quot;, old_java_opt))
+    tmpDir &amp;lt;- tempdir()
+    tmpArg &amp;lt;- paste0(&quot;-Djava.io.tmpdir=&quot;, tmpDir)
+    sparkRTestConfig &amp;lt;- list(spark.driver.extraJavaOptions = tmpArg,
+                             spark.executor.extraJavaOptions = tmpArg)
+  }

&lt;p&gt;-if (identical(Sys.getenv(&quot;NOT_CRAN&quot;), &quot;true&quot;)) &lt;/p&gt;
{
-  # set random seed for predictable results. mostly for base&apos;s sample() in tree and classification
-  set.seed(42)
-  # for testthat 1.0.2 later, change reporter from &quot;summary&quot; to default_reporter()
-  testthat:::run_tests(&quot;SparkR&quot;,
-                       file.path(sparkRDir, &quot;pkg&quot;, &quot;tests&quot;, &quot;fulltests&quot;),
-                       NULL,
-                       &quot;summary&quot;)
-}
&lt;p&gt;+  test_package(&quot;SparkR&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  if (identical(Sys.getenv(&quot;NOT_CRAN&quot;), &quot;true&quot;)) &lt;/p&gt;
{
+    # set random seed for predictable results. mostly for base&apos;s sample() in tree and classification
+    set.seed(42)
+    # for testthat 1.0.2 later, change reporter from &quot;summary&quot; to default_reporter()
+    testthat:::run_tests(&quot;SparkR&quot;,
+                         file.path(sparkRDir, &quot;pkg&quot;, &quot;tests&quot;, &quot;fulltests&quot;),
+                         NULL,
+                         &quot;summary&quot;)
+  }

&lt;p&gt;-SparkR:::uninstallDownloadedSpark()&lt;br/&gt;
+  SparkR:::uninstallDownloadedSpark()&lt;br/&gt;
+&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/R/pkg/vignettes/sparkr-vignettes.Rmd b/R/pkg/vignettes/sparkr-vignettes.Rmd&lt;br/&gt;
index 090363c5f8a3e..f80b45b4f36a8 100644&lt;br/&gt;
&amp;#8212; a/R/pkg/vignettes/sparkr-vignettes.Rmd&lt;br/&gt;
+++ b/R/pkg/vignettes/sparkr-vignettes.Rmd&lt;br/&gt;
@@ -57,6 +57,20 @@ First, let&apos;s load and attach the package.&lt;br/&gt;
 library(SparkR)&lt;br/&gt;
 ```&lt;/p&gt;

&lt;p&gt;+```&lt;/p&gt;
{r, include=FALSE}
&lt;p&gt;+# disable eval if java version not supported&lt;br/&gt;
+override_eval &amp;lt;- tryCatch(!is.numeric(SparkR:::checkJavaVersion()),&lt;br/&gt;
+          error = function(e) &lt;/p&gt;
{ TRUE },&lt;br/&gt;
+          warning = function(e) { TRUE }
&lt;p&gt;)&lt;br/&gt;
+&lt;br/&gt;
+if (override_eval) {&lt;br/&gt;
+  opts_hooks$set(eval = function(options) &lt;/p&gt;
{
+    options$eval = FALSE
+    options
+  }
&lt;p&gt;)&lt;br/&gt;
+}&lt;br/&gt;
+```&lt;br/&gt;
+&lt;br/&gt;
 `SparkSession` is the entry point into SparkR which connects your R program to a Spark cluster. You can create a `SparkSession` using `sparkR.session` and pass in options such as the application name, any Spark packages depended on, etc.&lt;/p&gt;

&lt;p&gt; We use default settings in which it runs in local mode. It auto downloads Spark package in the background if no previous installation is found. For more details about setup, see &lt;span class=&quot;error&quot;&gt;&amp;#91;Spark Session&amp;#93;&lt;/span&gt;(#SetupSparkSession).&lt;br/&gt;
@@ -157,8 +171,8 @@ Property Name | Property group | spark-submit equivalent&lt;br/&gt;
 `spark.driver.extraClassPath` | Runtime Environment | `--driver-class-path`&lt;br/&gt;
 `spark.driver.extraJavaOptions` | Runtime Environment | `--driver-java-options`&lt;br/&gt;
 `spark.driver.extraLibraryPath` | Runtime Environment | `--driver-library-path`&lt;br/&gt;
&lt;del&gt;`spark.yarn.keytab` | Application Properties | `&lt;/del&gt;-keytab`&lt;br/&gt;
&lt;del&gt;`spark.yarn.principal` | Application Properties | `&lt;/del&gt;-principal`&lt;br/&gt;
+`spark.kerberos.keytab` | Application Properties | `--keytab`&lt;br/&gt;
+`spark.kerberos.principal` | Application Properties | `--principal`&lt;/p&gt;

&lt;p&gt; *&lt;b&gt;For Windows users&lt;/b&gt;*: Due to different file prefixes across operating systems, to avoid the issue of potential wrong prefix, a current workaround is to specify `spark.sql.warehouse.dir` when starting the `SparkSession`.&lt;/p&gt;

&lt;p&gt;@@ -542,6 +556,7 @@ SparkR supports the following machine learning models and algorithms.&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;
			&lt;ol&gt;
				&lt;li&gt;Frequent Pattern Mining&lt;/li&gt;
			&lt;/ol&gt;
			&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul&gt;
	&lt;li&gt;FP-growth&lt;br/&gt;
+* PrefixSpan&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;
			&lt;ol&gt;
				&lt;li&gt;Statistics&lt;/li&gt;
			&lt;/ol&gt;
			&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;@@ -998,6 +1013,18 @@ We can make predictions based on the `antecedent`.&lt;br/&gt;
 head(predict(fpm, df))&lt;br/&gt;
 ```&lt;/p&gt;

&lt;p&gt;+#### PrefixSpan&lt;br/&gt;
+&lt;br/&gt;
+`spark.findFrequentSequentialPatterns` method can be used to find the complete set of frequent sequential patterns in the input sequences of itemsets.&lt;br/&gt;
+&lt;br/&gt;
+```&lt;/p&gt;
{r}
&lt;p&gt;+df &amp;lt;- createDataFrame(list(list(list(list(1L, 2L), list(3L))),&lt;br/&gt;
+                      list(list(list(1L), list(3L, 2L), list(1L, 2L))),&lt;br/&gt;
+                      list(list(list(1L, 2L), list(5L))),&lt;br/&gt;
+                      list(list(list(6L)))), schema = c(&quot;sequence&quot;))&lt;br/&gt;
+head(spark.findFrequentSequentialPatterns(df, minSupport = 0.5, maxPatternLength = 5L))&lt;br/&gt;
+```&lt;br/&gt;
+&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;
			&lt;ol&gt;
				&lt;li&gt;Kolmogorov-Smirnov Test&lt;/li&gt;
			&lt;/ol&gt;
			&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; `spark.kstest` runs a two-sided, one-sample &lt;span class=&quot;error&quot;&gt;&amp;#91;Kolmogorov-Smirnov (KS) test&amp;#93;&lt;/span&gt;(&lt;a href=&quot;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://en.wikipedia.org/wiki/Kolmogorov%E2%80%93Smirnov_test&lt;/a&gt;).&lt;br/&gt;
diff --git a/assembly/README b/assembly/README&lt;br/&gt;
index affd281a1385c..1fd6d8858348c 100644&lt;br/&gt;
&amp;#8212; a/assembly/README&lt;br/&gt;
+++ b/assembly/README&lt;br/&gt;
@@ -9,4 +9,4 @@ This module is off by default. To activate it specify the profile in the command&lt;/p&gt;

&lt;p&gt; If you need to build an assembly for a different version of Hadoop the&lt;br/&gt;
 hadoop-version system property needs to be set as in this example:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;-Dhadoop.version=2.7.7&lt;br/&gt;
+  -Dhadoop.version=2.7.4&lt;br/&gt;
diff --git a/assembly/pom.xml b/assembly/pom.xml&lt;br/&gt;
index 9608c96fd5369..68ebfadb668ab 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/assembly/pom.xml&lt;br/&gt;
+++ b/assembly/pom.xml&lt;br/&gt;
@@ -20,12 +20,12 @@&lt;br/&gt;
   &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&lt;br/&gt;
   &amp;lt;parent&amp;gt;&lt;br/&gt;
     &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-parent_2.11&amp;lt;/artifactId&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;version&amp;gt;2.4.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
+    &amp;lt;artifactId&amp;gt;spark-parent_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+    &amp;lt;version&amp;gt;3.0.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
     &amp;lt;relativePath&amp;gt;../pom.xml&amp;lt;/relativePath&amp;gt;&lt;br/&gt;
   &amp;lt;/parent&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-assembly_2.11&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+  &amp;lt;artifactId&amp;gt;spark-assembly_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
   &amp;lt;name&amp;gt;Spark Project Assembly&amp;lt;/name&amp;gt;&lt;br/&gt;
   &amp;lt;url&amp;gt;&lt;a href=&quot;http://spark.apache.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/&lt;/a&gt;&amp;lt;/url&amp;gt;&lt;br/&gt;
   &amp;lt;packaging&amp;gt;pom&amp;lt;/packaging&amp;gt;&lt;br/&gt;
diff --git a/bin/docker-image-tool.sh b/bin/docker-image-tool.sh&lt;br/&gt;
index d6371051ef7fb..fbf9c9e448fd1 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/bin/docker-image-tool.sh&lt;br/&gt;
+++ b/bin/docker-image-tool.sh&lt;br/&gt;
@@ -29,6 +29,20 @@ if [ -z &quot;${SPARK_HOME}&quot; ]; then&lt;br/&gt;
 fi&lt;br/&gt;
 . &quot;${SPARK_HOME}/bin/load-spark-env.sh&quot;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+CTX_DIR=&quot;$SPARK_HOME/target/tmp/docker&quot;&lt;br/&gt;
+&lt;br/&gt;
+function is_dev_build &lt;/p&gt;
{
+  [ ! -f &quot;$SPARK_HOME/RELEASE&quot; ]
+}
&lt;p&gt;+&lt;br/&gt;
+function cleanup_ctx_dir &lt;/p&gt;
{
+  if is_dev_build; then
+    rm -rf &quot;$CTX_DIR&quot;
+  fi
+}
&lt;p&gt;+&lt;br/&gt;
+trap cleanup_ctx_dir EXIT&lt;br/&gt;
+&lt;br/&gt;
 function image_ref {&lt;br/&gt;
   local image=&quot;$1&quot;&lt;br/&gt;
   local add_repo=&quot;${2:-1}&quot;&lt;br/&gt;
@@ -41,55 +55,142 @@ function image_ref &lt;/p&gt;
{
   echo &quot;$image&quot;
 }

&lt;p&gt;+function docker_push {&lt;br/&gt;
+  local image_name=&quot;$1&quot;&lt;br/&gt;
+  if [ ! -z $(docker images -q &quot;$(image_ref ${image_name})&quot;) ]; then&lt;br/&gt;
+    docker push &quot;$(image_ref ${image_name})&quot;&lt;br/&gt;
+    if [ $? -ne 0 ]; then&lt;br/&gt;
+      error &quot;Failed to push $image_name Docker image.&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  else&lt;br/&gt;
+    echo &quot;$(image_ref ${image_name}) image not found. Skipping push for this image.&quot;&lt;br/&gt;
+  fi&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+# Create a smaller build context for docker in dev builds to make the build faster. Docker&lt;br/&gt;
+# uploads all of the current directory to the daemon, and it can get pretty big with dev&lt;br/&gt;
+# builds that contain test log files and other artifacts.&lt;br/&gt;
+#&lt;br/&gt;
+# Three build contexts are created, one for each image: base, pyspark, and sparkr. For them&lt;br/&gt;
+# to have the desired effect, the docker command needs to be executed inside the appropriate&lt;br/&gt;
+# context directory.&lt;br/&gt;
+#&lt;br/&gt;
+# Note: docker does not support symlinks in the build context.&lt;br/&gt;
+function create_dev_build_context &lt;/p&gt;
{(
+  set -e
+  local BASE_CTX=&quot;$CTX_DIR/base&quot;
+  mkdir -p &quot;$BASE_CTX/kubernetes&quot;
+  cp -r &quot;resource-managers/kubernetes/docker/src/main/dockerfiles&quot; \
+    &quot;$BASE_CTX/kubernetes/dockerfiles&quot;
+
+  cp -r &quot;assembly/target/scala-$SPARK_SCALA_VERSION/jars&quot; &quot;$BASE_CTX/jars&quot;
+  cp -r &quot;resource-managers/kubernetes/integration-tests/tests&quot; \
+    &quot;$BASE_CTX/kubernetes/tests&quot;
+
+  mkdir &quot;$BASE_CTX/examples&quot;
+  cp -r &quot;examples/src&quot; &quot;$BASE_CTX/examples/src&quot;
+  # Copy just needed examples jars instead of everything.
+  mkdir &quot;$BASE_CTX/examples/jars&quot;
+  for i in examples/target/scala-$SPARK_SCALA_VERSION/jars/*; do
+    if [ ! -f &quot;$BASE_CTX/jars/$(basename $i)&quot; ]; then
+      cp $i &quot;$BASE_CTX/examples/jars&quot;
+    fi
+  done
+
+  for other in bin sbin data; do
+    cp -r &quot;$other&quot; &quot;$BASE_CTX/$other&quot;
+  done
+
+  local PYSPARK_CTX=&quot;$CTX_DIR/pyspark&quot;
+  mkdir -p &quot;$PYSPARK_CTX/kubernetes&quot;
+  cp -r &quot;resource-managers/kubernetes/docker/src/main/dockerfiles&quot; \
+    &quot;$PYSPARK_CTX/kubernetes/dockerfiles&quot;
+  mkdir &quot;$PYSPARK_CTX/python&quot;
+  cp -r &quot;python/lib&quot; &quot;$PYSPARK_CTX/python/lib&quot;
+
+  local R_CTX=&quot;$CTX_DIR/sparkr&quot;
+  mkdir -p &quot;$R_CTX/kubernetes&quot;
+  cp -r &quot;resource-managers/kubernetes/docker/src/main/dockerfiles&quot; \
+    &quot;$R_CTX/kubernetes/dockerfiles&quot;
+  cp -r &quot;R&quot; &quot;$R_CTX/R&quot;
+)}
&lt;p&gt;+&lt;br/&gt;
+function img_ctx_dir &lt;/p&gt;
{
+  if is_dev_build; then
+    echo &quot;$CTX_DIR/$1&quot;
+  else
+    echo &quot;$SPARK_HOME&quot;
+  fi
+}
&lt;p&gt;+&lt;br/&gt;
 function build {&lt;br/&gt;
   local BUILD_ARGS&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;local IMG_PATH&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;if [ ! -f &quot;$SPARK_HOME/RELEASE&quot; ]; then&lt;/li&gt;
	&lt;li&gt;# Set image build arguments accordingly if this is a source repo and not a distribution archive.&lt;/li&gt;
	&lt;li&gt;IMG_PATH=resource-managers/kubernetes/docker/src/main/dockerfiles&lt;/li&gt;
	&lt;li&gt;BUILD_ARGS=(&lt;/li&gt;
	&lt;li&gt;${BUILD_PARAMS}&lt;/li&gt;
	&lt;li&gt;--build-arg&lt;/li&gt;
	&lt;li&gt;img_path=$IMG_PATH&lt;/li&gt;
	&lt;li&gt;--build-arg&lt;/li&gt;
	&lt;li&gt;spark_jars=assembly/target/scala-$SPARK_SCALA_VERSION/jars&lt;/li&gt;
	&lt;li&gt;)&lt;/li&gt;
	&lt;li&gt;else&lt;/li&gt;
	&lt;li&gt;# Not passed as an argument to docker, but used to validate the Spark directory.&lt;/li&gt;
	&lt;li&gt;IMG_PATH=&quot;kubernetes/dockerfiles&quot;&lt;/li&gt;
	&lt;li&gt;BUILD_ARGS=(${BUILD_PARAMS})&lt;br/&gt;
+  local SPARK_ROOT=&quot;$SPARK_HOME&quot;&lt;br/&gt;
+&lt;br/&gt;
+  if is_dev_build; then&lt;br/&gt;
+    create_dev_build_context || error &quot;Failed to create docker build context.&quot;&lt;br/&gt;
+    SPARK_ROOT=&quot;$CTX_DIR/base&quot;&lt;br/&gt;
   fi&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if [ ! -d &quot;$IMG_PATH&quot; ]; then&lt;br/&gt;
+  # Verify that the Docker image content directory is present&lt;br/&gt;
+  if [ ! -d &quot;$SPARK_ROOT/kubernetes/dockerfiles&quot; ]; then&lt;br/&gt;
     error &quot;Cannot find docker image. This script must be run from a runnable distribution of Apache Spark.&quot;&lt;br/&gt;
   fi&lt;br/&gt;
+&lt;br/&gt;
+  # Verify that Spark has actually been built/is a runnable distribution&lt;br/&gt;
+  # i.e. the Spark JARs that the Docker files will place into the image are present&lt;br/&gt;
+  local TOTAL_JARS=$(ls $SPARK_ROOT/jars/spark-* | wc -l)&lt;br/&gt;
+  TOTAL_JARS=$(( $TOTAL_JARS ))&lt;br/&gt;
+  if [ &quot;${TOTAL_JARS}&quot; -eq 0 ]; then&lt;br/&gt;
+    error &quot;Cannot find Spark JARs. This script assumes that Apache Spark has first been built locally or this is a runnable distribution.&quot;&lt;br/&gt;
+  fi&lt;br/&gt;
+&lt;br/&gt;
+  local BUILD_ARGS=(${BUILD_PARAMS})&lt;br/&gt;
+&lt;br/&gt;
+  # If a custom SPARK_UID was set add it to build arguments&lt;br/&gt;
+  if [ -n &quot;$SPARK_UID&quot; ]; then&lt;br/&gt;
+    BUILD_ARGS+=(--build-arg spark_uid=$SPARK_UID)&lt;br/&gt;
+  fi&lt;br/&gt;
+&lt;br/&gt;
   local BINDING_BUILD_ARGS=(&lt;br/&gt;
     ${BUILD_PARAMS}&lt;br/&gt;
     --build-arg&lt;br/&gt;
     base_img=$(image_ref spark)&lt;br/&gt;
   )&lt;/li&gt;
	&lt;li&gt;local BASEDOCKERFILE=${BASEDOCKERFILE:-&quot;$IMG_PATH/spark/Dockerfile&quot;}&lt;/li&gt;
	&lt;li&gt;local PYDOCKERFILE=${PYDOCKERFILE:-&quot;$IMG_PATH/spark/bindings/python/Dockerfile&quot;}&lt;/li&gt;
	&lt;li&gt;local RDOCKERFILE=${RDOCKERFILE:-&quot;$IMG_PATH/spark/bindings/R/Dockerfile&quot;}&lt;br/&gt;
+  local BASEDOCKERFILE=${BASEDOCKERFILE:-&quot;kubernetes/dockerfiles/spark/Dockerfile&quot;}&lt;br/&gt;
+  local PYDOCKERFILE=${PYDOCKERFILE:-false}&lt;br/&gt;
+  local RDOCKERFILE=${RDOCKERFILE:-false}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;docker build $NOCACHEARG &quot;${BUILD_ARGS&lt;span class=&quot;error&quot;&gt;&amp;#91;@&amp;#93;&lt;/span&gt;}&quot; \&lt;br/&gt;
+  (cd $(img_ctx_dir base) &amp;amp;&amp;amp; docker build $NOCACHEARG &quot;${BUILD_ARGS&lt;span class=&quot;error&quot;&gt;&amp;#91;@&amp;#93;&lt;/span&gt;}&quot; \&lt;br/&gt;
     -t $(image_ref spark) \&lt;/li&gt;
	&lt;li&gt;-f &quot;$BASEDOCKERFILE&quot; .&lt;br/&gt;
+    -f &quot;$BASEDOCKERFILE&quot; .)&lt;br/&gt;
+  if [ $? -ne 0 ]; then&lt;br/&gt;
+    error &quot;Failed to build Spark JVM Docker image, please refer to Docker build output for details.&quot;&lt;br/&gt;
+  fi&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;docker build $NOCACHEARG &quot;${BINDING_BUILD_ARGS&lt;span class=&quot;error&quot;&gt;&amp;#91;@&amp;#93;&lt;/span&gt;}&quot; \&lt;/li&gt;
	&lt;li&gt;-t $(image_ref spark-py) \&lt;/li&gt;
	&lt;li&gt;-f &quot;$PYDOCKERFILE&quot; .&lt;br/&gt;
+  if [ &quot;${PYDOCKERFILE}&quot; != &quot;false&quot; ]; then&lt;br/&gt;
+    (cd $(img_ctx_dir pyspark) &amp;amp;&amp;amp; docker build $NOCACHEARG &quot;${BINDING_BUILD_ARGS&lt;span class=&quot;error&quot;&gt;&amp;#91;@&amp;#93;&lt;/span&gt;}&quot; \&lt;br/&gt;
+      -t $(image_ref spark-py) \&lt;br/&gt;
+      -f &quot;$PYDOCKERFILE&quot; .)&lt;br/&gt;
+      if [ $? -ne 0 ]; then&lt;br/&gt;
+        error &quot;Failed to build PySpark Docker image, please refer to Docker build output for details.&quot;&lt;br/&gt;
+      fi&lt;br/&gt;
+  fi&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;docker build $NOCACHEARG &quot;${BINDING_BUILD_ARGS&lt;span class=&quot;error&quot;&gt;&amp;#91;@&amp;#93;&lt;/span&gt;}&quot; \&lt;/li&gt;
	&lt;li&gt;-t $(image_ref spark-r) \&lt;/li&gt;
	&lt;li&gt;-f &quot;$RDOCKERFILE&quot; .&lt;br/&gt;
+  if [ &quot;${RDOCKERFILE}&quot; != &quot;false&quot; ]; then&lt;br/&gt;
+    (cd $(img_ctx_dir sparkr) &amp;amp;&amp;amp; docker build $NOCACHEARG &quot;${BINDING_BUILD_ARGS&lt;span class=&quot;error&quot;&gt;&amp;#91;@&amp;#93;&lt;/span&gt;}&quot; \&lt;br/&gt;
+      -t $(image_ref spark-r) \&lt;br/&gt;
+      -f &quot;$RDOCKERFILE&quot; .)&lt;br/&gt;
+    if [ $? -ne 0 ]; then&lt;br/&gt;
+      error &quot;Failed to build SparkR Docker image, please refer to Docker build output for details.&quot;&lt;br/&gt;
+    fi&lt;br/&gt;
+  fi&lt;br/&gt;
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; function push &lt;/p&gt;
{
-  docker push &quot;$(image_ref spark)&quot;
-  docker push &quot;$(image_ref spark-py)&quot;
-  docker push &quot;$(image_ref spark-r)&quot;
+  docker_push &quot;spark&quot;
+  docker_push &quot;spark-py&quot;
+  docker_push &quot;spark-r&quot;
 }

&lt;p&gt; function usage {&lt;br/&gt;
@@ -104,14 +205,18 @@ Commands:&lt;/p&gt;

&lt;p&gt; Options:&lt;br/&gt;
   -f file               Dockerfile to build for JVM based Jobs. By default builds the Dockerfile shipped with Spark.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;-p file               Dockerfile to build for PySpark Jobs. Builds Python dependencies and ships with Spark.&lt;/li&gt;
	&lt;li&gt;-R file               Dockerfile to build for SparkR Jobs. Builds R dependencies and ships with Spark.&lt;br/&gt;
+  -p file               (Optional) Dockerfile to build for PySpark Jobs. Builds Python dependencies and ships with Spark.&lt;br/&gt;
+                        Skips building PySpark docker image if not specified.&lt;br/&gt;
+  -R file               (Optional) Dockerfile to build for SparkR Jobs. Builds R dependencies and ships with Spark.&lt;br/&gt;
+                        Skips building SparkR docker image if not specified.&lt;br/&gt;
   -r repo               Repository address.&lt;br/&gt;
   -t tag                Tag to apply to the built image, or to identify the image to be pushed.&lt;br/&gt;
   -m                    Use minikube&apos;s Docker daemon.&lt;br/&gt;
   -n                    Build docker image with --no-cache&lt;/li&gt;
	&lt;li&gt;-b arg      Build arg to build or push the image. For multiple build args, this option needs to&lt;/li&gt;
	&lt;li&gt;be used separately for each build arg.&lt;br/&gt;
+  -u uid                UID to use in the USER directive to set the user the main Spark process runs as inside the&lt;br/&gt;
+                        resulting container&lt;br/&gt;
+  -b arg                Build arg to build or push the image. For multiple build args, this option needs to&lt;br/&gt;
+                        be used separately for each build arg.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; Using minikube when building images will do so directly into minikube&apos;s Docker daemon.&lt;br/&gt;
 There is no need to push the images into minikube in that case, they&apos;ll be automatically&lt;br/&gt;
@@ -125,6 +230,9 @@ Examples:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Build image in minikube with tag &quot;testing&quot;&lt;br/&gt;
     $0 -m -t testing build&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  - Build PySpark docker image&lt;br/&gt;
+    $0 -r docker.io/myrepo -t v2.3.0 -p kubernetes/dockerfiles/spark/bindings/python/Dockerfile build&lt;br/&gt;
+&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Build and push image with tag &quot;v2.3.0&quot; to docker.io/myrepo&lt;br/&gt;
     $0 -r docker.io/myrepo -t v2.3.0 build&lt;br/&gt;
     $0 -r docker.io/myrepo -t v2.3.0 push&lt;br/&gt;
@@ -143,7 +251,8 @@ PYDOCKERFILE=&lt;br/&gt;
 RDOCKERFILE=&lt;br/&gt;
 NOCACHEARG=&lt;br/&gt;
 BUILD_PARAMS=&lt;br/&gt;
-while getopts f&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/tongue.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;:R:mr:t:n:b: option&lt;br/&gt;
+SPARK_UID=&lt;br/&gt;
+while getopts f&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/tongue.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;:R:mr:t:nb:u: option&lt;br/&gt;
 do&lt;br/&gt;
  case &quot;${option}&quot;&lt;br/&gt;
  in&lt;br/&gt;
@@ -158,8 +267,12 @@ do&lt;br/&gt;
    if ! which minikube 1&amp;gt;/dev/null; then&lt;br/&gt;
      error &quot;Cannot find minikube.&quot;&lt;br/&gt;
    fi&lt;br/&gt;
+   if ! minikube status 1&amp;gt;/dev/null; then&lt;br/&gt;
+     error &quot;Cannot contact minikube. Make sure it&apos;s running.&quot;&lt;br/&gt;
+   fi&lt;br/&gt;
    eval $(minikube docker-env)&lt;br/&gt;
    ;;&lt;br/&gt;
+  u) SPARK_UID=${OPTARG};;&lt;br/&gt;
  esac&lt;br/&gt;
 done&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/bin/load-spark-env.sh b/bin/load-spark-env.sh&lt;br/&gt;
index 0b5006dbd63ac..0ada5d8d0fc1d 100644&lt;br/&gt;
&amp;#8212; a/bin/load-spark-env.sh&lt;br/&gt;
+++ b/bin/load-spark-env.sh&lt;br/&gt;
@@ -26,15 +26,17 @@ if [ -z &quot;${SPARK_HOME}&quot; ]; then&lt;br/&gt;
   source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home&lt;br/&gt;
 fi&lt;/p&gt;

&lt;p&gt;+SPARK_ENV_SH=&quot;spark-env.sh&quot;&lt;br/&gt;
 if [ -z &quot;$SPARK_ENV_LOADED&quot; ]; then&lt;br/&gt;
   export SPARK_ENV_LOADED=1&lt;/p&gt;

&lt;p&gt;   export SPARK_CONF_DIR=&quot;${SPARK_CONF_DIR:-&quot;${SPARK_HOME}&quot;/conf}&quot;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if [ -f &quot;${SPARK_CONF_DIR}/spark-env.sh&quot; ]; then&lt;br/&gt;
+  SPARK_ENV_SH=&quot;${SPARK_CONF_DIR}/${SPARK_ENV_SH}&quot;&lt;br/&gt;
+  if [[ -f &quot;${SPARK_ENV_SH}&quot; ]]; then&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;Promote all variable declarations to environment (exported) variables&lt;br/&gt;
     set -a&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;. &quot;${SPARK_CONF_DIR}/spark-env.sh&quot;&lt;br/&gt;
+    . ${SPARK_ENV_SH}&lt;br/&gt;
     set +a&lt;br/&gt;
   fi&lt;br/&gt;
 fi&lt;br/&gt;
@@ -42,19 +44,22 @@ fi&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;Setting SPARK_SCALA_VERSION if not already set.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; if [ -z &quot;$SPARK_SCALA_VERSION&quot; ]; then&lt;br/&gt;
+  SCALA_VERSION_1=2.12&lt;br/&gt;
+  SCALA_VERSION_2=2.11&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ASSEMBLY_DIR2=&quot;${SPARK_HOME}/assembly/target/scala-2.11&quot;&lt;/li&gt;
	&lt;li&gt;ASSEMBLY_DIR1=&quot;${SPARK_HOME}/assembly/target/scala-2.12&quot;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;if [[ -d &quot;$ASSEMBLY_DIR2&quot; &amp;amp;&amp;amp; -d &quot;$ASSEMBLY_DIR1&quot; ]]; then&lt;/li&gt;
	&lt;li&gt;echo -e &quot;Presence of build for multiple Scala versions detected.&quot; 1&amp;gt;&amp;amp;2&lt;/li&gt;
	&lt;li&gt;echo -e &apos;Either clean one of them or, export SPARK_SCALA_VERSION in spark-env.sh.&apos; 1&amp;gt;&amp;amp;2&lt;br/&gt;
+  ASSEMBLY_DIR_1=&quot;${SPARK_HOME}/assembly/target/scala-${SCALA_VERSION_1}&quot;&lt;br/&gt;
+  ASSEMBLY_DIR_2=&quot;${SPARK_HOME}/assembly/target/scala-${SCALA_VERSION_2}&quot;&lt;br/&gt;
+  ENV_VARIABLE_DOC=&quot;https://spark.apache.org/docs/latest/configuration.html#environment-variables&quot;&lt;br/&gt;
+  if [[ -d &quot;$ASSEMBLY_DIR_1&quot; &amp;amp;&amp;amp; -d &quot;$ASSEMBLY_DIR_2&quot; ]]; then&lt;br/&gt;
+    echo &quot;Presence of build for multiple Scala versions detected ($ASSEMBLY_DIR_1 and $ASSEMBLY_DIR_2).&quot; 1&amp;gt;&amp;amp;2&lt;br/&gt;
+    echo &quot;Remove one of them or, export SPARK_SCALA_VERSION=$SCALA_VERSION_1 in ${SPARK_ENV_SH}.&quot; 1&amp;gt;&amp;amp;2&lt;br/&gt;
+    echo &quot;Visit ${ENV_VARIABLE_DOC} for more details about setting environment variables in spark-env.sh.&quot; 1&amp;gt;&amp;amp;2&lt;br/&gt;
     exit 1&lt;br/&gt;
   fi&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if [ -d &quot;$ASSEMBLY_DIR2&quot; ]; then&lt;/li&gt;
	&lt;li&gt;export SPARK_SCALA_VERSION=&quot;2.11&quot;&lt;br/&gt;
+  if [[ -d &quot;$ASSEMBLY_DIR_1&quot; ]]; then&lt;br/&gt;
+    export SPARK_SCALA_VERSION=${SCALA_VERSION_1}&lt;br/&gt;
   else&lt;/li&gt;
	&lt;li&gt;export SPARK_SCALA_VERSION=&quot;2.12&quot;&lt;br/&gt;
+    export SPARK_SCALA_VERSION=${SCALA_VERSION_2}&lt;br/&gt;
   fi&lt;br/&gt;
 fi&lt;br/&gt;
diff --git a/bin/pyspark b/bin/pyspark&lt;br/&gt;
index 5d5affb1f97c3..1dcddcc6196b8 100755
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/bin/pyspark&lt;br/&gt;
+++ b/bin/pyspark&lt;br/&gt;
@@ -57,7 +57,7 @@ export PYSPARK_PYTHON&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;Add the PySpark classes to the Python path:&lt;br/&gt;
 export PYTHONPATH=&quot;${SPARK_HOME}/python/:$PYTHONPATH&quot;&lt;br/&gt;
-export PYTHONPATH=&quot;${SPARK_HOME}/python/lib/py4j-0.10.7-src.zip:$PYTHONPATH&quot;&lt;br/&gt;
+export PYTHONPATH=&quot;${SPARK_HOME}/python/lib/py4j-0.10.8.1-src.zip:$PYTHONPATH&quot;&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;Load the PySpark shell.py script when ./pyspark is used interactively:&lt;br/&gt;
 export OLD_PYTHONSTARTUP=&quot;$PYTHONSTARTUP&quot;&lt;br/&gt;
diff --git a/bin/pyspark2.cmd b/bin/pyspark2.cmd&lt;br/&gt;
index 15fa910c277b3..479fd464c7d3e 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/bin/pyspark2.cmd&lt;br/&gt;
+++ b/bin/pyspark2.cmd&lt;br/&gt;
@@ -30,7 +30,7 @@ if &quot;x%PYSPARK_DRIVER_PYTHON%&quot;==&quot;x&quot; (&lt;br/&gt;
 )&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; set PYTHONPATH=%SPARK_HOME%\python;%PYTHONPATH%&lt;br/&gt;
-set PYTHONPATH=%SPARK_HOME%\python\lib\py4j-0.10.7-src.zip;%PYTHONPATH%&lt;br/&gt;
+set PYTHONPATH=%SPARK_HOME%\python\lib\py4j-0.10.8.1-src.zip;%PYTHONPATH%&lt;/p&gt;

&lt;p&gt; set OLD_PYTHONSTARTUP=%PYTHONSTARTUP%&lt;br/&gt;
 set PYTHONSTARTUP=%SPARK_HOME%\python\pyspark\shell.py&lt;br/&gt;
diff --git a/bin/spark-shell b/bin/spark-shell&lt;br/&gt;
index 421f36cac3d47..e920137974980 100755&lt;br/&gt;
&amp;#8212; a/bin/spark-shell&lt;br/&gt;
+++ b/bin/spark-shell&lt;br/&gt;
@@ -32,7 +32,10 @@ if [ -z &quot;${SPARK_HOME}&quot; ]; then&lt;br/&gt;
   source &quot;$(dirname &quot;$0&quot;)&quot;/find-spark-home&lt;br/&gt;
 fi&lt;/p&gt;

&lt;p&gt;-export _SPARK_CMD_USAGE=&quot;Usage: ./bin/spark-shell &lt;span class=&quot;error&quot;&gt;&amp;#91;options&amp;#93;&lt;/span&gt;&quot;&lt;br/&gt;
+export _SPARK_CMD_USAGE=&quot;Usage: ./bin/spark-shell &lt;span class=&quot;error&quot;&gt;&amp;#91;options&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+Scala REPL options:&lt;br/&gt;
+  -I &amp;lt;file&amp;gt;                   preload &amp;lt;file&amp;gt;, enforcing line-by-line interpretation&quot;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4161&quot; title=&quot;Spark shell class path is not correctly set if &amp;quot;spark.driver.extraClassPath&amp;quot; is set in defaults.conf&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4161&quot;&gt;&lt;del&gt;SPARK-4161&lt;/del&gt;&lt;/a&gt;: scala does not assume use of the java classpath,&lt;/li&gt;
	&lt;li&gt;so we need to add the &quot;-Dscala.usejavacp=true&quot; flag manually. We&lt;br/&gt;
diff --git a/bin/spark-shell2.cmd b/bin/spark-shell2.cmd&lt;br/&gt;
index aaf71906c6526..549bf43bb6078 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/bin/spark-shell2.cmd&lt;br/&gt;
+++ b/bin/spark-shell2.cmd&lt;br/&gt;
@@ -20,7 +20,13 @@ rem&lt;br/&gt;
 rem Figure out where the Spark framework is installed&lt;br/&gt;
 call &quot;%~dp0find-spark-home.cmd&quot;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;-set _SPARK_CMD_USAGE=Usage: .\bin\spark-shell.cmd &lt;span class=&quot;error&quot;&gt;&amp;#91;options&amp;#93;&lt;/span&gt;&lt;br/&gt;
+set LF=^&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
+rem two empty lines are required&lt;br/&gt;
+set _SPARK_CMD_USAGE=Usage: .\bin\spark-shell.cmd &lt;span class=&quot;error&quot;&gt;&amp;#91;options&amp;#93;&lt;/span&gt;&lt;sup&gt;%LF%%LF%&lt;/sup&gt;%LF%%LF%^&lt;br/&gt;
+Scala REPL options:&lt;sup&gt;%LF%%LF%&lt;/sup&gt;&lt;br/&gt;
+  -I &lt;sup&gt;&amp;lt;file&lt;/sup&gt;&amp;gt;                   preload &lt;sup&gt;&amp;lt;file&lt;/sup&gt;&amp;gt;, enforcing line-by-line interpretation&lt;/p&gt;

&lt;p&gt; rem &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4161&quot; title=&quot;Spark shell class path is not correctly set if &amp;quot;spark.driver.extraClassPath&amp;quot; is set in defaults.conf&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4161&quot;&gt;&lt;del&gt;SPARK-4161&lt;/del&gt;&lt;/a&gt;: scala does not assume use of the java classpath,&lt;br/&gt;
 rem so we need to add the &quot;-Dscala.usejavacp=true&quot; flag manually. We&lt;br/&gt;
diff --git a/build/mvn b/build/mvn&lt;br/&gt;
index 2487b81abb4ea..4cb10e0d03fa4 100755&lt;br/&gt;
&amp;#8212; a/build/mvn&lt;br/&gt;
+++ b/build/mvn&lt;br/&gt;
@@ -116,7 +116,8 @@ install_zinc() {&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;the build/ folder&lt;br/&gt;
 install_scala() {&lt;/li&gt;
	&lt;li&gt;determine the Scala version used in Spark&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;local scala_version=`grep &quot;scala.version&quot; &quot;${_DIR}/../pom.xml&quot; | head -n1 | awk -F &apos;&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;lt;&amp;gt;&amp;#93;&lt;/span&gt;&apos; &apos;
{print $3}&apos;`&lt;br/&gt;
+  local scala_binary_version=`grep &quot;scala.binary.version&quot; &quot;${_DIR}/../pom.xml&quot; | head -n1 | awk -F &apos;&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;lt;&amp;gt;&amp;#93;&lt;/span&gt;&apos; &apos;{print $3}
&lt;p&gt;&apos;`&lt;br/&gt;
+  local scala_version=`grep &quot;scala.version&quot; &quot;${_DIR}/../pom.xml&quot; | grep ${scala_binary_version} | head -n1 | awk -F &apos;&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;lt;&amp;gt;&amp;#93;&lt;/span&gt;&apos; &apos;&lt;/p&gt;
{print $3}
&lt;p&gt;&apos;`&lt;br/&gt;
   local scala_bin=&quot;${_DIR}/scala-${scala_version}/bin/scala&quot;&lt;br/&gt;
   local TYPESAFE_MIRROR=${TYPESAFE_MIRROR:-&lt;a href=&quot;https://downloads.lightbend.com&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://downloads.lightbend.com&lt;/a&gt;}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -153,6 +154,7 @@ if [ -n &quot;${ZINC_INSTALL_FLAG}&quot; -o -z &quot;`&quot;${ZINC_BIN}&quot; -status -port ${ZINC_PORT}`&lt;br/&gt;
   export ZINC_OPTS=${ZINC_OPTS:-&quot;$_COMPILE_JVM_OPTS&quot;}&lt;br/&gt;
   &quot;${ZINC_BIN}&quot; -shutdown -port ${ZINC_PORT}&lt;br/&gt;
   &quot;${ZINC_BIN}&quot; -start -port ${ZINC_PORT} \&lt;br/&gt;
+    -server 127.0.0.1 -idle-timeout 3h \&lt;br/&gt;
     -scala-compiler &quot;${SCALA_COMPILER}&quot; \&lt;br/&gt;
     -scala-library &quot;${SCALA_LIBRARY}&quot; &amp;amp;&amp;gt;/dev/null&lt;br/&gt;
 fi&lt;br/&gt;
@@ &lt;del&gt;162,5 +164,12 @@ export MAVEN_OPTS=${MAVEN_OPTS:&lt;/del&gt;&quot;$_COMPILE_JVM_OPTS&quot;}&lt;/p&gt;

&lt;p&gt; echo &quot;Using \`mvn\` from path: $MVN_BIN&quot; 1&amp;gt;&amp;amp;2&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Last, call the `mvn` command as usual&lt;br/&gt;
+# call the `mvn` command as usual&lt;br/&gt;
+# &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25854&quot; title=&quot;mvn helper script always exits w/1, causing mvn builds to fail&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25854&quot;&gt;&lt;del&gt;SPARK-25854&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
 &quot;${MVN_BIN}&quot; -DzincPort=${ZINC_PORT} &quot;$@&quot;&lt;br/&gt;
+MVN_RETCODE=$?&lt;br/&gt;
+&lt;br/&gt;
+# Try to shut down zinc explicitly if the server is still running.&lt;br/&gt;
+&quot;${ZINC_BIN}&quot; -shutdown -port ${ZINC_PORT}&lt;br/&gt;
+&lt;br/&gt;
+exit $MVN_RETCODE&lt;br/&gt;
diff --git a/common/kvstore/pom.xml b/common/kvstore/pom.xml&lt;br/&gt;
index 8c148359c3029..f042a12fda3d2 100644
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/kvstore/pom.xml&lt;br/&gt;
+++ b/common/kvstore/pom.xml&lt;br/&gt;
@@ -21,12 +21,12 @@&lt;br/&gt;
   &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&lt;br/&gt;
   &amp;lt;parent&amp;gt;&lt;br/&gt;
     &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-parent_2.11&amp;lt;/artifactId&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;version&amp;gt;2.4.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
+    &amp;lt;artifactId&amp;gt;spark-parent_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+    &amp;lt;version&amp;gt;3.0.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
     &amp;lt;relativePath&amp;gt;../../pom.xml&amp;lt;/relativePath&amp;gt;&lt;br/&gt;
   &amp;lt;/parent&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-kvstore_2.11&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+  &amp;lt;artifactId&amp;gt;spark-kvstore_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
   &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;&lt;br/&gt;
   &amp;lt;name&amp;gt;Spark Project Local DB&amp;lt;/name&amp;gt;&lt;br/&gt;
   &amp;lt;url&amp;gt;&lt;a href=&quot;http://spark.apache.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/&lt;/a&gt;&amp;lt;/url&amp;gt;&lt;br/&gt;
diff --git a/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreSerializer.java b/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreSerializer.java&lt;br/&gt;
index bd8d9486acde5..771a9541bb349 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreSerializer.java&lt;br/&gt;
+++ b/common/kvstore/src/main/java/org/apache/spark/util/kvstore/KVStoreSerializer.java&lt;br/&gt;
@@ -54,11 +54,8 @@ public KVStoreSerializer() 
{
       return ((String) o).getBytes(UTF_8);
     }
&lt;p&gt; else {&lt;br/&gt;
       ByteArrayOutputStream bytes = new ByteArrayOutputStream();&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;GZIPOutputStream out = new GZIPOutputStream(bytes);&lt;/li&gt;
	&lt;li&gt;try 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+      try (GZIPOutputStream out = new GZIPOutputStream(bytes)) {
         mapper.writeValue(out, o);
-      } finally {
-        out.close();
       }       return bytes.toByteArray();     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -69,11 +66,8 @@ public KVStoreSerializer() {&lt;br/&gt;
     if (klass.equals(String.class)) &lt;/p&gt;
{
       return (T) new String(data, UTF_8);
     }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;GZIPInputStream in = new GZIPInputStream(new ByteArrayInputStream(data));&lt;/li&gt;
	&lt;li&gt;try 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+      try (GZIPInputStream in = new GZIPInputStream(new ByteArrayInputStream(data))) {
         return mapper.readValue(in, klass);
-      } finally {
-        in.close();
       }     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;   }&lt;br/&gt;
diff --git a/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java b/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java&lt;br/&gt;
index f62e85d435318..e3efc92c4a54a 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java&lt;br/&gt;
+++ b/common/kvstore/src/main/java/org/apache/spark/util/kvstore/LevelDBIterator.java&lt;br/&gt;
@@ -196,6 +196,7 @@ public synchronized void close() throws IOException {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;when Scala wrappers are used, this makes sure that, hopefully, the JNI resources held by&lt;/li&gt;
	&lt;li&gt;the iterator will eventually be released.&lt;br/&gt;
    */&lt;br/&gt;
+  @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
   @Override&lt;br/&gt;
   protected void finalize() throws Throwable {&lt;br/&gt;
     db.closeIterator(this);&lt;br/&gt;
diff --git a/common/kvstore/src/test/java/org/apache/spark/util/kvstore/LevelDBSuite.java b/common/kvstore/src/test/java/org/apache/spark/util/kvstore/LevelDBSuite.java&lt;br/&gt;
index 205f7df87c5bc..39a952f2b0df9 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/kvstore/src/test/java/org/apache/spark/util/kvstore/LevelDBSuite.java&lt;br/&gt;
+++ b/common/kvstore/src/test/java/org/apache/spark/util/kvstore/LevelDBSuite.java&lt;br/&gt;
@@ -217,7 +217,7 @@ public void testSkip() throws Exception {&lt;br/&gt;
   public void testNegativeIndexValues() throws Exception {&lt;br/&gt;
     List&amp;lt;Integer&amp;gt; expected = Arrays.asList(-100, -50, 0, 50, 100);&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;expected.stream().forEach(i -&amp;gt; {&lt;br/&gt;
+    expected.forEach(i -&amp;gt; {&lt;br/&gt;
       try 
{
         db.write(createCustomType1(i));
       }
&lt;p&gt; catch (Exception e) {&lt;br/&gt;
diff --git a/common/network-common/pom.xml b/common/network-common/pom.xml&lt;br/&gt;
index 8ca7733507f1b..56d01fa0e8b3d 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/network-common/pom.xml&lt;br/&gt;
+++ b/common/network-common/pom.xml&lt;br/&gt;
@@ -21,12 +21,12 @@&lt;br/&gt;
   &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&lt;br/&gt;
   &amp;lt;parent&amp;gt;&lt;br/&gt;
     &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-parent_2.11&amp;lt;/artifactId&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;version&amp;gt;2.4.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
+    &amp;lt;artifactId&amp;gt;spark-parent_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+    &amp;lt;version&amp;gt;3.0.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
     &amp;lt;relativePath&amp;gt;../../pom.xml&amp;lt;/relativePath&amp;gt;&lt;br/&gt;
   &amp;lt;/parent&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-network-common_2.11&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+  &amp;lt;artifactId&amp;gt;spark-network-common_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
   &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;&lt;br/&gt;
   &amp;lt;name&amp;gt;Spark Project Networking&amp;lt;/name&amp;gt;&lt;br/&gt;
   &amp;lt;url&amp;gt;&lt;a href=&quot;http://spark.apache.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/&lt;/a&gt;&amp;lt;/url&amp;gt;&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/TransportContext.java b/common/network-common/src/main/java/org/apache/spark/network/TransportContext.java&lt;br/&gt;
index ae91bc9cfdd08..480b52652de53 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/network-common/src/main/java/org/apache/spark/network/TransportContext.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/TransportContext.java&lt;br/&gt;
@@ -21,6 +21,8 @@&lt;br/&gt;
 import java.util.List;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import io.netty.channel.Channel;&lt;br/&gt;
+import io.netty.channel.ChannelPipeline;&lt;br/&gt;
+import io.netty.channel.EventLoopGroup;&lt;br/&gt;
 import io.netty.channel.socket.SocketChannel;&lt;br/&gt;
 import io.netty.handler.timeout.IdleStateHandler;&lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
@@ -32,11 +34,13 @@&lt;br/&gt;
 import org.apache.spark.network.client.TransportResponseHandler;&lt;br/&gt;
 import org.apache.spark.network.protocol.MessageDecoder;&lt;br/&gt;
 import org.apache.spark.network.protocol.MessageEncoder;&lt;br/&gt;
+import org.apache.spark.network.server.ChunkFetchRequestHandler;&lt;br/&gt;
 import org.apache.spark.network.server.RpcHandler;&lt;br/&gt;
 import org.apache.spark.network.server.TransportChannelHandler;&lt;br/&gt;
 import org.apache.spark.network.server.TransportRequestHandler;&lt;br/&gt;
 import org.apache.spark.network.server.TransportServer;&lt;br/&gt;
 import org.apache.spark.network.server.TransportServerBootstrap;&lt;br/&gt;
+import org.apache.spark.network.util.IOMode;&lt;br/&gt;
 import org.apache.spark.network.util.NettyUtils;&lt;br/&gt;
 import org.apache.spark.network.util.TransportConf;&lt;br/&gt;
 import org.apache.spark.network.util.TransportFrameDecoder;&lt;br/&gt;
@@ -61,6 +65,7 @@&lt;br/&gt;
   private final TransportConf conf;&lt;br/&gt;
   private final RpcHandler rpcHandler;&lt;br/&gt;
   private final boolean closeIdleConnections;&lt;br/&gt;
+  private final boolean isClientOnly;&lt;/p&gt;

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Force to create MessageEncoder and MessageDecoder so that we can make sure they will be created&lt;br/&gt;
@@ -77,17 +82,54 @@&lt;br/&gt;
   private static final MessageEncoder ENCODER = MessageEncoder.INSTANCE;&lt;br/&gt;
   private static final MessageDecoder DECODER = MessageDecoder.INSTANCE;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  // Separate thread pool for handling ChunkFetchRequest. This helps to enable throttling&lt;br/&gt;
+  // max number of TransportServer worker threads that are blocked on writing response&lt;br/&gt;
+  // of ChunkFetchRequest message back to the client via the underlying channel.&lt;br/&gt;
+  private static EventLoopGroup chunkFetchWorkers;&lt;br/&gt;
+&lt;br/&gt;
   public TransportContext(TransportConf conf, RpcHandler rpcHandler) &lt;/p&gt;
{
-    this(conf, rpcHandler, false);
+    this(conf, rpcHandler, false, false);
   }

&lt;p&gt;   public TransportContext(&lt;br/&gt;
       TransportConf conf,&lt;br/&gt;
       RpcHandler rpcHandler,&lt;br/&gt;
       boolean closeIdleConnections) &lt;/p&gt;
{
+    this(conf, rpcHandler, closeIdleConnections, false);
+  }
&lt;p&gt;+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Enables TransportContext initialization for underlying client and server.&lt;br/&gt;
+   *&lt;br/&gt;
+   * @param conf TransportConf&lt;br/&gt;
+   * @param rpcHandler RpcHandler responsible for handling requests and responses.&lt;br/&gt;
+   * @param closeIdleConnections Close idle connections if it is set to true.&lt;br/&gt;
+   * @param isClientOnly This config indicates the TransportContext is only used by a client.&lt;br/&gt;
+   *                     This config is more important when external shuffle is enabled.&lt;br/&gt;
+   *                     It stops creating extra event loop and subsequent thread pool&lt;br/&gt;
+   *                     for shuffle clients to handle chunked fetch requests.&lt;br/&gt;
+   */&lt;br/&gt;
+  public TransportContext(&lt;br/&gt;
+      TransportConf conf,&lt;br/&gt;
+      RpcHandler rpcHandler,&lt;br/&gt;
+      boolean closeIdleConnections,&lt;br/&gt;
+      boolean isClientOnly) {&lt;br/&gt;
     this.conf = conf;&lt;br/&gt;
     this.rpcHandler = rpcHandler;&lt;br/&gt;
     this.closeIdleConnections = closeIdleConnections;&lt;br/&gt;
+    this.isClientOnly = isClientOnly;&lt;br/&gt;
+&lt;br/&gt;
+    synchronized(TransportContext.class) {&lt;br/&gt;
+      if (chunkFetchWorkers == null &amp;amp;&amp;amp;&lt;br/&gt;
+          conf.getModuleName() != null &amp;amp;&amp;amp;&lt;br/&gt;
+          conf.getModuleName().equalsIgnoreCase(&quot;shuffle&quot;) &amp;amp;&amp;amp;&lt;br/&gt;
+          !isClientOnly) &lt;/p&gt;
{
+        chunkFetchWorkers = NettyUtils.createEventLoop(
+            IOMode.valueOf(conf.ioMode()),
+            conf.chunkFetchHandlerThreads(),
+            &quot;shuffle-chunk-fetch-handler&quot;);
+      }
&lt;p&gt;+    }&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   /**&lt;br/&gt;
@@ -144,14 +186,23 @@ public TransportChannelHandler initializePipeline(&lt;br/&gt;
       RpcHandler channelRpcHandler) {&lt;br/&gt;
     try {&lt;br/&gt;
       TransportChannelHandler channelHandler = createChannelHandler(channel, channelRpcHandler);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;channel.pipeline()&lt;br/&gt;
+      ChunkFetchRequestHandler chunkFetchHandler =&lt;br/&gt;
+        createChunkFetchHandler(channelHandler, channelRpcHandler);&lt;br/&gt;
+      ChannelPipeline pipeline = channel.pipeline()&lt;br/&gt;
         .addLast(&quot;encoder&quot;, ENCODER)&lt;br/&gt;
         .addLast(TransportFrameDecoder.HANDLER_NAME, NettyUtils.createFrameDecoder())&lt;br/&gt;
         .addLast(&quot;decoder&quot;, DECODER)&lt;/li&gt;
	&lt;li&gt;.addLast(&quot;idleStateHandler&quot;, new IdleStateHandler(0, 0, conf.connectionTimeoutMs() / 1000))&lt;br/&gt;
+        .addLast(&quot;idleStateHandler&quot;,&lt;br/&gt;
+          new IdleStateHandler(0, 0, conf.connectionTimeoutMs() / 1000))&lt;br/&gt;
         // NOTE: Chunks are currently guaranteed to be returned in the order of request, but this&lt;br/&gt;
         // would require more logic to guarantee if this were not part of the same event loop.&lt;br/&gt;
         .addLast(&quot;handler&quot;, channelHandler);&lt;br/&gt;
+      // Use a separate EventLoopGroup to handle ChunkFetchRequest messages for shuffle rpcs.&lt;br/&gt;
+      if (conf.getModuleName() != null &amp;amp;&amp;amp;&lt;br/&gt;
+          conf.getModuleName().equalsIgnoreCase(&quot;shuffle&quot;)&lt;br/&gt;
+          &amp;amp;&amp;amp; !isClientOnly) 
{
+        pipeline.addLast(chunkFetchWorkers, &quot;chunkFetchHandler&quot;, chunkFetchHandler);
+      }
&lt;p&gt;       return channelHandler;&lt;br/&gt;
     } catch (RuntimeException e) &lt;/p&gt;
{
       logger.error(&quot;Error while initializing Netty pipeline&quot;, e);
@@ -173,5 +224,14 @@ private TransportChannelHandler createChannelHandler(Channel channel, RpcHandler
       conf.connectionTimeoutMs(), closeIdleConnections);
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  /**&lt;br/&gt;
+   * Creates the dedicated ChannelHandler for ChunkFetchRequest messages.&lt;br/&gt;
+   */&lt;br/&gt;
+  private ChunkFetchRequestHandler createChunkFetchHandler(TransportChannelHandler channelHandler,&lt;br/&gt;
+      RpcHandler rpcHandler) &lt;/p&gt;
{
+    return new ChunkFetchRequestHandler(channelHandler.getClient(),
+      rpcHandler.getStreamManager(), conf.maxChunksBeingTransferred());
+  }
&lt;p&gt;+&lt;br/&gt;
   public TransportConf getConf() &lt;/p&gt;
{ return conf; }
&lt;p&gt; }&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/buffer/ManagedBuffer.java b/common/network-common/src/main/java/org/apache/spark/network/buffer/ManagedBuffer.java&lt;br/&gt;
index 1861f8d7fd8f3..2d573f512437e 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/buffer/ManagedBuffer.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/buffer/ManagedBuffer.java&lt;br/&gt;
@@ -36,7 +36,10 @@&lt;br/&gt;
  */&lt;br/&gt;
 public abstract class ManagedBuffer {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/** Number of bytes of the data. */&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Number of bytes of the data. If this buffer will decrypt for all of the views into the data,&lt;br/&gt;
+   * this is the size of the decrypted data.&lt;br/&gt;
+   */&lt;br/&gt;
   public abstract long size();&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java b/common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java&lt;br/&gt;
index 056505ef53356..64fdb32a67ada 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/crypto/AuthEngine.java&lt;br/&gt;
@@ -159,15 +159,21 @@ public void close() throws IOException {&lt;br/&gt;
     // accurately report the errors when they happen.&lt;br/&gt;
     RuntimeException error = null;&lt;br/&gt;
     byte[] dummy = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;8&amp;#93;&lt;/span&gt;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;try 
{
-      doCipherOp(encryptor, dummy, true);
-    }
&lt;p&gt; catch (Exception e) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;error = new RuntimeException(e);&lt;br/&gt;
+    if (encryptor != null) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+      try {
+        doCipherOp(Cipher.ENCRYPT_MODE, dummy, true);
+      } catch (Exception e) {
+        error = new RuntimeException(e);
+      }&lt;br/&gt;
+      encryptor = null;&lt;br/&gt;
     }&lt;br/&gt;
-    try {
-      doCipherOp(decryptor, dummy, true);
-    } catch (Exception e) {&lt;br/&gt;
-      error = new RuntimeException(e);&lt;br/&gt;
+    if (decryptor != null) {&lt;br/&gt;
+      try {
+        doCipherOp(Cipher.DECRYPT_MODE, dummy, true);
+      } catch (Exception e) {+        error = new RuntimeException(e);+      }+      decryptor = null;     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;     random.close();&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -189,11 +195,11 @@ public void close() throws IOException {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   private byte[] decrypt(byte[] in) throws GeneralSecurityException &lt;/p&gt;
{
-    return doCipherOp(decryptor, in, false);
+    return doCipherOp(Cipher.DECRYPT_MODE, in, false);
   }

&lt;p&gt;   private byte[] encrypt(byte[] in) throws GeneralSecurityException &lt;/p&gt;
{
-    return doCipherOp(encryptor, in, false);
+    return doCipherOp(Cipher.ENCRYPT_MODE, in, false);
   }

&lt;p&gt;   private void initializeForAuth(String cipher, byte[] nonce, SecretKeySpec key)&lt;br/&gt;
@@ -205,11 +211,13 @@ private void initializeForAuth(String cipher, byte[] nonce, SecretKeySpec key)&lt;br/&gt;
     byte[] iv = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;conf.ivLength()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
     System.arraycopy(nonce, 0, iv, 0, Math.min(nonce.length, iv.length));&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;encryptor = CryptoCipherFactory.getCryptoCipher(cipher, cryptoConf);&lt;/li&gt;
	&lt;li&gt;encryptor.init(Cipher.ENCRYPT_MODE, key, new IvParameterSpec(iv));&lt;br/&gt;
+    CryptoCipher _encryptor = CryptoCipherFactory.getCryptoCipher(cipher, cryptoConf);&lt;br/&gt;
+    _encryptor.init(Cipher.ENCRYPT_MODE, key, new IvParameterSpec(iv));&lt;br/&gt;
+    this.encryptor = _encryptor;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;decryptor = CryptoCipherFactory.getCryptoCipher(cipher, cryptoConf);&lt;/li&gt;
	&lt;li&gt;decryptor.init(Cipher.DECRYPT_MODE, key, new IvParameterSpec(iv));&lt;br/&gt;
+    CryptoCipher _decryptor = CryptoCipherFactory.getCryptoCipher(cipher, cryptoConf);&lt;br/&gt;
+    _decryptor.init(Cipher.DECRYPT_MODE, key, new IvParameterSpec(iv));&lt;br/&gt;
+    this.decryptor = _decryptor;&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
@@ -241,29 +249,52 @@ private SecretKeySpec generateKey(String kdf, int iterations, byte[] salt, int k&lt;br/&gt;
     return new SecretKeySpec(key.getEncoded(), conf.keyAlgorithm());&lt;br/&gt;
   }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private byte[] doCipherOp(CryptoCipher cipher, byte[] in, boolean isFinal)&lt;br/&gt;
+  private byte[] doCipherOp(int mode, byte[] in, boolean isFinal)&lt;br/&gt;
     throws GeneralSecurityException {&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Preconditions.checkState(cipher != null);&lt;br/&gt;
+    CryptoCipher cipher;&lt;br/&gt;
+    switch (mode) 
{
+      case Cipher.ENCRYPT_MODE:
+        cipher = encryptor;
+        break;
+      case Cipher.DECRYPT_MODE:
+        cipher = decryptor;
+        break;
+      default:
+        throw new IllegalArgumentException(String.valueOf(mode));
+    }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;int scale = 1;&lt;/li&gt;
	&lt;li&gt;while (true) {&lt;/li&gt;
	&lt;li&gt;int size = in.length * scale;&lt;/li&gt;
	&lt;li&gt;byte[] buffer = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;size&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;try {&lt;/li&gt;
	&lt;li&gt;int outSize = isFinal ? cipher.doFinal(in, 0, in.length, buffer, 0)&lt;/li&gt;
	&lt;li&gt;: cipher.update(in, 0, in.length, buffer, 0);&lt;/li&gt;
	&lt;li&gt;if (outSize != buffer.length) 
{
-          byte[] output = new byte[outSize];
-          System.arraycopy(buffer, 0, output, 0, output.length);
-          return output;
-        }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;return buffer;&lt;br/&gt;
+    Preconditions.checkState(cipher != null, &quot;Cipher is invalid because of previous error.&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    try {&lt;br/&gt;
+      int scale = 1;&lt;br/&gt;
+      while (true) {&lt;br/&gt;
+        int size = in.length * scale;&lt;br/&gt;
+        byte[] buffer = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;size&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        try 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+          int outSize = isFinal ? cipher.doFinal(in, 0, in.length, buffer, 0)+            }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; catch (ShortBufferException e) &lt;/p&gt;
{
+          // Try again with a bigger buffer.
+          scale *= 2;
         }&lt;/li&gt;
	&lt;li&gt;} catch (ShortBufferException e) 
{
-        // Try again with a bigger buffer.
-        scale *= 2;
       }
&lt;p&gt;+    } catch (InternalError ie) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+      // SPARK-25535. The commons-cryto library will throw InternalError if something goes wrong,+      // and leave bad state behind in the Java wrappers, so it&amp;#39;s not safe to use them afterwards.+      if (mode == Cipher.ENCRYPT_MODE) {
+        this.encryptor = null;
+      } else {
+        this.decryptor = null;
+      }+      throw ie;     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/common/network-common/src/main/java/org/apache/spark/network/crypto/TransportCipher.java b/common/network-common/src/main/java/org/apache/spark/network/crypto/TransportCipher.java&lt;br/&gt;
index b64e4b7a970b5..2745052265f7f 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/crypto/TransportCipher.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/crypto/TransportCipher.java&lt;br/&gt;
@@ -107,45 +107,72 @@ public void addToChannel(Channel ch) throws IOException {&lt;br/&gt;
   private static class EncryptionHandler extends ChannelOutboundHandlerAdapter {&lt;br/&gt;
     private final ByteArrayWritableChannel byteChannel;&lt;br/&gt;
     private final CryptoOutputStream cos;&lt;br/&gt;
+    private boolean isCipherValid;&lt;/p&gt;

&lt;p&gt;     EncryptionHandler(TransportCipher cipher) throws IOException &lt;/p&gt;
{
       byteChannel = new ByteArrayWritableChannel(STREAM_BUFFER_SIZE);
       cos = cipher.createOutputStream(byteChannel);
+      isCipherValid = true;
     }

&lt;p&gt;     @Override&lt;br/&gt;
     public void write(ChannelHandlerContext ctx, Object msg, ChannelPromise promise)&lt;br/&gt;
       throws Exception &lt;/p&gt;
{
-      ctx.write(new EncryptedMessage(cos, msg, byteChannel), promise);
+      ctx.write(new EncryptedMessage(this, cos, msg, byteChannel), promise);
     }

&lt;p&gt;     @Override&lt;br/&gt;
     public void close(ChannelHandlerContext ctx, ChannelPromise promise) throws Exception {&lt;br/&gt;
       try {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cos.close();&lt;br/&gt;
+        if (isCipherValid) 
{
+          cos.close();
+        }
&lt;p&gt;       } finally &lt;/p&gt;
{
         super.close(ctx, promise);
       }
&lt;p&gt;     }&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25535&quot; title=&quot;Work around bad error checking in commons-crypto&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25535&quot;&gt;&lt;del&gt;SPARK-25535&lt;/del&gt;&lt;/a&gt;. Workaround for &lt;a href=&quot;https://issues.apache.org/jira/browse/CRYPTO-141&quot; title=&quot;Errors in native code can leave Java wrappers in bad state&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CRYPTO-141&quot;&gt;&lt;del&gt;CRYPTO-141&lt;/del&gt;&lt;/a&gt;. Avoid further interaction with the underlying cipher&lt;br/&gt;
+     * after an error occurs.&lt;br/&gt;
+     */&lt;br/&gt;
+    void reportError() &lt;/p&gt;
{
+      this.isCipherValid = false;
+    }
&lt;p&gt;+&lt;br/&gt;
+    boolean isCipherValid() &lt;/p&gt;
{
+      return isCipherValid;
+    }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private static class DecryptionHandler extends ChannelInboundHandlerAdapter {&lt;br/&gt;
     private final CryptoInputStream cis;&lt;br/&gt;
     private final ByteArrayReadableChannel byteChannel;&lt;br/&gt;
+    private boolean isCipherValid;&lt;/p&gt;

&lt;p&gt;     DecryptionHandler(TransportCipher cipher) throws IOException &lt;/p&gt;
{
       byteChannel = new ByteArrayReadableChannel();
       cis = cipher.createInputStream(byteChannel);
+      isCipherValid = true;
     }

&lt;p&gt;     @Override&lt;br/&gt;
     public void channelRead(ChannelHandlerContext ctx, Object data) throws Exception {&lt;br/&gt;
+      if (!isCipherValid) &lt;/p&gt;
{
+        throw new IOException(&quot;Cipher is in invalid state.&quot;);
+      }&lt;br/&gt;
       byteChannel.feedData((ByteBuf) data);&lt;br/&gt;
 &lt;br/&gt;
       byte[] decryptedData = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;byteChannel.readableBytes()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
       int offset = 0;&lt;br/&gt;
       while (offset &amp;lt; decryptedData.length) {&lt;br/&gt;
-        offset += cis.read(decryptedData, offset, decryptedData.length - offset);&lt;br/&gt;
+        // &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25535&quot; title=&quot;Work around bad error checking in commons-crypto&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25535&quot;&gt;&lt;del&gt;SPARK-25535&lt;/del&gt;&lt;/a&gt;: workaround for &lt;a href=&quot;https://issues.apache.org/jira/browse/CRYPTO-141&quot; title=&quot;Errors in native code can leave Java wrappers in bad state&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CRYPTO-141&quot;&gt;&lt;del&gt;CRYPTO-141&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
+        try {
+          offset += cis.read(decryptedData, offset, decryptedData.length - offset);
+        } catch (InternalError ie) {
+          isCipherValid = false;
+          throw ie;
+        }&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
       ctx.fireChannelRead(Unpooled.wrappedBuffer(decryptedData, 0, decryptedData.length));&lt;br/&gt;
@@ -154,7 +181,9 @@ public void channelRead(ChannelHandlerContext ctx, Object data) throws Exception&lt;br/&gt;
     @Override&lt;br/&gt;
     public void channelInactive(ChannelHandlerContext ctx) throws Exception {&lt;br/&gt;
       try {&lt;br/&gt;
-        cis.close();&lt;br/&gt;
+        if (isCipherValid) {
+          cis.close();
+        }&lt;br/&gt;
       } finally {
         super.channelInactive(ctx);
       }&lt;br/&gt;
@@ -165,8 +194,9 @@ public void channelInactive(ChannelHandlerContext ctx) throws Exception {&lt;br/&gt;
     private final boolean isByteBuf;&lt;br/&gt;
     private final ByteBuf buf;&lt;br/&gt;
     private final FileRegion region;&lt;br/&gt;
+    private final CryptoOutputStream cos;&lt;br/&gt;
+    private final EncryptionHandler handler;&lt;br/&gt;
     private long transferred;&lt;br/&gt;
-    private CryptoOutputStream cos;&lt;br/&gt;
 &lt;br/&gt;
     // Due to streaming issue &lt;a href=&quot;https://issues.apache.org/jira/browse/CRYPTO-125&quot; title=&quot;CryptoOutputStream does not call write in a loop when underlying channel works in non-block mode.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CRYPTO-125&quot;&gt;&lt;del&gt;CRYPTO-125&lt;/del&gt;&lt;/a&gt;: &lt;a href=&quot;https://issues.apache.org/jira/browse/CRYPTO-125&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/CRYPTO-125&lt;/a&gt;, it has&lt;br/&gt;
     // to utilize two helper ByteArrayWritableChannel for streaming. One is used to receive raw data&lt;br/&gt;
@@ -176,9 +206,14 @@ public void channelInactive(ChannelHandlerContext ctx) throws Exception {&lt;br/&gt;
 &lt;br/&gt;
     private ByteBuffer currentEncrypted;&lt;br/&gt;
 &lt;br/&gt;
-    EncryptedMessage(CryptoOutputStream cos, Object msg, ByteArrayWritableChannel ch) {&lt;br/&gt;
+    EncryptedMessage(&lt;br/&gt;
+        EncryptionHandler handler,&lt;br/&gt;
+        CryptoOutputStream cos,&lt;br/&gt;
+        Object msg,&lt;br/&gt;
+        ByteArrayWritableChannel ch) {
       Preconditions.checkArgument(msg instanceof ByteBuf || msg instanceof FileRegion,
         &quot;Unrecognized message type: %s&quot;, msg.getClass().getName());
+      this.handler = handler;
       this.isByteBuf = msg instanceof ByteBuf;
       this.buf = isByteBuf ? (ByteBuf) msg : null;
       this.region = isByteBuf ? null : (FileRegion) msg;
@@ -261,6 +296,9 @@ public long transferTo(WritableByteChannel target, long position) throws IOExcep
     }&lt;br/&gt;
 &lt;br/&gt;
     private void encryptMore() throws IOException {&lt;br/&gt;
+      if (!handler.isCipherValid()) {+        throw new IOException(&quot;Cipher is in invalid state.&quot;);+      }
&lt;p&gt;       byteRawChannel.reset();&lt;/p&gt;

&lt;p&gt;       if (isByteBuf) {&lt;br/&gt;
@@ -269,8 +307,14 @@ private void encryptMore() throws IOException {&lt;br/&gt;
       } else &lt;/p&gt;
{
         region.transferTo(byteRawChannel, region.transferred());
       }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cos.write(byteRawChannel.getData(), 0, byteRawChannel.length());&lt;/li&gt;
	&lt;li&gt;cos.flush();&lt;br/&gt;
+&lt;br/&gt;
+      try 
{
+        cos.write(byteRawChannel.getData(), 0, byteRawChannel.length());
+        cos.flush();
+      }
&lt;p&gt; catch (InternalError ie) &lt;/p&gt;
{
+        handler.reportError();
+        throw ie;
+      }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       currentEncrypted = ByteBuffer.wrap(byteEncChannel.getData(),&lt;br/&gt;
         0, byteEncChannel.length());&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchFailure.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchFailure.java&lt;br/&gt;
index 7b28a9a969486..a7afbfa8621c8 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchFailure.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchFailure.java&lt;br/&gt;
@@ -33,7 +33,7 @@ public ChunkFetchFailure(StreamChunkId streamChunkId, String errorString) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.ChunkFetchFailure; }&lt;br/&gt;
+  public Message.Type type() { return Type.ChunkFetchFailure; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchRequest.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchRequest.java&lt;br/&gt;
index 26d063feb5fe3..fe54fcc50dc86 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchRequest.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchRequest.java&lt;br/&gt;
@@ -32,7 +32,7 @@ public ChunkFetchRequest(StreamChunkId streamChunkId) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.ChunkFetchRequest; }&lt;br/&gt;
+  public Message.Type type() { return Type.ChunkFetchRequest; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchSuccess.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchSuccess.java&lt;br/&gt;
index 94c2ac9b20e43..d5c9a9b3202fb 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchSuccess.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/ChunkFetchSuccess.java&lt;br/&gt;
@@ -39,7 +39,7 @@ public ChunkFetchSuccess(StreamChunkId streamChunkId, ManagedBuffer buffer) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.ChunkFetchSuccess; }&lt;br/&gt;
+  public Message.Type type() { return Type.ChunkFetchSuccess; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/OneWayMessage.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/OneWayMessage.java&lt;br/&gt;
index f7ffb1bd49bb6..1632fb9e03687 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/OneWayMessage.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/OneWayMessage.java&lt;br/&gt;
@@ -34,7 +34,7 @@ public OneWayMessage(ManagedBuffer body) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.OneWayMessage; }&lt;br/&gt;
+  public Message.Type type() { return Type.OneWayMessage; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcFailure.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcFailure.java&lt;br/&gt;
index a76624ef5dc96..61061903de23f 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcFailure.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcFailure.java&lt;br/&gt;
@@ -31,7 +31,7 @@ public RpcFailure(long requestId, String errorString) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.RpcFailure; }&lt;br/&gt;
+  public Message.Type type() { return Type.RpcFailure; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcRequest.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcRequest.java&lt;br/&gt;
index 2b30920f0598d..cc1bb95d2d566 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcRequest.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcRequest.java&lt;br/&gt;
@@ -38,7 +38,7 @@ public RpcRequest(long requestId, ManagedBuffer message) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.RpcRequest; }&lt;br/&gt;
+  public Message.Type type() { return Type.RpcRequest; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcResponse.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcResponse.java&lt;br/&gt;
index d73014ecd8506..c03291e9c0b23 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcResponse.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/RpcResponse.java&lt;br/&gt;
@@ -33,7 +33,7 @@ public RpcResponse(long requestId, ManagedBuffer message) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.RpcResponse; }&lt;br/&gt;
+  public Message.Type type() { return Type.RpcResponse; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamFailure.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamFailure.java&lt;br/&gt;
index 258ef81c6783d..68fcfa7748611 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamFailure.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamFailure.java&lt;br/&gt;
@@ -33,7 +33,7 @@ public StreamFailure(String streamId, String error) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.StreamFailure; }&lt;br/&gt;
+  public Message.Type type() { return Type.StreamFailure; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamRequest.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamRequest.java&lt;br/&gt;
index dc183c043ed9a..1b135af752bd8 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamRequest.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamRequest.java&lt;br/&gt;
@@ -34,7 +34,7 @@ public StreamRequest(String streamId) {&lt;br/&gt;
    }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.StreamRequest; }&lt;br/&gt;
+  public Message.Type type() { return Type.StreamRequest; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamResponse.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamResponse.java&lt;br/&gt;
index 50b811604b84b..568108c4fe5e8 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamResponse.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/StreamResponse.java&lt;br/&gt;
@@ -40,7 +40,7 @@ public StreamResponse(String streamId, long byteCount, ManagedBuffer buffer) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.StreamResponse; }&lt;br/&gt;
+  public Message.Type type() { return Type.StreamResponse; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/protocol/UploadStream.java b/common/network-common/src/main/java/org/apache/spark/network/protocol/UploadStream.java&lt;br/&gt;
index fa1d26e76b852..7d21151e01074 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/protocol/UploadStream.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/protocol/UploadStream.java&lt;br/&gt;
@@ -52,7 +52,7 @@ private UploadStream(long requestId, ManagedBuffer meta, long bodyByteCount) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.UploadStream; }&lt;br/&gt;
+  public Message.Type type() { return Type.UploadStream; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() &lt;/p&gt;
{
diff --git a/common/network-common/src/main/java/org/apache/spark/network/sasl/SaslMessage.java b/common/network-common/src/main/java/org/apache/spark/network/sasl/SaslMessage.java
index 7331c2b481fb1..1b03300d948e2 100644
--- a/common/network-common/src/main/java/org/apache/spark/network/sasl/SaslMessage.java
+++ b/common/network-common/src/main/java/org/apache/spark/network/sasl/SaslMessage.java
@@ -23,6 +23,7 @@
 import org.apache.spark.network.buffer.NettyManagedBuffer;
 import org.apache.spark.network.protocol.Encoders;
 import org.apache.spark.network.protocol.AbstractMessage;
+import org.apache.spark.network.protocol.Message;
 
 /**
  * Encodes a Sasl-related message which is attempting to authenticate using some credentials tagged
@@ -46,7 +47,7 @@
   }

&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Type type() 
{ return Type.User; }&lt;br/&gt;
+  public Message.Type type() { return Type.User; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public int encodedLength() {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/server/ChunkFetchRequestHandler.java b/common/network-common/src/main/java/org/apache/spark/network/server/ChunkFetchRequestHandler.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..f08d8b0f984cf&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/server/ChunkFetchRequestHandler.java&lt;br/&gt;
@@ -0,0 +1,135 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.network.server;&lt;br/&gt;
+&lt;br/&gt;
+import java.net.SocketAddress;&lt;br/&gt;
+&lt;br/&gt;
+import com.google.common.base.Throwables;&lt;br/&gt;
+import io.netty.channel.Channel;&lt;br/&gt;
+import io.netty.channel.ChannelFuture;&lt;br/&gt;
+import io.netty.channel.ChannelFutureListener;&lt;br/&gt;
+import io.netty.channel.ChannelHandlerContext;&lt;br/&gt;
+import io.netty.channel.SimpleChannelInboundHandler;&lt;br/&gt;
+import org.slf4j.Logger;&lt;br/&gt;
+import org.slf4j.LoggerFactory;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.spark.network.buffer.ManagedBuffer;&lt;br/&gt;
+import org.apache.spark.network.client.TransportClient;&lt;br/&gt;
+import org.apache.spark.network.protocol.ChunkFetchFailure;&lt;br/&gt;
+import org.apache.spark.network.protocol.ChunkFetchRequest;&lt;br/&gt;
+import org.apache.spark.network.protocol.ChunkFetchSuccess;&lt;br/&gt;
+import org.apache.spark.network.protocol.Encodable;&lt;br/&gt;
+&lt;br/&gt;
+import static org.apache.spark.network.util.NettyUtils.*;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * A dedicated ChannelHandler for processing ChunkFetchRequest messages. When sending response&lt;br/&gt;
+ * of ChunkFetchRequest messages to the clients, the thread performing the I/O on the underlying&lt;br/&gt;
+ * channel could potentially be blocked due to disk contentions. If several hundreds of clients&lt;br/&gt;
+ * send ChunkFetchRequest to the server at the same time, it could potentially occupying all&lt;br/&gt;
+ * threads from TransportServer&apos;s default EventLoopGroup for waiting for disk reads before it&lt;br/&gt;
+ * can send the block data back to the client as part of the ChunkFetchSuccess messages. As a&lt;br/&gt;
+ * result, it would leave no threads left to process other RPC messages, which takes much less&lt;br/&gt;
+ * time to process, and could lead to client timing out on either performing SASL authentication,&lt;br/&gt;
+ * registering executors, or waiting for response for an OpenBlocks messages.&lt;br/&gt;
+ */&lt;br/&gt;
+public class ChunkFetchRequestHandler extends SimpleChannelInboundHandler&amp;lt;ChunkFetchRequest&amp;gt; {&lt;br/&gt;
+  private static final Logger logger = LoggerFactory.getLogger(ChunkFetchRequestHandler.class);&lt;br/&gt;
+&lt;br/&gt;
+  private final TransportClient client;&lt;br/&gt;
+  private final StreamManager streamManager;&lt;br/&gt;
+  /** The max number of chunks being transferred and not finished yet. */&lt;br/&gt;
+  private final long maxChunksBeingTransferred;&lt;br/&gt;
+&lt;br/&gt;
+  public ChunkFetchRequestHandler(&lt;br/&gt;
+      TransportClient client,&lt;br/&gt;
+      StreamManager streamManager,&lt;br/&gt;
+      Long maxChunksBeingTransferred) &lt;/p&gt;
{
+    this.client = client;
+    this.streamManager = streamManager;
+    this.maxChunksBeingTransferred = maxChunksBeingTransferred;
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Override&lt;br/&gt;
+  public void exceptionCaught(ChannelHandlerContext ctx, Throwable cause) throws Exception &lt;/p&gt;
{
+    logger.warn(&quot;Exception in connection from &quot; + getRemoteAddress(ctx.channel()), cause);
+    ctx.close();
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Override&lt;br/&gt;
+  protected void channelRead0(&lt;br/&gt;
+      ChannelHandlerContext ctx,&lt;br/&gt;
+      final ChunkFetchRequest msg) throws Exception {&lt;br/&gt;
+    Channel channel = ctx.channel();&lt;br/&gt;
+    if (logger.isTraceEnabled()) {&lt;br/&gt;
+      logger.trace(&quot;Received req from {} to fetch block {}&quot;, getRemoteAddress(channel),&lt;br/&gt;
+        msg.streamChunkId);&lt;br/&gt;
+    }&lt;br/&gt;
+    long chunksBeingTransferred = streamManager.chunksBeingTransferred();&lt;br/&gt;
+    if (chunksBeingTransferred &amp;gt;= maxChunksBeingTransferred) {&lt;br/&gt;
+      logger.warn(&quot;The number of chunks being transferred {} is above {}, close the connection.&quot;,&lt;br/&gt;
+        chunksBeingTransferred, maxChunksBeingTransferred);&lt;br/&gt;
+      channel.close();&lt;br/&gt;
+      return;&lt;br/&gt;
+    }&lt;br/&gt;
+    ManagedBuffer buf;&lt;br/&gt;
+    try &lt;/p&gt;
{
+      streamManager.checkAuthorization(client, msg.streamChunkId.streamId);
+      streamManager.registerChannel(channel, msg.streamChunkId.streamId);
+      buf = streamManager.getChunk(msg.streamChunkId.streamId, msg.streamChunkId.chunkIndex);
+    }
&lt;p&gt; catch (Exception e) &lt;/p&gt;
{
+      logger.error(String.format(&quot;Error opening block %s for request from %s&quot;,
+        msg.streamChunkId, getRemoteAddress(channel)), e);
+      respond(channel, new ChunkFetchFailure(msg.streamChunkId,
+        Throwables.getStackTraceAsString(e)));
+      return;
+    }
&lt;p&gt;+&lt;br/&gt;
+    streamManager.chunkBeingSent(msg.streamChunkId.streamId);&lt;br/&gt;
+    respond(channel, new ChunkFetchSuccess(msg.streamChunkId, buf)).addListener(&lt;br/&gt;
+      (ChannelFutureListener) future -&amp;gt; streamManager.chunkSent(msg.streamChunkId.streamId));&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * The invocation to channel.writeAndFlush is async, and the actual I/O on the&lt;br/&gt;
+   * channel will be handled by the EventLoop the channel is registered to. So even&lt;br/&gt;
+   * though we are processing the ChunkFetchRequest in a separate thread pool, the actual I/O,&lt;br/&gt;
+   * which is the potentially blocking call that could deplete server handler threads, is still&lt;br/&gt;
+   * being processed by TransportServer&apos;s default EventLoopGroup. In order to throttle the max&lt;br/&gt;
+   * number of threads that channel I/O for sending response to ChunkFetchRequest, the thread&lt;br/&gt;
+   * calling channel.writeAndFlush will wait for the completion of sending response back to&lt;br/&gt;
+   * client by invoking await(). This will throttle the rate at which threads from&lt;br/&gt;
+   * ChunkFetchRequest dedicated EventLoopGroup submit channel I/O requests to TransportServer&apos;s&lt;br/&gt;
+   * default EventLoopGroup, thus making sure that we can reserve some threads in&lt;br/&gt;
+   * TransportServer&apos;s default EventLoopGroup for handling other RPC messages.&lt;br/&gt;
+   */&lt;br/&gt;
+  private ChannelFuture respond(&lt;br/&gt;
+      final Channel channel,&lt;br/&gt;
+      final Encodable result) throws InterruptedException {&lt;br/&gt;
+    final SocketAddress remoteAddress = channel.remoteAddress();&lt;br/&gt;
+    return channel.writeAndFlush(result).await().addListener((ChannelFutureListener) future -&amp;gt; {&lt;br/&gt;
+      if (future.isSuccess()) {&lt;br/&gt;
+        logger.trace(&quot;Sent result {} to client {}&quot;, result, remoteAddress);&lt;br/&gt;
+      } else &lt;/p&gt;
{
+        logger.error(String.format(&quot;Error sending result %s to %s; closing connection&quot;,
+          result, remoteAddress), future.cause());
+        channel.close();
+      }
&lt;p&gt;+    });&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java b/common/network-common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java&lt;br/&gt;
index 56782a8327876..c824a7b0d4740 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/server/TransportChannelHandler.java&lt;br/&gt;
@@ -18,7 +18,7 @@&lt;br/&gt;
 package org.apache.spark.network.server;&lt;/p&gt;

&lt;p&gt; import io.netty.channel.ChannelHandlerContext;&lt;br/&gt;
-import io.netty.channel.ChannelInboundHandlerAdapter;&lt;br/&gt;
+import io.netty.channel.SimpleChannelInboundHandler;&lt;br/&gt;
 import io.netty.handler.timeout.IdleState;&lt;br/&gt;
 import io.netty.handler.timeout.IdleStateEvent;&lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
@@ -26,6 +26,8 @@&lt;/p&gt;

&lt;p&gt; import org.apache.spark.network.client.TransportClient;&lt;br/&gt;
 import org.apache.spark.network.client.TransportResponseHandler;&lt;br/&gt;
+import org.apache.spark.network.protocol.ChunkFetchRequest;&lt;br/&gt;
+import org.apache.spark.network.protocol.Message;&lt;br/&gt;
 import org.apache.spark.network.protocol.RequestMessage;&lt;br/&gt;
 import org.apache.spark.network.protocol.ResponseMessage;&lt;br/&gt;
 import static org.apache.spark.network.util.NettyUtils.getRemoteAddress;&lt;br/&gt;
@@ -47,7 +49,7 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;on the channel for at least `requestTimeoutMs`. Note that this is duplex traffic; we will not&lt;/li&gt;
	&lt;li&gt;timeout if the client is continuously sending but getting no responses, for simplicity.&lt;br/&gt;
  */&lt;br/&gt;
-public class TransportChannelHandler extends ChannelInboundHandlerAdapter {&lt;br/&gt;
+public class TransportChannelHandler extends SimpleChannelInboundHandler&amp;lt;Message&amp;gt; {&lt;br/&gt;
   private static final Logger logger = LoggerFactory.getLogger(TransportChannelHandler.class);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private final TransportClient client;&lt;br/&gt;
@@ -112,8 +114,21 @@ public void channelInactive(ChannelHandlerContext ctx) throws Exception &lt;/p&gt;
{
     super.channelInactive(ctx);
   }

&lt;p&gt;+  /**&lt;br/&gt;
+   * Overwrite acceptInboundMessage to properly delegate ChunkFetchRequest messages&lt;br/&gt;
+   * to ChunkFetchRequestHandler.&lt;br/&gt;
+   */&lt;br/&gt;
   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void channelRead(ChannelHandlerContext ctx, Object request) throws Exception {&lt;br/&gt;
+  public boolean acceptInboundMessage(Object msg) throws Exception 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    if (msg instanceof ChunkFetchRequest) {
+      return false;
+    } else {
+      return super.acceptInboundMessage(msg);
+    }+  }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+&lt;br/&gt;
+  @Override&lt;br/&gt;
+  public void channelRead0(ChannelHandlerContext ctx, Message request) throws Exception {&lt;br/&gt;
     if (request instanceof RequestMessage) &lt;/p&gt;
{
       requestHandler.handle((RequestMessage) request);
     }
&lt;p&gt; else if (request instanceof ResponseMessage) {&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java b/common/network-common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java&lt;br/&gt;
index 9fac96dbe450d..3e089b4cae273 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/network-common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/server/TransportRequestHandler.java&lt;br/&gt;
@@ -24,6 +24,7 @@&lt;br/&gt;
 import com.google.common.base.Throwables;&lt;br/&gt;
 import io.netty.channel.Channel;&lt;br/&gt;
 import io.netty.channel.ChannelFuture;&lt;br/&gt;
+&lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
 import org.slf4j.LoggerFactory;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -97,9 +98,7 @@ public void channelInactive() {&lt;/p&gt;

&lt;p&gt;   @Override&lt;br/&gt;
   public void handle(RequestMessage request) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (request instanceof ChunkFetchRequest) 
{
-      processFetchRequest((ChunkFetchRequest) request);
-    }
&lt;p&gt; else if (request instanceof RpcRequest) {&lt;br/&gt;
+    if (request instanceof RpcRequest) &lt;/p&gt;
{
       processRpcRequest((RpcRequest) request);
     }
&lt;p&gt; else if (request instanceof OneWayMessage) {&lt;br/&gt;
       processOneWayMessage((OneWayMessage) request);&lt;br/&gt;
@@ -112,36 +111,6 @@ public void handle(RequestMessage request) {&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private void processFetchRequest(final ChunkFetchRequest req) {&lt;/li&gt;
	&lt;li&gt;if (logger.isTraceEnabled()) {&lt;/li&gt;
	&lt;li&gt;logger.trace(&quot;Received req from {} to fetch block {}&quot;, getRemoteAddress(channel),&lt;/li&gt;
	&lt;li&gt;req.streamChunkId);&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;long chunksBeingTransferred = streamManager.chunksBeingTransferred();&lt;/li&gt;
	&lt;li&gt;if (chunksBeingTransferred &amp;gt;= maxChunksBeingTransferred) {&lt;/li&gt;
	&lt;li&gt;logger.warn(&quot;The number of chunks being transferred {} is above {}, close the connection.&quot;,&lt;/li&gt;
	&lt;li&gt;chunksBeingTransferred, maxChunksBeingTransferred);&lt;/li&gt;
	&lt;li&gt;channel.close();&lt;/li&gt;
	&lt;li&gt;return;&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;ManagedBuffer buf;&lt;/li&gt;
	&lt;li&gt;try 
{
-      streamManager.checkAuthorization(reverseClient, req.streamChunkId.streamId);
-      streamManager.registerChannel(channel, req.streamChunkId.streamId);
-      buf = streamManager.getChunk(req.streamChunkId.streamId, req.streamChunkId.chunkIndex);
-    }
&lt;p&gt; catch (Exception e) &lt;/p&gt;
{
-      logger.error(String.format(&quot;Error opening block %s for request from %s&quot;,
-        req.streamChunkId, getRemoteAddress(channel)), e);
-      respond(new ChunkFetchFailure(req.streamChunkId, Throwables.getStackTraceAsString(e)));
-      return;
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;streamManager.chunkBeingSent(req.streamChunkId.streamId);&lt;/li&gt;
	&lt;li&gt;respond(new ChunkFetchSuccess(req.streamChunkId, buf)).addListener(future -&amp;gt; 
{
-      streamManager.chunkSent(req.streamChunkId.streamId);
-    }
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;br/&gt;
   private void processStreamRequest(final StreamRequest req) {&lt;br/&gt;
     if (logger.isTraceEnabled()) {&lt;br/&gt;
       logger.trace(&quot;Received req from {} to fetch stream {}&quot;, getRemoteAddress(channel),&lt;br/&gt;
diff --git a/common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java b/common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java&lt;br/&gt;
index 34e4bb5912dcb..43a6bc7dc3d06 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java&lt;br/&gt;
+++ b/common/network-common/src/main/java/org/apache/spark/network/util/TransportConf.java&lt;br/&gt;
@@ -21,6 +21,7 @@&lt;br/&gt;
 import java.util.Properties;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import com.google.common.primitives.Ints;&lt;br/&gt;
+import io.netty.util.NettyRuntime;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A central location that tracks all the settings we expose to users.&lt;br/&gt;
@@ -281,4 +282,35 @@ public Properties cryptoConf() {&lt;br/&gt;
   public long maxChunksBeingTransferred() 
{
     return conf.getLong(&quot;spark.shuffle.maxChunksBeingTransferred&quot;, Long.MAX_VALUE);
   }
&lt;p&gt;+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Percentage of io.serverThreads used by netty to process ChunkFetchRequest.&lt;br/&gt;
+   * Shuffle server will use a separate EventLoopGroup to process ChunkFetchRequest messages.&lt;br/&gt;
+   * Although when calling the async writeAndFlush on the underlying channel to send&lt;br/&gt;
+   * response back to client, the I/O on the channel is still being handled by&lt;br/&gt;
+   * &lt;/p&gt;
{@link org.apache.spark.network.server.TransportServer}
&lt;p&gt;&apos;s default EventLoopGroup&lt;br/&gt;
+   * that&apos;s registered with the Channel, by waiting inside the ChunkFetchRequest handler&lt;br/&gt;
+   * threads for the completion of sending back responses, we are able to put a limit on&lt;br/&gt;
+   * the max number of threads from TransportServer&apos;s default EventLoopGroup that are&lt;br/&gt;
+   * going to be consumed by writing response to ChunkFetchRequest, which are I/O intensive&lt;br/&gt;
+   * and could take long time to process due to disk contentions. By configuring a slightly&lt;br/&gt;
+   * higher number of shuffler server threads, we are able to reserve some threads for&lt;br/&gt;
+   * handling other RPC messages, thus making the Client less likely to experience timeout&lt;br/&gt;
+   * when sending RPC messages to the shuffle server. The number of threads used for handling&lt;br/&gt;
+   * chunked fetch requests are percentage of io.serverThreads (if defined) else it is a percentage&lt;br/&gt;
+   * of 2 * #cores. However, a percentage of 0 means netty default number of threads which&lt;br/&gt;
+   * is 2 * #cores ignoring io.serverThreads. The percentage here is configured via&lt;br/&gt;
+   * spark.shuffle.server.chunkFetchHandlerThreadsPercent. The returned value is rounded off to&lt;br/&gt;
+   * ceiling of the nearest integer.&lt;br/&gt;
+   */&lt;br/&gt;
+  public int chunkFetchHandlerThreads() &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    if (!this.getModuleName().equalsIgnoreCase(&amp;quot;shuffle&amp;quot;)) {
+      return 0;
+    }+    int chunkFetchHandlerThreadsPercent =+      conf.getInt(&amp;quot;spark.shuffle.server.chunkFetchHandlerThreadsPercent&amp;quot;, 100);+    return (int)Math.ceil(+     (this.serverThreads() &amp;gt; 0 ? this.serverThreads() }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; }&lt;br/&gt;
diff --git a/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchIntegrationSuite.java b/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchIntegrationSuite.java&lt;br/&gt;
index 824482af08dd4..37a8664a52661 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchIntegrationSuite.java&lt;br/&gt;
+++ b/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchIntegrationSuite.java&lt;br/&gt;
@@ -143,37 +143,39 @@ public void releaseBuffers() {&lt;br/&gt;
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private FetchResult fetchChunks(List&amp;lt;Integer&amp;gt; chunkIndices) throws Exception {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;TransportClient client = clientFactory.createClient(TestUtils.getLocalHost(), server.getPort());&lt;/li&gt;
	&lt;li&gt;final Semaphore sem = new Semaphore(0);&lt;br/&gt;
-&lt;br/&gt;
     final FetchResult res = new FetchResult();&lt;/li&gt;
	&lt;li&gt;res.successChunks = Collections.synchronizedSet(new HashSet&amp;lt;Integer&amp;gt;());&lt;/li&gt;
	&lt;li&gt;res.failedChunks = Collections.synchronizedSet(new HashSet&amp;lt;Integer&amp;gt;());&lt;/li&gt;
	&lt;li&gt;res.buffers = Collections.synchronizedList(new LinkedList&amp;lt;ManagedBuffer&amp;gt;());&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ChunkReceivedCallback callback = new ChunkReceivedCallback() {&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void onSuccess(int chunkIndex, ManagedBuffer buffer) 
{
-        buffer.retain();
-        res.successChunks.add(chunkIndex);
-        res.buffers.add(buffer);
-        sem.release();
-      }
&lt;p&gt;+    try (TransportClient client =&lt;br/&gt;
+      clientFactory.createClient(TestUtils.getLocalHost(), server.getPort())) {&lt;br/&gt;
+      final Semaphore sem = new Semaphore(0);&lt;br/&gt;
+&lt;br/&gt;
+      res.successChunks = Collections.synchronizedSet(new HashSet&amp;lt;Integer&amp;gt;());&lt;br/&gt;
+      res.failedChunks = Collections.synchronizedSet(new HashSet&amp;lt;Integer&amp;gt;());&lt;br/&gt;
+      res.buffers = Collections.synchronizedList(new LinkedList&amp;lt;ManagedBuffer&amp;gt;());&lt;br/&gt;
+&lt;br/&gt;
+      ChunkReceivedCallback callback = new ChunkReceivedCallback() {&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void onSuccess(int chunkIndex, ManagedBuffer buffer) &lt;/p&gt;
{
+          buffer.retain();
+          res.successChunks.add(chunkIndex);
+          res.buffers.add(buffer);
+          sem.release();
+        }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void onFailure(int chunkIndex, Throwable e) 
{
-        res.failedChunks.add(chunkIndex);
-        sem.release();
-      }&lt;/li&gt;
	&lt;li&gt;};&lt;br/&gt;
+        @Override&lt;br/&gt;
+        public void onFailure(int chunkIndex, Throwable e) 
{
+          res.failedChunks.add(chunkIndex);
+          sem.release();
+        }
&lt;p&gt;+      };&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (int chunkIndex : chunkIndices) 
{
-      client.fetchChunk(STREAM_ID, chunkIndex, callback);
-    }&lt;/li&gt;
	&lt;li&gt;if (!sem.tryAcquire(chunkIndices.size(), 5, TimeUnit.SECONDS)) {&lt;/li&gt;
	&lt;li&gt;fail(&quot;Timeout getting response from the server&quot;);&lt;br/&gt;
+      for (int chunkIndex : chunkIndices) 
{
+        client.fetchChunk(STREAM_ID, chunkIndex, callback);
+      }
&lt;p&gt;+      if (!sem.tryAcquire(chunkIndices.size(), 5, TimeUnit.SECONDS)) &lt;/p&gt;
{
+        fail(&quot;Timeout getting response from the server&quot;);
+      }&lt;br/&gt;
     }&lt;br/&gt;
-    client.close();&lt;br/&gt;
     return res;&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
diff --git a/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchRequestHandlerSuite.java b/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchRequestHandlerSuite.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..2c72c53a33ae8&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/common/network-common/src/test/java/org/apache/spark/network/ChunkFetchRequestHandlerSuite.java&lt;br/&gt;
@@ -0,0 +1,102 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.network;&lt;br/&gt;
+&lt;br/&gt;
+import io.netty.channel.ChannelHandlerContext;&lt;br/&gt;
+import java.util.ArrayList;&lt;br/&gt;
+import java.util.List;&lt;br/&gt;
+&lt;br/&gt;
+import io.netty.channel.Channel;&lt;br/&gt;
+import org.apache.spark.network.server.ChunkFetchRequestHandler;&lt;br/&gt;
+import org.junit.Test;&lt;br/&gt;
+&lt;br/&gt;
+import static org.mockito.Mockito.*;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.commons.lang3.tuple.ImmutablePair;&lt;br/&gt;
+import org.apache.commons.lang3.tuple.Pair;&lt;br/&gt;
+import org.apache.spark.network.buffer.ManagedBuffer;&lt;br/&gt;
+import org.apache.spark.network.client.TransportClient;&lt;br/&gt;
+import org.apache.spark.network.protocol.*;&lt;br/&gt;
+import org.apache.spark.network.server.NoOpRpcHandler;&lt;br/&gt;
+import org.apache.spark.network.server.OneForOneStreamManager;&lt;br/&gt;
+import org.apache.spark.network.server.RpcHandler;&lt;br/&gt;
+&lt;br/&gt;
+public class ChunkFetchRequestHandlerSuite {&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  public void handleChunkFetchRequest() throws Exception {&lt;br/&gt;
+    RpcHandler rpcHandler = new NoOpRpcHandler();&lt;br/&gt;
+    OneForOneStreamManager streamManager = (OneForOneStreamManager) (rpcHandler.getStreamManager());&lt;br/&gt;
+    Channel channel = mock(Channel.class);&lt;br/&gt;
+    ChannelHandlerContext context = mock(ChannelHandlerContext.class);&lt;br/&gt;
+    when(context.channel())&lt;br/&gt;
+      .thenAnswer(invocationOnMock0 -&amp;gt; {
+        return channel;
+      });&lt;br/&gt;
+    List&amp;lt;Pair&amp;lt;Object, ExtendedChannelPromise&amp;gt;&amp;gt; responseAndPromisePairs =&lt;br/&gt;
+      new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+    when(channel.writeAndFlush(any()))&lt;br/&gt;
+      .thenAnswer(invocationOnMock0 -&amp;gt; {
+        Object response = invocationOnMock0.getArguments()[0];
+        ExtendedChannelPromise channelFuture = new ExtendedChannelPromise(channel);
+        responseAndPromisePairs.add(ImmutablePair.of(response, channelFuture));
+        return channelFuture;
+      });&lt;br/&gt;
+&lt;br/&gt;
+    // Prepare the stream.&lt;br/&gt;
+    List&amp;lt;ManagedBuffer&amp;gt; managedBuffers = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+    managedBuffers.add(new TestManagedBuffer(10));&lt;br/&gt;
+    managedBuffers.add(new TestManagedBuffer(20));&lt;br/&gt;
+    managedBuffers.add(new TestManagedBuffer(30));&lt;br/&gt;
+    managedBuffers.add(new TestManagedBuffer(40));&lt;br/&gt;
+    long streamId = streamManager.registerStream(&quot;test-app&quot;, managedBuffers.iterator());&lt;br/&gt;
+    streamManager.registerChannel(channel, streamId);&lt;br/&gt;
+    TransportClient reverseClient = mock(TransportClient.class);&lt;br/&gt;
+    ChunkFetchRequestHandler requestHandler = new ChunkFetchRequestHandler(reverseClient,&lt;br/&gt;
+      rpcHandler.getStreamManager(), 2L);&lt;br/&gt;
+&lt;br/&gt;
+    RequestMessage request0 = new ChunkFetchRequest(new StreamChunkId(streamId, 0));&lt;br/&gt;
+    requestHandler.channelRead(context, request0);&lt;br/&gt;
+    assert responseAndPromisePairs.size() == 1;&lt;br/&gt;
+    assert responseAndPromisePairs.get(0).getLeft() instanceof ChunkFetchSuccess;&lt;br/&gt;
+    assert ((ChunkFetchSuccess) (responseAndPromisePairs.get(0).getLeft())).body() ==&lt;br/&gt;
+      managedBuffers.get(0);&lt;br/&gt;
+&lt;br/&gt;
+    RequestMessage request1 = new ChunkFetchRequest(new StreamChunkId(streamId, 1));&lt;br/&gt;
+    requestHandler.channelRead(context, request1);&lt;br/&gt;
+    assert responseAndPromisePairs.size() == 2;&lt;br/&gt;
+    assert responseAndPromisePairs.get(1).getLeft() instanceof ChunkFetchSuccess;&lt;br/&gt;
+    assert ((ChunkFetchSuccess) (responseAndPromisePairs.get(1).getLeft())).body() ==&lt;br/&gt;
+      managedBuffers.get(1);&lt;br/&gt;
+&lt;br/&gt;
+    // Finish flushing the response for request0.&lt;br/&gt;
+    responseAndPromisePairs.get(0).getRight().finish(true);&lt;br/&gt;
+&lt;br/&gt;
+    RequestMessage request2 = new ChunkFetchRequest(new StreamChunkId(streamId, 2));&lt;br/&gt;
+    requestHandler.channelRead(context, request2);&lt;br/&gt;
+    assert responseAndPromisePairs.size() == 3;&lt;br/&gt;
+    assert responseAndPromisePairs.get(2).getLeft() instanceof ChunkFetchSuccess;&lt;br/&gt;
+    assert ((ChunkFetchSuccess) (responseAndPromisePairs.get(2).getLeft())).body() ==&lt;br/&gt;
+      managedBuffers.get(2);&lt;br/&gt;
+&lt;br/&gt;
+    RequestMessage request3 = new ChunkFetchRequest(new StreamChunkId(streamId, 3));&lt;br/&gt;
+    requestHandler.channelRead(context, request3);&lt;br/&gt;
+    verify(channel, times(1)).close();&lt;br/&gt;
+    assert responseAndPromisePairs.size() == 3;&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/common/network-common/src/test/java/org/apache/spark/network/ExtendedChannelPromise.java b/common/network-common/src/test/java/org/apache/spark/network/ExtendedChannelPromise.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..573ffd627a2e7&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/common/network-common/src/test/java/org/apache/spark/network/ExtendedChannelPromise.java&lt;br/&gt;
@@ -0,0 +1,69 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.network;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.ArrayList;&lt;br/&gt;
+import java.util.List;&lt;br/&gt;
+&lt;br/&gt;
+import io.netty.channel.Channel;&lt;br/&gt;
+import io.netty.channel.ChannelPromise;&lt;br/&gt;
+import io.netty.channel.DefaultChannelPromise;&lt;br/&gt;
+import io.netty.util.concurrent.Future;&lt;br/&gt;
+import io.netty.util.concurrent.GenericFutureListener;&lt;br/&gt;
+&lt;br/&gt;
+class ExtendedChannelPromise extends DefaultChannelPromise {&lt;br/&gt;
+&lt;br/&gt;
+  private List&amp;lt;GenericFutureListener&amp;lt;Future&amp;lt;Void&amp;gt;&amp;gt;&amp;gt; listeners = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
+  private boolean success;&lt;br/&gt;
+&lt;br/&gt;
+  ExtendedChannelPromise(Channel channel) {
+    super(channel);
+    success = false;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Override&lt;br/&gt;
+  public ChannelPromise addListener(&lt;br/&gt;
+      GenericFutureListener&amp;lt;? extends Future&amp;lt;? super Void&amp;gt;&amp;gt; listener) {
+    @SuppressWarnings(&quot;unchecked&quot;)
+    GenericFutureListener&amp;lt;Future&amp;lt;Void&amp;gt;&amp;gt; gfListener =
+        (GenericFutureListener&amp;lt;Future&amp;lt;Void&amp;gt;&amp;gt;) listener;
+    listeners.add(gfListener);
+    return super.addListener(listener);
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Override&lt;br/&gt;
+  public boolean isSuccess() {
+    return success;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Override&lt;br/&gt;
+  public ChannelPromise await() throws InterruptedException {
+    return this;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  public void finish(boolean success) {&lt;br/&gt;
+    this.success = success;&lt;br/&gt;
+    listeners.forEach(listener -&amp;gt; {&lt;br/&gt;
+      try {
+        listener.operationComplete(this);
+      } catch (Exception e) {
+        // do nothing
+      }&lt;br/&gt;
+    });&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/common/network-common/src/test/java/org/apache/spark/network/RpcIntegrationSuite.java b/common/network-common/src/test/java/org/apache/spark/network/RpcIntegrationSuite.java&lt;br/&gt;
index 1f4d75c7e2ec5..1c0aa4da27ff9 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/test/java/org/apache/spark/network/RpcIntegrationSuite.java&lt;br/&gt;
+++ b/common/network-common/src/test/java/org/apache/spark/network/RpcIntegrationSuite.java&lt;br/&gt;
@@ -371,23 +371,33 @@ private void assertErrorsContain(Set&amp;lt;String&amp;gt; errors, Set&amp;lt;String&amp;gt; contains) {&lt;br/&gt;
 &lt;br/&gt;
   private void assertErrorAndClosed(RpcResult result, String expectedError) {&lt;br/&gt;
     assertTrue(&quot;unexpected success: &quot; + result.successMessages, result.successMessages.isEmpty());&lt;br/&gt;
-    // we expect 1 additional error, which contains &lt;b&gt;either&lt;/b&gt; &quot;closed&quot; or &quot;Connection reset&quot;&lt;br/&gt;
     Set&amp;lt;String&amp;gt; errors = result.errorMessages;&lt;br/&gt;
     assertEquals(&quot;Expected 2 errors, got &quot; + errors.size() + &quot;errors: &quot; +&lt;br/&gt;
         errors, 2, errors.size());&lt;br/&gt;
 &lt;br/&gt;
+    // We expect 1 additional error due to closed connection and here are possible keywords in the&lt;br/&gt;
+    // error message.&lt;br/&gt;
+    Set&amp;lt;String&amp;gt; possibleClosedErrors = Sets.newHashSet(&lt;br/&gt;
+        &quot;closed&quot;,&lt;br/&gt;
+        &quot;Connection reset&quot;,&lt;br/&gt;
+        &quot;java.nio.channels.ClosedChannelException&quot;,&lt;br/&gt;
+        &quot;java.io.IOException: Broken pipe&quot;&lt;br/&gt;
+    );&lt;br/&gt;
     Set&amp;lt;String&amp;gt; containsAndClosed = Sets.newHashSet(expectedError);&lt;br/&gt;
-    containsAndClosed.add(&quot;closed&quot;);&lt;br/&gt;
-    containsAndClosed.add(&quot;Connection reset&quot;);&lt;br/&gt;
+    containsAndClosed.addAll(possibleClosedErrors);&lt;br/&gt;
 &lt;br/&gt;
     Pair&amp;lt;Set&amp;lt;String&amp;gt;, Set&amp;lt;String&amp;gt;&amp;gt; r = checkErrorsContain(errors, containsAndClosed);&lt;br/&gt;
 &lt;br/&gt;
-    Set&amp;lt;String&amp;gt; errorsNotFound = r.getRight();&lt;br/&gt;
-    assertEquals(1, errorsNotFound.size());&lt;br/&gt;
-    String err = errorsNotFound.iterator().next();&lt;br/&gt;
-    assertTrue(err.equals(&quot;closed&quot;) || err.equals(&quot;Connection reset&quot;));&lt;br/&gt;
+    assertTrue(&quot;Got a non-empty set &quot; + r.getLeft(), r.getLeft().isEmpty());&lt;br/&gt;
 &lt;br/&gt;
-    assertTrue(r.getLeft().isEmpty());&lt;br/&gt;
+    Set&amp;lt;String&amp;gt; errorsNotFound = r.getRight();&lt;br/&gt;
+    assertEquals(&lt;br/&gt;
+        &quot;The size of &quot; + errorsNotFound + &quot; was not &quot; + (possibleClosedErrors.size() - 1),&lt;br/&gt;
+        possibleClosedErrors.size() - 1,&lt;br/&gt;
+        errorsNotFound.size());&lt;br/&gt;
+    for (String err: errorsNotFound) {
+      assertTrue(&quot;Found a wrong error &quot; + err, containsAndClosed.contains(err));
+    }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private Pair&amp;lt;Set&amp;lt;String&amp;gt;, Set&amp;lt;String&amp;gt;&amp;gt; checkErrorsContain(&lt;br/&gt;
diff --git a/common/network-common/src/test/java/org/apache/spark/network/TransportRequestHandlerSuite.java b/common/network-common/src/test/java/org/apache/spark/network/TransportRequestHandlerSuite.java&lt;br/&gt;
index 2656cbee95a20..ad640415a8e6d 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/test/java/org/apache/spark/network/TransportRequestHandlerSuite.java&lt;br/&gt;
+++ b/common/network-common/src/test/java/org/apache/spark/network/TransportRequestHandlerSuite.java&lt;br/&gt;
@@ -21,10 +21,6 @@&lt;br/&gt;
 import java.util.List;&lt;br/&gt;
 &lt;br/&gt;
 import io.netty.channel.Channel;&lt;br/&gt;
-import io.netty.channel.ChannelPromise;&lt;br/&gt;
-import io.netty.channel.DefaultChannelPromise;&lt;br/&gt;
-import io.netty.util.concurrent.Future;&lt;br/&gt;
-import io.netty.util.concurrent.GenericFutureListener;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 &lt;br/&gt;
 import static org.mockito.Mockito.*;&lt;br/&gt;
@@ -42,7 +38,7 @@&lt;br/&gt;
 public class TransportRequestHandlerSuite {&lt;br/&gt;
 &lt;br/&gt;
   @Test&lt;br/&gt;
-  public void handleFetchRequestAndStreamRequest() throws Exception {&lt;br/&gt;
+  public void handleStreamRequest() throws Exception {&lt;br/&gt;
     RpcHandler rpcHandler = new NoOpRpcHandler();&lt;br/&gt;
     OneForOneStreamManager streamManager = (OneForOneStreamManager) (rpcHandler.getStreamManager());&lt;br/&gt;
     Channel channel = mock(Channel.class);&lt;br/&gt;
@@ -68,18 +64,18 @@ public void handleFetchRequestAndStreamRequest() throws Exception {&lt;br/&gt;
     TransportRequestHandler requestHandler = new TransportRequestHandler(channel, reverseClient,&lt;br/&gt;
       rpcHandler, 2L);&lt;br/&gt;
 &lt;br/&gt;
-    RequestMessage request0 = new ChunkFetchRequest(new StreamChunkId(streamId, 0));&lt;br/&gt;
+    RequestMessage request0 = new StreamRequest(String.format(&quot;%d_%d&quot;, streamId, 0));&lt;br/&gt;
     requestHandler.handle(request0);&lt;br/&gt;
     assert responseAndPromisePairs.size() == 1;&lt;br/&gt;
-    assert responseAndPromisePairs.get(0).getLeft() instanceof ChunkFetchSuccess;&lt;br/&gt;
-    assert ((ChunkFetchSuccess) (responseAndPromisePairs.get(0).getLeft())).body() ==&lt;br/&gt;
+    assert responseAndPromisePairs.get(0).getLeft() instanceof StreamResponse;&lt;br/&gt;
+    assert ((StreamResponse) (responseAndPromisePairs.get(0).getLeft())).body() ==&lt;br/&gt;
       managedBuffers.get(0);&lt;br/&gt;
 &lt;br/&gt;
-    RequestMessage request1 = new ChunkFetchRequest(new StreamChunkId(streamId, 1));&lt;br/&gt;
+    RequestMessage request1 = new StreamRequest(String.format(&quot;%d_%d&quot;, streamId, 1));&lt;br/&gt;
     requestHandler.handle(request1);&lt;br/&gt;
     assert responseAndPromisePairs.size() == 2;&lt;br/&gt;
-    assert responseAndPromisePairs.get(1).getLeft() instanceof ChunkFetchSuccess;&lt;br/&gt;
-    assert ((ChunkFetchSuccess) (responseAndPromisePairs.get(1).getLeft())).body() ==&lt;br/&gt;
+    assert responseAndPromisePairs.get(1).getLeft() instanceof StreamResponse;&lt;br/&gt;
+    assert ((StreamResponse) (responseAndPromisePairs.get(1).getLeft())).body() ==&lt;br/&gt;
       managedBuffers.get(1);&lt;br/&gt;
 &lt;br/&gt;
     // Finish flushing the response for request0.&lt;br/&gt;
@@ -99,41 +95,4 @@ public void handleFetchRequestAndStreamRequest() throws Exception {
     verify(channel, times(1)).close();
     assert responseAndPromisePairs.size() == 3;
   }&lt;br/&gt;
-&lt;br/&gt;
-  private class ExtendedChannelPromise extends DefaultChannelPromise {&lt;br/&gt;
-&lt;br/&gt;
-    private List&amp;lt;GenericFutureListener&amp;lt;Future&amp;lt;Void&amp;gt;&amp;gt;&amp;gt; listeners = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
-    private boolean success;&lt;br/&gt;
-&lt;br/&gt;
-    ExtendedChannelPromise(Channel channel) {
-      super(channel);
-      success = false;
-    }&lt;br/&gt;
-&lt;br/&gt;
-    @Override&lt;br/&gt;
-    public ChannelPromise addListener(&lt;br/&gt;
-        GenericFutureListener&amp;lt;? extends Future&amp;lt;? super Void&amp;gt;&amp;gt; listener) {
-      @SuppressWarnings(&quot;unchecked&quot;)
-      GenericFutureListener&amp;lt;Future&amp;lt;Void&amp;gt;&amp;gt; gfListener =
-          (GenericFutureListener&amp;lt;Future&amp;lt;Void&amp;gt;&amp;gt;) listener;
-      listeners.add(gfListener);
-      return super.addListener(listener);
-    }&lt;br/&gt;
-&lt;br/&gt;
-    @Override&lt;br/&gt;
-    public boolean isSuccess() {
-      return success;
-    }&lt;br/&gt;
-&lt;br/&gt;
-    public void finish(boolean success) {&lt;br/&gt;
-      this.success = success;&lt;br/&gt;
-      listeners.forEach(listener -&amp;gt; {&lt;br/&gt;
-        try {
-          listener.operationComplete(this);
-        } catch (Exception e) {
-          // do nothing
-        }&lt;br/&gt;
-      });&lt;br/&gt;
-    }&lt;br/&gt;
-  }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/common/network-common/src/test/java/org/apache/spark/network/crypto/AuthEngineSuite.java b/common/network-common/src/test/java/org/apache/spark/network/crypto/AuthEngineSuite.java&lt;br/&gt;
index a3519fe4a423e..c0aa298a4017c 100644&lt;br/&gt;
&amp;#8212; a/common/network-common/src/test/java/org/apache/spark/network/crypto/AuthEngineSuite.java&lt;br/&gt;
+++ b/common/network-common/src/test/java/org/apache/spark/network/crypto/AuthEngineSuite.java&lt;br/&gt;
@@ -18,8 +18,11 @@&lt;br/&gt;
 package org.apache.spark.network.crypto;&lt;br/&gt;
 &lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
+import java.util.Map;&lt;br/&gt;
+import java.security.InvalidKeyException;&lt;br/&gt;
 import static java.nio.charset.StandardCharsets.UTF_8;&lt;br/&gt;
 &lt;br/&gt;
+import com.google.common.collect.ImmutableMap;&lt;br/&gt;
 import org.junit.BeforeClass;&lt;br/&gt;
 import org.junit.Test;&lt;br/&gt;
 import static org.junit.Assert.*;&lt;br/&gt;
@@ -104,4 +107,18 @@ public void testBadChallenge() throws Exception {
       challenge.cipher, challenge.keyLength, challenge.nonce, badChallenge));
   }&lt;br/&gt;
 &lt;br/&gt;
+  @Test(expected = InvalidKeyException.class)&lt;br/&gt;
+  public void testBadKeySize() throws Exception {&lt;br/&gt;
+    Map&amp;lt;String, String&amp;gt; mconf = ImmutableMap.of(&quot;spark.network.crypto.keyLength&quot;, &quot;42&quot;);&lt;br/&gt;
+    TransportConf conf = new TransportConf(&quot;rpc&quot;, new MapConfigProvider(mconf));&lt;br/&gt;
+&lt;br/&gt;
+    try (AuthEngine engine = new AuthEngine(&quot;appId&quot;, &quot;secret&quot;, conf)) {
+      engine.challenge();
+      fail(&quot;Should have failed to create challenge message.&quot;);
+
+      // Call close explicitly to make sure it&apos;s idempotent.
+      engine.close();
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/common/network-shuffle/pom.xml b/common/network-shuffle/pom.xml&lt;br/&gt;
index 05335df61a664..a6d99813a8501 100644&lt;br/&gt;
&amp;#8212; a/common/network-shuffle/pom.xml&lt;br/&gt;
+++ b/common/network-shuffle/pom.xml&lt;br/&gt;
@@ -21,12 +21,12 @@&lt;br/&gt;
   &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&lt;br/&gt;
   &amp;lt;parent&amp;gt;&lt;br/&gt;
     &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&lt;br/&gt;
-    &amp;lt;artifactId&amp;gt;spark-parent_2.11&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
-    &amp;lt;version&amp;gt;2.4.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
+    &amp;lt;artifactId&amp;gt;spark-parent_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+    &amp;lt;version&amp;gt;3.0.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
     &amp;lt;relativePath&amp;gt;../../pom.xml&amp;lt;/relativePath&amp;gt;&lt;br/&gt;
   &amp;lt;/parent&amp;gt;&lt;br/&gt;
 &lt;br/&gt;
-  &amp;lt;artifactId&amp;gt;spark-network-shuffle_2.11&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+  &amp;lt;artifactId&amp;gt;spark-network-shuffle_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
   &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;&lt;br/&gt;
   &amp;lt;name&amp;gt;Spark Project Shuffle Streaming Service&amp;lt;/name&amp;gt;&lt;br/&gt;
   &amp;lt;url&amp;gt;&lt;a href=&quot;http://spark.apache.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/&lt;/a&gt;&amp;lt;/url&amp;gt;&lt;br/&gt;
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFile.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFile.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..633622b35175b&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFile.java&lt;br/&gt;
@@ -0,0 +1,47 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.network.shuffle;&lt;br/&gt;
+&lt;br/&gt;
+import java.io.IOException;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * A handle on the file used when fetching remote data to disk.  Used to ensure the lifecycle of&lt;br/&gt;
+ * writing the data, reading it back, and then cleaning it up is followed.  Specific implementations&lt;br/&gt;
+ * may also handle encryption.  The data can be read only via DownloadFileWritableChannel,&lt;br/&gt;
+ * which ensures data is not read until after the writer is closed.&lt;br/&gt;
+ */&lt;br/&gt;
+public interface DownloadFile {&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Delete the file.&lt;br/&gt;
+   *&lt;br/&gt;
+   * @return  &amp;lt;code&amp;gt;true&amp;lt;/code&amp;gt; if and only if the file or directory is&lt;br/&gt;
+   *          successfully deleted; &amp;lt;code&amp;gt;false&amp;lt;/code&amp;gt; otherwise&lt;br/&gt;
+   */&lt;br/&gt;
+  boolean delete();&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * A channel for writing data to the file.  This special channel allows access to the data for&lt;br/&gt;
+   * reading, after the channel is closed, via {@link DownloadFileWritableChannel#closeAndRead()}.&lt;br/&gt;
+   */&lt;br/&gt;
+  DownloadFileWritableChannel openForWriting() throws IOException;&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * The path of the file, intended only for debug purposes.&lt;br/&gt;
+   */&lt;br/&gt;
+  String path();&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/TempFileManager.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileManager.java&lt;br/&gt;
similarity index 75%&lt;br/&gt;
rename from common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/TempFileManager.java&lt;br/&gt;
rename to common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileManager.java&lt;br/&gt;
index 552364d274f19..c335a17ae1fe0 100644&lt;br/&gt;
&amp;#8212; a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/TempFileManager.java&lt;br/&gt;
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileManager.java&lt;br/&gt;
@@ -17,20 +17,20 @@&lt;br/&gt;
 &lt;br/&gt;
 package org.apache.spark.network.shuffle;&lt;br/&gt;
 &lt;br/&gt;
-import java.io.File;&lt;br/&gt;
+import org.apache.spark.network.util.TransportConf;&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
- * A manager to create temp block files to reduce the memory usage and also clean temp&lt;br/&gt;
- * files when they won&apos;t be used any more.&lt;br/&gt;
+ * A manager to create temp block files used when fetching remote data to reduce the memory usage.&lt;br/&gt;
+ * It will clean files when they won&apos;t be used any more.&lt;br/&gt;
  */&lt;br/&gt;
-public interface TempFileManager {&lt;br/&gt;
+public interface DownloadFileManager {
 
   /** Create a temp block file. */
-  File createTempFile();
+  DownloadFile createTempFile(TransportConf transportConf);
 
   /**
    * Register a temp file to clean up when it won&apos;t be used any more. Return whether the
    * file is registered successfully. If `false`, the caller should clean up the file by itself.
    */
-  boolean registerTempFileToClean(File file);
+  boolean registerTempFileToClean(DownloadFile file);
 }&lt;br/&gt;
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileWritableChannel.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileWritableChannel.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..dbbbac43eb741&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/DownloadFileWritableChannel.java&lt;br/&gt;
@@ -0,0 +1,30 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.network.shuffle;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.spark.network.buffer.ManagedBuffer;&lt;br/&gt;
+&lt;br/&gt;
+import java.nio.channels.WritableByteChannel;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * A channel for writing data which is fetched to disk, which allows access to the written data only&lt;br/&gt;
+ * after the writer has been closed.  Used with DownloadFile and DownloadFileManager.&lt;br/&gt;
+ */&lt;br/&gt;
+public interface DownloadFileWritableChannel extends WritableByteChannel {
+  ManagedBuffer closeAndRead();
+}&lt;br/&gt;
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleClient.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleClient.java&lt;br/&gt;
index 7ed0b6e93a7a8..e49e27ab5aa79 100644&lt;br/&gt;
&amp;#8212; a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleClient.java&lt;br/&gt;
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ExternalShuffleClient.java&lt;br/&gt;
@@ -76,7 +76,7 @@ protected void checkInit() {&lt;br/&gt;
   @Override&lt;br/&gt;
   public void init(String appId) {&lt;br/&gt;
     this.appId = appId;&lt;br/&gt;
-    TransportContext context = new TransportContext(conf, new NoOpRpcHandler(), true);&lt;br/&gt;
+    TransportContext context = new TransportContext(conf, new NoOpRpcHandler(), true, true);&lt;br/&gt;
     List&amp;lt;TransportClientBootstrap&amp;gt; bootstraps = Lists.newArrayList();&lt;br/&gt;
     if (authEnabled) {&lt;br/&gt;
       bootstraps.add(new AuthClientBootstrap(conf, appId, secretKeyHolder));&lt;br/&gt;
@@ -91,7 +91,7 @@ public void fetchBlocks(&lt;br/&gt;
       String execId,&lt;br/&gt;
       String[] blockIds,&lt;br/&gt;
       BlockFetchingListener listener,&lt;br/&gt;
-      TempFileManager tempFileManager) {&lt;br/&gt;
+      DownloadFileManager downloadFileManager) {&lt;br/&gt;
     checkInit();&lt;br/&gt;
     logger.debug(&quot;External shuffle fetch from {}:{} (executor id {})&quot;, host, port, execId);&lt;br/&gt;
     try {&lt;br/&gt;
@@ -99,7 +99,7 @@ public void fetchBlocks(&lt;br/&gt;
           (blockIds1, listener1) -&amp;gt; {
             TransportClient client = clientFactory.createClient(host, port);
             new OneForOneBlockFetcher(client, appId, execId,
-              blockIds1, listener1, conf, tempFileManager).start();
+              blockIds1, listener1, conf, downloadFileManager).start();
           };&lt;br/&gt;
 &lt;br/&gt;
       int maxRetries = conf.maxIORetries();&lt;br/&gt;
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/OneForOneBlockFetcher.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/OneForOneBlockFetcher.java&lt;br/&gt;
index 0bc571874f07c..30587023877c1 100644&lt;br/&gt;
&amp;#8212; a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/OneForOneBlockFetcher.java&lt;br/&gt;
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/OneForOneBlockFetcher.java&lt;br/&gt;
@@ -17,18 +17,13 @@&lt;br/&gt;
 &lt;br/&gt;
 package org.apache.spark.network.shuffle;&lt;br/&gt;
 &lt;br/&gt;
-import java.io.File;&lt;br/&gt;
-import java.io.FileOutputStream;&lt;br/&gt;
 import java.io.IOException;&lt;br/&gt;
 import java.nio.ByteBuffer;&lt;br/&gt;
-import java.nio.channels.Channels;&lt;br/&gt;
-import java.nio.channels.WritableByteChannel;&lt;br/&gt;
 import java.util.Arrays;&lt;br/&gt;
 &lt;br/&gt;
 import org.slf4j.Logger;&lt;br/&gt;
 import org.slf4j.LoggerFactory;&lt;br/&gt;
 &lt;br/&gt;
-import org.apache.spark.network.buffer.FileSegmentManagedBuffer;&lt;br/&gt;
 import org.apache.spark.network.buffer.ManagedBuffer;&lt;br/&gt;
 import org.apache.spark.network.client.ChunkReceivedCallback;&lt;br/&gt;
 import org.apache.spark.network.client.RpcResponseCallback;&lt;br/&gt;
@@ -58,7 +53,7 @@&lt;br/&gt;
   private final BlockFetchingListener listener;&lt;br/&gt;
   private final ChunkReceivedCallback chunkCallback;&lt;br/&gt;
   private final TransportConf transportConf;&lt;br/&gt;
-  private final TempFileManager tempFileManager;&lt;br/&gt;
+  private final DownloadFileManager downloadFileManager;&lt;br/&gt;
 &lt;br/&gt;
   private StreamHandle streamHandle = null;&lt;br/&gt;
 &lt;br/&gt;
@@ -79,14 +74,14 @@ public OneForOneBlockFetcher(&lt;br/&gt;
       String[] blockIds,&lt;br/&gt;
       BlockFetchingListener listener,&lt;br/&gt;
       TransportConf transportConf,&lt;br/&gt;
-      TempFileManager tempFileManager) {&lt;br/&gt;
+      DownloadFileManager downloadFileManager) {
     this.client = client;
     this.openMessage = new OpenBlocks(appId, execId, blockIds);
     this.blockIds = blockIds;
     this.listener = listener;
     this.chunkCallback = new ChunkCallback();
     this.transportConf = transportConf;
-    this.tempFileManager = tempFileManager;
+    this.downloadFileManager = downloadFileManager;
   }&lt;br/&gt;
 &lt;br/&gt;
   /** Callback invoked on receipt of each chunk. We equate a single chunk to a single block. */&lt;br/&gt;
@@ -125,7 +120,7 @@ public void onSuccess(ByteBuffer response) {&lt;br/&gt;
           // Immediately request all chunks &amp;#8211; we expect that the total size of the request is&lt;br/&gt;
           // reasonable due to higher level chunking in [&lt;span class=&quot;error&quot;&gt;&amp;#91;ShuffleBlockFetcherIterator&amp;#93;&lt;/span&gt;].&lt;br/&gt;
           for (int i = 0; i &amp;lt; streamHandle.numChunks; i++) {&lt;br/&gt;
-            if (tempFileManager != null) {&lt;br/&gt;
+            if (downloadFileManager != null) {
               client.stream(OneForOneStreamManager.genStreamChunkId(streamHandle.streamId, i),
                 new DownloadCallback(i));
             } else {&lt;br/&gt;
@@ -159,13 +154,13 @@ private void failRemainingBlocks(String[] failedBlockIds, Throwable e) {&lt;br/&gt;
 &lt;br/&gt;
   private class DownloadCallback implements StreamCallback {&lt;br/&gt;
 &lt;br/&gt;
-    private WritableByteChannel channel = null;&lt;br/&gt;
-    private File targetFile = null;&lt;br/&gt;
+    private DownloadFileWritableChannel channel = null;&lt;br/&gt;
+    private DownloadFile targetFile = null;&lt;br/&gt;
     private int chunkIndex;&lt;br/&gt;
 &lt;br/&gt;
     DownloadCallback(int chunkIndex) throws IOException {
-      this.targetFile = tempFileManager.createTempFile();
-      this.channel = Channels.newChannel(new FileOutputStream(targetFile));
+      this.targetFile = downloadFileManager.createTempFile(transportConf);
+      this.channel = targetFile.openForWriting();
       this.chunkIndex = chunkIndex;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -178,11 +173,8 @@ public void onData(String streamId, ByteBuffer buf) throws IOException {&lt;br/&gt;
 &lt;br/&gt;
     @Override&lt;br/&gt;
     public void onComplete(String streamId) throws IOException {&lt;br/&gt;
-      channel.close();&lt;br/&gt;
-      ManagedBuffer buffer = new FileSegmentManagedBuffer(transportConf, targetFile, 0,&lt;br/&gt;
-        targetFile.length());&lt;br/&gt;
-      listener.onBlockFetchSuccess(blockIds&lt;span class=&quot;error&quot;&gt;&amp;#91;chunkIndex&amp;#93;&lt;/span&gt;, buffer);&lt;br/&gt;
-      if (!tempFileManager.registerTempFileToClean(targetFile)) {&lt;br/&gt;
+      listener.onBlockFetchSuccess(blockIds&lt;span class=&quot;error&quot;&gt;&amp;#91;chunkIndex&amp;#93;&lt;/span&gt;, channel.closeAndRead());&lt;br/&gt;
+      if (!downloadFileManager.registerTempFileToClean(targetFile)) {
         targetFile.delete();
       }&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RetryingBlockFetcher.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RetryingBlockFetcher.java&lt;br/&gt;
index f309dda8afca6..6bf3da94030d4 100644&lt;br/&gt;
&amp;#8212; a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RetryingBlockFetcher.java&lt;br/&gt;
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/RetryingBlockFetcher.java&lt;br/&gt;
@@ -101,7 +101,7 @@ void createAndStart(String[] blockIds, BlockFetchingListener listener)&lt;br/&gt;
 &lt;br/&gt;
   public RetryingBlockFetcher(&lt;br/&gt;
       TransportConf conf,&lt;br/&gt;
-      BlockFetchStarter fetchStarter,&lt;br/&gt;
+      RetryingBlockFetcher.BlockFetchStarter fetchStarter,&lt;br/&gt;
       String[] blockIds,&lt;br/&gt;
       BlockFetchingListener listener) {&lt;br/&gt;
     this.fetchStarter = fetchStarter;&lt;br/&gt;
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java&lt;br/&gt;
index 18b04fedcac5b..62b99c40f61f9 100644&lt;br/&gt;
&amp;#8212; a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java&lt;br/&gt;
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleClient.java&lt;br/&gt;
@@ -43,7 +43,7 @@ public void init(String appId) { }&lt;br/&gt;
    * @param execId the executor id.&lt;br/&gt;
    * @param blockIds block ids to fetch.&lt;br/&gt;
    * @param listener the listener to receive block fetching status.&lt;br/&gt;
-   * @param tempFileManager TempFileManager to create and clean temp files.&lt;br/&gt;
+   * @param downloadFileManager DownloadFileManager to create and clean temp files.&lt;br/&gt;
    *                        If it&apos;s not &amp;lt;code&amp;gt;null&amp;lt;/code&amp;gt;, the remote blocks will be streamed&lt;br/&gt;
    *                        into temp shuffle files to reduce the memory usage, otherwise,&lt;br/&gt;
    *                        they will be kept in memory.&lt;br/&gt;
@@ -54,7 +54,7 @@ public abstract void fetchBlocks(&lt;br/&gt;
       String execId,&lt;br/&gt;
       String[] blockIds,&lt;br/&gt;
       BlockFetchingListener listener,&lt;br/&gt;
-      TempFileManager tempFileManager);&lt;br/&gt;
+      DownloadFileManager downloadFileManager);&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
    * Get the shuffle MetricsSet from ShuffleClient, this will be used in MetricsSystem to&lt;br/&gt;
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleIndexInformation.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleIndexInformation.java&lt;br/&gt;
index 386738ece51a6..371149bef3974 100644&lt;br/&gt;
&amp;#8212; a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleIndexInformation.java&lt;br/&gt;
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/ShuffleIndexInformation.java&lt;br/&gt;
@@ -37,14 +37,8 @@ public ShuffleIndexInformation(File indexFile) throws IOException {&lt;br/&gt;
     size = (int)indexFile.length();&lt;br/&gt;
     ByteBuffer buffer = ByteBuffer.allocate(size);&lt;br/&gt;
     offsets = buffer.asLongBuffer();&lt;br/&gt;
-    DataInputStream dis = null;&lt;br/&gt;
-    try {&lt;br/&gt;
-      dis = new DataInputStream(Files.newInputStream(indexFile.toPath()));&lt;br/&gt;
+    try (DataInputStream dis = new DataInputStream(Files.newInputStream(indexFile.toPath()))) {
       dis.readFully(buffer.array());
-    } finally {&lt;br/&gt;
-      if (dis != null) {
-        dis.close();
-      }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
diff --git a/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/SimpleDownloadFile.java b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/SimpleDownloadFile.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..670612fd6f66a&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/common/network-shuffle/src/main/java/org/apache/spark/network/shuffle/SimpleDownloadFile.java&lt;br/&gt;
@@ -0,0 +1,91 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.spark.network.shuffle;&lt;br/&gt;
+&lt;br/&gt;
+import java.io.File;&lt;br/&gt;
+import java.io.FileNotFoundException;&lt;br/&gt;
+import java.io.FileOutputStream;&lt;br/&gt;
+import java.io.IOException;&lt;br/&gt;
+import java.nio.ByteBuffer;&lt;br/&gt;
+import java.nio.channels.Channels;&lt;br/&gt;
+import java.nio.channels.WritableByteChannel;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.spark.network.buffer.FileSegmentManagedBuffer;&lt;br/&gt;
+import org.apache.spark.network.buffer.ManagedBuffer;&lt;br/&gt;
+import org.apache.spark.network.util.TransportConf;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * A DownloadFile that does not take any encryption settings into account for reading and&lt;br/&gt;
+ * writing data.&lt;br/&gt;
+ *&lt;br/&gt;
+ * This does &lt;b&gt;not&lt;/b&gt; mean the data in the file is un-encrypted &amp;#8211; it could be that the data is&lt;br/&gt;
+ * already encrypted when its written, and subsequent layer is responsible for decrypting.&lt;br/&gt;
+ */&lt;br/&gt;
+public class SimpleDownloadFile implements DownloadFile {&lt;br/&gt;
+&lt;br/&gt;
+  private final File file;&lt;br/&gt;
+  private final TransportConf transportConf;&lt;br/&gt;
+&lt;br/&gt;
+  public SimpleDownloadFile(File file, TransportConf transportConf) {
+    this.file = file;
+    this.transportConf = transportConf;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Override&lt;br/&gt;
+  public boolean delete() {
+    return file.delete();
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Override&lt;br/&gt;
+  public DownloadFileWritableChannel openForWriting() throws IOException {
+    return new SimpleDownloadWritableChannel();
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @Override&lt;br/&gt;
+  public String path() {
+    return file.getAbsolutePath();
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private class SimpleDownloadWritableChannel implements DownloadFileWritableChannel {&lt;br/&gt;
+&lt;br/&gt;
+    private final WritableByteChannel channel;&lt;br/&gt;
+&lt;br/&gt;
+    SimpleDownloadWritableChannel() throws FileNotFoundException {
+      channel = Channels.newChannel(new FileOutputStream(file));
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public ManagedBuffer closeAndRead() {
+      return new FileSegmentManagedBuffer(transportConf, file, 0, file.length());
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public int write(ByteBuffer src) throws IOException {
+      return channel.write(src);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public boolean isOpen() {
+      return channel.isOpen();
+    }&lt;br/&gt;
+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public void close() throws IOException {
+      channel.close();
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolverSuite.java b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolverSuite.java&lt;br/&gt;
index d2072a54fa415..459629c5f05fe 100644&lt;br/&gt;
&amp;#8212; a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolverSuite.java&lt;br/&gt;
+++ b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleBlockResolverSuite.java&lt;br/&gt;
@@ -98,19 +98,19 @@ public void testSortShuffleBlocks() throws IOException {&lt;br/&gt;
     resolver.registerExecutor(&quot;app0&quot;, &quot;exec0&quot;,&lt;br/&gt;
       dataContext.createExecutorInfo(SORT_MANAGER));&lt;br/&gt;
 &lt;br/&gt;
-    InputStream block0Stream =&lt;br/&gt;
-      resolver.getBlockData(&quot;app0&quot;, &quot;exec0&quot;, 0, 0, 0).createInputStream();&lt;br/&gt;
-    String block0 = CharStreams.toString(&lt;br/&gt;
-        new InputStreamReader(block0Stream, StandardCharsets.UTF_8));&lt;br/&gt;
-    block0Stream.close();&lt;br/&gt;
-    assertEquals(sortBlock0, block0);&lt;br/&gt;
-&lt;br/&gt;
-    InputStream block1Stream =&lt;br/&gt;
-      resolver.getBlockData(&quot;app0&quot;, &quot;exec0&quot;, 0, 0, 1).createInputStream();&lt;br/&gt;
-    String block1 = CharStreams.toString(&lt;br/&gt;
-        new InputStreamReader(block1Stream, StandardCharsets.UTF_8));&lt;br/&gt;
-    block1Stream.close();&lt;br/&gt;
-    assertEquals(sortBlock1, block1);&lt;br/&gt;
+    try (InputStream block0Stream = resolver.getBlockData(&lt;br/&gt;
+        &quot;app0&quot;, &quot;exec0&quot;, 0, 0, 0).createInputStream()) {
+      String block0 =
+        CharStreams.toString(new InputStreamReader(block0Stream, StandardCharsets.UTF_8));
+      assertEquals(sortBlock0, block0);
+    }&lt;br/&gt;
+&lt;br/&gt;
+    try (InputStream block1Stream = resolver.getBlockData(&lt;br/&gt;
+        &quot;app0&quot;, &quot;exec0&quot;, 0, 0, 1).createInputStream()) {
+      String block1 =
+        CharStreams.toString(new InputStreamReader(block1Stream, StandardCharsets.UTF_8));
+      assertEquals(sortBlock1, block1);
+    }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   @Test&lt;br/&gt;
@@ -149,7 +149,7 @@ public void testNormalizeAndInternPathname() {&lt;br/&gt;
 &lt;br/&gt;
   private void assertPathsMatch(String p1, String p2, String p3, String expectedPathname) {&lt;br/&gt;
     String normPathname =&lt;br/&gt;
-        ExternalShuffleBlockResolver.createNormalizedInternedPathname(p1, p2, p3);&lt;br/&gt;
+      ExternalShuffleBlockResolver.createNormalizedInternedPathname(p1, p2, p3);&lt;br/&gt;
     assertEquals(expectedPathname, normPathname);&lt;br/&gt;
     File file = new File(normPathname);&lt;br/&gt;
     String returnedPath = file.getPath();&lt;br/&gt;
diff --git a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleIntegrationSuite.java b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleIntegrationSuite.java&lt;br/&gt;
index a6a1b8d0ac3f1..526b96b364473 100644&lt;br/&gt;
&amp;#8212; a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleIntegrationSuite.java&lt;br/&gt;
+++ b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleIntegrationSuite.java&lt;br/&gt;
@@ -133,37 +133,37 @@ private FetchResult fetchBlocks(&lt;br/&gt;
 &lt;br/&gt;
     final Semaphore requestsRemaining = new Semaphore(0);&lt;br/&gt;
 &lt;br/&gt;
-    ExternalShuffleClient client = new ExternalShuffleClient(clientConf, null, false, 5000);&lt;br/&gt;
-    client.init(APP_ID);&lt;br/&gt;
-    client.fetchBlocks(TestUtils.getLocalHost(), port, execId, blockIds,&lt;br/&gt;
-      new BlockFetchingListener() {&lt;br/&gt;
-        @Override&lt;br/&gt;
-        public void onBlockFetchSuccess(String blockId, ManagedBuffer data) {&lt;br/&gt;
-          synchronized (this) {&lt;br/&gt;
-            if (!res.successBlocks.contains(blockId) &amp;amp;&amp;amp; !res.failedBlocks.contains(blockId)) {&lt;br/&gt;
-              data.retain();&lt;br/&gt;
-              res.successBlocks.add(blockId);&lt;br/&gt;
-              res.buffers.add(data);&lt;br/&gt;
-              requestsRemaining.release();&lt;br/&gt;
+    try (ExternalShuffleClient client = new ExternalShuffleClient(clientConf, null, false, 5000)) {&lt;br/&gt;
+      client.init(APP_ID);&lt;br/&gt;
+      client.fetchBlocks(TestUtils.getLocalHost(), port, execId, blockIds,&lt;br/&gt;
+        new BlockFetchingListener() {&lt;br/&gt;
+          @Override&lt;br/&gt;
+          public void onBlockFetchSuccess(String blockId, ManagedBuffer data) {&lt;br/&gt;
+            synchronized (this) {&lt;br/&gt;
+              if (!res.successBlocks.contains(blockId) &amp;amp;&amp;amp; !res.failedBlocks.contains(blockId)) {
+                data.retain();
+                res.successBlocks.add(blockId);
+                res.buffers.add(data);
+                requestsRemaining.release();
+              }&lt;br/&gt;
             }&lt;br/&gt;
           }&lt;br/&gt;
-        }&lt;br/&gt;
-&lt;br/&gt;
-        @Override&lt;br/&gt;
-        public void onBlockFetchFailure(String blockId, Throwable exception) {&lt;br/&gt;
-          synchronized (this) {&lt;br/&gt;
-            if (!res.successBlocks.contains(blockId) &amp;amp;&amp;amp; !res.failedBlocks.contains(blockId)) {&lt;br/&gt;
-              res.failedBlocks.add(blockId);&lt;br/&gt;
-              requestsRemaining.release();&lt;br/&gt;
+&lt;br/&gt;
+          @Override&lt;br/&gt;
+          public void onBlockFetchFailure(String blockId, Throwable exception) {&lt;br/&gt;
+            synchronized (this) {&lt;br/&gt;
+              if (!res.successBlocks.contains(blockId) &amp;amp;&amp;amp; !res.failedBlocks.contains(blockId)) {
+                res.failedBlocks.add(blockId);
+                requestsRemaining.release();
+              }&lt;br/&gt;
             }&lt;br/&gt;
           }&lt;br/&gt;
-        }&lt;br/&gt;
-      }, null);&lt;br/&gt;
+        }, null);&lt;br/&gt;
 &lt;br/&gt;
-    if (!requestsRemaining.tryAcquire(blockIds.length, 5, TimeUnit.SECONDS)) {&lt;br/&gt;
-      fail(&quot;Timeout getting response from the server&quot;);&lt;br/&gt;
+      if (!requestsRemaining.tryAcquire(blockIds.length, 5, TimeUnit.SECONDS)) {+        fail(&quot;Timeout getting response from the server&quot;);+      }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;client.close();&lt;br/&gt;
     return res;&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleSecuritySuite.java b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleSecuritySuite.java&lt;br/&gt;
index 16bad9f1b319d..82caf392b821b 100644&lt;br/&gt;
&amp;#8212; a/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleSecuritySuite.java&lt;br/&gt;
+++ b/common/network-shuffle/src/test/java/org/apache/spark/network/shuffle/ExternalShuffleSecuritySuite.java&lt;br/&gt;
@@ -96,14 +96,16 @@ private void validate(String appId, String secretKey, boolean encrypt)&lt;br/&gt;
         ImmutableMap.of(&quot;spark.authenticate.enableSaslEncryption&quot;, &quot;true&quot;)));&lt;br/&gt;
     }&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ExternalShuffleClient client =&lt;/li&gt;
	&lt;li&gt;new ExternalShuffleClient(testConf, new TestSecretKeyHolder(appId, secretKey), true, 5000);&lt;/li&gt;
	&lt;li&gt;client.init(appId);&lt;/li&gt;
	&lt;li&gt;// Registration either succeeds or throws an exception.&lt;/li&gt;
	&lt;li&gt;client.registerWithShuffleServer(TestUtils.getLocalHost(), server.getPort(), &quot;exec0&quot;,&lt;/li&gt;
	&lt;li&gt;new ExecutorShuffleInfo(new String&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, 0,&lt;/li&gt;
	&lt;li&gt;&quot;org.apache.spark.shuffle.sort.SortShuffleManager&quot;));&lt;/li&gt;
	&lt;li&gt;client.close();&lt;br/&gt;
+    try (ExternalShuffleClient client =&lt;br/&gt;
+        new ExternalShuffleClient(&lt;br/&gt;
+          testConf, new TestSecretKeyHolder(appId, secretKey), true, 5000)) 
{
+      client.init(appId);
+      // Registration either succeeds or throws an exception.
+      client.registerWithShuffleServer(TestUtils.getLocalHost(), server.getPort(), &quot;exec0&quot;,
+        new ExecutorShuffleInfo(
+          new String[0], 0, &quot;org.apache.spark.shuffle.sort.SortShuffleManager&quot;)
+      );
+    }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /** Provides a secret key holder which always returns the given secret key, for a single appId. */&lt;br/&gt;
diff --git a/common/network-yarn/pom.xml b/common/network-yarn/pom.xml&lt;br/&gt;
index 564e6583c909e..55cdc3140aa08 100644&lt;br/&gt;
&amp;#8212; a/common/network-yarn/pom.xml&lt;br/&gt;
+++ b/common/network-yarn/pom.xml&lt;br/&gt;
@@ -21,12 +21,12 @@&lt;br/&gt;
   &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&lt;br/&gt;
   &amp;lt;parent&amp;gt;&lt;br/&gt;
     &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-parent_2.11&amp;lt;/artifactId&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;version&amp;gt;2.4.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
+    &amp;lt;artifactId&amp;gt;spark-parent_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+    &amp;lt;version&amp;gt;3.0.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
     &amp;lt;relativePath&amp;gt;../../pom.xml&amp;lt;/relativePath&amp;gt;&lt;br/&gt;
   &amp;lt;/parent&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-network-yarn_2.11&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+  &amp;lt;artifactId&amp;gt;spark-network-yarn_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
   &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;&lt;br/&gt;
   &amp;lt;name&amp;gt;Spark Project YARN Shuffle Service&amp;lt;/name&amp;gt;&lt;br/&gt;
   &amp;lt;url&amp;gt;&lt;a href=&quot;http://spark.apache.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/&lt;/a&gt;&amp;lt;/url&amp;gt;&lt;br/&gt;
diff --git a/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java b/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java&lt;br/&gt;
index d8b2ed6b5dc7b..72ae1a1295236 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java&lt;br/&gt;
+++ b/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleService.java&lt;br/&gt;
@@ -35,6 +35,8 @@&lt;br/&gt;
 import org.apache.hadoop.fs.FileSystem;&lt;br/&gt;
 import org.apache.hadoop.fs.Path;&lt;br/&gt;
 import org.apache.hadoop.fs.permission.FsPermission;&lt;br/&gt;
+import org.apache.hadoop.metrics2.impl.MetricsSystemImpl;&lt;br/&gt;
+import org.apache.hadoop.metrics2.lib.DefaultMetricsSystem;&lt;br/&gt;
 import org.apache.hadoop.yarn.api.records.ContainerId;&lt;br/&gt;
 import org.apache.hadoop.yarn.server.api.*;&lt;br/&gt;
 import org.apache.spark.network.util.LevelDBProvider;&lt;br/&gt;
@@ -168,6 +170,15 @@ protected void serviceInit(Configuration conf) throws Exception {&lt;br/&gt;
       TransportConf transportConf = new TransportConf(&quot;shuffle&quot;, new HadoopConfigProvider(conf));&lt;br/&gt;
       blockHandler = new ExternalShuffleBlockHandler(transportConf, registeredExecutorFile);&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+      // register metrics on the block handler into the Node Manager&apos;s metrics system.&lt;br/&gt;
+      YarnShuffleServiceMetrics serviceMetrics =&lt;br/&gt;
+        new YarnShuffleServiceMetrics(blockHandler.getAllMetrics());&lt;br/&gt;
+&lt;br/&gt;
+      MetricsSystemImpl metricsSystem = (MetricsSystemImpl) DefaultMetricsSystem.instance();&lt;br/&gt;
+      metricsSystem.register(&lt;br/&gt;
+        &quot;sparkShuffleService&quot;, &quot;Metrics on the Spark Shuffle Service&quot;, serviceMetrics);&lt;br/&gt;
+      logger.info(&quot;Registered metrics with Hadoop&apos;s DefaultMetricsSystem&quot;);&lt;br/&gt;
+&lt;br/&gt;
       // If authentication is enabled, set up the shuffle server to use a&lt;br/&gt;
       // special RPC handler that filters out unauthenticated fetch requests&lt;br/&gt;
       List&amp;lt;TransportServerBootstrap&amp;gt; bootstraps = Lists.newArrayList();&lt;br/&gt;
diff --git a/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleServiceMetrics.java b/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleServiceMetrics.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..3e4d479b862b3&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/common/network-yarn/src/main/java/org/apache/spark/network/yarn/YarnShuffleServiceMetrics.java&lt;br/&gt;
@@ -0,0 +1,137 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.network.yarn;&lt;br/&gt;
+&lt;br/&gt;
+import java.util.Map;&lt;br/&gt;
+&lt;br/&gt;
+import com.codahale.metrics.*;&lt;br/&gt;
+import org.apache.hadoop.metrics2.MetricsCollector;&lt;br/&gt;
+import org.apache.hadoop.metrics2.MetricsInfo;&lt;br/&gt;
+import org.apache.hadoop.metrics2.MetricsRecordBuilder;&lt;br/&gt;
+import org.apache.hadoop.metrics2.MetricsSource;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Forward &lt;/p&gt;
{@link org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.ShuffleMetrics}&lt;br/&gt;
+ * to hadoop metrics system.&lt;br/&gt;
+ * NodeManager by default exposes JMX endpoint where can be collected.&lt;br/&gt;
+ */&lt;br/&gt;
+class YarnShuffleServiceMetrics implements MetricsSource {&lt;br/&gt;
+&lt;br/&gt;
+  private final MetricSet metricSet;&lt;br/&gt;
+&lt;br/&gt;
+  YarnShuffleServiceMetrics(MetricSet metricSet) {
+    this.metricSet = metricSet;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Get metrics from the source&lt;br/&gt;
+   *&lt;br/&gt;
+   * @param collector to contain the resulting metrics snapshot&lt;br/&gt;
+   * @param all       if true, return all metrics even if unchanged.&lt;br/&gt;
+   */&lt;br/&gt;
+  @Override&lt;br/&gt;
+  public void getMetrics(MetricsCollector collector, boolean all) {&lt;br/&gt;
+    MetricsRecordBuilder metricsRecordBuilder = collector.addRecord(&quot;sparkShuffleService&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    for (Map.Entry&amp;lt;String, Metric&amp;gt; entry : metricSet.getMetrics().entrySet()) {
+      collectMetric(metricsRecordBuilder, entry.getKey(), entry.getValue());
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * The metric types used in&lt;br/&gt;
+   * {@link org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.ShuffleMetrics}
&lt;p&gt;.&lt;br/&gt;
+   * Visible for testing.&lt;br/&gt;
+   */&lt;br/&gt;
+  public static void collectMetric(&lt;br/&gt;
+    MetricsRecordBuilder metricsRecordBuilder, String name, Metric metric) {&lt;br/&gt;
+&lt;br/&gt;
+    if (metric instanceof Timer) &lt;/p&gt;
{
+      Timer t = (Timer) metric;
+      metricsRecordBuilder
+        .addCounter(new ShuffleServiceMetricsInfo(name + &quot;_count&quot;, &quot;Count of timer &quot; + name),
+          t.getCount())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + &quot;_rate15&quot;, &quot;15 minute rate of timer &quot; + name),
+          t.getFifteenMinuteRate())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + &quot;_rate5&quot;, &quot;5 minute rate of timer &quot; + name),
+          t.getFiveMinuteRate())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + &quot;_rate1&quot;, &quot;1 minute rate of timer &quot; + name),
+          t.getOneMinuteRate())
+        .addGauge(new ShuffleServiceMetricsInfo(name + &quot;_rateMean&quot;, &quot;Mean rate of timer &quot; + name),
+          t.getMeanRate());
+    }
&lt;p&gt; else if (metric instanceof Meter) &lt;/p&gt;
{
+      Meter m = (Meter) metric;
+      metricsRecordBuilder
+        .addCounter(new ShuffleServiceMetricsInfo(name + &quot;_count&quot;, &quot;Count of meter &quot; + name),
+          m.getCount())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + &quot;_rate15&quot;, &quot;15 minute rate of meter &quot; + name),
+          m.getFifteenMinuteRate())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + &quot;_rate5&quot;, &quot;5 minute rate of meter &quot; + name),
+          m.getFiveMinuteRate())
+        .addGauge(
+          new ShuffleServiceMetricsInfo(name + &quot;_rate1&quot;, &quot;1 minute rate of meter &quot; + name),
+          m.getOneMinuteRate())
+        .addGauge(new ShuffleServiceMetricsInfo(name + &quot;_rateMean&quot;, &quot;Mean rate of meter &quot; + name),
+          m.getMeanRate());
+    }
&lt;p&gt; else if (metric instanceof Gauge) {&lt;br/&gt;
+      final Object gaugeValue = ((Gauge) metric).getValue();&lt;br/&gt;
+      if (gaugeValue instanceof Integer) &lt;/p&gt;
{
+        metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfo(name), (Integer) gaugeValue);
+      }
&lt;p&gt; else if (gaugeValue instanceof Long) &lt;/p&gt;
{
+        metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfo(name), (Long) gaugeValue);
+      }
&lt;p&gt; else if (gaugeValue instanceof Float) &lt;/p&gt;
{
+        metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfo(name), (Float) gaugeValue);
+      }
&lt;p&gt; else if (gaugeValue instanceof Double) &lt;/p&gt;
{
+        metricsRecordBuilder.addGauge(getShuffleServiceMetricsInfo(name), (Double) gaugeValue);
+      }
&lt;p&gt; else &lt;/p&gt;
{
+        throw new IllegalStateException(
+                &quot;Not supported class type of metric[&quot; + name + &quot;] for value &quot; + gaugeValue);
+      }
&lt;p&gt;+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private static MetricsInfo getShuffleServiceMetricsInfo(String name) &lt;/p&gt;
{
+    return new ShuffleServiceMetricsInfo(name, &quot;Value of gauge &quot; + name);
+  }
&lt;p&gt;+&lt;br/&gt;
+  private static class ShuffleServiceMetricsInfo implements MetricsInfo {&lt;br/&gt;
+&lt;br/&gt;
+    private final String name;&lt;br/&gt;
+    private final String description;&lt;br/&gt;
+&lt;br/&gt;
+    ShuffleServiceMetricsInfo(String name, String description) &lt;/p&gt;
{
+      this.name = name;
+      this.description = description;
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public String name() &lt;/p&gt;
{
+      return name;
+    }
&lt;p&gt;+&lt;br/&gt;
+    @Override&lt;br/&gt;
+    public String description() &lt;/p&gt;
{
+      return description;
+    }
&lt;p&gt;+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/common/sketch/pom.xml b/common/sketch/pom.xml&lt;br/&gt;
index 2f04abe8c7e88..3c3c0d2d96a1c 100644&lt;br/&gt;
&amp;#8212; a/common/sketch/pom.xml&lt;br/&gt;
+++ b/common/sketch/pom.xml&lt;br/&gt;
@@ -21,12 +21,12 @@&lt;br/&gt;
   &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&lt;br/&gt;
   &amp;lt;parent&amp;gt;&lt;br/&gt;
     &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-parent_2.11&amp;lt;/artifactId&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;version&amp;gt;2.4.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
+    &amp;lt;artifactId&amp;gt;spark-parent_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+    &amp;lt;version&amp;gt;3.0.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
     &amp;lt;relativePath&amp;gt;../../pom.xml&amp;lt;/relativePath&amp;gt;&lt;br/&gt;
   &amp;lt;/parent&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-sketch_2.11&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+  &amp;lt;artifactId&amp;gt;spark-sketch_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
   &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;&lt;br/&gt;
   &amp;lt;name&amp;gt;Spark Project Sketch&amp;lt;/name&amp;gt;&lt;br/&gt;
   &amp;lt;url&amp;gt;&lt;a href=&quot;http://spark.apache.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/&lt;/a&gt;&amp;lt;/url&amp;gt;&lt;br/&gt;
diff --git a/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketch.java b/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketch.java&lt;br/&gt;
index f7c22dddb8cc0..06a248c9a27c2 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketch.java&lt;br/&gt;
+++ b/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketch.java&lt;br/&gt;
@@ -191,10 +191,9 @@ public static CountMinSketch readFrom(InputStream in) throws IOException {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Reads in a 
{@link CountMinSketch}
&lt;p&gt; from a byte array.&lt;br/&gt;
    */&lt;br/&gt;
   public static CountMinSketch readFrom(byte[] bytes) throws IOException {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;InputStream in = new ByteArrayInputStream(bytes);&lt;/li&gt;
	&lt;li&gt;CountMinSketch cms = readFrom(in);&lt;/li&gt;
	&lt;li&gt;in.close();&lt;/li&gt;
	&lt;li&gt;return cms;&lt;br/&gt;
+    try (InputStream in = new ByteArrayInputStream(bytes)) 
{
+      return readFrom(in);
+    }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
diff --git a/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketchImpl.java b/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketchImpl.java&lt;br/&gt;
index fd1906d2e5ae9..b78c1677a1213 100644&lt;br/&gt;
&amp;#8212; a/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketchImpl.java&lt;br/&gt;
+++ b/common/sketch/src/main/java/org/apache/spark/util/sketch/CountMinSketchImpl.java&lt;br/&gt;
@@ -322,10 +322,10 @@ public void writeTo(OutputStream out) throws IOException {&lt;/p&gt;

&lt;p&gt;   @Override&lt;br/&gt;
   public byte[] toByteArray() throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ByteArrayOutputStream out = new ByteArrayOutputStream();&lt;/li&gt;
	&lt;li&gt;writeTo(out);&lt;/li&gt;
	&lt;li&gt;out.close();&lt;/li&gt;
	&lt;li&gt;return out.toByteArray();&lt;br/&gt;
+    try (ByteArrayOutputStream out = new ByteArrayOutputStream()) 
{
+      writeTo(out);
+      return out.toByteArray();
+    }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   public static CountMinSketchImpl readFrom(InputStream in) throws IOException {&lt;br/&gt;
diff --git a/common/tags/pom.xml b/common/tags/pom.xml&lt;br/&gt;
index ba127408e1c59..883b73a69c9de 100644&lt;br/&gt;
&amp;#8212; a/common/tags/pom.xml&lt;br/&gt;
+++ b/common/tags/pom.xml&lt;br/&gt;
@@ -21,12 +21,12 @@&lt;br/&gt;
   &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&lt;br/&gt;
   &amp;lt;parent&amp;gt;&lt;br/&gt;
     &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-parent_2.11&amp;lt;/artifactId&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;version&amp;gt;2.4.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
+    &amp;lt;artifactId&amp;gt;spark-parent_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+    &amp;lt;version&amp;gt;3.0.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
     &amp;lt;relativePath&amp;gt;../../pom.xml&amp;lt;/relativePath&amp;gt;&lt;br/&gt;
   &amp;lt;/parent&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-tags_2.11&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+  &amp;lt;artifactId&amp;gt;spark-tags_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
   &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;&lt;br/&gt;
   &amp;lt;name&amp;gt;Spark Project Tags&amp;lt;/name&amp;gt;&lt;br/&gt;
   &amp;lt;url&amp;gt;&lt;a href=&quot;http://spark.apache.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/&lt;/a&gt;&amp;lt;/url&amp;gt;&lt;br/&gt;
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java b/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java&lt;br/&gt;
index 0ecef6db0e039..890f2faca28b0 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java&lt;br/&gt;
+++ b/common/tags/src/main/java/org/apache/spark/annotation/DeveloperApi.java&lt;br/&gt;
@@ -29,6 +29,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;of the known issue that Scaladoc displays only either the annotation or the comment, whichever&lt;/li&gt;
	&lt;li&gt;comes first.&lt;br/&gt;
  */&lt;br/&gt;
+@Documented&lt;br/&gt;
 @Retention(RetentionPolicy.RUNTIME)&lt;br/&gt;
 @Target(
{ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,
         ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE})&lt;br/&gt;
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/Evolving.java b/common/tags/src/main/java/org/apache/spark/annotation/Evolving.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..87e8948f204ff&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/common/tags/src/main/java/org/apache/spark/annotation/Evolving.java&lt;br/&gt;
@@ -0,0 +1,30 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.annotation;&lt;br/&gt;
+&lt;br/&gt;
+import java.lang.annotation.*;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * APIs that are meant to evolve towards becoming stable APIs, but are not stable APIs yet.&lt;br/&gt;
+ * Evolving interfaces can change from one feature release to another release (i.e. 2.1 to 2.2).&lt;br/&gt;
+ */&lt;br/&gt;
+@Documented&lt;br/&gt;
+@Retention(RetentionPolicy.RUNTIME)&lt;br/&gt;
+@Target({ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,
+  ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE})&lt;br/&gt;
+public @interface Evolving {}&lt;br/&gt;
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/Experimental.java b/common/tags/src/main/java/org/apache/spark/annotation/Experimental.java&lt;br/&gt;
index ff8120291455f..96875920cd9c3 100644&lt;br/&gt;
&amp;#8212; a/common/tags/src/main/java/org/apache/spark/annotation/Experimental.java&lt;br/&gt;
+++ b/common/tags/src/main/java/org/apache/spark/annotation/Experimental.java&lt;br/&gt;
@@ -30,6 +30,7 @@&lt;br/&gt;
  * of the known issue that Scaladoc displays only either the annotation or the comment, whichever&lt;br/&gt;
  * comes first.&lt;br/&gt;
  */&lt;br/&gt;
+@Documented&lt;br/&gt;
 @Retention(RetentionPolicy.RUNTIME)&lt;br/&gt;
 @Target({ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,         ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE}
&lt;p&gt;)&lt;br/&gt;
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/InterfaceStability.java b/common/tags/src/main/java/org/apache/spark/annotation/InterfaceStability.java&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 323098f69c6e1..0000000000000&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/tags/src/main/java/org/apache/spark/annotation/InterfaceStability.java&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,49 +0,0 @@&lt;br/&gt;
-/*&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements.  See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License.  You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-&lt;br/&gt;
-package org.apache.spark.annotation;&lt;br/&gt;
-&lt;br/&gt;
-import java.lang.annotation.Documented;&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;/li&gt;
	&lt;li&gt;* Annotation to inform users of how much to rely on a particular package,&lt;/li&gt;
	&lt;li&gt;* class or method not changing over time.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-public class InterfaceStability {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Stable APIs that retain source and binary compatibility within a major release.&lt;/li&gt;
	&lt;li&gt;* These interfaces can change from one major release to another major release&lt;/li&gt;
	&lt;li&gt;* (e.g. from 1.0 to 2.0).&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;@Documented&lt;/li&gt;
	&lt;li&gt;public @interface Stable {};&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* APIs that are meant to evolve towards becoming stable APIs, but are not stable APIs yet.&lt;/li&gt;
	&lt;li&gt;* Evolving interfaces can change from one feature release to another release (i.e. 2.1 to 2.2).&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;@Documented&lt;/li&gt;
	&lt;li&gt;public @interface Evolving {};&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Unstable APIs, with no guarantee on stability.&lt;/li&gt;
	&lt;li&gt;* Classes that are unannotated are considered Unstable.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;@Documented&lt;/li&gt;
	&lt;li&gt;public @interface Unstable {};&lt;br/&gt;
-}&lt;br/&gt;
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/Private.java b/common/tags/src/main/java/org/apache/spark/annotation/Private.java&lt;br/&gt;
index 9082fcf0c84bc..a460d608ae16b 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/tags/src/main/java/org/apache/spark/annotation/Private.java&lt;br/&gt;
+++ b/common/tags/src/main/java/org/apache/spark/annotation/Private.java&lt;br/&gt;
@@ -17,10 +17,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; package org.apache.spark.annotation;&lt;/p&gt;

&lt;p&gt;-import java.lang.annotation.ElementType;&lt;br/&gt;
-import java.lang.annotation.Retention;&lt;br/&gt;
-import java.lang.annotation.RetentionPolicy;&lt;br/&gt;
-import java.lang.annotation.Target;&lt;br/&gt;
+import java.lang.annotation.*;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A class that is considered private to the internals of Spark &amp;#8211; there is a high-likelihood&lt;br/&gt;
@@ -35,6 +32,7 @@&lt;/li&gt;
	&lt;li&gt;of the known issue that Scaladoc displays only either the annotation or the comment, whichever&lt;/li&gt;
	&lt;li&gt;comes first.&lt;br/&gt;
  */&lt;br/&gt;
+@Documented&lt;br/&gt;
 @Retention(RetentionPolicy.RUNTIME)&lt;br/&gt;
 @Target(
{ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,
         ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE}
&lt;p&gt;)&lt;br/&gt;
diff --git a/external/flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink/SparkSinkThreadFactory.scala b/common/tags/src/main/java/org/apache/spark/annotation/Stable.java&lt;br/&gt;
similarity index 61%&lt;br/&gt;
rename from external/flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink/SparkSinkThreadFactory.scala&lt;br/&gt;
rename to common/tags/src/main/java/org/apache/spark/annotation/Stable.java&lt;br/&gt;
index 845fc8debda75..b198bfbe91e10 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/external/flume-sink/src/main/scala/org/apache/spark/streaming/flume/sink/SparkSinkThreadFactory.scala&lt;br/&gt;
+++ b/common/tags/src/main/java/org/apache/spark/annotation/Stable.java&lt;br/&gt;
@@ -14,22 +14,18 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
-package org.apache.spark.streaming.flume.sink&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-import java.util.concurrent.ThreadFactory&lt;br/&gt;
-import java.util.concurrent.atomic.AtomicLong&lt;br/&gt;
+package org.apache.spark.annotation;&lt;br/&gt;
+&lt;br/&gt;
+import java.lang.annotation.*;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Thread factory that generates daemon threads with a specified name format.&lt;br/&gt;
+ * Stable APIs that retain source and binary compatibility within a major release.&lt;br/&gt;
+ * These interfaces can change from one major release to another major release&lt;br/&gt;
+ * (e.g. from 1.0 to 2.0).&lt;br/&gt;
  */&lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;sink&amp;#93;&lt;/span&gt; class SparkSinkThreadFactory(nameFormat: String) extends ThreadFactory {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private val threadId = new AtomicLong()&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;override def newThread(r: Runnable): Thread = 
{
-    val t = new Thread(r, nameFormat.format(threadId.incrementAndGet()))
-    t.setDaemon(true)
-    t
-  }
&lt;p&gt;-&lt;br/&gt;
-}&lt;br/&gt;
+@Documented&lt;br/&gt;
+@Retention(RetentionPolicy.RUNTIME)&lt;br/&gt;
+@Target(&lt;/p&gt;
{ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,
+  ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE})&lt;br/&gt;
+public @interface Stable {}&lt;br/&gt;
diff --git a/common/tags/src/main/java/org/apache/spark/annotation/Unstable.java b/common/tags/src/main/java/org/apache/spark/annotation/Unstable.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..88ee72125b23f&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/common/tags/src/main/java/org/apache/spark/annotation/Unstable.java&lt;br/&gt;
@@ -0,0 +1,30 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.annotation;&lt;br/&gt;
+&lt;br/&gt;
+import java.lang.annotation.*;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Unstable APIs, with no guarantee on stability.&lt;br/&gt;
+ * Classes that are unannotated are considered Unstable.&lt;br/&gt;
+ */&lt;br/&gt;
+@Documented&lt;br/&gt;
+@Retention(RetentionPolicy.RUNTIME)&lt;br/&gt;
+@Target({ElementType.TYPE, ElementType.FIELD, ElementType.METHOD, ElementType.PARAMETER,+  ElementType.CONSTRUCTOR, ElementType.LOCAL_VARIABLE, ElementType.PACKAGE}
&lt;p&gt;)&lt;br/&gt;
+public @interface Unstable {}&lt;br/&gt;
diff --git a/common/unsafe/pom.xml b/common/unsafe/pom.xml&lt;br/&gt;
index 1527854730394..93a4f67fd23f2 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/pom.xml&lt;br/&gt;
+++ b/common/unsafe/pom.xml&lt;br/&gt;
@@ -21,12 +21,12 @@&lt;br/&gt;
   &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&lt;br/&gt;
   &amp;lt;parent&amp;gt;&lt;br/&gt;
     &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-parent_2.11&amp;lt;/artifactId&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;version&amp;gt;2.4.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
+    &amp;lt;artifactId&amp;gt;spark-parent_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+    &amp;lt;version&amp;gt;3.0.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
     &amp;lt;relativePath&amp;gt;../../pom.xml&amp;lt;/relativePath&amp;gt;&lt;br/&gt;
   &amp;lt;/parent&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-unsafe_2.11&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+  &amp;lt;artifactId&amp;gt;spark-unsafe_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
   &amp;lt;packaging&amp;gt;jar&amp;lt;/packaging&amp;gt;&lt;br/&gt;
   &amp;lt;name&amp;gt;Spark Project Unsafe&amp;lt;/name&amp;gt;&lt;br/&gt;
   &amp;lt;url&amp;gt;&lt;a href=&quot;http://spark.apache.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/&lt;/a&gt;&amp;lt;/url&amp;gt;&lt;br/&gt;
@@ -89,6 +89,11 @@&lt;br/&gt;
       &amp;lt;artifactId&amp;gt;commons-lang3&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
       &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
     &amp;lt;/dependency&amp;gt;&lt;br/&gt;
+    &amp;lt;dependency&amp;gt;&lt;br/&gt;
+      &amp;lt;groupId&amp;gt;org.apache.commons&amp;lt;/groupId&amp;gt;&lt;br/&gt;
+      &amp;lt;artifactId&amp;gt;commons-text&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+      &amp;lt;scope&amp;gt;test&amp;lt;/scope&amp;gt;&lt;br/&gt;
+    &amp;lt;/dependency&amp;gt;&lt;br/&gt;
   &amp;lt;/dependencies&amp;gt;&lt;br/&gt;
   &amp;lt;build&amp;gt;&lt;br/&gt;
     &amp;lt;outputDirectory&amp;gt;target/scala-${scala.binary.version}/classes&amp;lt;/outputDirectory&amp;gt;&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/sql/catalyst/expressions/HiveHasher.java b/common/unsafe/src/main/java/org/apache/spark/sql/catalyst/expressions/HiveHasher.java&lt;br/&gt;
index 62b75ae8aa01d..73577437ac506 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/src/main/java/org/apache/spark/sql/catalyst/expressions/HiveHasher.java&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/sql/catalyst/expressions/HiveHasher.java&lt;br/&gt;
@@ -17,8 +17,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; package org.apache.spark.sql.catalyst.expressions;&lt;/p&gt;

&lt;p&gt;-import org.apache.spark.unsafe.memory.MemoryBlock;&lt;br/&gt;
-import org.apache.spark.unsafe.types.UTF8String;&lt;br/&gt;
+import org.apache.spark.unsafe.Platform;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Simulates Hive&apos;s hashing function from Hive v1.2.1&lt;br/&gt;
@@ -39,21 +38,12 @@ public static int hashLong(long input) 
{
     return (int) ((input &amp;gt;&amp;gt;&amp;gt; 32) ^ input);
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static int hashUnsafeBytesBlock(MemoryBlock mb) {&lt;/li&gt;
	&lt;li&gt;long lengthInBytes = mb.size();&lt;br/&gt;
+  public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes) {&lt;br/&gt;
     assert (lengthInBytes &amp;gt;= 0): &quot;lengthInBytes cannot be negative&quot;;&lt;br/&gt;
     int result = 0;&lt;/li&gt;
	&lt;li&gt;for (long i = 0; i &amp;lt; lengthInBytes; i++) {&lt;/li&gt;
	&lt;li&gt;result = (result * 31) + (int) mb.getByte&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
+    for (int i = 0; i &amp;lt; lengthInBytes; i++) 
{
+      result = (result * 31) + (int) Platform.getByte(base, offset + i);
     }
&lt;p&gt;     return result;&lt;br/&gt;
   }&lt;br/&gt;
-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes) 
{
-    return hashUnsafeBytesBlock(MemoryBlock.allocateFromObject(base, offset, lengthInBytes));
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static int hashUTF8String(UTF8String str) 
{
-    return hashUnsafeBytesBlock(str.getMemoryBlock());
-  }
&lt;p&gt; }&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java&lt;br/&gt;
index 54dcadf3a7754..4563efcfcf474 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/Platform.java&lt;br/&gt;
@@ -19,10 +19,10 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.lang.reflect.Constructor;&lt;br/&gt;
 import java.lang.reflect.Field;&lt;br/&gt;
+import java.lang.reflect.InvocationTargetException;&lt;br/&gt;
 import java.lang.reflect.Method;&lt;br/&gt;
 import java.nio.ByteBuffer;&lt;/p&gt;

&lt;p&gt;-import sun.misc.Cleaner;&lt;br/&gt;
 import sun.misc.Unsafe;&lt;/p&gt;

&lt;p&gt; public final class Platform &lt;/p&gt;
{
@@ -67,6 +67,60 @@
     unaligned = _unaligned;
   }

&lt;p&gt;+  // Access fields and constructors once and store them, for performance:&lt;br/&gt;
+&lt;br/&gt;
+  private static final Constructor&amp;lt;?&amp;gt; DBB_CONSTRUCTOR;&lt;br/&gt;
+  private static final Field DBB_CLEANER_FIELD;&lt;br/&gt;
+  static {&lt;br/&gt;
+    try &lt;/p&gt;
{
+      Class&amp;lt;?&amp;gt; cls = Class.forName(&quot;java.nio.DirectByteBuffer&quot;);
+      Constructor&amp;lt;?&amp;gt; constructor = cls.getDeclaredConstructor(Long.TYPE, Integer.TYPE);
+      constructor.setAccessible(true);
+      Field cleanerField = cls.getDeclaredField(&quot;cleaner&quot;);
+      cleanerField.setAccessible(true);
+      DBB_CONSTRUCTOR = constructor;
+      DBB_CLEANER_FIELD = cleanerField;
+    }
&lt;p&gt; catch (ClassNotFoundException | NoSuchMethodException | NoSuchFieldException e) &lt;/p&gt;
{
+      throw new IllegalStateException(e);
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private static final Method CLEANER_CREATE_METHOD;&lt;br/&gt;
+  static {&lt;br/&gt;
+    // The implementation of Cleaner changed from JDK 8 to 9&lt;br/&gt;
+    // Split java.version on non-digit chars:&lt;br/&gt;
+    int majorVersion = Integer.parseInt(System.getProperty(&quot;java.version&quot;).split(&quot;&lt;br class=&quot;atl-forced-newline&quot; /&gt;D+&quot;)&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;);&lt;br/&gt;
+    String cleanerClassName;&lt;br/&gt;
+    if (majorVersion &amp;lt; 9) {
+      cleanerClassName = &quot;sun.misc.Cleaner&quot;;
+    } else {
+      cleanerClassName = &quot;jdk.internal.ref.Cleaner&quot;;
+    }&lt;br/&gt;
+    try {&lt;br/&gt;
+      Class&amp;lt;?&amp;gt; cleanerClass = Class.forName(cleanerClassName);&lt;br/&gt;
+      Method createMethod = cleanerClass.getMethod(&quot;create&quot;, Object.class, Runnable.class);&lt;br/&gt;
+      // Accessing jdk.internal.ref.Cleaner should actually fail by default in JDK 9+,&lt;br/&gt;
+      // unfortunately, unless the user has allowed access with something like&lt;br/&gt;
+      // --add-opens java.base/java.lang=ALL-UNNAMED  If not, we can&apos;t really use the Cleaner&lt;br/&gt;
+      // hack below. It doesn&apos;t break, just means the user might run into the default JVM limit&lt;br/&gt;
+      // on off-heap memory and increase it or set the flag above. This tests whether it&apos;s&lt;br/&gt;
+      // available:&lt;br/&gt;
+      try {
+        createMethod.invoke(null, null, null);
+      } catch (IllegalAccessException e) {
+        // Don&apos;t throw an exception, but can&apos;t log here?
+        createMethod = null;
+      } catch (InvocationTargetException ite) {
+        // shouldn&apos;t happen; report it
+        throw new IllegalStateException(ite);
+      }&lt;br/&gt;
+      CLEANER_CREATE_METHOD = createMethod;&lt;br/&gt;
+    } catch (ClassNotFoundException | NoSuchMethodException e) {+      throw new IllegalStateException(e);+    }
&lt;p&gt;+&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@return true when running JVM is having sun&apos;s Unsafe package available in it and underlying&lt;/li&gt;
	&lt;li&gt;system having unaligned-access capability.&lt;br/&gt;
@@ -120,6 +174,11 @@ public static float getFloat(Object object, long offset) {&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   public static void putFloat(Object object, long offset, float value) {&lt;br/&gt;
+    if (Float.isNaN(value)) &lt;/p&gt;
{
+      value = Float.NaN;
+    }
&lt;p&gt; else if (value == -0.0f) &lt;/p&gt;
{
+      value = 0.0f;
+    }
&lt;p&gt;     _UNSAFE.putFloat(object, offset, value);&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;@@ -128,6 +187,11 @@ public static double getDouble(Object object, long offset) {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   public static void putDouble(Object object, long offset, double value) {&lt;br/&gt;
+    if (Double.isNaN(value)) &lt;/p&gt;
{
+      value = Double.NaN;
+    }
&lt;p&gt; else if (value == -0.0d) &lt;/p&gt;
{
+      value = 0.0d;
+    }
&lt;p&gt;     _UNSAFE.putDouble(object, offset, value);&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;@@ -159,18 +223,18 @@ public static long reallocateMemory(long address, long oldSize, long newSize) {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;MaxDirectMemorySize limit (the default limit is too low and we do not want to require users&lt;/li&gt;
	&lt;li&gt;to increase it).&lt;br/&gt;
    */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
   public static ByteBuffer allocateDirectBuffer(int size) {&lt;br/&gt;
     try {&lt;/li&gt;
	&lt;li&gt;Class&amp;lt;?&amp;gt; cls = Class.forName(&quot;java.nio.DirectByteBuffer&quot;);&lt;/li&gt;
	&lt;li&gt;Constructor&amp;lt;?&amp;gt; constructor = cls.getDeclaredConstructor(Long.TYPE, Integer.TYPE);&lt;/li&gt;
	&lt;li&gt;constructor.setAccessible(true);&lt;/li&gt;
	&lt;li&gt;Field cleanerField = cls.getDeclaredField(&quot;cleaner&quot;);&lt;/li&gt;
	&lt;li&gt;cleanerField.setAccessible(true);&lt;br/&gt;
       long memory = allocateMemory(size);&lt;/li&gt;
	&lt;li&gt;ByteBuffer buffer = (ByteBuffer) constructor.newInstance(memory, size);&lt;/li&gt;
	&lt;li&gt;Cleaner cleaner = Cleaner.create(buffer, () -&amp;gt; freeMemory(memory));&lt;/li&gt;
	&lt;li&gt;cleanerField.set(buffer, cleaner);&lt;br/&gt;
+      ByteBuffer buffer = (ByteBuffer) DBB_CONSTRUCTOR.newInstance(memory, size);&lt;br/&gt;
+      if (CLEANER_CREATE_METHOD != null) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        try {
+          DBB_CLEANER_FIELD.set(buffer,
+              CLEANER_CREATE_METHOD.invoke(null, buffer, (Runnable) () -&amp;gt; freeMemory(memory)));
+        } catch (IllegalAccessException | InvocationTargetException e) {
+          throw new IllegalStateException(e);
+        }+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;       return buffer;&lt;br/&gt;
     } catch (Exception e) {&lt;br/&gt;
       throwException(e);&lt;br/&gt;
@@ -187,7 +251,7 @@ public static void setMemory(long address, byte value, long size) {&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   public static void copyMemory(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Object src, long srcOffset, Object dst, long dstOffset, long length) {&lt;br/&gt;
+    Object src, long srcOffset, Object dst, long dstOffset, long length) {&lt;br/&gt;
     // Check if dstOffset is before or after srcOffset to determine if we should copy&lt;br/&gt;
     // forward or backwards. This is necessary in case src and dst overlap.&lt;br/&gt;
     if (dstOffset &amp;lt; srcOffset) {&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/UnsafeAlignedOffset.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/UnsafeAlignedOffset.java&lt;br/&gt;
index be62e40412f83..546e8780a6606 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/src/main/java/org/apache/spark/unsafe/UnsafeAlignedOffset.java&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/UnsafeAlignedOffset.java&lt;br/&gt;
@@ -39,7 +39,9 @@ public static int getSize(Object object, long offset) 
{
       case 8:
         return (int)Platform.getLong(object, offset);
       default:
+        // checkstyle.off: RegexpSinglelineJava
         throw new AssertionError(&quot;Illegal UAO_SIZE&quot;);
+        // checkstyle.on: RegexpSinglelineJava
     }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -52,7 +54,9 @@ public static void putSize(Object object, long offset, int value) &lt;/p&gt;
{
         Platform.putLong(object, offset, value);
         break;
       default:
+        // checkstyle.off: RegexpSinglelineJava
         throw new AssertionError(&quot;Illegal UAO_SIZE&quot;);
+        // checkstyle.on: RegexpSinglelineJava
     }
&lt;p&gt;   }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/array/ByteArrayMethods.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/array/ByteArrayMethods.java&lt;br/&gt;
index ef0f78d95d1ee..cec8c30887e2f 100644&lt;br/&gt;
&amp;#8212; a/common/unsafe/src/main/java/org/apache/spark/unsafe/array/ByteArrayMethods.java&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/array/ByteArrayMethods.java&lt;br/&gt;
@@ -18,7 +18,6 @@&lt;br/&gt;
 package org.apache.spark.unsafe.array;&lt;/p&gt;

&lt;p&gt; import org.apache.spark.unsafe.Platform;&lt;br/&gt;
-import org.apache.spark.unsafe.memory.MemoryBlock;&lt;/p&gt;

&lt;p&gt; public class ByteArrayMethods {&lt;/p&gt;

&lt;p&gt;@@ -53,25 +52,15 @@ public static long roundNumberOfBytesToNearestWord(long numBytes) {&lt;br/&gt;
   public static int MAX_ROUNDED_ARRAY_LENGTH = Integer.MAX_VALUE - 15;&lt;/p&gt;

&lt;p&gt;   private static final boolean unaligned = Platform.unaligned();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* MemoryBlock equality check for MemoryBlocks.&lt;/li&gt;
	&lt;li&gt;* @return true if the arrays are equal, false otherwise&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;public static boolean arrayEqualsBlock(&lt;/li&gt;
	&lt;li&gt;MemoryBlock leftBase, long leftOffset, MemoryBlock rightBase, long rightOffset, long length) 
{
-    return arrayEquals(leftBase.getBaseObject(), leftBase.getBaseOffset() + leftOffset,
-      rightBase.getBaseObject(), rightBase.getBaseOffset() + rightOffset, length);
-  }
&lt;p&gt;-&lt;br/&gt;
   /**&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Optimized byte array equality check for byte arrays.&lt;/li&gt;
	&lt;li&gt;@return true if the arrays are equal, false otherwise&lt;br/&gt;
    */&lt;br/&gt;
   public static boolean arrayEquals(&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Object leftBase, long leftOffset, Object rightBase, long rightOffset, long length) {&lt;br/&gt;
+      Object leftBase, long leftOffset, Object rightBase, long rightOffset, final long length) {&lt;br/&gt;
     int i = 0;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// check if starts align and we can get both offsets to be aligned&lt;br/&gt;
+    // check if stars align and we can get both offsets to be aligned&lt;br/&gt;
     if ((leftOffset % 8) == (rightOffset % 8)) {&lt;br/&gt;
       while ((leftOffset + i) % 8 != 0 &amp;amp;&amp;amp; i &amp;lt; length) {&lt;br/&gt;
         if (Platform.getByte(leftBase, leftOffset + i) !=&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/array/LongArray.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/array/LongArray.java&lt;br/&gt;
index b74d2de0691d5..2cd39bd60c2ac 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/src/main/java/org/apache/spark/unsafe/array/LongArray.java&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/array/LongArray.java&lt;br/&gt;
@@ -17,6 +17,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; package org.apache.spark.unsafe.array;&lt;/p&gt;

&lt;p&gt;+import org.apache.spark.unsafe.Platform;&lt;br/&gt;
 import org.apache.spark.unsafe.memory.MemoryBlock;&lt;/p&gt;

&lt;p&gt; /**&lt;br/&gt;
@@ -32,12 +33,16 @@&lt;br/&gt;
   private static final long WIDTH = 8;&lt;/p&gt;

&lt;p&gt;   private final MemoryBlock memory;&lt;br/&gt;
+  private final Object baseObj;&lt;br/&gt;
+  private final long baseOffset;&lt;/p&gt;

&lt;p&gt;   private final long length;&lt;/p&gt;

&lt;p&gt;   public LongArray(MemoryBlock memory) &lt;/p&gt;
{
     assert memory.size() &amp;lt; (long) Integer.MAX_VALUE * 8: &quot;Array size &amp;gt;= Integer.MAX_VALUE elements&quot;;
     this.memory = memory;
+    this.baseObj = memory.getBaseObject();
+    this.baseOffset = memory.getBaseOffset();
     this.length = memory.size() / WIDTH;
   }

&lt;p&gt;@@ -46,11 +51,11 @@ public MemoryBlock memoryBlock() {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   public Object getBaseObject() &lt;/p&gt;
{
-    return memory.getBaseObject();
+    return baseObj;
   }

&lt;p&gt;   public long getBaseOffset() &lt;/p&gt;
{
-    return memory.getBaseOffset();
+    return baseOffset;
   }

&lt;p&gt;   /**&lt;br/&gt;
@@ -64,8 +69,8 @@ public long size() {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Fill this all with 0L.&lt;br/&gt;
    */&lt;br/&gt;
   public void zeroOut() {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (long off = 0; off &amp;lt; length * WIDTH; off += WIDTH) {&lt;/li&gt;
	&lt;li&gt;memory.putLong(off, 0);&lt;br/&gt;
+    for (long off = baseOffset; off &amp;lt; baseOffset + length * WIDTH; off += WIDTH) 
{
+      Platform.putLong(baseObj, off, 0);
     }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -75,7 +80,7 @@ public void zeroOut() {&lt;br/&gt;
   public void set(int index, long value) &lt;/p&gt;
{
     assert index &amp;gt;= 0 : &quot;index (&quot; + index + &quot;) should &amp;gt;= 0&quot;;
     assert index &amp;lt; length : &quot;index (&quot; + index + &quot;) should &amp;lt; length (&quot; + length + &quot;)&quot;;
-    memory.putLong(index * WIDTH, value);
+    Platform.putLong(baseObj, baseOffset + index * WIDTH, value);
   }

&lt;p&gt;   /**&lt;br/&gt;
@@ -84,6 +89,6 @@ public void set(int index, long value) {&lt;br/&gt;
   public long get(int index) &lt;/p&gt;
{
     assert index &amp;gt;= 0 : &quot;index (&quot; + index + &quot;) should &amp;gt;= 0&quot;;
     assert index &amp;lt; length : &quot;index (&quot; + index + &quot;) should &amp;lt; length (&quot; + length + &quot;)&quot;;
-    return memory.getLong(index * WIDTH);
+    return Platform.getLong(baseObj, baseOffset + index * WIDTH);
   }
&lt;p&gt; }&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/hash/Murmur3_x86_32.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/hash/Murmur3_x86_32.java&lt;br/&gt;
index aff6e93d647fe..d239de6083ad0 100644&lt;br/&gt;
&amp;#8212; a/common/unsafe/src/main/java/org/apache/spark/unsafe/hash/Murmur3_x86_32.java&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/hash/Murmur3_x86_32.java&lt;br/&gt;
@@ -17,10 +17,7 @@&lt;/p&gt;

&lt;p&gt; package org.apache.spark.unsafe.hash;&lt;/p&gt;

&lt;p&gt;-import com.google.common.primitives.Ints;&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.spark.unsafe.memory.MemoryBlock;&lt;br/&gt;
-import org.apache.spark.unsafe.types.UTF8String;&lt;br/&gt;
+import org.apache.spark.unsafe.Platform;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;32-bit Murmur3 hasher.  This is based on Guava&apos;s Murmur3_32HashFunction.&lt;br/&gt;
@@ -52,70 +49,49 @@ public static int hashInt(int input, int seed) {&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   public int hashUnsafeWords(Object base, long offset, int lengthInBytes) &lt;/p&gt;
{
-    return hashUnsafeWordsBlock(MemoryBlock.allocateFromObject(base, offset, lengthInBytes), seed);
+    return hashUnsafeWords(base, offset, lengthInBytes, seed);
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static int hashUnsafeWordsBlock(MemoryBlock base, int seed) {&lt;br/&gt;
+  public static int hashUnsafeWords(Object base, long offset, int lengthInBytes, int seed) 
{
     // This is based on Guava&apos;s `Murmur32_Hasher.processRemaining(ByteBuffer)` method.
-    int lengthInBytes = Ints.checkedCast(base.size());
     assert (lengthInBytes % 8 == 0): &quot;lengthInBytes must be a multiple of 8 (word-aligned)&quot;;
-    int h1 = hashBytesByIntBlock(base, seed);
+    int h1 = hashBytesByInt(base, offset, lengthInBytes, seed);
     return fmix(h1, lengthInBytes);
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static int hashUnsafeWords(Object base, long offset, int lengthInBytes, int seed) 
{
-    // This is based on Guava&apos;s `Murmur32_Hasher.processRemaining(ByteBuffer)` method.
-    return hashUnsafeWordsBlock(MemoryBlock.allocateFromObject(base, offset, lengthInBytes), seed);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static int hashUnsafeBytesBlock(MemoryBlock base, int seed) {&lt;br/&gt;
+  public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes, int seed) {&lt;br/&gt;
     // This is not compatible with original and another implementations.&lt;br/&gt;
     // But remain it for backward compatibility for the components existing before 2.3.&lt;/li&gt;
	&lt;li&gt;int lengthInBytes = Ints.checkedCast(base.size());&lt;br/&gt;
     assert (lengthInBytes &amp;gt;= 0): &quot;lengthInBytes cannot be negative&quot;;&lt;br/&gt;
     int lengthAligned = lengthInBytes - lengthInBytes % 4;&lt;/li&gt;
	&lt;li&gt;int h1 = hashBytesByIntBlock(base.subBlock(0, lengthAligned), seed);&lt;br/&gt;
+    int h1 = hashBytesByInt(base, offset, lengthAligned, seed);&lt;br/&gt;
     for (int i = lengthAligned; i &amp;lt; lengthInBytes; i++) 
{
-      int halfWord = base.getByte(i);
+      int halfWord = Platform.getByte(base, offset + i);
       int k1 = mixK1(halfWord);
       h1 = mixH1(h1, k1);
     }
&lt;p&gt;     return fmix(h1, lengthInBytes);&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public static int hashUTF8String(UTF8String str, int seed) 
{
-    return hashUnsafeBytesBlock(str.getMemoryBlock(), seed);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static int hashUnsafeBytes(Object base, long offset, int lengthInBytes, int seed) 
{
-    return hashUnsafeBytesBlock(MemoryBlock.allocateFromObject(base, offset, lengthInBytes), seed);
-  }
&lt;p&gt;-&lt;br/&gt;
   public static int hashUnsafeBytes2(Object base, long offset, int lengthInBytes, int seed) &lt;/p&gt;
{
-    return hashUnsafeBytes2Block(MemoryBlock.allocateFromObject(base, offset, lengthInBytes), seed);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static int hashUnsafeBytes2Block(MemoryBlock base, int seed) {&lt;/li&gt;
	&lt;li&gt;// This is compatible with original and other implementations.&lt;br/&gt;
+    // This is compatible with original and another implementations.&lt;br/&gt;
     // Use this method for new components after Spark 2.3.&lt;/li&gt;
	&lt;li&gt;int lengthInBytes = Ints.checkedCast(base.size());&lt;/li&gt;
	&lt;li&gt;assert (lengthInBytes &amp;gt;= 0) : &quot;lengthInBytes cannot be negative&quot;;&lt;br/&gt;
+    assert (lengthInBytes &amp;gt;= 0): &quot;lengthInBytes cannot be negative&quot;;&lt;br/&gt;
     int lengthAligned = lengthInBytes - lengthInBytes % 4;&lt;/li&gt;
	&lt;li&gt;int h1 = hashBytesByIntBlock(base.subBlock(0, lengthAligned), seed);&lt;br/&gt;
+    int h1 = hashBytesByInt(base, offset, lengthAligned, seed);&lt;br/&gt;
     int k1 = 0;&lt;br/&gt;
     for (int i = lengthAligned, shift = 0; i &amp;lt; lengthInBytes; i++, shift += 8) 
{
-      k1 ^= (base.getByte(i) &amp;amp; 0xFF) &amp;lt;&amp;lt; shift;
+      k1 ^= (Platform.getByte(base, offset + i) &amp;amp; 0xFF) &amp;lt;&amp;lt; shift;
     }
&lt;p&gt;     h1 ^= mixK1(k1);&lt;br/&gt;
     return fmix(h1, lengthInBytes);&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private static int hashBytesByIntBlock(MemoryBlock base, int seed) {&lt;/li&gt;
	&lt;li&gt;long lengthInBytes = base.size();&lt;br/&gt;
+  private static int hashBytesByInt(Object base, long offset, int lengthInBytes, int seed) {&lt;br/&gt;
     assert (lengthInBytes % 4 == 0);&lt;br/&gt;
     int h1 = seed;&lt;/li&gt;
	&lt;li&gt;for (long i = 0; i &amp;lt; lengthInBytes; i += 4) {&lt;/li&gt;
	&lt;li&gt;int halfWord = base.getInt&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
+    for (int i = 0; i &amp;lt; lengthInBytes; i += 4) 
{
+      int halfWord = Platform.getInt(base, offset + i);
       int k1 = mixK1(halfWord);
       h1 = mixH1(h1, k1);
     }
&lt;p&gt;diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/ByteArrayMemoryBlock.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/ByteArrayMemoryBlock.java&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 9f238632bc87a..0000000000000&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/ByteArrayMemoryBlock.java&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,128 +0,0 @@&lt;br/&gt;
-/*&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements.  See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License.  You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-&lt;br/&gt;
-package org.apache.spark.unsafe.memory;&lt;br/&gt;
-&lt;br/&gt;
-import com.google.common.primitives.Ints;&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.spark.unsafe.Platform;&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;/li&gt;
	&lt;li&gt;* A consecutive block of memory with a byte array on Java heap.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-public final class ByteArrayMemoryBlock extends MemoryBlock {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private final byte[] array;&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;public ByteArrayMemoryBlock(byte[] obj, long offset, long size) 
{
-    super(obj, offset, size);
-    this.array = obj;
-    assert(offset + size &amp;lt;= Platform.BYTE_ARRAY_OFFSET + obj.length) :
-      &quot;The sum of size &quot; + size + &quot; and offset &quot; + offset + &quot; should not be larger than &quot; +
-        &quot;the size of the given memory space &quot; + (obj.length + Platform.BYTE_ARRAY_OFFSET);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public ByteArrayMemoryBlock(long length) 
{
-    this(new byte[Ints.checkedCast(length)], Platform.BYTE_ARRAY_OFFSET, length);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public MemoryBlock subBlock(long offset, long size) 
{
-    checkSubBlockRange(offset, size);
-    if (offset == 0 &amp;amp;&amp;amp; size == this.size()) return this;
-    return new ByteArrayMemoryBlock(array, this.offset + offset, size);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public byte[] getByteArray() 
{ return array; }&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Creates a memory block pointing to the memory used by the byte array.&lt;br/&gt;
-   */&lt;br/&gt;
-  public static ByteArrayMemoryBlock fromArray(final byte[] array) {
-    return new ByteArrayMemoryBlock(array, Platform.BYTE_ARRAY_OFFSET, array.length);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public int getInt(long offset) {
-    return Platform.getInt(array, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public void putInt(long offset, int value) {
-    Platform.putInt(array, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public boolean getBoolean(long offset) {
-    return Platform.getBoolean(array, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public void putBoolean(long offset, boolean value) {
-    Platform.putBoolean(array, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public byte getByte(long offset) {
-    return array[(int)(this.offset + offset - Platform.BYTE_ARRAY_OFFSET)];
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public void putByte(long offset, byte value) {
-    array[(int)(this.offset + offset - Platform.BYTE_ARRAY_OFFSET)] = value;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public short getShort(long offset) {
-    return Platform.getShort(array, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public void putShort(long offset, short value) {
-    Platform.putShort(array, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public long getLong(long offset) {
-    return Platform.getLong(array, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public void putLong(long offset, long value) {
-    Platform.putLong(array, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public float getFloat(long offset) {
-    return Platform.getFloat(array, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public void putFloat(long offset, float value) {
-    Platform.putFloat(array, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public double getDouble(long offset) {
-    return Platform.getDouble(array, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public void putDouble(long offset, double value) {
-    Platform.putDouble(array, this.offset + offset, value);
-  }&lt;br/&gt;
-}&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/HeapMemoryAllocator.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/HeapMemoryAllocator.java&lt;br/&gt;
index 36caf80888cda..2733760dd19ef 100644&lt;br/&gt;
&amp;#8212; a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/HeapMemoryAllocator.java&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/HeapMemoryAllocator.java&lt;br/&gt;
@@ -23,6 +23,8 @@&lt;br/&gt;
 import java.util.LinkedList;&lt;br/&gt;
 import java.util.Map;&lt;br/&gt;
 &lt;br/&gt;
+import org.apache.spark.unsafe.Platform;&lt;br/&gt;
+&lt;br/&gt;
 /**&lt;br/&gt;
  * A simple {@link MemoryAllocator} that can allocate up to 16GB using a JVM long primitive array.&lt;br/&gt;
  */&lt;br/&gt;
@@ -56,7 +58,7 @@ public MemoryBlock allocate(long size) throws OutOfMemoryError {&lt;br/&gt;
             final long[] array = arrayReference.get();&lt;br/&gt;
             if (array != null) {&lt;br/&gt;
               assert (array.length * 8L &amp;gt;= size);&lt;br/&gt;
-              MemoryBlock memory = OnHeapMemoryBlock.fromArray(array, size);&lt;br/&gt;
+              MemoryBlock memory = new MemoryBlock(array, Platform.LONG_ARRAY_OFFSET, size);&lt;br/&gt;
               if (MemoryAllocator.MEMORY_DEBUG_FILL_ENABLED) {
                 memory.fill(MemoryAllocator.MEMORY_DEBUG_FILL_CLEAN_VALUE);
               }&lt;br/&gt;
@@ -68,7 +70,7 @@ public MemoryBlock allocate(long size) throws OutOfMemoryError {&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
     long[] array = new long&lt;span class=&quot;error&quot;&gt;&amp;#91;numWords&amp;#93;&lt;/span&gt;;&lt;br/&gt;
-    MemoryBlock memory = OnHeapMemoryBlock.fromArray(array, size);&lt;br/&gt;
+    MemoryBlock memory = new MemoryBlock(array, Platform.LONG_ARRAY_OFFSET, size);&lt;br/&gt;
     if (MemoryAllocator.MEMORY_DEBUG_FILL_ENABLED) {
       memory.fill(MemoryAllocator.MEMORY_DEBUG_FILL_CLEAN_VALUE);
     }&lt;br/&gt;
@@ -77,13 +79,12 @@ public MemoryBlock allocate(long size) throws OutOfMemoryError {&lt;br/&gt;
 &lt;br/&gt;
   @Override&lt;br/&gt;
   public void free(MemoryBlock memory) {&lt;br/&gt;
-    assert(memory instanceof OnHeapMemoryBlock);&lt;br/&gt;
-    assert (memory.getBaseObject() != null) :&lt;br/&gt;
+    assert (memory.obj != null) :&lt;br/&gt;
       &quot;baseObject was null; are you trying to use the on-heap allocator to free off-heap memory?&quot;;&lt;br/&gt;
-    assert (memory.getPageNumber() != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :&lt;br/&gt;
+    assert (memory.pageNumber != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :&lt;br/&gt;
       &quot;page has already been freed&quot;;&lt;br/&gt;
-    assert ((memory.getPageNumber() == MemoryBlock.NO_PAGE_NUMBER)&lt;br/&gt;
-            || (memory.getPageNumber() == MemoryBlock.FREED_IN_TMM_PAGE_NUMBER)) :&lt;br/&gt;
+    assert ((memory.pageNumber == MemoryBlock.NO_PAGE_NUMBER)&lt;br/&gt;
+            || (memory.pageNumber == MemoryBlock.FREED_IN_TMM_PAGE_NUMBER)) :&lt;br/&gt;
       &quot;TMM-allocated pages must first be freed via TMM.freePage(), not directly in allocator &quot; +&lt;br/&gt;
         &quot;free()&quot;;&lt;br/&gt;
 &lt;br/&gt;
@@ -93,12 +94,12 @@ public void free(MemoryBlock memory) {&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     // Mark the page as freed (so we can detect double-frees).&lt;br/&gt;
-    memory.setPageNumber(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER);&lt;br/&gt;
+    memory.pageNumber = MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER;&lt;br/&gt;
 &lt;br/&gt;
     // As an additional layer of defense against use-after-free bugs, we mutate the&lt;br/&gt;
     // MemoryBlock to null out its reference to the long[] array.&lt;br/&gt;
-    long[] array = ((OnHeapMemoryBlock)memory).getLongArray();&lt;br/&gt;
-    memory.resetObjAndOffset();&lt;br/&gt;
+    long[] array = (long[]) memory.obj;&lt;br/&gt;
+    memory.setObjAndOffset(null, 0);&lt;br/&gt;
 &lt;br/&gt;
     long alignedSize = ((size + 7) / 8) * 8;&lt;br/&gt;
     if (shouldPool(alignedSize)) {
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryAllocator.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryAllocator.java
index 38315fb97b46a..7b588681d9790 100644
--- a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryAllocator.java
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryAllocator.java
@@ -38,7 +38,7 @@
 
   void free(MemoryBlock memory);
 
-  UnsafeMemoryAllocator UNSAFE = new UnsafeMemoryAllocator();
+  MemoryAllocator UNSAFE = new UnsafeMemoryAllocator();
 
-  HeapMemoryAllocator HEAP = new HeapMemoryAllocator();
+  MemoryAllocator HEAP = new HeapMemoryAllocator();
 }&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryBlock.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryBlock.java&lt;br/&gt;
index ca7213bbf92da..c333857358d30 100644&lt;br/&gt;
&amp;#8212; a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryBlock.java&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryBlock.java&lt;br/&gt;
@@ -22,10 +22,10 @@&lt;br/&gt;
 import org.apache.spark.unsafe.Platform;&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
- * A representation of a consecutive memory block in Spark. It defines the common interfaces&lt;br/&gt;
- * for memory accessing and mutating.&lt;br/&gt;
+ * A consecutive block of memory, starting at a {@link MemoryLocation} with a fixed size.&lt;br/&gt;
  */&lt;br/&gt;
-public abstract class MemoryBlock {&lt;br/&gt;
+public class MemoryBlock extends MemoryLocation {&lt;br/&gt;
+&lt;br/&gt;
   /** Special `pageNumber` value for pages which were not allocated by TaskMemoryManagers */&lt;br/&gt;
   public static final int NO_PAGE_NUMBER = -1;&lt;br/&gt;
 &lt;br/&gt;
@@ -45,163 +45,38 @@&lt;br/&gt;
    */&lt;br/&gt;
   public static final int FREED_IN_ALLOCATOR_PAGE_NUMBER = -3;&lt;br/&gt;
 &lt;br/&gt;
-  @Nullable&lt;br/&gt;
-  protected Object obj;&lt;br/&gt;
-&lt;br/&gt;
-  protected long offset;&lt;br/&gt;
-&lt;br/&gt;
-  protected long length;&lt;br/&gt;
+  private final long length;&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
    * Optional page number; used when this MemoryBlock represents a page allocated by a&lt;br/&gt;
-   * TaskMemoryManager. This field can be updated using setPageNumber method so that&lt;br/&gt;
-   * this can be modified by the TaskMemoryManager, which lives in a different package.&lt;br/&gt;
+   * TaskMemoryManager. This field is public so that it can be modified by the TaskMemoryManager,&lt;br/&gt;
+   * which lives in a different package.&lt;br/&gt;
    */&lt;br/&gt;
-  private int pageNumber = NO_PAGE_NUMBER;&lt;br/&gt;
+  public int pageNumber = NO_PAGE_NUMBER;&lt;br/&gt;
 &lt;br/&gt;
-  protected MemoryBlock(@Nullable Object obj, long offset, long length) {&lt;br/&gt;
-    if (offset &amp;lt; 0 || length &amp;lt; 0) {
-      throw new IllegalArgumentException(
-        &quot;Length &quot; + length + &quot; and offset &quot; + offset + &quot;must be non-negative&quot;);
-    }&lt;br/&gt;
-    this.obj = obj;&lt;br/&gt;
-    this.offset = offset;&lt;br/&gt;
+  public MemoryBlock(@Nullable Object obj, long offset, long length) {
+    super(obj, offset);
     this.length = length;
   }&lt;br/&gt;
 &lt;br/&gt;
-  protected MemoryBlock() {
-    this(null, 0, 0);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  public final Object getBaseObject() {
-    return obj;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  public final long getBaseOffset() {
-    return offset;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  public void resetObjAndOffset() {
-    this.obj = null;
-    this.offset = 0;
-  }&lt;br/&gt;
-&lt;br/&gt;
   /**&lt;br/&gt;
    * Returns the size of the memory block.&lt;br/&gt;
    */&lt;br/&gt;
-  public final long size() {&lt;br/&gt;
+  public long size() {
     return length;
   }&lt;br/&gt;
 &lt;br/&gt;
-  public final void setPageNumber(int pageNum) {
-    pageNumber = pageNum;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  public final int getPageNumber() {
-    return pageNumber;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Fills the memory block with the specified byte value.&lt;br/&gt;
-   */&lt;br/&gt;
-  public final void fill(byte value) {
-    Platform.setMemory(obj, offset, length, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Instantiate MemoryBlock for given object type with new offset&lt;br/&gt;
-   */&lt;br/&gt;
-  public static final MemoryBlock allocateFromObject(Object obj, long offset, long length) {&lt;br/&gt;
-    MemoryBlock mb = null;&lt;br/&gt;
-    if (obj instanceof byte[]) {
-      byte[] array = (byte[])obj;
-      mb = new ByteArrayMemoryBlock(array, offset, length);
-    } else if (obj instanceof long[]) {
-      long[] array = (long[])obj;
-      mb = new OnHeapMemoryBlock(array, offset, length);
-    } else if (obj == null) {
-      // we assume that to pass null pointer means off-heap
-      mb = new OffHeapMemoryBlock(offset, length);
-    } else {
-      throw new UnsupportedOperationException(
-        &quot;Instantiate MemoryBlock for type &quot; + obj.getClass() + &quot; is not supported now&quot;);
-    }&lt;br/&gt;
-    return mb;&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
   /**&lt;br/&gt;
-   * Just instantiate the sub-block with the same type of MemoryBlock with the new size and relative&lt;br/&gt;
-   * offset from the original offset. The data is not copied.&lt;br/&gt;
-   * If parameters are invalid, an exception is thrown.&lt;br/&gt;
+   * Creates a memory block pointing to the memory used by the long array.&lt;br/&gt;
    */&lt;br/&gt;
-  public abstract MemoryBlock subBlock(long offset, long size);&lt;br/&gt;
-&lt;br/&gt;
-  protected void checkSubBlockRange(long offset, long size) {&lt;br/&gt;
-    if (offset &amp;lt; 0 || size &amp;lt; 0) {
-      throw new ArrayIndexOutOfBoundsException(
-        &quot;Size &quot; + size + &quot; and offset &quot; + offset + &quot; must be non-negative&quot;);
-    }&lt;br/&gt;
-    if (offset + size &amp;gt; length) {
-      throw new ArrayIndexOutOfBoundsException(&quot;The sum of size &quot; + size + &quot; and offset &quot; +
-        offset + &quot; should not be larger than the length &quot; + length + &quot; in the MemoryBlock&quot;);
-    }&lt;br/&gt;
+  public static MemoryBlock fromLongArray(final long[] array) {
+    return new MemoryBlock(array, Platform.LONG_ARRAY_OFFSET, array.length * 8L);
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-   * getXXX/putXXX does not ensure guarantee behavior if the offset is invalid. e.g  cause illegal&lt;br/&gt;
-   * memory access, throw an exception, or etc.&lt;br/&gt;
-   * getXXX/putXXX uses an index based on this.offset that includes the size of metadata such as&lt;br/&gt;
-   * JVM object header. The offset is 0-based and is expected as an logical offset in the memory&lt;br/&gt;
-   * block.&lt;br/&gt;
+   * Fills the memory block with the specified byte value.&lt;br/&gt;
    */&lt;br/&gt;
-  public abstract int getInt(long offset);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract void putInt(long offset, int value);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract boolean getBoolean(long offset);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract void putBoolean(long offset, boolean value);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract byte getByte(long offset);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract void putByte(long offset, byte value);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract short getShort(long offset);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract void putShort(long offset, short value);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract long getLong(long offset);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract void putLong(long offset, long value);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract float getFloat(long offset);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract void putFloat(long offset, float value);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract double getDouble(long offset);&lt;br/&gt;
-&lt;br/&gt;
-  public abstract void putDouble(long offset, double value);&lt;br/&gt;
-&lt;br/&gt;
-  public static final void copyMemory(&lt;br/&gt;
-      MemoryBlock src, long srcOffset, MemoryBlock dst, long dstOffset, long length) {
-    assert(srcOffset + length &amp;lt;= src.length &amp;amp;&amp;amp; dstOffset + length &amp;lt;= dst.length);
-    Platform.copyMemory(src.getBaseObject(), src.getBaseOffset() + srcOffset,
-      dst.getBaseObject(), dst.getBaseOffset() + dstOffset, length);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  public static final void copyMemory(MemoryBlock src, MemoryBlock dst, long length) {
-    assert(length &amp;lt;= src.length &amp;amp;&amp;amp; length &amp;lt;= dst.length);
-    Platform.copyMemory(src.getBaseObject(), src.getBaseOffset(),
-      dst.getBaseObject(), dst.getBaseOffset(), length);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  public final void copyFrom(Object src, long srcOffset, long dstOffset, long length) {
-    assert(length &amp;lt;= this.length - srcOffset);
-    Platform.copyMemory(src, srcOffset, obj, offset + dstOffset, length);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  public final void writeTo(long srcOffset, Object dst, long dstOffset, long length) {&lt;br/&gt;
-    assert(length &amp;lt;= this.length - srcOffset);&lt;br/&gt;
-    Platform.copyMemory(obj, offset + srcOffset, dst, dstOffset, length);&lt;br/&gt;
+  public void fill(byte value) {
+    Platform.setMemory(obj, offset, length, value);
   }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryLocation.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryLocation.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..74ebc87dc978c&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/MemoryLocation.java&lt;br/&gt;
@@ -0,0 +1,54 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.unsafe.memory;&lt;br/&gt;
+&lt;br/&gt;
+import javax.annotation.Nullable;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * A memory location. Tracked either by a memory address (with off-heap allocation),&lt;br/&gt;
+ * or by an offset from a JVM object (in-heap allocation).&lt;br/&gt;
+ */&lt;br/&gt;
+public class MemoryLocation {&lt;br/&gt;
+&lt;br/&gt;
+  @Nullable&lt;br/&gt;
+  Object obj;&lt;br/&gt;
+&lt;br/&gt;
+  long offset;&lt;br/&gt;
+&lt;br/&gt;
+  public MemoryLocation(@Nullable Object obj, long offset) {
+    this.obj = obj;
+    this.offset = offset;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  public MemoryLocation() {
+    this(null, 0);
+  }&lt;br/&gt;
+&lt;br/&gt;
+  public void setObjAndOffset(Object newObj, long newOffset) {
+    this.obj = newObj;
+    this.offset = newOffset;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  public final Object getBaseObject() {
+    return obj;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  public final long getBaseOffset() {
+    return offset;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OffHeapMemoryBlock.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OffHeapMemoryBlock.java&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 3431b08980eb8..0000000000000&lt;br/&gt;
&amp;#8212; a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OffHeapMemoryBlock.java&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,105 +0,0 @@&lt;br/&gt;
-/*&lt;br/&gt;
- * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
- * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
- * this work for additional information regarding copyright ownership.&lt;br/&gt;
- * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
- * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
- * the License.  You may obtain a copy of the License at&lt;br/&gt;
- *&lt;br/&gt;
- *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
- *&lt;br/&gt;
- * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
- * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
- * See the License for the specific language governing permissions and&lt;br/&gt;
- * limitations under the License.&lt;br/&gt;
- */&lt;br/&gt;
-&lt;br/&gt;
-package org.apache.spark.unsafe.memory;&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.spark.unsafe.Platform;&lt;br/&gt;
-&lt;br/&gt;
-public class OffHeapMemoryBlock extends MemoryBlock {&lt;br/&gt;
-  public static final OffHeapMemoryBlock NULL = new OffHeapMemoryBlock(0, 0);&lt;br/&gt;
-&lt;br/&gt;
-  public OffHeapMemoryBlock(long address, long size) {
-    super(null, address, size);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public MemoryBlock subBlock(long offset, long size) {
-    checkSubBlockRange(offset, size);
-    if (offset == 0 &amp;amp;&amp;amp; size == this.size()) return this;
-    return new OffHeapMemoryBlock(this.offset + offset, size);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final int getInt(long offset) {
-    return Platform.getInt(null, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final void putInt(long offset, int value) {
-    Platform.putInt(null, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final boolean getBoolean(long offset) {
-    return Platform.getBoolean(null, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final void putBoolean(long offset, boolean value) {
-    Platform.putBoolean(null, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final byte getByte(long offset) {
-    return Platform.getByte(null, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final void putByte(long offset, byte value) {
-    Platform.putByte(null, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final short getShort(long offset) {
-    return Platform.getShort(null, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final void putShort(long offset, short value) {
-    Platform.putShort(null, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final long getLong(long offset) {
-    return Platform.getLong(null, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final void putLong(long offset, long value) {
-    Platform.putLong(null, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final float getFloat(long offset) {
-    return Platform.getFloat(null, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final void putFloat(long offset, float value) {
-    Platform.putFloat(null, this.offset + offset, value);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final double getDouble(long offset) {
-    return Platform.getDouble(null, this.offset + offset);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public final void putDouble(long offset, double value) {
-    Platform.putDouble(null, this.offset + offset, value);
-  }&lt;br/&gt;
-}&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OnHeapMemoryBlock.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OnHeapMemoryBlock.java&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index ee42bc27c9c5f..0000000000000&lt;br/&gt;
&amp;#8212; a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/OnHeapMemoryBlock.java&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,132 +0,0 @@&lt;br/&gt;
-/*&lt;br/&gt;
- * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
- * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
- * this work for additional information regarding copyright ownership.&lt;br/&gt;
- * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
- * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
- * the License.  You may obtain a copy of the License at&lt;br/&gt;
- *&lt;br/&gt;
- *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
- *&lt;br/&gt;
- * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
- * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
- * See the License for the specific language governing permissions and&lt;br/&gt;
- * limitations under the License.&lt;br/&gt;
- */&lt;br/&gt;
-&lt;br/&gt;
-package org.apache.spark.unsafe.memory;&lt;br/&gt;
-&lt;br/&gt;
-import com.google.common.primitives.Ints;&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.spark.unsafe.Platform;&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;br/&gt;
- * A consecutive block of memory with a long array on Java heap.&lt;br/&gt;
- */&lt;br/&gt;
-public final class OnHeapMemoryBlock extends MemoryBlock {&lt;br/&gt;
-&lt;br/&gt;
-  private final long[] array;&lt;br/&gt;
-&lt;br/&gt;
-  public OnHeapMemoryBlock(long[] obj, long offset, long size) {
-    super(obj, offset, size);
-    this.array = obj;
-    assert(offset + size &amp;lt;= obj.length * 8L + Platform.LONG_ARRAY_OFFSET) :
-      &quot;The sum of size &quot; + size + &quot; and offset &quot; + offset + &quot; should not be larger than &quot; +
-        &quot;the size of the given memory space &quot; + (obj.length * 8L + Platform.LONG_ARRAY_OFFSET);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  public OnHeapMemoryBlock(long size) {
-    this(new long[Ints.checkedCast((size + 7) / 8)], Platform.LONG_ARRAY_OFFSET, size);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @Override&lt;br/&gt;
-  public MemoryBlock subBlock(long offset, long size) {
-    checkSubBlockRange(offset, size);
-    if (offset == 0 &amp;amp;&amp;amp; size == this.size()) return this;
-    return new OnHeapMemoryBlock(array, this.offset + offset, size);
-  }&lt;br/&gt;
-&lt;br/&gt;
-  public long[] getLongArray() { return array; }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Creates a memory block pointing to the memory used by the long array.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;public static OnHeapMemoryBlock fromArray(final long[] array) 
{
-    return new OnHeapMemoryBlock(array, Platform.LONG_ARRAY_OFFSET, array.length * 8L);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static OnHeapMemoryBlock fromArray(final long[] array, long size) 
{
-    return new OnHeapMemoryBlock(array, Platform.LONG_ARRAY_OFFSET, size);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public int getInt(long offset) 
{
-    return Platform.getInt(array, this.offset + offset);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void putInt(long offset, int value) 
{
-    Platform.putInt(array, this.offset + offset, value);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public boolean getBoolean(long offset) 
{
-    return Platform.getBoolean(array, this.offset + offset);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void putBoolean(long offset, boolean value) 
{
-    Platform.putBoolean(array, this.offset + offset, value);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public byte getByte(long offset) 
{
-    return Platform.getByte(array, this.offset + offset);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void putByte(long offset, byte value) 
{
-    Platform.putByte(array, this.offset + offset, value);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public short getShort(long offset) 
{
-    return Platform.getShort(array, this.offset + offset);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void putShort(long offset, short value) 
{
-    Platform.putShort(array, this.offset + offset, value);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public long getLong(long offset) 
{
-    return Platform.getLong(array, this.offset + offset);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void putLong(long offset, long value) 
{
-    Platform.putLong(array, this.offset + offset, value);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public float getFloat(long offset) 
{
-    return Platform.getFloat(array, this.offset + offset);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void putFloat(long offset, float value) 
{
-    Platform.putFloat(array, this.offset + offset, value);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public double getDouble(long offset) 
{
-    return Platform.getDouble(array, this.offset + offset);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void putDouble(long offset, double value) 
{
-    Platform.putDouble(array, this.offset + offset, value);
-  }
&lt;p&gt;-}&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/UnsafeMemoryAllocator.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/UnsafeMemoryAllocator.java&lt;br/&gt;
index 5310bdf2779a9..4368fb615ba1e 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/UnsafeMemoryAllocator.java&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/memory/UnsafeMemoryAllocator.java&lt;br/&gt;
@@ -25,9 +25,9 @@&lt;br/&gt;
 public class UnsafeMemoryAllocator implements MemoryAllocator {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public OffHeapMemoryBlock allocate(long size) throws OutOfMemoryError {&lt;br/&gt;
+  public MemoryBlock allocate(long size) throws OutOfMemoryError {&lt;br/&gt;
     long address = Platform.allocateMemory(size);&lt;/li&gt;
	&lt;li&gt;OffHeapMemoryBlock memory = new OffHeapMemoryBlock(address, size);&lt;br/&gt;
+    MemoryBlock memory = new MemoryBlock(null, address, size);&lt;br/&gt;
     if (MemoryAllocator.MEMORY_DEBUG_FILL_ENABLED) 
{
       memory.fill(MemoryAllocator.MEMORY_DEBUG_FILL_CLEAN_VALUE);
     }
&lt;p&gt;@@ -36,25 +36,22 @@ public OffHeapMemoryBlock allocate(long size) throws OutOfMemoryError {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Override&lt;br/&gt;
   public void free(MemoryBlock memory) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assert(memory instanceof OffHeapMemoryBlock) :&lt;/li&gt;
	&lt;li&gt;&quot;UnsafeMemoryAllocator can only free OffHeapMemoryBlock.&quot;;&lt;/li&gt;
	&lt;li&gt;if (memory == OffHeapMemoryBlock.NULL) return;&lt;/li&gt;
	&lt;li&gt;assert (memory.getPageNumber() != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :&lt;br/&gt;
+    assert (memory.obj == null) :&lt;br/&gt;
+      &quot;baseObject not null; are you trying to use the off-heap allocator to free on-heap memory?&quot;;&lt;br/&gt;
+    assert (memory.pageNumber != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :&lt;br/&gt;
       &quot;page has already been freed&quot;;&lt;/li&gt;
	&lt;li&gt;assert ((memory.getPageNumber() == MemoryBlock.NO_PAGE_NUMBER)&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; (memory.getPageNumber() == MemoryBlock.FREED_IN_TMM_PAGE_NUMBER)) :&lt;br/&gt;
+    assert ((memory.pageNumber == MemoryBlock.NO_PAGE_NUMBER)&lt;br/&gt;
+            &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; (memory.pageNumber == MemoryBlock.FREED_IN_TMM_PAGE_NUMBER)) :&lt;br/&gt;
       &quot;TMM-allocated pages must be freed via TMM.freePage(), not directly in allocator free()&quot;;&lt;/th&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     if (MemoryAllocator.MEMORY_DEBUG_FILL_ENABLED) &lt;/p&gt;
{
       memory.fill(MemoryAllocator.MEMORY_DEBUG_FILL_FREED_VALUE);
     }
&lt;p&gt;-&lt;br/&gt;
     Platform.freeMemory(memory.offset);&lt;br/&gt;
-&lt;br/&gt;
     // As an additional layer of defense against use-after-free bugs, we mutate the&lt;br/&gt;
     // MemoryBlock to reset its pointer.&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;memory.resetObjAndOffset();&lt;br/&gt;
+    memory.offset = 0;&lt;br/&gt;
     // Mark the page as freed (so we can detect double-frees).&lt;/li&gt;
	&lt;li&gt;memory.setPageNumber(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER);&lt;br/&gt;
+    memory.pageNumber = MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER;&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java b/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java&lt;br/&gt;
index e91fc4391425c..3a3bfc4a94bb3 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java&lt;br/&gt;
+++ b/common/unsafe/src/main/java/org/apache/spark/unsafe/types/UTF8String.java&lt;br/&gt;
@@ -34,8 +34,6 @@&lt;br/&gt;
 import org.apache.spark.unsafe.Platform;&lt;br/&gt;
 import org.apache.spark.unsafe.array.ByteArrayMethods;&lt;br/&gt;
 import org.apache.spark.unsafe.hash.Murmur3_x86_32;&lt;br/&gt;
-import org.apache.spark.unsafe.memory.ByteArrayMemoryBlock;&lt;br/&gt;
-import org.apache.spark.unsafe.memory.MemoryBlock;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import static org.apache.spark.unsafe.Platform.*;&lt;/p&gt;

&lt;p&gt;@@ -53,13 +51,12 @@&lt;/p&gt;

&lt;p&gt;   // These are only updated by readExternal() or read()&lt;br/&gt;
   @Nonnull&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private MemoryBlock base;&lt;/li&gt;
	&lt;li&gt;// While numBytes has the same value as base.size(), to keep as int avoids cast from long to int&lt;br/&gt;
+  private Object base;&lt;br/&gt;
+  private long offset;&lt;br/&gt;
   private int numBytes;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public MemoryBlock getMemoryBlock() 
{ return base; }&lt;br/&gt;
-  public Object getBaseObject() { return base.getBaseObject(); }&lt;br/&gt;
-  public long getBaseOffset() { return base.getBaseOffset(); }&lt;br/&gt;
+  public Object getBaseObject() { return base; }
&lt;p&gt;+  public long getBaseOffset() &lt;/p&gt;
{ return offset; }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A char in UTF-8 encoding can take 1-4 bytes depending on the first byte which&lt;br/&gt;
@@ -112,8 +109,7 @@&lt;br/&gt;
    */&lt;br/&gt;
   public static UTF8String fromBytes(byte[] bytes) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     if (bytes != null) {
-      return new UTF8String(
-        new ByteArrayMemoryBlock(bytes, BYTE_ARRAY_OFFSET, bytes.length));
+      return new UTF8String(bytes, BYTE_ARRAY_OFFSET, bytes.length);
     } else {
       return null;
     }&lt;br/&gt;
@@ -126,13 +122,19 @@ public static UTF8String fromBytes(byte[] bytes) {&lt;br/&gt;
    */&lt;br/&gt;
   public static UTF8String fromBytes(byte[] bytes, int offset, int numBytes) {&lt;br/&gt;
     if (bytes != null) {
-      return new UTF8String(
-        new ByteArrayMemoryBlock(bytes, BYTE_ARRAY_OFFSET + offset, numBytes));
+      return new UTF8String(bytes, BYTE_ARRAY_OFFSET + offset, numBytes);
     } else {       return null;     }   }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  /**&lt;br/&gt;
+   * Creates an UTF8String from given address (base and offset) and length.&lt;br/&gt;
+   */&lt;br/&gt;
+  public static UTF8String fromAddress(Object base, long offset, int numBytes) &lt;/p&gt;
{
+    return new UTF8String(base, offset, numBytes);
+  }
&lt;p&gt;+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Creates an UTF8String from String.&lt;br/&gt;
    */&lt;br/&gt;
@@ -149,13 +151,16 @@ public static UTF8String blankString(int length) 
{
     return fromBytes(spaces);
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public UTF8String(MemoryBlock base) {&lt;br/&gt;
+  protected UTF8String(Object base, long offset, int numBytes) 
{
     this.base = base;
-    this.numBytes = Ints.checkedCast(base.size());
+    this.offset = offset;
+    this.numBytes = numBytes;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   // for serialization&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public UTF8String() {}&lt;br/&gt;
+  public UTF8String() 
{
+    this(null, 0, 0);
+  }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Writes the content of this string into a memory address, identified by an object and an offset.&lt;br/&gt;
@@ -163,7 +168,7 @@ public UTF8String() {}&lt;/li&gt;
	&lt;li&gt;bytes in this string.&lt;br/&gt;
    */&lt;br/&gt;
   public void writeToMemory(Object target, long targetOffset) 
{
-    base.writeTo(0, target, targetOffset, numBytes);
+    Platform.copyMemory(base, offset, target, targetOffset, numBytes);
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   public void writeTo(ByteBuffer buffer) {&lt;br/&gt;
@@ -183,9 +188,8 @@ public void writeTo(ByteBuffer buffer) {&lt;br/&gt;
    */&lt;br/&gt;
   @Nonnull&lt;br/&gt;
   public ByteBuffer getByteBuffer() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;long offset = base.getBaseOffset();&lt;/li&gt;
	&lt;li&gt;if (base instanceof ByteArrayMemoryBlock &amp;amp;&amp;amp; offset &amp;gt;= BYTE_ARRAY_OFFSET) {&lt;/li&gt;
	&lt;li&gt;final byte[] bytes = ((ByteArrayMemoryBlock) base).getByteArray();&lt;br/&gt;
+    if (base instanceof byte[] &amp;amp;&amp;amp; offset &amp;gt;= BYTE_ARRAY_OFFSET) {&lt;br/&gt;
+      final byte[] bytes = (byte[]) base;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // the offset includes an object header... this is only needed for unsafe copies&lt;br/&gt;
       final long arrayOffset = offset - BYTE_ARRAY_OFFSET;&lt;br/&gt;
@@ -252,12 +256,12 @@ public long getPrefix() {&lt;br/&gt;
     long mask = 0;&lt;br/&gt;
     if (IS_LITTLE_ENDIAN) {&lt;br/&gt;
       if (numBytes &amp;gt;= 8) &lt;/p&gt;
{
-        p = base.getLong(0);
+        p = Platform.getLong(base, offset);
       } else if (numBytes &amp;gt; 4) {
-        p = base.getLong(0);
+        p = Platform.getLong(base, offset);
         mask = (1L &amp;lt;&amp;lt; (8 - numBytes) * 8) - 1;
       } else if (numBytes &amp;gt; 0) {
-        p = (long) base.getInt(0);
+        p = (long) Platform.getInt(base, offset);
         mask = (1L &amp;lt;&amp;lt; (8 - numBytes) * 8) - 1;
       } else {&lt;br/&gt;
         p = 0;&lt;br/&gt;
@@ -266,12 +270,12 @@ public long getPrefix() {&lt;br/&gt;
     } else {&lt;br/&gt;
       // byteOrder == ByteOrder.BIG_ENDIAN&lt;br/&gt;
       if (numBytes &amp;gt;= 8) {-        p = base.getLong(0);+        p = Platform.getLong(base, offset);       }
&lt;p&gt; else if (numBytes &amp;gt; 4) &lt;/p&gt;
{
-        p = base.getLong(0);
+        p = Platform.getLong(base, offset);
         mask = (1L &amp;lt;&amp;lt; (8 - numBytes) * 8) - 1;
       }
&lt;p&gt; else if (numBytes &amp;gt; 0) &lt;/p&gt;
{
-        p = ((long) base.getInt(0)) &amp;lt;&amp;lt; 32;
+        p = ((long) Platform.getInt(base, offset)) &amp;lt;&amp;lt; 32;
         mask = (1L &amp;lt;&amp;lt; (8 - numBytes) * 8) - 1;
       }
&lt;p&gt; else {&lt;br/&gt;
         p = 0;&lt;br/&gt;
@@ -286,13 +290,12 @@ public long getPrefix() {&lt;br/&gt;
    */&lt;br/&gt;
   public byte[] getBytes() {&lt;br/&gt;
     // avoid copy if `base` is `byte[]`&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;long offset = base.getBaseOffset();&lt;/li&gt;
	&lt;li&gt;if (offset == BYTE_ARRAY_OFFSET &amp;amp;&amp;amp; base instanceof ByteArrayMemoryBlock&lt;/li&gt;
	&lt;li&gt;&amp;amp;&amp;amp; (((ByteArrayMemoryBlock) base).getByteArray()).length == numBytes) {&lt;/li&gt;
	&lt;li&gt;return ((ByteArrayMemoryBlock) base).getByteArray();&lt;br/&gt;
+    if (offset == BYTE_ARRAY_OFFSET &amp;amp;&amp;amp; base instanceof byte[]&lt;br/&gt;
+      &amp;amp;&amp;amp; ((byte[]) base).length == numBytes) 
{
+      return (byte[]) base;
     }
&lt;p&gt; else &lt;/p&gt;
{
       byte[] bytes = new byte[numBytes];
-      base.writeTo(0, bytes, BYTE_ARRAY_OFFSET, numBytes);
+      copyMemory(base, offset, bytes, BYTE_ARRAY_OFFSET, numBytes);
       return bytes;
     }
&lt;p&gt;   }&lt;br/&gt;
@@ -322,7 +325,7 @@ public UTF8String substring(final int start, final int until) {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     if (i &amp;gt; j) &lt;/p&gt;
{
       byte[] bytes = new byte[i - j];
-      base.writeTo(j, bytes, BYTE_ARRAY_OFFSET, i - j);
+      copyMemory(base, offset + j, bytes, BYTE_ARRAY_OFFSET, i - j);
       return fromBytes(bytes);
     }
&lt;p&gt; else {&lt;br/&gt;
       return EMPTY_UTF8;&lt;br/&gt;
@@ -363,14 +366,14 @@ public boolean contains(final UTF8String substring) {&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Returns the byte at position `i`.&lt;br/&gt;
    */&lt;br/&gt;
   private byte getByte(int i) 
{
-    return base.getByte(i);
+    return Platform.getByte(base, offset + i);
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private boolean matchAt(final UTF8String s, int pos) {&lt;br/&gt;
     if (s.numBytes + pos &amp;gt; numBytes || pos &amp;lt; 0) &lt;/p&gt;
{
       return false;
     }&lt;br/&gt;
-    return ByteArrayMethods.arrayEqualsBlock(base, pos, s.base, 0, s.numBytes);&lt;br/&gt;
+    return ByteArrayMethods.arrayEquals(base, offset + pos, s.base, s.offset, s.numBytes);&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   public boolean startsWith(final UTF8String prefix) {&lt;br/&gt;
@@ -497,7 +500,8 @@ public int findInSet(UTF8String match) {&lt;br/&gt;
     for (int i = 0; i &amp;lt; numBytes; i++) {&lt;br/&gt;
       if (getByte&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; == (byte) &apos;,&apos;) {&lt;br/&gt;
         if (i - (lastComma + 1) == match.numBytes &amp;amp;&amp;amp;&lt;br/&gt;
-          ByteArrayMethods.arrayEqualsBlock(base, lastComma + 1, match.base, 0, match.numBytes)) {&lt;br/&gt;
+          ByteArrayMethods.arrayEquals(base, offset + (lastComma + 1), match.base, match.offset,&lt;br/&gt;
+            match.numBytes)) {
           return n;
         }&lt;br/&gt;
         lastComma = i;&lt;br/&gt;
@@ -505,7 +509,8 @@ public int findInSet(UTF8String match) {&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
     if (numBytes - (lastComma + 1) == match.numBytes &amp;amp;&amp;amp;&lt;br/&gt;
-      ByteArrayMethods.arrayEqualsBlock(base, lastComma + 1, match.base, 0, match.numBytes)) {&lt;br/&gt;
+      ByteArrayMethods.arrayEquals(base, offset + (lastComma + 1), match.base, match.offset,&lt;br/&gt;
+        match.numBytes)) {
       return n;
     }&lt;br/&gt;
     return 0;&lt;br/&gt;
@@ -520,7 +525,7 @@ public int findInSet(UTF8String match) {&lt;br/&gt;
   private UTF8String copyUTF8String(int start, int end) {
     int len = end - start + 1;
     byte[] newBytes = new byte[len];
-    base.writeTo(start, newBytes, BYTE_ARRAY_OFFSET, len);
+    copyMemory(base, offset + start, newBytes, BYTE_ARRAY_OFFSET, len);
     return UTF8String.fromBytes(newBytes);
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -667,7 +672,8 @@ public UTF8String reverse() {&lt;br/&gt;
     int i = 0; // position in byte&lt;br/&gt;
     while (i &amp;lt; numBytes) {
       int len = numBytesForFirstByte(getByte(i));
-      base.writeTo(i, result, BYTE_ARRAY_OFFSET + result.length - i - len, len);
+      copyMemory(this.base, this.offset + i, result,
+        BYTE_ARRAY_OFFSET + result.length - i - len, len);
 
       i += len;
     }&lt;br/&gt;
@@ -681,7 +687,7 @@ public UTF8String repeat(int times) {&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     byte[] newBytes = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;numBytes * times&amp;#93;&lt;/span&gt;;&lt;br/&gt;
-    base.writeTo(0, newBytes, BYTE_ARRAY_OFFSET, numBytes);&lt;br/&gt;
+    copyMemory(this.base, this.offset, newBytes, BYTE_ARRAY_OFFSET, numBytes);&lt;br/&gt;
 &lt;br/&gt;
     int copied = 1;&lt;br/&gt;
     while (copied &amp;lt; times) {&lt;br/&gt;
@@ -718,7 +724,7 @@ public int indexOf(UTF8String v, int start) {&lt;br/&gt;
       if (i + v.numBytes &amp;gt; numBytes) {
         return -1;
       }&lt;br/&gt;
-      if (ByteArrayMethods.arrayEqualsBlock(base, i, v.base, 0, v.numBytes)) {&lt;br/&gt;
+      if (ByteArrayMethods.arrayEquals(base, offset + i, v.base, v.offset, v.numBytes)) {
         return c;
       }&lt;br/&gt;
       i += numBytesForFirstByte(getByte&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;);&lt;br/&gt;
@@ -734,7 +740,7 @@ public int indexOf(UTF8String v, int start) {&lt;br/&gt;
   private int find(UTF8String str, int start) {&lt;br/&gt;
     assert (str.numBytes &amp;gt; 0);&lt;br/&gt;
     while (start &amp;lt;= numBytes - str.numBytes) {&lt;br/&gt;
-      if (ByteArrayMethods.arrayEqualsBlock(base, start, str.base, 0, str.numBytes)) {&lt;br/&gt;
+      if (ByteArrayMethods.arrayEquals(base, offset + start, str.base, str.offset, str.numBytes)) {
         return start;
       }&lt;br/&gt;
       start += 1;&lt;br/&gt;
@@ -748,7 +754,7 @@ private int find(UTF8String str, int start) {&lt;br/&gt;
   private int rfind(UTF8String str, int start) {&lt;br/&gt;
     assert (str.numBytes &amp;gt; 0);&lt;br/&gt;
     while (start &amp;gt;= 0) {&lt;br/&gt;
-      if (ByteArrayMethods.arrayEqualsBlock(base, start, str.base, 0, str.numBytes)) {&lt;br/&gt;
+      if (ByteArrayMethods.arrayEquals(base, offset + start, str.base, str.offset, str.numBytes)) {         return start;       }&lt;br/&gt;
       start -= 1;&lt;br/&gt;
@@ -781,7 +787,7 @@ public UTF8String subStringIndex(UTF8String delim, int count) {
         return EMPTY_UTF8;
       }&lt;br/&gt;
       byte[] bytes = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;idx&amp;#93;&lt;/span&gt;;&lt;br/&gt;
-      base.writeTo(0, bytes, BYTE_ARRAY_OFFSET, idx);&lt;br/&gt;
+      copyMemory(base, offset, bytes, BYTE_ARRAY_OFFSET, idx);&lt;br/&gt;
       return fromBytes(bytes);&lt;br/&gt;
 &lt;br/&gt;
     } else {&lt;br/&gt;
@@ -801,7 +807,7 @@ public UTF8String subStringIndex(UTF8String delim, int count) {&lt;br/&gt;
       }&lt;br/&gt;
       int size = numBytes - delim.numBytes - idx;&lt;br/&gt;
       byte[] bytes = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;size&amp;#93;&lt;/span&gt;;&lt;br/&gt;
-      base.writeTo(idx + delim.numBytes, bytes, BYTE_ARRAY_OFFSET, size);&lt;br/&gt;
+      copyMemory(base, offset + idx + delim.numBytes, bytes, BYTE_ARRAY_OFFSET, size);&lt;br/&gt;
       return fromBytes(bytes);&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
@@ -824,15 +830,15 @@ public UTF8String rpad(int len, UTF8String pad) {&lt;br/&gt;
       UTF8String remain = pad.substring(0, spaces - padChars * count);&lt;br/&gt;
 &lt;br/&gt;
       byte[] data = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;this.numBytes + pad.numBytes * count + remain.numBytes&amp;#93;&lt;/span&gt;;&lt;br/&gt;
-      base.writeTo(0, data, BYTE_ARRAY_OFFSET, this.numBytes);&lt;br/&gt;
+      copyMemory(this.base, this.offset, data, BYTE_ARRAY_OFFSET, this.numBytes);&lt;br/&gt;
       int offset = this.numBytes;&lt;br/&gt;
       int idx = 0;&lt;br/&gt;
       while (idx &amp;lt; count) {
-        pad.base.writeTo(0, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);
+        copyMemory(pad.base, pad.offset, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);
         ++ idx;
         offset += pad.numBytes;
       }&lt;br/&gt;
-      remain.base.writeTo(0, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);&lt;br/&gt;
+      copyMemory(remain.base, remain.offset, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);&lt;br/&gt;
 &lt;br/&gt;
       return UTF8String.fromBytes(data);&lt;br/&gt;
     }&lt;br/&gt;
@@ -860,13 +866,13 @@ public UTF8String lpad(int len, UTF8String pad) {&lt;br/&gt;
       int offset = 0;&lt;br/&gt;
       int idx = 0;&lt;br/&gt;
       while (idx &amp;lt; count) {-        pad.base.writeTo(0, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);+        copyMemory(pad.base, pad.offset, data, BYTE_ARRAY_OFFSET + offset, pad.numBytes);         ++ idx;         offset += pad.numBytes;       }&lt;br/&gt;
-      remain.base.writeTo(0, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);&lt;br/&gt;
+      copyMemory(remain.base, remain.offset, data, BYTE_ARRAY_OFFSET + offset, remain.numBytes);&lt;br/&gt;
       offset += remain.numBytes;&lt;br/&gt;
-      base.writeTo(0, data, BYTE_ARRAY_OFFSET + offset, numBytes());&lt;br/&gt;
+      copyMemory(this.base, this.offset, data, BYTE_ARRAY_OFFSET + offset, numBytes());&lt;br/&gt;
 &lt;br/&gt;
       return UTF8String.fromBytes(data);&lt;br/&gt;
     }&lt;br/&gt;
@@ -891,8 +897,8 @@ public static UTF8String concat(UTF8String... inputs) {&lt;br/&gt;
     int offset = 0;&lt;br/&gt;
     for (int i = 0; i &amp;lt; inputs.length; i++) {&lt;br/&gt;
       int len = inputs&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;.numBytes;&lt;br/&gt;
-      inputs&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;.base.writeTo(&lt;br/&gt;
-        0,&lt;br/&gt;
+      copyMemory(&lt;br/&gt;
+        inputs&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;.base, inputs&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;.offset,&lt;br/&gt;
         result, BYTE_ARRAY_OFFSET + offset,&lt;br/&gt;
         len);&lt;br/&gt;
       offset += len;&lt;br/&gt;
@@ -931,8 +937,8 @@ public static UTF8String concatWs(UTF8String separator, UTF8String... inputs) {&lt;br/&gt;
     for (int i = 0, j = 0; i &amp;lt; inputs.length; i++) {&lt;br/&gt;
       if (inputs&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt; != null) {&lt;br/&gt;
         int len = inputs&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;.numBytes;&lt;br/&gt;
-        inputs&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;.base.writeTo(&lt;br/&gt;
-          0,&lt;br/&gt;
+        copyMemory(&lt;br/&gt;
+          inputs&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;.base, inputs&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;.offset,&lt;br/&gt;
           result, BYTE_ARRAY_OFFSET + offset,&lt;br/&gt;
           len);&lt;br/&gt;
         offset += len;&lt;br/&gt;
@@ -940,8 +946,8 @@ public static UTF8String concatWs(UTF8String separator, UTF8String... inputs) {&lt;br/&gt;
         j++;&lt;br/&gt;
         // Add separator if this is not the last input.&lt;br/&gt;
         if (j &amp;lt; numInputs) {&lt;br/&gt;
-          separator.base.writeTo(&lt;br/&gt;
-            0,&lt;br/&gt;
+          copyMemory(&lt;br/&gt;
+            separator.base, separator.offset,&lt;br/&gt;
             result, BYTE_ARRAY_OFFSET + offset,&lt;br/&gt;
             separator.numBytes);&lt;br/&gt;
           offset += separator.numBytes;&lt;br/&gt;
@@ -952,6 +958,12 @@ public static UTF8String concatWs(UTF8String separator, UTF8String... inputs) {&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   public UTF8String[] split(UTF8String pattern, int limit) {&lt;br/&gt;
+    // Java String&apos;s split method supports &quot;ignore empty string&quot; behavior when the limit is 0&lt;br/&gt;
+    // whereas other languages do not. To avoid this java specific behavior, we fall back to&lt;br/&gt;
+    // -1 when the limit is 0.&lt;br/&gt;
+    if (limit == 0) {
+      limit = -1;
+    }&lt;br/&gt;
     String[] splits = toString().split(pattern.toString(), limit);&lt;br/&gt;
     UTF8String[] res = new UTF8String&lt;span class=&quot;error&quot;&gt;&amp;#91;splits.length&amp;#93;&lt;/span&gt;;&lt;br/&gt;
     for (int i = 0; i &amp;lt; res.length; i++) {&lt;br/&gt;
@@ -1215,7 +1227,7 @@ public UTF8String clone() {&lt;br/&gt;
 &lt;br/&gt;
   public UTF8String copy() {
     byte[] bytes = new byte[numBytes];
-    base.writeTo(0, bytes, BYTE_ARRAY_OFFSET, numBytes);
+    copyMemory(base, offset, bytes, BYTE_ARRAY_OFFSET, numBytes);
     return fromBytes(bytes);
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -1223,10 +1235,11 @@ public UTF8String copy() {&lt;br/&gt;
   public int compareTo(@Nonnull final UTF8String other) {&lt;br/&gt;
     int len = Math.min(numBytes, other.numBytes);&lt;br/&gt;
     int wordMax = (len / 8) * 8;&lt;br/&gt;
-    MemoryBlock rbase = other.base;&lt;br/&gt;
+    long roffset = other.offset;&lt;br/&gt;
+    Object rbase = other.base;&lt;br/&gt;
     for (int i = 0; i &amp;lt; wordMax; i += 8) {&lt;br/&gt;
-      long left = base.getLong&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
-      long right = rbase.getLong&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;br/&gt;
+      long left = getLong(base, offset + i);&lt;br/&gt;
+      long right = getLong(rbase, roffset + i);&lt;br/&gt;
       if (left != right) {&lt;br/&gt;
         if (IS_LITTLE_ENDIAN) {&lt;br/&gt;
           return Long.compareUnsigned(Long.reverseBytes(left), Long.reverseBytes(right));&lt;br/&gt;
@@ -1237,7 +1250,7 @@ public int compareTo(@Nonnull final UTF8String other) {&lt;br/&gt;
     }&lt;br/&gt;
     for (int i = wordMax; i &amp;lt; len; i++) {&lt;br/&gt;
       // In UTF-8, the byte should be unsigned, so we should compare them as unsigned int.&lt;br/&gt;
-      int res = (getByte&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &amp;amp; 0xFF) - (rbase.getByte&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &amp;amp; 0xFF);&lt;br/&gt;
+      int res = (getByte&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &amp;amp; 0xFF) - (Platform.getByte(rbase, roffset + i) &amp;amp; 0xFF);&lt;br/&gt;
       if (res != 0) {
         return res;
       }&lt;br/&gt;
@@ -1256,7 +1269,7 @@ public boolean equals(final Object other) {&lt;br/&gt;
       if (numBytes != o.numBytes) {
         return false;
       }&lt;br/&gt;
-      return ByteArrayMethods.arrayEqualsBlock(base, 0, o.base, 0, numBytes);&lt;br/&gt;
+      return ByteArrayMethods.arrayEquals(base, offset, o.base, o.offset, numBytes);&lt;br/&gt;
     } else {       return false;     }
&lt;p&gt;@@ -1312,8 +1325,8 @@ public int levenshteinDistance(UTF8String other) {&lt;br/&gt;
               num_bytes_j != numBytesForFirstByte(s.getByte(i_bytes))) &lt;/p&gt;
{
           cost = 1;
         }
&lt;p&gt; else &lt;/p&gt;
{
-          cost = (ByteArrayMethods.arrayEqualsBlock(t.base, j_bytes, s.base,
-            i_bytes, num_bytes_j)) ? 0 : 1;
+          cost = (ByteArrayMethods.arrayEquals(t.base, t.offset + j_bytes, s.base,
+              s.offset + i_bytes, num_bytes_j)) ? 0 : 1;
         }
&lt;p&gt;         d&lt;span class=&quot;error&quot;&gt;&amp;#91;i + 1&amp;#93;&lt;/span&gt; = Math.min(Math.min(d&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt; + 1, p&lt;span class=&quot;error&quot;&gt;&amp;#91;i + 1&amp;#93;&lt;/span&gt; + 1), p&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt; + cost);&lt;br/&gt;
       }&lt;br/&gt;
@@ -1328,7 +1341,7 @@ public int levenshteinDistance(UTF8String other) {&lt;/p&gt;

&lt;p&gt;   @Override&lt;br/&gt;
   public int hashCode() &lt;/p&gt;
{
-    return Murmur3_x86_32.hashUnsafeBytesBlock(base,42);
+    return Murmur3_x86_32.hashUnsafeBytes(base, offset, numBytes, 42);
   }

&lt;p&gt;   /**&lt;br/&gt;
@@ -1391,10 +1404,10 @@ public void writeExternal(ObjectOutput out) throws IOException {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   public void readExternal(ObjectInput in) throws IOException, ClassNotFoundException &lt;/p&gt;
{
+    offset = BYTE_ARRAY_OFFSET;
     numBytes = in.readInt();
-    byte[] bytes = new byte[numBytes];
-    in.readFully(bytes);
-    base = ByteArrayMemoryBlock.fromArray(bytes);
+    base = new byte[numBytes];
+    in.readFully((byte[]) base);
   }

&lt;p&gt;   @Override&lt;br/&gt;
@@ -1406,10 +1419,10 @@ public void write(Kryo kryo, Output out) {&lt;/p&gt;

&lt;p&gt;   @Override&lt;br/&gt;
   public void read(Kryo kryo, Input in) &lt;/p&gt;
{
-    numBytes = in.readInt();
-    byte[] bytes = new byte[numBytes];
-    in.read(bytes);
-    base = ByteArrayMemoryBlock.fromArray(bytes);
+    this.offset = BYTE_ARRAY_OFFSET;
+    this.numBytes = in.readInt();
+    this.base = new byte[numBytes];
+    in.read((byte[]) base);
   }

&lt;p&gt; }&lt;br/&gt;
diff --git a/common/unsafe/src/test/java/org/apache/spark/unsafe/PlatformUtilSuite.java b/common/unsafe/src/test/java/org/apache/spark/unsafe/PlatformUtilSuite.java&lt;br/&gt;
index 583a148b3845d..2474081dad5c9 100644&lt;br/&gt;
&amp;#8212; a/common/unsafe/src/test/java/org/apache/spark/unsafe/PlatformUtilSuite.java&lt;br/&gt;
+++ b/common/unsafe/src/test/java/org/apache/spark/unsafe/PlatformUtilSuite.java&lt;br/&gt;
@@ -81,7 +81,7 @@ public void freeingOnHeapMemoryBlockResetsBaseObjectAndOffset() &lt;/p&gt;
{
     MemoryAllocator.HEAP.free(block);
     Assert.assertNull(block.getBaseObject());
     Assert.assertEquals(0, block.getBaseOffset());
-    Assert.assertEquals(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER, block.getPageNumber());
+    Assert.assertEquals(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER, block.pageNumber);
   }

&lt;p&gt;   @Test&lt;br/&gt;
@@ -92,7 +92,7 @@ public void freeingOffHeapMemoryBlockResetsOffset() &lt;/p&gt;
{
     MemoryAllocator.UNSAFE.free(block);
     Assert.assertNull(block.getBaseObject());
     Assert.assertEquals(0, block.getBaseOffset());
-    Assert.assertEquals(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER, block.getPageNumber());
+    Assert.assertEquals(MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER, block.pageNumber);
   }

&lt;p&gt;   @Test(expected = AssertionError.class)&lt;br/&gt;
@@ -157,4 +157,22 @@ public void heapMemoryReuse() &lt;/p&gt;
{
     Assert.assertEquals(onheap4.size(), 1024 * 1024 + 7);
     Assert.assertEquals(obj3, onheap4.getBaseObject());
   }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  // &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26021&quot; title=&quot;-0.0 and 0.0 not treated consistently, doesn&amp;#39;t match Hive&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26021&quot;&gt;&lt;del&gt;SPARK-26021&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
+  public void writeMinusZeroIsReplacedWithZero() &lt;/p&gt;
{
+    byte[] doubleBytes = new byte[Double.BYTES];
+    byte[] floatBytes = new byte[Float.BYTES];
+    Platform.putDouble(doubleBytes, Platform.BYTE_ARRAY_OFFSET, -0.0d);
+    Platform.putFloat(floatBytes, Platform.BYTE_ARRAY_OFFSET, -0.0f);
+
+    byte[] doubleBytes2 = new byte[Double.BYTES];
+    byte[] floatBytes2 = new byte[Float.BYTES];
+    Platform.putDouble(doubleBytes, Platform.BYTE_ARRAY_OFFSET, 0.0d);
+    Platform.putFloat(floatBytes, Platform.BYTE_ARRAY_OFFSET, 0.0f);
+
+    // Make sure the bytes we write from 0.0 and -0.0 are same.
+    Assert.assertArrayEquals(doubleBytes, doubleBytes2);
+    Assert.assertArrayEquals(floatBytes, floatBytes2);
+  }
&lt;p&gt; }&lt;br/&gt;
diff --git a/common/unsafe/src/test/java/org/apache/spark/unsafe/array/LongArraySuite.java b/common/unsafe/src/test/java/org/apache/spark/unsafe/array/LongArraySuite.java&lt;br/&gt;
index 8c2e98c2bfc54..fb8e53b3348f3 100644&lt;br/&gt;
&amp;#8212; a/common/unsafe/src/test/java/org/apache/spark/unsafe/array/LongArraySuite.java&lt;br/&gt;
+++ b/common/unsafe/src/test/java/org/apache/spark/unsafe/array/LongArraySuite.java&lt;br/&gt;
@@ -20,13 +20,14 @@&lt;br/&gt;
 import org.junit.Assert;&lt;br/&gt;
 import org.junit.Test;&lt;/p&gt;

&lt;p&gt;-import org.apache.spark.unsafe.memory.OnHeapMemoryBlock;&lt;br/&gt;
+import org.apache.spark.unsafe.memory.MemoryBlock;&lt;/p&gt;

&lt;p&gt; public class LongArraySuite {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   public void basicTest() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LongArray arr = new LongArray(new OnHeapMemoryBlock(16));&lt;br/&gt;
+    long[] bytes = new long&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+    LongArray arr = new LongArray(MemoryBlock.fromLongArray(bytes));&lt;br/&gt;
     arr.set(0, 1L);&lt;br/&gt;
     arr.set(1, 2L);&lt;br/&gt;
     arr.set(1, 3L);&lt;br/&gt;
diff --git a/common/unsafe/src/test/java/org/apache/spark/unsafe/hash/Murmur3_x86_32Suite.java b/common/unsafe/src/test/java/org/apache/spark/unsafe/hash/Murmur3_x86_32Suite.java&lt;br/&gt;
index d9898771720ae..6348a73bf3895 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/src/test/java/org/apache/spark/unsafe/hash/Murmur3_x86_32Suite.java&lt;br/&gt;
+++ b/common/unsafe/src/test/java/org/apache/spark/unsafe/hash/Murmur3_x86_32Suite.java&lt;br/&gt;
@@ -70,24 +70,6 @@ public void testKnownBytesInputs() 
{
       Murmur3_x86_32.hashUnsafeBytes2(tes, Platform.BYTE_ARRAY_OFFSET, tes.length, 0));
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testKnownWordsInputs() {&lt;/li&gt;
	&lt;li&gt;byte[] bytes = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;16&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;long offset = Platform.BYTE_ARRAY_OFFSET;&lt;/li&gt;
	&lt;li&gt;for (int i = 0; i &amp;lt; 16; i++) 
{
-      bytes[i] = 0;
-    }&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(-300363099, Murmur3_x86_32.hashUnsafeWords(bytes, offset, 16, 42));&lt;/li&gt;
	&lt;li&gt;for (int i = 0; i &amp;lt; 16; i++) 
{
-      bytes[i] = -1;
-    }&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(-1210324667, Murmur3_x86_32.hashUnsafeWords(bytes, offset, 16, 42));&lt;/li&gt;
	&lt;li&gt;for (int i = 0; i &amp;lt; 16; i++) 
{
-      bytes[i] = (byte)i;
-    }&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(-634919701, Murmur3_x86_32.hashUnsafeWords(bytes, offset, 16, 42));&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;br/&gt;
   @Test&lt;br/&gt;
   public void randomizedStressTest() {&lt;br/&gt;
     int size = 65536;&lt;br/&gt;
diff --git a/common/unsafe/src/test/java/org/apache/spark/unsafe/memory/MemoryBlockSuite.java b/common/unsafe/src/test/java/org/apache/spark/unsafe/memory/MemoryBlockSuite.java&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index ef5ff8ee70ec0..0000000000000
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/src/test/java/org/apache/spark/unsafe/memory/MemoryBlockSuite.java&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,179 +0,0 @@&lt;br/&gt;
-/*&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements.  See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License.  You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-&lt;br/&gt;
-package org.apache.spark.unsafe.memory;&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.spark.unsafe.Platform;&lt;br/&gt;
-import org.junit.Assert;&lt;br/&gt;
-import org.junit.Test;&lt;br/&gt;
-&lt;br/&gt;
-import java.nio.ByteOrder;&lt;br/&gt;
-&lt;br/&gt;
-import static org.hamcrest.core.StringContains.containsString;&lt;br/&gt;
-&lt;br/&gt;
-public class MemoryBlockSuite {&lt;/li&gt;
	&lt;li&gt;private static final boolean bigEndianPlatform =&lt;/li&gt;
	&lt;li&gt;ByteOrder.nativeOrder().equals(ByteOrder.BIG_ENDIAN);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private void check(MemoryBlock memory, Object obj, long offset, int length) {&lt;/li&gt;
	&lt;li&gt;memory.setPageNumber(1);&lt;/li&gt;
	&lt;li&gt;memory.fill((byte)-1);&lt;/li&gt;
	&lt;li&gt;memory.putBoolean(0, true);&lt;/li&gt;
	&lt;li&gt;memory.putByte(1, (byte)127);&lt;/li&gt;
	&lt;li&gt;memory.putShort(2, (short)257);&lt;/li&gt;
	&lt;li&gt;memory.putInt(4, 0x20000002);&lt;/li&gt;
	&lt;li&gt;memory.putLong(8, 0x1234567089ABCDEFL);&lt;/li&gt;
	&lt;li&gt;memory.putFloat(16, 1.0F);&lt;/li&gt;
	&lt;li&gt;memory.putLong(20, 0x1234567089ABCDEFL);&lt;/li&gt;
	&lt;li&gt;memory.putDouble(28, 2.0);&lt;/li&gt;
	&lt;li&gt;MemoryBlock.copyMemory(memory, 0L, memory, 36, 4);&lt;/li&gt;
	&lt;li&gt;int[] a = new int&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;a&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; = 0x12345678;&lt;/li&gt;
	&lt;li&gt;a&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; = 0x13579BDF;&lt;/li&gt;
	&lt;li&gt;memory.copyFrom(a, Platform.INT_ARRAY_OFFSET, 40, 8);&lt;/li&gt;
	&lt;li&gt;byte[] b = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;8&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;memory.writeTo(40, b, Platform.BYTE_ARRAY_OFFSET, 8);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(obj, memory.getBaseObject());&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(offset, memory.getBaseOffset());&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(length, memory.size());&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(1, memory.getPageNumber());&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(true, memory.getBoolean(0));&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals((byte)127, memory.getByte(1 ));&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals((short)257, memory.getShort(2));&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(0x20000002, memory.getInt(4));&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(0x1234567089ABCDEFL, memory.getLong(8));&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(1.0F, memory.getFloat(16), 0);&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(0x1234567089ABCDEFL, memory.getLong(20));&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(2.0, memory.getDouble(28), 0);&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(true, memory.getBoolean(36));&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals((byte)127, memory.getByte(37 ));&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals((short)257, memory.getShort(38));&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(a&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, memory.getInt(40));&lt;/li&gt;
	&lt;li&gt;Assert.assertEquals(a&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, memory.getInt(44));&lt;/li&gt;
	&lt;li&gt;if (bigEndianPlatform) 
{
-      Assert.assertEquals(a[0],
-        ((int)b[0] &amp;amp; 0xff) &amp;lt;&amp;lt; 24 | ((int)b[1] &amp;amp; 0xff) &amp;lt;&amp;lt; 16 |
-        ((int)b[2] &amp;amp; 0xff) &amp;lt;&amp;lt; 8 | ((int)b[3] &amp;amp; 0xff));
-      Assert.assertEquals(a[1],
-        ((int)b[4] &amp;amp; 0xff) &amp;lt;&amp;lt; 24 | ((int)b[5] &amp;amp; 0xff) &amp;lt;&amp;lt; 16 |
-        ((int)b[6] &amp;amp; 0xff) &amp;lt;&amp;lt; 8 | ((int)b[7] &amp;amp; 0xff));
-    }
&lt;p&gt; else &lt;/p&gt;
{
-      Assert.assertEquals(a[0],
-        ((int)b[3] &amp;amp; 0xff) &amp;lt;&amp;lt; 24 | ((int)b[2] &amp;amp; 0xff) &amp;lt;&amp;lt; 16 |
-        ((int)b[1] &amp;amp; 0xff) &amp;lt;&amp;lt; 8 | ((int)b[0] &amp;amp; 0xff));
-      Assert.assertEquals(a[1],
-        ((int)b[7] &amp;amp; 0xff) &amp;lt;&amp;lt; 24 | ((int)b[6] &amp;amp; 0xff) &amp;lt;&amp;lt; 16 |
-        ((int)b[5] &amp;amp; 0xff) &amp;lt;&amp;lt; 8 | ((int)b[4] &amp;amp; 0xff));
-    }&lt;/li&gt;
	&lt;li&gt;for (int i = 48; i &amp;lt; memory.size(); i++) 
{
-      Assert.assertEquals((byte) -1, memory.getByte(i));
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;assert(memory.subBlock(0, memory.size()) == memory);&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;try 
{
-      memory.subBlock(-8, 8);
-      Assert.fail();
-    }
&lt;p&gt; catch (Exception expected) &lt;/p&gt;
{
-      Assert.assertThat(expected.getMessage(), containsString(&quot;non-negative&quot;));
-    }&lt;br/&gt;
-&lt;br/&gt;
-    try {
-      memory.subBlock(0, -8);
-      Assert.fail();
-    } catch (Exception expected) {-      Assert.assertThat(expected.getMessage(), containsString(&quot;non-negative&quot;));-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;try 
{
-      memory.subBlock(0, length + 8);
-      Assert.fail();
-    }
&lt;p&gt; catch (Exception expected) &lt;/p&gt;
{
-      Assert.assertThat(expected.getMessage(), containsString(&quot;should not be larger than&quot;));
-    }&lt;br/&gt;
-&lt;br/&gt;
-    try {
-      memory.subBlock(8, length - 4);
-      Assert.fail();
-    } catch (Exception expected) {-      Assert.assertThat(expected.getMessage(), containsString(&quot;should not be larger than&quot;));-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;try 
{
-      memory.subBlock(length + 8, 4);
-      Assert.fail();
-    }
&lt;p&gt; catch (Exception expected) &lt;/p&gt;
{
-      Assert.assertThat(expected.getMessage(), containsString(&quot;should not be larger than&quot;));
-    }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;memory.setPageNumber(MemoryBlock.NO_PAGE_NUMBER);&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testByteArrayMemoryBlock() 
{
-    byte[] obj = new byte[56];
-    long offset = Platform.BYTE_ARRAY_OFFSET;
-    int length = obj.length;
-
-    MemoryBlock memory = new ByteArrayMemoryBlock(obj, offset, length);
-    check(memory, obj, offset, length);
-
-    memory = ByteArrayMemoryBlock.fromArray(obj);
-    check(memory, obj, offset, length);
-
-    obj = new byte[112];
-    memory = new ByteArrayMemoryBlock(obj, offset, length);
-    check(memory, obj, offset, length);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testOnHeapMemoryBlock() 
{
-    long[] obj = new long[7];
-    long offset = Platform.LONG_ARRAY_OFFSET;
-    int length = obj.length * 8;
-
-    MemoryBlock memory = new OnHeapMemoryBlock(obj, offset, length);
-    check(memory, obj, offset, length);
-
-    memory = OnHeapMemoryBlock.fromArray(obj);
-    check(memory, obj, offset, length);
-
-    obj = new long[14];
-    memory = new OnHeapMemoryBlock(obj, offset, length);
-    check(memory, obj, offset, length);
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testOffHeapArrayMemoryBlock() 
{
-    MemoryAllocator memoryAllocator = new UnsafeMemoryAllocator();
-    MemoryBlock memory = memoryAllocator.allocate(56);
-    Object obj = memory.getBaseObject();
-    long offset = memory.getBaseOffset();
-    int length = 56;
-
-    check(memory, obj, offset, length);
-    memoryAllocator.free(memory);
-
-    long address = Platform.allocateMemory(112);
-    memory = new OffHeapMemoryBlock(address, length);
-    obj = memory.getBaseObject();
-    offset = memory.getBaseOffset();
-    check(memory, obj, offset, length);
-    Platform.freeMemory(address);
-  }
&lt;p&gt;-}&lt;br/&gt;
diff --git a/common/unsafe/src/test/java/org/apache/spark/unsafe/types/UTF8StringSuite.java b/common/unsafe/src/test/java/org/apache/spark/unsafe/types/UTF8StringSuite.java&lt;br/&gt;
index 42dda30480702..cf9cc6b1800a9 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/common/unsafe/src/test/java/org/apache/spark/unsafe/types/UTF8StringSuite.java&lt;br/&gt;
+++ b/common/unsafe/src/test/java/org/apache/spark/unsafe/types/UTF8StringSuite.java&lt;br/&gt;
@@ -25,8 +25,7 @@&lt;br/&gt;
 import java.util.*;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import com.google.common.collect.ImmutableMap;&lt;br/&gt;
-import org.apache.spark.unsafe.memory.ByteArrayMemoryBlock;&lt;br/&gt;
-import org.apache.spark.unsafe.memory.OnHeapMemoryBlock;&lt;br/&gt;
+import org.apache.spark.unsafe.Platform;&lt;br/&gt;
 import org.junit.Test;&lt;/p&gt;

&lt;p&gt; import static org.junit.Assert.*;&lt;br/&gt;
@@ -394,12 +393,14 @@ public void substringSQL() {&lt;/p&gt;

&lt;p&gt;   @Test&lt;br/&gt;
   public void split() {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertTrue(Arrays.equals(fromString(&quot;ab,def,ghi&quot;).split(fromString(&quot;,&quot;), -1),&lt;/li&gt;
	&lt;li&gt;new UTF8String[]
{fromString(&quot;ab&quot;), fromString(&quot;def&quot;), fromString(&quot;ghi&quot;)}
&lt;p&gt;));&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;assertTrue(Arrays.equals(fromString(&quot;ab,def,ghi&quot;).split(fromString(&quot;,&quot;), 2),&lt;/li&gt;
	&lt;li&gt;new UTF8String[]
{fromString(&quot;ab&quot;), fromString(&quot;def,ghi&quot;)}));&lt;br/&gt;
-    assertTrue(Arrays.equals(fromString(&quot;ab,def,ghi&quot;).split(fromString(&quot;,&quot;), 2),&lt;br/&gt;
-      new UTF8String[]{fromString(&quot;ab&quot;), fromString(&quot;def,ghi&quot;)}
&lt;p&gt;));&lt;br/&gt;
+    UTF8String[] negativeAndZeroLimitCase =&lt;br/&gt;
+      new UTF8String[]&lt;/p&gt;
{fromString(&quot;ab&quot;), fromString(&quot;def&quot;), fromString(&quot;ghi&quot;), fromString(&quot;&quot;)}
&lt;p&gt;;&lt;br/&gt;
+    assertTrue(Arrays.equals(fromString(&quot;ab,def,ghi,&quot;).split(fromString(&quot;,&quot;), 0),&lt;br/&gt;
+      negativeAndZeroLimitCase));&lt;br/&gt;
+    assertTrue(Arrays.equals(fromString(&quot;ab,def,ghi,&quot;).split(fromString(&quot;,&quot;), -1),&lt;br/&gt;
+      negativeAndZeroLimitCase));&lt;br/&gt;
+    assertTrue(Arrays.equals(fromString(&quot;ab,def,ghi,&quot;).split(fromString(&quot;,&quot;), 2),&lt;br/&gt;
+      new UTF8String[]&lt;/p&gt;
{fromString(&quot;ab&quot;), fromString(&quot;def,ghi,&quot;)}
&lt;p&gt;));&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
@@ -513,6 +514,21 @@ public void soundex() &lt;/p&gt;
{
     assertEquals(fromString(&quot;&#19990;&#30028;&#21315;&#19990;&quot;).soundex(), fromString(&quot;&#19990;&#30028;&#21315;&#19990;&quot;));
   }

&lt;p&gt;+  @Test&lt;br/&gt;
+  public void writeToOutputStreamUnderflow() throws IOException {&lt;br/&gt;
+    // offset underflow is apparently supported?&lt;br/&gt;
+    final ByteArrayOutputStream outputStream = new ByteArrayOutputStream();&lt;br/&gt;
+    final byte[] test = &quot;01234567&quot;.getBytes(StandardCharsets.UTF_8);&lt;br/&gt;
+&lt;br/&gt;
+    for (int i = 1; i &amp;lt;= Platform.BYTE_ARRAY_OFFSET; ++i) &lt;/p&gt;
{
+      UTF8String.fromAddress(test, Platform.BYTE_ARRAY_OFFSET - i, test.length + i)
+          .writeTo(outputStream);
+      final ByteBuffer buffer = ByteBuffer.wrap(outputStream.toByteArray(), i, test.length);
+      assertEquals(&quot;01234567&quot;, StandardCharsets.UTF_8.decode(buffer).toString());
+      outputStream.reset();
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
   @Test&lt;br/&gt;
   public void writeToOutputStreamSlice() throws IOException {&lt;br/&gt;
     final ByteArrayOutputStream outputStream = new ByteArrayOutputStream();&lt;br/&gt;
@@ -520,7 +536,7 @@ public void writeToOutputStreamSlice() throws IOException {&lt;/p&gt;

&lt;p&gt;     for (int i = 0; i &amp;lt; test.length; ++i) {&lt;br/&gt;
       for (int j = 0; j &amp;lt; test.length - i; ++j) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new UTF8String(ByteArrayMemoryBlock.fromArray(test).subBlock(i, j))&lt;br/&gt;
+        UTF8String.fromAddress(test, Platform.BYTE_ARRAY_OFFSET + i, j)&lt;br/&gt;
             .writeTo(outputStream);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         assertArrayEquals(Arrays.copyOfRange(test, i, i + j), outputStream.toByteArray());&lt;br/&gt;
@@ -551,7 +567,7 @@ public void writeToOutputStreamOverflow() throws IOException {&lt;/p&gt;

&lt;p&gt;     for (final long offset : offsets) {&lt;br/&gt;
       try {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new UTF8String(ByteArrayMemoryBlock.fromArray(test).subBlock(offset, test.length))&lt;br/&gt;
+        fromAddress(test, BYTE_ARRAY_OFFSET + offset, test.length)&lt;br/&gt;
             .writeTo(outputStream);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         throw new IllegalStateException(Long.toString(offset));&lt;br/&gt;
@@ -578,25 +594,26 @@ public void writeToOutputStream() throws IOException {&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;   @Test&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public void writeToOutputStreamLongArray() throws IOException {&lt;br/&gt;
+  public void writeToOutputStreamIntArray() throws IOException {&lt;br/&gt;
     // verify that writes work on objects that are not byte arrays&lt;/li&gt;
	&lt;li&gt;final ByteBuffer buffer = StandardCharsets.UTF_8.encode(&quot;3&#21315;&#22823;&#21315;&#19990;&#30028;&quot;);&lt;br/&gt;
+    final ByteBuffer buffer = StandardCharsets.UTF_8.encode(&quot;&#22823;&#21315;&#19990;&#30028;&quot;);&lt;br/&gt;
     buffer.position(0);&lt;br/&gt;
     buffer.order(ByteOrder.nativeOrder());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     final int length = buffer.limit();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assertEquals(16, length);&lt;br/&gt;
+    assertEquals(12, length);&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final int longs = length / 8;&lt;/li&gt;
	&lt;li&gt;final long[] array = new long&lt;span class=&quot;error&quot;&gt;&amp;#91;longs&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+    final int ints = length / 4;&lt;br/&gt;
+    final int[] array = new int&lt;span class=&quot;error&quot;&gt;&amp;#91;ints&amp;#93;&lt;/span&gt;;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (int i = 0; i &amp;lt; longs; ++i) {&lt;/li&gt;
	&lt;li&gt;array&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt; = buffer.getLong();&lt;br/&gt;
+    for (int i = 0; i &amp;lt; ints; ++i) 
{
+      array[i] = buffer.getInt();
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     final ByteArrayOutputStream outputStream = new ByteArrayOutputStream();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new UTF8String(OnHeapMemoryBlock.fromArray(array)).writeTo(outputStream);&lt;/li&gt;
	&lt;li&gt;assertEquals(&quot;3&#21315;&#22823;&#21315;&#19990;&#30028;&quot;, outputStream.toString(&quot;UTF-8&quot;));&lt;br/&gt;
+    fromAddress(array, Platform.INT_ARRAY_OFFSET, length)&lt;br/&gt;
+        .writeTo(outputStream);&lt;br/&gt;
+    assertEquals(&quot;&#22823;&#21315;&#19990;&#30028;&quot;, outputStream.toString(&quot;UTF-8&quot;));&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @Test&lt;br/&gt;
diff --git a/common/unsafe/src/test/scala/org/apache/spark/unsafe/types/UTF8StringPropertyCheckSuite.scala b/common/unsafe/src/test/scala/org/apache/spark/unsafe/types/UTF8StringPropertyCheckSuite.scala&lt;br/&gt;
index 7d3331f44f015..fdb81a06d41c9 100644&lt;br/&gt;
&amp;#8212; a/common/unsafe/src/test/scala/org/apache/spark/unsafe/types/UTF8StringPropertyCheckSuite.scala&lt;br/&gt;
+++ b/common/unsafe/src/test/scala/org/apache/spark/unsafe/types/UTF8StringPropertyCheckSuite.scala&lt;br/&gt;
@@ -17,7 +17,7 @@&lt;/p&gt;

&lt;p&gt; package org.apache.spark.unsafe.types&lt;/p&gt;

&lt;p&gt;-import org.apache.commons.lang3.StringUtils&lt;br/&gt;
+import org.apache.commons.text.similarity.LevenshteinDistance&lt;br/&gt;
 import org.scalacheck.&lt;/p&gt;
{Arbitrary, Gen}
&lt;p&gt; import org.scalatest.prop.GeneratorDrivenPropertyChecks&lt;br/&gt;
 // scalastyle:off&lt;br/&gt;
@@ -63,6 +63,7 @@ class UTF8StringPropertyCheckSuite extends FunSuite with GeneratorDrivenProperty&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  // scalastyle:off caselocale&lt;br/&gt;
   test(&quot;toUpperCase&quot;) {&lt;br/&gt;
     forAll &lt;/p&gt;
{ (s: String) =&amp;gt;
       assert(toUTF8(s).toUpperCase === toUTF8(s.toUpperCase))
@@ -74,6 +75,7 @@ class UTF8StringPropertyCheckSuite extends FunSuite with GeneratorDrivenProperty
       assert(toUTF8(s).toLowerCase === toUTF8(s.toLowerCase))
     }
&lt;p&gt;   }&lt;br/&gt;
+  // scalastyle:on caselocale&lt;/p&gt;

&lt;p&gt;   test(&quot;compare&quot;) {&lt;br/&gt;
     forAll { (s1: String, s2: String) =&amp;gt;&lt;br/&gt;
@@ -230,7 +232,7 @@ class UTF8StringPropertyCheckSuite extends FunSuite with GeneratorDrivenProperty&lt;br/&gt;
   test(&quot;levenshteinDistance&quot;) {&lt;br/&gt;
     forAll &lt;/p&gt;
{ (one: String, another: String) =&amp;gt;
       assert(toUTF8(one).levenshteinDistance(toUTF8(another)) ===
-        StringUtils.getLevenshteinDistance(one, another))
+        LevenshteinDistance.getDefaultInstance.apply(one, another))
     }
&lt;p&gt;   }&lt;/p&gt;

&lt;p&gt;diff --git a/core/benchmarks/KryoBenchmark-results.txt b/core/benchmarks/KryoBenchmark-results.txt&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..91e22f3afc14f&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/benchmarks/KryoBenchmark-results.txt&lt;br/&gt;
@@ -0,0 +1,29 @@&lt;br/&gt;
+================================================================================================&lt;br/&gt;
+Benchmark Kryo Unsafe vs safe Serialization&lt;br/&gt;
+================================================================================================&lt;br/&gt;
+&lt;br/&gt;
+Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11 on Mac OS X 10.13.6&lt;br/&gt;
+Intel(R) Core(TM) i7-6920HQ CPU @ 2.90GHz&lt;br/&gt;
+&lt;br/&gt;
+Benchmark Kryo Unsafe vs safe Serialization: Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative&lt;br/&gt;
+------------------------------------------------------------------------------------------------&lt;br/&gt;
+basicTypes: Int with unsafe:true               138 /  149          7.2         138.0       1.0X&lt;br/&gt;
+basicTypes: Long with unsafe:true              168 /  173          6.0         167.7       0.8X&lt;br/&gt;
+basicTypes: Float with unsafe:true             153 /  174          6.5         153.1       0.9X&lt;br/&gt;
+basicTypes: Double with unsafe:true            161 /  185          6.2         161.1       0.9X&lt;br/&gt;
+Array: Int with unsafe:true                      2 /    3        409.7           2.4      56.5X&lt;br/&gt;
+Array: Long with unsafe:true                     4 /    5        232.5           4.3      32.1X&lt;br/&gt;
+Array: Float with unsafe:true                    3 /    4        367.3           2.7      50.7X&lt;br/&gt;
+Array: Double with unsafe:true                   4 /    5        228.5           4.4      31.5X&lt;br/&gt;
+Map of string-&amp;gt;Double  with unsafe:true         38 /   45         26.5          37.8       3.7X&lt;br/&gt;
+basicTypes: Int with unsafe:false              176 /  187          5.7         175.9       0.8X&lt;br/&gt;
+basicTypes: Long with unsafe:false             191 /  203          5.2         191.2       0.7X&lt;br/&gt;
+basicTypes: Float with unsafe:false            166 /  176          6.0         166.2       0.8X&lt;br/&gt;
+basicTypes: Double with unsafe:false           174 /  190          5.7         174.3       0.8X&lt;br/&gt;
+Array: Int with unsafe:false                    19 /   26         52.9          18.9       7.3X&lt;br/&gt;
+Array: Long with unsafe:false                   27 /   31         37.7          26.5       5.2X&lt;br/&gt;
+Array: Float with unsafe:false                   8 /   10        124.3           8.0      17.2X&lt;br/&gt;
+Array: Double with unsafe:false                 12 /   13         83.6          12.0      11.5X&lt;br/&gt;
+Map of string-&amp;gt;Double  with unsafe:false        38 /   42         26.1          38.3       3.6X&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
diff --git a/core/benchmarks/KryoSerializerBenchmark-results.txt b/core/benchmarks/KryoSerializerBenchmark-results.txt&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..c3ce336d93241&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/benchmarks/KryoSerializerBenchmark-results.txt&lt;br/&gt;
@@ -0,0 +1,12 @@&lt;br/&gt;
+================================================================================================&lt;br/&gt;
+Benchmark KryoPool vs &quot;pool of 1&quot;&lt;br/&gt;
+================================================================================================&lt;br/&gt;
+&lt;br/&gt;
+Java HotSpot(TM) 64-Bit Server VM 1.8.0_131-b11 on Mac OS X 10.14&lt;br/&gt;
+Intel(R) Core(TM) i7-4770HQ CPU @ 2.20GHz&lt;br/&gt;
+Benchmark KryoPool vs &quot;pool of 1&quot;:       Best/Avg Time(ms)    Rate(M/s)   Per Row(ns)   Relative&lt;br/&gt;
+------------------------------------------------------------------------------------------------&lt;br/&gt;
+KryoPool:true                                 2682 / 3425          0.0     5364627.9       1.0X&lt;br/&gt;
+KryoPool:false                                8176 / 9292          0.0    16351252.2       0.3X&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
diff --git a/core/pom.xml b/core/pom.xml&lt;br/&gt;
index 5fa3a86de6b01..49b1a54e32598 100644&lt;br/&gt;
&amp;#8212; a/core/pom.xml&lt;br/&gt;
+++ b/core/pom.xml&lt;br/&gt;
@@ -20,12 +20,12 @@&lt;br/&gt;
   &amp;lt;modelVersion&amp;gt;4.0.0&amp;lt;/modelVersion&amp;gt;&lt;br/&gt;
   &amp;lt;parent&amp;gt;&lt;br/&gt;
     &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-parent_2.11&amp;lt;/artifactId&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;version&amp;gt;2.4.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
+    &amp;lt;artifactId&amp;gt;spark-parent_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+    &amp;lt;version&amp;gt;3.0.0-SNAPSHOT&amp;lt;/version&amp;gt;&lt;br/&gt;
     &amp;lt;relativePath&amp;gt;../pom.xml&amp;lt;/relativePath&amp;gt;&lt;br/&gt;
   &amp;lt;/parent&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;spark-core_2.11&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+  &amp;lt;artifactId&amp;gt;spark-core_2.12&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
   &amp;lt;properties&amp;gt;&lt;br/&gt;
     &amp;lt;sbt.project.name&amp;gt;core&amp;lt;/sbt.project.name&amp;gt;&lt;br/&gt;
   &amp;lt;/properties&amp;gt;&lt;br/&gt;
@@ -56,7 +56,7 @@&lt;br/&gt;
     &amp;lt;/dependency&amp;gt;&lt;br/&gt;
     &amp;lt;dependency&amp;gt;&lt;br/&gt;
       &amp;lt;groupId&amp;gt;org.apache.xbean&amp;lt;/groupId&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;artifactId&amp;gt;xbean-asm6-shaded&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+      &amp;lt;artifactId&amp;gt;xbean-asm7-shaded&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
     &amp;lt;/dependency&amp;gt;&lt;br/&gt;
     &amp;lt;dependency&amp;gt;&lt;br/&gt;
       &amp;lt;groupId&amp;gt;org.apache.hadoop&amp;lt;/groupId&amp;gt;&lt;br/&gt;
@@ -350,7 +350,7 @@&lt;br/&gt;
     &amp;lt;dependency&amp;gt;&lt;br/&gt;
       &amp;lt;groupId&amp;gt;net.sf.py4j&amp;lt;/groupId&amp;gt;&lt;br/&gt;
       &amp;lt;artifactId&amp;gt;py4j&amp;lt;/artifactId&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;version&amp;gt;0.10.7&amp;lt;/version&amp;gt;&lt;br/&gt;
+      &amp;lt;version&amp;gt;0.10.8.1&amp;lt;/version&amp;gt;&lt;br/&gt;
     &amp;lt;/dependency&amp;gt;&lt;br/&gt;
     &amp;lt;dependency&amp;gt;&lt;br/&gt;
       &amp;lt;groupId&amp;gt;org.apache.spark&amp;lt;/groupId&amp;gt;&lt;br/&gt;
@@ -408,6 +408,19 @@&lt;br/&gt;
       &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;&lt;br/&gt;
     &amp;lt;/dependency&amp;gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    &amp;lt;!--&lt;br/&gt;
+     The following kafka dependency used to obtain delegation token.&lt;br/&gt;
+     In order to prevent spark-core from depending on kafka, these deps have been placed in the&lt;br/&gt;
+     &quot;provided&quot; scope, rather than the &quot;compile&quot; scope, and NoClassDefFoundError exceptions are&lt;br/&gt;
+     handled when the user explicitly use neither spark-streaming-kafka nor spark-sql-kafka modules.&lt;br/&gt;
+    --&amp;gt;&lt;br/&gt;
+    &amp;lt;dependency&amp;gt;&lt;br/&gt;
+      &amp;lt;groupId&amp;gt;org.apache.kafka&amp;lt;/groupId&amp;gt;&lt;br/&gt;
+      &amp;lt;artifactId&amp;gt;kafka-clients&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
+      &amp;lt;version&amp;gt;${kafka.version}&amp;lt;/version&amp;gt;&lt;br/&gt;
+      &amp;lt;scope&amp;gt;provided&amp;lt;/scope&amp;gt;&lt;br/&gt;
+    &amp;lt;/dependency&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
   &amp;lt;/dependencies&amp;gt;&lt;br/&gt;
   &amp;lt;build&amp;gt;&lt;br/&gt;
     &amp;lt;outputDirectory&amp;gt;target/scala-${scala.binary.version}/classes&amp;lt;/outputDirectory&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/ExecutorPlugin.java b/core/src/main/java/org/apache/spark/ExecutorPlugin.java&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..f86520c81df33&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/ExecutorPlugin.java&lt;br/&gt;
@@ -0,0 +1,57 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark;&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.spark.annotation.DeveloperApi;&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * A plugin which can be automatically instantiated within each Spark executor.  Users can specify&lt;br/&gt;
+ * plugins which should be created with the &quot;spark.executor.plugins&quot; configuration.  An instance&lt;br/&gt;
+ * of each plugin will be created for every executor, including those created by dynamic allocation,&lt;br/&gt;
+ * before the executor starts running any tasks.&lt;br/&gt;
+ *&lt;br/&gt;
+ * The specific api exposed to the end users still considered to be very unstable.  We will&lt;br/&gt;
+ * hopefully be able to keep compatibility by providing default implementations for any methods&lt;br/&gt;
+ * added, but make no guarantees this will always be possible across all Spark releases.&lt;br/&gt;
+ *&lt;br/&gt;
+ * Spark does nothing to verify the plugin is doing legitimate things, or to manage the resources&lt;br/&gt;
+ * it uses.  A plugin acquires the same privileges as the user running the task.  A bad plugin&lt;br/&gt;
+ * could also interfere with task execution and make the executor fail in unexpected ways.&lt;br/&gt;
+ */&lt;br/&gt;
+@DeveloperApi&lt;br/&gt;
+public interface ExecutorPlugin {&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Initialize the executor plugin.&lt;br/&gt;
+   *&lt;br/&gt;
+   * &amp;lt;p&amp;gt;Each executor will, during its initialization, invoke this method on each&lt;br/&gt;
+   * plugin provided in the spark.executor.plugins configuration.&amp;lt;/p&amp;gt;&lt;br/&gt;
+   *&lt;br/&gt;
+   * &amp;lt;p&amp;gt;Plugins should create threads in their implementation of this method for&lt;br/&gt;
+   * any polling, blocking, or intensive computation.&amp;lt;/p&amp;gt;&lt;br/&gt;
+   */&lt;br/&gt;
+  default void init() {}&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Clean up and terminate this plugin.&lt;br/&gt;
+   *&lt;br/&gt;
+   * &amp;lt;p&amp;gt;This function is called during the executor shutdown phase. The executor&lt;br/&gt;
+   * will wait for the plugin to terminate before continuing its own shutdown.&amp;lt;/p&amp;gt;&lt;br/&gt;
+   */&lt;br/&gt;
+  default void shutdown() {}&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/SparkFirehoseListener.java b/core/src/main/java/org/apache/spark/SparkFirehoseListener.java&lt;br/&gt;
index 94c5c11b61a50..731f6fc767dfd 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/java/org/apache/spark/SparkFirehoseListener.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/SparkFirehoseListener.java&lt;br/&gt;
@@ -103,6 +103,12 @@ public final void onExecutorMetricsUpdate(&lt;br/&gt;
     onEvent(executorMetricsUpdate);&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  @Override&lt;br/&gt;
+  public final void onStageExecutorMetrics(&lt;br/&gt;
+      SparkListenerStageExecutorMetrics executorMetrics) &lt;/p&gt;
{
+    onEvent(executorMetrics);
+  }
&lt;p&gt;+&lt;br/&gt;
   @Override&lt;br/&gt;
   public final void onExecutorAdded(SparkListenerExecutorAdded executorAdded) {&lt;br/&gt;
     onEvent(executorAdded);&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java b/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 0dd8fafbf2c82..0000000000000&lt;br/&gt;
&amp;#8212; a/core/src/main/java/org/apache/spark/api/java/JavaSparkContextVarargsWorkaround.java&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,67 +0,0 @@&lt;br/&gt;
-/*&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements.  See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License.  You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-&lt;br/&gt;
-package org.apache.spark.api.java;&lt;br/&gt;
-&lt;br/&gt;
-import java.util.ArrayList;&lt;br/&gt;
-import java.util.List;&lt;br/&gt;
-&lt;br/&gt;
-// See&lt;br/&gt;
-// &lt;a href=&quot;http://scala-programming-language.1934581.n4.nabble.com/Workaround-for-implementing-java-varargs-in-2-7-2-final-tp1944767p1944772.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://scala-programming-language.1934581.n4.nabble.com/Workaround-for-implementing-java-varargs-in-2-7-2-final-tp1944767p1944772.html&lt;/a&gt;&lt;br/&gt;
-abstract class JavaSparkContextVarargsWorkaround {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;@SafeVarargs&lt;/li&gt;
	&lt;li&gt;public final &amp;lt;T&amp;gt; JavaRDD&amp;lt;T&amp;gt; union(JavaRDD&amp;lt;T&amp;gt;... rdds) {&lt;/li&gt;
	&lt;li&gt;if (rdds.length == 0) 
{
-      throw new IllegalArgumentException(&quot;Union called on empty list&quot;);
-    }&lt;br/&gt;
-    List&amp;lt;JavaRDD&amp;lt;T&amp;gt;&amp;gt; rest = new ArrayList&amp;lt;&amp;gt;(rdds.length - 1);&lt;br/&gt;
-    for (int i = 1; i &amp;lt; rdds.length; i++) {
-      rest.add(rdds[i]);
-    }&lt;br/&gt;
-    return union(rdds&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, rest);&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  public JavaDoubleRDD union(JavaDoubleRDD... rdds) {&lt;br/&gt;
-    if (rdds.length == 0) {-      throw new IllegalArgumentException(&quot;Union called on empty list&quot;);-    }&lt;/li&gt;
	&lt;li&gt;List&amp;lt;JavaDoubleRDD&amp;gt; rest = new ArrayList&amp;lt;&amp;gt;(rdds.length - 1);&lt;/li&gt;
	&lt;li&gt;for (int i = 1; i &amp;lt; rdds.length; i++) 
{
-      rest.add(rdds[i]);
-    }&lt;br/&gt;
-    return union(rdds&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, rest);&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @SafeVarargs&lt;br/&gt;
-  public final &amp;lt;K, V&amp;gt; JavaPairRDD&amp;lt;K, V&amp;gt; union(JavaPairRDD&amp;lt;K, V&amp;gt;... rdds) {&lt;br/&gt;
-    if (rdds.length == 0) {
-      throw new IllegalArgumentException(&quot;Union called on empty list&quot;);
-    }&lt;br/&gt;
-    List&amp;lt;JavaPairRDD&amp;lt;K, V&amp;gt;&amp;gt; rest = new ArrayList&amp;lt;&amp;gt;(rdds.length - 1);&lt;br/&gt;
-    for (int i = 1; i &amp;lt; rdds.length; i++) {-      rest.add(rdds[i]);-    }&lt;/li&gt;
	&lt;li&gt;return union(rdds&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;, rest);&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// These methods take separate &quot;first&quot; and &quot;rest&quot; elements to avoid having the same type erasure&lt;/li&gt;
	&lt;li&gt;public abstract &amp;lt;T&amp;gt; JavaRDD&amp;lt;T&amp;gt; union(JavaRDD&amp;lt;T&amp;gt; first, List&amp;lt;JavaRDD&amp;lt;T&amp;gt;&amp;gt; rest);&lt;/li&gt;
	&lt;li&gt;public abstract JavaDoubleRDD union(JavaDoubleRDD first, List&amp;lt;JavaDoubleRDD&amp;gt; rest);&lt;/li&gt;
	&lt;li&gt;public abstract &amp;lt;K, V&amp;gt; JavaPairRDD&amp;lt;K, V&amp;gt; union(JavaPairRDD&amp;lt;K, V&amp;gt; first, List&amp;lt;JavaPairRDD&amp;lt;K, V&amp;gt;&amp;gt;&lt;/li&gt;
	&lt;li&gt;rest);&lt;br/&gt;
-}&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java b/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java&lt;br/&gt;
index f6d1288cb263d..92bf0ecc1b5cb 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/io/NioBufferedFileInputStream.java&lt;br/&gt;
@@ -27,7 +27,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;to read a file to avoid extra copy of data between Java and&lt;/li&gt;
	&lt;li&gt;native memory which happens when using 
{@link java.io.BufferedInputStream}
&lt;p&gt;.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;Unfortunately, this is not something already available in JDK,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* 
{@link sun.nio.ch.ChannelInputStream}
&lt;p&gt; supports reading a file using nio,&lt;br/&gt;
+ * &lt;/p&gt;
{@code sun.nio.ch.ChannelInputStream}
&lt;p&gt; supports reading a file using nio,&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;but does not support buffering.&lt;br/&gt;
  */&lt;br/&gt;
 public final class NioBufferedFileInputStream extends InputStream {&lt;br/&gt;
@@ -130,6 +130,7 @@ public synchronized void close() throws IOException 
{
     StorageUtils.dispose(byteBuffer);
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  @SuppressWarnings(&quot;deprecation&quot;)&lt;br/&gt;
   @Override&lt;br/&gt;
   protected void finalize() throws IOException {&lt;br/&gt;
     close();&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java b/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java&lt;br/&gt;
index 0cced9e222952..2e18715b600e0 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/io/ReadAheadInputStream.java&lt;br/&gt;
@@ -135,62 +135,58 @@ private void readAsync() throws IOException {&lt;br/&gt;
     } finally &lt;/p&gt;
{
       stateChangeLock.unlock();
     }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;executorService.execute(new Runnable() {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;@Override&lt;/li&gt;
	&lt;li&gt;public void run() {&lt;/li&gt;
	&lt;li&gt;stateChangeLock.lock();&lt;/li&gt;
	&lt;li&gt;try {&lt;/li&gt;
	&lt;li&gt;if (isClosed) 
{
-            readInProgress = false;
-            return;
-          }&lt;/li&gt;
	&lt;li&gt;// Flip this so that the close method will not close the underlying input stream when we&lt;/li&gt;
	&lt;li&gt;// are reading.&lt;/li&gt;
	&lt;li&gt;isReading = true;&lt;/li&gt;
	&lt;li&gt;} finally {&lt;/li&gt;
	&lt;li&gt;stateChangeLock.unlock();&lt;br/&gt;
+    executorService.execute(() -&amp;gt; {&lt;br/&gt;
+      stateChangeLock.lock();&lt;br/&gt;
+      try 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        if (isClosed) {
+          readInProgress = false;
+          return;
         }+        // Flip this so that the close method will not close the underlying input stream when we+        // are reading.+        isReading = true;+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; finally &lt;/p&gt;
{
+        stateChangeLock.unlock();
+      }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Please note that it is safe to release the lock and read into the read ahead buffer&lt;/li&gt;
	&lt;li&gt;// because either of following two conditions will hold - 1. The active buffer has&lt;/li&gt;
	&lt;li&gt;// data available to read so the reader will not read from the read ahead buffer.&lt;/li&gt;
	&lt;li&gt;// 2. This is the first time read is called or the active buffer is exhausted,&lt;/li&gt;
	&lt;li&gt;// in that case the reader waits for this async read to complete.&lt;/li&gt;
	&lt;li&gt;// So there is no race condition in both the situations.&lt;/li&gt;
	&lt;li&gt;int read = 0;&lt;/li&gt;
	&lt;li&gt;int off = 0, len = arr.length;&lt;/li&gt;
	&lt;li&gt;Throwable exception = null;&lt;/li&gt;
	&lt;li&gt;try {&lt;/li&gt;
	&lt;li&gt;// try to fill the read ahead buffer.&lt;/li&gt;
	&lt;li&gt;// if a reader is waiting, possibly return early.&lt;/li&gt;
	&lt;li&gt;do 
{
-            read = underlyingInputStream.read(arr, off, len);
-            if (read &amp;lt;= 0) break;
-            off += read;
-            len -= read;
-          }
&lt;p&gt; while (len &amp;gt; 0 &amp;amp;&amp;amp; !isWaiting.get());&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;} catch (Throwable ex) {&lt;/li&gt;
	&lt;li&gt;exception = ex;&lt;/li&gt;
	&lt;li&gt;if (ex instanceof Error) 
{
-            // `readException` may not be reported to the user. Rethrow Error to make sure at least
-            // The user can see Error in UncaughtExceptionHandler.
-            throw (Error) ex;
-          }&lt;/li&gt;
	&lt;li&gt;} finally {&lt;/li&gt;
	&lt;li&gt;stateChangeLock.lock();&lt;/li&gt;
	&lt;li&gt;readAheadBuffer.limit&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/lightbulb.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;;&lt;/li&gt;
	&lt;li&gt;if (read &amp;lt; 0 || (exception instanceof EOFException)) 
{
-            endOfStream = true;
-          }
&lt;p&gt; else if (exception != null) &lt;/p&gt;
{
-            readAborted = true;
-            readException = exception;
-          }&lt;/li&gt;
	&lt;li&gt;readInProgress = false;&lt;/li&gt;
	&lt;li&gt;signalAsyncReadComplete();&lt;/li&gt;
	&lt;li&gt;stateChangeLock.unlock();&lt;/li&gt;
	&lt;li&gt;closeUnderlyingInputStreamIfNecessary();&lt;br/&gt;
+      // Please note that it is safe to release the lock and read into the read ahead buffer&lt;br/&gt;
+      // because either of following two conditions will hold - 1. The active buffer has&lt;br/&gt;
+      // data available to read so the reader will not read from the read ahead buffer.&lt;br/&gt;
+      // 2. This is the first time read is called or the active buffer is exhausted,&lt;br/&gt;
+      // in that case the reader waits for this async read to complete.&lt;br/&gt;
+      // So there is no race condition in both the situations.&lt;br/&gt;
+      int read = 0;&lt;br/&gt;
+      int off = 0, len = arr.length;&lt;br/&gt;
+      Throwable exception = null;&lt;br/&gt;
+      try 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        // try to fill the read ahead buffer.+        // if a reader is waiting, possibly return early.+        do {
+          read = underlyingInputStream.read(arr, off, len);
+          if (read &amp;lt;= 0) break;
+          off += read;
+          len -= read;
+        } while (len &amp;gt; 0 &amp;amp;&amp;amp; !isWaiting.get());+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; catch (Throwable ex) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        exception = ex;+        if (ex instanceof Error) {
+          // `readException` may not be reported to the user. Rethrow Error to make sure at least
+          // The user can see Error in UncaughtExceptionHandler.
+          throw (Error) ex;
         }+      }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; finally &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        stateChangeLock.lock();+        readAheadBuffer.limit(off);+        if (read &amp;lt; 0 || (exception instanceof EOFException)) {
+          endOfStream = true;
+        } else if (exception != null) {
+          readAborted = true;
+          readException = exception;
+        }+        readInProgress = false;+        signalAsyncReadComplete();+        stateChangeLock.unlock();+        closeUnderlyingInputStreamIfNecessary();       }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;     });&lt;br/&gt;
   }&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java b/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java&lt;br/&gt;
index 115e1fbb79a2e..4bfd2d358f36f 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/memory/MemoryConsumer.java&lt;br/&gt;
@@ -83,10 +83,10 @@ public void spill() throws IOException {&lt;br/&gt;
   public abstract long spill(long size, MemoryConsumer trigger) throws IOException;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Allocates a LongArray of `size`. Note that this method may throw `OutOfMemoryError` if Spark&lt;/li&gt;
	&lt;li&gt;* doesn&apos;t have enough memory for this allocation, or throw `TooLargePageException` if this&lt;/li&gt;
	&lt;li&gt;* `LongArray` is too large to fit in a single page. The caller side should take care of these&lt;/li&gt;
	&lt;li&gt;* two exceptions, or make sure the `size` is small enough that won&apos;t trigger exceptions.&lt;br/&gt;
+   * Allocates a LongArray of `size`. Note that this method may throw `SparkOutOfMemoryError`&lt;br/&gt;
+   * if Spark doesn&apos;t have enough memory for this allocation, or throw `TooLargePageException`&lt;br/&gt;
+   * if this `LongArray` is too large to fit in a single page. The caller side should take care of&lt;br/&gt;
+   * these two exceptions, or make sure the `size` is small enough that won&apos;t trigger exceptions.&lt;br/&gt;
    *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@throws SparkOutOfMemoryError&lt;/li&gt;
	&lt;li&gt;@throws TooLargePageException&lt;br/&gt;
@@ -111,7 +111,7 @@ public void freeArray(LongArray array) {&lt;br/&gt;
   /**&lt;/li&gt;
	&lt;li&gt;Allocate a memory block with at least `required` bytes.&lt;br/&gt;
    *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* @throws OutOfMemoryError&lt;br/&gt;
+   * @throws SparkOutOfMemoryError&lt;br/&gt;
    */&lt;br/&gt;
   protected MemoryBlock allocatePage(long required) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     MemoryBlock page = taskMemoryManager.allocatePage(Math.max(pageSize, required), this);@@ -154,7 +154,9 @@ private void throwOom(final MemoryBlock page, final long required) {
       taskMemoryManager.freePage(page, this);
     }     taskMemoryManager.showMemoryUsage();+    // checkstyle.off}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; }&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java b/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java&lt;br/&gt;
index 8651a639c07f7..28b646ba3c951 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/memory/TaskMemoryManager.java&lt;br/&gt;
@@ -194,8 +194,10 @@ public long acquireExecutionMemory(long required, MemoryConsumer consumer) 
{
             throw new RuntimeException(e.getMessage());
           }
&lt;p&gt; catch (IOException e) &lt;/p&gt;
{
             logger.error(&quot;error while calling spill() on &quot; + c, e);
+            // checkstyle.off: RegexpSinglelineJava
             throw new SparkOutOfMemoryError(&quot;error while calling spill() on &quot; + c + &quot; : &quot;
               + e.getMessage());
+            // checkstyle.on: RegexpSinglelineJava
           }
&lt;p&gt;         }&lt;br/&gt;
       }&lt;br/&gt;
@@ -215,8 +217,10 @@ public long acquireExecutionMemory(long required, MemoryConsumer consumer) &lt;/p&gt;
{
           throw new RuntimeException(e.getMessage());
         }
&lt;p&gt; catch (IOException e) &lt;/p&gt;
{
           logger.error(&quot;error while calling spill() on &quot; + consumer, e);
+          // checkstyle.off: RegexpSinglelineJava
           throw new SparkOutOfMemoryError(&quot;error while calling spill() on &quot; + consumer + &quot; : &quot;
             + e.getMessage());
+          // checkstyle.on: RegexpSinglelineJava
         }
&lt;p&gt;       }&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -311,7 +315,7 @@ public MemoryBlock allocatePage(long size, MemoryConsumer consumer) &lt;/p&gt;
{
       // this could trigger spilling to free some pages.
       return allocatePage(size, consumer);
     }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;page.setPageNumber(pageNumber);&lt;br/&gt;
+    page.pageNumber = pageNumber;&lt;br/&gt;
     pageTable&lt;span class=&quot;error&quot;&gt;&amp;#91;pageNumber&amp;#93;&lt;/span&gt; = page;&lt;br/&gt;
     if (logger.isTraceEnabled()) {&lt;br/&gt;
       logger.trace(&quot;Allocate page number {} ({} bytes)&quot;, pageNumber, acquired);&lt;br/&gt;
@@ -323,25 +327,25 @@ public MemoryBlock allocatePage(long size, MemoryConsumer consumer) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Free a block of memory allocated via 
{@link TaskMemoryManager#allocatePage}
&lt;p&gt;.&lt;br/&gt;
    */&lt;br/&gt;
   public void freePage(MemoryBlock page, MemoryConsumer consumer) {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;assert (page.getPageNumber() != MemoryBlock.NO_PAGE_NUMBER) :&lt;br/&gt;
+    assert (page.pageNumber != MemoryBlock.NO_PAGE_NUMBER) :&lt;br/&gt;
       &quot;Called freePage() on memory that wasn&apos;t allocated with allocatePage()&quot;;&lt;/li&gt;
	&lt;li&gt;assert (page.getPageNumber() != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :&lt;br/&gt;
+    assert (page.pageNumber != MemoryBlock.FREED_IN_ALLOCATOR_PAGE_NUMBER) :&lt;br/&gt;
       &quot;Called freePage() on a memory block that has already been freed&quot;;&lt;/li&gt;
	&lt;li&gt;assert (page.getPageNumber() != MemoryBlock.FREED_IN_TMM_PAGE_NUMBER) :&lt;br/&gt;
+    assert (page.pageNumber != MemoryBlock.FREED_IN_TMM_PAGE_NUMBER) :&lt;br/&gt;
             &quot;Called freePage() on a memory block that has already been freed&quot;;&lt;/li&gt;
	&lt;li&gt;assert(allocatedPages.get(page.getPageNumber()));&lt;/li&gt;
	&lt;li&gt;pageTable&lt;span class=&quot;error&quot;&gt;&amp;#91;page.getPageNumber()&amp;#93;&lt;/span&gt; = null;&lt;br/&gt;
+    assert(allocatedPages.get(page.pageNumber));&lt;br/&gt;
+    pageTable&lt;span class=&quot;error&quot;&gt;&amp;#91;page.pageNumber&amp;#93;&lt;/span&gt; = null;&lt;br/&gt;
     synchronized (this) 
{
-      allocatedPages.clear(page.getPageNumber());
+      allocatedPages.clear(page.pageNumber);
     }
&lt;p&gt;     if (logger.isTraceEnabled()) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;logger.trace(&quot;Freed page number {} ({} bytes)&quot;, page.getPageNumber(), page.size());&lt;br/&gt;
+      logger.trace(&quot;Freed page number {} ({} bytes)&quot;, page.pageNumber, page.size());&lt;br/&gt;
     }&lt;br/&gt;
     long pageSize = page.size();&lt;br/&gt;
     // Clear the page number before passing the block to the MemoryAllocator&apos;s free().&lt;br/&gt;
     // Doing this allows the MemoryAllocator to detect when a TaskMemoryManager-managed&lt;br/&gt;
     // page has been inappropriately directly freed without calling TMM.freePage().&lt;/li&gt;
	&lt;li&gt;page.setPageNumber(MemoryBlock.FREED_IN_TMM_PAGE_NUMBER);&lt;br/&gt;
+    page.pageNumber = MemoryBlock.FREED_IN_TMM_PAGE_NUMBER;&lt;br/&gt;
     memoryManager.tungstenMemoryAllocator().free(page);&lt;br/&gt;
     releaseExecutionMemory(pageSize, consumer);&lt;br/&gt;
   }&lt;br/&gt;
@@ -363,7 +367,7 @@ public long encodePageNumberAndOffset(MemoryBlock page, long offsetInPage) 
{
       // relative to the page&apos;s base offset; this relative offset will fit in 51 bits.
       offsetInPage -= page.getBaseOffset();
     }&lt;/li&gt;
	&lt;li&gt;return encodePageNumberAndOffset(page.getPageNumber(), offsetInPage);&lt;br/&gt;
+    return encodePageNumberAndOffset(page.pageNumber, offsetInPage);&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @VisibleForTesting&lt;br/&gt;
@@ -434,7 +438,7 @@ public long cleanUpAllAllocatedMemory() {&lt;br/&gt;
       for (MemoryBlock page : pageTable) {&lt;br/&gt;
         if (page != null) &lt;/p&gt;
{
           logger.debug(&quot;unreleased page: &quot; + page + &quot; in task &quot; + taskAttemptId);
-          page.setPageNumber(MemoryBlock.FREED_IN_TMM_PAGE_NUMBER);
+          page.pageNumber = MemoryBlock.FREED_IN_TMM_PAGE_NUMBER;
           memoryManager.tungstenMemoryAllocator().free(page);
         }
&lt;p&gt;       }&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/package-info.java b/core/src/main/java/org/apache/spark/package-info.java&lt;br/&gt;
index 4426c7afcebdd..a029931f9e4c0 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/java/org/apache/spark/package-info.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/package-info.java&lt;br/&gt;
@@ -16,8 +16,8 @@&lt;br/&gt;
  */&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Core Spark classes in Scala. A few classes here, such as 
{@link org.apache.spark.Accumulator}&lt;/li&gt;
	&lt;li&gt;* and 
{@link org.apache.spark.storage.StorageLevel}, are also used in Java, but the&lt;br/&gt;
+ * Core Spark classes in Scala. A few classes here, such as&lt;br/&gt;
+ * {@link org.apache.spark.storage.StorageLevel}
&lt;p&gt;, are also used in Java, but the&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;{@link org.apache.spark.api.java}
&lt;p&gt; package contains the main Java API.&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.spark;&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java b/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java&lt;br/&gt;
index e3bd5496cf5ba..fda33cd8293d5 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/shuffle/sort/BypassMergeSortShuffleWriter.java&lt;br/&gt;
@@ -37,12 +37,11 @@&lt;br/&gt;
 import org.apache.spark.Partitioner;&lt;br/&gt;
 import org.apache.spark.ShuffleDependency;&lt;br/&gt;
 import org.apache.spark.SparkConf;&lt;br/&gt;
-import org.apache.spark.TaskContext;&lt;br/&gt;
-import org.apache.spark.executor.ShuffleWriteMetrics;&lt;br/&gt;
 import org.apache.spark.scheduler.MapStatus;&lt;br/&gt;
 import org.apache.spark.scheduler.MapStatus$;&lt;br/&gt;
 import org.apache.spark.serializer.Serializer;&lt;br/&gt;
 import org.apache.spark.serializer.SerializerInstance;&lt;br/&gt;
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;&lt;br/&gt;
 import org.apache.spark.shuffle.IndexShuffleBlockResolver;&lt;br/&gt;
 import org.apache.spark.shuffle.ShuffleWriter;&lt;br/&gt;
 import org.apache.spark.storage.*;&lt;br/&gt;
@@ -79,7 +78,7 @@&lt;br/&gt;
   private final int numPartitions;&lt;br/&gt;
   private final BlockManager blockManager;&lt;br/&gt;
   private final Partitioner partitioner;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final ShuffleWriteMetrics writeMetrics;&lt;br/&gt;
+  private final ShuffleWriteMetricsReporter writeMetrics;&lt;br/&gt;
   private final int shuffleId;&lt;br/&gt;
   private final int mapId;&lt;br/&gt;
   private final Serializer serializer;&lt;br/&gt;
@@ -103,8 +102,8 @@&lt;br/&gt;
       IndexShuffleBlockResolver shuffleBlockResolver,&lt;br/&gt;
       BypassMergeSortShuffleHandle&amp;lt;K, V&amp;gt; handle,&lt;br/&gt;
       int mapId,&lt;/li&gt;
	&lt;li&gt;TaskContext taskContext,&lt;/li&gt;
	&lt;li&gt;SparkConf conf) {&lt;br/&gt;
+      SparkConf conf,&lt;br/&gt;
+      ShuffleWriteMetricsReporter writeMetrics) 
{
     // Use getSizeAsKb (not bytes) to maintain backwards compatibility if no units are provided
     this.fileBufferSize = (int) conf.getSizeAsKb(&quot;spark.shuffle.file.buffer&quot;, &quot;32k&quot;) * 1024;
     this.transferToEnabled = conf.getBoolean(&quot;spark.file.transferTo&quot;, true);
@@ -114,7 +113,7 @@
     this.shuffleId = dep.shuffleId();
     this.partitioner = dep.partitioner();
     this.numPartitions = partitioner.numPartitions();
-    this.writeMetrics = taskContext.taskMetrics().shuffleWriteMetrics();
+    this.writeMetrics = writeMetrics;
     this.serializer = dep.serializer();
     this.shuffleBlockResolver = shuffleBlockResolver;
   }
&lt;p&gt;@@ -125,7 +124,7 @@ public void write(Iterator&amp;lt;Product2&amp;lt;K, V&amp;gt;&amp;gt; records) throws IOException {&lt;br/&gt;
     if (!records.hasNext()) &lt;/p&gt;
{
       partitionLengths = new long[numPartitions];
       shuffleBlockResolver.writeIndexFileAndCommit(shuffleId, mapId, partitionLengths, null);
-      mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths, 0);
+      mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);
       return;
     }
&lt;p&gt;     final SerializerInstance serInstance = serializer.newInstance();&lt;br/&gt;
@@ -152,9 +151,9 @@ public void write(Iterator&amp;lt;Product2&amp;lt;K, V&amp;gt;&amp;gt; records) throws IOException {&lt;br/&gt;
     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     for (int i = 0; i &amp;lt; numPartitions; i++) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final DiskBlockObjectWriter writer = partitionWriters&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;partitionWriterSegments&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt; = writer.commitAndGet();&lt;/li&gt;
	&lt;li&gt;writer.close();&lt;br/&gt;
+      try (DiskBlockObjectWriter writer = partitionWriters&lt;span class=&quot;error&quot;&gt;&amp;#91;i&amp;#93;&lt;/span&gt;) 
{
+        partitionWriterSegments[i] = writer.commitAndGet();
+      }
&lt;p&gt;     }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     File output = shuffleBlockResolver.getDataFile(shuffleId, mapId);&lt;br/&gt;
@@ -167,8 +166,7 @@ public void write(Iterator&amp;lt;Product2&amp;lt;K, V&amp;gt;&amp;gt; records) throws IOException {&lt;br/&gt;
         logger.error(&quot;Error while deleting temp file {}&quot;, tmp.getAbsolutePath());&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;mapStatus = MapStatus$.MODULE$.apply(&lt;/li&gt;
	&lt;li&gt;blockManager.shuffleServerId(), partitionLengths, writeMetrics.recordsWritten());&lt;br/&gt;
+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @VisibleForTesting&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java&lt;br/&gt;
index c7d2db4217d96..6ee9d5f0eec3b 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleExternalSorter.java&lt;br/&gt;
@@ -38,6 +38,7 @@&lt;br/&gt;
 import org.apache.spark.memory.TooLargePageException;&lt;br/&gt;
 import org.apache.spark.serializer.DummySerializerInstance;&lt;br/&gt;
 import org.apache.spark.serializer.SerializerInstance;&lt;br/&gt;
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;&lt;br/&gt;
 import org.apache.spark.storage.BlockManager;&lt;br/&gt;
 import org.apache.spark.storage.DiskBlockObjectWriter;&lt;br/&gt;
 import org.apache.spark.storage.FileSegment;&lt;br/&gt;
@@ -75,7 +76,7 @@&lt;br/&gt;
   private final TaskMemoryManager taskMemoryManager;&lt;br/&gt;
   private final BlockManager blockManager;&lt;br/&gt;
   private final TaskContext taskContext;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final ShuffleWriteMetrics writeMetrics;&lt;br/&gt;
+  private final ShuffleWriteMetricsReporter writeMetrics;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Force this sorter to spill when there are this many elements in memory.&lt;br/&gt;
@@ -113,7 +114,7 @@&lt;br/&gt;
       int initialSize,&lt;br/&gt;
       int numPartitions,&lt;br/&gt;
       SparkConf conf,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ShuffleWriteMetrics writeMetrics) {&lt;br/&gt;
+      ShuffleWriteMetricsReporter writeMetrics) {&lt;br/&gt;
     super(memoryManager,&lt;br/&gt;
       (int) Math.min(PackedRecordPointer.MAXIMUM_PAGE_SIZE_BYTES, memoryManager.pageSizeBytes()),&lt;br/&gt;
       memoryManager.getTungstenMemoryMode());&lt;br/&gt;
@@ -144,7 +145,7 @@&lt;br/&gt;
    */&lt;br/&gt;
   private void writeSortedFile(boolean isLastFile) {&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final ShuffleWriteMetrics writeMetricsToUse;&lt;br/&gt;
+    final ShuffleWriteMetricsReporter writeMetricsToUse;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     if (isLastFile) {&lt;br/&gt;
       // We&apos;re writing the final non-spill file, so we &lt;em&gt;do&lt;/em&gt; want to count this as shuffle bytes.&lt;br/&gt;
@@ -181,42 +182,43 @@ private void writeSortedFile(boolean isLastFile) {&lt;br/&gt;
     // around this, we pass a dummy no-op serializer.&lt;br/&gt;
     final SerializerInstance ser = DummySerializerInstance.INSTANCE;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final DiskBlockObjectWriter writer =&lt;/li&gt;
	&lt;li&gt;blockManager.getDiskWriter(blockId, file, ser, fileBufferSizeBytes, writeMetricsToUse);&lt;br/&gt;
-&lt;br/&gt;
     int currentPartition = -1;&lt;/li&gt;
	&lt;li&gt;final int uaoSize = UnsafeAlignedOffset.getUaoSize();&lt;/li&gt;
	&lt;li&gt;while (sortedRecords.hasNext()) {&lt;/li&gt;
	&lt;li&gt;sortedRecords.loadNext();&lt;/li&gt;
	&lt;li&gt;final int partition = sortedRecords.packedRecordPointer.getPartitionId();&lt;/li&gt;
	&lt;li&gt;assert (partition &amp;gt;= currentPartition);&lt;/li&gt;
	&lt;li&gt;if (partition != currentPartition) {&lt;/li&gt;
	&lt;li&gt;// Switch to the new partition&lt;/li&gt;
	&lt;li&gt;if (currentPartition != -1) {&lt;/li&gt;
	&lt;li&gt;final FileSegment fileSegment = writer.commitAndGet();&lt;/li&gt;
	&lt;li&gt;spillInfo.partitionLengths&lt;span class=&quot;error&quot;&gt;&amp;#91;currentPartition&amp;#93;&lt;/span&gt; = fileSegment.length();&lt;br/&gt;
+    final FileSegment committedSegment;&lt;br/&gt;
+    try (DiskBlockObjectWriter writer =&lt;br/&gt;
+        blockManager.getDiskWriter(blockId, file, ser, fileBufferSizeBytes, writeMetricsToUse)) {&lt;br/&gt;
+&lt;br/&gt;
+      final int uaoSize = UnsafeAlignedOffset.getUaoSize();&lt;br/&gt;
+      while (sortedRecords.hasNext()) {&lt;br/&gt;
+        sortedRecords.loadNext();&lt;br/&gt;
+        final int partition = sortedRecords.packedRecordPointer.getPartitionId();&lt;br/&gt;
+        assert (partition &amp;gt;= currentPartition);&lt;br/&gt;
+        if (partition != currentPartition) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+          // Switch to the new partition+          if (currentPartition != -1) {
+            final FileSegment fileSegment = writer.commitAndGet();
+            spillInfo.partitionLengths[currentPartition] = fileSegment.length();
+          }+          currentPartition = partition;         }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
	&lt;li&gt;currentPartition = partition;&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();&lt;/li&gt;
	&lt;li&gt;final Object recordPage = taskMemoryManager.getPage(recordPointer);&lt;/li&gt;
	&lt;li&gt;final long recordOffsetInPage = taskMemoryManager.getOffsetInPage(recordPointer);&lt;/li&gt;
	&lt;li&gt;int dataRemaining = UnsafeAlignedOffset.getSize(recordPage, recordOffsetInPage);&lt;/li&gt;
	&lt;li&gt;long recordReadPosition = recordOffsetInPage + uaoSize; // skip over record length&lt;/li&gt;
	&lt;li&gt;while (dataRemaining &amp;gt; 0) {&lt;/li&gt;
	&lt;li&gt;final int toTransfer = Math.min(diskWriteBufferSize, dataRemaining);&lt;/li&gt;
	&lt;li&gt;Platform.copyMemory(&lt;/li&gt;
	&lt;li&gt;recordPage, recordReadPosition, writeBuffer, Platform.BYTE_ARRAY_OFFSET, toTransfer);&lt;/li&gt;
	&lt;li&gt;writer.write(writeBuffer, 0, toTransfer);&lt;/li&gt;
	&lt;li&gt;recordReadPosition += toTransfer;&lt;/li&gt;
	&lt;li&gt;dataRemaining -= toTransfer;&lt;br/&gt;
+        final long recordPointer = sortedRecords.packedRecordPointer.getRecordPointer();&lt;br/&gt;
+        final Object recordPage = taskMemoryManager.getPage(recordPointer);&lt;br/&gt;
+        final long recordOffsetInPage = taskMemoryManager.getOffsetInPage(recordPointer);&lt;br/&gt;
+        int dataRemaining = UnsafeAlignedOffset.getSize(recordPage, recordOffsetInPage);&lt;br/&gt;
+        long recordReadPosition = recordOffsetInPage + uaoSize; // skip over record length&lt;br/&gt;
+        while (dataRemaining &amp;gt; 0) 
{
+          final int toTransfer = Math.min(diskWriteBufferSize, dataRemaining);
+          Platform.copyMemory(
+            recordPage, recordReadPosition, writeBuffer, Platform.BYTE_ARRAY_OFFSET, toTransfer);
+          writer.write(writeBuffer, 0, toTransfer);
+          recordReadPosition += toTransfer;
+          dataRemaining -= toTransfer;
+        }
&lt;p&gt;+        writer.recordWritten();&lt;br/&gt;
       }&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;writer.recordWritten();&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final FileSegment committedSegment = writer.commitAndGet();&lt;/li&gt;
	&lt;li&gt;writer.close();&lt;br/&gt;
+      committedSegment = writer.commitAndGet();&lt;br/&gt;
+    }&lt;br/&gt;
     // If `writeSortedFile()` was called from `closeAndGetSpills()` and no records were inserted,&lt;br/&gt;
     // then the file might be empty. Note that it might be better to avoid calling&lt;br/&gt;
     // writeSortedFile() in that case.&lt;br/&gt;
@@ -240,9 +242,14 @@ private void writeSortedFile(boolean isLastFile) 
{
       //
       // Note that we intentionally ignore the value of `writeMetricsToUse.shuffleWriteTime()`.
       // Consistent with ExternalSorter, we do not count this IO towards shuffle write time.
-      // This means that this IO time is not accounted for anywhere; SPARK-3577 will fix this.
-      writeMetrics.incRecordsWritten(writeMetricsToUse.recordsWritten());
-      taskContext.taskMetrics().incDiskBytesSpilled(writeMetricsToUse.bytesWritten());
+      // SPARK-3577 tracks the spill time separately.
+
+      // This is guaranteed to be a ShuffleWriteMetrics based on the if check in the beginning
+      // of this method.
+      writeMetrics.incRecordsWritten(
+        ((ShuffleWriteMetrics)writeMetricsToUse).recordsWritten());
+      taskContext.taskMetrics().incDiskBytesSpilled(
+        ((ShuffleWriteMetrics)writeMetricsToUse).bytesWritten());
     }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java&lt;br/&gt;
index 4b48599ad311e..0d069125dc60e 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleInMemorySorter.java&lt;br/&gt;
@@ -20,6 +20,7 @@&lt;br/&gt;
 import java.util.Comparator;&lt;/p&gt;

&lt;p&gt; import org.apache.spark.memory.MemoryConsumer;&lt;br/&gt;
+import org.apache.spark.unsafe.Platform;&lt;br/&gt;
 import org.apache.spark.unsafe.array.LongArray;&lt;br/&gt;
 import org.apache.spark.unsafe.memory.MemoryBlock;&lt;br/&gt;
 import org.apache.spark.util.collection.Sorter;&lt;br/&gt;
@@ -112,7 +113,13 @@ public void reset() {&lt;/p&gt;

&lt;p&gt;   public void expandPointerArray(LongArray newArray) {&lt;br/&gt;
     assert(newArray.size() &amp;gt; array.size());&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;MemoryBlock.copyMemory(array.memoryBlock(), newArray.memoryBlock(), pos * 8L);&lt;br/&gt;
+    Platform.copyMemory(&lt;br/&gt;
+      array.getBaseObject(),&lt;br/&gt;
+      array.getBaseOffset(),&lt;br/&gt;
+      newArray.getBaseObject(),&lt;br/&gt;
+      newArray.getBaseOffset(),&lt;br/&gt;
+      pos * 8L&lt;br/&gt;
+    );&lt;br/&gt;
     consumer.freeArray(array);&lt;br/&gt;
     array = newArray;&lt;br/&gt;
     usableCapacity = getUsableCapacity();&lt;br/&gt;
@@ -181,7 +188,10 @@ public ShuffleSorterIterator getSortedIterator() 
{
         PackedRecordPointer.PARTITION_ID_START_BYTE_INDEX,
         PackedRecordPointer.PARTITION_ID_END_BYTE_INDEX, false, false);
     }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;MemoryBlock unused = array.memoryBlock().subBlock(pos * 8L, (array.size() - pos) * 8L);&lt;br/&gt;
+      MemoryBlock unused = new MemoryBlock(&lt;br/&gt;
+        array.getBaseObject(),&lt;br/&gt;
+        array.getBaseOffset() + pos * 8L,&lt;br/&gt;
+        (array.size() - pos) * 8L);&lt;br/&gt;
       LongArray buffer = new LongArray(unused);&lt;br/&gt;
       Sorter&amp;lt;PackedRecordPointer, LongArray&amp;gt; sorter =&lt;br/&gt;
         new Sorter&amp;lt;&amp;gt;(new ShuffleSortDataFormat(buffer));&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java&lt;br/&gt;
index 254449e95443e..717bdd79d47ef 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/shuffle/sort/ShuffleSortDataFormat.java&lt;br/&gt;
@@ -17,8 +17,8 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; package org.apache.spark.shuffle.sort;&lt;/p&gt;

&lt;p&gt;+import org.apache.spark.unsafe.Platform;&lt;br/&gt;
 import org.apache.spark.unsafe.array.LongArray;&lt;br/&gt;
-import org.apache.spark.unsafe.memory.MemoryBlock;&lt;br/&gt;
 import org.apache.spark.util.collection.SortDataFormat;&lt;/p&gt;

&lt;p&gt; final class ShuffleSortDataFormat extends SortDataFormat&amp;lt;PackedRecordPointer, LongArray&amp;gt; {&lt;br/&gt;
@@ -60,8 +60,13 @@ public void copyElement(LongArray src, int srcPos, LongArray dst, int dstPos) {&lt;/p&gt;

&lt;p&gt;   @Override&lt;br/&gt;
   public void copyRange(LongArray src, int srcPos, LongArray dst, int dstPos, int length) &lt;/p&gt;
{
-    MemoryBlock.copyMemory(src.memoryBlock(), srcPos * 8L,
-      dst.memoryBlock(),dstPos * 8L,length * 8L);
+    Platform.copyMemory(
+      src.getBaseObject(),
+      src.getBaseOffset() + srcPos * 8L,
+      dst.getBaseObject(),
+      dst.getBaseOffset() + dstPos * 8L,
+      length * 8L
+    );
   }

&lt;p&gt;   @Override&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java b/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java&lt;br/&gt;
index 069e6d5f224d7..4b0c74341551e 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/shuffle/sort/UnsafeShuffleWriter.java&lt;br/&gt;
@@ -37,7 +37,6 @@&lt;/p&gt;

&lt;p&gt; import org.apache.spark.*;&lt;br/&gt;
 import org.apache.spark.annotation.Private;&lt;br/&gt;
-import org.apache.spark.executor.ShuffleWriteMetrics;&lt;br/&gt;
 import org.apache.spark.io.CompressionCodec;&lt;br/&gt;
 import org.apache.spark.io.CompressionCodec$;&lt;br/&gt;
 import org.apache.spark.io.NioBufferedFileInputStream;&lt;br/&gt;
@@ -47,6 +46,7 @@&lt;br/&gt;
 import org.apache.spark.network.util.LimitedInputStream;&lt;br/&gt;
 import org.apache.spark.scheduler.MapStatus;&lt;br/&gt;
 import org.apache.spark.scheduler.MapStatus$;&lt;br/&gt;
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;&lt;br/&gt;
 import org.apache.spark.serializer.SerializationStream;&lt;br/&gt;
 import org.apache.spark.serializer.SerializerInstance;&lt;br/&gt;
 import org.apache.spark.shuffle.IndexShuffleBlockResolver;&lt;br/&gt;
@@ -73,7 +73,7 @@&lt;br/&gt;
   private final TaskMemoryManager memoryManager;&lt;br/&gt;
   private final SerializerInstance serializer;&lt;br/&gt;
   private final Partitioner partitioner;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final ShuffleWriteMetrics writeMetrics;&lt;br/&gt;
+  private final ShuffleWriteMetricsReporter writeMetrics;&lt;br/&gt;
   private final int shuffleId;&lt;br/&gt;
   private final int mapId;&lt;br/&gt;
   private final TaskContext taskContext;&lt;br/&gt;
@@ -122,7 +122,8 @@ public UnsafeShuffleWriter(&lt;br/&gt;
       SerializedShuffleHandle&amp;lt;K, V&amp;gt; handle,&lt;br/&gt;
       int mapId,&lt;br/&gt;
       TaskContext taskContext,&lt;/li&gt;
	&lt;li&gt;SparkConf sparkConf) throws IOException {&lt;br/&gt;
+      SparkConf sparkConf,&lt;br/&gt;
+      ShuffleWriteMetricsReporter writeMetrics) throws IOException {&lt;br/&gt;
     final int numPartitions = handle.dependency().partitioner().numPartitions();&lt;br/&gt;
     if (numPartitions &amp;gt; SortShuffleManager.MAX_SHUFFLE_OUTPUT_PARTITIONS_FOR_SERIALIZED_MODE()) {&lt;br/&gt;
       throw new IllegalArgumentException(&lt;br/&gt;
@@ -138,7 +139,7 @@ public UnsafeShuffleWriter(&lt;br/&gt;
     this.shuffleId = dep.shuffleId();&lt;br/&gt;
     this.serializer = dep.serializer().newInstance();&lt;br/&gt;
     this.partitioner = dep.partitioner();&lt;/li&gt;
	&lt;li&gt;this.writeMetrics = taskContext.taskMetrics().shuffleWriteMetrics();&lt;br/&gt;
+    this.writeMetrics = writeMetrics;&lt;br/&gt;
     this.taskContext = taskContext;&lt;br/&gt;
     this.sparkConf = sparkConf;&lt;br/&gt;
     this.transferToEnabled = sparkConf.getBoolean(&quot;spark.file.transferTo&quot;, true);&lt;br/&gt;
@@ -248,8 +249,7 @@ void closeAndWriteOutput() throws IOException {&lt;br/&gt;
         logger.error(&quot;Error while deleting temp file {}&quot;, tmp.getAbsolutePath());&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;/li&gt;
	&lt;li&gt;mapStatus = MapStatus$.MODULE$.apply(&lt;/li&gt;
	&lt;li&gt;blockManager.shuffleServerId(), partitionLengths, writeMetrics.recordsWritten());&lt;br/&gt;
+    mapStatus = MapStatus$.MODULE$.apply(blockManager.shuffleServerId(), partitionLengths);&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @VisibleForTesting&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java b/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java&lt;br/&gt;
index 5d0555a8c28e1..fcba3b73445c9 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/storage/TimeTrackingOutputStream.java&lt;br/&gt;
@@ -21,7 +21,7 @@&lt;br/&gt;
 import java.io.OutputStream;&lt;/p&gt;

&lt;p&gt; import org.apache.spark.annotation.Private;&lt;br/&gt;
-import org.apache.spark.executor.ShuffleWriteMetrics;&lt;br/&gt;
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter;&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Intercepts write calls and tracks total time spent writing in order to update shuffle write&lt;br/&gt;
@@ -30,10 +30,11 @@&lt;br/&gt;
 @Private&lt;br/&gt;
 public final class TimeTrackingOutputStream extends OutputStream {&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private final ShuffleWriteMetrics writeMetrics;&lt;br/&gt;
+  private final ShuffleWriteMetricsReporter writeMetrics;&lt;br/&gt;
   private final OutputStream outputStream;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public TimeTrackingOutputStream(ShuffleWriteMetrics writeMetrics, OutputStream outputStream) {&lt;br/&gt;
+  public TimeTrackingOutputStream(&lt;br/&gt;
+      ShuffleWriteMetricsReporter writeMetrics, OutputStream outputStream) 
{
     this.writeMetrics = writeMetrics;
     this.outputStream = outputStream;
   }
&lt;p&gt;diff --git a/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java b/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java&lt;br/&gt;
index 9b6cbab38cbcc..a4e88598f7607 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/unsafe/map/BytesToBytesMap.java&lt;br/&gt;
@@ -31,6 +31,7 @@&lt;br/&gt;
 import org.apache.spark.SparkEnv;&lt;br/&gt;
 import org.apache.spark.executor.ShuffleWriteMetrics;&lt;br/&gt;
 import org.apache.spark.memory.MemoryConsumer;&lt;br/&gt;
+import org.apache.spark.memory.SparkOutOfMemoryError;&lt;br/&gt;
 import org.apache.spark.memory.TaskMemoryManager;&lt;br/&gt;
 import org.apache.spark.serializer.SerializerManager;&lt;br/&gt;
 import org.apache.spark.storage.BlockManager;&lt;br/&gt;
@@ -741,7 +742,7 @@ public boolean append(Object kbase, long koff, int klen, Object vbase, long voff&lt;br/&gt;
         if (numKeys &amp;gt;= growthThreshold &amp;amp;&amp;amp; longArray.size() &amp;lt; MAX_CAPACITY) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {           try {
             growAndRehash();
-          } catch (OutOfMemoryError oom) {
+          } catch (SparkOutOfMemoryError oom) {
             canGrowArray = false;
           }         }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;@@ -757,7 +758,7 @@ public boolean append(Object kbase, long koff, int klen, Object vbase, long voff&lt;br/&gt;
   private boolean acquireNewPage(long required) {&lt;br/&gt;
     try &lt;/p&gt;
{
       currentPage = allocatePage(required);
-    }
&lt;p&gt; catch (OutOfMemoryError e) &lt;/p&gt;
{
+    }
&lt;p&gt; catch (SparkOutOfMemoryError e) &lt;/p&gt;
{
       return false;
     }
&lt;p&gt;     dataPages.add(currentPage);&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java&lt;br/&gt;
index 399251b80e649..5056652a2420b 100644&lt;/p&gt;&lt;/li&gt;
			&lt;li&gt;a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeExternalSorter.java&lt;br/&gt;
@@ -544,7 +544,7 @@ public long spill() throws IOException {&lt;br/&gt;
           // is accessing the current record. We free this page in that caller&apos;s next loadNext()&lt;br/&gt;
           // call.&lt;br/&gt;
           for (MemoryBlock page : allocatedPages) {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;if (!loaded || page.getPageNumber() !=&lt;br/&gt;
+            if (!loaded || page.pageNumber !=&lt;br/&gt;
                     ((UnsafeInMemorySorter.SortedIterator)upstream).getCurrentPageNumber()) {&lt;br/&gt;
               released += page.size();&lt;br/&gt;
               freePage(page);&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java&lt;br/&gt;
index 717823ebbd320..1a9453a8b3e80 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeInMemorySorter.java&lt;br/&gt;
@@ -26,6 +26,7 @@&lt;br/&gt;
 import org.apache.spark.memory.MemoryConsumer;&lt;br/&gt;
 import org.apache.spark.memory.SparkOutOfMemoryError;&lt;br/&gt;
 import org.apache.spark.memory.TaskMemoryManager;&lt;br/&gt;
+import org.apache.spark.unsafe.Platform;&lt;br/&gt;
 import org.apache.spark.unsafe.UnsafeAlignedOffset;&lt;br/&gt;
 import org.apache.spark.unsafe.array.LongArray;&lt;br/&gt;
 import org.apache.spark.unsafe.memory.MemoryBlock;&lt;br/&gt;
@@ -213,9 +214,16 @@ public boolean hasSpaceForAnotherRecord() {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   public void expandPointerArray(LongArray newArray) {&lt;br/&gt;
     if (newArray.size() &amp;lt; array.size()) &lt;/p&gt;
{
+      // checkstyle.off: RegexpSinglelineJava
       throw new SparkOutOfMemoryError(&quot;Not enough memory to grow pointer array&quot;);
+      // checkstyle.on: RegexpSinglelineJava
     }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;MemoryBlock.copyMemory(array.memoryBlock(), newArray.memoryBlock(), pos * 8L);&lt;br/&gt;
+    Platform.copyMemory(&lt;br/&gt;
+      array.getBaseObject(),&lt;br/&gt;
+      array.getBaseOffset(),&lt;br/&gt;
+      newArray.getBaseObject(),&lt;br/&gt;
+      newArray.getBaseOffset(),&lt;br/&gt;
+      pos * 8L);&lt;br/&gt;
     consumer.freeArray(array);&lt;br/&gt;
     array = newArray;&lt;br/&gt;
     usableCapacity = getUsableCapacity();&lt;br/&gt;
@@ -342,7 +350,10 @@ public UnsafeSorterIterator getSortedIterator() 
{
           array, nullBoundaryPos, (pos - nullBoundaryPos) / 2L, 0, 7,
           radixSortSupport.sortDescending(), radixSortSupport.sortSigned());
       }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;MemoryBlock unused = array.memoryBlock().subBlock(pos * 8L, (array.size() - pos) * 8L);&lt;br/&gt;
+        MemoryBlock unused = new MemoryBlock(&lt;br/&gt;
+          array.getBaseObject(),&lt;br/&gt;
+          array.getBaseOffset() + pos * 8L,&lt;br/&gt;
+          (array.size() - pos) * 8L);&lt;br/&gt;
         LongArray buffer = new LongArray(unused);&lt;br/&gt;
         Sorter&amp;lt;RecordPointerAndKeyPrefix, LongArray&amp;gt; sorter =&lt;br/&gt;
           new Sorter&amp;lt;&amp;gt;(new UnsafeSortDataFormat(buffer));&lt;br/&gt;
diff --git a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java&lt;br/&gt;
index 9399024f01783..c1d71a23b1dbe 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java&lt;br/&gt;
+++ b/core/src/main/java/org/apache/spark/util/collection/unsafe/sort/UnsafeSorterSpillWriter.java&lt;br/&gt;
@@ -42,7 +42,10 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private final SparkConf conf = new SparkConf();&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/** The buffer size to use when writing the sorted records to an on-disk file */&lt;br/&gt;
+  /**&lt;br/&gt;
+   * The buffer size to use when writing the sorted records to an on-disk file, and&lt;br/&gt;
+   * this space used by prefix + len + recordLength must be greater than 4 + 8 bytes.&lt;br/&gt;
+   */&lt;br/&gt;
   private final int diskWriteBufferSize =&lt;br/&gt;
     (int) (long) conf.get(package$.MODULE$.SHUFFLE_DISK_WRITE_BUFFER_SIZE());&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html b/core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html&lt;br/&gt;
index 5c91304e49fd7..f2c17aef097a4 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html&lt;br/&gt;
+++ b/core/src/main/resources/org/apache/spark/ui/static/executorspage-template.html&lt;br/&gt;
@@ -16,10 +16,10 @@&lt;br/&gt;
 --&amp;gt;&lt;/p&gt;

&lt;p&gt; &amp;lt;script id=&quot;executors-summary-template&quot; type=&quot;text/html&quot;&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;h4 style=&quot;clear: left; display: inline-block;&quot;&amp;gt;Summary&amp;lt;/h4&amp;gt;&lt;br/&gt;
+  &amp;lt;h4 class=&quot;title-table&quot;&amp;gt;Summary&amp;lt;/h4&amp;gt;&lt;br/&gt;
   &amp;lt;div class=&quot;container-fluid&quot;&amp;gt;&lt;br/&gt;
     &amp;lt;div class=&quot;container-fluid&quot;&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;table id=&quot;summary-execs-table&quot; class=&quot;table table-striped compact&quot;&amp;gt;&lt;br/&gt;
+      &amp;lt;table id=&quot;summary-execs-table&quot; class=&quot;table table-striped compact cell-border&quot;&amp;gt;&lt;br/&gt;
         &amp;lt;thead&amp;gt;&lt;br/&gt;
         &amp;lt;th&amp;gt;&amp;lt;/th&amp;gt;&lt;br/&gt;
         &amp;lt;th&amp;gt;RDD Blocks&amp;lt;/th&amp;gt;&lt;br/&gt;
@@ -64,10 +64,10 @@ &amp;lt;h4 style=&quot;clear: left; display: inline-block;&quot;&amp;gt;Summary&amp;lt;/h4&amp;gt;&lt;br/&gt;
       &amp;lt;/table&amp;gt;&lt;br/&gt;
     &amp;lt;/div&amp;gt;&lt;br/&gt;
   &amp;lt;/div&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;h4 style=&quot;clear: left; display: inline-block;&quot;&amp;gt;Executors&amp;lt;/h4&amp;gt;&lt;br/&gt;
+  &amp;lt;h4 class=&quot;title-table&quot;&amp;gt;Executors&amp;lt;/h4&amp;gt;&lt;br/&gt;
   &amp;lt;div class=&quot;container-fluid&quot;&amp;gt;&lt;br/&gt;
     &amp;lt;div class=&quot;container-fluid&quot;&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;table id=&quot;active-executors-table&quot; class=&quot;table table-striped compact&quot;&amp;gt;&lt;br/&gt;
+      &amp;lt;table id=&quot;active-executors-table&quot; class=&quot;table table-striped compact cell-border&quot;&amp;gt;&lt;br/&gt;
         &amp;lt;thead&amp;gt;&lt;br/&gt;
         &amp;lt;tr&amp;gt;&lt;br/&gt;
           &amp;lt;th&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/executorspage.js b/core/src/main/resources/org/apache/spark/ui/static/executorspage.js&lt;br/&gt;
index 6717af3ac4daf..a48c02ae279ba 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/resources/org/apache/spark/ui/static/executorspage.js&lt;br/&gt;
+++ b/core/src/main/resources/org/apache/spark/ui/static/executorspage.js&lt;br/&gt;
@@ -59,78 +59,6 @@ $(document).ajaxStart(function () 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     $.blockUI({message: &apos;&amp;lt;h3&amp;gt;Loading Executors Page...&amp;lt;/h3&amp;gt;&apos;}); }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-function createTemplateURI(appId) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var words = document.baseURI.split(&apos;/&apos;);&lt;/li&gt;
	&lt;li&gt;var ind = words.indexOf(&quot;proxy&quot;);&lt;/li&gt;
	&lt;li&gt;if (ind &amp;gt; 0) 
{
-        var baseURI = words.slice(0, ind + 1).join(&apos;/&apos;) + &apos;/&apos; + appId + &apos;/static/executorspage-template.html&apos;;
-        return baseURI;
-    }&lt;/li&gt;
	&lt;li&gt;ind = words.indexOf(&quot;history&quot;);&lt;/li&gt;
	&lt;li&gt;if(ind &amp;gt; 0) 
{
-        var baseURI = words.slice(0, ind).join(&apos;/&apos;) + &apos;/static/executorspage-template.html&apos;;
-        return baseURI;
-    }&lt;/li&gt;
	&lt;li&gt;return location.origin + &quot;/static/executorspage-template.html&quot;;&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
-function getStandAloneppId(cb) {&lt;/li&gt;
	&lt;li&gt;var words = document.baseURI.split(&apos;/&apos;);&lt;/li&gt;
	&lt;li&gt;var ind = words.indexOf(&quot;proxy&quot;);&lt;/li&gt;
	&lt;li&gt;if (ind &amp;gt; 0) 
{
-        var appId = words[ind + 1];
-        cb(appId);
-        return;
-    }&lt;br/&gt;
-    ind = words.indexOf(&quot;history&quot;);&lt;br/&gt;
-    if (ind &amp;gt; 0) {-        var appId = words[ind + 1];-        cb(appId);-        return;-    }&lt;/li&gt;
	&lt;li&gt;//Looks like Web UI is running in standalone mode&lt;/li&gt;
	&lt;li&gt;//Let&apos;s get application-id using REST End Point&lt;/li&gt;
	&lt;li&gt;$.getJSON(location.origin + &quot;/api/v1/applications&quot;, function(response, status, jqXHR) {&lt;/li&gt;
	&lt;li&gt;if (response &amp;amp;&amp;amp; response.length &amp;gt; 0) 
{
-            var appId = response[0].id
-            cb(appId);
-            return;
-        }&lt;/li&gt;
	&lt;li&gt;});&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
-function createRESTEndPoint(appId) {&lt;/li&gt;
	&lt;li&gt;var words = document.baseURI.split(&apos;/&apos;);&lt;/li&gt;
	&lt;li&gt;var ind = words.indexOf(&quot;proxy&quot;);&lt;/li&gt;
	&lt;li&gt;if (ind &amp;gt; 0) 
{
-        var appId = words[ind + 1];
-        var newBaseURI = words.slice(0, ind + 2).join(&apos;/&apos;);
-        return newBaseURI + &quot;/api/v1/applications/&quot; + appId + &quot;/allexecutors&quot;
-    }&lt;/li&gt;
	&lt;li&gt;ind = words.indexOf(&quot;history&quot;);&lt;/li&gt;
	&lt;li&gt;if (ind &amp;gt; 0) {&lt;/li&gt;
	&lt;li&gt;var appId = words&lt;span class=&quot;error&quot;&gt;&amp;#91;ind + 1&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;var attemptId = words&lt;span class=&quot;error&quot;&gt;&amp;#91;ind + 2&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;var newBaseURI = words.slice(0, ind).join(&apos;/&apos;);&lt;/li&gt;
	&lt;li&gt;if (isNaN(attemptId)) 
{
-            return newBaseURI + &quot;/api/v1/applications/&quot; + appId + &quot;/allexecutors&quot;;
-        }
&lt;p&gt; else &lt;/p&gt;
{
-            return newBaseURI + &quot;/api/v1/applications/&quot; + appId + &quot;/&quot; + attemptId + &quot;/allexecutors&quot;;
-        }&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;return location.origin + &quot;/api/v1/applications/&quot; + appId + &quot;/allexecutors&quot;;&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
-function formatLogsCells(execLogs, type) {&lt;/li&gt;
	&lt;li&gt;if (type !== &apos;display&apos;) return Object.keys(execLogs);&lt;/li&gt;
	&lt;li&gt;if (!execLogs) return;&lt;/li&gt;
	&lt;li&gt;var result = &apos;&apos;;&lt;/li&gt;
	&lt;li&gt;$.each(execLogs, function (logName, logUrl) 
{
-        result += &apos;&amp;lt;div&amp;gt;&amp;lt;a href=&apos; + logUrl + &apos;&amp;gt;&apos; + logName + &apos;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt;&apos;
-    }
&lt;p&gt;);&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;return result;&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
 function logsExist(execs) {&lt;br/&gt;
     return execs.some(function(exec) {&lt;br/&gt;
         return !($.isEmptyObject(exec&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;executorLogs&amp;quot;&amp;#93;&lt;/span&gt;));&lt;br/&gt;
@@ -178,17 +106,13 @@ function totalDurationColor(totalGCTime, totalDuration) {&lt;br/&gt;
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; $(document).ready(function () {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;$.extend($.fn.dataTable.defaults, 
{
-        stateSave: true,
-        lengthMenu: [[20, 40, 60, 100, -1], [20, 40, 60, 100, &quot;All&quot;]],
-        pageLength: 20
-    }
&lt;p&gt;);&lt;br/&gt;
+    setDataTableDefaults();&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     executorsSummary = $(&quot;#active-executors&quot;);&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;getStandAloneppId(function (appId) {&lt;br/&gt;
+    getStandAloneAppId(function (appId) {&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;var endPoint = createRESTEndPoint(appId);&lt;br/&gt;
+        var endPoint = createRESTEndPointForExecutorsPage(appId);&lt;br/&gt;
         $.getJSON(endPoint, function (response, status, jqXHR) {&lt;br/&gt;
             var summary = [];&lt;br/&gt;
             var allExecCnt = 0;&lt;br/&gt;
@@ -408,7 +332,7 @@ $(document).ready(function () {&lt;br/&gt;
             };&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;             var data = &lt;/p&gt;
{executors: response, &quot;execSummary&quot;: [activeSummary, deadSummary, totalSummary]}
&lt;p&gt;;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;$.get(createTemplateURI(appId), function (template) {&lt;br/&gt;
+            $.get(createTemplateURI(appId, &quot;executorspage&quot;), function (template) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;                 executorsSummary.append(Mustache.render($(template).filter(&quot;#executors-summary-template&quot;).html(), data));&lt;br/&gt;
                 var selector = &quot;#active-executors-table&quot;;&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc.png b/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc.png&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..e1ba61a8055fc&lt;br/&gt;
Binary files /dev/null and b/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc.png differ&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc_disabled.png b/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc_disabled.png&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..fb11dfe24a6c5&lt;br/&gt;
Binary files /dev/null and b/core/src/main/resources/org/apache/spark/ui/static/images/sort_asc_disabled.png differ&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/images/sort_both.png b/core/src/main/resources/org/apache/spark/ui/static/images/sort_both.png&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..af5bc7c5a10b9&lt;br/&gt;
Binary files /dev/null and b/core/src/main/resources/org/apache/spark/ui/static/images/sort_both.png differ&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc.png b/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc.png&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..0e156deb5f61d&lt;br/&gt;
Binary files /dev/null and b/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc.png differ&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc_disabled.png b/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc_disabled.png&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..c9fdd8a1502fd&lt;br/&gt;
Binary files /dev/null and b/core/src/main/resources/org/apache/spark/ui/static/images/sort_desc_disabled.png differ&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/stagepage.js b/core/src/main/resources/org/apache/spark/ui/static/stagepage.js&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..4c83ec7e95ab1&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/resources/org/apache/spark/ui/static/stagepage.js&lt;br/&gt;
@@ -0,0 +1,958 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+var shouldBlockUI = true;&lt;br/&gt;
+&lt;br/&gt;
+$(document).ajaxStop(function () {&lt;br/&gt;
+    if (shouldBlockUI) &lt;/p&gt;
{
+        $.unblockUI();
+        shouldBlockUI = false;
+    }
&lt;p&gt;+});&lt;br/&gt;
+&lt;br/&gt;
+$(document).ajaxStart(function () {&lt;br/&gt;
+    if (shouldBlockUI) {&lt;br/&gt;
+        $.blockUI(&lt;/p&gt;
{message: &apos;&amp;lt;h3&amp;gt;Loading Stage Page...&amp;lt;/h3&amp;gt;&apos;}
&lt;p&gt;);&lt;br/&gt;
+    }&lt;br/&gt;
+});&lt;br/&gt;
+&lt;br/&gt;
+$.extend( $.fn.dataTable.ext.type.order, {&lt;br/&gt;
+    &quot;duration-pre&quot;: ConvertDurationString,&lt;br/&gt;
+&lt;br/&gt;
+    &quot;duration-asc&quot;: function ( a, b ) &lt;/p&gt;
{
+        a = ConvertDurationString( a );
+        b = ConvertDurationString( b );
+        return ((a &amp;lt; b) ? -1 : ((a &amp;gt; b) ? 1 : 0));
+    }
&lt;p&gt;,&lt;br/&gt;
+&lt;br/&gt;
+    &quot;duration-desc&quot;: function ( a, b ) &lt;/p&gt;
{
+        a = ConvertDurationString( a );
+        b = ConvertDurationString( b );
+        return ((a &amp;lt; b) ? 1 : ((a &amp;gt; b) ? -1 : 0));
+    }
&lt;p&gt;+} );&lt;br/&gt;
+&lt;br/&gt;
+// This function will only parse the URL under certain format&lt;br/&gt;
+// e.g. (history) &lt;a href=&quot;https://domain:50509/history/application_1536254569791_3806251/1/stages/stage/?id=4&amp;amp;attempt=1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://domain:50509/history/application_1536254569791_3806251/1/stages/stage/?id=4&amp;amp;attempt=1&lt;/a&gt;&lt;br/&gt;
+// e.g. (proxy) &lt;a href=&quot;https://domain:50505/proxy/application_1502220952225_59143/stages/stage?id=4&amp;amp;attempt=1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://domain:50505/proxy/application_1502220952225_59143/stages/stage?id=4&amp;amp;attempt=1&lt;/a&gt;&lt;br/&gt;
+function stageEndPoint(appId) {&lt;br/&gt;
+    var queryString = document.baseURI.split(&apos;?&apos;);&lt;br/&gt;
+    var words = document.baseURI.split(&apos;/&apos;);&lt;br/&gt;
+    var indexOfProxy = words.indexOf(&quot;proxy&quot;);&lt;br/&gt;
+    var stageId = queryString&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;.split(&quot;&amp;amp;&quot;).filter(word =&amp;gt; word.includes(&quot;id=&quot;))&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;.split(&quot;=&quot;)&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+    if (indexOfProxy &amp;gt; 0) &lt;/p&gt;
{
+        var appId = words[indexOfProxy + 1];
+        var newBaseURI = words.slice(0, words.indexOf(&quot;proxy&quot;) + 2).join(&apos;/&apos;);
+        return newBaseURI + &quot;/api/v1/applications/&quot; + appId + &quot;/stages/&quot; + stageId;
+    }
&lt;p&gt;+    var indexOfHistory = words.indexOf(&quot;history&quot;);&lt;br/&gt;
+    if (indexOfHistory &amp;gt; 0) {&lt;br/&gt;
+        var appId = words&lt;span class=&quot;error&quot;&gt;&amp;#91;indexOfHistory + 1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        var appAttemptId = words&lt;span class=&quot;error&quot;&gt;&amp;#91;indexOfHistory + 2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        var newBaseURI = words.slice(0, words.indexOf(&quot;history&quot;)).join(&apos;/&apos;);&lt;br/&gt;
+        if (isNaN(appAttemptId) || appAttemptId == &quot;0&quot;) &lt;/p&gt;
{
+            return newBaseURI + &quot;/api/v1/applications/&quot; + appId + &quot;/stages/&quot; + stageId;
+        }
&lt;p&gt; else &lt;/p&gt;
{
+            return newBaseURI + &quot;/api/v1/applications/&quot; + appId + &quot;/&quot; + appAttemptId + &quot;/stages/&quot; + stageId;
+        }
&lt;p&gt;+    }&lt;br/&gt;
+    return location.origin + &quot;/api/v1/applications/&quot; + appId + &quot;/stages/&quot; + stageId;&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+function getColumnNameForTaskMetricSummary(columnKey) {&lt;br/&gt;
+    switch(columnKey) &lt;/p&gt;
{
+        case &quot;executorRunTime&quot;:
+            return &quot;Duration&quot;;
+
+        case &quot;jvmGcTime&quot;:
+            return &quot;GC Time&quot;;
+
+        case &quot;gettingResultTime&quot;:
+            return &quot;Getting Result Time&quot;;
+
+        case &quot;inputMetrics&quot;:
+            return &quot;Input Size / Records&quot;;
+
+        case &quot;outputMetrics&quot;:
+            return &quot;Output Size / Records&quot;;
+
+        case &quot;peakExecutionMemory&quot;:
+            return &quot;Peak Execution Memory&quot;;
+
+        case &quot;resultSerializationTime&quot;:
+            return &quot;Result Serialization Time&quot;;
+
+        case &quot;schedulerDelay&quot;:
+            return &quot;Scheduler Delay&quot;;
+
+        case &quot;diskBytesSpilled&quot;:
+            return &quot;Shuffle spill (disk)&quot;;
+
+        case &quot;memoryBytesSpilled&quot;:
+            return &quot;Shuffle spill (memory)&quot;;
+
+        case &quot;shuffleReadMetrics&quot;:
+            return &quot;Shuffle Read Size / Records&quot;;
+
+        case &quot;shuffleWriteMetrics&quot;:
+            return &quot;Shuffle Write Size / Records&quot;;
+
+        case &quot;executorDeserializeTime&quot;:
+            return &quot;Task Deserialization Time&quot;;
+
+        case &quot;shuffleReadBlockedTime&quot;:
+            return &quot;Shuffle Read Blocked Time&quot;;
+
+        case &quot;shuffleRemoteReads&quot;:
+            return &quot;Shuffle Remote Reads&quot;;
+
+        default:
+            return &quot;NA&quot;;
+    }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+function displayRowsForSummaryMetricsTable(row, type, columnIndex) {&lt;br/&gt;
+    switch(row.columnKey) &lt;/p&gt;
{
+        case &apos;inputMetrics&apos;:
+            var str = formatBytes(row.data.bytesRead[columnIndex], type) + &quot; / &quot; +
+              row.data.recordsRead[columnIndex];
+            return str;
+            break;
+
+        case &apos;outputMetrics&apos;:
+            var str = formatBytes(row.data.bytesWritten[columnIndex], type) + &quot; / &quot; +
+              row.data.recordsWritten[columnIndex];
+            return str;
+            break;
+
+        case &apos;shuffleReadMetrics&apos;:
+            var str = formatBytes(row.data.readBytes[columnIndex], type) + &quot; / &quot; +
+              row.data.readRecords[columnIndex];
+            return str;
+            break;
+
+        case &apos;shuffleReadBlockedTime&apos;:
+            var str = formatDuration(row.data.fetchWaitTime[columnIndex]);
+            return str;
+            break;
+
+        case &apos;shuffleRemoteReads&apos;:
+            var str = formatBytes(row.data.remoteBytesRead[columnIndex], type);
+            return str;
+            break;
+
+        case &apos;shuffleWriteMetrics&apos;:
+            var str = formatBytes(row.data.writeBytes[columnIndex], type) + &quot; / &quot; +
+              row.data.writeRecords[columnIndex];
+            return str;
+            break;
+
+        default:
+            return (row.columnKey == &apos;peakExecutionMemory&apos; || row.columnKey == &apos;memoryBytesSpilled&apos;
+                    || row.columnKey == &apos;diskBytesSpilled&apos;) ? formatBytes(
+                    row.data[columnIndex], type) : (formatDuration(row.data[columnIndex]));
+
+    }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+function createDataTableForTaskSummaryMetricsTable(taskSummaryMetricsTable) {&lt;br/&gt;
+    var taskMetricsTable = &quot;#summary-metrics-table&quot;;&lt;br/&gt;
+    if ($.fn.dataTable.isDataTable(taskMetricsTable)) &lt;/p&gt;
{
+        taskSummaryMetricsDataTable.clear().draw();
+        taskSummaryMetricsDataTable.rows.add(taskSummaryMetricsTable).draw();
+    }
&lt;p&gt; else {&lt;br/&gt;
+        var taskConf = {&lt;br/&gt;
+            &quot;data&quot;: taskSummaryMetricsTable,&lt;br/&gt;
+            &quot;columns&quot;: [&lt;br/&gt;
+                &lt;/p&gt;
{data : &apos;metric&apos;}
&lt;p&gt;,&lt;br/&gt;
+                // Min&lt;br/&gt;
+                {&lt;br/&gt;
+                    data: function (row, type) &lt;/p&gt;
{
+                        return displayRowsForSummaryMetricsTable(row, type, 0);
+                    }
&lt;p&gt;+                },&lt;br/&gt;
+                // 25th percentile&lt;br/&gt;
+                {&lt;br/&gt;
+                    data: function (row, type) &lt;/p&gt;
{
+                        return displayRowsForSummaryMetricsTable(row, type, 1);
+                    }
&lt;p&gt;+                },&lt;br/&gt;
+                // Median&lt;br/&gt;
+                {&lt;br/&gt;
+                    data: function (row, type) &lt;/p&gt;
{
+                        return displayRowsForSummaryMetricsTable(row, type, 2);
+                    }
&lt;p&gt;+                },&lt;br/&gt;
+                // 75th percentile&lt;br/&gt;
+                {&lt;br/&gt;
+                    data: function (row, type) &lt;/p&gt;
{
+                        return displayRowsForSummaryMetricsTable(row, type, 3);
+                    }
&lt;p&gt;+                },&lt;br/&gt;
+                // Max&lt;br/&gt;
+                {&lt;br/&gt;
+                    data: function (row, type) &lt;/p&gt;
{
+                        return displayRowsForSummaryMetricsTable(row, type, 4);
+                    }
&lt;p&gt;+                }&lt;br/&gt;
+            ],&lt;br/&gt;
+            &quot;columnDefs&quot;: [&lt;br/&gt;
+                &lt;/p&gt;
{ &quot;type&quot;: &quot;duration&quot;, &quot;targets&quot;: 1 }
&lt;p&gt;,&lt;br/&gt;
+                &lt;/p&gt;
{ &quot;type&quot;: &quot;duration&quot;, &quot;targets&quot;: 2 }
&lt;p&gt;,&lt;br/&gt;
+                &lt;/p&gt;
{ &quot;type&quot;: &quot;duration&quot;, &quot;targets&quot;: 3 }
&lt;p&gt;,&lt;br/&gt;
+                &lt;/p&gt;
{ &quot;type&quot;: &quot;duration&quot;, &quot;targets&quot;: 4 }
&lt;p&gt;,&lt;br/&gt;
+                &lt;/p&gt;
{ &quot;type&quot;: &quot;duration&quot;, &quot;targets&quot;: 5 }
&lt;p&gt;+            ],&lt;br/&gt;
+            &quot;paging&quot;: false,&lt;br/&gt;
+            &quot;searching&quot;: false,&lt;br/&gt;
+            &quot;order&quot;: [&lt;span class=&quot;error&quot;&gt;&amp;#91;0, &amp;quot;asc&amp;quot;&amp;#93;&lt;/span&gt;],&lt;br/&gt;
+            &quot;bSort&quot;: false,&lt;br/&gt;
+            &quot;bAutoWidth&quot;: false&lt;br/&gt;
+        };&lt;br/&gt;
+        taskSummaryMetricsDataTable = $(taskMetricsTable).DataTable(taskConf);&lt;br/&gt;
+    }&lt;br/&gt;
+    taskSummaryMetricsTableCurrentStateArray = taskSummaryMetricsTable.slice();&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+function createRowMetadataForColumn(colKey, data, checkboxId) {&lt;br/&gt;
+  var row = &lt;/p&gt;
{
+      &quot;metric&quot;: getColumnNameForTaskMetricSummary(colKey),
+      &quot;data&quot;: data,
+      &quot;checkboxId&quot;: checkboxId,
+      &quot;columnKey&quot;: colKey
+  }
&lt;p&gt;;&lt;br/&gt;
+  return row;&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+function reselectCheckboxesBasedOnTaskTableState() {&lt;br/&gt;
+    var allChecked = true;&lt;br/&gt;
+    var taskSummaryMetricsTableCurrentFilteredArray = taskSummaryMetricsTableCurrentStateArray.slice();&lt;br/&gt;
+    if (typeof taskTableSelector !== &apos;undefined&apos; &amp;amp;&amp;amp; taskSummaryMetricsTableCurrentStateArray.length &amp;gt; 0) {&lt;br/&gt;
+        for (var k = 0; k &amp;lt; optionalColumns.length; k++) {&lt;br/&gt;
+            if (taskTableSelector.column(optionalColumns&lt;span class=&quot;error&quot;&gt;&amp;#91;k&amp;#93;&lt;/span&gt;).visible()) &lt;/p&gt;
{
+                $(&quot;#box-&quot;+optionalColumns[k]).prop(&apos;checked&apos;, true);
+                taskSummaryMetricsTableCurrentStateArray.push(taskSummaryMetricsTableArray.filter(row =&amp;gt; (row.checkboxId).toString() == optionalColumns[k])[0]);
+                taskSummaryMetricsTableCurrentFilteredArray = taskSummaryMetricsTableCurrentStateArray.slice();
+            }
&lt;p&gt; else &lt;/p&gt;
{
+                allChecked = false;
+            }
&lt;p&gt;+        }&lt;br/&gt;
+        if (allChecked) &lt;/p&gt;
{
+            $(&quot;#box-0&quot;).prop(&apos;checked&apos;, true);
+        }
&lt;p&gt;+        createDataTableForTaskSummaryMetricsTable(taskSummaryMetricsTableCurrentFilteredArray);&lt;br/&gt;
+    }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+function getStageAttemptId() &lt;/p&gt;
{
+  var words = document.baseURI.split(&apos;?&apos;);
+  var attemptIdStr = words[1].split(&apos;&amp;amp;&apos;)[1];
+  var digitsRegex = /[0-9]+/;
+  // We are using regex here to extract the stage attempt id as there might be certain url&apos;s with format
+  // like /proxy/application_1539986433979_27115/stages/stage/?id=0&amp;amp;attempt=0#tasksTitle
+  var stgAttemptId = words[1].split(&quot;&amp;amp;&quot;).filter(
+      word =&amp;gt; word.includes(&quot;attempt=&quot;))[0].split(&quot;=&quot;)[1].match(digitsRegex);
+  return stgAttemptId;
+}
&lt;p&gt;+&lt;br/&gt;
+var taskSummaryMetricsTableArray = [];&lt;br/&gt;
+var taskSummaryMetricsTableCurrentStateArray = [];&lt;br/&gt;
+var taskSummaryMetricsDataTable;&lt;br/&gt;
+var optionalColumns = &lt;span class=&quot;error&quot;&gt;&amp;#91;11, 12, 13, 14, 15, 16, 17&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+var taskTableSelector;&lt;br/&gt;
+&lt;br/&gt;
+$(document).ready(function () {&lt;br/&gt;
+    setDataTableDefaults();&lt;br/&gt;
+&lt;br/&gt;
+    $(&quot;#showAdditionalMetrics&quot;).append(&lt;br/&gt;
+        &quot;&amp;lt;div&amp;gt;&amp;lt;a id=&apos;additionalMetrics&apos;&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;span class=&apos;expand-input-rate-arrow arrow-closed&apos; id=&apos;arrowtoggle1&apos;&amp;gt;&amp;lt;/span&amp;gt;&quot; +&lt;br/&gt;
+        &quot; Show Additional Metrics&quot; +&lt;br/&gt;
+        &quot;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;div class=&apos;container-fluid container-fluid-div&apos; id=&apos;toggle-metrics&apos; hidden&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;div&amp;gt;&amp;lt;input type=&apos;checkbox&apos; class=&apos;toggle-vis&apos; id=&apos;box-0&apos; data-column=&apos;0&apos;&amp;gt; Select All&amp;lt;/div&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;div id=&apos;scheduler_delay&apos; class=&apos;scheduler-delay-checkbox-div&apos;&amp;gt;&amp;lt;input type=&apos;checkbox&apos; class=&apos;toggle-vis&apos; id=&apos;box-11&apos; data-column=&apos;11&apos;&amp;gt; Scheduler Delay&amp;lt;/div&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;div id=&apos;task_deserialization_time&apos; class=&apos;task-deserialization-time-checkbox-div&apos;&amp;gt;&amp;lt;input type=&apos;checkbox&apos; class=&apos;toggle-vis&apos; id=&apos;box-12&apos; data-column=&apos;12&apos;&amp;gt; Task Deserialization Time&amp;lt;/div&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;div id=&apos;shuffle_read_blocked_time&apos; class=&apos;shuffle-read-blocked-time-checkbox-div&apos;&amp;gt;&amp;lt;input type=&apos;checkbox&apos; class=&apos;toggle-vis&apos; id=&apos;box-13&apos; data-column=&apos;13&apos;&amp;gt; Shuffle Read Blocked Time&amp;lt;/div&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;div id=&apos;shuffle_remote_reads&apos; class=&apos;shuffle-remote-reads-checkbox-div&apos;&amp;gt;&amp;lt;input type=&apos;checkbox&apos; class=&apos;toggle-vis&apos; id=&apos;box-14&apos; data-column=&apos;14&apos;&amp;gt; Shuffle Remote Reads&amp;lt;/div&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;div id=&apos;result_serialization_time&apos; class=&apos;result-serialization-time-checkbox-div&apos;&amp;gt;&amp;lt;input type=&apos;checkbox&apos; class=&apos;toggle-vis&apos; id=&apos;box-15&apos; data-column=&apos;15&apos;&amp;gt; Result Serialization Time&amp;lt;/div&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;div id=&apos;getting_result_time&apos; class=&apos;getting-result-time-checkbox-div&apos;&amp;gt;&amp;lt;input type=&apos;checkbox&apos; class=&apos;toggle-vis&apos; id=&apos;box-16&apos; data-column=&apos;16&apos;&amp;gt; Getting Result Time&amp;lt;/div&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;div id=&apos;peak_execution_memory&apos; class=&apos;peak-execution-memory-checkbox-div&apos;&amp;gt;&amp;lt;input type=&apos;checkbox&apos; class=&apos;toggle-vis&apos; id=&apos;box-17&apos; data-column=&apos;17&apos;&amp;gt; Peak Execution Memory&amp;lt;/div&amp;gt;&quot; +&lt;br/&gt;
+        &quot;&amp;lt;/div&amp;gt;&quot;);&lt;br/&gt;
+&lt;br/&gt;
+    $(&apos;#scheduler_delay&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)&lt;br/&gt;
+        .attr(&quot;data-placement&quot;, &quot;right&quot;)&lt;br/&gt;
+        .attr(&quot;title&quot;, &quot;Scheduler delay includes time to ship the task from the scheduler to the executor, and time to send &quot; +&lt;br/&gt;
+            &quot;the task result from the executor to the scheduler. If scheduler delay is large, consider decreasing the size of tasks or decreasing the size of task results.&quot;);&lt;br/&gt;
+    $(&apos;#task_deserialization_time&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)&lt;br/&gt;
+        .attr(&quot;data-placement&quot;, &quot;right&quot;)&lt;br/&gt;
+        .attr(&quot;title&quot;, &quot;Time spent deserializing the task closure on the executor, including the time to read the broadcasted task.&quot;);&lt;br/&gt;
+    $(&apos;#shuffle_read_blocked_time&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)&lt;br/&gt;
+        .attr(&quot;data-placement&quot;, &quot;right&quot;)&lt;br/&gt;
+        .attr(&quot;title&quot;, &quot;Time that the task spent blocked waiting for shuffle data to be read from remote machines.&quot;);&lt;br/&gt;
+    $(&apos;#shuffle_remote_reads&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)&lt;br/&gt;
+        .attr(&quot;data-placement&quot;, &quot;right&quot;)&lt;br/&gt;
+        .attr(&quot;title&quot;, &quot;Total shuffle bytes read from remote executors. This is a subset of the shuffle read bytes; the remaining shuffle data is read locally. &quot;);&lt;br/&gt;
+    $(&apos;#result_serialization_time&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)&lt;br/&gt;
+            .attr(&quot;data-placement&quot;, &quot;right&quot;)&lt;br/&gt;
+            .attr(&quot;title&quot;, &quot;Time spent serializing the task result on the executor before sending it back to the driver.&quot;);&lt;br/&gt;
+    $(&apos;#getting_result_time&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)&lt;br/&gt;
+            .attr(&quot;data-placement&quot;, &quot;right&quot;)&lt;br/&gt;
+            .attr(&quot;title&quot;, &quot;Time that the driver spends fetching task results from workers. If this is large, consider decreasing the amount of data returned from each task.&quot;);&lt;br/&gt;
+    $(&apos;#peak_execution_memory&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)&lt;br/&gt;
+            .attr(&quot;data-placement&quot;, &quot;right&quot;)&lt;br/&gt;
+            .attr(&quot;title&quot;, &quot;Execution memory refers to the memory used by internal data structures created during &quot; +&lt;br/&gt;
+                &quot;shuffles, aggregations and joins when Tungsten is enabled. The value of this accumulator &quot; +&lt;br/&gt;
+                &quot;should be approximately the sum of the peak sizes across all such data structures created &quot; +&lt;br/&gt;
+                &quot;in this task. For SQL jobs, this only tracks all unsafe operators, broadcast joins, and &quot; +&lt;br/&gt;
+                &quot;external sort.&quot;);&lt;br/&gt;
+    $(&apos;&lt;span class=&quot;error&quot;&gt;&amp;#91;data-toggle=&amp;quot;tooltip&amp;quot;&amp;#93;&lt;/span&gt;&apos;).tooltip();&lt;br/&gt;
+    tasksSummary = $(&quot;#parent-container&quot;);&lt;br/&gt;
+    getStandAloneAppId(function (appId) {&lt;br/&gt;
+&lt;br/&gt;
+        var endPoint = stageEndPoint(appId);&lt;br/&gt;
+        var stageAttemptId = getStageAttemptId();&lt;br/&gt;
+        $.getJSON(endPoint + &quot;/&quot; + stageAttemptId, function(response, status, jqXHR) {&lt;br/&gt;
+&lt;br/&gt;
+            var responseBody = response;&lt;br/&gt;
+            var dataToShow = {};&lt;br/&gt;
+            dataToShow.showInputData = responseBody.inputBytes &amp;gt; 0;&lt;br/&gt;
+            dataToShow.showOutputData = responseBody.outputBytes &amp;gt; 0;&lt;br/&gt;
+            dataToShow.showShuffleReadData = responseBody.shuffleReadBytes &amp;gt; 0;&lt;br/&gt;
+            dataToShow.showShuffleWriteData = responseBody.shuffleWriteBytes &amp;gt; 0;&lt;br/&gt;
+            dataToShow.showBytesSpilledData =&lt;br/&gt;
+                (responseBody.diskBytesSpilled &amp;gt; 0 || responseBody.memoryBytesSpilled &amp;gt; 0);&lt;br/&gt;
+&lt;br/&gt;
+            if (!dataToShow.showShuffleReadData) &lt;/p&gt;
{
+                $(&apos;#shuffle_read_blocked_time&apos;).remove();
+                $(&apos;#shuffle_remote_reads&apos;).remove();
+                optionalColumns.splice(2, 2);
+            }
&lt;p&gt;+&lt;br/&gt;
+            // prepare data for executor summary table&lt;br/&gt;
+            stageExecutorSummaryInfoKeys = Object.keys(responseBody.executorSummary);&lt;br/&gt;
+            $.getJSON(createRESTEndPointForExecutorsPage(appId),&lt;br/&gt;
+              function(executorSummaryResponse, status, jqXHR) {&lt;br/&gt;
+                var executorDetailsMap = {};&lt;br/&gt;
+                executorSummaryResponse.forEach(function (executorDetail) &lt;/p&gt;
{
+                    executorDetailsMap[executorDetail.id] = executorDetail;
+                }
&lt;p&gt;);&lt;br/&gt;
+&lt;br/&gt;
+                var executorSummaryTable = [];&lt;br/&gt;
+                stageExecutorSummaryInfoKeys.forEach(function (columnKeyIndex) {&lt;br/&gt;
+                    var executorSummary = responseBody.executorSummary&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKeyIndex&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+                    var executorDetail = executorDetailsMap&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKeyIndex.toString()&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+                    executorSummary.id = columnKeyIndex;&lt;br/&gt;
+                    executorSummary.executorLogs = {};&lt;br/&gt;
+                    executorSummary.hostPort = &quot;CANNOT FIND ADDRESS&quot;;&lt;br/&gt;
+&lt;br/&gt;
+                    if (executorDetail) {&lt;br/&gt;
+                        if (executorDetail&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;executorLogs&amp;quot;&amp;#93;&lt;/span&gt;) &lt;/p&gt;
{
+                            responseBody.executorSummary[columnKeyIndex].executorLogs =
+                                executorDetail[&quot;executorLogs&quot;];
+                            }
&lt;p&gt;+                        if (executorDetail&lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;hostPort&amp;quot;&amp;#93;&lt;/span&gt;) &lt;/p&gt;
{
+                            responseBody.executorSummary[columnKeyIndex].hostPort =
+                                executorDetail[&quot;hostPort&quot;];
+                        }
&lt;p&gt;+                    }&lt;br/&gt;
+                    executorSummaryTable.push(responseBody.executorSummary&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKeyIndex&amp;#93;&lt;/span&gt;);&lt;br/&gt;
+                });&lt;br/&gt;
+                // building task aggregated metrics by executor table&lt;br/&gt;
+                var executorSummaryConf = {&lt;br/&gt;
+                    &quot;data&quot;: executorSummaryTable,&lt;br/&gt;
+                    &quot;columns&quot;: [&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;id&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;executorLogs&quot;, render: formatLogsCells}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;hostPort&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) &lt;/p&gt;
{
+                                return type === &apos;display&apos; ? formatDuration(row.taskTime) : row.taskTime;
+                            }
&lt;p&gt;+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) &lt;/p&gt;
{
+                                var totaltasks = row.succeededTasks + row.failedTasks + row.killedTasks;
+                                return type === &apos;display&apos; ? totaltasks : totaltasks.toString();
+                            }
&lt;p&gt;+                        },&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;failedTasks&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;killedTasks&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;succeededTasks&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;isBlacklistedForStage&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) &lt;/p&gt;
{
+                                return row.inputRecords != 0 ? formatBytes(row.inputBytes, type) + &quot; / &quot; + row.inputRecords : &quot;&quot;;
+                            }
&lt;p&gt;+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) &lt;/p&gt;
{
+                                return row.outputRecords != 0 ? formatBytes(row.outputBytes, type) + &quot; / &quot; + row.outputRecords : &quot;&quot;;
+                            }
&lt;p&gt;+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) &lt;/p&gt;
{
+                                return row.shuffleReadRecords != 0 ? formatBytes(row.shuffleRead, type) + &quot; / &quot; + row.shuffleReadRecords : &quot;&quot;;
+                            }
&lt;p&gt;+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) &lt;/p&gt;
{
+                                return row.shuffleWriteRecords != 0 ? formatBytes(row.shuffleWrite, type) + &quot; / &quot; + row.shuffleWriteRecords : &quot;&quot;;
+                            }
&lt;p&gt;+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) &lt;/p&gt;
{
+                                return typeof row.memoryBytesSpilled != &apos;undefined&apos; ? formatBytes(row.memoryBytesSpilled, type) : &quot;&quot;;
+                            }
&lt;p&gt;+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) &lt;/p&gt;
{
+                                return typeof row.diskBytesSpilled != &apos;undefined&apos; ? formatBytes(row.diskBytesSpilled, type) : &quot;&quot;;
+                            }
&lt;p&gt;+                        }&lt;br/&gt;
+                    ],&lt;br/&gt;
+                    &quot;order&quot;: [&lt;span class=&quot;error&quot;&gt;&amp;#91;0, &amp;quot;asc&amp;quot;&amp;#93;&lt;/span&gt;],&lt;br/&gt;
+                    &quot;bAutoWidth&quot;: false&lt;br/&gt;
+                }&lt;br/&gt;
+                var executorSummaryTableSelector =&lt;br/&gt;
+                    $(&quot;#summary-executor-table&quot;).DataTable(executorSummaryConf);&lt;br/&gt;
+                $(&apos;#parent-container &lt;span class=&quot;error&quot;&gt;&amp;#91;data-toggle=&amp;quot;tooltip&amp;quot;&amp;#93;&lt;/span&gt;&apos;).tooltip();&lt;br/&gt;
+&lt;br/&gt;
+                executorSummaryTableSelector.column(9).visible(dataToShow.showInputData);&lt;br/&gt;
+                if (dataToShow.showInputData) &lt;/p&gt;
{
+                    $(&apos;#executor-summary-input&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)
+                        .attr(&quot;data-placement&quot;, &quot;top&quot;)
+                        .attr(&quot;title&quot;, &quot;Bytes and records read from Hadoop or from Spark storage.&quot;);
+                    $(&apos;#executor-summary-input&apos;).tooltip(true);
+                }
&lt;p&gt;+                executorSummaryTableSelector.column(10).visible(dataToShow.showOutputData);&lt;br/&gt;
+                if (dataToShow.showOutputData) &lt;/p&gt;
{
+                    $(&apos;#executor-summary-output&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)
+                        .attr(&quot;data-placement&quot;, &quot;top&quot;)
+                        .attr(&quot;title&quot;, &quot;Bytes and records written to Hadoop.&quot;);
+                    $(&apos;#executor-summary-output&apos;).tooltip(true);
+                }
&lt;p&gt;+                executorSummaryTableSelector.column(11).visible(dataToShow.showShuffleReadData);&lt;br/&gt;
+                if (dataToShow.showShuffleReadData) &lt;/p&gt;
{
+                    $(&apos;#executor-summary-shuffle-read&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)
+                        .attr(&quot;data-placement&quot;, &quot;top&quot;)
+                        .attr(&quot;title&quot;, &quot;Total shuffle bytes and records read (includes both data read locally and data read from remote executors).&quot;);
+                    $(&apos;#executor-summary-shuffle-read&apos;).tooltip(true);
+                }
&lt;p&gt;+                executorSummaryTableSelector.column(12).visible(dataToShow.showShuffleWriteData);&lt;br/&gt;
+                if (dataToShow.showShuffleWriteData) &lt;/p&gt;
{
+                    $(&apos;#executor-summary-shuffle-write&apos;).attr(&quot;data-toggle&quot;, &quot;tooltip&quot;)
+                        .attr(&quot;data-placement&quot;, &quot;top&quot;)
+                        .attr(&quot;title&quot;, &quot;Bytes and records written to disk in order to be read by a shuffle in a future stage.&quot;);
+                    $(&apos;#executor-summary-shuffle-write&apos;).tooltip(true);
+                }
&lt;p&gt;+                executorSummaryTableSelector.column(13).visible(dataToShow.showBytesSpilledData);&lt;br/&gt;
+                executorSummaryTableSelector.column(14).visible(dataToShow.showBytesSpilledData);&lt;br/&gt;
+            });&lt;br/&gt;
+&lt;br/&gt;
+            // prepare data for accumulatorUpdates&lt;br/&gt;
+            var accumulatorTable = responseBody.accumulatorUpdates.filter(accumUpdate =&amp;gt;&lt;br/&gt;
+                !(accumUpdate.name).toString().includes(&quot;internal.&quot;));&lt;br/&gt;
+&lt;br/&gt;
+            // rendering the UI page&lt;br/&gt;
+            var data = &lt;/p&gt;
{&quot;executors&quot;: response}
&lt;p&gt;;&lt;br/&gt;
+            $.get(createTemplateURI(appId, &quot;stagespage&quot;), function(template) {&lt;br/&gt;
+                tasksSummary.append(Mustache.render($(template).filter(&quot;#stages-summary-template&quot;).html(), data));&lt;br/&gt;
+&lt;br/&gt;
+                $(&quot;#additionalMetrics&quot;).click(function(){&lt;br/&gt;
+                    $(&quot;#arrowtoggle1&quot;).toggleClass(&quot;arrow-open arrow-closed&quot;);&lt;br/&gt;
+                    $(&quot;#toggle-metrics&quot;).toggle();&lt;br/&gt;
+                    if (window.localStorage) &lt;/p&gt;
{
+                        window.localStorage.setItem(&quot;arrowtoggle1class&quot;, $(&quot;#arrowtoggle1&quot;).attr(&apos;class&apos;));
+                    }
&lt;p&gt;+                });&lt;br/&gt;
+&lt;br/&gt;
+                $(&quot;#aggregatedMetrics&quot;).click(function(){&lt;br/&gt;
+                    $(&quot;#arrowtoggle2&quot;).toggleClass(&quot;arrow-open arrow-closed&quot;);&lt;br/&gt;
+                    $(&quot;#toggle-aggregatedMetrics&quot;).toggle();&lt;br/&gt;
+                    if (window.localStorage) &lt;/p&gt;
{
+                        window.localStorage.setItem(&quot;arrowtoggle2class&quot;, $(&quot;#arrowtoggle2&quot;).attr(&apos;class&apos;));
+                    }
&lt;p&gt;+                });&lt;br/&gt;
+&lt;br/&gt;
+                var quantiles = &quot;0,0.25,0.5,0.75,1.0&quot;;&lt;br/&gt;
+                $.getJSON(endPoint + &quot;/&quot; + stageAttemptId + &quot;/taskSummary?quantiles=&quot; + quantiles,&lt;br/&gt;
+                  function(taskMetricsResponse, status, jqXHR) {&lt;br/&gt;
+                    var taskMetricKeys = Object.keys(taskMetricsResponse);&lt;br/&gt;
+                    taskMetricKeys.forEach(function (columnKey) {&lt;br/&gt;
+                        switch(columnKey) {&lt;br/&gt;
+                            case &quot;shuffleReadMetrics&quot;:&lt;br/&gt;
+                                var row1 = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 3);&lt;br/&gt;
+                                var row2 = createRowMetadataForColumn(&lt;br/&gt;
+                                    &quot;shuffleReadBlockedTime&quot;, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 13);&lt;br/&gt;
+                                var row3 = createRowMetadataForColumn(&lt;br/&gt;
+                                    &quot;shuffleRemoteReads&quot;, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 14);&lt;br/&gt;
+                                if (dataToShow.showShuffleReadData) &lt;/p&gt;
{
+                                    taskSummaryMetricsTableArray.push(row1);
+                                    taskSummaryMetricsTableArray.push(row2);
+                                    taskSummaryMetricsTableArray.push(row3);
+                                }
&lt;p&gt;+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            case &quot;schedulerDelay&quot;:&lt;br/&gt;
+                                var row = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 11);&lt;br/&gt;
+                                taskSummaryMetricsTableArray.push(row);&lt;br/&gt;
+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            case &quot;executorDeserializeTime&quot;:&lt;br/&gt;
+                                var row = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 12);&lt;br/&gt;
+                                taskSummaryMetricsTableArray.push(row);&lt;br/&gt;
+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            case &quot;resultSerializationTime&quot;:&lt;br/&gt;
+                                var row = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 15);&lt;br/&gt;
+                                taskSummaryMetricsTableArray.push(row);&lt;br/&gt;
+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            case &quot;gettingResultTime&quot;:&lt;br/&gt;
+                                var row = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 16);&lt;br/&gt;
+                                taskSummaryMetricsTableArray.push(row);&lt;br/&gt;
+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            case &quot;peakExecutionMemory&quot;:&lt;br/&gt;
+                                var row = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 17);&lt;br/&gt;
+                                taskSummaryMetricsTableArray.push(row);&lt;br/&gt;
+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            case &quot;inputMetrics&quot;:&lt;br/&gt;
+                                var row = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 1);&lt;br/&gt;
+                                if (dataToShow.showInputData) &lt;/p&gt;
{
+                                    taskSummaryMetricsTableArray.push(row);
+                                }&lt;br/&gt;
+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            case &quot;outputMetrics&quot;:&lt;br/&gt;
+                                var row = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 2);&lt;br/&gt;
+                                if (dataToShow.showOutputData) {+                                    taskSummaryMetricsTableArray.push(row);+                                }
&lt;p&gt;+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            case &quot;shuffleWriteMetrics&quot;:&lt;br/&gt;
+                                var row = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 4);&lt;br/&gt;
+                                if (dataToShow.showShuffleWriteData) &lt;/p&gt;
{
+                                    taskSummaryMetricsTableArray.push(row);
+                                }&lt;br/&gt;
+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            case &quot;diskBytesSpilled&quot;:&lt;br/&gt;
+                                var row = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 5);&lt;br/&gt;
+                                if (dataToShow.showBytesSpilledData) {+                                    taskSummaryMetricsTableArray.push(row);+                                }
&lt;p&gt;+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            case &quot;memoryBytesSpilled&quot;:&lt;br/&gt;
+                                var row = createRowMetadataForColumn(&lt;br/&gt;
+                                    columnKey, taskMetricsResponse&lt;span class=&quot;error&quot;&gt;&amp;#91;columnKey&amp;#93;&lt;/span&gt;, 6);&lt;br/&gt;
+                                if (dataToShow.showBytesSpilledData) &lt;/p&gt;
{
+                                    taskSummaryMetricsTableArray.push(row);
+                                }
&lt;p&gt;+                                break;&lt;br/&gt;
+&lt;br/&gt;
+                            default:&lt;br/&gt;
+                                if (getColumnNameForTaskMetricSummary(columnKey) != &quot;NA&quot;) &lt;/p&gt;
{
+                                    var row = createRowMetadataForColumn(
+                                        columnKey, taskMetricsResponse[columnKey], 0);
+                                    taskSummaryMetricsTableArray.push(row);
+                                }
&lt;p&gt;+                                break;&lt;br/&gt;
+                        }&lt;br/&gt;
+                    });&lt;br/&gt;
+                    var taskSummaryMetricsTableFilteredArray =&lt;br/&gt;
+                        taskSummaryMetricsTableArray.filter(row =&amp;gt; row.checkboxId &amp;lt; 11);&lt;br/&gt;
+                    taskSummaryMetricsTableCurrentStateArray = taskSummaryMetricsTableFilteredArray.slice();&lt;br/&gt;
+                    reselectCheckboxesBasedOnTaskTableState();&lt;br/&gt;
+                });&lt;br/&gt;
+&lt;br/&gt;
+                // building accumulator update table&lt;br/&gt;
+                var accumulatorConf = {&lt;br/&gt;
+                    &quot;data&quot;: accumulatorTable,&lt;br/&gt;
+                    &quot;columns&quot;: [&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;id&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;name&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;value&quot;}
&lt;p&gt;+                    ],&lt;br/&gt;
+                    &quot;paging&quot;: false,&lt;br/&gt;
+                    &quot;searching&quot;: false,&lt;br/&gt;
+                    &quot;order&quot;: [&lt;span class=&quot;error&quot;&gt;&amp;#91;0, &amp;quot;asc&amp;quot;&amp;#93;&lt;/span&gt;],&lt;br/&gt;
+                    &quot;bAutoWidth&quot;: false&lt;br/&gt;
+                }&lt;br/&gt;
+                $(&quot;#accumulator-table&quot;).DataTable(accumulatorConf);&lt;br/&gt;
+&lt;br/&gt;
+                // building tasks table that uses server side functionality&lt;br/&gt;
+                var totalTasksToShow = responseBody.numCompleteTasks + responseBody.numActiveTasks;&lt;br/&gt;
+                var taskTable = &quot;#active-tasks-table&quot;;&lt;br/&gt;
+                var taskConf = {&lt;br/&gt;
+                    &quot;serverSide&quot;: true,&lt;br/&gt;
+                    &quot;paging&quot;: true,&lt;br/&gt;
+                    &quot;info&quot;: true,&lt;br/&gt;
+                    &quot;processing&quot;: true,&lt;br/&gt;
+                    &quot;lengthMenu&quot;: [&lt;span class=&quot;error&quot;&gt;&amp;#91;20, 40, 60, 100, totalTasksToShow&amp;#93;&lt;/span&gt;, &lt;span class=&quot;error&quot;&gt;&amp;#91;20, 40, 60, 100, &amp;quot;All&amp;quot;&amp;#93;&lt;/span&gt;],&lt;br/&gt;
+                    &quot;orderMulti&quot;: false,&lt;br/&gt;
+                    &quot;bAutoWidth&quot;: false,&lt;br/&gt;
+                    &quot;ajax&quot;: {&lt;br/&gt;
+                        &quot;url&quot;: endPoint + &quot;/&quot; + stageAttemptId + &quot;/taskTable&quot;,&lt;br/&gt;
+                        &quot;data&quot;: function (data) {&lt;br/&gt;
+                            var columnIndexToSort = 0;&lt;br/&gt;
+                            var columnNameToSort = &quot;Index&quot;;&lt;br/&gt;
+                            if (data.order&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;.column &amp;amp;&amp;amp; data.order&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;.column != &quot;&quot;) &lt;/p&gt;
{
+                                columnIndexToSort = parseInt(data.order[0].column);
+                                columnNameToSort = data.columns[columnIndexToSort].name;
+                            }
&lt;p&gt;+                            delete data.columns;&lt;br/&gt;
+                            data.numTasks = totalTasksToShow;&lt;br/&gt;
+                            data.columnIndexToSort = columnIndexToSort;&lt;br/&gt;
+                            data.columnNameToSort = columnNameToSort;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        &quot;dataSrc&quot;: function (jsons) &lt;/p&gt;
{
+                            var jsonStr = JSON.stringify(jsons);
+                            var tasksToShow = JSON.parse(jsonStr);
+                            return tasksToShow.aaData;
+                        }
&lt;p&gt;,&lt;br/&gt;
+                        &quot;error&quot;: function (jqXHR, textStatus, errorThrown) &lt;/p&gt;
{
+                            alert(&quot;Unable to connect to the server. Looks like the Spark &quot; +
+                              &quot;application must have ended. Please Switch to the history UI.&quot;);
+                            $(&quot;#active-tasks-table_processing&quot;).css(&quot;display&quot;,&quot;none&quot;);
+                        }
&lt;p&gt;+                    },&lt;br/&gt;
+                    &quot;columns&quot;: [&lt;br/&gt;
+                        {data: function (row, type) &lt;/p&gt;
{
+                            return type !== &apos;display&apos; ? (isNaN(row.index) ? 0 : row.index ) : row.index;
+                            }
&lt;p&gt;,&lt;br/&gt;
+                            name: &quot;Index&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;taskId&quot;, name: &quot;ID&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;attempt&quot;, name: &quot;Attempt&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;status&quot;, name: &quot;Status&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;taskLocality&quot;, name: &quot;Locality Level&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;executorId&quot;, name: &quot;Executor ID&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;host&quot;, name: &quot;Host&quot;}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;executorLogs&quot;, name: &quot;Logs&quot;, render: formatLogsCells}
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{data : &quot;launchTime&quot;, name: &quot;Launch Time&quot;, render: formatDate}
&lt;p&gt;,&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.duration) &lt;/p&gt;
{
+                                    return type === &apos;display&apos; ? formatDuration(row.duration) : row.duration;
+                                }
&lt;p&gt; else &lt;/p&gt;
{
+                                    return &quot;&quot;;
+                                }&lt;br/&gt;
+                            },&lt;br/&gt;
+                            name: &quot;Duration&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.jvmGcTime) {
+                                    return type === &apos;display&apos; ? formatDuration(row.taskMetrics.jvmGcTime) : row.taskMetrics.jvmGcTime;
+                                } else {+                                    return &quot;&quot;;+                                }
&lt;p&gt;+                            },&lt;br/&gt;
+                            name: &quot;GC Time&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.schedulerDelay) &lt;/p&gt;
{
+                                    return type === &apos;display&apos; ? formatDuration(row.schedulerDelay) : row.schedulerDelay;
+                                }
&lt;p&gt; else &lt;/p&gt;
{
+                                    return &quot;&quot;;
+                                }&lt;br/&gt;
+                            },&lt;br/&gt;
+                            name: &quot;Scheduler Delay&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.executorDeserializeTime) {
+                                    return type === &apos;display&apos; ? formatDuration(row.taskMetrics.executorDeserializeTime) : row.taskMetrics.executorDeserializeTime;
+                                } else {+                                    return &quot;&quot;;+                                }
&lt;p&gt;+                            },&lt;br/&gt;
+                            name: &quot;Task Deserialization Time&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.shuffleReadMetrics) &lt;/p&gt;
{
+                                    return type === &apos;display&apos; ? formatDuration(row.taskMetrics.shuffleReadMetrics.fetchWaitTime) : row.taskMetrics.shuffleReadMetrics.fetchWaitTime;
+                                }
&lt;p&gt; else &lt;/p&gt;
{
+                                    return &quot;&quot;;
+                                }&lt;br/&gt;
+                            },&lt;br/&gt;
+                            name: &quot;Shuffle Read Blocked Time&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.shuffleReadMetrics) {
+                                    return type === &apos;display&apos; ? formatBytes(row.taskMetrics.shuffleReadMetrics.remoteBytesRead, type) : row.taskMetrics.shuffleReadMetrics.remoteBytesRead;
+                                } else {+                                    return &quot;&quot;;+                                }
&lt;p&gt;+                            },&lt;br/&gt;
+                            name: &quot;Shuffle Remote Reads&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.resultSerializationTime) &lt;/p&gt;
{
+                                    return type === &apos;display&apos; ? formatDuration(row.taskMetrics.resultSerializationTime) : row.taskMetrics.resultSerializationTime;
+                                }
&lt;p&gt; else &lt;/p&gt;
{
+                                    return &quot;&quot;;
+                                }&lt;br/&gt;
+                            },&lt;br/&gt;
+                            name: &quot;Result Serialization Time&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.gettingResultTime) {
+                                    return type === &apos;display&apos; ? formatDuration(row.gettingResultTime) : row.gettingResultTime;
+                                } else {+                                    return &quot;&quot;;+                                }
&lt;p&gt;+                            },&lt;br/&gt;
+                            name: &quot;Getting Result Time&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.peakExecutionMemory) &lt;/p&gt;
{
+                                    return type === &apos;display&apos; ? formatBytes(row.taskMetrics.peakExecutionMemory, type) : row.taskMetrics.peakExecutionMemory;
+                                }
&lt;p&gt; else &lt;/p&gt;
{
+                                    return &quot;&quot;;
+                                }&lt;br/&gt;
+                            },&lt;br/&gt;
+                            name: &quot;Peak Execution Memory&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (accumulatorTable.length &amp;gt; 0 &amp;amp;&amp;amp; row.accumulatorUpdates.length &amp;gt; 0) {
+                                    var accIndex = row.accumulatorUpdates.length - 1;
+                                    return row.accumulatorUpdates[accIndex].name + &apos; : &apos; + row.accumulatorUpdates[accIndex].update;
+                                } else {+                                    return &quot;&quot;;+                                }
&lt;p&gt;+                            },&lt;br/&gt;
+                            name: &quot;Accumulators&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.inputMetrics &amp;amp;&amp;amp; row.taskMetrics.inputMetrics.bytesRead &amp;gt; 0) {&lt;br/&gt;
+                                    if (type === &apos;display&apos;) &lt;/p&gt;
{
+                                        return formatBytes(row.taskMetrics.inputMetrics.bytesRead, type) + &quot; / &quot; + row.taskMetrics.inputMetrics.recordsRead;
+                                    }
&lt;p&gt; else &lt;/p&gt;
{
+                                        return row.taskMetrics.inputMetrics.bytesRead + &quot; / &quot; + row.taskMetrics.inputMetrics.recordsRead;
+                                    }
&lt;p&gt;+                                } else &lt;/p&gt;
{
+                                    return &quot;&quot;;
+                                }&lt;br/&gt;
+                            },&lt;br/&gt;
+                            name: &quot;Input Size / Records&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.outputMetrics &amp;amp;&amp;amp; row.taskMetrics.outputMetrics.bytesWritten &amp;gt; 0) {&lt;br/&gt;
+                                    if (type === &apos;display&apos;) {
+                                        return formatBytes(row.taskMetrics.outputMetrics.bytesWritten, type) + &quot; / &quot; + row.taskMetrics.outputMetrics.recordsWritten;
+                                    } else {
+                                        return row.taskMetrics.outputMetrics.bytesWritten + &quot; / &quot; + row.taskMetrics.outputMetrics.recordsWritten;
+                                    }&lt;br/&gt;
+                                } else {+                                    return &quot;&quot;;+                                }
&lt;p&gt;+                            },&lt;br/&gt;
+                            name: &quot;Output Size / Records&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.shuffleWriteMetrics &amp;amp;&amp;amp; row.taskMetrics.shuffleWriteMetrics.writeTime &amp;gt; 0) &lt;/p&gt;
{
+                                    return type === &apos;display&apos; ? formatDuration(parseInt(row.taskMetrics.shuffleWriteMetrics.writeTime) / 1000000) : row.taskMetrics.shuffleWriteMetrics.writeTime;
+                                }
&lt;p&gt; else &lt;/p&gt;
{
+                                    return &quot;&quot;;
+                                }&lt;br/&gt;
+                            },&lt;br/&gt;
+                            name: &quot;Write Time&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.shuffleWriteMetrics &amp;amp;&amp;amp; row.taskMetrics.shuffleWriteMetrics.bytesWritten &amp;gt; 0) {&lt;br/&gt;
+                                    if (type === &apos;display&apos;) {
+                                        return formatBytes(row.taskMetrics.shuffleWriteMetrics.bytesWritten, type) + &quot; / &quot; + row.taskMetrics.shuffleWriteMetrics.recordsWritten;
+                                    } else {
+                                        return row.taskMetrics.shuffleWriteMetrics.bytesWritten + &quot; / &quot; + row.taskMetrics.shuffleWriteMetrics.recordsWritten;
+                                    }&lt;br/&gt;
+                                } else {+                                    return &quot;&quot;;+                                }
&lt;p&gt;+                            },&lt;br/&gt;
+                            name: &quot;Shuffle Write Size / Records&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.shuffleReadMetrics &amp;amp;&amp;amp; row.taskMetrics.shuffleReadMetrics.localBytesRead &amp;gt; 0) {&lt;br/&gt;
+                                    var totalBytesRead = parseInt(row.taskMetrics.shuffleReadMetrics.localBytesRead) + parseInt(row.taskMetrics.shuffleReadMetrics.remoteBytesRead);&lt;br/&gt;
+                                    if (type === &apos;display&apos;) &lt;/p&gt;
{
+                                        return formatBytes(totalBytesRead, type) + &quot; / &quot; + row.taskMetrics.shuffleReadMetrics.recordsRead;
+                                    }
&lt;p&gt; else &lt;/p&gt;
{
+                                        return totalBytesRead + &quot; / &quot; + row.taskMetrics.shuffleReadMetrics.recordsRead;
+                                    }
&lt;p&gt;+                                } else &lt;/p&gt;
{
+                                    return &quot;&quot;;
+                                }&lt;br/&gt;
+                            },&lt;br/&gt;
+                            name: &quot;Shuffle Read Size / Records&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.memoryBytesSpilled &amp;amp;&amp;amp; row.taskMetrics.memoryBytesSpilled &amp;gt; 0) {
+                                    return type === &apos;display&apos; ? formatBytes(row.taskMetrics.memoryBytesSpilled, type) : row.taskMetrics.memoryBytesSpilled;
+                                } else {+                                    return &quot;&quot;;+                                }
&lt;p&gt;+                            },&lt;br/&gt;
+                            name: &quot;Shuffle Spill (Memory)&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                if (row.taskMetrics &amp;amp;&amp;amp; row.taskMetrics.diskBytesSpilled &amp;amp;&amp;amp; row.taskMetrics.diskBytesSpilled &amp;gt; 0) &lt;/p&gt;
{
+                                    return type === &apos;display&apos; ? formatBytes(row.taskMetrics.diskBytesSpilled, type) : row.taskMetrics.diskBytesSpilled;
+                                }
&lt;p&gt; else &lt;/p&gt;
{
+                                    return &quot;&quot;;
+                                }&lt;br/&gt;
+                            },&lt;br/&gt;
+                            name: &quot;Shuffle Spill (Disk)&quot;&lt;br/&gt;
+                        },&lt;br/&gt;
+                        {&lt;br/&gt;
+                            data : function (row, type) {&lt;br/&gt;
+                                var msg = row.errorMessage;&lt;br/&gt;
+                                if (typeof msg === &apos;undefined&apos;) {+                                    return &quot;&quot;;+                                }
&lt;p&gt; else &lt;/p&gt;
{
+                                    var formHead = msg.substring(0, msg.indexOf(&quot;at&quot;));
+                                    var form = &quot;&amp;lt;span onclick=\&quot;this.parentNode.querySelector(&apos;.stacktrace-details&apos;).classList.toggle(&apos;collapsed&apos;)\&quot; class=\&quot;expand-details\&quot;&amp;gt;+details&amp;lt;/span&amp;gt;&quot;;
+                                    var formMsg = &quot;&amp;lt;div class=\&quot;stacktrace-details collapsed\&quot;&amp;gt;&amp;lt;pre&amp;gt;&quot; + row.errorMessage + &quot;&amp;lt;/pre&amp;gt;&amp;lt;/div&amp;gt;&quot;;
+                                    return formHead + form + formMsg;
+                                }
&lt;p&gt;+                            },&lt;br/&gt;
+                            name: &quot;Errors&quot;&lt;br/&gt;
+                        }&lt;br/&gt;
+                    ],&lt;br/&gt;
+                    &quot;columnDefs&quot;: [&lt;br/&gt;
+                        &lt;/p&gt;
{ &quot;visible&quot;: false, &quot;targets&quot;: 11 }
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{ &quot;visible&quot;: false, &quot;targets&quot;: 12 }
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{ &quot;visible&quot;: false, &quot;targets&quot;: 13 }
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{ &quot;visible&quot;: false, &quot;targets&quot;: 14 }
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{ &quot;visible&quot;: false, &quot;targets&quot;: 15 }
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{ &quot;visible&quot;: false, &quot;targets&quot;: 16 }
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{ &quot;visible&quot;: false, &quot;targets&quot;: 17 }
&lt;p&gt;,&lt;br/&gt;
+                        &lt;/p&gt;
{ &quot;visible&quot;: false, &quot;targets&quot;: 18 }
&lt;p&gt;+                    ],&lt;br/&gt;
+                };&lt;br/&gt;
+                taskTableSelector = $(taskTable).DataTable(taskConf);&lt;br/&gt;
+                $(&apos;#active-tasks-table_filter input&apos;).unbind();&lt;br/&gt;
+                var searchEvent;&lt;br/&gt;
+                $(&apos;#active-tasks-table_filter input&apos;).bind(&apos;keyup&apos;, function(e) {&lt;br/&gt;
+                  if (typeof searchEvent !== &apos;undefined&apos;) &lt;/p&gt;
{
+                    window.clearTimeout(searchEvent);
+                  }
&lt;p&gt;+                  var value = this.value;&lt;br/&gt;
+                  searchEvent = window.setTimeout(function()&lt;/p&gt;
{
+                    taskTableSelector.search( value ).draw();}
&lt;p&gt;, 500);&lt;br/&gt;
+                });&lt;br/&gt;
+                reselectCheckboxesBasedOnTaskTableState();&lt;br/&gt;
+&lt;br/&gt;
+                // hide or show columns dynamically event&lt;br/&gt;
+                $(&apos;input.toggle-vis&apos;).on(&apos;click&apos;, function(e){&lt;br/&gt;
+                    // Get the column&lt;br/&gt;
+                    var para = $(this).attr(&apos;data-column&apos;);&lt;br/&gt;
+                    if (para == &quot;0&quot;) {&lt;br/&gt;
+                        var column = taskTableSelector.column(optionalColumns);&lt;br/&gt;
+                        if ($(this).is(&quot;:checked&quot;)) &lt;/p&gt;
{
+                            $(&quot;.toggle-vis&quot;).prop(&apos;checked&apos;, true);
+                            column.visible(true);
+                            createDataTableForTaskSummaryMetricsTable(taskSummaryMetricsTableArray);
+                        }
&lt;p&gt; else &lt;/p&gt;
{
+                            $(&quot;.toggle-vis&quot;).prop(&apos;checked&apos;, false);
+                            column.visible(false);
+                            var taskSummaryMetricsTableFilteredArray =
+                                taskSummaryMetricsTableArray.filter(row =&amp;gt; row.checkboxId &amp;lt; 11);
+                            createDataTableForTaskSummaryMetricsTable(taskSummaryMetricsTableFilteredArray);
+                        }
&lt;p&gt;+                    } else {&lt;br/&gt;
+                        var column = taskTableSelector.column(para);&lt;br/&gt;
+                        // Toggle the visibility&lt;br/&gt;
+                        column.visible(!column.visible());&lt;br/&gt;
+                        var taskSummaryMetricsTableFilteredArray = [];&lt;br/&gt;
+                        if ($(this).is(&quot;:checked&quot;)) &lt;/p&gt;
{
+                            taskSummaryMetricsTableCurrentStateArray.push(taskSummaryMetricsTableArray.filter(row =&amp;gt; (row.checkboxId).toString() == para)[0]);
+                            taskSummaryMetricsTableFilteredArray = taskSummaryMetricsTableCurrentStateArray.slice();
+                        }
&lt;p&gt; else &lt;/p&gt;
{
+                            taskSummaryMetricsTableFilteredArray =
+                                taskSummaryMetricsTableCurrentStateArray.filter(row =&amp;gt; (row.checkboxId).toString() != para);
+                        }
&lt;p&gt;+                        createDataTableForTaskSummaryMetricsTable(taskSummaryMetricsTableFilteredArray);&lt;br/&gt;
+                    }&lt;br/&gt;
+                });&lt;br/&gt;
+&lt;br/&gt;
+                // title number and toggle list&lt;br/&gt;
+                $(&quot;#summaryMetricsTitle&quot;).html(&quot;Summary Metrics for &quot; + &quot;&amp;lt;a href=&apos;#tasksTitle&apos;&amp;gt;&quot; + responseBody.numCompleteTasks + &quot; Completed Tasks&quot; + &quot;&amp;lt;/a&amp;gt;&quot;);&lt;br/&gt;
+                $(&quot;#tasksTitle&quot;).html(&quot;Task (&quot; + totalTasksToShow + &quot;)&quot;);&lt;br/&gt;
+&lt;br/&gt;
+                // hide or show the accumulate update table&lt;br/&gt;
+                if (accumulatorTable.length == 0) &lt;/p&gt;
{
+                    $(&quot;#accumulator-update-table&quot;).hide();
+                }
&lt;p&gt; else &lt;/p&gt;
{
+                    taskTableSelector.column(18).visible(true);
+                    $(&quot;#accumulator-update-table&quot;).show();
+                }
&lt;p&gt;+                // Showing relevant stage data depending on stage type for task table and executor&lt;br/&gt;
+                // summary table&lt;br/&gt;
+                taskTableSelector.column(19).visible(dataToShow.showInputData);&lt;br/&gt;
+                taskTableSelector.column(20).visible(dataToShow.showOutputData);&lt;br/&gt;
+                taskTableSelector.column(21).visible(dataToShow.showShuffleWriteData);&lt;br/&gt;
+                taskTableSelector.column(22).visible(dataToShow.showShuffleWriteData);&lt;br/&gt;
+                taskTableSelector.column(23).visible(dataToShow.showShuffleReadData);&lt;br/&gt;
+                taskTableSelector.column(24).visible(dataToShow.showBytesSpilledData);&lt;br/&gt;
+                taskTableSelector.column(25).visible(dataToShow.showBytesSpilledData);&lt;br/&gt;
+&lt;br/&gt;
+                if (window.localStorage) {&lt;br/&gt;
+                    if (window.localStorage.getItem(&quot;arrowtoggle1class&quot;) !== null &amp;amp;&amp;amp;&lt;br/&gt;
+                        window.localStorage.getItem(&quot;arrowtoggle1class&quot;).includes(&quot;arrow-open&quot;)) &lt;/p&gt;
{
+                        $(&quot;#arrowtoggle1&quot;).toggleClass(&quot;arrow-open arrow-closed&quot;);
+                        $(&quot;#toggle-metrics&quot;).toggle();
+                    }
&lt;p&gt;+                    if (window.localStorage.getItem(&quot;arrowtoggle2class&quot;) !== null &amp;amp;&amp;amp;&lt;br/&gt;
+                        window.localStorage.getItem(&quot;arrowtoggle2class&quot;).includes(&quot;arrow-open&quot;)) &lt;/p&gt;
{
+                        $(&quot;#arrowtoggle2&quot;).toggleClass(&quot;arrow-open arrow-closed&quot;);
+                        $(&quot;#toggle-aggregatedMetrics&quot;).toggle();
+                    }
&lt;p&gt;+                }&lt;br/&gt;
+            });&lt;br/&gt;
+        });&lt;br/&gt;
+    });&lt;br/&gt;
+});&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html b/core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..6f950c61b2d63&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/resources/org/apache/spark/ui/static/stagespage-template.html&lt;br/&gt;
@@ -0,0 +1,124 @@&lt;br/&gt;
+&amp;lt;!--&lt;br/&gt;
+Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+this work for additional information regarding copyright ownership.&lt;br/&gt;
+The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+(the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+the License.  You may obtain a copy of the License at&lt;br/&gt;
+&lt;br/&gt;
+&lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+&lt;br/&gt;
+Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+See the License for the specific language governing permissions and&lt;br/&gt;
+limitations under the License.&lt;br/&gt;
+--&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+&amp;lt;script id=&quot;stages-summary-template&quot; type=&quot;text/html&quot;&amp;gt;&lt;br/&gt;
+    &amp;lt;h4 id=&quot;summaryMetricsTitle&quot; class=&quot;title-table&quot;&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
+    &amp;lt;div class=&quot;container-fluid&quot;&amp;gt;&lt;br/&gt;
+        &amp;lt;div class=&quot;container-fluid&quot;&amp;gt;&lt;br/&gt;
+            &amp;lt;table id=&quot;summary-metrics-table&quot; class=&quot;table table-striped compact table-dataTable cell-border&quot;&amp;gt;&lt;br/&gt;
+                &amp;lt;thead&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Metric&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Min&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;25th percentile&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Median&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;75th percentile&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Max&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;/thead&amp;gt;&lt;br/&gt;
+                &amp;lt;tbody&amp;gt;&lt;br/&gt;
+                &amp;lt;/tbody&amp;gt;&lt;br/&gt;
+            &amp;lt;/table&amp;gt;&lt;br/&gt;
+        &amp;lt;/div&amp;gt;&lt;br/&gt;
+    &amp;lt;/div&amp;gt;&lt;br/&gt;
+    &amp;lt;a id=&quot;aggregatedMetrics&quot;&amp;gt;&lt;br/&gt;
+        &amp;lt;span class=&quot;expand-input-rate-arrow arrow-closed&quot; id=&quot;arrowtoggle2&quot;&amp;gt;&amp;lt;/span&amp;gt;&lt;br/&gt;
+        &amp;lt;h4 class=&quot;title-table&quot;&amp;gt;Aggregated Metrics by Executor&amp;lt;/h4&amp;gt;&lt;br/&gt;
+    &amp;lt;/a&amp;gt;&lt;br/&gt;
+    &amp;lt;br&amp;gt;&lt;br/&gt;
+    &amp;lt;div class=&quot;container-fluid&quot; id=&quot;toggle-aggregatedMetrics&quot; hidden&amp;gt;&lt;br/&gt;
+        &amp;lt;div class=&quot;container-fluid&quot;&amp;gt;&lt;br/&gt;
+            &amp;lt;table id=&quot;summary-executor-table&quot; class=&quot;table table-striped compact table-dataTable cell-border&quot;&amp;gt;&lt;br/&gt;
+                &amp;lt;thead&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Executor ID&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Logs&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Address&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Task Time&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Total Tasks&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Failed Tasks&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Killed Tasks&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Succeeded Tasks&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;&lt;br/&gt;
+          &amp;lt;span data-toggle=&quot;tooltip&quot; data-placement=&quot;top&quot;&lt;br/&gt;
+                title=&quot;Shows if this executor has been blacklisted by the scheduler due to task failures.&quot;&amp;gt;&lt;br/&gt;
+            Blacklisted&amp;lt;/span&amp;gt;&lt;br/&gt;
+                &amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;&amp;lt;span id=&quot;executor-summary-input&quot;&amp;gt;Input Size / Records&amp;lt;/span&amp;gt;&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;&amp;lt;span id=&quot;executor-summary-output&quot;&amp;gt;Output Size / Records&amp;lt;/span&amp;gt;&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;&amp;lt;span id=&quot;executor-summary-shuffle-read&quot;&amp;gt;Shuffle Read Size / Records&amp;lt;/span&amp;gt;&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;&amp;lt;span id=&quot;executor-summary-shuffle-write&quot;&amp;gt;Shuffle Write Size / Records&amp;lt;/span&amp;gt;&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Shuffle Spill (Memory) &amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Shuffle Spill (Disk) &amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;/thead&amp;gt;&lt;br/&gt;
+                &amp;lt;tbody&amp;gt;&lt;br/&gt;
+                &amp;lt;/tbody&amp;gt;&lt;br/&gt;
+            &amp;lt;/table&amp;gt;&lt;br/&gt;
+        &amp;lt;/div&amp;gt;&lt;br/&gt;
+    &amp;lt;/div&amp;gt;&lt;br/&gt;
+    &amp;lt;div class=&quot;container-fluid&quot; id=&quot;accumulator-update-table&quot;&amp;gt;&lt;br/&gt;
+        &amp;lt;h4 class=&quot;title-table&quot;&amp;gt;Accumulators&amp;lt;/h4&amp;gt;&lt;br/&gt;
+        &amp;lt;div class=&quot;container-fluid&quot;&amp;gt;&lt;br/&gt;
+            &amp;lt;table id=&quot;accumulator-table&quot; class=&quot;table table-striped compact table-dataTable cell-border&quot;&amp;gt;&lt;br/&gt;
+                &amp;lt;thead&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;ID&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Name&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;th&amp;gt;Value&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;/thead&amp;gt;&lt;br/&gt;
+                &amp;lt;tbody&amp;gt;&lt;br/&gt;
+                &amp;lt;/tbody&amp;gt;&lt;br/&gt;
+            &amp;lt;/table&amp;gt;&lt;br/&gt;
+        &amp;lt;/div&amp;gt;&lt;br/&gt;
+    &amp;lt;/div&amp;gt;&lt;br/&gt;
+    &amp;lt;h4 id=&quot;tasksTitle&quot; class=&quot;title-table&quot;&amp;gt;&amp;lt;/h4&amp;gt;&lt;br/&gt;
+    &amp;lt;div class=&quot;container-fluid&quot;&amp;gt;&lt;br/&gt;
+        &amp;lt;div class=&quot;container-fluid&quot;&amp;gt;&lt;br/&gt;
+            &amp;lt;table id=&quot;active-tasks-table&quot; class=&quot;table table-striped compact table-dataTable cell-border&quot;&amp;gt;&lt;br/&gt;
+                &amp;lt;thead&amp;gt;&lt;br/&gt;
+                &amp;lt;tr&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Index&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Task ID&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Attempt&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Status&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Locality level&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Executor ID&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Host&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Logs&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Launch Time&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Duration&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;GC Time&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Scheduler Delay&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Task Deserialization Time&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Shuffle Read Blocked Time&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Shuffle Remote Reads&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Result Serialization Time&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Getting Result Time&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Peak Execution Memory&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Accumulators&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Input Size / Records&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Output Size / Records&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Write Time&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Shuffle Write Size / Records&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Shuffle Read Size / Records&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Shuffle Spill (Memory)&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Shuffle Spill (Disk)&amp;lt;/th&amp;gt;&lt;br/&gt;
+                    &amp;lt;th&amp;gt;Errors&amp;lt;/th&amp;gt;&lt;br/&gt;
+                &amp;lt;/tr&amp;gt;&lt;br/&gt;
+                &amp;lt;/thead&amp;gt;&lt;br/&gt;
+                &amp;lt;tbody&amp;gt;&lt;br/&gt;
+                &amp;lt;/tbody&amp;gt;&lt;br/&gt;
+            &amp;lt;/table&amp;gt;&lt;br/&gt;
+        &amp;lt;/div&amp;gt;&lt;br/&gt;
+    &amp;lt;/div&amp;gt;&lt;br/&gt;
+&amp;lt;/script&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/utils.js b/core/src/main/resources/org/apache/spark/ui/static/utils.js&lt;br/&gt;
index 4f63f6413d6de..deeafad4eb5f5 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/resources/org/apache/spark/ui/static/utils.js&lt;br/&gt;
+++ b/core/src/main/resources/org/apache/spark/ui/static/utils.js&lt;br/&gt;
@@ -18,7 +18,7 @@&lt;br/&gt;
 // this function works exactly the same as UIUtils.formatDuration&lt;br/&gt;
 function formatDuration(milliseconds) {&lt;br/&gt;
     if (milliseconds &amp;lt; 100) &lt;/p&gt;
{
-        return milliseconds + &quot; ms&quot;;
+        return parseInt(milliseconds).toFixed(1) + &quot; ms&quot;;
     }
&lt;p&gt;     var seconds = milliseconds * 1.0 / 1000;&lt;br/&gt;
     if (seconds &amp;lt; 1) {&lt;br/&gt;
@@ -74,3 +74,114 @@ function getTimeZone() &lt;/p&gt;
{
     return new Date().toString().match(/\((.*)\)/)[1];
   }
&lt;p&gt; }&lt;br/&gt;
+&lt;br/&gt;
+function formatLogsCells(execLogs, type) {&lt;br/&gt;
+  if (type !== &apos;display&apos;) return Object.keys(execLogs);&lt;br/&gt;
+  if (!execLogs) return;&lt;br/&gt;
+  var result = &apos;&apos;;&lt;br/&gt;
+  $.each(execLogs, function (logName, logUrl) &lt;/p&gt;
{
+    result += &apos;&amp;lt;div&amp;gt;&amp;lt;a href=&apos; + logUrl + &apos;&amp;gt;&apos; + logName + &apos;&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt;&apos;
+  }
&lt;p&gt;);&lt;br/&gt;
+  return result;&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+function getStandAloneAppId(cb) {&lt;br/&gt;
+  var words = document.baseURI.split(&apos;/&apos;);&lt;br/&gt;
+  var ind = words.indexOf(&quot;proxy&quot;);&lt;br/&gt;
+  if (ind &amp;gt; 0) &lt;/p&gt;
{
+    var appId = words[ind + 1];
+    cb(appId);
+    return;
+  }&lt;br/&gt;
+  ind = words.indexOf(&quot;history&quot;);&lt;br/&gt;
+  if (ind &amp;gt; 0) {+    var appId = words[ind + 1];+    cb(appId);+    return;+  }
&lt;p&gt;+  // Looks like Web UI is running in standalone mode&lt;br/&gt;
+  // Let&apos;s get application-id using REST End Point&lt;br/&gt;
+  $.getJSON(location.origin + &quot;/api/v1/applications&quot;, function(response, status, jqXHR) {&lt;br/&gt;
+    if (response &amp;amp;&amp;amp; response.length &amp;gt; 0) &lt;/p&gt;
{
+      var appId = response[0].id;
+      cb(appId);
+      return;
+    }
&lt;p&gt;+  });&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+// This function is a helper function for sorting in datatable.&lt;br/&gt;
+// When the data is in duration (e.g. 12ms 2s 2min 2h )&lt;br/&gt;
+// It will convert the string into integer for correct ordering&lt;br/&gt;
+function ConvertDurationString(data) {&lt;br/&gt;
+  data = data.toString();&lt;br/&gt;
+  var units = data.replace(/&lt;span class=&quot;error&quot;&gt;&amp;#91;\d\.&amp;#93;&lt;/span&gt;/g, &apos;&apos; )&lt;br/&gt;
+                  .replace(&apos; &apos;, &apos;&apos;)&lt;br/&gt;
+                  .toLowerCase();&lt;br/&gt;
+  var multiplier = 1;&lt;br/&gt;
+&lt;br/&gt;
+  switch(units) &lt;/p&gt;
{
+    case &apos;s&apos;:
+      multiplier = 1000;
+      break;
+    case &apos;min&apos;:
+      multiplier = 600000;
+      break;
+    case &apos;h&apos;:
+      multiplier = 3600000;
+      break;
+    default:
+      break;
+  }
&lt;p&gt;+  return parseFloat(data) * multiplier;&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+function createTemplateURI(appId, templateName) {&lt;br/&gt;
+  var words = document.baseURI.split(&apos;/&apos;);&lt;br/&gt;
+  var ind = words.indexOf(&quot;proxy&quot;);&lt;br/&gt;
+  if (ind &amp;gt; 0) &lt;/p&gt;
{
+    var baseURI = words.slice(0, ind + 1).join(&apos;/&apos;) + &apos;/&apos; + appId + &apos;/static/&apos; + templateName + &apos;-template.html&apos;;
+    return baseURI;
+  }
&lt;p&gt;+  ind = words.indexOf(&quot;history&quot;);&lt;br/&gt;
+  if(ind &amp;gt; 0) &lt;/p&gt;
{
+    var baseURI = words.slice(0, ind).join(&apos;/&apos;) + &apos;/static/&apos; + templateName + &apos;-template.html&apos;;
+    return baseURI;
+  }
&lt;p&gt;+  return location.origin + &quot;/static/&quot; + templateName + &quot;-template.html&quot;;&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+function setDataTableDefaults() {&lt;br/&gt;
+  $.extend($.fn.dataTable.defaults, &lt;/p&gt;
{
+    stateSave: true,
+    lengthMenu: [[20, 40, 60, 100, -1], [20, 40, 60, 100, &quot;All&quot;]],
+    pageLength: 20
+  }
&lt;p&gt;);&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+function formatDate(date) &lt;/p&gt;
{
+  if (date &amp;lt;= 0) return &quot;-&quot;;
+  else return date.split(&quot;.&quot;)[0].replace(&quot;T&quot;, &quot; &quot;);
+}
&lt;p&gt;+&lt;br/&gt;
+function createRESTEndPointForExecutorsPage(appId) {&lt;br/&gt;
+    var words = document.baseURI.split(&apos;/&apos;);&lt;br/&gt;
+    var ind = words.indexOf(&quot;proxy&quot;);&lt;br/&gt;
+    if (ind &amp;gt; 0) &lt;/p&gt;
{
+        var appId = words[ind + 1];
+        var newBaseURI = words.slice(0, ind + 2).join(&apos;/&apos;);
+        return newBaseURI + &quot;/api/v1/applications/&quot; + appId + &quot;/allexecutors&quot;
+    }
&lt;p&gt;+    ind = words.indexOf(&quot;history&quot;);&lt;br/&gt;
+    if (ind &amp;gt; 0) {&lt;br/&gt;
+        var appId = words&lt;span class=&quot;error&quot;&gt;&amp;#91;ind + 1&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        var attemptId = words&lt;span class=&quot;error&quot;&gt;&amp;#91;ind + 2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
+        var newBaseURI = words.slice(0, ind).join(&apos;/&apos;);&lt;br/&gt;
+        if (isNaN(attemptId)) &lt;/p&gt;
{
+            return newBaseURI + &quot;/api/v1/applications/&quot; + appId + &quot;/allexecutors&quot;;
+        }
&lt;p&gt; else &lt;/p&gt;
{
+            return newBaseURI + &quot;/api/v1/applications/&quot; + appId + &quot;/&quot; + attemptId + &quot;/allexecutors&quot;;
+        }
&lt;p&gt;+    }&lt;br/&gt;
+    return location.origin + &quot;/api/v1/applications/&quot; + appId + &quot;/allexecutors&quot;;&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/package.scala b/core/src/main/resources/org/apache/spark/ui/static/webui-dataTables.css&lt;br/&gt;
similarity index 79%&lt;br/&gt;
rename from external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/package.scala&lt;br/&gt;
rename to core/src/main/resources/org/apache/spark/ui/static/webui-dataTables.css&lt;br/&gt;
index 47c5187f8751f..f6b4abed21e0d 100644&lt;br/&gt;
&amp;#8212; a/external/kafka-0-8/src/main/scala/org/apache/spark/streaming/kafka/package.scala&lt;br/&gt;
+++ b/core/src/main/resources/org/apache/spark/ui/static/webui-dataTables.css&lt;br/&gt;
@@ -15,9 +15,6 @@&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;limitations under the License.&lt;br/&gt;
  */&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-package org.apache.spark.streaming&lt;br/&gt;
+table.dataTable thead .sorting_asc &lt;/p&gt;
{ background: url(&apos;images/sort_asc.png&apos;) no-repeat bottom right; }

&lt;p&gt;-/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Kafka receiver for spark streaming,&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-package object kafka&lt;br/&gt;
+table.dataTable thead .sorting_desc 
{ background: url(&apos;images/sort_desc.png&apos;) no-repeat bottom right; }
&lt;p&gt;\ No newline at end of file&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/webui.css b/core/src/main/resources/org/apache/spark/ui/static/webui.css&lt;br/&gt;
index 935d9b1aec615..fe5bb25687af1 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/resources/org/apache/spark/ui/static/webui.css&lt;br/&gt;
+++ b/core/src/main/resources/org/apache/spark/ui/static/webui.css&lt;br/&gt;
@@ -88,6 +88,10 @@ a.kill-link 
{
   float: right;
 }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+a.name-link &lt;/p&gt;
{
+  word-wrap: break-word;
+}
&lt;p&gt;+&lt;br/&gt;
 span.expand-details {&lt;br/&gt;
   font-size: 10pt;&lt;br/&gt;
   cursor: pointer;&lt;br/&gt;
@@ -251,4 +255,110 @@ a.expandbutton {&lt;/p&gt;

&lt;p&gt; .table-cell-width-limited td &lt;/p&gt;
{
   max-width: 600px;
+}
&lt;p&gt;+&lt;br/&gt;
+.paginate_button.active &amp;gt; a &lt;/p&gt;
{
+    color: #999999;
+    text-decoration: underline;
+}
&lt;p&gt;+&lt;br/&gt;
+.title-table &lt;/p&gt;
{
+  clear: left;
+  display: inline-block;
+}
&lt;p&gt;+&lt;br/&gt;
+.table-dataTable &lt;/p&gt;
{
+  width: 100%;
+}
&lt;p&gt;+&lt;br/&gt;
+.container-fluid-div &lt;/p&gt;
{
+  width: 200px;
+}
&lt;p&gt;+&lt;br/&gt;
+.scheduler-delay-checkbox-div &lt;/p&gt;
{
+  width: 120px;
+}
&lt;p&gt;+&lt;br/&gt;
+.task-deserialization-time-checkbox-div &lt;/p&gt;
{
+  width: 175px;
+}
&lt;p&gt;+&lt;br/&gt;
+.shuffle-read-blocked-time-checkbox-div &lt;/p&gt;
{
+  width: 187px;
+}
&lt;p&gt;+&lt;br/&gt;
+.shuffle-remote-reads-checkbox-div &lt;/p&gt;
{
+  width: 157px;
+}
&lt;p&gt;+&lt;br/&gt;
+.result-serialization-time-checkbox-div &lt;/p&gt;
{
+  width: 171px;
+}
&lt;p&gt;+&lt;br/&gt;
+.getting-result-time-checkbox-div &lt;/p&gt;
{
+  width: 141px;
+}
&lt;p&gt;+&lt;br/&gt;
+.peak-execution-memory-checkbox-div &lt;/p&gt;
{
+  width: 170px;
+}
&lt;p&gt;+&lt;br/&gt;
+#active-tasks-table th &lt;/p&gt;
{
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#active-tasks-table th:first-child &lt;/p&gt;
{
+  border-left: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#accumulator-table th &lt;/p&gt;
{
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#accumulator-table th:first-child &lt;/p&gt;
{
+  border-left: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#summary-executor-table th &lt;/p&gt;
{
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#summary-executor-table th:first-child &lt;/p&gt;
{
+  border-left: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#summary-metrics-table th &lt;/p&gt;
{
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#summary-metrics-table th:first-child &lt;/p&gt;
{
+  border-left: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#summary-execs-table th &lt;/p&gt;
{
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#summary-execs-table th:first-child &lt;/p&gt;
{
+  border-left: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#active-executors-table th &lt;/p&gt;
{
+  border-top: 1px solid #dddddd;
+  border-bottom: 1px solid #dddddd;
+  border-right: 1px solid #dddddd;
+}
&lt;p&gt;+&lt;br/&gt;
+#active-executors-table th:first-child &lt;/p&gt;
{
+  border-left: 1px solid #dddddd;
 }
&lt;p&gt;\ No newline at end of file&lt;br/&gt;
diff --git a/core/src/main/resources/org/apache/spark/ui/static/webui.js b/core/src/main/resources/org/apache/spark/ui/static/webui.js&lt;br/&gt;
index f01c567ba58ad..b1254e08fa504 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/resources/org/apache/spark/ui/static/webui.js&lt;br/&gt;
+++ b/core/src/main/resources/org/apache/spark/ui/static/webui.js&lt;br/&gt;
@@ -83,4 +83,7 @@ $(function() &lt;/p&gt;
{
   collapseTablePageLoad(&apos;collapse-aggregated-rdds&apos;,&apos;aggregated-rdds&apos;);
   collapseTablePageLoad(&apos;collapse-aggregated-activeBatches&apos;,&apos;aggregated-activeBatches&apos;);
   collapseTablePageLoad(&apos;collapse-aggregated-completedBatches&apos;,&apos;aggregated-completedBatches&apos;);
+  collapseTablePageLoad(&apos;collapse-aggregated-runningExecutions&apos;,&apos;aggregated-runningExecutions&apos;);
+  collapseTablePageLoad(&apos;collapse-aggregated-completedExecutions&apos;,&apos;aggregated-completedExecutions&apos;);
+  collapseTablePageLoad(&apos;collapse-aggregated-failedExecutions&apos;,&apos;aggregated-failedExecutions&apos;);
 }
&lt;p&gt;);&lt;br/&gt;
\ No newline at end of file&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/Accumulable.scala b/core/src/main/scala/org/apache/spark/Accumulable.scala&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 3092074232d18..0000000000000&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/Accumulable.scala&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,226 +0,0 @@&lt;br/&gt;
-/*&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements.  See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License.  You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-&lt;br/&gt;
-package org.apache.spark&lt;br/&gt;
-&lt;br/&gt;
-import java.io.Serializable&lt;br/&gt;
-&lt;br/&gt;
-import scala.collection.generic.Growable&lt;br/&gt;
-import scala.reflect.ClassTag&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.spark.scheduler.AccumulableInfo&lt;br/&gt;
-import org.apache.spark.serializer.JavaSerializer&lt;br/&gt;
-import org.apache.spark.util.
{AccumulatorContext, AccumulatorMetadata, LegacyAccumulatorWrapper}
&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* A data type that can be accumulated, i.e. has a commutative and associative &quot;add&quot; operation,&lt;/li&gt;
	&lt;li&gt;* but where the result type, `R`, may be different from the element type being added, `T`.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* You must define how to add data, and how to merge two of these together.  For some data types,&lt;/li&gt;
	&lt;li&gt;* such as a counter, these might be the same operation. In that case, you can use the simpler&lt;/li&gt;
	&lt;li&gt;* [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;]. They won&apos;t always be the same, though &amp;#8211; e.g., imagine you are&lt;/li&gt;
	&lt;li&gt;* accumulating a set. You will add items to the set, and you will union two sets together.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Operations are not thread-safe.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param id ID of this accumulator; for internal use only.&lt;/li&gt;
	&lt;li&gt;* @param initialValue initial value of accumulator&lt;/li&gt;
	&lt;li&gt;* @param param helper object defining how to add elements of type `R` and `T`&lt;/li&gt;
	&lt;li&gt;* @param name human-readable name for use in Spark&apos;s web UI&lt;/li&gt;
	&lt;li&gt;* @param countFailedValues whether to accumulate values from failed tasks. This is set to true&lt;/li&gt;
	&lt;li&gt;*                          for system and time metrics like serialization time or bytes spilled,&lt;/li&gt;
	&lt;li&gt;*                          and false for things with absolute values like number of input rows.&lt;/li&gt;
	&lt;li&gt;*                          This should be used for internal metrics only.&lt;/li&gt;
	&lt;li&gt;* @tparam R the full accumulated data (result type)&lt;/li&gt;
	&lt;li&gt;* @tparam T partial data that can be added in&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-class Accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt; private (&lt;/li&gt;
	&lt;li&gt;val id: Long,&lt;/li&gt;
	&lt;li&gt;// SI-8813: This must explicitly be a private val, or else scala 2.11 doesn&apos;t compile&lt;/li&gt;
	&lt;li&gt;@transient private val initialValue: R,&lt;/li&gt;
	&lt;li&gt;param: AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt;,&lt;/li&gt;
	&lt;li&gt;val name: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;/li&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val countFailedValues: Boolean)&lt;/li&gt;
	&lt;li&gt;extends Serializable {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def this(&lt;/li&gt;
	&lt;li&gt;initialValue: R,&lt;/li&gt;
	&lt;li&gt;param: AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt;,&lt;/li&gt;
	&lt;li&gt;name: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;/li&gt;
	&lt;li&gt;countFailedValues: Boolean) = 
{
-    this(AccumulatorContext.newId(), initialValue, param, name, countFailedValues)
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def this(initialValue: R, param: AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt;, name: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;) = 
{
-    this(initialValue, param, name, false /* countFailedValues */)
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;def this(initialValue: R, param: AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt;) = this(initialValue, param, None)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val zero = param.zero(initialValue)&lt;/li&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val newAcc = new LegacyAccumulatorWrapper(initialValue, param)&lt;/li&gt;
	&lt;li&gt;newAcc.metadata = AccumulatorMetadata(id, name, countFailedValues)&lt;/li&gt;
	&lt;li&gt;// Register the new accumulator in ctor, to follow the previous behaviour.&lt;/li&gt;
	&lt;li&gt;AccumulatorContext.register(newAcc)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Add more data to this accumulator / accumulable&lt;/li&gt;
	&lt;li&gt;* @param term the data to add&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;def += (term: T) 
{ newAcc.add(term) }&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Add more data to this accumulator / accumulable&lt;br/&gt;
-   * @param term the data to add&lt;br/&gt;
-   */&lt;br/&gt;
-  def add(term: T) { newAcc.add(term) }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Merge two accumulable objects together&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Normally, a user will not want to use this version, but will instead call `+=`.&lt;/li&gt;
	&lt;li&gt;* @param term the other `R` that will get merged with this&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;def ++= (term: R) 
{ newAcc._value = param.addInPlace(newAcc._value, term) }&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Merge two accumulable objects together&lt;br/&gt;
-   *&lt;br/&gt;
-   * Normally, a user will not want to use this version, but will instead call `add`.&lt;br/&gt;
-   * @param term the other `R` that will get merged with this&lt;br/&gt;
-   */&lt;br/&gt;
-  def merge(term: R) { newAcc._value = param.addInPlace(newAcc._value, term) }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Access the accumulator&apos;s current value; only allowed on driver.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;def value: R = {&lt;/li&gt;
	&lt;li&gt;if (newAcc.isAtDriverSide) 
{
-      newAcc.value
-    }
&lt;p&gt; else &lt;/p&gt;
{
-      throw new UnsupportedOperationException(&quot;Can&apos;t read accumulator value in task&quot;)
-    }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Get the current value of this accumulator from within a task.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* This is NOT the global value of the accumulator.  To get the global value after a&lt;/li&gt;
	&lt;li&gt;* completed operation on the dataset, call `value`.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* The typical use of this method is to directly mutate the local value, eg., to add&lt;/li&gt;
	&lt;li&gt;* an element to a Set.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;def localValue: R = newAcc.value&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Set the accumulator&apos;s value; only allowed on driver.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;def value_= (newValue: R) {&lt;/li&gt;
	&lt;li&gt;if (newAcc.isAtDriverSide) 
{
-      newAcc._value = newValue
-    }
&lt;p&gt; else &lt;/p&gt;
{
-      throw new UnsupportedOperationException(&quot;Can&apos;t assign accumulator value in task&quot;)
-    }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Set the accumulator&apos;s value. For internal use only.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;def setValue(newValue: R): Unit = 
{ newAcc._value = newValue }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Set the accumulator&apos;s value. For internal use only.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def setValueAny(newValue: Any): Unit = 
{ setValue(newValue.asInstanceOf[R]) }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;AccumulableInfo&amp;#93;&lt;/span&gt;] representation of this [&lt;span class=&quot;error&quot;&gt;&amp;#91;Accumulable&amp;#93;&lt;/span&gt;] with the provided values.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def toInfo(update: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Any&amp;#93;&lt;/span&gt;, value: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Any&amp;#93;&lt;/span&gt;): AccumulableInfo = 
{
-    val isInternal = name.exists(_.startsWith(InternalAccumulator.METRICS_PREFIX))
-    new AccumulableInfo(id, name, update, value, isInternal, countFailedValues)
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;override def toString: String = if (newAcc._value == null) &quot;null&quot; else newAcc._value.toString&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;/li&gt;
	&lt;li&gt;* Helper object defining how to accumulate values of a particular type. An implicit&lt;/li&gt;
	&lt;li&gt;* AccumulableParam needs to be available when you create [&lt;span class=&quot;error&quot;&gt;&amp;#91;Accumulable&amp;#93;&lt;/span&gt;]s of a specific type.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @tparam R the full accumulated data (result type)&lt;/li&gt;
	&lt;li&gt;* @tparam T partial data that can be added in&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-trait AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt; extends Serializable 
{
-  /**
-   * Add additional data to the accumulator value. Is allowed to modify and return `r`
-   * for efficiency (to avoid allocating objects).
-   *
-   * @param r the current value of the accumulator
-   * @param t the data to be added to the accumulator
-   * @return the new value of the accumulator
-   */
-  def addAccumulator(r: R, t: T): R
-
-  /**
-   * Merge two accumulated values together. Is allowed to modify and return the first value
-   * for efficiency (to avoid allocating objects).
-   *
-   * @param r1 one set of accumulated data
-   * @param r2 another set of accumulated data
-   * @return both data sets merged together
-   */
-  def addInPlace(r1: R, r2: R): R
-
-  /**
-   * Return the &quot;zero&quot; (identity) value for an accumulator type, given its initial value. For
-   * example, if R was a vector of N dimensions, this would return a vector of N zeroes.
-   */
-  def zero(initialValue: R): R
-}
&lt;p&gt;-&lt;br/&gt;
-&lt;br/&gt;
-@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class&lt;br/&gt;
-GrowableAccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;R : ClassTag, T&amp;#93;&lt;/span&gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;(implicit rg: R =&amp;gt; Growable&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; with TraversableOnce&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; with Serializable)&lt;/li&gt;
	&lt;li&gt;extends AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt; {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;def addAccumulator(growable: R, elem: T): R = 
{
-    growable += elem
-    growable
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;def addInPlace(t1: R, t2: R): R = 
{
-    t1 ++= t2
-    t1
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;def zero(initialValue: R): R = 
{
-    // We need to clone initialValue, but it&apos;s hard to specify that R should also be Cloneable.
-    // Instead we&apos;ll serialize it to a buffer and load it back.
-    val ser = new JavaSerializer(new SparkConf(false)).newInstance()
-    val copy = ser.deserialize[R](ser.serialize(initialValue))
-    copy.clear()   // In case it contained stuff
-    copy
-  }
&lt;p&gt;-}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/Accumulator.scala b/core/src/main/scala/org/apache/spark/Accumulator.scala&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 9d5fbefc824ad..0000000000000&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/Accumulator.scala&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,117 +0,0 @@&lt;br/&gt;
-/*&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements.  See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License.  You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-&lt;br/&gt;
-package org.apache.spark&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;/li&gt;
	&lt;li&gt;* A simpler value of [&lt;span class=&quot;error&quot;&gt;&amp;#91;Accumulable&amp;#93;&lt;/span&gt;] where the result type being accumulated is the same&lt;/li&gt;
	&lt;li&gt;* as the types of elements being merged, i.e. variables that are only &quot;added&quot; to through an&lt;/li&gt;
	&lt;li&gt;* associative and commutative operation and can therefore be efficiently supported in parallel.&lt;/li&gt;
	&lt;li&gt;* They can be used to implement counters (as in MapReduce) or sums. Spark natively supports&lt;/li&gt;
	&lt;li&gt;* accumulators of numeric value types, and programmers can add support for new types.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* An accumulator is created from an initial value `v` by calling `SparkContext.accumulator`.&lt;/li&gt;
	&lt;li&gt;* Tasks running on the cluster can then add to it using the `+=` operator.&lt;/li&gt;
	&lt;li&gt;* However, they cannot read its value. Only the driver program can read the accumulator&apos;s value,&lt;/li&gt;
	&lt;li&gt;* using its &lt;a href=&quot;#value]&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;[#value]&lt;/a&gt; method.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* The interpreter session below shows an accumulator being used to add up the elements of an array:&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* {{
{
- * scala&amp;gt; val accum = sc.accumulator(0)
- * accum: org.apache.spark.Accumulator[Int] = 0
- *
- * scala&amp;gt; sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&amp;gt; accum += x)
- * ...
- * 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s
- *
- * scala&amp;gt; accum.value
- * res2: Int = 10
- * }
&lt;p&gt;}}&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param initialValue initial value of accumulator&lt;/li&gt;
	&lt;li&gt;* @param param helper object defining how to add elements of type `T`&lt;/li&gt;
	&lt;li&gt;* @param name human-readable name associated with this accumulator&lt;/li&gt;
	&lt;li&gt;* @param countFailedValues whether to accumulate values from failed tasks&lt;/li&gt;
	&lt;li&gt;* @tparam T result type&lt;br/&gt;
-*/&lt;br/&gt;
-@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-class Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; (&lt;/li&gt;
	&lt;li&gt;// SI-8813: This must explicitly be a private val, or else scala 2.11 doesn&apos;t compile&lt;/li&gt;
	&lt;li&gt;@transient private val initialValue: T,&lt;/li&gt;
	&lt;li&gt;param: AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;,&lt;/li&gt;
	&lt;li&gt;name: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None,&lt;/li&gt;
	&lt;li&gt;countFailedValues: Boolean = false)&lt;/li&gt;
	&lt;li&gt;extends Accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;T, T&amp;#93;&lt;/span&gt;(initialValue, param, name, countFailedValues)&lt;br/&gt;
-&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;/li&gt;
	&lt;li&gt;* A simpler version of [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.AccumulableParam&amp;#93;&lt;/span&gt;] where the only data type you can add&lt;/li&gt;
	&lt;li&gt;* in is the same type as the accumulated value. An implicit AccumulatorParam object needs to be&lt;/li&gt;
	&lt;li&gt;* available when you create Accumulators of a specific type.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @tparam T type of value to accumulate&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-trait AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; extends AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;T, T&amp;#93;&lt;/span&gt; {&lt;/li&gt;
	&lt;li&gt;def addAccumulator(t1: T, t2: T): T = 
{
-    addInPlace(t1, t2)
-  }
&lt;p&gt;-}&lt;br/&gt;
-&lt;br/&gt;
-&lt;br/&gt;
-@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-object AccumulatorParam {&lt;br/&gt;
-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// The following implicit objects were in SparkContext before 1.2 and users had to&lt;/li&gt;
	&lt;li&gt;// `import SparkContext._` to enable them. Now we move them here to make the compiler find&lt;/li&gt;
	&lt;li&gt;// them automatically. However, as there are duplicate codes in SparkContext for backward&lt;/li&gt;
	&lt;li&gt;// compatibility, please update them accordingly if you modify the following implicit objects.&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;/li&gt;
	&lt;li&gt;implicit object DoubleAccumulatorParam extends AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;Double&amp;#93;&lt;/span&gt; 
{
-    def addInPlace(t1: Double, t2: Double): Double = t1 + t2
-    def zero(initialValue: Double): Double = 0.0
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;/li&gt;
	&lt;li&gt;implicit object IntAccumulatorParam extends AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; 
{
-    def addInPlace(t1: Int, t2: Int): Int = t1 + t2
-    def zero(initialValue: Int): Int = 0
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;/li&gt;
	&lt;li&gt;implicit object LongAccumulatorParam extends AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; 
{
-    def addInPlace(t1: Long, t2: Long): Long = t1 + t2
-    def zero(initialValue: Long): Long = 0L
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;/li&gt;
	&lt;li&gt;implicit object FloatAccumulatorParam extends AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;Float&amp;#93;&lt;/span&gt; 
{
-    def addInPlace(t1: Float, t2: Float): Float = t1 + t2
-    def zero(initialValue: Float): Float = 0f
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// Note: when merging values, this param just adopts the newer value. This is used only&lt;/li&gt;
	&lt;li&gt;// internally for things that shouldn&apos;t really be accumulated across tasks, like input&lt;/li&gt;
	&lt;li&gt;// read method, which should be the same across all tasks in the same stage.&lt;/li&gt;
	&lt;li&gt;@deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;/li&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object StringAccumulatorParam extends AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; 
{
-    def addInPlace(t1: String, t2: String): String = t2
-    def zero(initialValue: String): String = &quot;&quot;
-  }
&lt;p&gt;-}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala b/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala&lt;br/&gt;
index 90a5c4130f799..6a497afac444d 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/BarrierTaskContext.scala&lt;br/&gt;
@@ -41,14 +41,14 @@ import org.apache.spark.util._&lt;br/&gt;
 class BarrierTaskContext private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; (&lt;br/&gt;
     taskContext: TaskContext) extends TaskContext with Logging {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  import BarrierTaskContext._&lt;br/&gt;
+&lt;br/&gt;
   // Find the driver side RPCEndpointRef of the coordinator that handles all the barrier() calls.&lt;br/&gt;
   private val barrierCoordinator: RpcEndpointRef = &lt;/p&gt;
{
     val env = SparkEnv.get
     RpcUtils.makeDriverRef(&quot;barrierSync&quot;, env.conf, env.rpcEnv)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private val timer = new Timer(&quot;Barrier task timer for barrier() calls.&quot;)&lt;br/&gt;
-&lt;br/&gt;
   // Local barrierEpoch that identify a barrier() call from current task, it shall be identical&lt;br/&gt;
   // with the driver side epoch.&lt;br/&gt;
   private var barrierEpoch = 0&lt;br/&gt;
@@ -158,8 +158,6 @@ class BarrierTaskContext private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; (&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   override def isInterrupted(): Boolean = taskContext.isInterrupted()&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def isRunningLocally(): Boolean = taskContext.isRunningLocally()&lt;br/&gt;
-&lt;br/&gt;
   override def addTaskCompletionListener(listener: TaskCompletionListener): this.type = {&lt;br/&gt;
     taskContext.addTaskCompletionListener(listener)&lt;br/&gt;
     this&lt;br/&gt;
@@ -234,4 +232,7 @@ object BarrierTaskContext 
{
   @Experimental
   @Since(&quot;2.4.0&quot;)
   def get(): BarrierTaskContext = TaskContext.get().asInstanceOf[BarrierTaskContext]
+
+  private val timer = new Timer(&quot;Barrier task timer for barrier() calls.&quot;)
+
 }
&lt;p&gt;diff --git a/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala b/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala&lt;br/&gt;
index bcbc8df0d5865..ab0ae55ed357d 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/HeartbeatReceiver.scala&lt;br/&gt;
@@ -22,6 +22,7 @@ import java.util.concurrent.
{ScheduledFuture, TimeUnit}
&lt;p&gt; import scala.collection.mutable&lt;br/&gt;
 import scala.concurrent.Future&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import org.apache.spark.executor.ExecutorMetrics&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.rpc.&lt;/p&gt;
{RpcCallContext, RpcEnv, ThreadSafeRpcEndpoint}
&lt;p&gt; import org.apache.spark.scheduler._&lt;br/&gt;
@@ -37,7 +38,8 @@ import org.apache.spark.util._&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; case class Heartbeat(&lt;br/&gt;
     executorId: String,&lt;br/&gt;
     accumUpdates: Array[(Long, Seq[AccumulatorV2&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;])], // taskId -&amp;gt; accumulator updates&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;blockManagerId: BlockManagerId)&lt;br/&gt;
+    blockManagerId: BlockManagerId,&lt;br/&gt;
+    executorUpdates: ExecutorMetrics) // executor level updates&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;An event that SparkContext uses to notify HeartbeatReceiver that SparkContext.taskScheduler is&lt;br/&gt;
@@ -119,14 +121,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class HeartbeatReceiver(sc: SparkContext, clock: Clock)&lt;br/&gt;
       context.reply(true)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Messages received from executors&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case heartbeat @ Heartbeat(executorId, accumUpdates, blockManagerId) =&amp;gt;&lt;br/&gt;
+    case heartbeat @ Heartbeat(executorId, accumUpdates, blockManagerId, executorMetrics) =&amp;gt;&lt;br/&gt;
       if (scheduler != null) {&lt;br/&gt;
         if (executorLastSeen.contains(executorId)) {&lt;br/&gt;
           executorLastSeen(executorId) = clock.getTimeMillis()&lt;br/&gt;
           eventLoopThread.submit(new Runnable {&lt;br/&gt;
             override def run(): Unit = Utils.tryLogNonFatalError 
{
               val unknownExecutor = !scheduler.executorHeartbeatReceived(
-                executorId, accumUpdates, blockManagerId)
+                executorId, accumUpdates, blockManagerId, executorMetrics)
               val response = HeartbeatResponse(reregisterBlockManager = unknownExecutor)
               context.reply(response)
             }
&lt;p&gt;diff --git a/core/src/main/scala/org/apache/spark/Heartbeater.scala b/core/src/main/scala/org/apache/spark/Heartbeater.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..84091eef04306&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;/dev/null&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/Heartbeater.scala&lt;br/&gt;
@@ -0,0 +1,71 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark&lt;br/&gt;
+&lt;br/&gt;
+import java.util.concurrent.TimeUnit&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.spark.executor.ExecutorMetrics&lt;br/&gt;
+import org.apache.spark.internal.Logging&lt;br/&gt;
+import org.apache.spark.memory.MemoryManager&lt;br/&gt;
+import org.apache.spark.metrics.ExecutorMetricType&lt;br/&gt;
+import org.apache.spark.util.
{ThreadUtils, Utils}&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Creates a heartbeat thread which will call the specified reportHeartbeat function at&lt;br/&gt;
+ * intervals of intervalMs.&lt;br/&gt;
+ *&lt;br/&gt;
+ * @param memoryManager the memory manager for execution and storage memory.&lt;br/&gt;
+ * @param reportHeartbeat the heartbeat reporting function to call.&lt;br/&gt;
+ * @param name the thread name for the heartbeater.&lt;br/&gt;
+ * @param intervalMs the interval between heartbeats.&lt;br/&gt;
+ */&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class Heartbeater(&lt;br/&gt;
+    memoryManager: MemoryManager,&lt;br/&gt;
+    reportHeartbeat: () =&amp;gt; Unit,&lt;br/&gt;
+    name: String,&lt;br/&gt;
+    intervalMs: Long) extends Logging {&lt;br/&gt;
+  // Executor for the heartbeat task&lt;br/&gt;
+  private val heartbeater = ThreadUtils.newDaemonSingleThreadScheduledExecutor(name)&lt;br/&gt;
+&lt;br/&gt;
+  /** Schedules a task to report a heartbeat. */&lt;br/&gt;
+  def start(): Unit = {&lt;br/&gt;
+    // Wait a random interval so the heartbeats don&apos;t end up in sync&lt;br/&gt;
+    val initialDelay = intervalMs + (math.random * intervalMs).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
+    val heartbeatTask = new Runnable() {
+      override def run(): Unit = Utils.logUncaughtExceptions(reportHeartbeat())
+    }&lt;br/&gt;
+    heartbeater.scheduleAtFixedRate(heartbeatTask, initialDelay, intervalMs, TimeUnit.MILLISECONDS)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /** Stops the heartbeat thread. */&lt;br/&gt;
+  def stop(): Unit = {
+    heartbeater.shutdown()
+    heartbeater.awaitTermination(10, TimeUnit.SECONDS)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Get the current executor level metrics. These are returned as an array, with the index&lt;br/&gt;
+   * determined by ExecutorMetricType.values&lt;br/&gt;
+   */&lt;br/&gt;
+  def getCurrentMetrics(): ExecutorMetrics = {
+    val metrics = ExecutorMetricType.values.map(_.getMetricValue(memoryManager)).toArray
+    new ExecutorMetrics(metrics)
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala b/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala&lt;br/&gt;
index ff85e11409e35..f8a6f1d0d8cbb 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/MapOutputStatistics.scala&lt;br/&gt;
@@ -23,9 +23,5 @@ package org.apache.spark&lt;br/&gt;
  * @param shuffleId ID of the shuffle&lt;br/&gt;
  * @param bytesByPartitionId approximate number of output bytes for each map output partition&lt;br/&gt;
  *   (may be inexact due to use of compressed map statuses)&lt;br/&gt;
- * @param recordsByPartitionId number of output records for each map output partition&lt;br/&gt;
  */&lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class MapOutputStatistics(&lt;br/&gt;
-    val shuffleId: Int,&lt;br/&gt;
-    val bytesByPartitionId: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;,&lt;br/&gt;
-    val recordsByPartitionId: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class MapOutputStatistics(val shuffleId: Int, val bytesByPartitionId: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/MapOutputTracker.scala b/core/src/main/scala/org/apache/spark/MapOutputTracker.scala&lt;br/&gt;
index 41575ce4e6e3d..1c4fa4bc6541f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/MapOutputTracker.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/MapOutputTracker.scala&lt;br/&gt;
@@ -522,19 +522,16 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class MapOutputTrackerMaster(&lt;br/&gt;
   def getStatistics(dep: ShuffleDependency&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _, _&amp;#93;&lt;/span&gt;): MapOutputStatistics = {&lt;br/&gt;
     shuffleStatuses(dep.shuffleId).withMapStatuses { statuses =&amp;gt;&lt;br/&gt;
       val totalSizes = new Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;(dep.partitioner.numPartitions)&lt;br/&gt;
-      val recordsByMapTask = new Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;(statuses.length)&lt;br/&gt;
-&lt;br/&gt;
       val parallelAggThreshold = conf.get(&lt;br/&gt;
         SHUFFLE_MAP_OUTPUT_PARALLEL_AGGREGATION_THRESHOLD)&lt;br/&gt;
       val parallelism = math.min(&lt;br/&gt;
         Runtime.getRuntime.availableProcessors(),&lt;br/&gt;
         statuses.length.toLong * totalSizes.length / parallelAggThreshold + 1).toInt&lt;br/&gt;
       if (parallelism &amp;lt;= 1) {&lt;br/&gt;
-        statuses.zipWithIndex.foreach { case (s, index) =&amp;gt;&lt;br/&gt;
+        for (s &amp;lt;- statuses) {&lt;br/&gt;
           for (i &amp;lt;- 0 until totalSizes.length) {
             totalSizes(i) += s.getSizeForBlock(i)
           }&lt;br/&gt;
-          recordsByMapTask(index) = s.numberOfOutput&lt;br/&gt;
         }&lt;br/&gt;
       } else {
         val threadPool = ThreadUtils.newDaemonFixedThreadPool(parallelism, &quot;map-output-aggregate&quot;)
@@ -551,11 +548,8 @@ private[spark] class MapOutputTrackerMaster(
         } finally {
           threadPool.shutdown()
         }&lt;br/&gt;
-        statuses.zipWithIndex.foreach { case (s, index) =&amp;gt;
-          recordsByMapTask(index) = s.numberOfOutput
-        }&lt;br/&gt;
       }&lt;br/&gt;
-      new MapOutputStatistics(dep.shuffleId, totalSizes, recordsByMapTask)&lt;br/&gt;
+      new MapOutputStatistics(dep.shuffleId, totalSizes)&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/SparkConf.scala b/core/src/main/scala/org/apache/spark/SparkConf.scala&lt;br/&gt;
index 6c4c5c94cfa28..21c5cbc04d813 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/SparkConf.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/SparkConf.scala&lt;br/&gt;
@@ -25,9 +25,9 @@ import scala.collection.mutable.LinkedHashSet&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.avro.{Schema, SchemaNormalization}&lt;br/&gt;
 &lt;br/&gt;
-import org.apache.spark.deploy.history.config._&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.internal.config._&lt;br/&gt;
+import org.apache.spark.internal.config.History._&lt;br/&gt;
 import org.apache.spark.serializer.KryoSerializer&lt;br/&gt;
 import org.apache.spark.util.Utils&lt;br/&gt;
 &lt;br/&gt;
@@ -609,13 +609,14 @@ class SparkConf(loadDefaults: Boolean) extends Cloneable with Logging with Seria&lt;br/&gt;
     require(!encryptionEnabled || get(NETWORK_AUTH_ENABLED),&lt;br/&gt;
       s&quot;${NETWORK_AUTH_ENABLED.key} must be enabled when enabling encryption.&quot;)&lt;br/&gt;
 &lt;br/&gt;
-    val executorTimeoutThreshold = getTimeAsSeconds(&quot;spark.network.timeout&quot;, &quot;120s&quot;)&lt;br/&gt;
-    val executorHeartbeatInterval = getTimeAsSeconds(&quot;spark.executor.heartbeatInterval&quot;, &quot;10s&quot;)&lt;br/&gt;
+    val executorTimeoutThresholdMs =&lt;br/&gt;
+      getTimeAsSeconds(&quot;spark.network.timeout&quot;, &quot;120s&quot;) * 1000&lt;br/&gt;
+    val executorHeartbeatIntervalMs = get(EXECUTOR_HEARTBEAT_INTERVAL)&lt;br/&gt;
     // If spark.executor.heartbeatInterval bigger than spark.network.timeout,&lt;br/&gt;
     // it will almost always cause ExecutorLostFailure. See &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-22754&quot; title=&quot;Check spark.executor.heartbeatInterval setting in case of ExecutorLost&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-22754&quot;&gt;&lt;del&gt;SPARK-22754&lt;/del&gt;&lt;/a&gt;.&lt;br/&gt;
-    require(executorTimeoutThreshold &amp;gt; executorHeartbeatInterval, &quot;The value of &quot; +&lt;br/&gt;
-      s&quot;spark.network.timeout=${executorTimeoutThreshold}s must be no less than the value of &quot; +&lt;br/&gt;
-      s&quot;spark.executor.heartbeatInterval=${executorHeartbeatInterval}s.&quot;)&lt;br/&gt;
+    require(executorTimeoutThresholdMs &amp;gt; executorHeartbeatIntervalMs, &quot;The value of &quot; +&lt;br/&gt;
+      s&quot;spark.network.timeout=${executorTimeoutThresholdMs}ms must be no less than the value of &quot; +&lt;br/&gt;
+      s&quot;spark.executor.heartbeatInterval=${executorHeartbeatIntervalMs}ms.&quot;)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
@@ -726,7 +727,13 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object SparkConf extends Logging {&lt;br/&gt;
     DRIVER_MEMORY_OVERHEAD.key -&amp;gt; Seq(&lt;br/&gt;
       AlternateConfig(&quot;spark.yarn.driver.memoryOverhead&quot;, &quot;2.3&quot;)),&lt;br/&gt;
     EXECUTOR_MEMORY_OVERHEAD.key -&amp;gt; Seq(&lt;br/&gt;
-      AlternateConfig(&quot;spark.yarn.executor.memoryOverhead&quot;, &quot;2.3&quot;))&lt;br/&gt;
+      AlternateConfig(&quot;spark.yarn.executor.memoryOverhead&quot;, &quot;2.3&quot;)),&lt;br/&gt;
+    KEYTAB.key -&amp;gt; Seq(&lt;br/&gt;
+      AlternateConfig(&quot;spark.yarn.keytab&quot;, &quot;3.0&quot;)),&lt;br/&gt;
+    PRINCIPAL.key -&amp;gt; Seq(&lt;br/&gt;
+      AlternateConfig(&quot;spark.yarn.principal&quot;, &quot;3.0&quot;)),&lt;br/&gt;
+    KERBEROS_RELOGIN_PERIOD.key -&amp;gt; Seq(&lt;br/&gt;
+      AlternateConfig(&quot;spark.yarn.kerberos.relogin.period&quot;, &quot;3.0&quot;))&lt;br/&gt;
   )&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/SparkContext.scala b/core/src/main/scala/org/apache/spark/SparkContext.scala&lt;br/&gt;
index e5b1e0ecd1586..845a3d5f6d6f9 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/SparkContext.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/SparkContext.scala&lt;br/&gt;
@@ -25,7 +25,6 @@ import java.util.concurrent.atomic.{AtomicBoolean, AtomicInteger, AtomicReferenc&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.Map&lt;br/&gt;
-import scala.collection.generic.Growable&lt;br/&gt;
 import scala.collection.mutable.HashMap&lt;br/&gt;
 import scala.language.implicitConversions&lt;br/&gt;
 import scala.reflect.{classTag, ClassTag}&lt;br/&gt;
@@ -51,14 +50,15 @@ import org.apache.spark.partial.{ApproximateEvaluator, PartialResult}&lt;br/&gt;
 import org.apache.spark.rdd._&lt;br/&gt;
 import org.apache.spark.rpc.RpcEndpointRef&lt;br/&gt;
 import org.apache.spark.scheduler._&lt;br/&gt;
-import org.apache.spark.scheduler.cluster.{CoarseGrainedSchedulerBackend, StandaloneSchedulerBackend}&lt;br/&gt;
+import org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend&lt;br/&gt;
 import org.apache.spark.scheduler.local.LocalSchedulerBackend&lt;br/&gt;
-import org.apache.spark.status.AppStatusStore&lt;br/&gt;
+import org.apache.spark.status.{AppStatusSource, AppStatusStore}&lt;br/&gt;
 import org.apache.spark.status.api.v1.ThreadStackTrace&lt;br/&gt;
 import org.apache.spark.storage._&lt;br/&gt;
 import org.apache.spark.storage.BlockManagerMessages.TriggerThreadDump&lt;br/&gt;
 import org.apache.spark.ui.{ConsoleProgressBar, SparkUI}&lt;br/&gt;
 import org.apache.spark.util._&lt;br/&gt;
+import org.apache.spark.util.logging.DriverLogger&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
  * Main entry point for Spark functionality. A SparkContext represents the connection to a Spark&lt;br/&gt;
@@ -206,6 +206,7 @@ class SparkContext(config: SparkConf) extends Logging {&lt;br/&gt;
   private var _applicationId: String = _&lt;br/&gt;
   private var _applicationAttemptId: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
   private var _eventLogger: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;EventLoggingListener&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
+  private var _driverLogger: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;DriverLogger&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
   private var _executorAllocationManager: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorAllocationManager&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
   private var _cleaner: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ContextCleaner&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
   private var _listenerBusStarted: Boolean = false&lt;br/&gt;
@@ -213,6 +214,7 @@ class SparkContext(config: SparkConf) extends Logging {&lt;br/&gt;
   private var _files: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = _&lt;br/&gt;
   private var _shutdownHookRef: AnyRef = _&lt;br/&gt;
   private var _statusStore: AppStatusStore = _&lt;br/&gt;
+  private var _heartbeater: Heartbeater = _&lt;br/&gt;
 &lt;br/&gt;
   /* ------------------------------------------------------------------------------------- *&lt;br/&gt;
    | Accessors and public fields. These provide access to the internal state of the        |&lt;br/&gt;
@@ -371,6 +373,8 @@ class SparkContext(config: SparkConf) extends Logging {
       throw new SparkException(&quot;An application name must be set in your configuration&quot;)
     }&lt;br/&gt;
 &lt;br/&gt;
+    _driverLogger = DriverLogger(_conf)&lt;br/&gt;
+&lt;br/&gt;
     // log out spark.app.name in the Spark driver logs&lt;br/&gt;
     logInfo(s&quot;Submitted application: $appName&quot;)&lt;br/&gt;
 &lt;br/&gt;
@@ -417,7 +421,8 @@ class SparkContext(config: SparkConf) extends Logging {&lt;br/&gt;
 &lt;br/&gt;
     // Initialize the app status store and listener before SparkEnv is created so that it gets&lt;br/&gt;
     // all events.&lt;br/&gt;
-    _statusStore = AppStatusStore.createLiveStore(conf)&lt;br/&gt;
+    val appStatusSource = AppStatusSource.createSource(conf)&lt;br/&gt;
+    _statusStore = AppStatusStore.createLiveStore(conf, appStatusSource)&lt;br/&gt;
     listenerBus.addToStatusQueue(_statusStore.listener.get)&lt;br/&gt;
 &lt;br/&gt;
     // Create the Spark execution environment (cache, map output tracker, etc)&lt;br/&gt;
@@ -496,6 +501,13 @@ class SparkContext(config: SparkConf) extends Logging {&lt;br/&gt;
     _dagScheduler = new DAGScheduler(this)&lt;br/&gt;
     _heartbeatReceiver.ask&lt;span class=&quot;error&quot;&gt;&amp;#91;Boolean&amp;#93;&lt;/span&gt;(TaskSchedulerIsSet)&lt;br/&gt;
 &lt;br/&gt;
+    // create and start the heartbeater for collecting memory metrics&lt;br/&gt;
+    _heartbeater = new Heartbeater(env.memoryManager,&lt;br/&gt;
+      () =&amp;gt; SparkContext.this.reportHeartBeat(),&lt;br/&gt;
+      &quot;driver-heartbeater&quot;,&lt;br/&gt;
+      conf.get(EXECUTOR_HEARTBEAT_INTERVAL))&lt;br/&gt;
+    _heartbeater.start()&lt;br/&gt;
+&lt;br/&gt;
     // start TaskScheduler after taskScheduler sets DAGScheduler reference in DAGScheduler&apos;s&lt;br/&gt;
     // constructor&lt;br/&gt;
     _taskScheduler.start()&lt;br/&gt;
@@ -563,7 +575,7 @@ class SparkContext(config: SparkConf) extends Logging {&lt;br/&gt;
     _executorAllocationManager.foreach { e =&amp;gt;
       _env.metricsSystem.registerSource(e.executorAllocationManagerSource)
     }&lt;br/&gt;
-&lt;br/&gt;
+    appStatusSource.foreach(&lt;em&gt;env.metricsSystem.registerSource(&lt;/em&gt;))&lt;br/&gt;
     // Make sure the context is stopped if the user forgets about it. This avoids leaving&lt;br/&gt;
     // unfinished event logs around after the JVM exits cleanly. It doesn&apos;t help if the JVM&lt;br/&gt;
     // is killed, though.&lt;br/&gt;
@@ -1330,76 +1342,6 @@ class SparkContext(config: SparkConf) extends Logging {&lt;br/&gt;
 &lt;br/&gt;
   // Methods for creating shared variables&lt;br/&gt;
 &lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] variable of a given type, which tasks can &quot;add&quot;&lt;br/&gt;
-   * values to using the `+=` method. Only the driver can access the accumulator&apos;s `value`.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(initialValue: T)(implicit param: AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;): Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = {
-    val acc = new Accumulator(initialValue, param)
-    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
-    acc
-  }&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] variable of a given type, with a name for display&lt;br/&gt;
-   * in the Spark UI. Tasks can &quot;add&quot; values to the accumulator using the `+=` method. Only the&lt;br/&gt;
-   * driver can access the accumulator&apos;s `value`.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(initialValue: T, name: String)(implicit param: AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;)&lt;br/&gt;
-    : Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = {
-    val acc = new Accumulator(initialValue, param, Option(name))
-    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
-    acc
-  }&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulable&amp;#93;&lt;/span&gt;] shared variable, to which tasks can add values&lt;br/&gt;
-   * with `+=`. Only the driver can access the accumulable&apos;s `value`.&lt;br/&gt;
-   * @tparam R accumulator result type&lt;br/&gt;
-   * @tparam T type that can be added to the accumulator&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt;(initialValue: R)(implicit param: AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt;)&lt;br/&gt;
-    : Accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt; = {
-    val acc = new Accumulable(initialValue, param)
-    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
-    acc
-  }&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulable&amp;#93;&lt;/span&gt;] shared variable, with a name for display in the&lt;br/&gt;
-   * Spark UI. Tasks can add values to the accumulable using the `+=` operator. Only the driver can&lt;br/&gt;
-   * access the accumulable&apos;s `value`.&lt;br/&gt;
-   * @tparam R accumulator result type&lt;br/&gt;
-   * @tparam T type that can be added to the accumulator&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt;(initialValue: R, name: String)(implicit param: AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt;)&lt;br/&gt;
-    : Accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt; = {
-    val acc = new Accumulable(initialValue, param, Option(name))
-    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
-    acc
-  }&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an accumulator from a &quot;mutable collection&quot; type.&lt;br/&gt;
-   *&lt;br/&gt;
-   * Growable and TraversableOnce are the standard APIs that guarantee += and ++=, implemented by&lt;br/&gt;
-   * standard mutable collections. So you can use this with mutable Map, Set, etc.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulableCollection[R &amp;lt;% Growable&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; with TraversableOnce&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; with Serializable: ClassTag, T]&lt;br/&gt;
-      (initialValue: R): Accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt; = {
-    // TODO the context bound (&amp;lt;%) above should be replaced with simple type bound and implicit
-    // conversion but is a breaking change. This should be fixed in Spark 3.x.
-    val param = new GrowableAccumulableParam[R, T]
-    val acc = new Accumulable(initialValue, param)
-    cleaner.foreach(_.registerAccumulatorForCleanup(acc.newAcc))
-    acc
-  }&lt;br/&gt;
-&lt;br/&gt;
   /**&lt;br/&gt;
    * Register the given accumulator.&lt;br/&gt;
    *&lt;br/&gt;
@@ -1930,6 +1872,9 @@ class SparkContext(config: SparkConf) extends Logging {&lt;br/&gt;
     Utils.tryLogNonFatalError {
       postApplicationEnd()
     }&lt;br/&gt;
+    Utils.tryLogNonFatalError {
+      _driverLogger.foreach(_.stop())
+    }&lt;br/&gt;
     Utils.tryLogNonFatalError {
       _ui.foreach(_.stop())
     }&lt;br/&gt;
@@ -1959,6 +1904,12 @@ class SparkContext(config: SparkConf) extends Logging {&lt;br/&gt;
     Utils.tryLogNonFatalError {
       _eventLogger.foreach(_.stop())
     }&lt;br/&gt;
+    if (_heartbeater != null) {&lt;br/&gt;
+      Utils.tryLogNonFatalError {
+        _heartbeater.stop()
+      }&lt;br/&gt;
+      _heartbeater = null&lt;br/&gt;
+    }&lt;br/&gt;
     if (env != null &amp;amp;&amp;amp; _heartbeatReceiver != null) {&lt;br/&gt;
       Utils.tryLogNonFatalError {&lt;br/&gt;
         env.rpcEnv.stop(_heartbeatReceiver)&lt;br/&gt;
@@ -2409,6 +2360,7 @@ class SparkContext(config: SparkConf) extends Logging {
     // the cluster manager to get an application ID (in case the cluster manager provides one).
     listenerBus.post(SparkListenerApplicationStart(appName, Some(applicationId),
       startTime, sparkUser, applicationAttemptId, schedulerBackend.getDriverLogUrls))
+    _driverLogger.foreach(_.startSync(_hadoopConfiguration))
   }&lt;br/&gt;
 &lt;br/&gt;
   /** Post the application end event */&lt;br/&gt;
@@ -2429,6 +2381,14 @@ class SparkContext(config: SparkConf) extends Logging {&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  /** Reports heartbeat metrics for the driver. */&lt;br/&gt;
+  private def reportHeartBeat(): Unit = {
+    val driverUpdates = _heartbeater.getCurrentMetrics()
+    val accumUpdates = new Array[(Long, Int, Int, Seq[AccumulableInfo])](0)
+    listenerBus.post(SparkListenerExecutorMetricsUpdate(&quot;driver&quot;, accumUpdates,
+      Some(driverUpdates)))
+  }&lt;br/&gt;
+&lt;br/&gt;
   // In order to prevent multiple SparkContexts from being active at the same time, mark this&lt;br/&gt;
   // context as having finished construction.&lt;br/&gt;
   // NOTE: this must be placed at the end of the SparkContext constructor.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/SparkEnv.scala b/core/src/main/scala/org/apache/spark/SparkEnv.scala&lt;br/&gt;
index 72123f2232532..66038eeaea54f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/SparkEnv.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/SparkEnv.scala&lt;br/&gt;
@@ -261,7 +261,7 @@ object SparkEnv extends Logging {&lt;br/&gt;
       // SparkConf, then one taking no arguments&lt;br/&gt;
       try {
         cls.getConstructor(classOf[SparkConf], java.lang.Boolean.TYPE)
-          .newInstance(conf, new java.lang.Boolean(isDriver))
+          .newInstance(conf, java.lang.Boolean.valueOf(isDriver))
           .asInstanceOf[T]
       } catch {&lt;br/&gt;
         case _: NoSuchMethodException =&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/TaskContext.scala b/core/src/main/scala/org/apache/spark/TaskContext.scala&lt;br/&gt;
index 2b939dabb1105..959f246f3f9f6 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/TaskContext.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/TaskContext.scala&lt;br/&gt;
@@ -96,13 +96,6 @@ abstract class TaskContext extends Serializable {&lt;br/&gt;
    */&lt;br/&gt;
   def isInterrupted(): Boolean&lt;br/&gt;
 &lt;br/&gt;
-  /**&lt;br/&gt;
-   * Returns true if the task is running locally in the driver program.&lt;br/&gt;
-   * @return false&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;Local execution was removed, so this always returns false&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def isRunningLocally(): Boolean&lt;br/&gt;
-&lt;br/&gt;
   /**&lt;br/&gt;
    * Adds a (Java friendly) listener to be executed on task completion.&lt;br/&gt;
    * This will be called in all situations - success, failure, or cancellation. Adding a listener&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/TaskContextImpl.scala b/core/src/main/scala/org/apache/spark/TaskContextImpl.scala&lt;br/&gt;
index 89730424e5acf..76296c5d0abd3 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/TaskContextImpl.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/TaskContextImpl.scala&lt;br/&gt;
@@ -157,8 +157,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TaskContextImpl(&lt;br/&gt;
   @GuardedBy(&quot;this&quot;)&lt;br/&gt;
   override def isCompleted(): Boolean = synchronized(completed)&lt;br/&gt;
 &lt;br/&gt;
-  override def isRunningLocally(): Boolean = false&lt;br/&gt;
-&lt;br/&gt;
   override def isInterrupted(): Boolean = reasonIfKilled.isDefined&lt;br/&gt;
 &lt;br/&gt;
   override def getLocalProperty(key: String): String = localProperties.getProperty(key)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala b/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala&lt;br/&gt;
index 9544475ff0428..50ed8d9bd3f68 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/java/JavaPairRDD.scala&lt;br/&gt;
@@ -19,7 +19,7 @@ package org.apache.spark.api.java&lt;br/&gt;
 &lt;br/&gt;
 import java.{lang =&amp;gt; jl}&lt;br/&gt;
 import java.lang.{Iterable =&amp;gt; JIterable}&lt;br/&gt;
-import java.util.{Comparator, List =&amp;gt; JList}&lt;br/&gt;
+import java.util.{Comparator, Iterator =&amp;gt; JIterator, List =&amp;gt; JList}&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.language.implicitConversions&lt;br/&gt;
@@ -34,7 +34,8 @@ import org.apache.spark.{HashPartitioner, Partitioner}&lt;br/&gt;
 import org.apache.spark.Partitioner._&lt;br/&gt;
 import org.apache.spark.api.java.JavaSparkContext.fakeClassTag&lt;br/&gt;
 import org.apache.spark.api.java.JavaUtils.mapAsSerializableJavaMap&lt;br/&gt;
-import org.apache.spark.api.java.function.{Function =&amp;gt; JFunction, Function2 =&amp;gt; JFunction2, PairFunction}&lt;br/&gt;
+import org.apache.spark.api.java.function.{FlatMapFunction, Function =&amp;gt; JFunction,
+  Function2 =&amp;gt; JFunction2, PairFunction}&lt;br/&gt;
 import org.apache.spark.partial.{BoundedDouble, PartialResult}&lt;br/&gt;
 import org.apache.spark.rdd.{OrderedRDDFunctions, RDD}&lt;br/&gt;
 import org.apache.spark.rdd.RDD.rddToPairRDDFunctions&lt;br/&gt;
@@ -674,8 +675,8 @@ class JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(val rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    * Pass each value in the key-value pair RDD through a flatMap function without changing the&lt;br/&gt;
    * keys; this also retains the original RDD&apos;s partitioning.&lt;br/&gt;
    */&lt;br/&gt;
-  def flatMapValues&lt;span class=&quot;error&quot;&gt;&amp;#91;U&amp;#93;&lt;/span&gt;(f: JFunction[V, java.lang.Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;U&amp;#93;&lt;/span&gt;]): JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, U&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    def fn: (V) =&amp;gt; Iterable&lt;span class=&quot;error&quot;&gt;&amp;#91;U&amp;#93;&lt;/span&gt; = (x: V) =&amp;gt; f.call&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.asScala&lt;br/&gt;
+  def flatMapValues&lt;span class=&quot;error&quot;&gt;&amp;#91;U&amp;#93;&lt;/span&gt;(f: FlatMapFunction&lt;span class=&quot;error&quot;&gt;&amp;#91;V, U&amp;#93;&lt;/span&gt;): JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, U&amp;#93;&lt;/span&gt; = {
+    def fn: (V) =&amp;gt; Iterator[U] = (x: V) =&amp;gt; f.call(x).asScala
     implicit val ctag: ClassTag[U] = fakeClassTag
     fromRDD(rdd.flatMapValues(fn))
   }&lt;br/&gt;
@@ -951,7 +952,7 @@ class JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(val rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    *&lt;br/&gt;
    * The algorithm used is based on streamlib&apos;s implementation of &quot;HyperLogLog in Practice:&lt;br/&gt;
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm&quot;, available&lt;br/&gt;
-   * &amp;lt;a href=&quot;http://dx.doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
+   * &amp;lt;a href=&quot;https://doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
    *&lt;br/&gt;
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.&lt;br/&gt;
    *                   It must be greater than 0.000017.&lt;br/&gt;
@@ -968,7 +969,7 @@ class JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(val rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    *&lt;br/&gt;
    * The algorithm used is based on streamlib&apos;s implementation of &quot;HyperLogLog in Practice:&lt;br/&gt;
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm&quot;, available&lt;br/&gt;
-   * &amp;lt;a href=&quot;http://dx.doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
+   * &amp;lt;a href=&quot;https://doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
    *&lt;br/&gt;
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.&lt;br/&gt;
    *                   It must be greater than 0.000017.&lt;br/&gt;
@@ -984,7 +985,7 @@ class JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(val rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    *&lt;br/&gt;
    * The algorithm used is based on streamlib&apos;s implementation of &quot;HyperLogLog in Practice:&lt;br/&gt;
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm&quot;, available&lt;br/&gt;
-   * &amp;lt;a href=&quot;http://dx.doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
+   * &amp;lt;a href=&quot;https://doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
    *&lt;br/&gt;
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.&lt;br/&gt;
    *                   It must be greater than 0.000017.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala b/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala&lt;br/&gt;
index 91ae1002abd21..5ba821935ac69 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala&lt;br/&gt;
@@ -685,7 +685,7 @@ trait JavaRDDLike[T, This &amp;lt;: JavaRDDLike&lt;span class=&quot;error&quot;&gt;&amp;#91;T, This&amp;#93;&lt;/span&gt;] extends Serializable {&lt;br/&gt;
    *&lt;br/&gt;
    * The algorithm used is based on streamlib&apos;s implementation of &quot;HyperLogLog in Practice:&lt;br/&gt;
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm&quot;, available&lt;br/&gt;
-   * &amp;lt;a href=&quot;http://dx.doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
+   * &amp;lt;a href=&quot;https://doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
    *&lt;br/&gt;
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.&lt;br/&gt;
    *                   It must be greater than 0.000017.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala b/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala&lt;br/&gt;
index 09c83849e26b2..03f259d73e975 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/java/JavaSparkContext.scala&lt;br/&gt;
@@ -21,6 +21,7 @@ import java.io.Closeable&lt;br/&gt;
 import java.util&lt;br/&gt;
 import java.util.{Map =&amp;gt; JMap}&lt;br/&gt;
 &lt;br/&gt;
+import scala.annotation.varargs&lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.language.implicitConversions&lt;br/&gt;
 import scala.reflect.ClassTag&lt;br/&gt;
@@ -30,11 +31,10 @@ import org.apache.hadoop.mapred.{InputFormat, JobConf}&lt;br/&gt;
 import org.apache.hadoop.mapreduce.{InputFormat =&amp;gt; NewInputFormat}&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark._&lt;br/&gt;
-import org.apache.spark.AccumulatorParam._&lt;br/&gt;
 import org.apache.spark.api.java.JavaSparkContext.fakeClassTag&lt;br/&gt;
 import org.apache.spark.broadcast.Broadcast&lt;br/&gt;
 import org.apache.spark.input.PortableDataStream&lt;br/&gt;
-import org.apache.spark.rdd.{EmptyRDD, HadoopRDD, NewHadoopRDD, RDD}&lt;br/&gt;
+import org.apache.spark.rdd.{EmptyRDD, HadoopRDD, NewHadoopRDD}&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
  * A Java-friendly version of [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.SparkContext&amp;#93;&lt;/span&gt;] that returns&lt;br/&gt;
@@ -43,8 +43,7 @@ import org.apache.spark.rdd.{EmptyRDD, HadoopRDD, NewHadoopRDD, RDD}&lt;br/&gt;
  * Only one SparkContext may be active per JVM.  You must `stop()` the active SparkContext before&lt;br/&gt;
  * creating a new one.  This limitation may eventually be removed; see &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2243&quot; title=&quot;Support multiple SparkContexts in the same JVM&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2243&quot;&gt;&lt;del&gt;SPARK-2243&lt;/del&gt;&lt;/a&gt; for more details.&lt;br/&gt;
  */&lt;br/&gt;
-class JavaSparkContext(val sc: SparkContext)&lt;br/&gt;
-  extends JavaSparkContextVarargsWorkaround with Closeable {&lt;br/&gt;
+class JavaSparkContext(val sc: SparkContext) extends Closeable {
 
   /**
    * Create a JavaSparkContext that loads settings from system properties (for instance, when
@@ -507,141 +506,31 @@ class JavaSparkContext(val sc: SparkContext)
     new JavaNewHadoopRDD(rdd.asInstanceOf[NewHadoopRDD[K, V]])
   }&lt;br/&gt;
 &lt;br/&gt;
-  /** Build the union of two or more RDDs. */&lt;br/&gt;
-  override def union&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(first: JavaRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;, rest: java.util.List[JavaRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;]): JavaRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    val rdds: Seq[RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;] = (Seq(first) ++ rest.asScala).map(_.rdd)&lt;br/&gt;
-    implicit val ctag: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = first.classTag&lt;br/&gt;
-    sc.union(rdds)&lt;br/&gt;
+  /** Build the union of JavaRDDs. */&lt;br/&gt;
+  @varargs&lt;br/&gt;
+  def union&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(rdds: JavaRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;*): JavaRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = {
+    require(rdds.nonEmpty, &quot;Union called on no RDDs&quot;)
+    implicit val ctag: ClassTag[T] = rdds.head.classTag
+    sc.union(rdds.map(_.rdd))
   }&lt;br/&gt;
 &lt;br/&gt;
-  /** Build the union of two or more RDDs. */&lt;br/&gt;
-  override def union&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(first: JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;, rest: java.util.List[JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;])&lt;br/&gt;
-      : JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    val rdds: Seq[RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;] = (Seq(first) ++ rest.asScala).map(_.rdd)&lt;br/&gt;
-    implicit val ctag: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt; = first.classTag&lt;br/&gt;
-    implicit val ctagK: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;K&amp;#93;&lt;/span&gt; = first.kClassTag&lt;br/&gt;
-    implicit val ctagV: ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;V&amp;#93;&lt;/span&gt; = first.vClassTag&lt;br/&gt;
-    new JavaPairRDD(sc.union(rdds))&lt;br/&gt;
+  /** Build the union of JavaPairRDDs. */&lt;br/&gt;
+  @varargs&lt;br/&gt;
+  def union&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(rdds: JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;*): JavaPairRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt; = {
+    require(rdds.nonEmpty, &quot;Union called on no RDDs&quot;)
+    implicit val ctag: ClassTag[(K, V)] = rdds.head.classTag
+    implicit val ctagK: ClassTag[K] = rdds.head.kClassTag
+    implicit val ctagV: ClassTag[V] = rdds.head.vClassTag
+    new JavaPairRDD(sc.union(rdds.map(_.rdd)))
   }&lt;br/&gt;
 &lt;br/&gt;
-  /** Build the union of two or more RDDs. */&lt;br/&gt;
-  override def union(first: JavaDoubleRDD, rest: java.util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;JavaDoubleRDD&amp;#93;&lt;/span&gt;): JavaDoubleRDD = {&lt;br/&gt;
-    val rdds: Seq[RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;Double&amp;#93;&lt;/span&gt;] = (Seq(first) ++ rest.asScala).map(_.srdd)&lt;br/&gt;
-    new JavaDoubleRDD(sc.union(rdds))&lt;br/&gt;
+  /** Build the union of JavaDoubleRDDs. */&lt;br/&gt;
+  @varargs&lt;br/&gt;
+  def union(rdds: JavaDoubleRDD*): JavaDoubleRDD = {
+    require(rdds.nonEmpty, &quot;Union called on no RDDs&quot;)
+    new JavaDoubleRDD(sc.union(rdds.map(_.srdd)))
   }&lt;br/&gt;
 &lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] integer variable, which tasks can &quot;add&quot; values&lt;br/&gt;
-   * to using the `add` method. Only the master can access the accumulator&apos;s `value`.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use sc().longAccumulator()&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def intAccumulator(initialValue: Int): Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Integer&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    sc.accumulator(initialValue)(IntAccumulatorParam).asInstanceOf[Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Integer&amp;#93;&lt;/span&gt;]&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] integer variable, which tasks can &quot;add&quot; values&lt;br/&gt;
-   * to using the `add` method. Only the master can access the accumulator&apos;s `value`.&lt;br/&gt;
-   *&lt;br/&gt;
-   * This version supports naming the accumulator for display in Spark&apos;s web UI.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use sc().longAccumulator(String)&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def intAccumulator(initialValue: Int, name: String): Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Integer&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    sc.accumulator(initialValue, name)(IntAccumulatorParam)&lt;br/&gt;
-      .asInstanceOf[Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Integer&amp;#93;&lt;/span&gt;]&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] double variable, which tasks can &quot;add&quot; values&lt;br/&gt;
-   * to using the `add` method. Only the master can access the accumulator&apos;s `value`.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use sc().doubleAccumulator()&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def doubleAccumulator(initialValue: Double): Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Double&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    sc.accumulator(initialValue)(DoubleAccumulatorParam).asInstanceOf[Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Double&amp;#93;&lt;/span&gt;]&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] double variable, which tasks can &quot;add&quot; values&lt;br/&gt;
-   * to using the `add` method. Only the master can access the accumulator&apos;s `value`.&lt;br/&gt;
-   *&lt;br/&gt;
-   * This version supports naming the accumulator for display in Spark&apos;s web UI.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use sc().doubleAccumulator(String)&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def doubleAccumulator(initialValue: Double, name: String): Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Double&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    sc.accumulator(initialValue, name)(DoubleAccumulatorParam)&lt;br/&gt;
-      .asInstanceOf[Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Double&amp;#93;&lt;/span&gt;]&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] integer variable, which tasks can &quot;add&quot; values&lt;br/&gt;
-   * to using the `add` method. Only the master can access the accumulator&apos;s `value`.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use sc().longAccumulator()&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulator(initialValue: Int): Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Integer&amp;#93;&lt;/span&gt; = intAccumulator(initialValue)&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] integer variable, which tasks can &quot;add&quot; values&lt;br/&gt;
-   * to using the `add` method. Only the master can access the accumulator&apos;s `value`.&lt;br/&gt;
-   *&lt;br/&gt;
-   * This version supports naming the accumulator for display in Spark&apos;s web UI.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use sc().longAccumulator(String)&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulator(initialValue: Int, name: String): Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Integer&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    intAccumulator(initialValue, name)&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] double variable, which tasks can &quot;add&quot; values&lt;br/&gt;
-   * to using the `add` method. Only the master can access the accumulator&apos;s `value`.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use sc().doubleAccumulator()&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulator(initialValue: Double): Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Double&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    doubleAccumulator(initialValue)&lt;br/&gt;
-&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] double variable, which tasks can &quot;add&quot; values&lt;br/&gt;
-   * to using the `add` method. Only the master can access the accumulator&apos;s `value`.&lt;br/&gt;
-   *&lt;br/&gt;
-   * This version supports naming the accumulator for display in Spark&apos;s web UI.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use sc().doubleAccumulator(String)&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulator(initialValue: Double, name: String): Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;java.lang.Double&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    doubleAccumulator(initialValue, name)&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] variable of a given type, which tasks can &quot;add&quot;&lt;br/&gt;
-   * values to using the `add` method. Only the master can access the accumulator&apos;s `value`.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(initialValue: T, accumulatorParam: AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;): Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    sc.accumulator(initialValue)(accumulatorParam)&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulator&amp;#93;&lt;/span&gt;] variable of a given type, which tasks can &quot;add&quot;&lt;br/&gt;
-   * values to using the `add` method. Only the master can access the accumulator&apos;s `value`.&lt;br/&gt;
-   *&lt;br/&gt;
-   * This version supports naming the accumulator for display in Spark&apos;s web UI.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(initialValue: T, name: String, accumulatorParam: AccumulatorParam&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;)&lt;br/&gt;
-      : Accumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    sc.accumulator(initialValue, name)(accumulatorParam)&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulable&amp;#93;&lt;/span&gt;] shared variable of the given type, to which tasks&lt;br/&gt;
-   * can &quot;add&quot; values with `add`. Only the master can access the accumulable&apos;s `value`.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;T, R&amp;#93;&lt;/span&gt;(initialValue: T, param: AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;T, R&amp;#93;&lt;/span&gt;): Accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;T, R&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    sc.accumulable(initialValue)(param)&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Create an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulable&amp;#93;&lt;/span&gt;] shared variable of the given type, to which tasks&lt;br/&gt;
-   * can &quot;add&quot; values with `add`. Only the master can access the accumulable&apos;s `value`.&lt;br/&gt;
-   *&lt;br/&gt;
-   * This version supports naming the accumulator for display in Spark&apos;s web UI.&lt;br/&gt;
-   */&lt;br/&gt;
-  @deprecated(&quot;use AccumulatorV2&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;T, R&amp;#93;&lt;/span&gt;(initialValue: T, name: String, param: AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;T, R&amp;#93;&lt;/span&gt;)&lt;br/&gt;
-      : Accumulable&lt;span class=&quot;error&quot;&gt;&amp;#91;T, R&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-    sc.accumulable(initialValue, name)(param)&lt;br/&gt;
-&lt;br/&gt;
   /**&lt;br/&gt;
    * Broadcast a read-only variable to the cluster, returning a&lt;br/&gt;
    * [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.broadcast.Broadcast&amp;#93;&lt;/span&gt;] object for reading it in distributed functions.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala b/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala&lt;br/&gt;
index 6259bead3ea88..2ab8add63efae 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonHadoopUtil.scala&lt;br/&gt;
@@ -43,7 +43,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;python&amp;#93;&lt;/span&gt; object Converter extends Logging {&lt;br/&gt;
                   defaultConverter: Converter&lt;span class=&quot;error&quot;&gt;&amp;#91;Any, Any&amp;#93;&lt;/span&gt;): Converter&lt;span class=&quot;error&quot;&gt;&amp;#91;Any, Any&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     converterClass.map { cc =&amp;gt;&lt;br/&gt;
       Try {
-        val c = Utils.classForName(cc).newInstance().asInstanceOf[Converter[Any, Any]]
+        val c = Utils.classForName(cc).getConstructor().
+          newInstance().asInstanceOf[Converter[Any, Any]]
         logInfo(s&quot;Loaded converter: $cc&quot;)
         c
       } match {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala b/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala&lt;br/&gt;
index e639a842754bd..5ed5070558af7 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonRDD.scala&lt;br/&gt;
@@ -24,8 +24,10 @@ import java.util.{ArrayList =&amp;gt; JArrayList, List =&amp;gt; JList, Map =&amp;gt; JMap}&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable&lt;br/&gt;
+import scala.concurrent.Promise&lt;br/&gt;
+import scala.concurrent.duration.Duration&lt;br/&gt;
 import scala.language.existentials&lt;br/&gt;
-import scala.util.control.NonFatal&lt;br/&gt;
+import scala.util.Try&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.hadoop.conf.Configuration&lt;br/&gt;
 import org.apache.hadoop.io.compress.CompressionCodec&lt;br/&gt;
@@ -37,6 +39,7 @@ import org.apache.spark.api.java.{JavaPairRDD, JavaRDD, JavaSparkContext}&lt;br/&gt;
 import org.apache.spark.broadcast.Broadcast&lt;br/&gt;
 import org.apache.spark.input.PortableDataStream&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
+import org.apache.spark.network.util.JavaUtils&lt;br/&gt;
 import org.apache.spark.rdd.RDD&lt;br/&gt;
 import org.apache.spark.security.SocketAuthHelper&lt;br/&gt;
 import org.apache.spark.util._&lt;br/&gt;
@@ -169,27 +172,34 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object PythonRDD extends Logging {&lt;br/&gt;
 &lt;br/&gt;
   def readRDDFromFile(sc: JavaSparkContext, filename: String, parallelism: Int):&lt;br/&gt;
   JavaRDD[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;] = {
-    val file = new DataInputStream(new FileInputStream(filename))
+    readRDDFromInputStream(sc.sc, new FileInputStream(filename), parallelism)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def readRDDFromInputStream(&lt;br/&gt;
+      sc: SparkContext,&lt;br/&gt;
+      in: InputStream,&lt;br/&gt;
+      parallelism: Int): JavaRDD[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
+    val din = new DataInputStream(in)&lt;br/&gt;
     try {&lt;br/&gt;
       val objs = new mutable.ArrayBuffer[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;]&lt;br/&gt;
       try {&lt;br/&gt;
         while (true) {
-          val length = file.readInt()
+          val length = din.readInt()
           val obj = new Array[Byte](length)
-          file.readFully(obj)
+          din.readFully(obj)
           objs += obj
         }&lt;br/&gt;
       } catch {
         case eof: EOFException =&amp;gt; // No-op
       }&lt;br/&gt;
-      JavaRDD.fromRDD(sc.sc.parallelize(objs, parallelism))&lt;br/&gt;
+      JavaRDD.fromRDD(sc.parallelize(objs, parallelism))&lt;br/&gt;
     } finally {
-      file.close()
+      din.close()
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  def readBroadcastFromFile(sc: JavaSparkContext, path: String): Broadcast&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonBroadcast&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    sc.broadcast(new PythonBroadcast(path))&lt;br/&gt;
+  def setupBroadcast(path: String): PythonBroadcast = {
+    new PythonBroadcast(path)
   }&lt;br/&gt;
 &lt;br/&gt;
   def writeIteratorToStream&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(iter: Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;, dataOut: DataOutputStream) {&lt;br/&gt;
@@ -419,34 +429,15 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object PythonRDD extends Logging {&lt;br/&gt;
    */&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def serveToStream(&lt;br/&gt;
       threadName: String)(writeFunc: OutputStream =&amp;gt; Unit): Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Any&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    val serverSocket = new ServerSocket(0, 1, InetAddress.getByName(&quot;localhost&quot;))&lt;br/&gt;
-    // Close the socket if no connection in 15 seconds&lt;br/&gt;
-    serverSocket.setSoTimeout(15000)&lt;br/&gt;
-&lt;br/&gt;
-    new Thread(threadName) {&lt;br/&gt;
-      setDaemon(true)&lt;br/&gt;
-      override def run() {&lt;br/&gt;
-        try {&lt;br/&gt;
-          val sock = serverSocket.accept()&lt;br/&gt;
-          authHelper.authClient(sock)&lt;br/&gt;
-&lt;br/&gt;
-          val out = new BufferedOutputStream(sock.getOutputStream)&lt;br/&gt;
-          Utils.tryWithSafeFinally {
-            writeFunc(out)
-          } {
-            out.close()
-            sock.close()
-          }&lt;br/&gt;
-        } catch {
-          case NonFatal(e) =&amp;gt;
-            logError(s&quot;Error while sending iterator&quot;, e)
-        } finally {
-          serverSocket.close()
-        }&lt;br/&gt;
+    val (port, secret) = PythonServer.setupOneConnectionServer(authHelper, threadName) { s =&amp;gt;&lt;br/&gt;
+      val out = new BufferedOutputStream(s.getOutputStream())&lt;br/&gt;
+      Utils.tryWithSafeFinally {
+        writeFunc(out)
+      } {
+        out.close()
       }&lt;br/&gt;
-    }.start()&lt;br/&gt;
-&lt;br/&gt;
-    Array(serverSocket.getLocalPort, authHelper.secret)&lt;br/&gt;
+    }&lt;br/&gt;
+    Array(port, secret)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def getMergedConf(confAsMap: java.util.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
@@ -664,13 +655,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonAccumulatorV2(&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
-/**&lt;br/&gt;
- * A Wrapper for Python Broadcast, which is written into disk by Python. It also will&lt;br/&gt;
- * write the data into disk after deserialization, then Python can read it from disks.&lt;br/&gt;
- */&lt;br/&gt;
 // scalastyle:off no.finalize&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonBroadcast(@transient var path: String) extends Serializable&lt;br/&gt;
-  with Logging {&lt;br/&gt;
+    with Logging {
+
+  private var encryptionServer: PythonServer[Unit] = null
+  private var decryptionServer: PythonServer[Unit] = null
 
   /**
    * Read data from disks, then copy it to `out`
@@ -713,5 +703,255 @@ private[spark] class PythonBroadcast(@transient var path: String) extends Serial
     }&lt;br/&gt;
     super.finalize()&lt;br/&gt;
   }&lt;br/&gt;
+&lt;br/&gt;
+  def setupEncryptionServer(): Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Any&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    encryptionServer = new PythonServer&lt;span class=&quot;error&quot;&gt;&amp;#91;Unit&amp;#93;&lt;/span&gt;(&quot;broadcast-encrypt-server&quot;) {&lt;br/&gt;
+      override def handleConnection(sock: Socket): Unit = {
+        val env = SparkEnv.get
+        val in = sock.getInputStream()
+        val abspath = new File(path).getAbsolutePath
+        val out = env.serializerManager.wrapForEncryption(new FileOutputStream(abspath))
+        DechunkedInputStream.dechunkAndCopyToOutput(in, out)
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    Array(encryptionServer.port, encryptionServer.secret)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def setupDecryptionServer(): Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Any&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    decryptionServer = new PythonServer&lt;span class=&quot;error&quot;&gt;&amp;#91;Unit&amp;#93;&lt;/span&gt;(&quot;broadcast-decrypt-server-for-driver&quot;) {&lt;br/&gt;
+      override def handleConnection(sock: Socket): Unit = {&lt;br/&gt;
+        val out = new DataOutputStream(new BufferedOutputStream(sock.getOutputStream()))&lt;br/&gt;
+        Utils.tryWithSafeFinally {&lt;br/&gt;
+          val in = SparkEnv.get.serializerManager.wrapForEncryption(new FileInputStream(path))&lt;br/&gt;
+          Utils.tryWithSafeFinally {
+            Utils.copyStream(in, out, false)
+          } {
+            in.close()
+          }&lt;br/&gt;
+          out.flush()&lt;br/&gt;
+        } {
+          JavaUtils.closeQuietly(out)
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    Array(decryptionServer.port, decryptionServer.secret)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def waitTillBroadcastDataSent(): Unit = decryptionServer.getResult()&lt;br/&gt;
+&lt;br/&gt;
+  def waitTillDataReceived(): Unit = encryptionServer.getResult()&lt;br/&gt;
 }&lt;br/&gt;
 // scalastyle:on no.finalize&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * The inverse of pyspark&apos;s ChunkedStream for sending data of unknown size.&lt;br/&gt;
+ *&lt;br/&gt;
+ * We might be serializing a really large object from python &amp;#8211; we don&apos;t want&lt;br/&gt;
+ * python to buffer the whole thing in memory, nor can it write to a file,&lt;br/&gt;
+ * so we don&apos;t know the length in advance.  So python writes it in chunks, each chunk&lt;br/&gt;
+ * preceeded by a length, till we get a &quot;length&quot; of -1 which serves as EOF.&lt;br/&gt;
+ *&lt;br/&gt;
+ * Tested from python tests.&lt;br/&gt;
+ */&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class DechunkedInputStream(wrapped: InputStream) extends InputStream with Logging {&lt;br/&gt;
+  private val din = new DataInputStream(wrapped)&lt;br/&gt;
+  private var remainingInChunk = din.readInt()&lt;br/&gt;
+&lt;br/&gt;
+  override def read(): Int = {&lt;br/&gt;
+    val into = new Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;(1)&lt;br/&gt;
+    val n = read(into, 0, 1)&lt;br/&gt;
+    if (n == -1) {
+      -1
+    } else {&lt;br/&gt;
+      // if you just cast a byte to an int, then anything &amp;gt; 127 is negative, which is interpreted&lt;br/&gt;
+      // as an EOF&lt;br/&gt;
+      val b = into(0)&lt;br/&gt;
+      if (b &amp;lt; 0) {
+        256 + b
+      } else {
+        b
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  override def read(dest: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;, off: Int, len: Int): Int = {&lt;br/&gt;
+    if (remainingInChunk == -1) {
+      return -1
+    }&lt;br/&gt;
+    var destSpace = len&lt;br/&gt;
+    var destPos = off&lt;br/&gt;
+    while (destSpace &amp;gt; 0 &amp;amp;&amp;amp; remainingInChunk != -1) {&lt;br/&gt;
+      val toCopy = math.min(remainingInChunk, destSpace)&lt;br/&gt;
+      val read = din.read(dest, destPos, toCopy)&lt;br/&gt;
+      destPos += read&lt;br/&gt;
+      destSpace -= read&lt;br/&gt;
+      remainingInChunk -= read&lt;br/&gt;
+      if (remainingInChunk == 0) {
+        remainingInChunk = din.readInt()
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    assert(destSpace == 0 || remainingInChunk == -1)&lt;br/&gt;
+    return destPos - off&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  override def close(): Unit = wrapped.close()&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object DechunkedInputStream {&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Dechunks the input, copies to output, and closes both input and the output safely.&lt;br/&gt;
+   */&lt;br/&gt;
+  def dechunkAndCopyToOutput(chunked: InputStream, out: OutputStream): Unit = {&lt;br/&gt;
+    val dechunked = new DechunkedInputStream(chunked)&lt;br/&gt;
+    Utils.tryWithSafeFinally {
+      Utils.copyStream(dechunked, out)
+    } {
+      JavaUtils.closeQuietly(out)
+      JavaUtils.closeQuietly(dechunked)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Creates a server in the jvm to communicate with python for handling one batch of data, with&lt;br/&gt;
+ * authentication and error handling.&lt;br/&gt;
+ */&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class PythonServer&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(&lt;br/&gt;
+    authHelper: SocketAuthHelper,&lt;br/&gt;
+    threadName: String) {&lt;br/&gt;
+&lt;br/&gt;
+  def this(env: SparkEnv, threadName: String) = this(new SocketAuthHelper(env.conf), threadName)&lt;br/&gt;
+  def this(threadName: String) = this(SparkEnv.get, threadName)&lt;br/&gt;
+&lt;br/&gt;
+  val (port, secret) = PythonServer.setupOneConnectionServer(authHelper, threadName) { sock =&amp;gt;
+    promise.complete(Try(handleConnection(sock)))
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Handle a connection which has already been authenticated.  Any error from this function&lt;br/&gt;
+   * will clean up this connection and the entire server, and get propogated to [&lt;span class=&quot;error&quot;&gt;&amp;#91;getResult&amp;#93;&lt;/span&gt;].&lt;br/&gt;
+   */&lt;br/&gt;
+  def handleConnection(sock: Socket): T&lt;br/&gt;
+&lt;br/&gt;
+  val promise = Promise&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Blocks indefinitely for [&lt;span class=&quot;error&quot;&gt;&amp;#91;handleConnection&amp;#93;&lt;/span&gt;] to finish, and returns that result.  If&lt;br/&gt;
+   * handleConnection throws an exception, this will throw an exception which includes the original&lt;br/&gt;
+   * exception as a cause.&lt;br/&gt;
+   */&lt;br/&gt;
+  def getResult(): T = {
+    getResult(Duration.Inf)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def getResult(wait: Duration): T = {
+    ThreadUtils.awaitResult(promise.future, wait)
+  }&lt;br/&gt;
+&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object PythonServer {&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Create a socket server and run user function on the socket in a background thread.&lt;br/&gt;
+   *&lt;br/&gt;
+   * The socket server can only accept one connection, or close if no connection&lt;br/&gt;
+   * in 15 seconds.&lt;br/&gt;
+   *&lt;br/&gt;
+   * The thread will terminate after the supplied user function, or if there are any exceptions.&lt;br/&gt;
+   *&lt;br/&gt;
+   * If you need to get a result of the supplied function, create a subclass of [&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonServer&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+   *&lt;br/&gt;
+   * @return The port number of a local socket and the secret for authentication.&lt;br/&gt;
+   */&lt;br/&gt;
+  def setupOneConnectionServer(&lt;br/&gt;
+      authHelper: SocketAuthHelper,&lt;br/&gt;
+      threadName: String)&lt;br/&gt;
+      (func: Socket =&amp;gt; Unit): (Int, String) = {&lt;br/&gt;
+    val serverSocket = new ServerSocket(0, 1, InetAddress.getByAddress(Array(127, 0, 0, 1)))&lt;br/&gt;
+    // Close the socket if no connection in 15 seconds&lt;br/&gt;
+    serverSocket.setSoTimeout(15000)&lt;br/&gt;
+&lt;br/&gt;
+    new Thread(threadName) {&lt;br/&gt;
+      setDaemon(true)&lt;br/&gt;
+      override def run(): Unit = {&lt;br/&gt;
+        var sock: Socket = null&lt;br/&gt;
+        try {
+          sock = serverSocket.accept()
+          authHelper.authClient(sock)
+          func(sock)
+        } finally {
+          JavaUtils.closeQuietly(serverSocket)
+          JavaUtils.closeQuietly(sock)
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }.start()&lt;br/&gt;
+    (serverSocket.getLocalPort, authHelper.secret)&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Sends decrypted broadcast data to python worker.  See [&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonRunner&amp;#93;&lt;/span&gt;] for entire protocol.&lt;br/&gt;
+ */&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class EncryptedPythonBroadcastServer(&lt;br/&gt;
+    val env: SparkEnv,&lt;br/&gt;
+    val idsAndFiles: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;(Long, String)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    extends PythonServer&lt;span class=&quot;error&quot;&gt;&amp;#91;Unit&amp;#93;&lt;/span&gt;(&quot;broadcast-decrypt-server&quot;) with Logging {&lt;br/&gt;
+&lt;br/&gt;
+  override def handleConnection(socket: Socket): Unit = {&lt;br/&gt;
+    val out = new DataOutputStream(new BufferedOutputStream(socket.getOutputStream()))&lt;br/&gt;
+    var socketIn: InputStream = null&lt;br/&gt;
+    // send the broadcast id, then the decrypted data.  We don&apos;t need to send the length, the&lt;br/&gt;
+    // the python pickle module just needs a stream.&lt;br/&gt;
+    Utils.tryWithSafeFinally {&lt;br/&gt;
+      (idsAndFiles).foreach { case (id, path) =&amp;gt;&lt;br/&gt;
+        out.writeLong(id)&lt;br/&gt;
+        val in = env.serializerManager.wrapForEncryption(new FileInputStream(path))&lt;br/&gt;
+        Utils.tryWithSafeFinally {
+          Utils.copyStream(in, out, false)
+        } {
+          in.close()
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+      logTrace(&quot;waiting for python to accept broadcast data over socket&quot;)&lt;br/&gt;
+      out.flush()&lt;br/&gt;
+      socketIn = socket.getInputStream()&lt;br/&gt;
+      socketIn.read()&lt;br/&gt;
+      logTrace(&quot;done serving broadcast data&quot;)&lt;br/&gt;
+    } {
+      JavaUtils.closeQuietly(socketIn)
+      JavaUtils.closeQuietly(out)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def waitTillBroadcastDataSent(): Unit = {
+    getResult()
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Helper for making RDD[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;] from some python data, by reading the data from python&lt;br/&gt;
+ * over a socket.  This is used in preference to writing data to a file when encryption is enabled.&lt;br/&gt;
+ */&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class PythonRDDServer&lt;br/&gt;
+    extends PythonServer[JavaRDD[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;]](&quot;pyspark-parallelize-server&quot;) {&lt;br/&gt;
+&lt;br/&gt;
+  def handleConnection(sock: Socket): JavaRDD[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;] = {
+    val in = sock.getInputStream()
+    val dechunkedInput: InputStream = new DechunkedInputStream(in)
+    streamToRDD(dechunkedInput)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  protected def streamToRDD(input: InputStream): RDD[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonParallelizeServer(sc: SparkContext, parallelism: Int)&lt;br/&gt;
+    extends PythonRDDServer {&lt;br/&gt;
+&lt;br/&gt;
+  override protected def streamToRDD(input: InputStream): RDD[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;] = {
+    PythonRDD.readRDDFromInputStream(sc, input, parallelism)
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala b/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala&lt;br/&gt;
index 4c53bc269a104..f73e95eac8f79 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonRunner.scala&lt;br/&gt;
@@ -106,15 +106,17 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class BasePythonRunner&lt;span class=&quot;error&quot;&gt;&amp;#91;IN, OUT&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       envVars.put(&quot;PYSPARK_EXECUTOR_MEMORY_MB&quot;, memoryMb.get.toString)&lt;br/&gt;
     }&lt;br/&gt;
     val worker: Socket = env.createPythonWorker(pythonExec, envVars.asScala.toMap)&lt;br/&gt;
-    // Whether is the worker released into idle pool&lt;br/&gt;
-    val released = new AtomicBoolean(false)&lt;br/&gt;
+    // Whether is the worker released into idle pool or closed. When any codes try to release or&lt;br/&gt;
+    // close a worker, they should use `releasedOrClosed.compareAndSet` to flip the state to make&lt;br/&gt;
+    // sure there is only one winner that is going to release or close the worker.&lt;br/&gt;
+    val releasedOrClosed = new AtomicBoolean(false)&lt;br/&gt;
 &lt;br/&gt;
     // Start a thread to feed the process input from our parent&apos;s iterator&lt;br/&gt;
     val writerThread = newWriterThread(env, worker, inputIterator, partitionIndex, context)&lt;br/&gt;
 &lt;br/&gt;
     context.addTaskCompletionListener&lt;span class=&quot;error&quot;&gt;&amp;#91;Unit&amp;#93;&lt;/span&gt; { _ =&amp;gt;&lt;br/&gt;
       writerThread.shutdownOnTaskCompletion()&lt;br/&gt;
-      if (!reuseWorker || !released.get) {&lt;br/&gt;
+      if (!reuseWorker || releasedOrClosed.compareAndSet(false, true)) {&lt;br/&gt;
         try {
           worker.close()
         } catch {
@@ -131,7 +133,7 @@ private[spark] abstract class BasePythonRunner[IN, OUT](
     val stream = new DataInputStream(new BufferedInputStream(worker.getInputStream, bufferSize))
 
     val stdoutIterator = newReaderIterator(
-      stream, writerThread, startTime, env, worker, released, context)
+      stream, writerThread, startTime, env, worker, releasedOrClosed, context)
     new InterruptibleIterator(context, stdoutIterator)
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -148,7 +150,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class BasePythonRunner&lt;span class=&quot;error&quot;&gt;&amp;#91;IN, OUT&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       startTime: Long,&lt;br/&gt;
       env: SparkEnv,&lt;br/&gt;
       worker: Socket,&lt;br/&gt;
-      released: AtomicBoolean,&lt;br/&gt;
+      releasedOrClosed: AtomicBoolean,&lt;br/&gt;
       context: TaskContext): Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;OUT&amp;#93;&lt;/span&gt;&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
@@ -289,19 +291,51 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class BasePythonRunner&lt;span class=&quot;error&quot;&gt;&amp;#91;IN, OUT&amp;#93;&lt;/span&gt;(&lt;br/&gt;
         val newBids = broadcastVars.map(_.id).toSet&lt;br/&gt;
         // number of different broadcasts&lt;br/&gt;
         val toRemove = oldBids.diff(newBids)&lt;br/&gt;
-        val cnt = toRemove.size + newBids.diff(oldBids).size&lt;br/&gt;
+        val addedBids = newBids.diff(oldBids)&lt;br/&gt;
+        val cnt = toRemove.size + addedBids.size&lt;br/&gt;
+        val needsDecryptionServer = env.serializerManager.encryptionEnabled &amp;amp;&amp;amp; addedBids.nonEmpty&lt;br/&gt;
+        dataOut.writeBoolean(needsDecryptionServer)&lt;br/&gt;
         dataOut.writeInt(cnt)&lt;br/&gt;
-        for (bid &amp;lt;- toRemove) {&lt;br/&gt;
-          // remove the broadcast from worker&lt;br/&gt;
-          dataOut.writeLong(- bid - 1)  // bid &amp;gt;= 0&lt;br/&gt;
-          oldBids.remove(bid)&lt;br/&gt;
+        def sendBidsToRemove(): Unit = {&lt;br/&gt;
+          for (bid &amp;lt;- toRemove) {
+            // remove the broadcast from worker
+            dataOut.writeLong(-bid - 1) // bid &amp;gt;= 0
+            oldBids.remove(bid)
+          }&lt;br/&gt;
         }&lt;br/&gt;
-        for (broadcast &amp;lt;- broadcastVars) {&lt;br/&gt;
-          if (!oldBids.contains(broadcast.id)) {&lt;br/&gt;
+        if (needsDecryptionServer) {&lt;br/&gt;
+          // if there is encryption, we setup a server which reads the encrypted files, and sends&lt;br/&gt;
+          // the decrypted data to python&lt;br/&gt;
+          val idsAndFiles = broadcastVars.flatMap { broadcast =&amp;gt;&lt;br/&gt;
+            if (!oldBids.contains(broadcast.id)) {
+              Some((broadcast.id, broadcast.value.path))
+            } else {
+              None
+            }&lt;br/&gt;
+          }&lt;br/&gt;
+          val server = new EncryptedPythonBroadcastServer(env, idsAndFiles)&lt;br/&gt;
+          dataOut.writeInt(server.port)&lt;br/&gt;
+          logTrace(s&quot;broadcast decryption server setup on ${server.port}&quot;)&lt;br/&gt;
+          PythonRDD.writeUTF(server.secret, dataOut)&lt;br/&gt;
+          sendBidsToRemove()&lt;br/&gt;
+          idsAndFiles.foreach { case (id, _) =&amp;gt;
             // send new broadcast
-            dataOut.writeLong(broadcast.id)
-            PythonRDD.writeUTF(broadcast.value.path, dataOut)
-            oldBids.add(broadcast.id)
+            dataOut.writeLong(id)
+            oldBids.add(id)
+          }&lt;br/&gt;
+          dataOut.flush()&lt;br/&gt;
+          logTrace(&quot;waiting for python to read decrypted broadcast data from server&quot;)&lt;br/&gt;
+          server.waitTillBroadcastDataSent()&lt;br/&gt;
+          logTrace(&quot;done sending decrypted data to python&quot;)&lt;br/&gt;
+        } else {&lt;br/&gt;
+          sendBidsToRemove()&lt;br/&gt;
+          for (broadcast &amp;lt;- broadcastVars) {&lt;br/&gt;
+            if (!oldBids.contains(broadcast.id)) {
+              // send new broadcast
+              dataOut.writeLong(broadcast.id)
+              PythonRDD.writeUTF(broadcast.value.path, dataOut)
+              oldBids.add(broadcast.id)
+            }&lt;br/&gt;
           }&lt;br/&gt;
         }&lt;br/&gt;
         dataOut.flush()&lt;br/&gt;
@@ -360,7 +394,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class BasePythonRunner&lt;span class=&quot;error&quot;&gt;&amp;#91;IN, OUT&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       startTime: Long,&lt;br/&gt;
       env: SparkEnv,&lt;br/&gt;
       worker: Socket,&lt;br/&gt;
-      released: AtomicBoolean,&lt;br/&gt;
+      releasedOrClosed: AtomicBoolean,&lt;br/&gt;
       context: TaskContext)&lt;br/&gt;
     extends Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;OUT&amp;#93;&lt;/span&gt; {
 
@@ -431,9 +465,8 @@ private[spark] abstract class BasePythonRunner[IN, OUT](
       }&lt;br/&gt;
       // Check whether the worker is ready to be re-used.&lt;br/&gt;
       if (stream.readInt() == SpecialLengths.END_OF_STREAM) {&lt;br/&gt;
-        if (reuseWorker) {&lt;br/&gt;
+        if (reuseWorker &amp;amp;&amp;amp; releasedOrClosed.compareAndSet(false, true)) {
           env.releasePythonWorker(pythonExec, envVars.asScala.toMap, worker)
-          released.set(true)
         }&lt;br/&gt;
       }&lt;br/&gt;
       eos = true&lt;br/&gt;
@@ -533,9 +566,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonRunner(funcs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;ChainedPythonFunctions&amp;#93;&lt;/span&gt;)&lt;br/&gt;
       startTime: Long,&lt;br/&gt;
       env: SparkEnv,&lt;br/&gt;
       worker: Socket,&lt;br/&gt;
-      released: AtomicBoolean,&lt;br/&gt;
+      releasedOrClosed: AtomicBoolean,&lt;br/&gt;
       context: TaskContext): Iterator[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
-    new ReaderIterator(stream, writerThread, startTime, env, worker, released, context) {&lt;br/&gt;
+    new ReaderIterator(stream, writerThread, startTime, env, worker, releasedOrClosed, context) {&lt;br/&gt;
 &lt;br/&gt;
       protected override def read(): Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
         if (writerThread.exception.isDefined) {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala b/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala&lt;br/&gt;
index 27a5e19f96a14..b6b0cac910d69 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonUtils.scala&lt;br/&gt;
@@ -32,7 +32,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object PythonUtils {&lt;br/&gt;
     val pythonPath = new ArrayBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;&lt;br/&gt;
     for (sparkHome &amp;lt;- sys.env.get(&quot;SPARK_HOME&quot;)) {
       pythonPath += Seq(sparkHome, &quot;python&quot;, &quot;lib&quot;, &quot;pyspark.zip&quot;).mkString(File.separator)
-      pythonPath += Seq(sparkHome, &quot;python&quot;, &quot;lib&quot;, &quot;py4j-0.10.7-src.zip&quot;).mkString(File.separator)
+      pythonPath +=
+        Seq(sparkHome, &quot;python&quot;, &quot;lib&quot;, &quot;py4j-0.10.8.1-src.zip&quot;).mkString(File.separator)
     }&lt;br/&gt;
     pythonPath ++= SparkContext.jarOfObject(this)&lt;br/&gt;
     pythonPath.mkString(File.pathSeparator)&lt;br/&gt;
@@ -74,4 +75,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object PythonUtils {&lt;br/&gt;
   def toScalaMap&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(jm: java.util.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt; = {
     jm.asScala.toMap
   }&lt;br/&gt;
+&lt;br/&gt;
+  def getEncryptionEnabled(sc: JavaSparkContext): Boolean = {
+    sc.conf.get(org.apache.spark.internal.config.IO_ENCRYPTION_ENABLED)
+  }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala b/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala&lt;br/&gt;
index 6afa37aa36fd3..1f2f503a28d49 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/python/PythonWorkerFactory.scala&lt;br/&gt;
@@ -21,6 +21,7 @@ import java.io.{DataInputStream, DataOutputStream, EOFException, InputStream, Ou&lt;br/&gt;
 import java.net.{InetAddress, ServerSocket, Socket, SocketException}&lt;br/&gt;
 import java.nio.charset.StandardCharsets&lt;br/&gt;
 import java.util.Arrays&lt;br/&gt;
+import javax.annotation.concurrent.GuardedBy&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable&lt;br/&gt;
@@ -31,7 +32,7 @@ import org.apache.spark.security.SocketAuthHelper&lt;br/&gt;
 import org.apache.spark.util.{RedirectThread, Utils}&lt;br/&gt;
 &lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonWorkerFactory(pythonExec: String, envVars: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
-  extends Logging {&lt;br/&gt;
+  extends Logging { self =&amp;gt;&lt;br/&gt;
 &lt;br/&gt;
   import PythonWorkerFactory._&lt;br/&gt;
 &lt;br/&gt;
@@ -39,7 +40,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonWorkerFactory(pythonExec: String, envVars: Map[String&lt;br/&gt;
   // pyspark/daemon.py (by default) and tell it to fork new workers for our tasks. This daemon&lt;br/&gt;
   // currently only works on UNIX-based systems now because it uses signals for child management,&lt;br/&gt;
   // so we can also fall back to launching workers, pyspark/worker.py (by default) directly.&lt;br/&gt;
-  val useDaemon = {&lt;br/&gt;
+  private val useDaemon = {&lt;br/&gt;
     val useDaemonEnabled = SparkEnv.get.conf.getBoolean(&quot;spark.python.use.daemon&quot;, true)&lt;br/&gt;
 &lt;br/&gt;
     // This flag is ignored on Windows as it&apos;s unable to fork.&lt;br/&gt;
@@ -51,44 +52,52 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonWorkerFactory(pythonExec: String, envVars: Map[String&lt;br/&gt;
   // as expert-only option, and shouldn&apos;t be used before knowing what it means exactly.&lt;br/&gt;
 &lt;br/&gt;
   // This configuration indicates the module to run the daemon to execute its Python workers.&lt;br/&gt;
-  val daemonModule = SparkEnv.get.conf.getOption(&quot;spark.python.daemon.module&quot;).map { value =&amp;gt;
-    logInfo(
-      s&quot;Python daemon module in PySpark is set to [$value] in &apos;spark.python.daemon.module&apos;, &quot; +
-      &quot;using this to start the daemon up. Note that this configuration only has an effect when &quot; +
-      &quot;&apos;spark.python.use.daemon&apos; is enabled and the platform is not Windows.&quot;)
-    value
-  }.getOrElse(&quot;pyspark.daemon&quot;)&lt;br/&gt;
+  private val daemonModule =&lt;br/&gt;
+    SparkEnv.get.conf.getOption(&quot;spark.python.daemon.module&quot;).map { value =&amp;gt;
+      logInfo(
+        s&quot;Python daemon module in PySpark is set to [$value] in &apos;spark.python.daemon.module&apos;, &quot; +
+        &quot;using this to start the daemon up. Note that this configuration only has an effect when &quot; +
+        &quot;&apos;spark.python.use.daemon&apos; is enabled and the platform is not Windows.&quot;)
+      value
+    }.getOrElse(&quot;pyspark.daemon&quot;)&lt;br/&gt;
 &lt;br/&gt;
   // This configuration indicates the module to run each Python worker.&lt;br/&gt;
-  val workerModule = SparkEnv.get.conf.getOption(&quot;spark.python.worker.module&quot;).map { value =&amp;gt;
-    logInfo(
-      s&quot;Python worker module in PySpark is set to [$value] in &apos;spark.python.worker.module&apos;, &quot; +
-      &quot;using this to start the worker up. Note that this configuration only has an effect when &quot; +
-      &quot;&apos;spark.python.use.daemon&apos; is disabled or the platform is Windows.&quot;)
-    value
-  }.getOrElse(&quot;pyspark.worker&quot;)&lt;br/&gt;
+  private val workerModule =&lt;br/&gt;
+    SparkEnv.get.conf.getOption(&quot;spark.python.worker.module&quot;).map { value =&amp;gt;
+      logInfo(
+        s&quot;Python worker module in PySpark is set to [$value] in &apos;spark.python.worker.module&apos;, &quot; +
+        &quot;using this to start the worker up. Note that this configuration only has an effect when &quot; +
+        &quot;&apos;spark.python.use.daemon&apos; is disabled or the platform is Windows.&quot;)
+      value
+    }.getOrElse(&quot;pyspark.worker&quot;)&lt;br/&gt;
 &lt;br/&gt;
   private val authHelper = new SocketAuthHelper(SparkEnv.get.conf)&lt;br/&gt;
 &lt;br/&gt;
-  var daemon: Process = null&lt;br/&gt;
+  @GuardedBy(&quot;self&quot;)&lt;br/&gt;
+  private var daemon: Process = null&lt;br/&gt;
   val daemonHost = InetAddress.getByAddress(Array(127, 0, 0, 1))&lt;br/&gt;
-  var daemonPort: Int = 0&lt;br/&gt;
-  val daemonWorkers = new mutable.WeakHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;Socket, Int&amp;#93;&lt;/span&gt;()&lt;br/&gt;
-  val idleWorkers = new mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;Socket&amp;#93;&lt;/span&gt;()&lt;br/&gt;
-  var lastActivity = 0L&lt;br/&gt;
+  @GuardedBy(&quot;self&quot;)&lt;br/&gt;
+  private var daemonPort: Int = 0&lt;br/&gt;
+  @GuardedBy(&quot;self&quot;)&lt;br/&gt;
+  private val daemonWorkers = new mutable.WeakHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;Socket, Int&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+  @GuardedBy(&quot;self&quot;)&lt;br/&gt;
+  private val idleWorkers = new mutable.Queue&lt;span class=&quot;error&quot;&gt;&amp;#91;Socket&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+  @GuardedBy(&quot;self&quot;)&lt;br/&gt;
+  private var lastActivity = 0L&lt;br/&gt;
   new MonitorThread().start()&lt;br/&gt;
 &lt;br/&gt;
-  var simpleWorkers = new mutable.WeakHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;Socket, Process&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+  @GuardedBy(&quot;self&quot;)&lt;br/&gt;
+  private val simpleWorkers = new mutable.WeakHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;Socket, Process&amp;#93;&lt;/span&gt;()&lt;br/&gt;
 &lt;br/&gt;
-  val pythonPath = PythonUtils.mergePythonPaths(&lt;br/&gt;
+  private val pythonPath = PythonUtils.mergePythonPaths(&lt;br/&gt;
     PythonUtils.sparkPythonPath,&lt;br/&gt;
     envVars.getOrElse(&quot;PYTHONPATH&quot;, &quot;&quot;),&lt;br/&gt;
     sys.env.getOrElse(&quot;PYTHONPATH&quot;, &quot;&quot;))&lt;br/&gt;
 &lt;br/&gt;
   def create(): Socket = {&lt;br/&gt;
     if (useDaemon) {&lt;br/&gt;
-      synchronized {&lt;br/&gt;
-        if (idleWorkers.size &amp;gt; 0) {&lt;br/&gt;
+      self.synchronized {&lt;br/&gt;
+        if (idleWorkers.nonEmpty) {
           return idleWorkers.dequeue()
         }&lt;br/&gt;
       }&lt;br/&gt;
@@ -117,7 +126,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonWorkerFactory(pythonExec: String, envVars: Map[String&lt;br/&gt;
       socket&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    synchronized {&lt;br/&gt;
+    self.synchronized {&lt;br/&gt;
       // Start the daemon if it hasn&apos;t been started&lt;br/&gt;
       startDaemon()&lt;br/&gt;
 &lt;br/&gt;
@@ -163,7 +172,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonWorkerFactory(pythonExec: String, envVars: Map[String&lt;br/&gt;
       try {&lt;br/&gt;
         val socket = serverSocket.accept()&lt;br/&gt;
         authHelper.authClient(socket)&lt;br/&gt;
-        simpleWorkers.put(socket, worker)&lt;br/&gt;
+        self.synchronized {
+          simpleWorkers.put(socket, worker)
+        }&lt;br/&gt;
         return socket&lt;br/&gt;
       } catch {
         case e: Exception =&amp;gt;
@@ -178,7 +189,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   }&lt;br/&gt;
 &lt;br/&gt;
   private def startDaemon() {&lt;br/&gt;
-    synchronized {&lt;br/&gt;
+    self.synchronized {&lt;br/&gt;
       // Is it already running?&lt;br/&gt;
       if (daemon != null) {&lt;br/&gt;
         return&lt;br/&gt;
@@ -278,7 +289,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonWorkerFactory(pythonExec: String, envVars: Map[String&lt;br/&gt;
 &lt;br/&gt;
     override def run() {&lt;br/&gt;
       while (true) {&lt;br/&gt;
-        synchronized {&lt;br/&gt;
+        self.synchronized {&lt;br/&gt;
           if (lastActivity + IDLE_WORKER_TIMEOUT_MS &amp;lt; System.currentTimeMillis()) {
             cleanupIdleWorkers()
             lastActivity = System.currentTimeMillis()
@@ -303,7 +314,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   }&lt;br/&gt;
 &lt;br/&gt;
   private def stopDaemon() {&lt;br/&gt;
-    synchronized {&lt;br/&gt;
+    self.synchronized {&lt;br/&gt;
       if (useDaemon) {
         cleanupIdleWorkers()
 
@@ -325,7 +336,7 @@ private[spark] class PythonWorkerFactory(pythonExec: String, envVars: Map[String
   }&lt;br/&gt;
 &lt;br/&gt;
   def stopWorker(worker: Socket) {&lt;br/&gt;
-    synchronized {&lt;br/&gt;
+    self.synchronized {&lt;br/&gt;
       if (useDaemon) {&lt;br/&gt;
         if (daemon != null) {&lt;br/&gt;
           daemonWorkers.get(worker).foreach { pid =&amp;gt;&lt;br/&gt;
@@ -345,7 +356,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class PythonWorkerFactory(pythonExec: String, envVars: Map[String&lt;br/&gt;
 &lt;br/&gt;
   def releaseWorker(worker: Socket) {&lt;br/&gt;
     if (useDaemon) {&lt;br/&gt;
-      synchronized {&lt;br/&gt;
+      self.synchronized {
         lastActivity = System.currentTimeMillis()
         idleWorkers.enqueue(worker)
       }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/r/RBackend.scala b/core/src/main/scala/org/apache/spark/api/r/RBackend.scala&lt;br/&gt;
index 7ce2581555014..50c8fdf5316d6 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/r/RBackend.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/r/RBackend.scala&lt;br/&gt;
@@ -17,7 +17,7 @@&lt;br/&gt;
 &lt;br/&gt;
 package org.apache.spark.api.r&lt;br/&gt;
 &lt;br/&gt;
-import java.io.{DataInputStream, DataOutputStream, File, FileOutputStream, IOException}&lt;br/&gt;
+import java.io.{DataOutputStream, File, FileOutputStream, IOException}&lt;br/&gt;
 import java.net.{InetAddress, InetSocketAddress, ServerSocket, Socket}&lt;br/&gt;
 import java.util.concurrent.TimeUnit&lt;br/&gt;
 &lt;br/&gt;
@@ -32,8 +32,6 @@ import io.netty.handler.timeout.ReadTimeoutHandler&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.SparkConf&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
-import org.apache.spark.network.util.JavaUtils&lt;br/&gt;
-import org.apache.spark.util.Utils&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
  * Netty-based backend server that is used to communicate between R and Java.&lt;br/&gt;
@@ -99,7 +97,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class RBackend {&lt;br/&gt;
     if (bootstrap != null &amp;amp;&amp;amp; bootstrap.config().group() != null) {
       bootstrap.config().group().shutdownGracefully()
     }&lt;br/&gt;
-    if (bootstrap != null &amp;amp;&amp;amp; bootstrap.childGroup() != null) {&lt;br/&gt;
+    if (bootstrap != null &amp;amp;&amp;amp; bootstrap.config().childGroup() != null) {
       bootstrap.config().childGroup().shutdownGracefully()
     }&lt;br/&gt;
     bootstrap = null&lt;br/&gt;
@@ -147,7 +145,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object RBackend extends Logging {&lt;br/&gt;
       new Thread(&quot;wait for socket to close&quot;) {&lt;br/&gt;
         setDaemon(true)&lt;br/&gt;
         override def run(): Unit = {&lt;br/&gt;
-          // any un-catched exception will also shutdown JVM&lt;br/&gt;
+          // any uncaught exception will also shutdown JVM&lt;br/&gt;
           val buf = new Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;(1024)&lt;br/&gt;
           // shutdown JVM if R does not connect back in 10 seconds&lt;br/&gt;
           serverSocket.setSoTimeout(10000)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/r/RRDD.scala b/core/src/main/scala/org/apache/spark/api/r/RRDD.scala&lt;br/&gt;
index 295355c7bf018..1dc61c7eef33c 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/r/RRDD.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/r/RRDD.scala&lt;br/&gt;
@@ -17,7 +17,9 @@&lt;br/&gt;
 &lt;br/&gt;
 package org.apache.spark.api.r&lt;br/&gt;
 &lt;br/&gt;
-import java.io.File&lt;br/&gt;
+import java.io.{DataInputStream, File}&lt;br/&gt;
+import java.net.Socket&lt;br/&gt;
+import java.nio.charset.StandardCharsets.UTF_8&lt;br/&gt;
 import java.util.{Map =&amp;gt; JMap}&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
@@ -25,10 +27,11 @@ import scala.reflect.ClassTag&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark._&lt;br/&gt;
 import org.apache.spark.api.java.{JavaPairRDD, JavaRDD, JavaSparkContext}&lt;br/&gt;
-import org.apache.spark.api.python.PythonRDD&lt;br/&gt;
+import org.apache.spark.api.python.{PythonRDD, PythonServer}&lt;br/&gt;
 import org.apache.spark.broadcast.Broadcast&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.rdd.RDD&lt;br/&gt;
+import org.apache.spark.security.SocketAuthHelper&lt;br/&gt;
 &lt;br/&gt;
 private abstract class BaseRRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T: ClassTag, U: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     parent: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;,&lt;br/&gt;
@@ -163,3 +166,29 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;r&amp;#93;&lt;/span&gt; object RRDD {
     PythonRDD.readRDDFromFile(jsc, fileName, parallelism)
   }&lt;br/&gt;
 }&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Helper for making RDD[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;] from some R data, by reading the data from R&lt;br/&gt;
+ * over a socket. This is used in preference to writing data to a file when encryption is enabled.&lt;br/&gt;
+ */&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class RParallelizeServer(sc: JavaSparkContext, parallelism: Int)&lt;br/&gt;
+    extends PythonServer[JavaRDD[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;]](&lt;br/&gt;
+      new RSocketAuthHelper(), &quot;sparkr-parallelize-server&quot;) {&lt;br/&gt;
+&lt;br/&gt;
+  override def handleConnection(sock: Socket): JavaRDD[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;] = {
+    val in = sock.getInputStream()
+    PythonRDD.readRDDFromInputStream(sc.sc, in, parallelism)
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class RSocketAuthHelper extends SocketAuthHelper(SparkEnv.get.conf) {&lt;br/&gt;
+  override protected def readUtf8(s: Socket): String = {
+    val din = new DataInputStream(s.getInputStream())
+    val len = din.readInt()
+    val bytes = new Array[Byte](len)
+    din.readFully(bytes)
+    // The R code adds a null terminator to serialized strings, so ignore it here.
+    assert(bytes(bytes.length - 1) == 0) // sanity check.
+    new String(bytes, 0, bytes.length - 1, UTF_8)
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/r/RUtils.scala b/core/src/main/scala/org/apache/spark/api/r/RUtils.scala&lt;br/&gt;
index fdd8cf62f0e5f..9bf35af1da925 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/r/RUtils.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/r/RUtils.scala&lt;br/&gt;
@@ -21,6 +21,8 @@ import java.io.File&lt;br/&gt;
 import java.util.Arrays&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.{SparkEnv, SparkException}&lt;br/&gt;
+import org.apache.spark.api.java.JavaSparkContext&lt;br/&gt;
+import org.apache.spark.api.python.PythonUtils&lt;br/&gt;
 &lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object RUtils {&lt;br/&gt;
   // Local path where R binary packages built from R source code contained in the spark&lt;br/&gt;
@@ -104,4 +106,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object RUtils {
       case e: Exception =&amp;gt; false
     }&lt;br/&gt;
   }&lt;br/&gt;
+&lt;br/&gt;
+  def getEncryptionEnabled(sc: JavaSparkContext): Boolean = PythonUtils.getEncryptionEnabled(sc)&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/api/r/SerDe.scala b/core/src/main/scala/org/apache/spark/api/r/SerDe.scala&lt;br/&gt;
index 537ab57f9664d..6e0a3f63988d4 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/api/r/SerDe.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/api/r/SerDe.scala&lt;br/&gt;
@@ -74,9 +74,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object SerDe {&lt;br/&gt;
       jvmObjectTracker: JVMObjectTracker): Object = {&lt;br/&gt;
     dataType match {&lt;br/&gt;
       case &apos;n&apos; =&amp;gt; null&lt;br/&gt;
-      case &apos;i&apos; =&amp;gt; new java.lang.Integer(readInt(dis))&lt;br/&gt;
-      case &apos;d&apos; =&amp;gt; new java.lang.Double(readDouble(dis))&lt;br/&gt;
-      case &apos;b&apos; =&amp;gt; new java.lang.Boolean(readBoolean(dis))&lt;br/&gt;
+      case &apos;i&apos; =&amp;gt; java.lang.Integer.valueOf(readInt(dis))&lt;br/&gt;
+      case &apos;d&apos; =&amp;gt; java.lang.Double.valueOf(readDouble(dis))&lt;br/&gt;
+      case &apos;b&apos; =&amp;gt; java.lang.Boolean.valueOf(readBoolean(dis))&lt;br/&gt;
       case &apos;c&apos; =&amp;gt; readString(dis)&lt;br/&gt;
       case &apos;e&apos; =&amp;gt; readMap(dis, jvmObjectTracker)&lt;br/&gt;
       case &apos;r&apos; =&amp;gt; readBytes(dis)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala b/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala&lt;br/&gt;
index cbd49e070f2eb..26ead57316e18 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/broadcast/TorrentBroadcast.scala&lt;br/&gt;
@@ -18,6 +18,7 @@&lt;br/&gt;
 package org.apache.spark.broadcast&lt;br/&gt;
 &lt;br/&gt;
 import java.io._&lt;br/&gt;
+import java.lang.ref.SoftReference&lt;br/&gt;
 import java.nio.ByteBuffer&lt;br/&gt;
 import java.util.zip.Adler32&lt;br/&gt;
 &lt;br/&gt;
@@ -61,9 +62,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TorrentBroadcast&lt;span class=&quot;error&quot;&gt;&amp;#91;T: ClassTag&amp;#93;&lt;/span&gt;(obj: T, id: Long)&lt;br/&gt;
    * Value of the broadcast object on executors. This is reconstructed by [&lt;span class=&quot;error&quot;&gt;&amp;#91;readBroadcastBlock&amp;#93;&lt;/span&gt;],&lt;br/&gt;
    * which builds this value by reading blocks from the driver and/or other executors.&lt;br/&gt;
    *&lt;br/&gt;
-   * On the driver, if the value is required, it is read lazily from the block manager.&lt;br/&gt;
+   * On the driver, if the value is required, it is read lazily from the block manager. We hold&lt;br/&gt;
+   * a soft reference so that it can be garbage collected if required, as we can always reconstruct&lt;br/&gt;
+   * in the future.&lt;br/&gt;
    */&lt;br/&gt;
-  @transient private lazy val _value: T = readBroadcastBlock()&lt;br/&gt;
+  @transient private var _value: SoftReference&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = _&lt;br/&gt;
 &lt;br/&gt;
   /** The compression codec to use, or None if compression is disabled */&lt;br/&gt;
   @transient private var compressionCodec: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;CompressionCodec&amp;#93;&lt;/span&gt; = _&lt;br/&gt;
@@ -92,8 +95,15 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TorrentBroadcast&lt;span class=&quot;error&quot;&gt;&amp;#91;T: ClassTag&amp;#93;&lt;/span&gt;(obj: T, id: Long)&lt;br/&gt;
   /** The checksum for all the blocks. */&lt;br/&gt;
   private var checksums: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; = _&lt;br/&gt;
 &lt;br/&gt;
-  override protected def getValue() = {&lt;br/&gt;
-    _value&lt;br/&gt;
+  override protected def getValue() = synchronized {&lt;br/&gt;
+    val memoized: T = if (_value == null) null.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; else _value.get&lt;br/&gt;
+    if (memoized != null) {
+      memoized
+    } else {
+      val newlyRead = readBroadcastBlock()
+      _value = new SoftReference[T](newlyRead)
+      newlyRead
+    }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def calcChecksum(block: ByteBuffer): Int = {
@@ -205,8 +215,8 @@ private[spark] class TorrentBroadcast[T: ClassTag](obj: T, id: Long)
   }&lt;br/&gt;
 &lt;br/&gt;
   private def readBroadcastBlock(): T = Utils.tryOrIOException {&lt;br/&gt;
-    TorrentBroadcast.synchronized {&lt;br/&gt;
-      val broadcastCache = SparkEnv.get.broadcastManager.cachedValues&lt;br/&gt;
+    val broadcastCache = SparkEnv.get.broadcastManager.cachedValues&lt;br/&gt;
+    broadcastCache.synchronized {&lt;br/&gt;
 &lt;br/&gt;
       Option(broadcastCache.get(broadcastId)).map(_.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;).getOrElse {&lt;br/&gt;
         setConf(SparkEnv.get.conf)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala b/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala&lt;br/&gt;
index 178bdcfccb603..5a17a6b6e169c 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/DependencyUtils.scala&lt;br/&gt;
@@ -61,11 +61,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;deploy&amp;#93;&lt;/span&gt; object DependencyUtils extends Logging {&lt;br/&gt;
       hadoopConf: Configuration,&lt;br/&gt;
       secMgr: SecurityManager): String = {&lt;br/&gt;
     val targetDir = Utils.createTempDir()&lt;br/&gt;
+    val userJarName = userJar.split(File.separatorChar).last&lt;br/&gt;
     Option(jars)&lt;br/&gt;
       .map {
         resolveGlobPaths(_, hadoopConf)
           .split(&quot;,&quot;)
-          .filterNot(_.contains(userJar.split(&quot;/&quot;).last))
+          .filterNot(_.contains(userJarName))
           .mkString(&quot;,&quot;)
       }&lt;br/&gt;
       .filterNot(_ == &quot;&quot;)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala b/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala&lt;br/&gt;
index 4cc0063d010ef..7bb2a419107d6 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala&lt;br/&gt;
@@ -30,24 +30,20 @@ import scala.util.control.NonFatal&lt;br/&gt;
 &lt;br/&gt;
 import com.google.common.primitives.Longs&lt;br/&gt;
 import org.apache.hadoop.conf.Configuration&lt;br/&gt;
-import org.apache.hadoop.fs.{FileStatus, FileSystem, Path, PathFilter}&lt;br/&gt;
+import org.apache.hadoop.fs._&lt;br/&gt;
 import org.apache.hadoop.mapred.JobConf&lt;br/&gt;
 import org.apache.hadoop.security.{Credentials, UserGroupInformation}&lt;br/&gt;
 import org.apache.hadoop.security.token.{Token, TokenIdentifier}&lt;br/&gt;
 import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.{SparkConf, SparkException}&lt;br/&gt;
-import org.apache.spark.annotation.DeveloperApi&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
-import org.apache.spark.internal.config._&lt;br/&gt;
 import org.apache.spark.util.Utils&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
- * :: DeveloperApi ::&lt;br/&gt;
  * Contains util methods to interact with Hadoop from Spark.&lt;br/&gt;
  */&lt;br/&gt;
-@DeveloperApi&lt;br/&gt;
-class SparkHadoopUtil extends Logging {&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SparkHadoopUtil extends Logging {&lt;br/&gt;
   private val sparkConf = new SparkConf(false).loadFromSystemProperties(true)&lt;br/&gt;
   val conf: Configuration = newConfiguration(sparkConf)&lt;br/&gt;
   UserGroupInformation.setConfiguration(conf)&lt;br/&gt;
@@ -387,7 +383,7 @@ class SparkHadoopUtil extends Logging {&lt;br/&gt;
 &lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
-object SparkHadoopUtil {&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object SparkHadoopUtil {&lt;br/&gt;
 &lt;br/&gt;
   private lazy val instance = new SparkHadoopUtil&lt;br/&gt;
 &lt;br/&gt;
@@ -412,20 +408,6 @@ object SparkHadoopUtil {&lt;br/&gt;
 &lt;br/&gt;
   def get: SparkHadoopUtil = instance&lt;br/&gt;
 &lt;br/&gt;
-  /**&lt;br/&gt;
-   * Given an expiration date for the current set of credentials, calculate the time when new&lt;br/&gt;
-   * credentials should be created.&lt;br/&gt;
-   *&lt;br/&gt;
-   * @param expirationDate Drop-dead expiration date&lt;br/&gt;
-   * @param conf Spark configuration&lt;br/&gt;
-   * @return Timestamp when new credentials should be created.&lt;br/&gt;
-   */&lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def nextCredentialRenewalTime(expirationDate: Long, conf: SparkConf): Long = {
-    val ct = System.currentTimeMillis
-    val ratio = conf.get(CREDENTIALS_RENEWAL_INTERVAL_RATIO)
-    (ct + (ratio * (expirationDate - ct))).toLong
-  }&lt;br/&gt;
-&lt;br/&gt;
   /**&lt;br/&gt;
    * Returns a Configuration object with Spark configuration applied on top. Unlike&lt;br/&gt;
    * the instance method, this will always return a Configuration instance, and not a&lt;br/&gt;
@@ -471,4 +453,33 @@ object SparkHadoopUtil {
       hadoopConf.set(key.substring(&quot;spark.hadoop.&quot;.length), value)
     }&lt;br/&gt;
   }&lt;br/&gt;
+&lt;br/&gt;
+  // scalastyle:off line.size.limit&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Create a path that uses replication instead of erasure coding (ec), regardless of the default&lt;br/&gt;
+   * configuration in hdfs for the given path.  This can be helpful as hdfs ec doesn&apos;t support&lt;br/&gt;
+   * hflush(), hsync(), or append()&lt;br/&gt;
+   * &lt;a href=&quot;https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#Limitations&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://hadoop.apache.org/docs/r3.0.0/hadoop-project-dist/hadoop-hdfs/HDFSErasureCoding.html#Limitations&lt;/a&gt;&lt;br/&gt;
+   */&lt;br/&gt;
+  // scalastyle:on line.size.limit&lt;br/&gt;
+  def createNonECFile(fs: FileSystem, path: Path): FSDataOutputStream = {&lt;br/&gt;
+    try {
+      // Use reflection as this uses apis only avialable in hadoop 3
+      val builderMethod = fs.getClass().getMethod(&quot;createFile&quot;, classOf[Path])
+      val builder = builderMethod.invoke(fs, path)
+      val builderCls = builder.getClass()
+      // this may throw a NoSuchMethodException if the path is not on hdfs
+      val replicateMethod = builderCls.getMethod(&quot;replicate&quot;)
+      val buildMethod = builderCls.getMethod(&quot;build&quot;)
+      val b2 = replicateMethod.invoke(builder)
+      buildMethod.invoke(b2).asInstanceOf[FSDataOutputStream]
+    } catch {
+      case  _: NoSuchMethodException =&amp;gt;
+        // No createFile() method, we&apos;re using an older hdfs client, which doesn&apos;t give us control
+        // over EC vs. replication.  Older hdfs doesn&apos;t have EC anyway, so just create a file with
+        // old apis.
+        fs.create(path)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala b/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala&lt;br/&gt;
index cf902db8709e7..324f6f8894d34 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala&lt;br/&gt;
@@ -318,7 +318,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SparkSubmit extends Logging {&lt;br/&gt;
 &lt;br/&gt;
       if (!StringUtils.isBlank(resolvedMavenCoordinates)) {&lt;br/&gt;
         args.jars = mergeFileLists(args.jars, resolvedMavenCoordinates)&lt;br/&gt;
-        if (args.isPython) {&lt;br/&gt;
+        if (args.isPython || isInternal(args.primaryResource)) {
           args.pyFiles = mergeFileLists(args.pyFiles, resolvedMavenCoordinates)
         }&lt;br/&gt;
       }&lt;br/&gt;
@@ -335,7 +335,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SparkSubmit extends Logging {&lt;br/&gt;
     val targetDir = Utils.createTempDir()&lt;br/&gt;
 &lt;br/&gt;
     // assure a keytab is available from any place in a JVM&lt;br/&gt;
-    if (clusterManager == YARN || clusterManager == LOCAL || isMesosClient) {&lt;br/&gt;
+    if (clusterManager == YARN || clusterManager == LOCAL || isMesosClient || isKubernetesCluster) {&lt;br/&gt;
       if (args.principal != null) {&lt;br/&gt;
         if (args.keytab != null) {&lt;br/&gt;
           require(new File(args.keytab).exists(), s&quot;Keytab file: ${args.keytab} does not exist&quot;)&lt;br/&gt;
@@ -520,6 +520,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SparkSubmit extends Logging {&lt;br/&gt;
         confKey = &quot;spark.driver.extraJavaOptions&quot;),&lt;br/&gt;
       OptionAssigner(args.driverExtraLibraryPath, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,&lt;br/&gt;
         confKey = &quot;spark.driver.extraLibraryPath&quot;),&lt;br/&gt;
+      OptionAssigner(args.principal, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,&lt;br/&gt;
+        confKey = PRINCIPAL.key),&lt;br/&gt;
+      OptionAssigner(args.keytab, ALL_CLUSTER_MGRS, ALL_DEPLOY_MODES,&lt;br/&gt;
+        confKey = KEYTAB.key),&lt;br/&gt;
 &lt;br/&gt;
       // Propagate attributes for dependency resolution at the driver side&lt;br/&gt;
       OptionAssigner(args.packages, STANDALONE | MESOS, CLUSTER, confKey = &quot;spark.jars.packages&quot;),&lt;br/&gt;
@@ -537,8 +541,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SparkSubmit extends Logging {&lt;br/&gt;
       OptionAssigner(args.jars, YARN, ALL_DEPLOY_MODES, confKey = &quot;spark.yarn.dist.jars&quot;),&lt;br/&gt;
       OptionAssigner(args.files, YARN, ALL_DEPLOY_MODES, confKey = &quot;spark.yarn.dist.files&quot;),&lt;br/&gt;
       OptionAssigner(args.archives, YARN, ALL_DEPLOY_MODES, confKey = &quot;spark.yarn.dist.archives&quot;),&lt;br/&gt;
-      OptionAssigner(args.principal, YARN, ALL_DEPLOY_MODES, confKey = &quot;spark.yarn.principal&quot;),&lt;br/&gt;
-      OptionAssigner(args.keytab, YARN, ALL_DEPLOY_MODES, confKey = &quot;spark.yarn.keytab&quot;),&lt;br/&gt;
 &lt;br/&gt;
       // Other options&lt;br/&gt;
       OptionAssigner(args.executorCores, STANDALONE | YARN | KUBERNETES, ALL_DEPLOY_MODES,&lt;br/&gt;
@@ -644,7 +646,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SparkSubmit extends Logging {&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    if (clusterManager == MESOS &amp;amp;&amp;amp; UserGroupInformation.isSecurityEnabled) {&lt;br/&gt;
+    if ((clusterManager == MESOS || clusterManager == KUBERNETES)&lt;br/&gt;
+       &amp;amp;&amp;amp; UserGroupInformation.isSecurityEnabled) {
       setRMPrincipal(sparkConf)
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -760,8 +763,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SparkSubmit extends Logging {&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   // &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-20328&quot; title=&quot;HadoopRDDs create a MapReduce JobConf, but are not MapReduce jobs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-20328&quot;&gt;&lt;del&gt;SPARK-20328&lt;/del&gt;&lt;/a&gt;. HadoopRDD calls into a Hadoop library that fetches delegation tokens with&lt;br/&gt;
-  // renewer set to the YARN ResourceManager.  Since YARN isn&apos;t configured in Mesos mode, we&lt;br/&gt;
-  // must trick it into thinking we&apos;re YARN.&lt;br/&gt;
+  // renewer set to the YARN ResourceManager.  Since YARN isn&apos;t configured in Mesos or Kubernetes&lt;br/&gt;
+  // mode, we must trick it into thinking we&apos;re YARN.&lt;br/&gt;
   private def setRMPrincipal(sparkConf: SparkConf): Unit = {&lt;br/&gt;
     val shortUserName = UserGroupInformation.getCurrentUser.getShortUserName&lt;br/&gt;
     val key = s&quot;spark.hadoop.${YarnConfiguration.RM_PRINCIPAL}&quot;&lt;br/&gt;
@@ -826,7 +829,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SparkSubmit extends Logging {&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     val app: SparkApplication = if (classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;SparkApplication&amp;#93;&lt;/span&gt;.isAssignableFrom(mainClass)) {
-      mainClass.newInstance().asInstanceOf[SparkApplication]
+      mainClass.getConstructor().newInstance().asInstanceOf[SparkApplication]
     } else {&lt;br/&gt;
       // &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-4170&quot; title=&quot;Closure problems when running Scala app that &amp;quot;extends App&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-4170&quot;&gt;&lt;del&gt;SPARK-4170&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
       if (classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;scala.App&amp;#93;&lt;/span&gt;.isAssignableFrom(mainClass)) {&lt;br/&gt;
@@ -925,8 +928,6 @@ object SparkSubmit extends CommandLineUtils with Logging {&lt;br/&gt;
         } catch {
           case e: SparkUserAppException =&amp;gt;
             exitFn(e.exitCode)
-          case e: SparkException =&amp;gt;
-            printErrorAndExit(e.getMessage())
         }&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
@@ -991,9 +992,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object SparkSubmitUtils {&lt;br/&gt;
 &lt;br/&gt;
   // Exposed for testing.&lt;br/&gt;
   // These components are used to make the default exclusion rules for Spark dependencies.&lt;br/&gt;
-  // We need to specify each component explicitly, otherwise we miss spark-streaming-kafka-0-8 and&lt;br/&gt;
-  // other spark-streaming utility components. Underscore is there to differentiate between&lt;br/&gt;
-  // spark-streaming_2.1x and spark-streaming-kafka-0-8-assembly_2.1x&lt;br/&gt;
+  // We need to specify each component explicitly, otherwise we miss&lt;br/&gt;
+  // spark-streaming utility components. Underscore is there to differentiate between&lt;br/&gt;
+  // spark-streaming_2.1x and spark-streaming-kafka-0-10-assembly_2.1x&lt;br/&gt;
   val IVY_DEFAULT_EXCLUDES = Seq(&quot;catalyst_&quot;, &quot;core_&quot;, &quot;graphx_&quot;, &quot;kvstore_&quot;, &quot;launcher_&quot;, &quot;mllib_&quot;,&lt;br/&gt;
     &quot;mllib-local_&quot;, &quot;network-common_&quot;, &quot;network-shuffle_&quot;, &quot;repl_&quot;, &quot;sketch_&quot;, &quot;sql_&quot;, &quot;streaming_&quot;,&lt;br/&gt;
     &quot;tags_&quot;, &quot;unsafe_&quot;)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala b/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala&lt;br/&gt;
index 0998757715457..4cf08a7980f55 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/SparkSubmitArguments.scala&lt;br/&gt;
@@ -199,8 +199,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;deploy&amp;#93;&lt;/span&gt; class SparkSubmitArguments(args: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;, env: Map[String, S&lt;br/&gt;
     numExecutors = Option(numExecutors)&lt;br/&gt;
       .getOrElse(sparkProperties.get(&quot;spark.executor.instances&quot;).orNull)&lt;br/&gt;
     queue = Option(queue).orElse(sparkProperties.get(&quot;spark.yarn.queue&quot;)).orNull&lt;br/&gt;
-    keytab = Option(keytab).orElse(sparkProperties.get(&quot;spark.yarn.keytab&quot;)).orNull&lt;br/&gt;
-    principal = Option(principal).orElse(sparkProperties.get(&quot;spark.yarn.principal&quot;)).orNull&lt;br/&gt;
+    keytab = Option(keytab)&lt;br/&gt;
+      .orElse(sparkProperties.get(&quot;spark.kerberos.keytab&quot;))&lt;br/&gt;
+      .orElse(sparkProperties.get(&quot;spark.yarn.keytab&quot;))&lt;br/&gt;
+      .orNull&lt;br/&gt;
+    principal = Option(principal)&lt;br/&gt;
+      .orElse(sparkProperties.get(&quot;spark.kerberos.principal&quot;))&lt;br/&gt;
+      .orElse(sparkProperties.get(&quot;spark.yarn.principal&quot;))&lt;br/&gt;
+      .orNull&lt;br/&gt;
     dynamicAllocationEnabled =&lt;br/&gt;
       sparkProperties.get(&quot;spark.dynamicAllocation.enabled&quot;).exists(&quot;true&quot;.equalsIgnoreCase)&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala b/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala&lt;br/&gt;
index 44d23908146c7..da6e5f03aabb5 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala&lt;br/&gt;
@@ -19,7 +19,6 @@ package org.apache.spark.deploy.history&lt;br/&gt;
 &lt;br/&gt;
 import java.io.{File, FileNotFoundException, IOException}&lt;br/&gt;
 import java.nio.file.Files&lt;br/&gt;
-import java.nio.file.attribute.PosixFilePermissions&lt;br/&gt;
 import java.util.{Date, ServiceLoader}&lt;br/&gt;
 import java.util.concurrent.{ConcurrentHashMap, ExecutorService, Future, TimeUnit}&lt;br/&gt;
 import java.util.zip.{ZipEntry, ZipOutputStream}&lt;br/&gt;
@@ -35,7 +34,7 @@ import com.fasterxml.jackson.annotation.JsonIgnore&lt;br/&gt;
 import com.google.common.io.ByteStreams&lt;br/&gt;
 import com.google.common.util.concurrent.MoreExecutors&lt;br/&gt;
 import org.apache.hadoop.fs.{FileStatus, FileSystem, Path}&lt;br/&gt;
-import org.apache.hadoop.hdfs.DistributedFileSystem&lt;br/&gt;
+import org.apache.hadoop.hdfs.{DFSInputStream, DistributedFileSystem}&lt;br/&gt;
 import org.apache.hadoop.hdfs.protocol.HdfsConstants&lt;br/&gt;
 import org.apache.hadoop.security.AccessControlException&lt;br/&gt;
 import org.fusesource.leveldbjni.internal.NativeDB&lt;br/&gt;
@@ -43,13 +42,15 @@ import org.fusesource.leveldbjni.internal.NativeDB&lt;br/&gt;
 import org.apache.spark.{SecurityManager, SparkConf, SparkException}&lt;br/&gt;
 import org.apache.spark.deploy.SparkHadoopUtil&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
+import org.apache.spark.internal.config.DRIVER_LOG_DFS_DIR&lt;br/&gt;
+import org.apache.spark.internal.config.History._&lt;br/&gt;
+import org.apache.spark.internal.config.Status._&lt;br/&gt;
 import org.apache.spark.io.CompressionCodec&lt;br/&gt;
 import org.apache.spark.scheduler._&lt;br/&gt;
 import org.apache.spark.scheduler.ReplayListenerBus._&lt;br/&gt;
 import org.apache.spark.status._&lt;br/&gt;
 import org.apache.spark.status.KVUtils._&lt;br/&gt;
 import org.apache.spark.status.api.v1.{ApplicationAttemptInfo, ApplicationInfo}&lt;br/&gt;
-import org.apache.spark.status.config._&lt;br/&gt;
 import org.apache.spark.ui.SparkUI&lt;br/&gt;
 import org.apache.spark.util.{Clock, SystemClock, ThreadUtils, Utils}&lt;br/&gt;
 import org.apache.spark.util.kvstore._&lt;br/&gt;
@@ -87,7 +88,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class FsHistoryProvider(conf: SparkConf, clock: Clock)&lt;br/&gt;
     this(conf, new SystemClock())&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  import config._&lt;br/&gt;
   import FsHistoryProvider._&lt;br/&gt;
 &lt;br/&gt;
   // Interval between safemode checks.&lt;br/&gt;
@@ -98,7 +98,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class FsHistoryProvider(conf: SparkConf, clock: Clock)&lt;br/&gt;
   private val UPDATE_INTERVAL_S = conf.getTimeAsSeconds(&quot;spark.history.fs.update.interval&quot;, &quot;10s&quot;)&lt;br/&gt;
 &lt;br/&gt;
   // Interval between each cleaner checks for event logs to delete&lt;br/&gt;
-  private val CLEAN_INTERVAL_S = conf.getTimeAsSeconds(&quot;spark.history.fs.cleaner.interval&quot;, &quot;1d&quot;)&lt;br/&gt;
+  private val CLEAN_INTERVAL_S = conf.get(CLEANER_INTERVAL_S)&lt;br/&gt;
 &lt;br/&gt;
   // Number of threads used to replay event logs.&lt;br/&gt;
   private val NUM_PROCESSING_THREADS = conf.getInt(SPARK_HISTORY_FS_NUM_REPLAY_THREADS,&lt;br/&gt;
@@ -133,9 +133,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class FsHistoryProvider(conf: SparkConf, clock: Clock)&lt;br/&gt;
 &lt;br/&gt;
   // Visible for testing.&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; val listing: KVStore = storePath.map { path =&amp;gt;&lt;br/&gt;
-    val perms = PosixFilePermissions.fromString(&quot;rwx------&quot;)&lt;br/&gt;
-    val dbPath = Files.createDirectories(new File(path, &quot;listing.ldb&quot;).toPath(),&lt;br/&gt;
-      PosixFilePermissions.asFileAttribute(perms)).toFile()&lt;br/&gt;
+    val dbPath = Files.createDirectories(new File(path, &quot;listing.ldb&quot;).toPath()).toFile()&lt;br/&gt;
+    Utils.chmod700(dbPath)&lt;br/&gt;
 &lt;br/&gt;
     val metadata = new FsHistoryProviderMetadata(CURRENT_LISTING_VERSION,&lt;br/&gt;
       AppStatusStore.CURRENT_VERSION, logDir.toString())&lt;br/&gt;
@@ -276,11 +275,18 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class FsHistoryProvider(conf: SparkConf, clock: Clock)&lt;br/&gt;
       pool.scheduleWithFixedDelay(&lt;br/&gt;
         getRunner(() =&amp;gt; checkForLogs()), 0, UPDATE_INTERVAL_S, TimeUnit.SECONDS)&lt;br/&gt;
 &lt;br/&gt;
-      if (conf.getBoolean(&quot;spark.history.fs.cleaner.enabled&quot;, false)) {&lt;br/&gt;
+      if (conf.get(CLEANER_ENABLED)) {
         // A task that periodically cleans event logs on disk.
         pool.scheduleWithFixedDelay(
           getRunner(() =&amp;gt; cleanLogs()), 0, CLEAN_INTERVAL_S, TimeUnit.SECONDS)
       }&lt;br/&gt;
+&lt;br/&gt;
+      if (conf.contains(DRIVER_LOG_DFS_DIR) &amp;amp;&amp;amp; conf.get(DRIVER_LOG_CLEANER_ENABLED)) {
+        pool.scheduleWithFixedDelay(getRunner(() =&amp;gt; cleanDriverLogs()),
+          0,
+          conf.get(DRIVER_LOG_CLEANER_INTERVAL),
+          TimeUnit.SECONDS)
+      }&lt;br/&gt;
     } else {
       logDebug(&quot;Background update thread disabled for testing&quot;)
     }&lt;br/&gt;
@@ -451,10 +457,32 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class FsHistoryProvider(conf: SparkConf, clock: Clock)&lt;br/&gt;
               listing.write(info.copy(lastProcessed = newLastScanTime, fileSize = entry.getLen()))&lt;br/&gt;
             }&lt;br/&gt;
 &lt;br/&gt;
-            if (info.fileSize &amp;lt; entry.getLen()) {&lt;br/&gt;
+            if (shouldReloadLog(info, entry)) {&lt;br/&gt;
               if (info.appId.isDefined &amp;amp;&amp;amp; fastInProgressParsing) {&lt;br/&gt;
                 // When fast in-progress parsing is on, we don&apos;t need to re-parse when the&lt;br/&gt;
                 // size changes, but we do need to invalidate any existing UIs.&lt;br/&gt;
+                // Also, we need to update the `lastUpdated time` to display the updated time in&lt;br/&gt;
+                // the HistoryUI and to avoid cleaning the inprogress app while running.&lt;br/&gt;
+                val appInfo = listing.read(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ApplicationInfoWrapper&amp;#93;&lt;/span&gt;, info.appId.get)&lt;br/&gt;
+&lt;br/&gt;
+                val attemptList = appInfo.attempts.map { attempt =&amp;gt;&lt;br/&gt;
+                  if (attempt.info.attemptId == info.attemptId) {
+                    new AttemptInfoWrapper(
+                      attempt.info.copy(lastUpdated = new Date(newLastScanTime)),
+                      attempt.logPath,
+                      attempt.fileSize,
+                      attempt.adminAcls,
+                      attempt.viewAcls,
+                      attempt.adminAclsGroups,
+                      attempt.viewAclsGroups)
+                  } else {
+                    attempt
+                  }&lt;br/&gt;
+                }&lt;br/&gt;
+&lt;br/&gt;
+                val updatedAppInfo = new ApplicationInfoWrapper(appInfo.info, attemptList)&lt;br/&gt;
+                listing.write(updatedAppInfo)&lt;br/&gt;
+&lt;br/&gt;
                 invalidateUI(info.appId.get, info.attemptId)&lt;br/&gt;
                 false&lt;br/&gt;
               } else {
@@ -468,8 +496,8 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
               // If the file is currently not being tracked by the SHS, add an entry for it and try
               // to parse it. This will allow the cleaner code to detect the file as stale later on
               // if it was not possible to parse it.
-              listing.write(LogInfo(entry.getPath().toString(), newLastScanTime, None, None,
-                entry.getLen()))
+              listing.write(LogInfo(entry.getPath().toString(), newLastScanTime, LogType.EventLogs,
+                None, None, entry.getLen()))
               entry.getLen() &amp;gt; 0
           }&lt;br/&gt;
         }&lt;br/&gt;
@@ -543,6 +571,24 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class FsHistoryProvider(conf: SparkConf, clock: Clock)&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; def shouldReloadLog(info: LogInfo, entry: FileStatus): Boolean = {&lt;br/&gt;
+    var result = info.fileSize &amp;lt; entry.getLen&lt;br/&gt;
+    if (!result &amp;amp;&amp;amp; info.logPath.endsWith(EventLoggingListener.IN_PROGRESS)) {&lt;br/&gt;
+      try {&lt;br/&gt;
+        result = Utils.tryWithResource(fs.open(entry.getPath)) { in =&amp;gt;&lt;br/&gt;
+          in.getWrappedStream match {
+            case dfsIn: DFSInputStream =&amp;gt; info.fileSize &amp;lt; dfsIn.getFileLength
+            case _ =&amp;gt; false
+          }&lt;br/&gt;
+        }&lt;br/&gt;
+      } catch {&lt;br/&gt;
+        case e: Exception =&amp;gt;&lt;br/&gt;
+          logDebug(s&quot;Failed to check the length for the file : ${info.logPath}&quot;, e)&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    result&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   private def cleanAppData(appId: String, attemptId: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;, logPath: String): Unit = {&lt;br/&gt;
     try {
       val app = load(appId)
@@ -708,7 +754,7 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
         // listing data is good.
         invalidateUI(app.info.id, app.attempts.head.info.attemptId)
         addListing(app)
-        listing.write(LogInfo(logPath.toString(), scanTime, Some(app.info.id),
+        listing.write(LogInfo(logPath.toString(), scanTime, LogType.EventLogs, Some(app.info.id),
           app.attempts.head.info.attemptId, fileStatus.getLen()))
 
         // For a finished log, remove the corresponding &quot;in progress&quot; entry from the listing DB if
@@ -737,7 +783,8 @@ private[history] class FsHistoryProvider(conf: SparkConf, clock: Clock)
         // If the app hasn&apos;t written down its app ID to the logs, still record the entry in the
         // listing db, with an empty ID. This will make the log eligible for deletion if the app
         // does not make progress after the configured max log age.
-        listing.write(LogInfo(logPath.toString(), scanTime, None, None, fileStatus.getLen()))
+        listing.write(
+          LogInfo(logPath.toString(), scanTime, LogType.EventLogs, None, None, fileStatus.getLen()))
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -782,7 +829,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class FsHistoryProvider(conf: SparkConf, clock: Clock)&lt;br/&gt;
         val logPath = new Path(logDir, attempt.logPath)&lt;br/&gt;
         listing.delete(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogInfo&amp;#93;&lt;/span&gt;, logPath.toString())&lt;br/&gt;
         cleanAppData(app.id, attempt.info.attemptId, logPath.toString())&lt;br/&gt;
-        deleteLog(logPath)&lt;br/&gt;
+        deleteLog(fs, logPath)&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
       if (remaining.isEmpty) {&lt;br/&gt;
@@ -796,11 +843,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class FsHistoryProvider(conf: SparkConf, clock: Clock)&lt;br/&gt;
       .reverse()&lt;br/&gt;
       .first(maxTime)&lt;br/&gt;
       .asScala&lt;br/&gt;
+      .filter { l =&amp;gt; l.logType == null || l.logType == LogType.EventLogs }&lt;br/&gt;
       .toList&lt;br/&gt;
     stale.foreach { log =&amp;gt;&lt;br/&gt;
       if (log.appId.isEmpty) {&lt;br/&gt;
         logInfo(s&quot;Deleting invalid / corrupt event log ${log.logPath}&quot;)&lt;br/&gt;
-        deleteLog(new Path(log.logPath))&lt;br/&gt;
+        deleteLog(fs, new Path(log.logPath))&lt;br/&gt;
         listing.delete(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogInfo&amp;#93;&lt;/span&gt;, log.logPath)&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
@@ -808,6 +856,61 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class FsHistoryProvider(conf: SparkConf, clock: Clock)&lt;br/&gt;
     clearBlacklist(CLEAN_INTERVAL_S)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * Delete driver logs from the configured spark dfs dir that exceed the configured max age&lt;br/&gt;
+   */&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; def cleanDriverLogs(): Unit = Utils.tryLog {&lt;br/&gt;
+    val driverLogDir = conf.get(DRIVER_LOG_DFS_DIR).get&lt;br/&gt;
+    val driverLogFs = new Path(driverLogDir).getFileSystem(hadoopConf)&lt;br/&gt;
+    val currentTime = clock.getTimeMillis()&lt;br/&gt;
+    val maxTime = currentTime - conf.get(MAX_DRIVER_LOG_AGE_S) * 1000&lt;br/&gt;
+    val logFiles = driverLogFs.listLocatedStatus(new Path(driverLogDir))&lt;br/&gt;
+    while (logFiles.hasNext()) {&lt;br/&gt;
+      val f = logFiles.next()&lt;br/&gt;
+      // Do not rely on &apos;modtime&apos; as it is not updated for all filesystems when files are written to&lt;br/&gt;
+      val deleteFile =&lt;br/&gt;
+        try {&lt;br/&gt;
+          val info = listing.read(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogInfo&amp;#93;&lt;/span&gt;, f.getPath().toString())&lt;br/&gt;
+          // Update the lastprocessedtime of file if it&apos;s length or modification time has changed&lt;br/&gt;
+          if (info.fileSize &amp;lt; f.getLen() || info.lastProcessed &amp;lt; f.getModificationTime()) {
+            listing.write(
+              info.copy(lastProcessed = currentTime, fileSize = f.getLen()))
+            false
+          } else if (info.lastProcessed &amp;gt; maxTime) {
+            false
+          } else {
+            true
+          }&lt;br/&gt;
+        } catch {
+          case e: NoSuchElementException =&amp;gt;
+            // For every new driver log file discovered, create a new entry in listing
+            listing.write(LogInfo(f.getPath().toString(), currentTime, LogType.DriverLogs, None,
+              None, f.getLen()))
+          false
+        }&lt;br/&gt;
+      if (deleteFile) {&lt;br/&gt;
+        logInfo(s&quot;Deleting expired driver log for: ${f.getPath().getName()}&quot;)&lt;br/&gt;
+        listing.delete(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogInfo&amp;#93;&lt;/span&gt;, f.getPath().toString())&lt;br/&gt;
+        deleteLog(driverLogFs, f.getPath())&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    // Delete driver log file entries that exceed the configured max age and&lt;br/&gt;
+    // may have been deleted on filesystem externally.&lt;br/&gt;
+    val stale = listing.view(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogInfo&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+      .index(&quot;lastProcessed&quot;)&lt;br/&gt;
+      .reverse()&lt;br/&gt;
+      .first(maxTime)&lt;br/&gt;
+      .asScala&lt;br/&gt;
+      .filter { l =&amp;gt; l.logType != null &amp;amp;&amp;amp; l.logType == LogType.DriverLogs }&lt;br/&gt;
+      .toList&lt;br/&gt;
+    stale.foreach { log =&amp;gt;&lt;br/&gt;
+      logInfo(s&quot;Deleting invalid driver log ${log.logPath}&quot;)&lt;br/&gt;
+      listing.delete(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LogInfo&amp;#93;&lt;/span&gt;, log.logPath)&lt;br/&gt;
+      deleteLog(driverLogFs, new Path(log.logPath))&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
    * Rebuilds the application state store from its event log.&lt;br/&gt;
    */&lt;br/&gt;
@@ -964,7 +1067,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class FsHistoryProvider(conf: SparkConf, clock: Clock)&lt;br/&gt;
       throw new NoSuchElementException(s&quot;Cannot find attempt $attemptId of $appId.&quot;))&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def deleteLog(log: Path): Unit = {&lt;br/&gt;
+  private def deleteLog(fs: FileSystem, log: Path): Unit = {&lt;br/&gt;
     if (isBlacklisted(log)) {
       logDebug(s&quot;Skipping deleting $log as we don&apos;t have permissions on it.&quot;)
     } else {&lt;br/&gt;
@@ -1009,6 +1112,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; case class FsHistoryProviderMetadata(&lt;br/&gt;
     uiVersion: Long,&lt;br/&gt;
     logDir: String)&lt;br/&gt;
 &lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; object LogType extends Enumeration {
+  val DriverLogs, EventLogs = Value
+}&lt;br/&gt;
+&lt;br/&gt;
 /**&lt;br/&gt;
  * Tracking info for event logs detected in the configured log directory. Tracks both valid and&lt;br/&gt;
  * invalid logs (e.g. unparseable logs, recorded as logs with no app ID) so that the cleaner&lt;br/&gt;
@@ -1017,6 +1124,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; case class FsHistoryProviderMetadata(&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; case class LogInfo(&lt;br/&gt;
     @KVIndexParam logPath: String,&lt;br/&gt;
     @KVIndexParam(&quot;lastProcessed&quot;) lastProcessed: Long,&lt;br/&gt;
+    logType: LogType.Value,&lt;br/&gt;
     appId: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     attemptId: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     fileSize: Long)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala b/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala&lt;br/&gt;
index 32667ddf5c7ea..00ca4efa4d266 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/history/HistoryPage.scala&lt;br/&gt;
@@ -31,8 +31,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class HistoryPage(parent: HistoryServer) extends WebUIPage(&quot;&quot;)&lt;br/&gt;
     val requestedIncomplete =&lt;br/&gt;
       Option(UIUtils.stripXSS(request.getParameter(&quot;showIncomplete&quot;))).getOrElse(&quot;false&quot;).toBoolean&lt;br/&gt;
 &lt;br/&gt;
-    val allAppsSize = parent.getApplicationList()&lt;br/&gt;
-      .count(isApplicationCompleted(_) != requestedIncomplete)&lt;br/&gt;
+    val displayApplications = parent.getApplicationList()&lt;br/&gt;
+      .exists(isApplicationCompleted(_) != requestedIncomplete)&lt;br/&gt;
     val eventLogsUnderProcessCount = parent.getEventLogsUnderProcess()&lt;br/&gt;
     val lastUpdatedTime = parent.getLastUpdatedTime()&lt;br/&gt;
     val providerConfig = parent.getProviderConfig()&lt;br/&gt;
@@ -63,9 +63,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class HistoryPage(parent: HistoryServer) extends WebUIPage(&quot;&quot;)&lt;br/&gt;
             }&lt;br/&gt;
 &lt;br/&gt;
             {&lt;br/&gt;
-            if (allAppsSize &amp;gt; 0) {&lt;br/&gt;
+            if (displayApplications) {&lt;br/&gt;
               &amp;lt;script src={UIUtils.prependBaseUri(
-                  request, &quot;/static/dataTables.rowsGroup.js&quot;)}&amp;gt;&amp;lt;/script&amp;gt; ++&lt;br/&gt;
+                request, &quot;/static/dataTables.rowsGroup.js&quot;)}&amp;gt;&amp;lt;/script&amp;gt; ++&lt;br/&gt;
                 &amp;lt;div id=&quot;history-summary&quot; class=&quot;row-fluid&quot;&amp;gt;&amp;lt;/div&amp;gt; ++&lt;br/&gt;
                 &amp;lt;script src={UIUtils.prependBaseUri(request, &quot;/static/historypage.js&quot;)}&amp;gt;&amp;lt;/script&amp;gt; ++&lt;br/&gt;
                 &amp;lt;script&amp;gt;setAppLimit({parent.maxApplications})&amp;lt;/script&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala&lt;br/&gt;
index 56f3f59504a7d..5856c7057b745 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServer.scala&lt;br/&gt;
@@ -28,9 +28,9 @@ import org.eclipse.jetty.servlet.{ServletContextHandler, ServletHolder}&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.{SecurityManager, SparkConf}&lt;br/&gt;
 import org.apache.spark.deploy.SparkHadoopUtil&lt;br/&gt;
-import org.apache.spark.deploy.history.config.HISTORY_SERVER_UI_PORT&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.internal.config._&lt;br/&gt;
+import org.apache.spark.internal.config.History.HISTORY_SERVER_UI_PORT&lt;br/&gt;
 import org.apache.spark.status.api.v1.{ApiRootResource, ApplicationInfo, UIRoot}&lt;br/&gt;
 import org.apache.spark.ui.{SparkUI, UIUtils, WebUI}&lt;br/&gt;
 import org.apache.spark.ui.JettyUtils._&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala&lt;br/&gt;
index 080ba12c2f0d1..49f00cb10179e 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerArguments.scala&lt;br/&gt;
@@ -34,35 +34,21 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class HistoryServerArguments(conf: SparkConf, args: Array[Strin&lt;br/&gt;
 &lt;br/&gt;
   @tailrec&lt;br/&gt;
   private def parse(args: List&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
-    if (args.length == 1) {
-      setLogDirectory(args.head)
-    } else {&lt;br/&gt;
-      args match {&lt;br/&gt;
-        case (&quot;--dir&quot; | &quot;-d&quot;) :: value :: tail =&amp;gt;&lt;br/&gt;
-          setLogDirectory(value)&lt;br/&gt;
-          parse(tail)&lt;br/&gt;
+    args match {
+      case (&quot;--help&quot; | &quot;-h&quot;) :: tail =&amp;gt;
+        printUsageAndExit(0)
 
-        case (&quot;--help&quot; | &quot;-h&quot;) :: tail =&amp;gt;
-          printUsageAndExit(0)
+      case (&quot;--properties-file&quot;) :: value :: tail =&amp;gt;
+        propertiesFile = value
+        parse(tail)
 
-        case (&quot;--properties-file&quot;) :: value :: tail =&amp;gt;
-          propertiesFile = value
-          parse(tail)
+      case Nil =&amp;gt;
 
-        case Nil =&amp;gt;
-
-        case _ =&amp;gt;
-          printUsageAndExit(1)
-      }&lt;br/&gt;
+      case _ =&amp;gt;&lt;br/&gt;
+        printUsageAndExit(1)&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def setLogDirectory(value: String): Unit = {
-    logWarning(&quot;Setting log directory through the command line is deprecated as of &quot; +
-      &quot;Spark 1.1.0. Please set this through spark.history.fs.logDirectory instead.&quot;)
-    conf.set(&quot;spark.history.fs.logDirectory&quot;, value)
-  }&lt;br/&gt;
-&lt;br/&gt;
    // This mutates the SparkConf, so all accesses to it must be made after this line&lt;br/&gt;
    Utils.loadDefaultSparkProperties(conf, propertiesFile)&lt;br/&gt;
 &lt;br/&gt;
@@ -73,8 +59,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;history&amp;#93;&lt;/span&gt; class HistoryServerArguments(conf: SparkConf, args: Array[Strin&lt;br/&gt;
       |Usage: HistoryServer &lt;span class=&quot;error&quot;&gt;&amp;#91;options&amp;#93;&lt;/span&gt;&lt;br/&gt;
       |&lt;br/&gt;
       |Options:&lt;br/&gt;
-      |  DIR                         Deprecated; set spark.history.fs.logDirectory directly&lt;br/&gt;
-      |  --dir DIR (-d DIR)          Deprecated; set spark.history.fs.logDirectory directly&lt;br/&gt;
       |  --properties-file FILE      Path to a custom Spark properties file.&lt;br/&gt;
       |                              Default is conf/spark-defaults.conf.&lt;br/&gt;
       |&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala&lt;br/&gt;
index c03a360b91ef8..0a1f33395ad62 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/history/HistoryServerDiskManager.scala&lt;br/&gt;
@@ -18,8 +18,6 @@&lt;br/&gt;
 package org.apache.spark.deploy.history&lt;br/&gt;
 &lt;br/&gt;
 import java.io.File&lt;br/&gt;
-import java.nio.file.Files&lt;br/&gt;
-import java.nio.file.attribute.PosixFilePermissions&lt;br/&gt;
 import java.util.concurrent.atomic.AtomicLong&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
@@ -29,6 +27,7 @@ import org.apache.commons.io.FileUtils&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.SparkConf&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
+import org.apache.spark.internal.config.History._&lt;br/&gt;
 import org.apache.spark.status.KVUtils._&lt;br/&gt;
 import org.apache.spark.util.{Clock, Utils}&lt;br/&gt;
 import org.apache.spark.util.kvstore.KVStore&lt;br/&gt;
@@ -52,8 +51,6 @@ private class HistoryServerDiskManager(&lt;br/&gt;
     listing: KVStore,&lt;br/&gt;
     clock: Clock) extends Logging {&lt;br/&gt;
 &lt;br/&gt;
-  import config._&lt;br/&gt;
-&lt;br/&gt;
   private val appStoreDir = new File(path, &quot;apps&quot;)&lt;br/&gt;
   if (!appStoreDir.isDirectory() &amp;amp;&amp;amp; !appStoreDir.mkdir()) {&lt;br/&gt;
     throw new IllegalArgumentException(s&quot;Failed to create app directory ($appStoreDir).&quot;)&lt;br/&gt;
@@ -107,9 +104,8 @@ private class HistoryServerDiskManager(&lt;br/&gt;
     val needed = approximateSize(eventLogSize, isCompressed)&lt;br/&gt;
     makeRoom(needed)&lt;br/&gt;
 &lt;br/&gt;
-    val perms = PosixFilePermissions.fromString(&quot;rwx------&quot;)&lt;br/&gt;
-    val tmp = Files.createTempDirectory(tmpStoreDir.toPath(), &quot;appstore&quot;,&lt;br/&gt;
-      PosixFilePermissions.asFileAttribute(perms)).toFile()&lt;br/&gt;
+    val tmp = Utils.createTempDir(tmpStoreDir.getPath(), &quot;appstore&quot;)&lt;br/&gt;
+    Utils.chmod700(tmp)&lt;br/&gt;
 &lt;br/&gt;
     updateUsage(needed)&lt;br/&gt;
     val current = currentUsage.get()&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala b/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala&lt;br/&gt;
index 31a8e3e60c067..afa413fe165df 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/rest/RestSubmissionClient.scala&lt;br/&gt;
@@ -408,6 +408,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class RestSubmissionClient(master: String) extends Logging {&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object RestSubmissionClient {&lt;br/&gt;
+&lt;br/&gt;
+  // SPARK_HOME and SPARK_CONF_DIR are filtered out because they are usually wrong&lt;br/&gt;
+  // on the remote machine (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-12345&quot; title=&quot;Mesos cluster mode is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-12345&quot;&gt;&lt;del&gt;SPARK-12345&lt;/del&gt;&lt;/a&gt;) (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25934&quot; title=&quot;Mesos: SPARK_CONF_DIR should not be propogated by spark submit&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25934&quot;&gt;&lt;del&gt;SPARK-25934&lt;/del&gt;&lt;/a&gt;)&lt;br/&gt;
+  private val BLACKLISTED_SPARK_ENV_VARS = Set(&quot;SPARK_ENV_LOADED&quot;, &quot;SPARK_HOME&quot;, &quot;SPARK_CONF_DIR&quot;)&lt;br/&gt;
   private val REPORT_DRIVER_STATUS_INTERVAL = 1000&lt;br/&gt;
   private val REPORT_DRIVER_STATUS_MAX_TRIES = 10&lt;br/&gt;
   val PROTOCOL_VERSION = &quot;v1&quot;&lt;br/&gt;
@@ -417,9 +421,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object RestSubmissionClient {&lt;br/&gt;
    */&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;rest&amp;#93;&lt;/span&gt; def filterSystemEnvironment(env: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     env.filterKeys { k =&amp;gt;
-      // SPARK_HOME is filtered out because it is usually wrong on the remote machine (SPARK-12345)
-      (k.startsWith(&quot;SPARK_&quot;) &amp;amp;&amp;amp; k != &quot;SPARK_ENV_LOADED&quot; &amp;amp;&amp;amp; k != &quot;SPARK_HOME&quot;) ||
-        k.startsWith(&quot;MESOS_&quot;)
+      (k.startsWith(&quot;SPARK_&quot;) &amp;amp;&amp;amp; !BLACKLISTED_SPARK_ENV_VARS.contains(k)) || k.startsWith(&quot;MESOS_&quot;)
     }&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala b/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala&lt;br/&gt;
index 22b65abce611a..afa1a5fbba792 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/rest/StandaloneRestServer.scala&lt;br/&gt;
@@ -138,6 +138,16 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;rest&amp;#93;&lt;/span&gt; class StandaloneSubmitRequestServlet(&lt;br/&gt;
     val driverExtraClassPath = sparkProperties.get(&quot;spark.driver.extraClassPath&quot;)&lt;br/&gt;
     val driverExtraLibraryPath = sparkProperties.get(&quot;spark.driver.extraLibraryPath&quot;)&lt;br/&gt;
     val superviseDriver = sparkProperties.get(&quot;spark.driver.supervise&quot;)&lt;br/&gt;
+    // The semantics of &quot;spark.master&quot; and the masterUrl are different. While the&lt;br/&gt;
+    // property &quot;spark.master&quot; could contain all registered masters, masterUrl&lt;br/&gt;
+    // contains only the active master. To make sure a Spark driver can recover&lt;br/&gt;
+    // in a multi-master setup, we use the &quot;spark.master&quot; property while submitting&lt;br/&gt;
+    // the driver.&lt;br/&gt;
+    val masters = sparkProperties.get(&quot;spark.master&quot;)&lt;br/&gt;
+    val (_, masterPort) = Utils.extractHostPortFromSparkUrl(masterUrl)&lt;br/&gt;
+    val masterRestPort = this.conf.getInt(&quot;spark.master.rest.port&quot;, 6066)&lt;br/&gt;
+    val updatedMasters = masters.map(&lt;br/&gt;
+      _.replace(s&quot;:$masterRestPort&quot;, s&quot;:$masterPort&quot;)).getOrElse(masterUrl)&lt;br/&gt;
     val appArgs = request.appArgs&lt;br/&gt;
     // Filter SPARK_LOCAL_(IP|HOSTNAME) environment variables from being set on the remote system.&lt;br/&gt;
     val environmentVariables =&lt;br/&gt;
@@ -146,7 +156,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;rest&amp;#93;&lt;/span&gt; class StandaloneSubmitRequestServlet(&lt;br/&gt;
     // Construct driver description&lt;br/&gt;
     val conf = new SparkConf(false)&lt;br/&gt;
       .setAll(sparkProperties)&lt;br/&gt;
-      .set(&quot;spark.master&quot;, masterUrl)&lt;br/&gt;
+      .set(&quot;spark.master&quot;, updatedMasters)&lt;br/&gt;
     val extraClassPath = driverExtraClassPath.toSeq.flatMap(_.split(File.pathSeparator))&lt;br/&gt;
     val extraLibraryPath = driverExtraLibraryPath.toSeq.flatMap(_.split(File.pathSeparator))&lt;br/&gt;
     val extraJavaOpts = driverExtraJavaOptions.map(Utils.splitCommandString).getOrElse(Seq.empty)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala b/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala&lt;br/&gt;
index ef5a7e35ad562..97b689cdadd5f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/rest/SubmitRestProtocolMessage.scala&lt;br/&gt;
@@ -36,7 +36,7 @@ import org.apache.spark.util.Utils&lt;br/&gt;
  *   (2) the Spark version of the client / server&lt;br/&gt;
  *   (3) an optional message&lt;br/&gt;
  */&lt;br/&gt;
-@JsonInclude(Include.NON_NULL)&lt;br/&gt;
+@JsonInclude(Include.NON_ABSENT)&lt;br/&gt;
 @JsonAutoDetect(getterVisibility = Visibility.ANY, setterVisibility = Visibility.ANY)&lt;br/&gt;
 @JsonPropertyOrder(alphabetic = true)&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;rest&amp;#93;&lt;/span&gt; abstract class SubmitRestProtocolMessage {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala b/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala&lt;br/&gt;
index ab8d8d96a9b08..126a6ab801369 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/security/HadoopDelegationTokenManager.scala&lt;br/&gt;
@@ -17,76 +17,158 @@&lt;br/&gt;
 &lt;br/&gt;
 package org.apache.spark.deploy.security&lt;br/&gt;
 &lt;br/&gt;
+import java.io.File&lt;br/&gt;
+import java.security.PrivilegedExceptionAction&lt;br/&gt;
+import java.util.concurrent.{ScheduledExecutorService, TimeUnit}&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicReference&lt;br/&gt;
+&lt;br/&gt;
 import org.apache.hadoop.conf.Configuration&lt;br/&gt;
 import org.apache.hadoop.fs.FileSystem&lt;br/&gt;
-import org.apache.hadoop.security.Credentials&lt;br/&gt;
+import org.apache.hadoop.security.{Credentials, UserGroupInformation}&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.SparkConf&lt;br/&gt;
+import org.apache.spark.deploy.SparkHadoopUtil&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
+import org.apache.spark.internal.config._&lt;br/&gt;
+import org.apache.spark.rpc.RpcEndpointRef&lt;br/&gt;
+import org.apache.spark.scheduler.cluster.CoarseGrainedClusterMessages.UpdateDelegationTokens&lt;br/&gt;
+import org.apache.spark.ui.UIUtils&lt;br/&gt;
+import org.apache.spark.util.ThreadUtils&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
- * Manages all the registered HadoopDelegationTokenProviders and offer APIs for other modules to&lt;br/&gt;
- * obtain delegation tokens and their renewal time. By default [&lt;span class=&quot;error&quot;&gt;&amp;#91;HadoopFSDelegationTokenProvider&amp;#93;&lt;/span&gt;],&lt;br/&gt;
- * [&lt;span class=&quot;error&quot;&gt;&amp;#91;HiveDelegationTokenProvider&amp;#93;&lt;/span&gt;] and [&lt;span class=&quot;error&quot;&gt;&amp;#91;HBaseDelegationTokenProvider&amp;#93;&lt;/span&gt;] will be loaded in if not&lt;br/&gt;
- * explicitly disabled.&lt;br/&gt;
+ * Manager for delegation tokens in a Spark application.&lt;br/&gt;
+ *&lt;br/&gt;
+ * This manager has two modes of operation:&lt;br/&gt;
+ *&lt;br/&gt;
+ * 1.  When configured with a principal and a keytab, it will make sure long-running apps can run&lt;br/&gt;
+ * without interruption while accessing secured services. It periodically logs in to the KDC with&lt;br/&gt;
+ * user-provided credentials, and contacts all the configured secure services to obtain delegation&lt;br/&gt;
+ * tokens to be distributed to the rest of the application.&lt;br/&gt;
+ *&lt;br/&gt;
+ * Because the Hadoop UGI API does not expose the TTL of the TGT, a configuration controls how often&lt;br/&gt;
+ * to check that a relogin is necessary. This is done reasonably often since the check is a no-op&lt;br/&gt;
+ * when the relogin is not yet needed. The check period can be overridden in the configuration.&lt;br/&gt;
  *&lt;br/&gt;
- * Also, each HadoopDelegationTokenProvider is controlled by&lt;br/&gt;
- * spark.security.credentials.{service}.enabled, and will not be loaded if this config is set to&lt;br/&gt;
- * false. For example, Hive&apos;s delegation token provider [&lt;span class=&quot;error&quot;&gt;&amp;#91;HiveDelegationTokenProvider&amp;#93;&lt;/span&gt;] can be&lt;br/&gt;
- * enabled/disabled by the configuration spark.security.credentials.hive.enabled.&lt;br/&gt;
+ * New delegation tokens are created once 75% of the renewal interval of the original tokens has&lt;br/&gt;
+ * elapsed. The new tokens are sent to the Spark driver endpoint once it&apos;s registered with the AM.&lt;br/&gt;
+ * The driver is tasked with distributing the tokens to other processes that might need them.&lt;br/&gt;
  *&lt;br/&gt;
- * @param sparkConf Spark configuration&lt;br/&gt;
- * @param hadoopConf Hadoop configuration&lt;br/&gt;
- * @param fileSystems Delegation tokens will be fetched for these Hadoop filesystems.&lt;br/&gt;
+ * 2. When operating without an explicit principal and keytab, token renewal will not be available.&lt;br/&gt;
+ * Starting the manager will distribute an initial set of delegation tokens to the provided Spark&lt;br/&gt;
+ * driver, but the app will not get new tokens when those expire.&lt;br/&gt;
+ *&lt;br/&gt;
+ * It can also be used just to create delegation tokens, by calling the `obtainDelegationTokens`&lt;br/&gt;
+ * method. This option does not require calling the `start` method, but leaves it up to the&lt;br/&gt;
+ * caller to distribute the tokens that were generated.&lt;br/&gt;
  */&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class HadoopDelegationTokenManager(&lt;br/&gt;
-    sparkConf: SparkConf,&lt;br/&gt;
-    hadoopConf: Configuration,&lt;br/&gt;
-    fileSystems: Configuration =&amp;gt; Set&lt;span class=&quot;error&quot;&gt;&amp;#91;FileSystem&amp;#93;&lt;/span&gt;)&lt;br/&gt;
-  extends Logging {&lt;br/&gt;
+    protected val sparkConf: SparkConf,&lt;br/&gt;
+    protected val hadoopConf: Configuration) extends Logging {&lt;br/&gt;
 &lt;br/&gt;
   private val deprecatedProviderEnabledConfigs = List(&lt;br/&gt;
     &quot;spark.yarn.security.tokens.%s.enabled&quot;,&lt;br/&gt;
     &quot;spark.yarn.security.credentials.%s.enabled&quot;)&lt;br/&gt;
   private val providerEnabledConfig = &quot;spark.security.credentials.%s.enabled&quot;&lt;br/&gt;
 &lt;br/&gt;
-  // Maintain all the registered delegation token providers&lt;br/&gt;
-  private val delegationTokenProviders = getDelegationTokenProviders&lt;br/&gt;
+  private val principal = sparkConf.get(PRINCIPAL).orNull&lt;br/&gt;
+  private val keytab = sparkConf.get(KEYTAB).orNull&lt;br/&gt;
+&lt;br/&gt;
+  require((principal == null) == (keytab == null),&lt;br/&gt;
+    &quot;Both principal and keytab must be defined, or neither.&quot;)&lt;br/&gt;
+  require(keytab == null || new File(keytab).isFile(), s&quot;Cannot find keytab at $keytab.&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  private val delegationTokenProviders = loadProviders()&lt;br/&gt;
   logDebug(&quot;Using the following builtin delegation token providers: &quot; +&lt;br/&gt;
     s&quot;${delegationTokenProviders.keys.mkString(&quot;, &quot;)}.&quot;)&lt;br/&gt;
 &lt;br/&gt;
-  /** Construct a [&lt;span class=&quot;error&quot;&gt;&amp;#91;HadoopDelegationTokenManager&amp;#93;&lt;/span&gt;] for the default Hadoop filesystem */&lt;br/&gt;
-  def this(sparkConf: SparkConf, hadoopConf: Configuration) = {&lt;br/&gt;
-    this(&lt;br/&gt;
-      sparkConf,&lt;br/&gt;
-      hadoopConf,&lt;br/&gt;
-      hadoopConf =&amp;gt; Set(FileSystem.get(hadoopConf).getHomeDirectory.getFileSystem(hadoopConf)))&lt;br/&gt;
+  private var renewalExecutor: ScheduledExecutorService = _&lt;br/&gt;
+  private val driverRef = new AtomicReference&lt;span class=&quot;error&quot;&gt;&amp;#91;RpcEndpointRef&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+&lt;br/&gt;
+  /** Set the endpoint used to send tokens to the driver. */&lt;br/&gt;
+  def setDriverRef(ref: RpcEndpointRef): Unit = {
+    driverRef.set(ref)
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def getDelegationTokenProviders: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, HadoopDelegationTokenProvider&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    val providers = Seq(new HadoopFSDelegationTokenProvider(fileSystems)) ++&lt;br/&gt;
-      safeCreateProvider(new HiveDelegationTokenProvider) ++&lt;br/&gt;
-      safeCreateProvider(new HBaseDelegationTokenProvider)&lt;br/&gt;
+  /** @return Whether delegation token renewal is enabled. */&lt;br/&gt;
+  def renewalEnabled: Boolean = principal != null&lt;br/&gt;
 &lt;br/&gt;
-    // Filter out providers for which spark.security.credentials.{service}.enabled is false.&lt;br/&gt;
-    providers&lt;br/&gt;
-      .filter { p =&amp;gt; isServiceEnabled(p.serviceName) }&lt;br/&gt;
-      .map { p =&amp;gt; (p.serviceName, p) }&lt;br/&gt;
-      .toMap&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Start the token renewer. Requires a principal and keytab. Upon start, the renewer will:&lt;br/&gt;
+   *&lt;br/&gt;
+   * - log in the configured principal, and set up a task to keep that user&apos;s ticket renewed&lt;br/&gt;
+   * - obtain delegation tokens from all available providers&lt;br/&gt;
+   * - send the tokens to the driver, if it&apos;s already registered&lt;br/&gt;
+   * - schedule a periodic task to update the tokens when needed.&lt;br/&gt;
+   *&lt;br/&gt;
+   * @return The newly logged in user.&lt;br/&gt;
+   */&lt;br/&gt;
+  def start(): UserGroupInformation = {&lt;br/&gt;
+    require(renewalEnabled, &quot;Token renewal must be enabled to start the renewer.&quot;)&lt;br/&gt;
+    renewalExecutor =&lt;br/&gt;
+      ThreadUtils.newDaemonSingleThreadScheduledExecutor(&quot;Credential Renewal Thread&quot;)&lt;br/&gt;
+&lt;br/&gt;
+    val originalCreds = UserGroupInformation.getCurrentUser().getCredentials()&lt;br/&gt;
+    val ugi = doLogin()&lt;br/&gt;
+&lt;br/&gt;
+    val tgtRenewalTask = new Runnable() {&lt;br/&gt;
+      override def run(): Unit = {
+        ugi.checkTGTAndReloginFromKeytab()
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    val tgtRenewalPeriod = sparkConf.get(KERBEROS_RELOGIN_PERIOD)&lt;br/&gt;
+    renewalExecutor.scheduleAtFixedRate(tgtRenewalTask, tgtRenewalPeriod, tgtRenewalPeriod,&lt;br/&gt;
+      TimeUnit.SECONDS)&lt;br/&gt;
+&lt;br/&gt;
+    val creds = obtainTokensAndScheduleRenewal(ugi)&lt;br/&gt;
+    ugi.addCredentials(creds)&lt;br/&gt;
+&lt;br/&gt;
+    val driver = driverRef.get()&lt;br/&gt;
+    if (driver != null) {
+      val tokens = SparkHadoopUtil.get.serialize(creds)
+      driver.send(UpdateDelegationTokens(tokens))
+    }&lt;br/&gt;
+&lt;br/&gt;
+    // Transfer the original user&apos;s tokens to the new user, since it may contain needed tokens&lt;br/&gt;
+    // (such as those user to connect to YARN). Explicitly avoid overwriting tokens that already&lt;br/&gt;
+    // exist in the current user&apos;s credentials, since those were freshly obtained above&lt;br/&gt;
+    // (see &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-23361&quot; title=&quot;Driver restart fails if it happens after 7 days from app submission&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-23361&quot;&gt;&lt;del&gt;SPARK-23361&lt;/del&gt;&lt;/a&gt;).&lt;br/&gt;
+    val existing = ugi.getCredentials()&lt;br/&gt;
+    existing.mergeAll(originalCreds)&lt;br/&gt;
+    ugi.addCredentials(existing)&lt;br/&gt;
+    ugi&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def safeCreateProvider(&lt;br/&gt;
-      createFn: =&amp;gt; HadoopDelegationTokenProvider): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;HadoopDelegationTokenProvider&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    try {
-      Some(createFn)
-    } catch {&lt;br/&gt;
-      case t: Throwable =&amp;gt;&lt;br/&gt;
-        logDebug(s&quot;Failed to load built in provider.&quot;, t)&lt;br/&gt;
-        None&lt;br/&gt;
+  def stop(): Unit = {&lt;br/&gt;
+    if (renewalExecutor != null) {
+      renewalExecutor.shutdown()
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  def isServiceEnabled(serviceName: String): Boolean = {&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Fetch new delegation tokens for configured services, storing them in the given credentials.&lt;br/&gt;
+   * Tokens are fetched for the current logged in user.&lt;br/&gt;
+   *&lt;br/&gt;
+   * @param creds Credentials object where to store the delegation tokens.&lt;br/&gt;
+   * @return The time by which the tokens must be renewed.&lt;br/&gt;
+   */&lt;br/&gt;
+  def obtainDelegationTokens(creds: Credentials): Long = {&lt;br/&gt;
+    delegationTokenProviders.values.flatMap { provider =&amp;gt;&lt;br/&gt;
+      if (provider.delegationTokensRequired(sparkConf, hadoopConf)) {
+        provider.obtainDelegationTokens(hadoopConf, sparkConf, creds)
+      } else {&lt;br/&gt;
+        logDebug(s&quot;Service ${provider.serviceName} does not require a token.&quot; +&lt;br/&gt;
+          s&quot; Check your configuration to see if security is disabled or not.&quot;)&lt;br/&gt;
+        None&lt;br/&gt;
+      }&lt;br/&gt;
+    }.foldLeft(Long.MaxValue)(math.min)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  // Visible for testing.&lt;br/&gt;
+  def isProviderLoaded(serviceName: String): Boolean = {
+    delegationTokenProviders.contains(serviceName)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  protected def isServiceEnabled(serviceName: String): Boolean = {&lt;br/&gt;
     val key = providerEnabledConfig.format(serviceName)&lt;br/&gt;
 &lt;br/&gt;
     deprecatedProviderEnabledConfigs.foreach { pattern =&amp;gt;
@@ -110,32 +192,107 @@ private[spark] class HadoopDelegationTokenManager(
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-   * Get delegation token provider for the specified service.&lt;br/&gt;
+   * List of file systems for which to obtain delegation tokens. The base implementation&lt;br/&gt;
+   * returns just the default file system in the given Hadoop configuration.&lt;br/&gt;
    */&lt;br/&gt;
-  def getServiceDelegationTokenProvider(service: String): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;HadoopDelegationTokenProvider&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    delegationTokenProviders.get(service)&lt;br/&gt;
+  protected def fileSystemsToAccess(): Set&lt;span class=&quot;error&quot;&gt;&amp;#91;FileSystem&amp;#93;&lt;/span&gt; = {
+    Set(FileSystem.get(hadoopConf))
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def scheduleRenewal(delay: Long): Unit = {&lt;br/&gt;
+    val _delay = math.max(0, delay)&lt;br/&gt;
+    logInfo(s&quot;Scheduling login from keytab in ${UIUtils.formatDuration(delay)}.&quot;)&lt;br/&gt;
+&lt;br/&gt;
+    val renewalTask = new Runnable() {&lt;br/&gt;
+      override def run(): Unit = {
+        updateTokensTask()
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    renewalExecutor.schedule(renewalTask, _delay, TimeUnit.MILLISECONDS)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-   * Writes delegation tokens to creds.  Delegation tokens are fetched from all registered&lt;br/&gt;
-   * providers.&lt;br/&gt;
-   *&lt;br/&gt;
-   * @param hadoopConf hadoop Configuration&lt;br/&gt;
-   * @param creds Credentials that will be updated in place (overwritten)&lt;br/&gt;
-   * @return Time after which the fetched delegation tokens should be renewed.&lt;br/&gt;
+   * Periodic task to login to the KDC and create new delegation tokens. Re-schedules itself&lt;br/&gt;
+   * to fetch the next set of tokens when needed.&lt;br/&gt;
    */&lt;br/&gt;
-  def obtainDelegationTokens(&lt;br/&gt;
-      hadoopConf: Configuration,&lt;br/&gt;
-      creds: Credentials): Long = {&lt;br/&gt;
-    delegationTokenProviders.values.flatMap { provider =&amp;gt;&lt;br/&gt;
-      if (provider.delegationTokensRequired(sparkConf, hadoopConf)) {&lt;br/&gt;
-        provider.obtainDelegationTokens(hadoopConf, sparkConf, creds)&lt;br/&gt;
+  private def updateTokensTask(): Unit = {&lt;br/&gt;
+    try {&lt;br/&gt;
+      val freshUGI = doLogin()&lt;br/&gt;
+      val creds = obtainTokensAndScheduleRenewal(freshUGI)&lt;br/&gt;
+      val tokens = SparkHadoopUtil.get.serialize(creds)&lt;br/&gt;
+&lt;br/&gt;
+      val driver = driverRef.get()&lt;br/&gt;
+      if (driver != null) {
+        logInfo(&quot;Updating delegation tokens.&quot;)
+        driver.send(UpdateDelegationTokens(tokens))
       } else {&lt;br/&gt;
-        logDebug(s&quot;Service ${provider.serviceName} does not require a token.&quot; +&lt;br/&gt;
-          s&quot; Check your configuration to see if security is disabled or not.&quot;)&lt;br/&gt;
-        None&lt;br/&gt;
+        // This shouldn&apos;t really happen, since the driver should register way before tokens expire.&lt;br/&gt;
+        logWarning(&quot;Delegation tokens close to expiration but no driver has registered yet.&quot;)&lt;br/&gt;
+        SparkHadoopUtil.get.addDelegationTokens(tokens, sparkConf)&lt;br/&gt;
       }&lt;br/&gt;
-    }.foldLeft(Long.MaxValue)(math.min)&lt;br/&gt;
+    } catch {&lt;br/&gt;
+      case e: Exception =&amp;gt;&lt;br/&gt;
+        val delay = TimeUnit.SECONDS.toMillis(sparkConf.get(CREDENTIALS_RENEWAL_RETRY_WAIT))&lt;br/&gt;
+        logWarning(s&quot;Failed to update tokens, will try again in ${UIUtils.formatDuration(delay)}!&quot; +&lt;br/&gt;
+          &quot; If this happens too often tasks will fail.&quot;, e)&lt;br/&gt;
+        scheduleRenewal(delay)&lt;br/&gt;
+    }&lt;br/&gt;
   }&lt;br/&gt;
-}&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * Obtain new delegation tokens from the available providers. Schedules a new task to fetch&lt;br/&gt;
+   * new tokens before the new set expires.&lt;br/&gt;
+   *&lt;br/&gt;
+   * @return Credentials containing the new tokens.&lt;br/&gt;
+   */&lt;br/&gt;
+  private def obtainTokensAndScheduleRenewal(ugi: UserGroupInformation): Credentials = {&lt;br/&gt;
+    ugi.doAs(new PrivilegedExceptionAction&lt;span class=&quot;error&quot;&gt;&amp;#91;Credentials&amp;#93;&lt;/span&gt;() {&lt;br/&gt;
+      override def run(): Credentials = {
+        val creds = new Credentials()
+        val nextRenewal = obtainDelegationTokens(creds)
+
+        // Calculate the time when new credentials should be created, based on the configured
+        // ratio.
+        val now = System.currentTimeMillis
+        val ratio = sparkConf.get(CREDENTIALS_RENEWAL_INTERVAL_RATIO)
+        val delay = (ratio * (nextRenewal - now)).toLong
+        scheduleRenewal(delay)
+        creds
+      }&lt;br/&gt;
+    })&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def doLogin(): UserGroupInformation = {
+    logInfo(s&quot;Attempting to login to KDC using principal: $principal&quot;)
+    val ugi = UserGroupInformation.loginUserFromKeytabAndReturnUGI(principal, keytab)
+    logInfo(&quot;Successfully logged into KDC.&quot;)
+    ugi
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def loadProviders(): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, HadoopDelegationTokenProvider&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    val providers = Seq(&lt;br/&gt;
+      new HadoopFSDelegationTokenProvider(&lt;br/&gt;
+        () =&amp;gt; HadoopDelegationTokenManager.this.fileSystemsToAccess())) ++&lt;br/&gt;
+      safeCreateProvider(new HiveDelegationTokenProvider) ++&lt;br/&gt;
+      safeCreateProvider(new HBaseDelegationTokenProvider) ++&lt;br/&gt;
+      safeCreateProvider(new KafkaDelegationTokenProvider)&lt;br/&gt;
+&lt;br/&gt;
+    // Filter out providers for which spark.security.credentials.{service}.enabled is false.&lt;br/&gt;
+    providers&lt;br/&gt;
+      .filter { p =&amp;gt; isServiceEnabled(p.serviceName) }&lt;br/&gt;
+      .map { p =&amp;gt; (p.serviceName, p) }&lt;br/&gt;
+      .toMap&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def safeCreateProvider(&lt;br/&gt;
+      createFn: =&amp;gt; HadoopDelegationTokenProvider): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;HadoopDelegationTokenProvider&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    try {
+      Some(createFn)
+    } catch {
+      case t: Throwable =&amp;gt;
+        logDebug(s&quot;Failed to load built in provider.&quot;, t)
+        None
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala b/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala&lt;br/&gt;
index 21ca669ea98f0..767b5521e8d7b 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/security/HadoopFSDelegationTokenProvider.scala&lt;br/&gt;
@@ -30,7 +30,7 @@ import org.apache.spark.{SparkConf, SparkException}&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.internal.config._&lt;br/&gt;
 &lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;deploy&amp;#93;&lt;/span&gt; class HadoopFSDelegationTokenProvider(fileSystems: Configuration =&amp;gt; Set&lt;span class=&quot;error&quot;&gt;&amp;#91;FileSystem&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;deploy&amp;#93;&lt;/span&gt; class HadoopFSDelegationTokenProvider(fileSystems: () =&amp;gt; Set&lt;span class=&quot;error&quot;&gt;&amp;#91;FileSystem&amp;#93;&lt;/span&gt;)&lt;br/&gt;
     extends HadoopDelegationTokenProvider with Logging {&lt;br/&gt;
 &lt;br/&gt;
   // This tokenRenewalInterval will be set in the first call to obtainDelegationTokens.&lt;br/&gt;
@@ -44,8 +44,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;deploy&amp;#93;&lt;/span&gt; class HadoopFSDelegationTokenProvider(fileSystems: Configuration&lt;br/&gt;
       hadoopConf: Configuration,&lt;br/&gt;
       sparkConf: SparkConf,&lt;br/&gt;
       creds: Credentials): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-&lt;br/&gt;
-    val fsToGetTokens = fileSystems(hadoopConf)&lt;br/&gt;
+    val fsToGetTokens = fileSystems()&lt;br/&gt;
     val fetchCreds = fetchDelegationTokens(getTokenRenewer(hadoopConf), fsToGetTokens, creds)&lt;br/&gt;
 &lt;br/&gt;
     // Get the token renewal interval if it is not set. It will only be called once.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/security/KafkaDelegationTokenProvider.scala b/core/src/main/scala/org/apache/spark/deploy/security/KafkaDelegationTokenProvider.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..45995be630cc5&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/security/KafkaDelegationTokenProvider.scala&lt;br/&gt;
@@ -0,0 +1,61 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.deploy.security&lt;br/&gt;
+&lt;br/&gt;
+import scala.language.existentials&lt;br/&gt;
+import scala.util.control.NonFatal&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.hadoop.conf.Configuration&lt;br/&gt;
+import org.apache.hadoop.security.Credentials&lt;br/&gt;
+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.spark.SparkConf&lt;br/&gt;
+import org.apache.spark.internal.Logging&lt;br/&gt;
+import org.apache.spark.internal.config._&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;security&amp;#93;&lt;/span&gt; class KafkaDelegationTokenProvider&lt;br/&gt;
+  extends HadoopDelegationTokenProvider with Logging {&lt;br/&gt;
+&lt;br/&gt;
+  override def serviceName: String = &quot;kafka&quot;&lt;br/&gt;
+&lt;br/&gt;
+  override def obtainDelegationTokens(&lt;br/&gt;
+      hadoopConf: Configuration,&lt;br/&gt;
+      sparkConf: SparkConf,&lt;br/&gt;
+      creds: Credentials): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    try {
+      logDebug(&quot;Attempting to fetch Kafka security token.&quot;)
+      val (token, nextRenewalDate) = KafkaTokenUtil.obtainToken(sparkConf)
+      creds.addToken(token.getService, token)
+      return Some(nextRenewalDate)
+    } catch {
+      case NonFatal(e) =&amp;gt;
+        logInfo(s&quot;Failed to get token from service $serviceName&quot;, e)
+    }&lt;br/&gt;
+    None&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  override def delegationTokensRequired(&lt;br/&gt;
+      sparkConf: SparkConf,&lt;br/&gt;
+      hadoopConf: Configuration): Boolean = {
+    val protocol = sparkConf.get(Kafka.SECURITY_PROTOCOL)
+    sparkConf.contains(Kafka.BOOTSTRAP_SERVERS) &amp;amp;&amp;amp;
+      (protocol == SASL_SSL.name ||
+        protocol == SSL.name ||
+        protocol == SASL_PLAINTEXT.name)
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/security/KafkaTokenUtil.scala b/core/src/main/scala/org/apache/spark/deploy/security/KafkaTokenUtil.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..c890cee59ffe0&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/deploy/security/KafkaTokenUtil.scala&lt;br/&gt;
@@ -0,0 +1,202 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.deploy.security&lt;br/&gt;
+&lt;br/&gt;
+import java.{ util =&amp;gt; ju }&lt;br/&gt;
+import java.text.SimpleDateFormat&lt;br/&gt;
+&lt;br/&gt;
+import scala.util.control.NonFatal&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.hadoop.io.Text&lt;br/&gt;
+import org.apache.hadoop.security.token.{Token, TokenIdentifier}&lt;br/&gt;
+import org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier&lt;br/&gt;
+import org.apache.kafka.clients.CommonClientConfigs&lt;br/&gt;
+import org.apache.kafka.clients.admin.{AdminClient, CreateDelegationTokenOptions}&lt;br/&gt;
+import org.apache.kafka.common.config.SaslConfigs&lt;br/&gt;
+import org.apache.kafka.common.security.JaasContext&lt;br/&gt;
+import org.apache.kafka.common.security.auth.SecurityProtocol.{SASL_PLAINTEXT, SASL_SSL, SSL}&lt;br/&gt;
+import org.apache.kafka.common.security.token.delegation.DelegationToken&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.spark.SparkConf&lt;br/&gt;
+import org.apache.spark.internal.Logging&lt;br/&gt;
+import org.apache.spark.internal.config._&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object KafkaTokenUtil extends Logging {&lt;br/&gt;
+  val TOKEN_KIND = new Text(&quot;KAFKA_DELEGATION_TOKEN&quot;)&lt;br/&gt;
+  val TOKEN_SERVICE = new Text(&quot;kafka.server.delegation.token&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class KafkaDelegationTokenIdentifier extends AbstractDelegationTokenIdentifier {
+    override def getKind: Text = TOKEN_KIND
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;security&amp;#93;&lt;/span&gt; def obtainToken(sparkConf: SparkConf): (Token&lt;span class=&quot;error&quot;&gt;&amp;#91;_ &amp;lt;: TokenIdentifier&amp;#93;&lt;/span&gt;, Long) = {
+    val adminClient = AdminClient.create(createAdminClientProperties(sparkConf))
+    val createDelegationTokenOptions = new CreateDelegationTokenOptions()
+    val createResult = adminClient.createDelegationToken(createDelegationTokenOptions)
+    val token = createResult.delegationToken().get()
+    printToken(token)
+
+    (new Token[KafkaDelegationTokenIdentifier](
+      token.tokenInfo.tokenId.getBytes,
+      token.hmacAsBase64String.getBytes,
+      TOKEN_KIND,
+      TOKEN_SERVICE
+    ), token.tokenInfo.expiryTimestamp)
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;security&amp;#93;&lt;/span&gt; def createAdminClientProperties(sparkConf: SparkConf): ju.Properties = {&lt;br/&gt;
+    val adminClientProperties = new ju.Properties&lt;br/&gt;
+&lt;br/&gt;
+    val bootstrapServers = sparkConf.get(Kafka.BOOTSTRAP_SERVERS)&lt;br/&gt;
+    require(bootstrapServers.nonEmpty, s&quot;Tried to obtain kafka delegation token but bootstrap &quot; +&lt;br/&gt;
+      &quot;servers not configured.&quot;)&lt;br/&gt;
+    adminClientProperties.put(CommonClientConfigs.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers.get)&lt;br/&gt;
+&lt;br/&gt;
+    val protocol = sparkConf.get(Kafka.SECURITY_PROTOCOL)&lt;br/&gt;
+    adminClientProperties.put(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG, protocol)&lt;br/&gt;
+    protocol match {
+      case SASL_SSL.name =&amp;gt;
+        setTrustStoreProperties(sparkConf, adminClientProperties)
+
+      case SSL.name =&amp;gt;
+        setTrustStoreProperties(sparkConf, adminClientProperties)
+        setKeyStoreProperties(sparkConf, adminClientProperties)
+        logWarning(&quot;Obtaining kafka delegation token with SSL protocol. Please &quot; +
+          &quot;configure 2-way authentication on the broker side.&quot;)
+
+      case SASL_PLAINTEXT.name =&amp;gt;
+        logWarning(&quot;Obtaining kafka delegation token through plain communication channel. Please &quot; +
+          &quot;consider the security impact.&quot;)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    // There are multiple possibilities to log in and applied in the following order:&lt;br/&gt;
+    // - JVM global security provided -&amp;gt; try to log in with JVM global security configuration&lt;br/&gt;
+    //   which can be configured for example with &apos;java.security.auth.login.config&apos;.&lt;br/&gt;
+    //   For this no additional parameter needed.&lt;br/&gt;
+    // - Keytab is provided -&amp;gt; try to log in with kerberos module and keytab using kafka&apos;s dynamic&lt;br/&gt;
+    //   JAAS configuration.&lt;br/&gt;
+    // - Keytab not provided -&amp;gt; try to log in with kerberos module and ticket cache using kafka&apos;s&lt;br/&gt;
+    //   dynamic JAAS configuration.&lt;br/&gt;
+    // Kafka client is unable to use subject from JVM which already logged in&lt;br/&gt;
+    // to kdc (see &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7677&quot; title=&quot;Client login with already existing JVM subject&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7677&quot;&gt;KAFKA-7677&lt;/a&gt;)&lt;br/&gt;
+    if (isGlobalJaasConfigurationProvided) {
+      logDebug(&quot;JVM global security configuration detected, using it for login.&quot;)
+    } else {&lt;br/&gt;
+      adminClientProperties.put(SaslConfigs.SASL_MECHANISM, SaslConfigs.GSSAPI_MECHANISM)&lt;br/&gt;
+      if (sparkConf.contains(KEYTAB)) {
+        logDebug(&quot;Keytab detected, using it for login.&quot;)
+        val jaasParams = getKeytabJaasParams(sparkConf)
+        adminClientProperties.put(SaslConfigs.SASL_JAAS_CONFIG, jaasParams)
+      } else {
+        logDebug(&quot;Using ticket cache for login.&quot;)
+        val jaasParams = getTicketCacheJaasParams(sparkConf)
+        adminClientProperties.put(SaslConfigs.SASL_JAAS_CONFIG, jaasParams)
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    adminClientProperties&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def isGlobalJaasConfigurationProvided: Boolean = {&lt;br/&gt;
+    try {
+      JaasContext.loadClientContext(ju.Collections.emptyMap[String, Object]())
+      true
+    } catch {
+      case NonFatal(_) =&amp;gt; false
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def setTrustStoreProperties(sparkConf: SparkConf, properties: ju.Properties): Unit = {&lt;br/&gt;
+    sparkConf.get(Kafka.TRUSTSTORE_LOCATION).foreach { truststoreLocation =&amp;gt;
+      properties.put(&quot;ssl.truststore.location&quot;, truststoreLocation)
+    }&lt;br/&gt;
+    sparkConf.get(Kafka.TRUSTSTORE_PASSWORD).foreach { truststorePassword =&amp;gt;
+      properties.put(&quot;ssl.truststore.password&quot;, truststorePassword)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def setKeyStoreProperties(sparkConf: SparkConf, properties: ju.Properties): Unit = {&lt;br/&gt;
+    sparkConf.get(Kafka.KEYSTORE_LOCATION).foreach { keystoreLocation =&amp;gt;
+      properties.put(&quot;ssl.keystore.location&quot;, keystoreLocation)
+    }&lt;br/&gt;
+    sparkConf.get(Kafka.KEYSTORE_PASSWORD).foreach { keystorePassword =&amp;gt;
+      properties.put(&quot;ssl.keystore.password&quot;, keystorePassword)
+    }&lt;br/&gt;
+    sparkConf.get(Kafka.KEY_PASSWORD).foreach { keyPassword =&amp;gt;
+      properties.put(&quot;ssl.key.password&quot;, keyPassword)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;security&amp;#93;&lt;/span&gt; def getKeytabJaasParams(sparkConf: SparkConf): String = {&lt;br/&gt;
+    val serviceName = sparkConf.get(Kafka.KERBEROS_SERVICE_NAME)&lt;br/&gt;
+    require(serviceName.nonEmpty, &quot;Kerberos service name must be defined&quot;)&lt;br/&gt;
+&lt;br/&gt;
+    val params =&lt;br/&gt;
+      s&quot;&quot;&quot;&lt;br/&gt;
+      |${getKrb5LoginModuleName} required&lt;br/&gt;
+      | useKeyTab=true&lt;br/&gt;
+      | serviceName=&quot;${serviceName.get}&quot;&lt;br/&gt;
+      | keyTab=&quot;${sparkConf.get(KEYTAB).get}&quot;&lt;br/&gt;
+      | principal=&quot;${sparkConf.get(PRINCIPAL).get}&quot;;&lt;br/&gt;
+      &quot;&quot;&quot;.stripMargin.replace(&quot;\n&quot;, &quot;&quot;)&lt;br/&gt;
+    logDebug(s&quot;Krb keytab JAAS params: $params&quot;)&lt;br/&gt;
+    params&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def getTicketCacheJaasParams(sparkConf: SparkConf): String = {&lt;br/&gt;
+    val serviceName = sparkConf.get(Kafka.KERBEROS_SERVICE_NAME)&lt;br/&gt;
+    require(serviceName.nonEmpty, &quot;Kerberos service name must be defined&quot;)&lt;br/&gt;
+&lt;br/&gt;
+    val params =&lt;br/&gt;
+      s&quot;&quot;&quot;&lt;br/&gt;
+      |${getKrb5LoginModuleName} required&lt;br/&gt;
+      | useTicketCache=true&lt;br/&gt;
+      | serviceName=&quot;${serviceName.get}&quot;;&lt;br/&gt;
+      &quot;&quot;&quot;.stripMargin.replace(&quot;\n&quot;, &quot;&quot;)&lt;br/&gt;
+    logDebug(s&quot;Krb ticket cache JAAS params: $params&quot;)&lt;br/&gt;
+    params&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Krb5LoginModule package vary in different JVMs.&lt;br/&gt;
+   * Please see Hadoop UserGroupInformation for further details.&lt;br/&gt;
+   */&lt;br/&gt;
+  private def getKrb5LoginModuleName(): String = {&lt;br/&gt;
+    if (System.getProperty(&quot;java.vendor&quot;).contains(&quot;IBM&quot;)) {
+      &quot;com.ibm.security.auth.module.Krb5LoginModule&quot;
+    } else {
+      &quot;com.sun.security.auth.module.Krb5LoginModule&quot;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def printToken(token: DelegationToken): Unit = {&lt;br/&gt;
+    if (log.isDebugEnabled) {
+      val dateFormat = new SimpleDateFormat(&quot;yyyy-MM-dd&apos;T&apos;HH:mm&quot;)
+      logDebug(&quot;%-15s %-30s %-15s %-25s %-15s %-15s %-15s&quot;.format(
+        &quot;TOKENID&quot;, &quot;HMAC&quot;, &quot;OWNER&quot;, &quot;RENEWERS&quot;, &quot;ISSUEDATE&quot;, &quot;EXPIRYDATE&quot;, &quot;MAXDATE&quot;))
+      val tokenInfo = token.tokenInfo
+      logDebug(&quot;%-15s [hidden] %-15s %-25s %-15s %-15s %-15s&quot;.format(
+        tokenInfo.tokenId,
+        tokenInfo.owner,
+        tokenInfo.renewersAsString,
+        dateFormat.format(tokenInfo.issueTimestamp),
+        dateFormat.format(tokenInfo.expiryTimestamp),
+        dateFormat.format(tokenInfo.maxTimestamp)))
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/executor/Executor.scala b/core/src/main/scala/org/apache/spark/executor/Executor.scala&lt;br/&gt;
index 86b19578037df..a30a501e5d4a1 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/executor/Executor.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/executor/Executor.scala&lt;br/&gt;
@@ -28,6 +28,7 @@ import javax.annotation.concurrent.GuardedBy&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable.{ArrayBuffer, HashMap, Map}&lt;br/&gt;
+import scala.concurrent.duration._&lt;br/&gt;
 import scala.util.control.NonFatal&lt;br/&gt;
 &lt;br/&gt;
 import com.google.common.util.concurrent.ThreadFactoryBuilder&lt;br/&gt;
@@ -38,7 +39,7 @@ import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.internal.config._&lt;br/&gt;
 import org.apache.spark.memory.{SparkOutOfMemoryError, TaskMemoryManager}&lt;br/&gt;
 import org.apache.spark.rpc.RpcTimeout&lt;br/&gt;
-import org.apache.spark.scheduler.{DirectTaskResult, IndirectTaskResult, Task, TaskDescription}&lt;br/&gt;
+import org.apache.spark.scheduler._&lt;br/&gt;
 import org.apache.spark.shuffle.FetchFailedException&lt;br/&gt;
 import org.apache.spark.storage.{StorageLevel, TaskResultBlockId}&lt;br/&gt;
 import org.apache.spark.util._&lt;br/&gt;
@@ -120,7 +121,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class Executor(&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   // Whether to load classes in user jars before those in Spark jars&lt;br/&gt;
-  private val userClassPathFirst = conf.getBoolean(&quot;spark.executor.userClassPathFirst&quot;, false)&lt;br/&gt;
+  private val userClassPathFirst = conf.get(EXECUTOR_USER_CLASS_PATH_FIRST)&lt;br/&gt;
 &lt;br/&gt;
   // Whether to monitor killed / interrupted tasks&lt;br/&gt;
   private val taskReaperEnabled = conf.getBoolean(&quot;spark.task.reaper.enabled&quot;, false)&lt;br/&gt;
@@ -136,6 +137,29 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class Executor(&lt;br/&gt;
   // for fetching remote cached RDD blocks, so need to make sure it uses the right classloader too.&lt;br/&gt;
   env.serializerManager.setDefaultClassLoader(replClassLoader)&lt;br/&gt;
 &lt;br/&gt;
+  private val executorPlugins: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorPlugin&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    val pluginNames = conf.get(EXECUTOR_PLUGINS)&lt;br/&gt;
+    if (pluginNames.nonEmpty) {&lt;br/&gt;
+      logDebug(s&quot;Initializing the following plugins: ${pluginNames.mkString(&quot;, &quot;)}&quot;)&lt;br/&gt;
+&lt;br/&gt;
+      // Plugins need to load using a class loader that includes the executor&apos;s user classpath&lt;br/&gt;
+      val pluginList: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorPlugin&amp;#93;&lt;/span&gt; =&lt;br/&gt;
+        Utils.withContextClassLoader(replClassLoader) {&lt;br/&gt;
+          val plugins = Utils.loadExtensions(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorPlugin&amp;#93;&lt;/span&gt;, pluginNames, conf)&lt;br/&gt;
+          plugins.foreach { plugin =&amp;gt;
+            plugin.init()
+            logDebug(s&quot;Successfully loaded plugin &quot; + plugin.getClass().getCanonicalName())
+          }&lt;br/&gt;
+          plugins&lt;br/&gt;
+        }&lt;br/&gt;
+&lt;br/&gt;
+      logDebug(&quot;Finished initializing plugins&quot;)&lt;br/&gt;
+      pluginList&lt;br/&gt;
+    } else {
+      Nil
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   // Max size of direct result. If task result is bigger than this, we use the block manager&lt;br/&gt;
   // to send the result back.&lt;br/&gt;
   private val maxDirectResultSize = Math.min(&lt;br/&gt;
@@ -147,19 +171,34 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class Executor(&lt;br/&gt;
   // Maintains the list of running tasks.&lt;br/&gt;
   private val runningTasks = new ConcurrentHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;Long, TaskRunner&amp;#93;&lt;/span&gt;&lt;br/&gt;
 &lt;br/&gt;
-  // Executor for the heartbeat task.&lt;br/&gt;
-  private val heartbeater = ThreadUtils.newDaemonSingleThreadScheduledExecutor(&quot;driver-heartbeater&quot;)&lt;br/&gt;
-&lt;br/&gt;
-  // must be initialized before running startDriverHeartbeat()&lt;br/&gt;
-  private val heartbeatReceiverRef =&lt;br/&gt;
-    RpcUtils.makeDriverRef(HeartbeatReceiver.ENDPOINT_NAME, conf, env.rpcEnv)&lt;br/&gt;
-&lt;br/&gt;
   /**&lt;br/&gt;
    * When an executor is unable to send heartbeats to the driver more than `HEARTBEAT_MAX_FAILURES`&lt;br/&gt;
    * times, it should kill itself. The default value is 60. It means we will retry to send&lt;br/&gt;
    * heartbeats about 10 minutes because the heartbeat interval is 10s.&lt;br/&gt;
    */&lt;br/&gt;
-  private val HEARTBEAT_MAX_FAILURES = conf.getInt(&quot;spark.executor.heartbeat.maxFailures&quot;, 60)&lt;br/&gt;
+  private val HEARTBEAT_MAX_FAILURES = conf.get(EXECUTOR_HEARTBEAT_MAX_FAILURES)&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Whether to drop empty accumulators from heartbeats sent to the driver. Including the empty&lt;br/&gt;
+   * accumulators (that satisfy isZero) can make the size of the heartbeat message very large.&lt;br/&gt;
+   */&lt;br/&gt;
+  private val HEARTBEAT_DROP_ZEROES = conf.get(EXECUTOR_HEARTBEAT_DROP_ZERO_ACCUMULATOR_UPDATES)&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Interval to send heartbeats, in milliseconds&lt;br/&gt;
+   */&lt;br/&gt;
+  private val HEARTBEAT_INTERVAL_MS = conf.get(EXECUTOR_HEARTBEAT_INTERVAL)&lt;br/&gt;
+&lt;br/&gt;
+  // Executor for the heartbeat task.&lt;br/&gt;
+  private val heartbeater = new Heartbeater(&lt;br/&gt;
+    env.memoryManager,&lt;br/&gt;
+    () =&amp;gt; Executor.this.reportHeartBeat(),&lt;br/&gt;
+    &quot;executor-heartbeater&quot;,&lt;br/&gt;
+    HEARTBEAT_INTERVAL_MS)&lt;br/&gt;
+&lt;br/&gt;
+  // must be initialized before running startDriverHeartbeat()&lt;br/&gt;
+  private val heartbeatReceiverRef =&lt;br/&gt;
+    RpcUtils.makeDriverRef(HeartbeatReceiver.ENDPOINT_NAME, conf, env.rpcEnv)&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
    * Count the failure times of heartbeat. It should only be accessed in the heartbeat thread. Each&lt;br/&gt;
@@ -167,7 +206,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class Executor(&lt;br/&gt;
    */&lt;br/&gt;
   private var heartbeatFailures = 0&lt;br/&gt;
 &lt;br/&gt;
-  startDriverHeartbeater()&lt;br/&gt;
+  heartbeater.start()&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;executor&amp;#93;&lt;/span&gt; def numRunningTasks: Int = runningTasks.size()&lt;br/&gt;
 &lt;br/&gt;
@@ -216,9 +255,25 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class Executor(&lt;br/&gt;
 &lt;br/&gt;
   def stop(): Unit = {&lt;br/&gt;
     env.metricsSystem.report()&lt;br/&gt;
-    heartbeater.shutdown()&lt;br/&gt;
-    heartbeater.awaitTermination(10, TimeUnit.SECONDS)&lt;br/&gt;
+    try {
+      heartbeater.stop()
+    } catch {
+      case NonFatal(e) =&amp;gt;
+        logWarning(&quot;Unable to stop heartbeater&quot;, e)
+     }&lt;br/&gt;
     threadPool.shutdown()&lt;br/&gt;
+&lt;br/&gt;
+    // Notify plugins that executor is shutting down so they can terminate cleanly&lt;br/&gt;
+    Utils.withContextClassLoader(replClassLoader) {&lt;br/&gt;
+      executorPlugins.foreach { plugin =&amp;gt;&lt;br/&gt;
+        try {
+          plugin.shutdown()
+        } catch {
+          case e: Exception =&amp;gt;
+            logWarning(&quot;Plugin &quot; + plugin.getClass().getCanonicalName() + &quot; shutdown failed&quot;, e)
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
     if (!isLocal) {
       env.stop()
     }&lt;br/&gt;
@@ -464,7 +519,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class Executor(&lt;br/&gt;
         executorSource.METRIC_OUTPUT_BYTES_WRITTEN&lt;br/&gt;
           .inc(task.metrics.outputMetrics.bytesWritten)&lt;br/&gt;
         executorSource.METRIC_OUTPUT_RECORDS_WRITTEN&lt;br/&gt;
-          .inc(task.metrics.inputMetrics.recordsRead)&lt;br/&gt;
+          .inc(task.metrics.outputMetrics.recordsWritten)&lt;br/&gt;
         executorSource.METRIC_RESULT_SIZE.inc(task.metrics.resultSize)&lt;br/&gt;
         executorSource.METRIC_DISK_BYTES_SPILLED.inc(task.metrics.diskBytesSpilled)&lt;br/&gt;
         executorSource.METRIC_MEMORY_BYTES_SPILLED.inc(task.metrics.memoryBytesSpilled)&lt;br/&gt;
@@ -787,18 +842,28 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class Executor(&lt;br/&gt;
     val accumUpdates = new ArrayBuffer[(Long, Seq[AccumulatorV2&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;])]()&lt;br/&gt;
     val curGCTime = computeTotalGcTime()&lt;br/&gt;
 &lt;br/&gt;
+    // get executor level memory metrics&lt;br/&gt;
+    val executorUpdates = heartbeater.getCurrentMetrics()&lt;br/&gt;
+&lt;br/&gt;
     for (taskRunner &amp;lt;- runningTasks.values().asScala) {&lt;br/&gt;
       if (taskRunner.task != null) {&lt;br/&gt;
         taskRunner.task.metrics.mergeShuffleReadMetrics()&lt;br/&gt;
         taskRunner.task.metrics.setJvmGCTime(curGCTime - taskRunner.startGCTime)&lt;br/&gt;
-        accumUpdates += ((taskRunner.taskId, taskRunner.task.metrics.accumulators()))&lt;br/&gt;
+        val accumulatorsToReport =&lt;br/&gt;
+          if (HEARTBEAT_DROP_ZEROES) {
+            taskRunner.task.metrics.accumulators().filterNot(_.isZero)
+          } else {
+            taskRunner.task.metrics.accumulators()
+          }&lt;br/&gt;
+        accumUpdates += ((taskRunner.taskId, accumulatorsToReport))&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    val message = Heartbeat(executorId, accumUpdates.toArray, env.blockManager.blockManagerId)&lt;br/&gt;
+    val message = Heartbeat(executorId, accumUpdates.toArray, env.blockManager.blockManagerId,&lt;br/&gt;
+      executorUpdates)&lt;br/&gt;
     try {&lt;br/&gt;
       val response = heartbeatReceiverRef.askSync&lt;span class=&quot;error&quot;&gt;&amp;#91;HeartbeatResponse&amp;#93;&lt;/span&gt;(&lt;br/&gt;
-          message, RpcTimeout(conf, &quot;spark.executor.heartbeatInterval&quot;, &quot;10s&quot;))&lt;br/&gt;
+        message, new RpcTimeout(HEARTBEAT_INTERVAL_MS.millis, EXECUTOR_HEARTBEAT_INTERVAL.key))&lt;br/&gt;
       if (response.reregisterBlockManager) {
         logInfo(&quot;Told to re-register on heartbeat&quot;)
         env.blockManager.reregister()
@@ -815,21 +880,6 @@ private[spark] class Executor(
         }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
-&lt;br/&gt;
-  /**&lt;br/&gt;
-   * Schedules a task to report heartbeat and partial metrics for active tasks to driver.&lt;br/&gt;
-   */&lt;br/&gt;
-  private def startDriverHeartbeater(): Unit = {&lt;br/&gt;
-    val intervalMs = conf.getTimeAsMs(&quot;spark.executor.heartbeatInterval&quot;, &quot;10s&quot;)&lt;br/&gt;
-&lt;br/&gt;
-    // Wait a random interval so the heartbeats don&apos;t end up in sync&lt;br/&gt;
-    val initialDelay = intervalMs + (math.random * intervalMs).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;&lt;br/&gt;
-&lt;br/&gt;
-    val heartbeatTask = new Runnable() {
-      override def run(): Unit = Utils.logUncaughtExceptions(reportHeartBeat())
-    }&lt;br/&gt;
-    heartbeater.scheduleAtFixedRate(heartbeatTask, initialDelay, intervalMs, TimeUnit.MILLISECONDS)&lt;br/&gt;
-  }&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Executor {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/executor/ExecutorMetrics.scala b/core/src/main/scala/org/apache/spark/executor/ExecutorMetrics.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..1befd27de1cba&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/executor/ExecutorMetrics.scala&lt;br/&gt;
@@ -0,0 +1,81 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.spark.executor&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.spark.annotation.DeveloperApi&lt;br/&gt;
+import org.apache.spark.metrics.ExecutorMetricType&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * :: DeveloperApi ::&lt;br/&gt;
+ * Metrics tracked for executors and the driver.&lt;br/&gt;
+ *&lt;br/&gt;
+ * Executor-level metrics are sent from each executor to the driver as part of the Heartbeat.&lt;br/&gt;
+ */&lt;br/&gt;
+@DeveloperApi&lt;br/&gt;
+class ExecutorMetrics private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; extends Serializable {&lt;br/&gt;
+&lt;br/&gt;
+  // Metrics are indexed by ExecutorMetricType.values&lt;br/&gt;
+  private val metrics = new Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;(ExecutorMetricType.values.length)&lt;br/&gt;
+&lt;br/&gt;
+  // the first element is initialized to -1, indicating that the values for the array&lt;br/&gt;
+  // haven&apos;t been set yet.&lt;br/&gt;
+  metrics(0) = -1&lt;br/&gt;
+&lt;br/&gt;
+  /** Returns the value for the specified metricType. */&lt;br/&gt;
+  def getMetricValue(metricType: ExecutorMetricType): Long = {
+    metrics(ExecutorMetricType.metricIdxMap(metricType))
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /** Returns true if the values for the metrics have been set, false otherwise. */&lt;br/&gt;
+  def isSet(): Boolean = metrics(0) &amp;gt; -1&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def this(metrics: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;) {
+    this()
+    Array.copy(metrics, 0, this.metrics, 0, Math.min(metrics.size, this.metrics.size))
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Constructor: create the ExecutorMetrics with the values specified.&lt;br/&gt;
+   *&lt;br/&gt;
+   * @param executorMetrics map of executor metric name to value&lt;br/&gt;
+   */&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def this(executorMetrics: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Long&amp;#93;&lt;/span&gt;) {&lt;br/&gt;
+    this()&lt;br/&gt;
+    (0 until ExecutorMetricType.values.length).foreach { idx =&amp;gt;
+      metrics(idx) = executorMetrics.getOrElse(ExecutorMetricType.values(idx).name, 0L)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Compare the specified executor metrics values with the current executor metric values,&lt;br/&gt;
+   * and update the value for any metrics where the new value for the metric is larger.&lt;br/&gt;
+   *&lt;br/&gt;
+   * @param executorMetrics the executor metrics to compare&lt;br/&gt;
+   * @return if there is a new peak value for any metric&lt;br/&gt;
+   */&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def compareAndUpdatePeakValues(executorMetrics: ExecutorMetrics): Boolean = {&lt;br/&gt;
+    var updated = false&lt;br/&gt;
+&lt;br/&gt;
+    (0 until ExecutorMetricType.values.length).foreach { idx =&amp;gt;&lt;br/&gt;
+       if (executorMetrics.metrics(idx) &amp;gt; metrics(idx)) {
+        updated = true
+        metrics(idx) = executorMetrics.metrics(idx)
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+    updated&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala b/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala&lt;br/&gt;
index 4be395c8358b2..12c4b8f67f71c 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/executor/ShuffleReadMetrics.scala&lt;br/&gt;
@@ -18,6 +18,7 @@&lt;br/&gt;
 package org.apache.spark.executor&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.annotation.DeveloperApi&lt;br/&gt;
+import org.apache.spark.shuffle.ShuffleReadMetricsReporter&lt;br/&gt;
 import org.apache.spark.util.LongAccumulator&lt;br/&gt;
 &lt;br/&gt;
 &lt;br/&gt;
@@ -123,12 +124,13 @@ class ShuffleReadMetrics private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; () extends Serializable {&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
+&lt;br/&gt;
 /**&lt;br/&gt;
  * A temporary shuffle read metrics holder that is used to collect shuffle read metrics for each&lt;br/&gt;
  * shuffle dependency, and all temporary metrics will be merged into the [&lt;span class=&quot;error&quot;&gt;&amp;#91;ShuffleReadMetrics&amp;#93;&lt;/span&gt;] at&lt;br/&gt;
  * last.&lt;br/&gt;
  */&lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TempShuffleReadMetrics {&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TempShuffleReadMetrics extends ShuffleReadMetricsReporter {&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var _remoteBlocksFetched = 0L&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var _localBlocksFetched = 0L&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var _remoteBytesRead = 0L&lt;br/&gt;
@@ -137,13 +139,13 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TempShuffleReadMetrics {&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var _fetchWaitTime = 0L&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var _recordsRead = 0L&lt;br/&gt;
 &lt;br/&gt;
-  def incRemoteBlocksFetched(v: Long): Unit = _remoteBlocksFetched += v&lt;br/&gt;
-  def incLocalBlocksFetched(v: Long): Unit = _localBlocksFetched += v&lt;br/&gt;
-  def incRemoteBytesRead(v: Long): Unit = _remoteBytesRead += v&lt;br/&gt;
-  def incRemoteBytesReadToDisk(v: Long): Unit = _remoteBytesReadToDisk += v&lt;br/&gt;
-  def incLocalBytesRead(v: Long): Unit = _localBytesRead += v&lt;br/&gt;
-  def incFetchWaitTime(v: Long): Unit = _fetchWaitTime += v&lt;br/&gt;
-  def incRecordsRead(v: Long): Unit = _recordsRead += v&lt;br/&gt;
+  override def incRemoteBlocksFetched(v: Long): Unit = _remoteBlocksFetched += v&lt;br/&gt;
+  override def incLocalBlocksFetched(v: Long): Unit = _localBlocksFetched += v&lt;br/&gt;
+  override def incRemoteBytesRead(v: Long): Unit = _remoteBytesRead += v&lt;br/&gt;
+  override def incRemoteBytesReadToDisk(v: Long): Unit = _remoteBytesReadToDisk += v&lt;br/&gt;
+  override def incLocalBytesRead(v: Long): Unit = _localBytesRead += v&lt;br/&gt;
+  override def incFetchWaitTime(v: Long): Unit = _fetchWaitTime += v&lt;br/&gt;
+  override def incRecordsRead(v: Long): Unit = _recordsRead += v&lt;br/&gt;
 &lt;br/&gt;
   def remoteBlocksFetched: Long = _remoteBlocksFetched&lt;br/&gt;
   def localBlocksFetched: Long = _localBlocksFetched&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala b/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala&lt;br/&gt;
index ada2e1bc08593..d0b0e7da079c9 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/executor/ShuffleWriteMetrics.scala&lt;br/&gt;
@@ -18,6 +18,7 @@&lt;br/&gt;
 package org.apache.spark.executor&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.annotation.DeveloperApi&lt;br/&gt;
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter&lt;br/&gt;
 import org.apache.spark.util.LongAccumulator&lt;br/&gt;
 &lt;br/&gt;
 &lt;br/&gt;
@@ -27,7 +28,7 @@ import org.apache.spark.util.LongAccumulator&lt;br/&gt;
  * Operations are not thread-safe.&lt;br/&gt;
  */&lt;br/&gt;
 @DeveloperApi&lt;br/&gt;
-class ShuffleWriteMetrics private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; () extends Serializable {&lt;br/&gt;
+class ShuffleWriteMetrics private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; () extends ShuffleWriteMetricsReporter with Serializable {&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;executor&amp;#93;&lt;/span&gt; val _bytesWritten = new LongAccumulator&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;executor&amp;#93;&lt;/span&gt; val _recordsWritten = new LongAccumulator&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;executor&amp;#93;&lt;/span&gt; val _writeTime = new LongAccumulator&lt;br/&gt;
@@ -47,23 +48,13 @@ class ShuffleWriteMetrics private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; () extends Serializable {&lt;br/&gt;
    */&lt;br/&gt;
   def writeTime: Long = _writeTime.sum&lt;br/&gt;
 &lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def incBytesWritten(v: Long): Unit = _bytesWritten.add(v)&lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def incRecordsWritten(v: Long): Unit = _recordsWritten.add(v)&lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def incWriteTime(v: Long): Unit = _writeTime.add(v)&lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def decBytesWritten(v: Long): Unit = {&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; override def incBytesWritten(v: Long): Unit = _bytesWritten.add(v)&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; override def incRecordsWritten(v: Long): Unit = _recordsWritten.add(v)&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; override def incWriteTime(v: Long): Unit = _writeTime.add(v)&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; override def decBytesWritten(v: Long): Unit = {
     _bytesWritten.setValue(bytesWritten - v)
   }&lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def decRecordsWritten(v: Long): Unit = {&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; override def decRecordsWritten(v: Long): Unit = {
     _recordsWritten.setValue(recordsWritten - v)
   }&lt;br/&gt;
-&lt;br/&gt;
-  // Legacy methods for backward compatibility.&lt;br/&gt;
-  // TODO: remove these once we make this class private.&lt;br/&gt;
-  @deprecated(&quot;use bytesWritten instead&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def shuffleBytesWritten: Long = bytesWritten&lt;br/&gt;
-  @deprecated(&quot;use writeTime instead&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def shuffleWriteTime: Long = writeTime&lt;br/&gt;
-  @deprecated(&quot;use recordsWritten instead&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def shuffleRecordsWritten: Long = recordsWritten&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala b/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala&lt;br/&gt;
index ab020aaf6fa4f..5b33c110154d6 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/input/PortableDataStream.scala&lt;br/&gt;
@@ -52,6 +52,18 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class StreamFileInputFormat&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;&lt;br/&gt;
     val totalBytes = files.filterNot(&lt;em&gt;.isDirectory).map(&lt;/em&gt;.getLen + openCostInBytes).sum&lt;br/&gt;
     val bytesPerCore = totalBytes / defaultParallelism&lt;br/&gt;
     val maxSplitSize = Math.min(defaultMaxSplitBytes, Math.max(openCostInBytes, bytesPerCore))&lt;br/&gt;
+&lt;br/&gt;
+    // For small files we need to ensure the min split size per node &amp;amp; rack &amp;lt;= maxSplitSize&lt;br/&gt;
+    val jobConfig = context.getConfiguration&lt;br/&gt;
+    val minSplitSizePerNode = jobConfig.getLong(CombineFileInputFormat.SPLIT_MINSIZE_PERNODE, 0L)&lt;br/&gt;
+    val minSplitSizePerRack = jobConfig.getLong(CombineFileInputFormat.SPLIT_MINSIZE_PERRACK, 0L)&lt;br/&gt;
+&lt;br/&gt;
+    if (maxSplitSize &amp;lt; minSplitSizePerNode) {
+      super.setMinSplitSizeNode(maxSplitSize)
+    }&lt;br/&gt;
+    if (maxSplitSize &amp;lt; minSplitSizePerRack) {
+      super.setMinSplitSizeRack(maxSplitSize)
+    }&lt;br/&gt;
     super.setMaxSplitSize(maxSplitSize)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/internal/Logging.scala b/core/src/main/scala/org/apache/spark/internal/Logging.scala&lt;br/&gt;
index c0d709ad25f29..00db9af846ab9 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/internal/Logging.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/internal/Logging.scala&lt;br/&gt;
@@ -17,7 +17,11 @@&lt;br/&gt;
 &lt;br/&gt;
 package org.apache.spark.internal&lt;br/&gt;
 &lt;br/&gt;
-import org.apache.log4j.{Level, LogManager, PropertyConfigurator}&lt;br/&gt;
+import java.util.concurrent.ConcurrentHashMap&lt;br/&gt;
+&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.log4j._&lt;br/&gt;
 import org.slf4j.{Logger, LoggerFactory}&lt;br/&gt;
 import org.slf4j.impl.StaticLoggerBinder&lt;br/&gt;
 &lt;br/&gt;
@@ -143,13 +147,25 @@ trait Logging {&lt;br/&gt;
         // overriding the root logger&apos;s config if they&apos;re different.&lt;br/&gt;
         val replLogger = LogManager.getLogger(logName)&lt;br/&gt;
         val replLevel = Option(replLogger.getLevel()).getOrElse(Level.WARN)&lt;br/&gt;
+        // Update the consoleAppender threshold to replLevel&lt;br/&gt;
         if (replLevel != rootLogger.getEffectiveLevel()) {&lt;br/&gt;
           if (!silent) {
             System.err.printf(&quot;Setting default log level to \&quot;%s\&quot;.\n&quot;, replLevel)
             System.err.println(&quot;To adjust logging level use sc.setLogLevel(newLevel). &quot; +
               &quot;For SparkR, use setLogLevel(newLevel).&quot;)
           }&lt;br/&gt;
-          rootLogger.setLevel(replLevel)&lt;br/&gt;
+          rootLogger.getAllAppenders().asScala.foreach {&lt;br/&gt;
+            case ca: ConsoleAppender =&amp;gt;&lt;br/&gt;
+              Option(ca.getThreshold()) match {&lt;br/&gt;
+                case Some(t) =&amp;gt;&lt;br/&gt;
+                  Logging.consoleAppenderToThreshold.put(ca, t)&lt;br/&gt;
+                  if (!t.isGreaterOrEqual(replLevel)) {
+                    ca.setThreshold(replLevel)
+                  }&lt;br/&gt;
+                case None =&amp;gt; ca.setThreshold(replLevel)&lt;br/&gt;
+              }&lt;br/&gt;
+            case _ =&amp;gt; // no-op&lt;br/&gt;
+          }&lt;br/&gt;
         }&lt;br/&gt;
       }&lt;br/&gt;
       // scalastyle:on println&lt;br/&gt;
@@ -166,6 +182,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Logging {&lt;br/&gt;
   @volatile private var initialized = false&lt;br/&gt;
   @volatile private var defaultRootLevel: Level = null&lt;br/&gt;
   @volatile private var defaultSparkLog4jConfig = false&lt;br/&gt;
+  private val consoleAppenderToThreshold = new ConcurrentHashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;ConsoleAppender, Priority&amp;#93;&lt;/span&gt;()&lt;br/&gt;
 &lt;br/&gt;
   val initLock = new Object()&lt;br/&gt;
   try {&lt;br/&gt;
@@ -192,7 +209,13 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Logging {
         defaultSparkLog4jConfig = false
         LogManager.resetConfiguration()
       } else {&lt;br/&gt;
-        LogManager.getRootLogger().setLevel(defaultRootLevel)&lt;br/&gt;
+        val rootLogger = LogManager.getRootLogger()&lt;br/&gt;
+        rootLogger.setLevel(defaultRootLevel)&lt;br/&gt;
+        rootLogger.getAllAppenders().asScala.foreach {
+          case ca: ConsoleAppender =&amp;gt;
+            ca.setThreshold(consoleAppenderToThreshold.get(ca))
+          case _ =&amp;gt; // no-op
+        }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
     this.initialized = false&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/deploy/history/config.scala b/core/src/main/scala/org/apache/spark/internal/config/History.scala&lt;br/&gt;
similarity index 77%&lt;br/&gt;
rename from core/src/main/scala/org/apache/spark/deploy/history/config.scala&lt;br/&gt;
rename to core/src/main/scala/org/apache/spark/internal/config/History.scala&lt;br/&gt;
index 25ba9edb9e014..b7d8061d26d21 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/deploy/history/config.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/internal/config/History.scala&lt;br/&gt;
@@ -15,14 +15,13 @@&lt;br/&gt;
  * limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
 &lt;br/&gt;
-package org.apache.spark.deploy.history&lt;br/&gt;
+package org.apache.spark.internal.config&lt;br/&gt;
 &lt;br/&gt;
 import java.util.concurrent.TimeUnit&lt;br/&gt;
 &lt;br/&gt;
-import org.apache.spark.internal.config.ConfigBuilder&lt;br/&gt;
 import org.apache.spark.network.util.ByteUnit&lt;br/&gt;
 &lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object config {&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object History {&lt;br/&gt;
 &lt;br/&gt;
   val DEFAULT_LOG_DIR = &quot;file:/tmp/spark-events&quot;&lt;br/&gt;
 &lt;br/&gt;
@@ -30,6 +29,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object config {&lt;br/&gt;
     .stringConf&lt;br/&gt;
     .createWithDefault(DEFAULT_LOG_DIR)&lt;br/&gt;
 &lt;br/&gt;
+  val CLEANER_ENABLED = ConfigBuilder(&quot;spark.history.fs.cleaner.enabled&quot;)&lt;br/&gt;
+    .booleanConf&lt;br/&gt;
+    .createWithDefault(false)&lt;br/&gt;
+&lt;br/&gt;
+  val CLEANER_INTERVAL_S = ConfigBuilder(&quot;spark.history.fs.cleaner.interval&quot;)&lt;br/&gt;
+    .timeConf(TimeUnit.SECONDS)&lt;br/&gt;
+    .createWithDefaultString(&quot;1d&quot;)&lt;br/&gt;
+&lt;br/&gt;
   val MAX_LOG_AGE_S = ConfigBuilder(&quot;spark.history.fs.cleaner.maxAge&quot;)&lt;br/&gt;
     .timeConf(TimeUnit.SECONDS)&lt;br/&gt;
     .createWithDefaultString(&quot;7d&quot;)&lt;br/&gt;
@@ -64,4 +71,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object config {
       .bytesConf(ByteUnit.BYTE)
       .createWithDefaultString(&quot;1m&quot;)
 
+  val DRIVER_LOG_CLEANER_ENABLED = ConfigBuilder(&quot;spark.history.fs.driverlog.cleaner.enabled&quot;)
+    .fallbackConf(CLEANER_ENABLED)
+
+  val DRIVER_LOG_CLEANER_INTERVAL = ConfigBuilder(&quot;spark.history.fs.driverlog.cleaner.interval&quot;)
+    .fallbackConf(CLEANER_INTERVAL_S)
+
+  val MAX_DRIVER_LOG_AGE_S = ConfigBuilder(&quot;spark.history.fs.driverlog.cleaner.maxAge&quot;)
+    .fallbackConf(MAX_LOG_AGE_S)
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/internal/config/Kafka.scala b/core/src/main/scala/org/apache/spark/internal/config/Kafka.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..85d74c27142ad&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/internal/config/Kafka.scala&lt;br/&gt;
@@ -0,0 +1,82 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.internal.config&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Kafka {
+
+  val BOOTSTRAP_SERVERS =
+    ConfigBuilder(&quot;spark.kafka.bootstrap.servers&quot;)
+      .doc(&quot;A list of coma separated host/port pairs to use for establishing the initial &quot; +
+        &quot;connection to the Kafka cluster. For further details please see kafka documentation. &quot; +
+        &quot;Only used to obtain delegation token.&quot;)
+      .stringConf
+      .createOptional
+
+  val SECURITY_PROTOCOL =
+    ConfigBuilder(&quot;spark.kafka.security.protocol&quot;)
+      .doc(&quot;Protocol used to communicate with brokers. For further details please see kafka &quot; +
+        &quot;documentation. Only used to obtain delegation token.&quot;)
+      .stringConf
+      .createWithDefault(&quot;SASL_SSL&quot;)
+
+  val KERBEROS_SERVICE_NAME =
+    ConfigBuilder(&quot;spark.kafka.sasl.kerberos.service.name&quot;)
+      .doc(&quot;The Kerberos principal name that Kafka runs as. This can be defined either in &quot; +
+        &quot;Kafka&apos;s JAAS config or in Kafka&apos;s config. For further details please see kafka &quot; +
+        &quot;documentation. Only used to obtain delegation token.&quot;)
+      .stringConf
+      .createOptional
+
+  val TRUSTSTORE_LOCATION =
+    ConfigBuilder(&quot;spark.kafka.ssl.truststore.location&quot;)
+      .doc(&quot;The location of the trust store file. For further details please see kafka &quot; +
+        &quot;documentation. Only used to obtain delegation token.&quot;)
+      .stringConf
+      .createOptional
+
+  val TRUSTSTORE_PASSWORD =
+    ConfigBuilder(&quot;spark.kafka.ssl.truststore.password&quot;)
+      .doc(&quot;The store password for the trust store file. This is optional for client and only &quot; +
+        &quot;needed if ssl.truststore.location is configured. For further details please see kafka &quot; +
+        &quot;documentation. Only used to obtain delegation token.&quot;)
+      .stringConf
+      .createOptional
+
+  val KEYSTORE_LOCATION =
+    ConfigBuilder(&quot;spark.kafka.ssl.keystore.location&quot;)
+      .doc(&quot;The location of the key store file. This is optional for client and can be used for &quot; +
+        &quot;two-way authentication for client. For further details please see kafka documentation. &quot; +
+        &quot;Only used to obtain delegation token.&quot;)
+      .stringConf
+      .createOptional
+
+  val KEYSTORE_PASSWORD =
+    ConfigBuilder(&quot;spark.kafka.ssl.keystore.password&quot;)
+      .doc(&quot;The store password for the key store file. This is optional for client and only &quot; +
+        &quot;needed if ssl.keystore.location is configured. For further details please see kafka &quot; +
+        &quot;documentation. Only used to obtain delegation token.&quot;)
+      .stringConf
+      .createOptional
+
+  val KEY_PASSWORD =
+    ConfigBuilder(&quot;spark.kafka.ssl.key.password&quot;)
+      .doc(&quot;The password of the private key in the key store file. This is optional for client. &quot; +
+        &quot;For further details please see kafka documentation. Only used to obtain delegation token.&quot;)
+      .stringConf
+      .createOptional
+}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/config.scala b/core/src/main/scala/org/apache/spark/internal/config/Status.scala&lt;br/&gt;
similarity index 83%&lt;br/&gt;
rename from core/src/main/scala/org/apache/spark/status/config.scala&lt;br/&gt;
rename to core/src/main/scala/org/apache/spark/internal/config/Status.scala&lt;br/&gt;
index 67801b8f046f4..c56157227f8fc 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/status/config.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/internal/config/Status.scala&lt;br/&gt;
@@ -15,13 +15,11 @@&lt;br/&gt;
  * limitations under the License.&lt;br/&gt;
  */&lt;br/&gt;
 &lt;br/&gt;
-package org.apache.spark.status&lt;br/&gt;
+package org.apache.spark.internal.config&lt;br/&gt;
 &lt;br/&gt;
 import java.util.concurrent.TimeUnit&lt;br/&gt;
 &lt;br/&gt;
-import org.apache.spark.internal.config._&lt;br/&gt;
-&lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object config {&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Status {&lt;br/&gt;
 &lt;br/&gt;
   val ASYNC_TRACKING_ENABLED = ConfigBuilder(&quot;spark.appStateStore.asyncTracking.enable&quot;)&lt;br/&gt;
     .booleanConf&lt;br/&gt;
@@ -51,4 +49,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object config {
     .intConf
     .createWithDefault(Int.MaxValue)
 
+  val APP_STATUS_METRICS_ENABLED =
+    ConfigBuilder(&quot;spark.app.status.metrics.enabled&quot;)
+      .doc(&quot;Whether Dropwizard/Codahale metrics &quot; +
+        &quot;will be reported for the status of the running spark app.&quot;)
+      .booleanConf
+      .createWithDefault(false)
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/internal/config/package.scala b/core/src/main/scala/org/apache/spark/internal/config/package.scala&lt;br/&gt;
index 319e664a19677..646b3881a79b0 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/internal/config/package.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/internal/config/package.scala&lt;br/&gt;
@@ -21,6 +21,7 @@ import java.util.concurrent.TimeUnit&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.launcher.SparkLauncher&lt;br/&gt;
 import org.apache.spark.network.util.ByteUnit&lt;br/&gt;
+import org.apache.spark.unsafe.array.ByteArrayMethods&lt;br/&gt;
 import org.apache.spark.util.Utils&lt;br/&gt;
 &lt;br/&gt;
 package object config {&lt;br/&gt;
@@ -48,6 +49,19 @@ package object config {&lt;br/&gt;
     .bytesConf(ByteUnit.MiB)&lt;br/&gt;
     .createOptional&lt;br/&gt;
 &lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val DRIVER_LOG_DFS_DIR =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.driver.log.dfsDir&quot;).stringConf.createOptional&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val DRIVER_LOG_LAYOUT =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.driver.log.layout&quot;)&lt;br/&gt;
+      .stringConf&lt;br/&gt;
+      .createOptional&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val DRIVER_LOG_PERSISTTODFS =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.driver.log.persistToDfs.enabled&quot;)&lt;br/&gt;
+      .booleanConf&lt;br/&gt;
+      .createWithDefault(false)&lt;br/&gt;
+&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EVENT_LOG_COMPRESS =&lt;br/&gt;
     ConfigBuilder(&quot;spark.eventLog.compress&quot;)&lt;br/&gt;
       .booleanConf&lt;br/&gt;
@@ -58,6 +72,11 @@ package object config {&lt;br/&gt;
       .booleanConf&lt;br/&gt;
       .createWithDefault(false)&lt;br/&gt;
 &lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EVENT_LOG_ALLOW_EC =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.eventLog.allowErasureCoding&quot;)&lt;br/&gt;
+      .booleanConf&lt;br/&gt;
+      .createWithDefault(false)&lt;br/&gt;
+&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EVENT_LOG_TESTING =&lt;br/&gt;
     ConfigBuilder(&quot;spark.eventLog.testing&quot;)&lt;br/&gt;
       .internal()&lt;br/&gt;
@@ -69,15 +88,34 @@ package object config {&lt;br/&gt;
     .bytesConf(ByteUnit.KiB)&lt;br/&gt;
     .createWithDefaultString(&quot;100k&quot;)&lt;br/&gt;
 &lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EVENT_LOG_STAGE_EXECUTOR_METRICS =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.eventLog.logStageExecutorMetrics.enabled&quot;)&lt;br/&gt;
+      .booleanConf&lt;br/&gt;
+      .createWithDefault(false)&lt;br/&gt;
+&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EVENT_LOG_OVERWRITE =&lt;br/&gt;
     ConfigBuilder(&quot;spark.eventLog.overwrite&quot;).booleanConf.createWithDefault(false)&lt;br/&gt;
 &lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EVENT_LOG_CALLSITE_FORM =&lt;br/&gt;
-    ConfigBuilder(&quot;spark.eventLog.callsite&quot;).stringConf.createWithDefault(&quot;short&quot;)&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EVENT_LOG_CALLSITE_LONG_FORM =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.eventLog.longForm.enabled&quot;).booleanConf.createWithDefault(false)&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EXECUTOR_CLASS_PATH =&lt;br/&gt;
     ConfigBuilder(SparkLauncher.EXECUTOR_EXTRA_CLASSPATH).stringConf.createOptional&lt;br/&gt;
 &lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EXECUTOR_HEARTBEAT_DROP_ZERO_ACCUMULATOR_UPDATES =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.executor.heartbeat.dropZeroAccumulatorUpdates&quot;)&lt;br/&gt;
+      .internal()&lt;br/&gt;
+      .booleanConf&lt;br/&gt;
+      .createWithDefault(true)&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EXECUTOR_HEARTBEAT_INTERVAL =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.executor.heartbeatInterval&quot;)&lt;br/&gt;
+      .timeConf(TimeUnit.MILLISECONDS)&lt;br/&gt;
+      .createWithDefaultString(&quot;10s&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EXECUTOR_HEARTBEAT_MAX_FAILURES =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.executor.heartbeat.maxFailures&quot;).internal().intConf.createWithDefault(60)&lt;br/&gt;
+&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EXECUTOR_JAVA_OPTIONS =&lt;br/&gt;
     ConfigBuilder(SparkLauncher.EXECUTOR_EXTRA_JAVA_OPTIONS).stringConf.createOptional&lt;br/&gt;
 &lt;br/&gt;
@@ -147,14 +185,18 @@ package object config {&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val SHUFFLE_SERVICE_PORT =&lt;br/&gt;
     ConfigBuilder(&quot;spark.shuffle.service.port&quot;).intConf.createWithDefault(7337)&lt;br/&gt;
 &lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val KEYTAB = ConfigBuilder(&quot;spark.yarn.keytab&quot;)&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val KEYTAB = ConfigBuilder(&quot;spark.kerberos.keytab&quot;)&lt;br/&gt;
     .doc(&quot;Location of user&apos;s keytab.&quot;)&lt;br/&gt;
     .stringConf.createOptional&lt;br/&gt;
 &lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val PRINCIPAL = ConfigBuilder(&quot;spark.yarn.principal&quot;)&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val PRINCIPAL = ConfigBuilder(&quot;spark.kerberos.principal&quot;)&lt;br/&gt;
     .doc(&quot;Name of the Kerberos principal.&quot;)&lt;br/&gt;
     .stringConf.createOptional&lt;br/&gt;
 &lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val KERBEROS_RELOGIN_PERIOD = ConfigBuilder(&quot;spark.kerberos.relogin.period&quot;)&lt;br/&gt;
+    .timeConf(TimeUnit.SECONDS)&lt;br/&gt;
+    .createWithDefaultString(&quot;1m&quot;)&lt;br/&gt;
+&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EXECUTOR_INSTANCES = ConfigBuilder(&quot;spark.executor.instances&quot;)&lt;br/&gt;
     .intConf&lt;br/&gt;
     .createOptional&lt;br/&gt;
@@ -239,7 +281,7 @@ package object config {&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val LISTENER_BUS_EVENT_QUEUE_CAPACITY =&lt;br/&gt;
     ConfigBuilder(&quot;spark.scheduler.listenerbus.eventqueue.capacity&quot;)&lt;br/&gt;
       .intConf&lt;br/&gt;
-      .checkValue(_ &amp;gt; 0, &quot;The capacity of listener bus event queue must not be negative&quot;)&lt;br/&gt;
+      .checkValue(_ &amp;gt; 0, &quot;The capacity of listener bus event queue must be positive&quot;)&lt;br/&gt;
       .createWithDefault(10000)&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val LISTENER_BUS_METRICS_MAX_LISTENER_CLASSES_TIMED =&lt;br/&gt;
@@ -387,8 +429,9 @@ package object config {&lt;br/&gt;
       .internal()&lt;br/&gt;
       .doc(&quot;The chunk size in bytes during writing out the bytes of ChunkedByteBuffer.&quot;)&lt;br/&gt;
       .bytesConf(ByteUnit.BYTE)&lt;br/&gt;
-      .checkValue(_ &amp;lt;= Int.MaxValue, &quot;The chunk size during writing out the bytes of&quot; +&lt;br/&gt;
-        &quot; ChunkedByteBuffer should not larger than Int.MaxValue.&quot;)&lt;br/&gt;
+      .checkValue(_ &amp;lt;= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,&lt;br/&gt;
+        &quot;The chunk size during writing out the bytes of ChunkedByteBuffer should&quot; +&lt;br/&gt;
+          s&quot; be less than or equal to ${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}.&quot;)&lt;br/&gt;
       .createWithDefault(64 * 1024 * 1024)&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val CHECKPOINT_COMPRESS =&lt;br/&gt;
@@ -459,8 +502,9 @@ package object config {&lt;br/&gt;
         &quot;otherwise specified. These buffers reduce the number of disk seeks and system calls &quot; +&lt;br/&gt;
         &quot;made in creating intermediate shuffle files.&quot;)&lt;br/&gt;
       .bytesConf(ByteUnit.KiB)&lt;br/&gt;
-      .checkValue(v =&amp;gt; v &amp;gt; 0 &amp;amp;&amp;amp; v &amp;lt;= Int.MaxValue / 1024,&lt;br/&gt;
-        s&quot;The file buffer size must be greater than 0 and less than ${Int.MaxValue / 1024}.&quot;)&lt;br/&gt;
+      .checkValue(v =&amp;gt; v &amp;gt; 0 &amp;amp;&amp;amp; v &amp;lt;= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024,&lt;br/&gt;
+        s&quot;The file buffer size must be positive and less than or equal to&quot; +&lt;br/&gt;
+          s&quot; ${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024}.&quot;)&lt;br/&gt;
       .createWithDefaultString(&quot;32k&quot;)&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val SHUFFLE_UNSAFE_FILE_OUTPUT_BUFFER_SIZE =&lt;br/&gt;
@@ -468,16 +512,18 @@ package object config {&lt;br/&gt;
       .doc(&quot;The file system for this buffer size after each partition &quot; +&lt;br/&gt;
         &quot;is written in unsafe shuffle writer. In KiB unless otherwise specified.&quot;)&lt;br/&gt;
       .bytesConf(ByteUnit.KiB)&lt;br/&gt;
-      .checkValue(v =&amp;gt; v &amp;gt; 0 &amp;amp;&amp;amp; v &amp;lt;= Int.MaxValue / 1024,&lt;br/&gt;
-        s&quot;The buffer size must be greater than 0 and less than ${Int.MaxValue / 1024}.&quot;)&lt;br/&gt;
+      .checkValue(v =&amp;gt; v &amp;gt; 0 &amp;amp;&amp;amp; v &amp;lt;= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024,&lt;br/&gt;
+        s&quot;The buffer size must be positive and less than or equal to&quot; +&lt;br/&gt;
+          s&quot; ${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH / 1024}.&quot;)&lt;br/&gt;
       .createWithDefaultString(&quot;32k&quot;)&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val SHUFFLE_DISK_WRITE_BUFFER_SIZE =&lt;br/&gt;
     ConfigBuilder(&quot;spark.shuffle.spill.diskWriteBufferSize&quot;)&lt;br/&gt;
       .doc(&quot;The buffer size, in bytes, to use when writing the sorted records to an on-disk file.&quot;)&lt;br/&gt;
       .bytesConf(ByteUnit.BYTE)&lt;br/&gt;
-      .checkValue(v =&amp;gt; v &amp;gt; 0 &amp;amp;&amp;amp; v &amp;lt;= Int.MaxValue,&lt;br/&gt;
-        s&quot;The buffer size must be greater than 0 and less than ${Int.MaxValue}.&quot;)&lt;br/&gt;
+      .checkValue(v =&amp;gt; v &amp;gt; 12 &amp;amp;&amp;amp; v &amp;lt;= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,&lt;br/&gt;
+        s&quot;The buffer size must be greater than 12 and less than or equal to &quot; +&lt;br/&gt;
+          s&quot;${ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH}.&quot;)&lt;br/&gt;
       .createWithDefault(1024 * 1024)&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val UNROLL_MEMORY_CHECK_PERIOD =&lt;br/&gt;
@@ -524,6 +570,12 @@ package object config {&lt;br/&gt;
       .stringConf&lt;br/&gt;
       .createOptional&lt;br/&gt;
 &lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val UI_REQUEST_HEADER_SIZE =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.ui.requestHeaderSize&quot;)&lt;br/&gt;
+      .doc(&quot;Value for HTTP request header size in bytes.&quot;)&lt;br/&gt;
+      .bytesConf(ByteUnit.BYTE)&lt;br/&gt;
+      .createWithDefaultString(&quot;8k&quot;)&lt;br/&gt;
+&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val EXTRA_LISTENERS = ConfigBuilder(&quot;spark.extraListeners&quot;)&lt;br/&gt;
     .doc(&quot;Class names of listeners to add to SparkContext during initialization.&quot;)&lt;br/&gt;
     .stringConf&lt;br/&gt;
@@ -580,7 +632,7 @@ package object config {&lt;br/&gt;
       .internal()&lt;br/&gt;
       .doc(&quot;For testing only, controls the size of chunks when memory mapping a file&quot;)&lt;br/&gt;
       .bytesConf(ByteUnit.BYTE)&lt;br/&gt;
-      .createWithDefault(Int.MaxValue)&lt;br/&gt;
+      .createWithDefault(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val BARRIER_SYNC_TIMEOUT =&lt;br/&gt;
     ConfigBuilder(&quot;spark.barrier.sync.timeout&quot;)&lt;br/&gt;
@@ -592,6 +644,14 @@ package object config {&lt;br/&gt;
       .checkValue(v =&amp;gt; v &amp;gt; 0, &quot;The value should be a positive time value.&quot;)&lt;br/&gt;
       .createWithDefaultString(&quot;365d&quot;)&lt;br/&gt;
 &lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val UNSCHEDULABLE_TASKSET_TIMEOUT =&lt;br/&gt;
+    ConfigBuilder(&quot;spark.scheduler.blacklist.unschedulableTaskSetTimeout&quot;)&lt;br/&gt;
+      .doc(&quot;The timeout in seconds to wait to acquire a new executor and schedule a task &quot; +&lt;br/&gt;
+        &quot;before aborting a TaskSet which is unschedulable because of being completely blacklisted.&quot;)&lt;br/&gt;
+      .timeConf(TimeUnit.SECONDS)&lt;br/&gt;
+      .checkValue(v =&amp;gt; v &amp;gt;= 0, &quot;The value should be a non negative time value.&quot;)&lt;br/&gt;
+      .createWithDefault(120)&lt;br/&gt;
+&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val BARRIER_MAX_CONCURRENT_TASKS_CHECK_INTERVAL =&lt;br/&gt;
     ConfigBuilder(&quot;spark.scheduler.barrier.maxConcurrentTasksCheck.interval&quot;)&lt;br/&gt;
       .doc(&quot;Time in seconds to wait between a max concurrent tasks check failure and the next &quot; +&lt;br/&gt;
@@ -618,4 +678,14 @@ package object config {
       .intConf
       .checkValue(v =&amp;gt; v &amp;gt; 0, &quot;The max failures should be a positive value.&quot;)
       .createWithDefault(40)
+
+  private[spark] val EXECUTOR_PLUGINS =
+    ConfigBuilder(&quot;spark.executor.plugins&quot;)
+      .doc(&quot;Comma-separated list of class names for \&quot;plugins\&quot; implementing &quot; +
+        &quot;org.apache.spark.ExecutorPlugin.  Plugins have the same privileges as any task &quot; +
+        &quot;in a Spark executor.  They can also interfere with task execution and fail in &quot; +
+        &quot;unexpected ways.  So be sure to only use this for trusted plugins.&quot;)
+      .stringConf
+      .toSequence
+      .createWithDefault(Nil)
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala b/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala&lt;br/&gt;
index 3e60c50ada59b..7477e03bfaa76 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/internal/io/HadoopMapReduceCommitProtocol.scala&lt;br/&gt;
@@ -91,7 +91,7 @@ class HadoopMapReduceCommitProtocol(&lt;br/&gt;
   private def stagingDir = new Path(path, &quot;.spark-staging-&quot; + jobId)&lt;br/&gt;
 &lt;br/&gt;
   protected def setupCommitter(context: TaskAttemptContext): OutputCommitter = {&lt;br/&gt;
-    val format = context.getOutputFormatClass.newInstance()&lt;br/&gt;
+    val format = context.getOutputFormatClass.getConstructor().newInstance()&lt;br/&gt;
     // If OutputFormat is Configurable, we should set conf to it.&lt;br/&gt;
     format match {&lt;br/&gt;
       case c: Configurable =&amp;gt; c.setConf(context.getConfiguration)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala b/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala&lt;br/&gt;
index 9ebd0aa301592..3a58ea816937b 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/internal/io/SparkHadoopWriter.scala&lt;br/&gt;
@@ -256,7 +256,7 @@ class HadoopMapRedWriteConfigUtil&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V: ClassTag&amp;#93;&lt;/span&gt;(conf: SerializableJobConf)&lt;br/&gt;
   private def getOutputFormat(): OutputFormat&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt; = {
     require(outputFormat != null, &quot;Must call initOutputFormat first.&quot;)
 
-    outputFormat.newInstance()
+    outputFormat.getConstructor().newInstance()
   }&lt;br/&gt;
 &lt;br/&gt;
   // --------------------------------------------------------------------------&lt;br/&gt;
@@ -379,7 +379,7 @@ class HadoopMapReduceWriteConfigUtil&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V: ClassTag&amp;#93;&lt;/span&gt;(conf: SerializableConfigura&lt;br/&gt;
   private def getOutputFormat(): NewOutputFormat&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt; = {     require(outputFormat != null, &quot;Must call initOutputFormat first.&quot;) -    outputFormat.newInstance()+    outputFormat.getConstructor().newInstance()   }&lt;br/&gt;
 &lt;br/&gt;
   // --------------------------------------------------------------------------&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala b/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala&lt;br/&gt;
index 7722db56ee297..0664c5ac752c1 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/io/CompressionCodec.scala&lt;br/&gt;
@@ -154,72 +154,19 @@ class LZFCompressionCodec(conf: SparkConf) extends CompressionCodec {&lt;br/&gt;
  */&lt;br/&gt;
 @DeveloperApi&lt;br/&gt;
 class SnappyCompressionCodec(conf: SparkConf) extends CompressionCodec {&lt;br/&gt;
-  val version = SnappyCompressionCodec.version&lt;br/&gt;
 &lt;br/&gt;
-  override def compressedOutputStream(s: OutputStream): OutputStream = {
-    val blockSize = conf.getSizeAsBytes(&quot;spark.io.compression.snappy.blockSize&quot;, &quot;32k&quot;).toInt
-    new SnappyOutputStreamWrapper(new SnappyOutputStream(s, blockSize))
-  }&lt;br/&gt;
-&lt;br/&gt;
-  override def compressedInputStream(s: InputStream): InputStream = new SnappyInputStream(s)&lt;br/&gt;
-}&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;br/&gt;
- * Object guards against memory leak bug in snappy-java library:&lt;br/&gt;
- * (&lt;a href=&quot;https://github.com/xerial/snappy-java/issues/131&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/xerial/snappy-java/issues/131&lt;/a&gt;).&lt;br/&gt;
- * Before a new version of the library, we only call the method once and cache the result.&lt;br/&gt;
- */&lt;br/&gt;
-private final object SnappyCompressionCodec {&lt;br/&gt;
-  private lazy val version: String = try {&lt;br/&gt;
+  try {
     Snappy.getNativeLibraryVersion
   } catch {
     case e: Error =&amp;gt; throw new IllegalArgumentException(e)
   }&lt;br/&gt;
-}&lt;br/&gt;
 &lt;br/&gt;
-/**&lt;br/&gt;
- * Wrapper over `SnappyOutputStream` which guards against write-after-close and double-close&lt;br/&gt;
- * issues. See &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-7660&quot; title=&quot;Snappy-java buffer-sharing bug leads to data corruption / test failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-7660&quot;&gt;&lt;del&gt;SPARK-7660&lt;/del&gt;&lt;/a&gt; for more details. This wrapping can be removed if we upgrade to a version&lt;br/&gt;
- * of snappy-java that contains the fix for &lt;a href=&quot;https://github.com/xerial/snappy-java/issues/107&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/xerial/snappy-java/issues/107&lt;/a&gt;.&lt;br/&gt;
- */&lt;br/&gt;
-private final class SnappyOutputStreamWrapper(os: SnappyOutputStream) extends OutputStream {&lt;br/&gt;
-&lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var closed: Boolean = false&lt;br/&gt;
-&lt;br/&gt;
-  override def write(b: Int): Unit = {&lt;br/&gt;
-    if (closed) {
-      throw new IOException(&quot;Stream is closed&quot;)
-    }&lt;br/&gt;
-    os.write(b)&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  override def write(b: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;): Unit = {&lt;br/&gt;
-    if (closed) {-      throw new IOException(&quot;Stream is closed&quot;)-    }&lt;br/&gt;
-    os.write(b)&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  override def write(b: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;, off: Int, len: Int): Unit = {&lt;br/&gt;
-    if (closed) {
-      throw new IOException(&quot;Stream is closed&quot;)
-    }&lt;br/&gt;
-    os.write(b, off, len)&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  override def flush(): Unit = {&lt;br/&gt;
-    if (closed) {-      throw new IOException(&quot;Stream is closed&quot;)-    }&lt;br/&gt;
-    os.flush()&lt;br/&gt;
+  override def compressedOutputStream(s: OutputStream): OutputStream = {
+    val blockSize = conf.getSizeAsBytes(&quot;spark.io.compression.snappy.blockSize&quot;, &quot;32k&quot;).toInt
+    new SnappyOutputStream(s, blockSize)
   }&lt;br/&gt;
 &lt;br/&gt;
-  override def close(): Unit = {&lt;br/&gt;
-    if (!closed) {
-      closed = true
-      os.close()
-    }&lt;br/&gt;
-  }&lt;br/&gt;
+  override def compressedInputStream(s: InputStream): InputStream = new SnappyInputStream(s)&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala b/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala&lt;br/&gt;
index 0641adc2ab699..4fde2d0beaa71 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/memory/MemoryManager.scala&lt;br/&gt;
@@ -180,6 +180,34 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class MemoryManager(&lt;br/&gt;
     onHeapStorageMemoryPool.memoryUsed + offHeapStorageMemoryPool.memoryUsed&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   *  On heap execution memory currently in use, in bytes.&lt;br/&gt;
+   */&lt;br/&gt;
+  final def onHeapExecutionMemoryUsed: Long = synchronized {
+    onHeapExecutionMemoryPool.memoryUsed
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   *  Off heap execution memory currently in use, in bytes.&lt;br/&gt;
+   */&lt;br/&gt;
+  final def offHeapExecutionMemoryUsed: Long = synchronized {
+    offHeapExecutionMemoryPool.memoryUsed
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   *  On heap storage memory currently in use, in bytes.&lt;br/&gt;
+   */&lt;br/&gt;
+  final def onHeapStorageMemoryUsed: Long = synchronized {
+    onHeapStorageMemoryPool.memoryUsed
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   *  Off heap storage memory currently in use, in bytes.&lt;br/&gt;
+   */&lt;br/&gt;
+  final def offHeapStorageMemoryUsed: Long = synchronized {
+    offHeapStorageMemoryPool.memoryUsed
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
    * Returns the execution memory consumption, in bytes, for the given task.&lt;br/&gt;
    */&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/metrics/ExecutorMetricType.scala b/core/src/main/scala/org/apache/spark/metrics/ExecutorMetricType.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..cd10dad25e87b&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/metrics/ExecutorMetricType.scala&lt;br/&gt;
@@ -0,0 +1,104 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.spark.metrics&lt;br/&gt;
+&lt;br/&gt;
+import java.lang.management.{BufferPoolMXBean, ManagementFactory}&lt;br/&gt;
+import javax.management.ObjectName&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.spark.memory.MemoryManager&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Executor metric types for executor-level metrics stored in ExecutorMetrics.&lt;br/&gt;
+ */&lt;br/&gt;
+sealed trait ExecutorMetricType {
+  private[spark] def getMetricValue(memoryManager: MemoryManager): Long
+  private[spark] val name = getClass().getName().stripSuffix(&quot;$&quot;).split(&quot;&quot;&quot;\.&quot;&quot;&quot;).last
+}&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class MemoryManagerExecutorMetricType(&lt;br/&gt;
+    f: MemoryManager =&amp;gt; Long) extends ExecutorMetricType {&lt;br/&gt;
+  override private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def getMetricValue(memoryManager: MemoryManager): Long = {
+    f(memoryManager)
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class MBeanExecutorMetricType(mBeanName: String)&lt;br/&gt;
+  extends ExecutorMetricType {&lt;br/&gt;
+  private val bean = ManagementFactory.newPlatformMXBeanProxy(&lt;br/&gt;
+    ManagementFactory.getPlatformMBeanServer,&lt;br/&gt;
+    new ObjectName(mBeanName).toString, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BufferPoolMXBean&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+&lt;br/&gt;
+  override private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def getMetricValue(memoryManager: MemoryManager): Long = {
+    bean.getMemoryUsed
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+case object JVMHeapMemory extends ExecutorMetricType {&lt;br/&gt;
+  override private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def getMetricValue(memoryManager: MemoryManager): Long = {
+    ManagementFactory.getMemoryMXBean.getHeapMemoryUsage().getUsed()
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+case object JVMOffHeapMemory extends ExecutorMetricType {&lt;br/&gt;
+  override private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def getMetricValue(memoryManager: MemoryManager): Long = {
+    ManagementFactory.getMemoryMXBean.getNonHeapMemoryUsage().getUsed()
+  }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+case object OnHeapExecutionMemory extends MemoryManagerExecutorMetricType(&lt;br/&gt;
+  _.onHeapExecutionMemoryUsed)&lt;br/&gt;
+&lt;br/&gt;
+case object OffHeapExecutionMemory extends MemoryManagerExecutorMetricType(&lt;br/&gt;
+  _.offHeapExecutionMemoryUsed)&lt;br/&gt;
+&lt;br/&gt;
+case object OnHeapStorageMemory extends MemoryManagerExecutorMetricType(&lt;br/&gt;
+  _.onHeapStorageMemoryUsed)&lt;br/&gt;
+&lt;br/&gt;
+case object OffHeapStorageMemory extends MemoryManagerExecutorMetricType(&lt;br/&gt;
+  _.offHeapStorageMemoryUsed)&lt;br/&gt;
+&lt;br/&gt;
+case object OnHeapUnifiedMemory extends MemoryManagerExecutorMetricType(&lt;br/&gt;
+  (m =&amp;gt; m.onHeapExecutionMemoryUsed + m.onHeapStorageMemoryUsed))&lt;br/&gt;
+&lt;br/&gt;
+case object OffHeapUnifiedMemory extends MemoryManagerExecutorMetricType(&lt;br/&gt;
+  (m =&amp;gt; m.offHeapExecutionMemoryUsed + m.offHeapStorageMemoryUsed))&lt;br/&gt;
+&lt;br/&gt;
+case object DirectPoolMemory extends MBeanExecutorMetricType(&lt;br/&gt;
+  &quot;java.nio:type=BufferPool,name=direct&quot;)&lt;br/&gt;
+&lt;br/&gt;
+case object MappedPoolMemory extends MBeanExecutorMetricType(&lt;br/&gt;
+  &quot;java.nio:type=BufferPool,name=mapped&quot;)&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object ExecutorMetricType {
+  // List of all executor metric types
+  val values = IndexedSeq(
+    JVMHeapMemory,
+    JVMOffHeapMemory,
+    OnHeapExecutionMemory,
+    OffHeapExecutionMemory,
+    OnHeapStorageMemory,
+    OffHeapStorageMemory,
+    OnHeapUnifiedMemory,
+    OffHeapUnifiedMemory,
+    DirectPoolMemory,
+    MappedPoolMemory
+  )
+
+  // Map of executor metric type to its index in values.
+  val metricIdxMap =
+    Map[ExecutorMetricType, Int](ExecutorMetricType.values.zipWithIndex: _*)
+}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala b/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala&lt;br/&gt;
index 3457a2632277d..bb7b434e9a113 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/metrics/MetricsSystem.scala&lt;br/&gt;
@@ -179,7 +179,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class MetricsSystem private (&lt;br/&gt;
     sourceConfigs.foreach { kv =&amp;gt;&lt;br/&gt;
       val classPath = kv._2.getProperty(&quot;class&quot;)&lt;br/&gt;
       try {
-        val source = Utils.classForName(classPath).newInstance()
+        val source = Utils.classForName(classPath).getConstructor().newInstance()
         registerSource(source.asInstanceOf[Source])
       } catch {&lt;br/&gt;
         case e: Exception =&amp;gt; logError(&quot;Source class &quot; + classPath + &quot; cannot be instantiated&quot;, e)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala b/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala&lt;br/&gt;
index 859a2f6bcd456..61e74e05169cc 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/metrics/sink/StatsdSink.scala&lt;br/&gt;
@@ -17,7 +17,7 @@&lt;br/&gt;
 &lt;br/&gt;
 package org.apache.spark.metrics.sink&lt;br/&gt;
 &lt;br/&gt;
-import java.util.Properties&lt;br/&gt;
+import java.util.{Locale, Properties}&lt;br/&gt;
 import java.util.concurrent.TimeUnit&lt;br/&gt;
 &lt;br/&gt;
 import com.codahale.metrics.MetricRegistry&lt;br/&gt;
@@ -52,7 +52,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class StatsdSink(&lt;br/&gt;
 &lt;br/&gt;
   val pollPeriod = property.getProperty(STATSD_KEY_PERIOD, STATSD_DEFAULT_PERIOD).toInt&lt;br/&gt;
   val pollUnit =&lt;br/&gt;
-    TimeUnit.valueOf(property.getProperty(STATSD_KEY_UNIT, STATSD_DEFAULT_UNIT).toUpperCase)&lt;br/&gt;
+    TimeUnit.valueOf(&lt;br/&gt;
+      property.getProperty(STATSD_KEY_UNIT, STATSD_DEFAULT_UNIT).toUpperCase(Locale.ROOT))&lt;br/&gt;
 &lt;br/&gt;
   val prefix = property.getProperty(STATSD_KEY_PREFIX, STATSD_DEFAULT_PREFIX)&lt;br/&gt;
 &lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala b/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala&lt;br/&gt;
index 1d8a266d0079c..a58c8fa2e763f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/network/BlockTransferService.scala&lt;br/&gt;
@@ -26,8 +26,8 @@ import scala.reflect.ClassTag&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.network.buffer.{FileSegmentManagedBuffer, ManagedBuffer, NioManagedBuffer}&lt;br/&gt;
-import org.apache.spark.network.shuffle.{BlockFetchingListener, ShuffleClient, TempFileManager}&lt;br/&gt;
-import org.apache.spark.storage.{BlockId, StorageLevel}&lt;br/&gt;
+import org.apache.spark.network.shuffle.{BlockFetchingListener, DownloadFileManager, ShuffleClient}&lt;br/&gt;
+import org.apache.spark.storage.{BlockId, EncryptedManagedBuffer, StorageLevel}&lt;br/&gt;
 import org.apache.spark.util.ThreadUtils&lt;br/&gt;
 &lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;&lt;br/&gt;
@@ -68,7 +68,7 @@ abstract class BlockTransferService extends ShuffleClient with Closeable with Lo&lt;br/&gt;
       execId: String,&lt;br/&gt;
       blockIds: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       listener: BlockFetchingListener,&lt;br/&gt;
-      tempFileManager: TempFileManager): Unit&lt;br/&gt;
+      tempFileManager: DownloadFileManager): Unit&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
    * Upload a single block to a remote node, available only after [&lt;span class=&quot;error&quot;&gt;&amp;#91;init&amp;#93;&lt;/span&gt;] is invoked.&lt;br/&gt;
@@ -92,7 +92,7 @@ abstract class BlockTransferService extends ShuffleClient with Closeable with Lo&lt;br/&gt;
       port: Int,&lt;br/&gt;
       execId: String,&lt;br/&gt;
       blockId: String,&lt;br/&gt;
-      tempFileManager: TempFileManager): ManagedBuffer = {&lt;br/&gt;
+      tempFileManager: DownloadFileManager): ManagedBuffer = {&lt;br/&gt;
     // A monitor for the thread to wait on.&lt;br/&gt;
     val result = Promise&lt;span class=&quot;error&quot;&gt;&amp;#91;ManagedBuffer&amp;#93;&lt;/span&gt;()&lt;br/&gt;
     fetchBlocks(host, port, execId, Array(blockId),&lt;br/&gt;
@@ -104,6 +104,8 @@ abstract class BlockTransferService extends ShuffleClient with Closeable with Lo&lt;br/&gt;
           data match {&lt;br/&gt;
             case f: FileSegmentManagedBuffer =&amp;gt;&lt;br/&gt;
               result.success(f)&lt;br/&gt;
+            case e: EncryptedManagedBuffer =&amp;gt;&lt;br/&gt;
+              result.success(e)&lt;br/&gt;
             case _ =&amp;gt;&lt;br/&gt;
               val ret = ByteBuffer.allocate(data.size.toInt)&lt;br/&gt;
               ret.put(data.nioByteBuffer())&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala b/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala&lt;br/&gt;
index 1905632a936d3..dc55685b1e7bd 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/network/netty/NettyBlockTransferService.scala&lt;br/&gt;
@@ -33,7 +33,7 @@ import org.apache.spark.network.buffer.{ManagedBuffer, NioManagedBuffer}&lt;br/&gt;
 import org.apache.spark.network.client.{RpcResponseCallback, TransportClientBootstrap, TransportClientFactory}&lt;br/&gt;
 import org.apache.spark.network.crypto.{AuthClientBootstrap, AuthServerBootstrap}&lt;br/&gt;
 import org.apache.spark.network.server._&lt;br/&gt;
-import org.apache.spark.network.shuffle.{BlockFetchingListener, OneForOneBlockFetcher, RetryingBlockFetcher, TempFileManager}&lt;br/&gt;
+import org.apache.spark.network.shuffle.{BlockFetchingListener, DownloadFileManager, OneForOneBlockFetcher, RetryingBlockFetcher}&lt;br/&gt;
 import org.apache.spark.network.shuffle.protocol.{UploadBlock, UploadBlockStream}&lt;br/&gt;
 import org.apache.spark.network.util.JavaUtils&lt;br/&gt;
 import org.apache.spark.serializer.JavaSerializer&lt;br/&gt;
@@ -106,7 +106,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class NettyBlockTransferService(&lt;br/&gt;
       execId: String,&lt;br/&gt;
       blockIds: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       listener: BlockFetchingListener,&lt;br/&gt;
-      tempFileManager: TempFileManager): Unit = {&lt;br/&gt;
+      tempFileManager: DownloadFileManager): Unit = {&lt;br/&gt;
     logTrace(s&quot;Fetch blocks from $host:$port (executor id $execId)&quot;)&lt;br/&gt;
     try {&lt;br/&gt;
       val blockFetchStarter = new RetryingBlockFetcher.BlockFetchStarter {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/package.scala b/core/src/main/scala/org/apache/spark/package.scala&lt;br/&gt;
index 8058a4d5dbdea..5d0639e92c36a 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/package.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/package.scala&lt;br/&gt;
@@ -19,6 +19,8 @@ package org.apache&lt;br/&gt;
 &lt;br/&gt;
 import java.util.Properties&lt;br/&gt;
 &lt;br/&gt;
+import org.apache.spark.util.VersionUtils&lt;br/&gt;
+&lt;br/&gt;
 /**&lt;br/&gt;
  * Core Spark functionality. [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.SparkContext&amp;#93;&lt;/span&gt;] serves as the main entry point to&lt;br/&gt;
  * Spark, while [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.rdd.RDD&amp;#93;&lt;/span&gt;] is the data type representing a distributed collection,&lt;br/&gt;
@@ -89,6 +91,7 @@ package object spark {&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   val SPARK_VERSION = SparkBuildInfo.spark_version&lt;br/&gt;
+  val SPARK_VERSION_SHORT = VersionUtils.shortVersion(SparkBuildInfo.spark_version)&lt;br/&gt;
   val SPARK_BRANCH = SparkBuildInfo.spark_branch&lt;br/&gt;
   val SPARK_REVISION = SparkBuildInfo.spark_revision&lt;br/&gt;
   val SPARK_BUILD_USER = SparkBuildInfo.spark_build_user&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala b/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala&lt;br/&gt;
index a14bad47dfe10..039dbcbd5e035 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/rdd/BinaryFileRDD.scala&lt;br/&gt;
@@ -41,7 +41,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class BinaryFileRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     // traversing a large number of directories and files. Parallelize it.&lt;br/&gt;
     conf.setIfUnset(FileInputFormat.LIST_STATUS_NUM_THREADS,&lt;br/&gt;
       Runtime.getRuntime.availableProcessors().toString)&lt;br/&gt;
-    val inputFormat = inputFormatClass.newInstance&lt;br/&gt;
+    val inputFormat = inputFormatClass.getConstructor().newInstance()&lt;br/&gt;
     inputFormat match {
       case configurable: Configurable =&amp;gt;
         configurable.setConf(conf)
diff --git a/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala b/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala
index 4574c3724962e..7e76731f5e454 100644
--- a/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala
+++ b/core/src/main/scala/org/apache/spark/rdd/CoGroupedRDD.scala
@@ -143,8 +143,10 @@ class CoGroupedRDD[K: ClassTag](
 
       case shuffleDependency: ShuffleDependency[_, _, _] =&amp;gt;
         // Read map outputs of shuffle
+        val metrics = context.taskMetrics().createTempShuffleReadMetrics()
         val it = SparkEnv.get.shuffleManager
-          .getReader(shuffleDependency.shuffleHandle, split.index, split.index + 1, context)
+          .getReader(
+            shuffleDependency.shuffleHandle, split.index, split.index + 1, context, metrics)
           .read()
         rddIterators += ((it, depNum))
     }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala b/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala&lt;br/&gt;
index 2d66d25ba39fa..483de28d92ab7 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/rdd/NewHadoopRDD.scala&lt;br/&gt;
@@ -120,7 +120,7 @@ class NewHadoopRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   override def getPartitions: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Partition&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    val inputFormat = inputFormatClass.newInstance&lt;br/&gt;
+    val inputFormat = inputFormatClass.getConstructor().newInstance()&lt;br/&gt;
     inputFormat match {
       case configurable: Configurable =&amp;gt;
         configurable.setConf(_conf)
@@ -183,7 +183,7 @@ class NewHadoopRDD[K, V](
         }&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
-      private val format = inputFormatClass.newInstance&lt;br/&gt;
+      private val format = inputFormatClass.getConstructor().newInstance()&lt;br/&gt;
       format match {&lt;br/&gt;
         case configurable: Configurable =&amp;gt;&lt;br/&gt;
           configurable.setConf(conf)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala b/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala&lt;br/&gt;
index a5992022d0832..5b1c024257529 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/rdd/OrderedRDDFunctions.scala&lt;br/&gt;
@@ -35,7 +35,8 @@ import org.apache.spark.internal.Logging&lt;br/&gt;
  *&lt;br/&gt;
  *   val rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, Int)&amp;#93;&lt;/span&gt; = ...&lt;br/&gt;
  *   implicit val caseInsensitiveOrdering = new Ordering&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; {
- *     override def compare(a: String, b: String) = a.toLowerCase.compare(b.toLowerCase)
+ *     override def compare(a: String, b: String) =
+ *       a.toLowerCase(Locale.ROOT).compare(b.toLowerCase(Locale.ROOT))
  *   }&lt;br/&gt;
  *&lt;br/&gt;
  *   // Sort by key, using the above case insensitive ordering.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala b/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala&lt;br/&gt;
index e68c6b1366c7f..4bf4f082d0382 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/rdd/PairRDDFunctions.scala&lt;br/&gt;
@@ -394,7 +394,7 @@ class PairRDDFunctions&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(self: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    *&lt;br/&gt;
    * The algorithm used is based on streamlib&apos;s implementation of &quot;HyperLogLog in Practice:&lt;br/&gt;
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm&quot;, available&lt;br/&gt;
-   * &amp;lt;a href=&quot;http://dx.doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
+   * &amp;lt;a href=&quot;https://doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
    *&lt;br/&gt;
    * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is&lt;br/&gt;
    * greater than `p`) would trigger sparse representation of registers, which may reduce the&lt;br/&gt;
@@ -436,7 +436,7 @@ class PairRDDFunctions&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(self: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    *&lt;br/&gt;
    * The algorithm used is based on streamlib&apos;s implementation of &quot;HyperLogLog in Practice:&lt;br/&gt;
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm&quot;, available&lt;br/&gt;
-   * &amp;lt;a href=&quot;http://dx.doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
+   * &amp;lt;a href=&quot;https://doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
    *&lt;br/&gt;
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.&lt;br/&gt;
    *                   It must be greater than 0.000017.&lt;br/&gt;
@@ -456,7 +456,7 @@ class PairRDDFunctions&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(self: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    *&lt;br/&gt;
    * The algorithm used is based on streamlib&apos;s implementation of &quot;HyperLogLog in Practice:&lt;br/&gt;
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm&quot;, available&lt;br/&gt;
-   * &amp;lt;a href=&quot;http://dx.doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
+   * &amp;lt;a href=&quot;https://doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
    *&lt;br/&gt;
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.&lt;br/&gt;
    *                   It must be greater than 0.000017.&lt;br/&gt;
@@ -473,7 +473,7 @@ class PairRDDFunctions&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(self: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, V)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
    *&lt;br/&gt;
    * The algorithm used is based on streamlib&apos;s implementation of &quot;HyperLogLog in Practice:&lt;br/&gt;
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm&quot;, available&lt;br/&gt;
-   * &amp;lt;a href=&quot;http://dx.doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
+   * &amp;lt;a href=&quot;https://doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
    *&lt;br/&gt;
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.&lt;br/&gt;
    *                   It must be greater than 0.000017.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/rdd/RDD.scala b/core/src/main/scala/org/apache/spark/rdd/RDD.scala&lt;br/&gt;
index 61ad6dfdb2215..6a25ee20b2c68 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/rdd/RDD.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/rdd/RDD.scala&lt;br/&gt;
@@ -42,7 +42,8 @@ import org.apache.spark.partial.GroupedCountEvaluator&lt;br/&gt;
 import org.apache.spark.partial.PartialResult&lt;br/&gt;
 import org.apache.spark.storage.{RDDBlockId, StorageLevel}&lt;br/&gt;
 import org.apache.spark.util.{BoundedPriorityQueue, Utils}&lt;br/&gt;
-import org.apache.spark.util.collection.{OpenHashMap, Utils =&amp;gt; collectionUtils}&lt;br/&gt;
+import org.apache.spark.util.collection.{ExternalAppendOnlyMap, OpenHashMap,
+  Utils =&amp;gt; collectionUtils}&lt;br/&gt;
 import org.apache.spark.util.random.{BernoulliCellSampler, BernoulliSampler, PoissonSampler,
   SamplingUtils}&lt;br/&gt;
 &lt;br/&gt;
@@ -396,7 +397,20 @@ abstract class RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
    * Return a new RDD containing the distinct elements in this RDD.&lt;br/&gt;
    */&lt;br/&gt;
   def distinct(numPartitions: Int)(implicit ord: Ordering&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = null): RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = withScope {&lt;br/&gt;
-    map(x =&amp;gt; (x, null)).reduceByKey((x, y) =&amp;gt; x, numPartitions).map(_._1)&lt;br/&gt;
+    def removeDuplicatesInPartition(partition: Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;): Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = {
+      // Create an instance of external append only map which ignores values.
+      val map = new ExternalAppendOnlyMap[T, Null, Null](
+        createCombiner = value =&amp;gt; null,
+        mergeValue = (a, b) =&amp;gt; a,
+        mergeCombiners = (a, b) =&amp;gt; a)
+      map.insertAll(partition.map(_ -&amp;gt; null))
+      map.iterator.map(_._1)
+    }&lt;br/&gt;
+    partitioner match {
+      case Some(p) if numPartitions == partitions.length =&amp;gt;
+        mapPartitions(removeDuplicatesInPartition, preservesPartitioning = true)
+      case _ =&amp;gt; map(x =&amp;gt; (x, null)).reduceByKey((x, y) =&amp;gt; x, numPartitions).map(_._1)
+    }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
@@ -1244,7 +1258,7 @@ abstract class RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
    *&lt;br/&gt;
    * The algorithm used is based on streamlib&apos;s implementation of &quot;HyperLogLog in Practice:&lt;br/&gt;
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm&quot;, available&lt;br/&gt;
-   * &amp;lt;a href=&quot;http://dx.doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
+   * &amp;lt;a href=&quot;https://doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
    *&lt;br/&gt;
    * The relative accuracy is approximately `1.054 / sqrt(2^p)`. Setting a nonzero (`sp` is greater&lt;br/&gt;
    * than `p`) would trigger sparse representation of registers, which may reduce the memory&lt;br/&gt;
@@ -1276,7 +1290,7 @@ abstract class RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
    *&lt;br/&gt;
    * The algorithm used is based on streamlib&apos;s implementation of &quot;HyperLogLog in Practice:&lt;br/&gt;
    * Algorithmic Engineering of a State of The Art Cardinality Estimation Algorithm&quot;, available&lt;br/&gt;
-   * &amp;lt;a href=&quot;http://dx.doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
+   * &amp;lt;a href=&quot;https://doi.org/10.1145/2452376.2452456&quot;&amp;gt;here&amp;lt;/a&amp;gt;.&lt;br/&gt;
    *&lt;br/&gt;
    * @param relativeSD Relative accuracy. Smaller values create counters that require more space.&lt;br/&gt;
    *                   It must be greater than 0.000017.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala b/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala&lt;br/&gt;
index 53d69ba26811f..3abb2d8a11f35 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/rdd/RDDOperationScope.scala&lt;br/&gt;
@@ -41,7 +41,7 @@ import org.apache.spark.internal.Logging&lt;br/&gt;
  * There is no particular relationship between an operation scope and a stage or a job.&lt;br/&gt;
  * A scope may live inside one stage (e.g. map) or span across multiple jobs (e.g. take).&lt;br/&gt;
  */&lt;br/&gt;
-@JsonInclude(Include.NON_NULL)&lt;br/&gt;
+@JsonInclude(Include.NON_ABSENT)&lt;br/&gt;
 @JsonPropertyOrder(Array(&quot;id&quot;, &quot;name&quot;, &quot;parent&quot;))&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class RDDOperationScope(&lt;br/&gt;
     val name: String,&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala b/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala&lt;br/&gt;
index e8f9b27b7eb55..5ec99b7f4f3ab 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/rdd/ShuffledRDD.scala&lt;br/&gt;
@@ -101,7 +101,9 @@ class ShuffledRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag, V: ClassTag, C: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
 &lt;br/&gt;
   override def compute(split: Partition, context: TaskContext): Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;(K, C)&amp;#93;&lt;/span&gt; = {
     val dep = dependencies.head.asInstanceOf[ShuffleDependency[K, V, C]]
-    SparkEnv.get.shuffleManager.getReader(dep.shuffleHandle, split.index, split.index + 1, context)
+    val metrics = context.taskMetrics().createTempShuffleReadMetrics()
+    SparkEnv.get.shuffleManager.getReader(
+      dep.shuffleHandle, split.index, split.index + 1, context, metrics)
       .read()
       .asInstanceOf[Iterator[(K, C)]]
   }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala b/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala&lt;br/&gt;
index a733eaa5d7e53..42d190377f104 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/rdd/SubtractedRDD.scala&lt;br/&gt;
@@ -107,9 +107,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SubtractedRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;K: ClassTag, V: ClassTag, W: ClassTag&amp;#93;&lt;/span&gt;(&lt;br/&gt;
             .asInstanceOf[Iterator[Product2&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;]].foreach(op)&lt;br/&gt;
 &lt;br/&gt;
         case shuffleDependency: ShuffleDependency&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _, _&amp;#93;&lt;/span&gt; =&amp;gt;&lt;br/&gt;
+          val metrics = context.taskMetrics().createTempShuffleReadMetrics()&lt;br/&gt;
           val iter = SparkEnv.get.shuffleManager&lt;br/&gt;
             .getReader(&lt;br/&gt;
-              shuffleDependency.shuffleHandle, partition.index, partition.index + 1, context)&lt;br/&gt;
+              shuffleDependency.shuffleHandle,&lt;br/&gt;
+              partition.index,&lt;br/&gt;
+              partition.index + 1,&lt;br/&gt;
+              context,&lt;br/&gt;
+              metrics)&lt;br/&gt;
             .read()&lt;br/&gt;
           iter.foreach(op)&lt;br/&gt;
       }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala b/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala&lt;br/&gt;
index 9f3d0745c33c9..eada762b99c8e 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/rdd/WholeTextFileRDD.scala&lt;br/&gt;
@@ -44,7 +44,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class WholeTextFileRDD(&lt;br/&gt;
     // traversing a large number of directories and files. Parallelize it.&lt;br/&gt;
     conf.setIfUnset(FileInputFormat.LIST_STATUS_NUM_THREADS,&lt;br/&gt;
       Runtime.getRuntime.availableProcessors().toString)&lt;br/&gt;
-    val inputFormat = inputFormatClass.newInstance&lt;br/&gt;
+    val inputFormat = inputFormatClass.getConstructor().newInstance()&lt;br/&gt;
     inputFormat match {&lt;br/&gt;
       case configurable: Configurable =&amp;gt;&lt;br/&gt;
         configurable.setConf(conf)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala b/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala&lt;br/&gt;
index 0a5fe5a1d3ee1..bd0fe90b1f3b6 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/AccumulableInfo.scala&lt;br/&gt;
@@ -22,7 +22,7 @@ import org.apache.spark.annotation.DeveloperApi&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
  * :: DeveloperApi ::&lt;br/&gt;
- * Information about an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.Accumulable&amp;#93;&lt;/span&gt;] modified during a task or stage.&lt;br/&gt;
+ * Information about an [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.util.AccumulatorV2&amp;#93;&lt;/span&gt;] modified during a task or stage.&lt;br/&gt;
  *&lt;br/&gt;
  * @param id accumulator ID&lt;br/&gt;
  * @param name accumulator name&lt;br/&gt;
@@ -47,33 +47,3 @@ case class AccumulableInfo private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; (&lt;br/&gt;
     private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val countFailedValues: Boolean,&lt;br/&gt;
     // TODO: use this to identify internal task metrics instead of encoding it in the name&lt;br/&gt;
     private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val metadata: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None)&lt;br/&gt;
-&lt;br/&gt;
-&lt;br/&gt;
-/**&lt;br/&gt;
- * A collection of deprecated constructors. This will be removed soon.&lt;br/&gt;
- */&lt;br/&gt;
-object AccumulableInfo {&lt;br/&gt;
-&lt;br/&gt;
-  @deprecated(&quot;do not create AccumulableInfo&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def apply(&lt;br/&gt;
-      id: Long,&lt;br/&gt;
-      name: String,&lt;br/&gt;
-      update: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
-      value: String,&lt;br/&gt;
-      internal: Boolean): AccumulableInfo = {
-    new AccumulableInfo(
-      id, Option(name), update, Option(value), internal, countFailedValues = false)
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @deprecated(&quot;do not create AccumulableInfo&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def apply(id: Long, name: String, update: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;, value: String): AccumulableInfo = {
-    new AccumulableInfo(
-      id, Option(name), update, Option(value), internal = false, countFailedValues = false)
-  }&lt;br/&gt;
-&lt;br/&gt;
-  @deprecated(&quot;do not create AccumulableInfo&quot;, &quot;2.0.0&quot;)&lt;br/&gt;
-  def apply(id: Long, name: String, value: String): AccumulableInfo = {
-    new AccumulableInfo(
-      id, Option(name), None, Option(value), internal = false, countFailedValues = false)
-  }&lt;br/&gt;
-}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala b/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala&lt;br/&gt;
index e2b6df4600590..7cd2b862216ee 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/AsyncEventQueue.scala&lt;br/&gt;
@@ -169,7 +169,8 @@ private class AsyncEventQueue(&lt;br/&gt;
           val prevLastReportTimestamp = lastReportTimestamp&lt;br/&gt;
           lastReportTimestamp = System.currentTimeMillis()&lt;br/&gt;
           val previous = new java.util.Date(prevLastReportTimestamp)&lt;br/&gt;
-          logWarning(s&quot;Dropped $droppedCount events from $name since $previous.&quot;)&lt;br/&gt;
+          logWarning(s&quot;Dropped $droppedCount events from $name since &quot; +&lt;br/&gt;
+            s&quot;${if (prevLastReportTimestamp == 0) &quot;the application started&quot; else s&quot;$previous&quot;}.&quot;)&lt;br/&gt;
         }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala b/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala&lt;br/&gt;
index 980fbbe516b91..ef6d02d85c27b 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/BlacklistTracker.scala&lt;br/&gt;
@@ -146,21 +146,31 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;scheduler&amp;#93;&lt;/span&gt; class BlacklistTracker (&lt;br/&gt;
     nextExpiryTime = math.min(execMinExpiry, nodeMinExpiry)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  private def killExecutor(exec: String, msg: String): Unit = {&lt;br/&gt;
+    allocationClient match {
+      case Some(a) =&amp;gt;
+        logInfo(msg)
+        a.killExecutors(Seq(exec), adjustTargetNumExecutors = false, countFailures = false,
+          force = true)
+      case None =&amp;gt;
+        logInfo(s&quot;Not attempting to kill blacklisted executor id $exec &quot; +
+          s&quot;since allocation client is not defined.&quot;)
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   private def killBlacklistedExecutor(exec: String): Unit = {&lt;br/&gt;
     if (conf.get(config.BLACKLIST_KILL_ENABLED)) {&lt;br/&gt;
-      allocationClient match {&lt;br/&gt;
-        case Some(a) =&amp;gt;&lt;br/&gt;
-          logInfo(s&quot;Killing blacklisted executor id $exec &quot; +&lt;br/&gt;
-            s&quot;since ${config.BLACKLIST_KILL_ENABLED.key} is set.&quot;)&lt;br/&gt;
-          a.killExecutors(Seq(exec), adjustTargetNumExecutors = false, countFailures = false,&lt;br/&gt;
-            force = true)&lt;br/&gt;
-        case None =&amp;gt;&lt;br/&gt;
-          logWarning(s&quot;Not attempting to kill blacklisted executor id $exec &quot; +&lt;br/&gt;
-            s&quot;since allocation client is not defined.&quot;)&lt;br/&gt;
-      }&lt;br/&gt;
+      killExecutor(exec,&lt;br/&gt;
+        s&quot;Killing blacklisted executor id $exec since ${config.BLACKLIST_KILL_ENABLED.key} is set.&quot;)&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;scheduler&amp;#93;&lt;/span&gt; def killBlacklistedIdleExecutor(exec: String): Unit = {
+    killExecutor(exec,
+      s&quot;Killing blacklisted idle executor id $exec because of task unschedulability and trying &quot; +
+        &quot;to acquire a new executor.&quot;)
+  }&lt;br/&gt;
+&lt;br/&gt;
   private def killExecutorsOnBlacklistedNode(node: String): Unit = {&lt;br/&gt;
     if (conf.get(config.BLACKLIST_KILL_ENABLED)) {&lt;br/&gt;
       allocationClient match {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala b/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala&lt;br/&gt;
index 50c91da8b13d1..06966e77db81e 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala&lt;br/&gt;
@@ -35,7 +35,7 @@ import org.apache.commons.lang3.SerializationUtils&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark._&lt;br/&gt;
 import org.apache.spark.broadcast.Broadcast&lt;br/&gt;
-import org.apache.spark.executor.TaskMetrics&lt;br/&gt;
+import org.apache.spark.executor.{ExecutorMetrics, TaskMetrics}&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.internal.config&lt;br/&gt;
 import org.apache.spark.network.util.JavaUtils&lt;br/&gt;
@@ -264,8 +264,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class DAGScheduler(&lt;br/&gt;
       execId: String,&lt;br/&gt;
       // (taskId, stageId, stageAttemptId, accumUpdates)&lt;br/&gt;
       accumUpdates: Array[(Long, Int, Int, Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;AccumulableInfo&amp;#93;&lt;/span&gt;)],&lt;br/&gt;
-      blockManagerId: BlockManagerId): Boolean = {&lt;br/&gt;
-    listenerBus.post(SparkListenerExecutorMetricsUpdate(execId, accumUpdates))&lt;br/&gt;
+      blockManagerId: BlockManagerId,&lt;br/&gt;
+      // executor metrics indexed by ExecutorMetricType.values&lt;br/&gt;
+      executorUpdates: ExecutorMetrics): Boolean = {
+    listenerBus.post(SparkListenerExecutorMetricsUpdate(execId, accumUpdates,
+      Some(executorUpdates)))
     blockManagerMaster.driverEndpoint.askSync[Boolean](
       BlockManagerHeartbeat(blockManagerId), new RpcTimeout(600 seconds, &quot;BlockManagerHeartbeat&quot;))
   }&lt;br/&gt;
@@ -1242,9 +1245,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class DAGScheduler(&lt;br/&gt;
   private def updateAccumulators(event: CompletionEvent): Unit = {&lt;br/&gt;
     val task = event.task&lt;br/&gt;
     val stage = stageIdToStage(task.stageId)&lt;br/&gt;
-    try {&lt;br/&gt;
-      event.accumUpdates.foreach { updates =&amp;gt;&lt;br/&gt;
-        val id = updates.id&lt;br/&gt;
+&lt;br/&gt;
+    event.accumUpdates.foreach { updates =&amp;gt;&lt;br/&gt;
+      val id = updates.id&lt;br/&gt;
+      try {&lt;br/&gt;
         // Find the corresponding accumulator on the driver and update it&lt;br/&gt;
         val acc: AccumulatorV2&lt;span class=&quot;error&quot;&gt;&amp;#91;Any, Any&amp;#93;&lt;/span&gt; = AccumulatorContext.get(id) match {
           case Some(accum) =&amp;gt; accum.asInstanceOf[AccumulatorV2[Any, Any]]
@@ -1258,10 +1262,17 @@ private[spark] class DAGScheduler(
           event.taskInfo.setAccumulables(
             acc.toInfo(Some(updates.value), Some(acc.value)) +: event.taskInfo.accumulables)
         }&lt;br/&gt;
+      } catch {&lt;br/&gt;
+        case NonFatal(e) =&amp;gt;&lt;br/&gt;
+          // Log the class name to make it easy to find the bad implementation&lt;br/&gt;
+          val accumClassName = AccumulatorContext.get(id) match {
+            case Some(accum) =&amp;gt; accum.getClass.getName
+            case None =&amp;gt; &quot;Unknown class&quot;
+          }&lt;br/&gt;
+          logError(&lt;br/&gt;
+            s&quot;Failed to update accumulator $id ($accumClassName) for task ${task.partitionId}&quot;,&lt;br/&gt;
+            e)&lt;br/&gt;
       }&lt;br/&gt;
-    } catch {&lt;br/&gt;
-      case NonFatal(e) =&amp;gt;&lt;br/&gt;
-        logError(s&quot;Failed to update accumulators for task ${task.partitionId}&quot;, e)&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -1284,6 +1295,27 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class DAGScheduler(&lt;br/&gt;
       Utils.getFormattedClassName(event.task), event.reason, event.taskInfo, taskMetrics))&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * Check [&lt;span class=&quot;error&quot;&gt;&amp;#91;SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL&amp;#93;&lt;/span&gt;] in job properties to see if we should&lt;br/&gt;
+   * interrupt running tasks. Returns `false` if the property value is not a boolean value&lt;br/&gt;
+   */&lt;br/&gt;
+  private def shouldInterruptTaskThread(job: ActiveJob): Boolean = {&lt;br/&gt;
+    if (job.properties == null) {
+      false
+    } else {&lt;br/&gt;
+      val shouldInterruptThread =&lt;br/&gt;
+        job.properties.getProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, &quot;false&quot;)&lt;br/&gt;
+      try {
+        shouldInterruptThread.toBoolean
+      } catch {&lt;br/&gt;
+        case e: IllegalArgumentException =&amp;gt;&lt;br/&gt;
+          logWarning(s&quot;${SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL} in Job ${job.jobId} &quot; +&lt;br/&gt;
+            s&quot;is invalid: $shouldInterruptThread. Using &apos;false&apos; instead&quot;, e)&lt;br/&gt;
+          false&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
    * Responds to a task finishing. This is called inside the event loop so it assumes that it can&lt;br/&gt;
    * modify the scheduler&apos;s internal state. Use taskEnded() to post a task end event from outside.&lt;br/&gt;
@@ -1353,6 +1385,21 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class DAGScheduler(&lt;br/&gt;
                   if (job.numFinished == job.numPartitions) {&lt;br/&gt;
                     markStageAsFinished(resultStage)&lt;br/&gt;
                     cleanupStateForJobAndIndependentStages(job)&lt;br/&gt;
+                    try {&lt;br/&gt;
+                      // killAllTaskAttempts will fail if a SchedulerBackend does not implement&lt;br/&gt;
+                      // killTask.&lt;br/&gt;
+                      logInfo(s&quot;Job ${job.jobId} is finished. Cancelling potential speculative &quot; +&lt;br/&gt;
+                        &quot;or zombie tasks for this job&quot;)&lt;br/&gt;
+                      // ResultStage is only used by this job. It&apos;s safe to kill speculative or&lt;br/&gt;
+                      // zombie tasks in this stage.&lt;br/&gt;
+                      taskScheduler.killAllTaskAttempts(&lt;br/&gt;
+                        stageId,&lt;br/&gt;
+                        shouldInterruptTaskThread(job),&lt;br/&gt;
+                        reason = &quot;Stage finished&quot;)&lt;br/&gt;
+                    } catch {
+                      case e: UnsupportedOperationException =&amp;gt;
+                        logWarning(s&quot;Could not cancel tasks for stage $stageId&quot;, e)
+                    }&lt;br/&gt;
                     listenerBus.post(&lt;br/&gt;
                       SparkListenerJobEnd(job.jobId, clock.getTimeMillis(), JobSucceeded))&lt;br/&gt;
                   }&lt;br/&gt;
@@ -1362,7 +1409,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class DAGScheduler(&lt;br/&gt;
                   try {
                     job.listener.taskSucceeded(rt.outputId, event.result)
                   } catch {
-                    case e: Exception =&amp;gt;
+                    case e: Throwable if !Utils.isFatalError(e) =&amp;gt;
                       // TODO: Perhaps we want to mark the resultStage as failed?
                       job.listener.jobFailed(new SparkDriverExecutionException(e))
                   }&lt;br/&gt;
@@ -1879,10 +1926,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class DAGScheduler(&lt;br/&gt;
     val error = new SparkException(failureReason, exception.getOrElse(null))&lt;br/&gt;
     var ableToCancelStages = true&lt;br/&gt;
 &lt;br/&gt;
-    val shouldInterruptThread =&lt;br/&gt;
-      if (job.properties == null) false&lt;br/&gt;
-      else job.properties.getProperty(SparkContext.SPARK_JOB_INTERRUPT_ON_CANCEL, &quot;false&quot;).toBoolean&lt;br/&gt;
-&lt;br/&gt;
     // Cancel all independent, running stages.&lt;br/&gt;
     val stages = jobIdToStageIds(job.jobId)&lt;br/&gt;
     if (stages.isEmpty) {&lt;br/&gt;
@@ -1902,12 +1945,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class DAGScheduler(&lt;br/&gt;
           val stage = stageIdToStage(stageId)&lt;br/&gt;
           if (runningStages.contains(stage)) {&lt;br/&gt;
             try { // cancelTasks will fail if a SchedulerBackend does not implement killTask
-              taskScheduler.cancelTasks(stageId, shouldInterruptThread)
+              taskScheduler.cancelTasks(stageId, shouldInterruptTaskThread(job))
               markStageAsFinished(stage, Some(failureReason))
             } catch {
               case e: UnsupportedOperationException =&amp;gt;
-                logInfo(s&quot;Could not cancel tasks for stage $stageId&quot;, e)
-              ableToCancelStages = false
+                logWarning(s&quot;Could not cancel tasks for stage $stageId&quot;, e)
+                ableToCancelStages = false
             }&lt;br/&gt;
           }&lt;br/&gt;
         }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala b/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala&lt;br/&gt;
index 69bc51c1ecf90..5f697fe99258d 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala&lt;br/&gt;
@@ -20,22 +20,19 @@ package org.apache.spark.scheduler&lt;br/&gt;
 import java.io._&lt;br/&gt;
 import java.net.URI&lt;br/&gt;
 import java.nio.charset.StandardCharsets&lt;br/&gt;
-import java.util.EnumSet&lt;br/&gt;
 import java.util.Locale&lt;br/&gt;
 &lt;br/&gt;
-import scala.collection.mutable&lt;br/&gt;
-import scala.collection.mutable.ArrayBuffer&lt;br/&gt;
+import scala.collection.mutable.{ArrayBuffer, Map}&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.hadoop.conf.Configuration&lt;br/&gt;
 import org.apache.hadoop.fs.{FileSystem, FSDataOutputStream, Path}&lt;br/&gt;
 import org.apache.hadoop.fs.permission.FsPermission&lt;br/&gt;
-import org.apache.hadoop.hdfs.DFSOutputStream&lt;br/&gt;
-import org.apache.hadoop.hdfs.client.HdfsDataOutputStream.SyncFlag&lt;br/&gt;
 import org.json4s.JsonAST.JValue&lt;br/&gt;
 import org.json4s.jackson.JsonMethods._&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.{SPARK_VERSION, SparkConf}&lt;br/&gt;
 import org.apache.spark.deploy.SparkHadoopUtil&lt;br/&gt;
+import org.apache.spark.executor.ExecutorMetrics&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.internal.config._&lt;br/&gt;
 import org.apache.spark.io.CompressionCodec&lt;br/&gt;
@@ -51,6 +48,7 @@ import org.apache.spark.util.{JsonProtocol, Utils}&lt;br/&gt;
  *   spark.eventLog.overwrite - Whether to overwrite any existing files.&lt;br/&gt;
  *   spark.eventLog.dir - Path to the directory in which events are logged.&lt;br/&gt;
  *   spark.eventLog.buffer.kb - Buffer size to use when writing to output streams&lt;br/&gt;
+ *   spark.eventLog.logStageExecutorMetrics.enabled - Whether to log stage executor metrics&lt;br/&gt;
  */&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class EventLoggingListener(&lt;br/&gt;
     appId: String,&lt;br/&gt;
@@ -69,6 +67,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class EventLoggingListener(&lt;br/&gt;
   private val shouldCompress = sparkConf.get(EVENT_LOG_COMPRESS)&lt;br/&gt;
   private val shouldOverwrite = sparkConf.get(EVENT_LOG_OVERWRITE)&lt;br/&gt;
   private val shouldLogBlockUpdates = sparkConf.get(EVENT_LOG_BLOCK_UPDATES)&lt;br/&gt;
+  private val shouldAllowECLogs = sparkConf.get(EVENT_LOG_ALLOW_EC)&lt;br/&gt;
+  private val shouldLogStageExecutorMetrics = sparkConf.get(EVENT_LOG_STAGE_EXECUTOR_METRICS)&lt;br/&gt;
   private val testing = sparkConf.get(EVENT_LOG_TESTING)&lt;br/&gt;
   private val outputBufferSize = sparkConf.get(EVENT_LOG_OUTPUT_BUFFER_SIZE).toInt&lt;br/&gt;
   private val fileSystem = Utils.getHadoopFileSystem(logBaseDir, hadoopConf)&lt;br/&gt;
@@ -93,6 +93,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class EventLoggingListener(&lt;br/&gt;
   // Visible for tests only.&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;scheduler&amp;#93;&lt;/span&gt; val logPath = getLogPath(logBaseDir, appId, appAttemptId, compressionCodecName)&lt;br/&gt;
 &lt;br/&gt;
+  // map of (stageId, stageAttempt), to peak executor metrics for the stage&lt;br/&gt;
+  private val liveStageExecutorMetrics = Map.empty[(Int, Int), Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, ExecutorMetrics&amp;#93;&lt;/span&gt;]&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
    * Creates the log file in the configured log directory.&lt;br/&gt;
    */&lt;br/&gt;
@@ -117,7 +120,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class EventLoggingListener(&lt;br/&gt;
       if ((isDefaultLocal &amp;amp;&amp;amp; uri.getScheme == null) || uri.getScheme == &quot;file&quot;) {
         new FileOutputStream(uri.getPath)
       } else {&lt;br/&gt;
-        hadoopDataStream = Some(fileSystem.create(path))&lt;br/&gt;
+        hadoopDataStream = Some(if (shouldAllowECLogs) {
+          fileSystem.create(path)
+        } else {
+          SparkHadoopUtil.createNonECFile(fileSystem, path)
+        })&lt;br/&gt;
         hadoopDataStream.get&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
@@ -144,10 +151,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class EventLoggingListener(&lt;br/&gt;
     // scalastyle:on println&lt;br/&gt;
     if (flushLogger) {&lt;br/&gt;
       writer.foreach(_.flush())&lt;br/&gt;
-      hadoopDataStream.foreach(ds =&amp;gt; ds.getWrappedStream match {
-        case wrapped: DFSOutputStream =&amp;gt; wrapped.hsync(EnumSet.of(SyncFlag.UPDATE_LENGTH))
-        case _ =&amp;gt; ds.hflush()
-      })&lt;br/&gt;
+      hadoopDataStream.foreach(_.hflush())&lt;br/&gt;
     }&lt;br/&gt;
     if (testing) {
       loggedEvents += eventJson
@@ -155,7 +159,14 @@ private[spark] class EventLoggingListener(
   }&lt;br/&gt;
 &lt;br/&gt;
   // Events that do not trigger a flush&lt;br/&gt;
-  override def onStageSubmitted(event: SparkListenerStageSubmitted): Unit = logEvent(event)&lt;br/&gt;
+  override def onStageSubmitted(event: SparkListenerStageSubmitted): Unit = {&lt;br/&gt;
+    logEvent(event)&lt;br/&gt;
+    if (shouldLogStageExecutorMetrics) {
+      // record the peak metrics for the new stage
+      liveStageExecutorMetrics.put((event.stageInfo.stageId, event.stageInfo.attemptNumber()),
+        Map.empty[String, ExecutorMetrics])
+    }&lt;br/&gt;
+  }&lt;br/&gt;
 &lt;br/&gt;
   override def onTaskStart(event: SparkListenerTaskStart): Unit = logEvent(event)&lt;br/&gt;
 &lt;br/&gt;
@@ -169,6 +180,26 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class EventLoggingListener(&lt;br/&gt;
 &lt;br/&gt;
   // Events that trigger a flush&lt;br/&gt;
   override def onStageCompleted(event: SparkListenerStageCompleted): Unit = {&lt;br/&gt;
+    if (shouldLogStageExecutorMetrics) {&lt;br/&gt;
+      // clear out any previous attempts, that did not have a stage completed event&lt;br/&gt;
+      val prevAttemptId = event.stageInfo.attemptNumber() - 1&lt;br/&gt;
+      for (attemptId &amp;lt;- 0 to prevAttemptId) {
+        liveStageExecutorMetrics.remove((event.stageInfo.stageId, attemptId))
+      }&lt;br/&gt;
+&lt;br/&gt;
+      // log the peak executor metrics for the stage, for each live executor,&lt;br/&gt;
+      // whether or not the executor is running tasks for the stage&lt;br/&gt;
+      val executorOpt = liveStageExecutorMetrics.remove(&lt;br/&gt;
+        (event.stageInfo.stageId, event.stageInfo.attemptNumber()))&lt;br/&gt;
+      executorOpt.foreach { execMap =&amp;gt;&lt;br/&gt;
+        execMap.foreach { case (executorId, peakExecutorMetrics) =&amp;gt;
+            logEvent(new SparkListenerStageExecutorMetrics(executorId, event.stageInfo.stageId,
+              event.stageInfo.attemptNumber(), peakExecutorMetrics))
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    // log stage completed event&lt;br/&gt;
     logEvent(event, flushLogger = true)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -234,8 +265,18 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class EventLoggingListener(&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  // No-op because logging every update would be overkill&lt;br/&gt;
-  override def onExecutorMetricsUpdate(event: SparkListenerExecutorMetricsUpdate): Unit = { }&lt;br/&gt;
+  override def onExecutorMetricsUpdate(event: SparkListenerExecutorMetricsUpdate): Unit = {&lt;br/&gt;
+    if (shouldLogStageExecutorMetrics) {&lt;br/&gt;
+      // For the active stages, record any new peak values for the memory metrics for the executor&lt;br/&gt;
+      event.executorUpdates.foreach { executorUpdates =&amp;gt;&lt;br/&gt;
+        liveStageExecutorMetrics.values.foreach { peakExecutorMetrics =&amp;gt;
+          val peakMetrics = peakExecutorMetrics.getOrElseUpdate(
+            event.execId, new ExecutorMetrics())
+          peakMetrics.compareAndUpdatePeakValues(executorUpdates)
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
 &lt;br/&gt;
   override def onOtherEvent(event: SparkListenerEvent): Unit = {&lt;br/&gt;
     if (event.logEvent) {&lt;br/&gt;
@@ -296,7 +337,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object EventLoggingListener extends Logging {&lt;br/&gt;
   private val LOG_FILE_PERMISSIONS = new FsPermission(Integer.parseInt(&quot;770&quot;, 8).toShort)&lt;br/&gt;
 &lt;br/&gt;
   // A cache for compression codecs to avoid creating the same codec many times&lt;br/&gt;
-  private val codecMap = new mutable.HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, CompressionCodec&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  private val codecMap = Map.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;String, CompressionCodec&amp;#93;&lt;/span&gt;&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
    * Write metadata about an event log to the given stream.&lt;br/&gt;
@@ -341,19 +382,15 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object EventLoggingListener extends Logging {&lt;br/&gt;
       appId: String,&lt;br/&gt;
       appAttemptId: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       compressionCodecName: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None): String = {&lt;br/&gt;
-    val base = new Path(logBaseDir).toString.stripSuffix(&quot;/&quot;) + &quot;/&quot; + sanitize(appId)&lt;br/&gt;
+    val base = new Path(logBaseDir).toString.stripSuffix(&quot;/&quot;) + &quot;/&quot; + Utils.sanitizeDirName(appId)&lt;br/&gt;
     val codec = compressionCodecName.map(&quot;.&quot; + _).getOrElse(&quot;&quot;)&lt;br/&gt;
     if (appAttemptId.isDefined) {
-      base + &quot;_&quot; + sanitize(appAttemptId.get) + codec
+      base + &quot;_&quot; + Utils.sanitizeDirName(appAttemptId.get) + codec
     } else {
       base + codec
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def sanitize(str: String): String = {&lt;br/&gt;
-    str.replaceAll(&quot;[ :/]&quot;, &quot;-&quot;).replaceAll(&quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;.${}&amp;#39;\&amp;quot;&amp;#93;&lt;/span&gt;&quot;, &quot;_&quot;).toLowerCase(Locale.ROOT)&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
   /**&lt;br/&gt;
    * Opens an event log file and returns an input stream that contains the event data.&lt;br/&gt;
    *&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala b/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala&lt;br/&gt;
index 7e1d75fe723d6..64f0a060a247c 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/MapStatus.scala&lt;br/&gt;
@@ -20,7 +20,6 @@ package org.apache.spark.scheduler&lt;br/&gt;
 import java.io.{Externalizable, ObjectInput, ObjectOutput}&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.mutable&lt;br/&gt;
-import scala.collection.mutable.ArrayBuffer&lt;br/&gt;
 &lt;br/&gt;
 import org.roaringbitmap.RoaringBitmap&lt;br/&gt;
 &lt;br/&gt;
@@ -31,8 +30,7 @@ import org.apache.spark.util.Utils&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
  * Result returned by a ShuffleMapTask to a scheduler. Includes the block manager address that the&lt;br/&gt;
- * task ran on, the sizes of outputs for each reducer, and the number of outputs of the map task,&lt;br/&gt;
- * for passing on to the reduce tasks.&lt;br/&gt;
+ * task ran on as well as the sizes of outputs for each reducer, for passing on to the reduce tasks.&lt;br/&gt;
  */&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; sealed trait MapStatus {&lt;br/&gt;
   /** Location where this task was run. */&lt;br/&gt;
@@ -45,23 +43,24 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; sealed trait MapStatus {
    * necessary for correctness, since block fetchers are allowed to skip zero-size blocks.
    */
   def getSizeForBlock(reduceId: Int): Long
-
-  /**
-   * The number of outputs for the map task.
-   */
-  def numberOfOutput: Long
 }&lt;br/&gt;
 &lt;br/&gt;
 &lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object MapStatus {&lt;br/&gt;
 &lt;br/&gt;
-  def apply(loc: BlockManagerId, uncompressedSizes: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;, numOutput: Long): MapStatus = {&lt;br/&gt;
-    if (uncompressedSizes.length &amp;gt;  Option(SparkEnv.get)&lt;br/&gt;
-      .map(_.conf.get(config.SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS))&lt;br/&gt;
-      .getOrElse(config.SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS.defaultValue.get)) {&lt;br/&gt;
-      HighlyCompressedMapStatus(loc, uncompressedSizes, numOutput)&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Min partition number to use [&lt;span class=&quot;error&quot;&gt;&amp;#91;HighlyCompressedMapStatus&amp;#93;&lt;/span&gt;]. A bit ugly here because in test&lt;br/&gt;
+   * code we can&apos;t assume SparkEnv.get exists.&lt;br/&gt;
+   */&lt;br/&gt;
+  private lazy val minPartitionsToUseHighlyCompressMapStatus = Option(SparkEnv.get)&lt;br/&gt;
+    .map(_.conf.get(config.SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS))&lt;br/&gt;
+    .getOrElse(config.SHUFFLE_MIN_NUM_PARTS_TO_HIGHLY_COMPRESS.defaultValue.get)&lt;br/&gt;
+&lt;br/&gt;
+  def apply(loc: BlockManagerId, uncompressedSizes: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;): MapStatus = {&lt;br/&gt;
+    if (uncompressedSizes.length &amp;gt; minPartitionsToUseHighlyCompressMapStatus) {
+      HighlyCompressedMapStatus(loc, uncompressedSizes)
     } else {
-      new CompressedMapStatus(loc, uncompressedSizes, numOutput)
+      new CompressedMapStatus(loc, uncompressedSizes)
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -104,34 +103,29 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object MapStatus {&lt;br/&gt;
  */&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class CompressedMapStatus(&lt;br/&gt;
     private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var loc: BlockManagerId,&lt;br/&gt;
-    private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var compressedSizes: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;,&lt;br/&gt;
-    private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var numOutput: Long)&lt;br/&gt;
+    private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var compressedSizes: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;)&lt;br/&gt;
   extends MapStatus with Externalizable {&lt;br/&gt;
 &lt;br/&gt;
-  protected def this() = this(null, null.asInstanceOf[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;], -1)  // For deserialization only&lt;br/&gt;
+  protected def this() = this(null, null.asInstanceOf[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;])  // For deserialization only&lt;br/&gt;
 &lt;br/&gt;
-  def this(loc: BlockManagerId, uncompressedSizes: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;, numOutput: Long) {&lt;br/&gt;
-    this(loc, uncompressedSizes.map(MapStatus.compressSize), numOutput)&lt;br/&gt;
+  def this(loc: BlockManagerId, uncompressedSizes: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;) {
+    this(loc, uncompressedSizes.map(MapStatus.compressSize))
   }&lt;br/&gt;
 &lt;br/&gt;
   override def location: BlockManagerId = loc&lt;br/&gt;
 &lt;br/&gt;
-  override def numberOfOutput: Long = numOutput&lt;br/&gt;
-&lt;br/&gt;
   override def getSizeForBlock(reduceId: Int): Long = {
     MapStatus.decompressSize(compressedSizes(reduceId))
   }&lt;br/&gt;
 &lt;br/&gt;
   override def writeExternal(out: ObjectOutput): Unit = Utils.tryOrIOException {
     loc.writeExternal(out)
-    out.writeLong(numOutput)
     out.writeInt(compressedSizes.length)
     out.write(compressedSizes)
   }&lt;br/&gt;
 &lt;br/&gt;
   override def readExternal(in: ObjectInput): Unit = Utils.tryOrIOException {&lt;br/&gt;
     loc = BlockManagerId(in)&lt;br/&gt;
-    numOutput = in.readLong()&lt;br/&gt;
     val len = in.readInt()&lt;br/&gt;
     compressedSizes = new Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;(len)&lt;br/&gt;
     in.readFully(compressedSizes)&lt;br/&gt;
@@ -154,20 +148,17 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class HighlyCompressedMapStatus private (&lt;br/&gt;
     private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var numNonEmptyBlocks: Int,&lt;br/&gt;
     private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var emptyBlocks: RoaringBitmap,&lt;br/&gt;
     private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var avgSize: Long,&lt;br/&gt;
-    private var hugeBlockSizes: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;Int, Byte&amp;#93;&lt;/span&gt;,&lt;br/&gt;
-    private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var numOutput: Long)&lt;br/&gt;
+    private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var hugeBlockSizes: scala.collection.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;Int, Byte&amp;#93;&lt;/span&gt;)&lt;br/&gt;
   extends MapStatus with Externalizable {&lt;br/&gt;
 &lt;br/&gt;
   // loc could be null when the default constructor is called during deserialization&lt;br/&gt;
   require(loc == null || avgSize &amp;gt; 0 || hugeBlockSizes.size &amp;gt; 0 || numNonEmptyBlocks == 0,&lt;br/&gt;
     &quot;Average size can only be zero for map stages that produced no output&quot;)&lt;br/&gt;
 &lt;br/&gt;
-  protected def this() = this(null, -1, null, -1, null, -1)  // For deserialization only&lt;br/&gt;
+  protected def this() = this(null, -1, null, -1, null)  // For deserialization only&lt;br/&gt;
 &lt;br/&gt;
   override def location: BlockManagerId = loc&lt;br/&gt;
 &lt;br/&gt;
-  override def numberOfOutput: Long = numOutput&lt;br/&gt;
-&lt;br/&gt;
   override def getSizeForBlock(reduceId: Int): Long = {&lt;br/&gt;
     assert(hugeBlockSizes != null)&lt;br/&gt;
     if (emptyBlocks.contains(reduceId)) {&lt;br/&gt;
@@ -182,7 +173,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class HighlyCompressedMapStatus private (&lt;br/&gt;
 &lt;br/&gt;
   override def writeExternal(out: ObjectOutput): Unit = Utils.tryOrIOException {&lt;br/&gt;
     loc.writeExternal(out)&lt;br/&gt;
-    out.writeLong(numOutput)&lt;br/&gt;
     emptyBlocks.writeExternal(out)&lt;br/&gt;
     out.writeLong(avgSize)&lt;br/&gt;
     out.writeInt(hugeBlockSizes.size)&lt;br/&gt;
@@ -194,26 +184,22 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class HighlyCompressedMapStatus private (&lt;br/&gt;
 &lt;br/&gt;
   override def readExternal(in: ObjectInput): Unit = Utils.tryOrIOException {&lt;br/&gt;
     loc = BlockManagerId(in)&lt;br/&gt;
-    numOutput = in.readLong()&lt;br/&gt;
     emptyBlocks = new RoaringBitmap()&lt;br/&gt;
     emptyBlocks.readExternal(in)&lt;br/&gt;
     avgSize = in.readLong()&lt;br/&gt;
     val count = in.readInt()&lt;br/&gt;
-    val hugeBlockSizesArray = mutable.ArrayBuffer[Tuple2&lt;span class=&quot;error&quot;&gt;&amp;#91;Int, Byte&amp;#93;&lt;/span&gt;]()&lt;br/&gt;
+    val hugeBlockSizesImpl = mutable.Map.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Int, Byte&amp;#93;&lt;/span&gt;&lt;br/&gt;
     (0 until count).foreach { _ =&amp;gt;
       val block = in.readInt()
       val size = in.readByte()
-      hugeBlockSizesArray += Tuple2(block, size)
+      hugeBlockSizesImpl(block) = size
     }&lt;br/&gt;
-    hugeBlockSizes = hugeBlockSizesArray.toMap&lt;br/&gt;
+    hugeBlockSizes = hugeBlockSizesImpl&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object HighlyCompressedMapStatus {&lt;br/&gt;
-  def apply(&lt;br/&gt;
-      loc: BlockManagerId,&lt;br/&gt;
-      uncompressedSizes: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;,&lt;br/&gt;
-      numOutput: Long): HighlyCompressedMapStatus = {&lt;br/&gt;
+  def apply(loc: BlockManagerId, uncompressedSizes: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt;): HighlyCompressedMapStatus = {&lt;br/&gt;
     // We must keep track of which blocks are empty so that we don&apos;t report a zero-sized&lt;br/&gt;
     // block as being non-empty (or vice-versa) when using the average block size.&lt;br/&gt;
     var i = 0&lt;br/&gt;
@@ -228,7 +214,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object HighlyCompressedMapStatus {&lt;br/&gt;
     val threshold = Option(SparkEnv.get)&lt;br/&gt;
       .map(_.conf.get(config.SHUFFLE_ACCURATE_BLOCK_THRESHOLD))&lt;br/&gt;
       .getOrElse(config.SHUFFLE_ACCURATE_BLOCK_THRESHOLD.defaultValue.get)&lt;br/&gt;
-    val hugeBlockSizesArray = ArrayBuffer[Tuple2&lt;span class=&quot;error&quot;&gt;&amp;#91;Int, Byte&amp;#93;&lt;/span&gt;]()&lt;br/&gt;
+    val hugeBlockSizes = mutable.Map.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Int, Byte&amp;#93;&lt;/span&gt;&lt;br/&gt;
     while (i &amp;lt; totalNumBlocks) {&lt;br/&gt;
       val size = uncompressedSizes&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
       if (size &amp;gt; 0) {&lt;br/&gt;
@@ -239,7 +225,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object HighlyCompressedMapStatus {
           totalSmallBlockSize += size
           numSmallBlocks += 1
         } else {
-          hugeBlockSizesArray += Tuple2(i, MapStatus.compressSize(uncompressedSizes(i)))
+          hugeBlockSizes(i) = MapStatus.compressSize(uncompressedSizes(i))
         }&lt;br/&gt;
       } else {&lt;br/&gt;
         emptyBlocks.add&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
@@ -254,6 +240,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object HighlyCompressedMapStatus {
     emptyBlocks.trim()
     emptyBlocks.runOptimize()
     new HighlyCompressedMapStatus(loc, numNonEmptyBlocks, emptyBlocks, avgSize,
-      hugeBlockSizesArray.toMap, numOutput)
+      hugeBlockSizes)
   }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala b/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala&lt;br/&gt;
index 226c23733c870..4c6b0c1227b18 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/ReplayListenerBus.scala&lt;br/&gt;
@@ -118,6 +118,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class ReplayListenerBus extends SparkListenerBus with Logging {&lt;br/&gt;
       case e: HaltReplayException =&amp;gt;&lt;br/&gt;
         // Just stop replay.&lt;br/&gt;
       case _: EOFException if maybeTruncated =&amp;gt;&lt;br/&gt;
+      case _: IOException if maybeTruncated =&amp;gt;&lt;br/&gt;
+        logWarning(s&quot;Failed to read Spark event log: $sourceName&quot;)&lt;br/&gt;
       case ioe: IOException =&amp;gt;&lt;br/&gt;
         throw ioe&lt;br/&gt;
       case e: Exception =&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala b/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala&lt;br/&gt;
index f2cd65fd523ab..5412717d61988 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/ShuffleMapTask.scala&lt;br/&gt;
@@ -95,7 +95,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class ShuffleMapTask(&lt;br/&gt;
     var writer: ShuffleWriter&lt;span class=&quot;error&quot;&gt;&amp;#91;Any, Any&amp;#93;&lt;/span&gt; = null&lt;br/&gt;
     try {
       val manager = SparkEnv.get.shuffleManager
-      writer = manager.getWriter[Any, Any](dep.shuffleHandle, partitionId, context)
+      writer = manager.getWriter[Any, Any](
+        dep.shuffleHandle, partitionId, context, context.taskMetrics().shuffleWriteMetrics)
       writer.write(rdd.iterator(partition, context).asInstanceOf[Iterator[_ &amp;lt;: Product2[Any, Any]]])
       writer.stop(success = true).get
     } catch {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala b/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala&lt;br/&gt;
index 8a112f6a37b96..e92b8a2718df0 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala&lt;br/&gt;
@@ -26,7 +26,7 @@ import com.fasterxml.jackson.annotation.JsonTypeInfo&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.{SparkConf, TaskEndReason}&lt;br/&gt;
 import org.apache.spark.annotation.DeveloperApi&lt;br/&gt;
-import org.apache.spark.executor.TaskMetrics&lt;br/&gt;
+import org.apache.spark.executor.{ExecutorMetrics, TaskMetrics}&lt;br/&gt;
 import org.apache.spark.scheduler.cluster.ExecutorInfo&lt;br/&gt;
 import org.apache.spark.storage.{BlockManagerId, BlockUpdatedInfo}&lt;br/&gt;
 import org.apache.spark.ui.SparkUI&lt;br/&gt;
@@ -160,11 +160,29 @@ case class SparkListenerBlockUpdated(blockUpdatedInfo: BlockUpdatedInfo) extends&lt;br/&gt;
  * Periodic updates from executors.&lt;br/&gt;
  * @param execId executor id&lt;br/&gt;
  * @param accumUpdates sequence of (taskId, stageId, stageAttemptId, accumUpdates)&lt;br/&gt;
+ * @param executorUpdates executor level metrics updates&lt;br/&gt;
  */&lt;br/&gt;
 @DeveloperApi&lt;br/&gt;
 case class SparkListenerExecutorMetricsUpdate(&lt;br/&gt;
     execId: String,&lt;br/&gt;
-    accumUpdates: Seq[(Long, Int, Int, Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;AccumulableInfo&amp;#93;&lt;/span&gt;)])&lt;br/&gt;
+    accumUpdates: Seq[(Long, Int, Int, Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;AccumulableInfo&amp;#93;&lt;/span&gt;)],&lt;br/&gt;
+    executorUpdates: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorMetrics&amp;#93;&lt;/span&gt; = None)&lt;br/&gt;
+  extends SparkListenerEvent&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Peak metric values for the executor for the stage, written to the history log at stage&lt;br/&gt;
+ * completion.&lt;br/&gt;
+ * @param execId executor id&lt;br/&gt;
+ * @param stageId stage id&lt;br/&gt;
+ * @param stageAttemptId stage attempt&lt;br/&gt;
+ * @param executorMetrics executor level metrics, indexed by ExecutorMetricType.values&lt;br/&gt;
+ */&lt;br/&gt;
+@DeveloperApi&lt;br/&gt;
+case class SparkListenerStageExecutorMetrics(&lt;br/&gt;
+    execId: String,&lt;br/&gt;
+    stageId: Int,&lt;br/&gt;
+    stageAttemptId: Int,&lt;br/&gt;
+    executorMetrics: ExecutorMetrics)&lt;br/&gt;
   extends SparkListenerEvent&lt;br/&gt;
 &lt;br/&gt;
 @DeveloperApi&lt;br/&gt;
@@ -264,6 +282,13 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; trait SparkListenerInterface {&lt;br/&gt;
    */&lt;br/&gt;
   def onExecutorMetricsUpdate(executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * Called with the peak memory metrics for a given (executor, stage) combination. Note that this&lt;br/&gt;
+   * is only present when reading from the event log (as in the history server), and is never&lt;br/&gt;
+   * called in a live application.&lt;br/&gt;
+   */&lt;br/&gt;
+  def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
    * Called when the driver registers a new executor.&lt;br/&gt;
    */&lt;br/&gt;
@@ -361,6 +386,9 @@ abstract class SparkListener extends SparkListenerInterface {&lt;br/&gt;
   override def onExecutorMetricsUpdate(&lt;br/&gt;
       executorMetricsUpdate: SparkListenerExecutorMetricsUpdate): Unit = { }&lt;br/&gt;
 &lt;br/&gt;
+  override def onStageExecutorMetrics(&lt;br/&gt;
+      executorMetrics: SparkListenerStageExecutorMetrics): Unit = { }&lt;br/&gt;
+&lt;br/&gt;
   override def onExecutorAdded(executorAdded: SparkListenerExecutorAdded): Unit = { }&lt;br/&gt;
 &lt;br/&gt;
   override def onExecutorRemoved(executorRemoved: SparkListenerExecutorRemoved): Unit = { }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala b/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala&lt;br/&gt;
index ff19cc65552e0..8f6b7ad309602 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/SparkListenerBus.scala&lt;br/&gt;
@@ -57,6 +57,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; trait SparkListenerBus&lt;br/&gt;
         listener.onApplicationEnd(applicationEnd)&lt;br/&gt;
       case metricsUpdate: SparkListenerExecutorMetricsUpdate =&amp;gt;&lt;br/&gt;
         listener.onExecutorMetricsUpdate(metricsUpdate)&lt;br/&gt;
+      case stageExecutorMetrics: SparkListenerStageExecutorMetrics =&amp;gt;&lt;br/&gt;
+        listener.onStageExecutorMetrics(stageExecutorMetrics)&lt;br/&gt;
       case executorAdded: SparkListenerExecutorAdded =&amp;gt;&lt;br/&gt;
         listener.onExecutorAdded(executorAdded)&lt;br/&gt;
       case executorRemoved: SparkListenerExecutorRemoved =&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala b/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala&lt;br/&gt;
index 903e25b7986f2..33a68f24bd53a 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/StageInfo.scala&lt;br/&gt;
@@ -30,7 +30,7 @@ import org.apache.spark.storage.RDDInfo&lt;br/&gt;
 @DeveloperApi&lt;br/&gt;
 class StageInfo(&lt;br/&gt;
     val stageId: Int,&lt;br/&gt;
-    @deprecated(&quot;Use attemptNumber instead&quot;, &quot;2.3.0&quot;) val attemptId: Int,&lt;br/&gt;
+    private val attemptId: Int,&lt;br/&gt;
     val name: String,&lt;br/&gt;
     val numTasks: Int,&lt;br/&gt;
     val rddInfos: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;RDDInfo&amp;#93;&lt;/span&gt;,&lt;br/&gt;
@@ -56,6 +56,8 @@ class StageInfo(&lt;br/&gt;
     completionTime = Some(System.currentTimeMillis)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  // This would just be the second constructor arg, except we need to maintain this method&lt;br/&gt;
+  // with parentheses for compatibility&lt;br/&gt;
   def attemptNumber(): Int = attemptId&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def getStatusString: String = {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala b/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala&lt;br/&gt;
index 95f7ae4fd39a2..94221eb0d5515 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/TaskScheduler.scala&lt;br/&gt;
@@ -17,6 +17,7 @@&lt;br/&gt;
 &lt;br/&gt;
 package org.apache.spark.scheduler&lt;br/&gt;
 &lt;br/&gt;
+import org.apache.spark.executor.ExecutorMetrics&lt;br/&gt;
 import org.apache.spark.scheduler.SchedulingMode.SchedulingMode&lt;br/&gt;
 import org.apache.spark.storage.BlockManagerId&lt;br/&gt;
 import org.apache.spark.util.AccumulatorV2&lt;br/&gt;
@@ -74,14 +75,15 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; trait TaskScheduler {&lt;br/&gt;
   def defaultParallelism(): Int&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-   * Update metrics for in-progress tasks and let the master know that the BlockManager is still&lt;br/&gt;
-   * alive. Return true if the driver knows about the given block manager. Otherwise, return false,&lt;br/&gt;
-   * indicating that the block manager should re-register.&lt;br/&gt;
+   * Update metrics for in-progress tasks and executor metrics, and let the master know that the&lt;br/&gt;
+   * BlockManager is still alive. Return true if the driver knows about the given block manager.&lt;br/&gt;
+   * Otherwise, return false, indicating that the block manager should re-register.&lt;br/&gt;
    */&lt;br/&gt;
   def executorHeartbeatReceived(&lt;br/&gt;
       execId: String,&lt;br/&gt;
       accumUpdates: Array[(Long, Seq[AccumulatorV2&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;])],&lt;br/&gt;
-      blockManagerId: BlockManagerId): Boolean&lt;br/&gt;
+      blockManagerId: BlockManagerId,&lt;br/&gt;
+      executorUpdates: ExecutorMetrics): Boolean&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
    * Get an application ID associated with the job.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala b/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala&lt;br/&gt;
index 8b71170668639..61556ea642614 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala&lt;br/&gt;
@@ -28,13 +28,14 @@ import scala.util.Random&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark._&lt;br/&gt;
 import org.apache.spark.TaskState.TaskState&lt;br/&gt;
+import org.apache.spark.executor.ExecutorMetrics&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.internal.config&lt;br/&gt;
 import org.apache.spark.rpc.RpcEndpoint&lt;br/&gt;
 import org.apache.spark.scheduler.SchedulingMode.SchedulingMode&lt;br/&gt;
 import org.apache.spark.scheduler.TaskLocality.TaskLocality&lt;br/&gt;
 import org.apache.spark.storage.BlockManagerId&lt;br/&gt;
-import org.apache.spark.util.{AccumulatorV2, ThreadUtils, Utils}&lt;br/&gt;
+import org.apache.spark.util.{AccumulatorV2, SystemClock, ThreadUtils, Utils}&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
  * Schedules tasks for multiple types of clusters by acting through a SchedulerBackend.&lt;br/&gt;
@@ -116,6 +117,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TaskSchedulerImpl(&lt;br/&gt;
 &lt;br/&gt;
   protected val executorIdToHost = new HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;&lt;br/&gt;
 &lt;br/&gt;
+  private val abortTimer = new Timer(true)&lt;br/&gt;
+  private val clock = new SystemClock&lt;br/&gt;
+  // Exposed for testing&lt;br/&gt;
+  val unschedulableTaskSetToExpiryTime = new HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskSetManager, Long&amp;#93;&lt;/span&gt;&lt;br/&gt;
+&lt;br/&gt;
   // Listener object to pass upcalls into&lt;br/&gt;
   var dagScheduler: DAGScheduler = null&lt;br/&gt;
 &lt;br/&gt;
@@ -414,9 +420,53 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TaskSchedulerImpl(&lt;br/&gt;
             launchedAnyTask |= launchedTaskAtCurrentMaxLocality&lt;br/&gt;
           } while (launchedTaskAtCurrentMaxLocality)&lt;br/&gt;
         }&lt;br/&gt;
+&lt;br/&gt;
         if (!launchedAnyTask) {&lt;br/&gt;
-          taskSet.abortIfCompletelyBlacklisted(hostToExecutors)&lt;br/&gt;
+          taskSet.getCompletelyBlacklistedTaskIfAny(hostToExecutors).foreach { taskIndex =&amp;gt;&lt;br/&gt;
+              // If the taskSet is unschedulable we try to find an existing idle blacklisted&lt;br/&gt;
+              // executor. If we cannot find one, we abort immediately. Else we kill the idle&lt;br/&gt;
+              // executor and kick off an abortTimer which if it doesn&apos;t schedule a task within the&lt;br/&gt;
+              // the timeout will abort the taskSet if we were unable to schedule any task from the&lt;br/&gt;
+              // taskSet.&lt;br/&gt;
+              // Note 1: We keep track of schedulability on a per taskSet basis rather than on a per&lt;br/&gt;
+              // task basis.&lt;br/&gt;
+              // Note 2: The taskSet can still be aborted when there are more than one idle&lt;br/&gt;
+              // blacklisted executors and dynamic allocation is on. This can happen when a killed&lt;br/&gt;
+              // idle executor isn&apos;t replaced in time by ExecutorAllocationManager as it relies on&lt;br/&gt;
+              // pending tasks and doesn&apos;t kill executors on idle timeouts, resulting in the abort&lt;br/&gt;
+              // timer to expire and abort the taskSet.&lt;br/&gt;
+              executorIdToRunningTaskIds.find(x =&amp;gt; !isExecutorBusy(x._1)) match {&lt;br/&gt;
+                case Some ((executorId, _)) =&amp;gt;&lt;br/&gt;
+                  if (!unschedulableTaskSetToExpiryTime.contains(taskSet)) {
+                    blacklistTrackerOpt.foreach(blt =&amp;gt; blt.killBlacklistedIdleExecutor(executorId))
+
+                    val timeout = conf.get(config.UNSCHEDULABLE_TASKSET_TIMEOUT) * 1000
+                    unschedulableTaskSetToExpiryTime(taskSet) = clock.getTimeMillis() + timeout
+                    logInfo(s&quot;Waiting for $timeout ms for completely &quot;
+                      + s&quot;blacklisted task to be schedulable again before aborting $taskSet.&quot;)
+                    abortTimer.schedule(
+                      createUnschedulableTaskSetAbortTimer(taskSet, taskIndex), timeout)
+                  }&lt;br/&gt;
+                case None =&amp;gt; // Abort Immediately&lt;br/&gt;
+                  logInfo(&quot;Cannot schedule any task because of complete blacklisting. No idle&quot; +&lt;br/&gt;
+                    s&quot; executors can be found to kill. Aborting $taskSet.&quot; )&lt;br/&gt;
+                  taskSet.abortSinceCompletelyBlacklisted(taskIndex)&lt;br/&gt;
+              }&lt;br/&gt;
+          }&lt;br/&gt;
+        } else {&lt;br/&gt;
+          // We want to defer killing any taskSets as long as we have a non blacklisted executor&lt;br/&gt;
+          // which can be used to schedule a task from any active taskSets. This ensures that the&lt;br/&gt;
+          // job can make progress.&lt;br/&gt;
+          // Note: It is theoretically possible that a taskSet never gets scheduled on a&lt;br/&gt;
+          // non-blacklisted executor and the abort timer doesn&apos;t kick in because of a constant&lt;br/&gt;
+          // submission of new TaskSets. See the PR for more details.&lt;br/&gt;
+          if (unschedulableTaskSetToExpiryTime.nonEmpty) {
+            logInfo(&quot;Clearing the expiry times for all unschedulable taskSets as a task was &quot; +
+              &quot;recently scheduled.&quot;)
+            unschedulableTaskSetToExpiryTime.clear()
+          }&lt;br/&gt;
         }&lt;br/&gt;
+&lt;br/&gt;
         if (launchedAnyTask &amp;amp;&amp;amp; taskSet.isBarrier) {
           // Check whether the barrier tasks are partially launched.
           // TODO SPARK-24818 handle the assert failure case (that can happen when some locality
@@ -452,6 +502,23 @@ private[spark] class TaskSchedulerImpl(
     return tasks
   }&lt;br/&gt;
 &lt;br/&gt;
+  private def createUnschedulableTaskSetAbortTimer(&lt;br/&gt;
+      taskSet: TaskSetManager,&lt;br/&gt;
+      taskIndex: Int): TimerTask = {&lt;br/&gt;
+    new TimerTask() {&lt;br/&gt;
+      override def run() {&lt;br/&gt;
+        if (unschedulableTaskSetToExpiryTime.contains(taskSet) &amp;amp;&amp;amp;&lt;br/&gt;
+            unschedulableTaskSetToExpiryTime(taskSet) &amp;lt;= clock.getTimeMillis()) {
+          logInfo(&quot;Cannot schedule any task because of complete blacklisting. &quot; +
+            s&quot;Wait time for scheduling expired. Aborting $taskSet.&quot;)
+          taskSet.abortSinceCompletelyBlacklisted(taskIndex)
+        } else {
+          this.cancel()
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
    * Shuffle offers around to avoid always placing tasks on the same workers.  Exposed to allow&lt;br/&gt;
    * overriding in tests, so it can be deterministic.&lt;br/&gt;
@@ -508,14 +575,15 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TaskSchedulerImpl(&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-   * Update metrics for in-progress tasks and let the master know that the BlockManager is still&lt;br/&gt;
-   * alive. Return true if the driver knows about the given block manager. Otherwise, return false,&lt;br/&gt;
-   * indicating that the block manager should re-register.&lt;br/&gt;
+   * Update metrics for in-progress tasks and executor metrics, and let the master know that the&lt;br/&gt;
+   * BlockManager is still alive. Return true if the driver knows about the given block manager.&lt;br/&gt;
+   * Otherwise, return false, indicating that the block manager should re-register.&lt;br/&gt;
    */&lt;br/&gt;
   override def executorHeartbeatReceived(&lt;br/&gt;
       execId: String,&lt;br/&gt;
       accumUpdates: Array[(Long, Seq[AccumulatorV2&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;])],&lt;br/&gt;
-      blockManagerId: BlockManagerId): Boolean = {&lt;br/&gt;
+      blockManagerId: BlockManagerId,&lt;br/&gt;
+      executorMetrics: ExecutorMetrics): Boolean = {&lt;br/&gt;
     // (taskId, stageId, stageAttemptId, accumUpdates)&lt;br/&gt;
     val accumUpdatesWithTaskIds: Array[(Long, Int, Int, Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;AccumulableInfo&amp;#93;&lt;/span&gt;)] = {&lt;br/&gt;
       accumUpdates.flatMap { case (id, updates) =&amp;gt;
@@ -525,7 +593,8 @@ private[spark] class TaskSchedulerImpl(
         }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
-    dagScheduler.executorHeartbeatReceived(execId, accumUpdatesWithTaskIds, blockManagerId)&lt;br/&gt;
+    dagScheduler.executorHeartbeatReceived(execId, accumUpdatesWithTaskIds, blockManagerId,&lt;br/&gt;
+      executorMetrics)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   def handleTaskGettingResult(taskSetManager: TaskSetManager, tid: Long): Unit = synchronized {
@@ -587,6 +656,7 @@ private[spark] class TaskSchedulerImpl(
       barrierCoordinator.stop()
     }&lt;br/&gt;
     starvationTimer.cancel()&lt;br/&gt;
+    abortTimer.cancel()&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   override def defaultParallelism(): Int = backend.defaultParallelism()&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala b/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala&lt;br/&gt;
index d5e85a11cb279..6bf60dd8e9dfa 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala&lt;br/&gt;
@@ -623,8 +623,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TaskSetManager(&lt;br/&gt;
    *&lt;br/&gt;
    * It is possible that this taskset has become impossible to schedule &lt;b&gt;anywhere&lt;/b&gt; due to the&lt;br/&gt;
    * blacklist.  The most common scenario would be if there are fewer executors than&lt;br/&gt;
-   * spark.task.maxFailures. We need to detect this so we can fail the task set, otherwise the job&lt;br/&gt;
-   * will hang.&lt;br/&gt;
+   * spark.task.maxFailures. We need to detect this so we can avoid the job from being hung.&lt;br/&gt;
+   * We try to acquire new executor/s by killing an existing idle blacklisted executor.&lt;br/&gt;
    *&lt;br/&gt;
    * There&apos;s a tradeoff here: we could make sure all tasks in the task set are schedulable, but that&lt;br/&gt;
    * would add extra time to each iteration of the scheduling loop. Here, we take the approach of&lt;br/&gt;
@@ -635,9 +635,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TaskSetManager(&lt;br/&gt;
    * failures (this is because the method picks one unscheduled task, and then iterates through each&lt;br/&gt;
    * executor until it finds one that the task isn&apos;t blacklisted on).&lt;br/&gt;
    */&lt;br/&gt;
-  private&lt;span class=&quot;error&quot;&gt;&amp;#91;scheduler&amp;#93;&lt;/span&gt; def abortIfCompletelyBlacklisted(&lt;br/&gt;
-      hostToExecutors: HashMap[String, HashSet&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;]): Unit = {&lt;br/&gt;
-    taskSetBlacklistHelperOpt.foreach { taskSetBlacklist =&amp;gt;&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;scheduler&amp;#93;&lt;/span&gt; def getCompletelyBlacklistedTaskIfAny(&lt;br/&gt;
+      hostToExecutors: HashMap[String, HashSet&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;]): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    taskSetBlacklistHelperOpt.flatMap { taskSetBlacklist =&amp;gt;
       val appBlacklist = blacklistTracker.get
       // Only look for unschedulable tasks when at least one executor has registered. Otherwise,
       // task sets will be (unnecessarily) aborted in cases when no executors have registered yet.
@@ -658,11 +658,11 @@ private[spark] class TaskSetManager(
           }&lt;br/&gt;
         }&lt;br/&gt;
 &lt;br/&gt;
-        pendingTask.foreach { indexInTaskSet =&amp;gt;&lt;br/&gt;
+        pendingTask.find { indexInTaskSet =&amp;gt;&lt;br/&gt;
           // try to find some executor this task can run on.  Its possible that some &lt;b&gt;other&lt;/b&gt;&lt;br/&gt;
           // task isn&apos;t schedulable anywhere, but we will discover that in some later call,&lt;br/&gt;
           // when that unschedulable task is the last task remaining.&lt;br/&gt;
-          val blacklistedEverywhere = hostToExecutors.forall { case (host, execsOnHost) =&amp;gt;&lt;br/&gt;
+          hostToExecutors.forall { case (host, execsOnHost) =&amp;gt;
             // Check if the task can run on the node
             val nodeBlacklisted =
               appBlacklist.isNodeBlacklisted(host) ||
@@ -679,22 +679,27 @@ private[spark] class TaskSetManager(
               }&lt;br/&gt;
             }&lt;br/&gt;
           }&lt;br/&gt;
-          if (blacklistedEverywhere) {&lt;br/&gt;
-            val partition = tasks(indexInTaskSet).partitionId&lt;br/&gt;
-            abort(s&quot;&quot;&quot;&lt;br/&gt;
-              |Aborting $taskSet because task $indexInTaskSet (partition $partition)&lt;br/&gt;
-              |cannot run anywhere due to node and executor blacklist.&lt;br/&gt;
-              |Most recent failure:&lt;br/&gt;
-              |${taskSetBlacklist.getLatestFailureReason}&lt;br/&gt;
-              |&lt;br/&gt;
-              |Blacklisting behavior can be configured via spark.blacklist.*.&lt;br/&gt;
-              |&quot;&quot;&quot;.stripMargin)&lt;br/&gt;
-          }&lt;br/&gt;
         }&lt;br/&gt;
+      } else {
+        None
       }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;scheduler&amp;#93;&lt;/span&gt; def abortSinceCompletelyBlacklisted(indexInTaskSet: Int): Unit = {&lt;br/&gt;
+    taskSetBlacklistHelperOpt.foreach { taskSetBlacklist =&amp;gt;&lt;br/&gt;
+      val partition = tasks(indexInTaskSet).partitionId&lt;br/&gt;
+      abort(s&quot;&quot;&quot;&lt;br/&gt;
+         |Aborting $taskSet because task $indexInTaskSet (partition $partition)&lt;br/&gt;
+         |cannot run anywhere due to node and executor blacklist.&lt;br/&gt;
+         |Most recent failure:&lt;br/&gt;
+         |${taskSetBlacklist.getLatestFailureReason}&lt;br/&gt;
+         |&lt;br/&gt;
+         |Blacklisting behavior can be configured via spark.blacklist.*.&lt;br/&gt;
+         |&quot;&quot;&quot;.stripMargin)&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
    * Marks the task as getting result and notifies the DAG Scheduler&lt;br/&gt;
    */&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala b/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala&lt;br/&gt;
index de7c0d813ae65..329158a44d369 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/scheduler/cluster/CoarseGrainedSchedulerBackend.scala&lt;br/&gt;
@@ -18,13 +18,17 @@&lt;br/&gt;
 package org.apache.spark.scheduler.cluster&lt;br/&gt;
 &lt;br/&gt;
 import java.util.concurrent.TimeUnit&lt;br/&gt;
-import java.util.concurrent.atomic.AtomicInteger&lt;br/&gt;
+import java.util.concurrent.atomic.{AtomicInteger, AtomicReference}&lt;br/&gt;
 import javax.annotation.concurrent.GuardedBy&lt;br/&gt;
 &lt;br/&gt;
 import scala.collection.mutable.{ArrayBuffer, HashMap, HashSet}&lt;br/&gt;
 import scala.concurrent.Future&lt;br/&gt;
 &lt;br/&gt;
+import org.apache.hadoop.security.UserGroupInformation&lt;br/&gt;
+&lt;br/&gt;
 import org.apache.spark.{ExecutorAllocationClient, SparkEnv, SparkException, TaskState}&lt;br/&gt;
+import org.apache.spark.deploy.SparkHadoopUtil&lt;br/&gt;
+import org.apache.spark.deploy.security.HadoopDelegationTokenManager&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.rpc._&lt;br/&gt;
 import org.apache.spark.scheduler._&lt;br/&gt;
@@ -95,6 +99,12 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp&lt;br/&gt;
   // The num of current max ExecutorId used to re-register appMaster&lt;br/&gt;
   @volatile protected var currentExecutorIdCounter = 0&lt;br/&gt;
 &lt;br/&gt;
+  // Current set of delegation tokens to send to executors.&lt;br/&gt;
+  private val delegationTokens = new AtomicReference[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;]()&lt;br/&gt;
+&lt;br/&gt;
+  // The token manager used to create security tokens.&lt;br/&gt;
+  private var delegationTokenManager: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;HadoopDelegationTokenManager&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
+&lt;br/&gt;
   private val reviveThread =&lt;br/&gt;
     ThreadUtils.newDaemonSingleThreadScheduledExecutor(&quot;driver-revive-thread&quot;)&lt;br/&gt;
 &lt;br/&gt;
@@ -152,6 +162,8 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp&lt;br/&gt;
         }&lt;br/&gt;
 &lt;br/&gt;
       case UpdateDelegationTokens(newDelegationTokens) =&amp;gt;&lt;br/&gt;
+        SparkHadoopUtil.get.addDelegationTokens(newDelegationTokens, conf)&lt;br/&gt;
+        delegationTokens.set(newDelegationTokens)&lt;br/&gt;
         executorDataMap.values.foreach { ed =&amp;gt;
           ed.executorEndpoint.send(UpdateDelegationTokens(newDelegationTokens))
         }&lt;br/&gt;
@@ -230,7 +242,7 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp&lt;br/&gt;
         val reply = SparkAppConfig(&lt;br/&gt;
           sparkProperties,&lt;br/&gt;
           SparkEnv.get.securityManager.getIOEncryptionKey(),&lt;br/&gt;
-          fetchHadoopDelegationTokens())&lt;br/&gt;
+          Option(delegationTokens.get()))&lt;br/&gt;
         context.reply(reply)&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -390,6 +402,21 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp&lt;br/&gt;
 &lt;br/&gt;
     // TODO (prashant) send conf instead of properties&lt;br/&gt;
     driverEndpoint = createDriverEndpointRef(properties)&lt;br/&gt;
+&lt;br/&gt;
+    if (UserGroupInformation.isSecurityEnabled()) {&lt;br/&gt;
+      delegationTokenManager = createTokenManager()&lt;br/&gt;
+      delegationTokenManager.foreach { dtm =&amp;gt;&lt;br/&gt;
+        dtm.setDriverRef(driverEndpoint)&lt;br/&gt;
+        val creds = if (dtm.renewalEnabled) {
+          dtm.start().getCredentials()
+        } else {
+          val creds = UserGroupInformation.getCurrentUser().getCredentials()
+          dtm.obtainDelegationTokens(creds)
+          creds
+        }&lt;br/&gt;
+        delegationTokens.set(SparkHadoopUtil.get.serialize(creds))&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   protected def createDriverEndpointRef(&lt;br/&gt;
@@ -416,6 +443,7 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp&lt;br/&gt;
   override def stop() {&lt;br/&gt;
     reviveThread.shutdownNow()&lt;br/&gt;
     stopExecutors()&lt;br/&gt;
+    delegationTokenManager.foreach(_.stop())&lt;br/&gt;
     try {&lt;br/&gt;
       if (driverEndpoint != null) {
         driverEndpoint.askSync[Boolean](StopDriver)
@@ -684,7 +712,13 @@ class CoarseGrainedSchedulerBackend(scheduler: TaskSchedulerImpl, val rpcEnv: Rp
     true
   }&lt;br/&gt;
 &lt;br/&gt;
-  protected def fetchHadoopDelegationTokens(): Option[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;] = { None }&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Create the delegation token manager to be used for the application. This method is called&lt;br/&gt;
+   * once during the start of the scheduler backend (so after the object has already been&lt;br/&gt;
+   * fully constructed), only if security is enabled in the Hadoop configuration.&lt;br/&gt;
+   */&lt;br/&gt;
+  protected def createTokenManager(): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;HadoopDelegationTokenManager&amp;#93;&lt;/span&gt; = None&lt;br/&gt;
+&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object CoarseGrainedSchedulerBackend {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala b/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala&lt;br/&gt;
index 00621976b77f4..18b735b8035ab 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/security/CryptoStreamUtils.scala&lt;br/&gt;
@@ -16,7 +16,7 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.spark.security&lt;br/&gt;
 &lt;br/&gt;
-import java.io.{InputStream, OutputStream}&lt;br/&gt;
+import java.io.{Closeable, InputStream, IOException, OutputStream}&lt;br/&gt;
 import java.nio.ByteBuffer&lt;br/&gt;
 import java.nio.channels.{ReadableByteChannel, WritableByteChannel}&lt;br/&gt;
 import java.util.Properties&lt;br/&gt;
@@ -54,8 +54,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object CryptoStreamUtils extends Logging {
     val params = new CryptoParams(key, sparkConf)
     val iv = createInitializationVector(params.conf)
     os.write(iv)
-    new CryptoOutputStream(params.transformation, params.conf, os, params.keySpec,
-      new IvParameterSpec(iv))
+    new ErrorHandlingOutputStream(
+      new CryptoOutputStream(params.transformation, params.conf, os, params.keySpec,
+        new IvParameterSpec(iv)),
+      os)
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
@@ -70,8 +72,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object CryptoStreamUtils extends Logging {
     val helper = new CryptoHelperChannel(channel)
 
     helper.write(ByteBuffer.wrap(iv))
-    new CryptoOutputStream(params.transformation, params.conf, helper, params.keySpec,
-      new IvParameterSpec(iv))
+    new ErrorHandlingWritableChannel(
+      new CryptoOutputStream(params.transformation, params.conf, helper, params.keySpec,
+        new IvParameterSpec(iv)),
+      helper)
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
@@ -84,8 +88,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object CryptoStreamUtils extends Logging {
     val iv = new Array[Byte](IV_LENGTH_IN_BYTES)
     ByteStreams.readFully(is, iv)
     val params = new CryptoParams(key, sparkConf)
-    new CryptoInputStream(params.transformation, params.conf, is, params.keySpec,
-      new IvParameterSpec(iv))
+    new ErrorHandlingInputStream(
+      new CryptoInputStream(params.transformation, params.conf, is, params.keySpec,
+        new IvParameterSpec(iv)),
+      is)
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
@@ -100,8 +106,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object CryptoStreamUtils extends Logging {
     JavaUtils.readFully(channel, buf)
 
     val params = new CryptoParams(key, sparkConf)
-    new CryptoInputStream(params.transformation, params.conf, channel, params.keySpec,
-      new IvParameterSpec(iv))
+    new ErrorHandlingReadableChannel(
+      new CryptoInputStream(params.transformation, params.conf, channel, params.keySpec,
+        new IvParameterSpec(iv)),
+      channel)
   }&lt;br/&gt;
 &lt;br/&gt;
   def toCryptoConf(conf: SparkConf): Properties = {&lt;br/&gt;
@@ -157,6 +165,117 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object CryptoStreamUtils extends Logging {&lt;br/&gt;
 &lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25535&quot; title=&quot;Work around bad error checking in commons-crypto&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25535&quot;&gt;&lt;del&gt;SPARK-25535&lt;/del&gt;&lt;/a&gt;. The commons-cryto library will throw InternalError if something goes&lt;br/&gt;
+   * wrong, and leave bad state behind in the Java wrappers, so it&apos;s not safe to use them&lt;br/&gt;
+   * afterwards. This wrapper detects that situation and avoids further calls into the&lt;br/&gt;
+   * commons-crypto code, while still allowing the underlying streams to be closed.&lt;br/&gt;
+   *&lt;br/&gt;
+   * This should be removed once &lt;a href=&quot;https://issues.apache.org/jira/browse/CRYPTO-141&quot; title=&quot;Errors in native code can leave Java wrappers in bad state&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CRYPTO-141&quot;&gt;&lt;del&gt;CRYPTO-141&lt;/del&gt;&lt;/a&gt; is fixed (and Spark upgrades its commons-crypto&lt;br/&gt;
+   * dependency).&lt;br/&gt;
+   */&lt;br/&gt;
+  trait BaseErrorHandler extends Closeable {&lt;br/&gt;
+&lt;br/&gt;
+    private var closed = false&lt;br/&gt;
+&lt;br/&gt;
+    /** The encrypted stream that may get into an unhealthy state. */&lt;br/&gt;
+    protected def cipherStream: Closeable&lt;br/&gt;
+&lt;br/&gt;
+    /**&lt;br/&gt;
+     * The underlying stream that is being wrapped by the encrypted stream, so that it can be&lt;br/&gt;
+     * closed even if there&apos;s an error in the crypto layer.&lt;br/&gt;
+     */&lt;br/&gt;
+    protected def original: Closeable&lt;br/&gt;
+&lt;br/&gt;
+    protected def safeCall&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(fn: =&amp;gt; T): T = {&lt;br/&gt;
+      if (closed) {
+        throw new IOException(&quot;Cipher stream is closed.&quot;)
+      }&lt;br/&gt;
+      try {
+        fn
+      } catch {
+        case ie: InternalError =&amp;gt;
+          closed = true
+          original.close()
+          throw ie
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+    override def close(): Unit = {&lt;br/&gt;
+      if (!closed) {
+        cipherStream.close()
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  // Visible for testing.&lt;br/&gt;
+  class ErrorHandlingReadableChannel(&lt;br/&gt;
+      protected val cipherStream: ReadableByteChannel,&lt;br/&gt;
+      protected val original: ReadableByteChannel)&lt;br/&gt;
+    extends ReadableByteChannel with BaseErrorHandler {&lt;br/&gt;
+&lt;br/&gt;
+    override def read(src: ByteBuffer): Int = safeCall {
+      cipherStream.read(src)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    override def isOpen(): Boolean = cipherStream.isOpen()&lt;br/&gt;
+&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private class ErrorHandlingInputStream(&lt;br/&gt;
+      protected val cipherStream: InputStream,&lt;br/&gt;
+      protected val original: InputStream)&lt;br/&gt;
+    extends InputStream with BaseErrorHandler {&lt;br/&gt;
+&lt;br/&gt;
+    override def read(b: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;): Int = safeCall {
+      cipherStream.read(b)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    override def read(b: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;, off: Int, len: Int): Int = safeCall {
+      cipherStream.read(b, off, len)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    override def read(): Int = safeCall {
+      cipherStream.read()
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private class ErrorHandlingWritableChannel(&lt;br/&gt;
+      protected val cipherStream: WritableByteChannel,&lt;br/&gt;
+      protected val original: WritableByteChannel)&lt;br/&gt;
+    extends WritableByteChannel with BaseErrorHandler {&lt;br/&gt;
+&lt;br/&gt;
+    override def write(src: ByteBuffer): Int = safeCall {
+      cipherStream.write(src)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    override def isOpen(): Boolean = cipherStream.isOpen()&lt;br/&gt;
+&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private class ErrorHandlingOutputStream(&lt;br/&gt;
+      protected val cipherStream: OutputStream,&lt;br/&gt;
+      protected val original: OutputStream)&lt;br/&gt;
+    extends OutputStream with BaseErrorHandler {&lt;br/&gt;
+&lt;br/&gt;
+    override def flush(): Unit = safeCall {
+      cipherStream.flush()
+    }&lt;br/&gt;
+&lt;br/&gt;
+    override def write(b: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;): Unit = safeCall {
+      cipherStream.write(b)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    override def write(b: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;, off: Int, len: Int): Unit = safeCall {
+      cipherStream.write(b, off, len)
+    }&lt;br/&gt;
+&lt;br/&gt;
+    override def write(b: Int): Unit = safeCall {+      cipherStream.write(b)+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   private class CryptoParams(key: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;, sparkConf: SparkConf) {&lt;br/&gt;
 &lt;br/&gt;
     val keySpec = new SecretKeySpec(key, &quot;AES&quot;)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala b/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala&lt;br/&gt;
index 72427dd6ce4d4..1e1c27c477877 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/serializer/KryoSerializer.scala&lt;br/&gt;
@@ -30,6 +30,7 @@ import scala.util.control.NonFatal&lt;br/&gt;
 import com.esotericsoftware.kryo.{Kryo, KryoException, Serializer =&amp;gt; KryoClassSerializer}&lt;br/&gt;
 import com.esotericsoftware.kryo.io.{Input =&amp;gt; KryoInput, Output =&amp;gt; KryoOutput}&lt;br/&gt;
 import com.esotericsoftware.kryo.io.{UnsafeInput =&amp;gt; KryoUnsafeInput, UnsafeOutput =&amp;gt; KryoUnsafeOutput}&lt;br/&gt;
+import com.esotericsoftware.kryo.pool.{KryoCallback, KryoFactory, KryoPool}&lt;br/&gt;
 import com.esotericsoftware.kryo.serializers.{JavaSerializer =&amp;gt; KryoJavaSerializer}&lt;br/&gt;
 import com.twitter.chill.{AllScalaRegistrar, EmptyScalaKryoInstantiator}&lt;br/&gt;
 import org.apache.avro.generic.{GenericData, GenericRecord}&lt;br/&gt;
@@ -41,7 +42,7 @@ import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.network.util.ByteUnit&lt;br/&gt;
 import org.apache.spark.scheduler.{CompressedMapStatus, HighlyCompressedMapStatus}&lt;br/&gt;
 import org.apache.spark.storage._&lt;br/&gt;
-import org.apache.spark.util.{BoundedPriorityQueue, SerializableConfiguration, SerializableJobConf, Utils}&lt;br/&gt;
+import org.apache.spark.util.{BoundedPriorityQueue, ByteBufferInputStream, SerializableConfiguration, SerializableJobConf, Utils}&lt;br/&gt;
 import org.apache.spark.util.collection.CompactBuffer&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
@@ -84,6 +85,7 @@ class KryoSerializer(conf: SparkConf)&lt;br/&gt;
   private val avroSchemas = conf.getAvroSchema&lt;br/&gt;
   // whether to use unsafe based IO for serialization&lt;br/&gt;
   private val useUnsafe = conf.getBoolean(&quot;spark.kryo.unsafe&quot;, false)&lt;br/&gt;
+  private val usePool = conf.getBoolean(&quot;spark.kryo.pool&quot;, true)&lt;br/&gt;
 &lt;br/&gt;
   def newKryoOutput(): KryoOutput =&lt;br/&gt;
     if (useUnsafe) {
@@ -92,6 +94,36 @@ class KryoSerializer(conf: SparkConf)
       new KryoOutput(bufferSize, math.max(bufferSize, maxBufferSize))
     }&lt;br/&gt;
 &lt;br/&gt;
+  @transient&lt;br/&gt;
+  private lazy val factory: KryoFactory = new KryoFactory() {&lt;br/&gt;
+    override def create: Kryo = {
+      newKryo()
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private class PoolWrapper extends KryoPool {&lt;br/&gt;
+    private var pool: KryoPool = getPool&lt;br/&gt;
+&lt;br/&gt;
+    override def borrow(): Kryo = pool.borrow()&lt;br/&gt;
+&lt;br/&gt;
+    override def release(kryo: Kryo): Unit = pool.release(kryo)&lt;br/&gt;
+&lt;br/&gt;
+    override def run&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(kryoCallback: KryoCallback&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;): T = pool.run(kryoCallback)&lt;br/&gt;
+&lt;br/&gt;
+    def reset(): Unit = {
+      pool = getPool
+    }&lt;br/&gt;
+&lt;br/&gt;
+    private def getPool: KryoPool = {
+      new KryoPool.Builder(factory).softReferences.build
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  @transient&lt;br/&gt;
+  private lazy val internalPool = new PoolWrapper&lt;br/&gt;
+&lt;br/&gt;
+  def pool: KryoPool = internalPool&lt;br/&gt;
+&lt;br/&gt;
   def newKryo(): Kryo = {&lt;br/&gt;
     val instantiator = new EmptyScalaKryoInstantiator&lt;br/&gt;
     val kryo = instantiator.newKryo()&lt;br/&gt;
@@ -132,7 +164,8 @@ class KryoSerializer(conf: SparkConf)&lt;br/&gt;
         .foreach { className =&amp;gt; kryo.register(Class.forName(className, true, classLoader)) }&lt;br/&gt;
       // Allow the user to register their own classes by setting spark.kryo.registrator.&lt;br/&gt;
       userRegistrators&lt;br/&gt;
-        .map(Class.forName(_, true, classLoader).newInstance().asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;KryoRegistrator&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+        .map(Class.forName(_, true, classLoader).getConstructor().&lt;br/&gt;
+          newInstance().asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;KryoRegistrator&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         .foreach { reg =&amp;gt; reg.registerClasses(kryo) }&lt;br/&gt;
       // scalastyle:on classforname&lt;br/&gt;
     } catch {&lt;br/&gt;
@@ -182,6 +215,12 @@ class KryoSerializer(conf: SparkConf)&lt;br/&gt;
     // We can&apos;t load those class directly in order to avoid unnecessary jar dependencies.&lt;br/&gt;
     // We load them safely, ignore it if the class not found.&lt;br/&gt;
     Seq(&lt;br/&gt;
+      &quot;org.apache.spark.ml.attribute.Attribute&quot;,&lt;br/&gt;
+      &quot;org.apache.spark.ml.attribute.AttributeGroup&quot;,&lt;br/&gt;
+      &quot;org.apache.spark.ml.attribute.BinaryAttribute&quot;,&lt;br/&gt;
+      &quot;org.apache.spark.ml.attribute.NominalAttribute&quot;,&lt;br/&gt;
+      &quot;org.apache.spark.ml.attribute.NumericAttribute&quot;,&lt;br/&gt;
+&lt;br/&gt;
       &quot;org.apache.spark.ml.feature.Instance&quot;,&lt;br/&gt;
       &quot;org.apache.spark.ml.feature.LabeledPoint&quot;,&lt;br/&gt;
       &quot;org.apache.spark.ml.feature.OffsetInstance&quot;,&lt;br/&gt;
@@ -191,6 +230,7 @@ class KryoSerializer(conf: SparkConf)&lt;br/&gt;
       &quot;org.apache.spark.ml.linalg.SparseMatrix&quot;,&lt;br/&gt;
       &quot;org.apache.spark.ml.linalg.SparseVector&quot;,&lt;br/&gt;
       &quot;org.apache.spark.ml.linalg.Vector&quot;,&lt;br/&gt;
+      &quot;org.apache.spark.ml.stat.distribution.MultivariateGaussian&quot;,&lt;br/&gt;
       &quot;org.apache.spark.ml.tree.impl.TreePoint&quot;,&lt;br/&gt;
       &quot;org.apache.spark.mllib.clustering.VectorWithNorm&quot;,&lt;br/&gt;
       &quot;org.apache.spark.mllib.linalg.DenseMatrix&quot;,&lt;br/&gt;
@@ -199,7 +239,8 @@ class KryoSerializer(conf: SparkConf)&lt;br/&gt;
       &quot;org.apache.spark.mllib.linalg.SparseMatrix&quot;,&lt;br/&gt;
       &quot;org.apache.spark.mllib.linalg.SparseVector&quot;,&lt;br/&gt;
       &quot;org.apache.spark.mllib.linalg.Vector&quot;,&lt;br/&gt;
-      &quot;org.apache.spark.mllib.regression.LabeledPoint&quot;&lt;br/&gt;
+      &quot;org.apache.spark.mllib.regression.LabeledPoint&quot;,&lt;br/&gt;
+      &quot;org.apache.spark.mllib.stat.distribution.MultivariateGaussian&quot;&lt;br/&gt;
     ).foreach { name =&amp;gt;&lt;br/&gt;
       try {
         val clazz = Utils.classForName(name)
@@ -214,8 +255,14 @@ class KryoSerializer(conf: SparkConf)
     kryo
   }&lt;br/&gt;
 &lt;br/&gt;
+  override def setDefaultClassLoader(classLoader: ClassLoader): Serializer = {
+    super.setDefaultClassLoader(classLoader)
+    internalPool.reset()
+    this
+  }&lt;br/&gt;
+&lt;br/&gt;
   override def newInstance(): SerializerInstance = {
-    new KryoSerializerInstance(this, useUnsafe)
+    new KryoSerializerInstance(this, useUnsafe, usePool)
   }&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; override lazy val supportsRelocationOfSerializedObjects: Boolean = {
@@ -298,7 +345,8 @@ class KryoDeserializationStream(
   }&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class KryoSerializerInstance(ks: KryoSerializer, useUnsafe: Boolean)&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class KryoSerializerInstance(&lt;br/&gt;
+   ks: KryoSerializer, useUnsafe: Boolean, usePool: Boolean)&lt;br/&gt;
   extends SerializerInstance {&lt;br/&gt;
   /**&lt;br/&gt;
    * A re-used [&lt;span class=&quot;error&quot;&gt;&amp;#91;Kryo&amp;#93;&lt;/span&gt;] instance. Methods will borrow this instance by calling `borrowKryo()`, do&lt;br/&gt;
@@ -306,22 +354,29 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class KryoSerializerInstance(ks: KryoSerializer, useUnsafe: Boole&lt;br/&gt;
    * pool of size one. SerializerInstances are not thread-safe, hence accesses to this field are&lt;br/&gt;
    * not synchronized.&lt;br/&gt;
    */&lt;br/&gt;
-  @Nullable private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var cachedKryo: Kryo = borrowKryo()&lt;br/&gt;
+  @Nullable private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var cachedKryo: Kryo = if (usePool) null else borrowKryo()&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
    * Borrows a [&lt;span class=&quot;error&quot;&gt;&amp;#91;Kryo&amp;#93;&lt;/span&gt;] instance. If possible, this tries to re-use a cached Kryo instance;&lt;br/&gt;
    * otherwise, it allocates a new instance.&lt;br/&gt;
    */&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;serializer&amp;#93;&lt;/span&gt; def borrowKryo(): Kryo = {&lt;br/&gt;
-    if (cachedKryo != null) {&lt;br/&gt;
-      val kryo = cachedKryo&lt;br/&gt;
-      // As a defensive measure, call reset() to clear any Kryo state that might have been modified&lt;br/&gt;
-      // by the last operation to borrow this instance (see &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-7766&quot; title=&quot;KryoSerializerInstance reuse is not safe when auto-reset is disabled&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-7766&quot;&gt;&lt;del&gt;SPARK-7766&lt;/del&gt;&lt;/a&gt; for discussion of this issue)&lt;br/&gt;
+    if (usePool) {
+      val kryo = ks.pool.borrow()
       kryo.reset()
-      cachedKryo = null
       kryo
     } else {&lt;br/&gt;
-      ks.newKryo()&lt;br/&gt;
+      if (cachedKryo != null) {
+        val kryo = cachedKryo
+        // As a defensive measure, call reset() to clear any Kryo state that might have
+        // been modified by the last operation to borrow this instance
+        // (see SPARK-7766 for discussion of this issue)
+        kryo.reset()
+        cachedKryo = null
+        kryo
+      } else {
+        ks.newKryo()
+      }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -331,8 +386,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class KryoSerializerInstance(ks: KryoSerializer, useUnsafe: Boole&lt;br/&gt;
    * re-use.&lt;br/&gt;
    */&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;serializer&amp;#93;&lt;/span&gt; def releaseKryo(kryo: Kryo): Unit = {&lt;br/&gt;
-    if (cachedKryo == null) {&lt;br/&gt;
-      cachedKryo = kryo&lt;br/&gt;
+    if (usePool) {
+      ks.pool.release(kryo)
+    } else {&lt;br/&gt;
+      if (cachedKryo == null) {
+        cachedKryo = kryo
+      }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -358,7 +417,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class KryoSerializerInstance(ks: KryoSerializer, useUnsafe: Boole&lt;br/&gt;
   override def deserialize&lt;span class=&quot;error&quot;&gt;&amp;#91;T: ClassTag&amp;#93;&lt;/span&gt;(bytes: ByteBuffer): T = {&lt;br/&gt;
     val kryo = borrowKryo()&lt;br/&gt;
     try {&lt;br/&gt;
-      input.setBuffer(bytes.array(), bytes.arrayOffset() + bytes.position(), bytes.remaining())&lt;br/&gt;
+      if (bytes.hasArray) {
+        input.setBuffer(bytes.array(), bytes.arrayOffset() + bytes.position(), bytes.remaining())
+      } else {
+        input.setBuffer(new Array[Byte](4096))
+        input.setInputStream(new ByteBufferInputStream(bytes))
+      }&lt;br/&gt;
       kryo.readClassAndObject(input).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;&lt;br/&gt;
     } finally {&lt;br/&gt;
       releaseKryo(kryo)&lt;br/&gt;
@@ -370,7 +434,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class KryoSerializerInstance(ks: KryoSerializer, useUnsafe: Boole&lt;br/&gt;
     val oldClassLoader = kryo.getClassLoader&lt;br/&gt;
     try {&lt;br/&gt;
       kryo.setClassLoader(loader)&lt;br/&gt;
-      input.setBuffer(bytes.array(), bytes.arrayOffset() + bytes.position(), bytes.remaining())&lt;br/&gt;
+      if (bytes.hasArray) {+        input.setBuffer(bytes.array(), bytes.arrayOffset() + bytes.position(), bytes.remaining())+      } else {
+        input.setBuffer(new Array[Byte](4096))
+        input.setInputStream(new ByteBufferInputStream(bytes))
+      }&lt;br/&gt;
       kryo.readClassAndObject(input).asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;&lt;br/&gt;
     } finally {
       kryo.setClassLoader(oldClassLoader)
diff --git a/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala b/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala
index 74b0e0b3a741a..27e2f98c58f0c 100644
--- a/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala
+++ b/core/src/main/scala/org/apache/spark/shuffle/BlockStoreShuffleReader.scala
@@ -33,6 +33,7 @@ private[spark] class BlockStoreShuffleReader[K, C](
     startPartition: Int,
     endPartition: Int,
     context: TaskContext,
+    readMetrics: ShuffleReadMetricsReporter,
     serializerManager: SerializerManager = SparkEnv.get.serializerManager,
     blockManager: BlockManager = SparkEnv.get.blockManager,
     mapOutputTracker: MapOutputTracker = SparkEnv.get.mapOutputTracker)
@@ -53,7 +54,8 @@ private[spark] class BlockStoreShuffleReader[K, C](
       SparkEnv.get.conf.getInt(&quot;spark.reducer.maxReqsInFlight&quot;, Int.MaxValue),
       SparkEnv.get.conf.get(config.REDUCER_MAX_BLOCKS_IN_FLIGHT_PER_ADDRESS),
       SparkEnv.get.conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM),
-      SparkEnv.get.conf.getBoolean(&quot;spark.shuffle.detectCorrupt&quot;, true))
+      SparkEnv.get.conf.getBoolean(&quot;spark.shuffle.detectCorrupt&quot;, true),
+      readMetrics)
 
     val serializerInstance = dep.serializer.newInstance()
 
@@ -66,7 +68,6 @@ private[spark] class BlockStoreShuffleReader[K, C](
     }&lt;br/&gt;
 &lt;br/&gt;
     // Update the context task metrics for each record read.&lt;br/&gt;
-    val readMetrics = context.taskMetrics.createTempShuffleReadMetrics()&lt;br/&gt;
     val metricIter = CompletionIterator[(Any, Any), Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;(Any, Any)&amp;#93;&lt;/span&gt;](&lt;br/&gt;
       recordIter.map { record =&amp;gt;&lt;br/&gt;
         readMetrics.incRecordsRead(1)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala b/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala&lt;br/&gt;
index 4ea8a7120a9cc..18a743fbfa6fc 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/shuffle/ShuffleManager.scala&lt;br/&gt;
@@ -38,7 +38,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; trait ShuffleManager {&lt;br/&gt;
       dependency: ShuffleDependency&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V, C&amp;#93;&lt;/span&gt;): ShuffleHandle&lt;br/&gt;
 &lt;br/&gt;
   /** Get a writer for a given partition. Called on executors by map tasks. */&lt;br/&gt;
-  def getWriter&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(handle: ShuffleHandle, mapId: Int, context: TaskContext): ShuffleWriter&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  def getWriter&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(&lt;br/&gt;
+      handle: ShuffleHandle,&lt;br/&gt;
+      mapId: Int,&lt;br/&gt;
+      context: TaskContext,&lt;br/&gt;
+      metrics: ShuffleWriteMetricsReporter): ShuffleWriter&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
    * Get a reader for a range of reduce partitions (startPartition to endPartition-1, inclusive).&lt;br/&gt;
@@ -48,7 +52,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; trait ShuffleManager {&lt;br/&gt;
       handle: ShuffleHandle,&lt;br/&gt;
       startPartition: Int,&lt;br/&gt;
       endPartition: Int,&lt;br/&gt;
-      context: TaskContext): ShuffleReader&lt;span class=&quot;error&quot;&gt;&amp;#91;K, C&amp;#93;&lt;/span&gt;&lt;br/&gt;
+      context: TaskContext,&lt;br/&gt;
+      metrics: ShuffleReadMetricsReporter): ShuffleReader&lt;span class=&quot;error&quot;&gt;&amp;#91;K, C&amp;#93;&lt;/span&gt;&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
    * Remove a shuffle&apos;s metadata from the ShuffleManager.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/shuffle/metrics.scala b/core/src/main/scala/org/apache/spark/shuffle/metrics.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..33be677bc90cb&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/shuffle/metrics.scala&lt;br/&gt;
@@ -0,0 +1,52 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.spark.shuffle&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * An interface for reporting shuffle read metrics, for each shuffle. This interface assumes&lt;br/&gt;
+ * all the methods are called on a single-threaded, i.e. concrete implementations would not need&lt;br/&gt;
+ * to synchronize.&lt;br/&gt;
+ *&lt;br/&gt;
+ * All methods have additional Spark visibility modifier to allow public, concrete implementations&lt;br/&gt;
+ * that still have these methods marked as private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;.&lt;br/&gt;
+ */&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; trait ShuffleReadMetricsReporter {
+  private[spark] def incRemoteBlocksFetched(v: Long): Unit
+  private[spark] def incLocalBlocksFetched(v: Long): Unit
+  private[spark] def incRemoteBytesRead(v: Long): Unit
+  private[spark] def incRemoteBytesReadToDisk(v: Long): Unit
+  private[spark] def incLocalBytesRead(v: Long): Unit
+  private[spark] def incFetchWaitTime(v: Long): Unit
+  private[spark] def incRecordsRead(v: Long): Unit
+}&lt;br/&gt;
+&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * An interface for reporting shuffle write metrics. This interface assumes all the methods are&lt;br/&gt;
+ * called on a single-threaded, i.e. concrete implementations would not need to synchronize.&lt;br/&gt;
+ *&lt;br/&gt;
+ * All methods have additional Spark visibility modifier to allow public, concrete implementations&lt;br/&gt;
+ * that still have these methods marked as private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;.&lt;br/&gt;
+ */&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; trait ShuffleWriteMetricsReporter {
+  private[spark] def incBytesWritten(v: Long): Unit
+  private[spark] def incRecordsWritten(v: Long): Unit
+  private[spark] def incWriteTime(v: Long): Unit
+  private[spark] def decBytesWritten(v: Long): Unit
+  private[spark] def decRecordsWritten(v: Long): Unit
+}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala b/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala&lt;br/&gt;
index 0caf84c6050a8..b51a843a31c31 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleManager.scala&lt;br/&gt;
@@ -114,16 +114,19 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SortShuffleManager(conf: SparkConf) extends ShuffleManager&lt;br/&gt;
       handle: ShuffleHandle,&lt;br/&gt;
       startPartition: Int,&lt;br/&gt;
       endPartition: Int,&lt;br/&gt;
-      context: TaskContext): ShuffleReader&lt;span class=&quot;error&quot;&gt;&amp;#91;K, C&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+      context: TaskContext,&lt;br/&gt;
+      metrics: ShuffleReadMetricsReporter): ShuffleReader&lt;span class=&quot;error&quot;&gt;&amp;#91;K, C&amp;#93;&lt;/span&gt; = {
     new BlockStoreShuffleReader(
-      handle.asInstanceOf[BaseShuffleHandle[K, _, C]], startPartition, endPartition, context)
+      handle.asInstanceOf[BaseShuffleHandle[K, _, C]],
+      startPartition, endPartition, context, metrics)
   }&lt;br/&gt;
 &lt;br/&gt;
   /** Get a writer for a given partition. Called on executors by map tasks. */&lt;br/&gt;
   override def getWriter&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       handle: ShuffleHandle,&lt;br/&gt;
       mapId: Int,&lt;br/&gt;
-      context: TaskContext): ShuffleWriter&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+      context: TaskContext,&lt;br/&gt;
+      metrics: ShuffleWriteMetricsReporter): ShuffleWriter&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V&amp;#93;&lt;/span&gt; = {
     numMapsForShuffle.putIfAbsent(
       handle.shuffleId, handle.asInstanceOf[BaseShuffleHandle[_, _, _]].numMaps)
     val env = SparkEnv.get
@@ -136,15 +139,16 @@ private[spark] class SortShuffleManager(conf: SparkConf) extends ShuffleManager
           unsafeShuffleHandle,
           mapId,
           context,
-          env.conf)
+          env.conf,
+          metrics)
       case bypassMergeSortHandle: BypassMergeSortShuffleHandle[K @unchecked, V @unchecked] =&amp;gt;
         new BypassMergeSortShuffleWriter(
           env.blockManager,
           shuffleBlockResolver.asInstanceOf[IndexShuffleBlockResolver],
           bypassMergeSortHandle,
           mapId,
-          context,
-          env.conf)
+          env.conf,
+          metrics)
       case other: BaseShuffleHandle[K @unchecked, V @unchecked, _] =&amp;gt;
         new SortShuffleWriter(shuffleBlockResolver, other, mapId, context)
     }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala b/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala&lt;br/&gt;
index 91fc26762e533..274399b9cc1f3 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/shuffle/sort/SortShuffleWriter.scala&lt;br/&gt;
@@ -70,8 +70,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class SortShuffleWriter&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V, C&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       val blockId = ShuffleBlockId(dep.shuffleId, mapId, IndexShuffleBlockResolver.NOOP_REDUCE_ID)&lt;br/&gt;
       val partitionLengths = sorter.writePartitionedFile(blockId, tmp)&lt;br/&gt;
       shuffleBlockResolver.writeIndexFileAndCommit(dep.shuffleId, mapId, partitionLengths, tmp)&lt;br/&gt;
-      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths,&lt;br/&gt;
-        writeMetrics.recordsWritten)&lt;br/&gt;
+      mapStatus = MapStatus(blockManager.shuffleServerId, partitionLengths)&lt;br/&gt;
     } finally {&lt;br/&gt;
       if (tmp.exists() &amp;amp;&amp;amp; !tmp.delete()) {&lt;br/&gt;
         logError(s&quot;Error while deleting temp file ${tmp.getAbsolutePath}&quot;)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala b/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala&lt;br/&gt;
index 91b75e4852999..bd3f58b6182c0 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/AppStatusListener.scala&lt;br/&gt;
@@ -25,8 +25,9 @@ import scala.collection.JavaConverters._&lt;br/&gt;
 import scala.collection.mutable.HashMap&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark._&lt;br/&gt;
-import org.apache.spark.executor.TaskMetrics&lt;br/&gt;
+import org.apache.spark.executor.{ExecutorMetrics, TaskMetrics}&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
+import org.apache.spark.internal.config.Status._&lt;br/&gt;
 import org.apache.spark.scheduler._&lt;br/&gt;
 import org.apache.spark.status.api.v1&lt;br/&gt;
 import org.apache.spark.storage._&lt;br/&gt;
@@ -44,10 +45,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
     kvstore: ElementTrackingStore,&lt;br/&gt;
     conf: SparkConf,&lt;br/&gt;
     live: Boolean,&lt;br/&gt;
+    appStatusSource: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;AppStatusSource&amp;#93;&lt;/span&gt; = None,&lt;br/&gt;
     lastUpdateTime: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; = None) extends SparkListener with Logging {
 
-  import config._
-
   private var sparkVersion = SPARK_VERSION
   private var appInfo: v1.ApplicationInfo = null
   private var appSummary = new AppSummary(0, 0)
@@ -66,6 +66,7 @@ private[spark] class AppStatusListener(
   private val liveStages = new ConcurrentHashMap[(Int, Int), LiveStage]()
   private val liveJobs = new HashMap[Int, LiveJob]()
   private val liveExecutors = new HashMap[String, LiveExecutor]()
+  private val deadExecutors = new HashMap[String, LiveExecutor]()
   private val liveTasks = new HashMap[Long, LiveTask]()
   private val liveRDDs = new HashMap[Int, LiveRDD]()
   private val pools = new HashMap[String, SchedulerPool]()
@@ -204,6 +205,19 @@ private[spark] class AppStatusListener(
           update(rdd, now)
         }&lt;br/&gt;
       }&lt;br/&gt;
+      if (isExecutorActiveForLiveStages(exec)) {
+        // the executor was running for a currently active stage, so save it for now in
+        // deadExecutors, and remove when there are no active stages overlapping with the
+        // executor.
+        deadExecutors.put(event.executorId, exec)
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  /** Was the specified executor active for any currently live stages? */&lt;br/&gt;
+  private def isExecutorActiveForLiveStages(exec: LiveExecutor): Boolean = {&lt;br/&gt;
+    liveStages.values.asScala.exists { stage =&amp;gt;
+      stage.info.submissionTime.getOrElse(0L) &amp;lt; exec.removeTime.getTime
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -266,6 +280,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
   private def updateBlackListStatus(execId: String, blacklisted: Boolean): Unit = {&lt;br/&gt;
     liveExecutors.get(execId).foreach { exec =&amp;gt;&lt;br/&gt;
       exec.isBlacklisted = blacklisted&lt;br/&gt;
+      if (blacklisted) {
+        appStatusSource.foreach(_.BLACKLISTED_EXECUTORS.inc())
+      } else {
+        appStatusSource.foreach(_.UNBLACKLISTED_EXECUTORS.inc())
+      }&lt;br/&gt;
       liveUpdate(exec, System.nanoTime())&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
@@ -368,16 +387,40 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
       job.status = event.jobResult match {&lt;br/&gt;
-        case JobSucceeded =&amp;gt; JobExecutionStatus.SUCCEEDED&lt;br/&gt;
-        case JobFailed(_) =&amp;gt; JobExecutionStatus.FAILED&lt;br/&gt;
+        case JobSucceeded =&amp;gt;&lt;br/&gt;
+          appStatusSource.foreach{_.SUCCEEDED_JOBS.inc()}&lt;br/&gt;
+          JobExecutionStatus.SUCCEEDED&lt;br/&gt;
+        case JobFailed(_) =&amp;gt;&lt;br/&gt;
+          appStatusSource.foreach{_.FAILED_JOBS.inc()}&lt;br/&gt;
+          JobExecutionStatus.FAILED&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
       job.completionTime = if (event.time &amp;gt; 0) Some(new Date(event.time)) else None&lt;br/&gt;
+&lt;br/&gt;
+      for {
+        source &amp;lt;- appStatusSource
+        submissionTime &amp;lt;- job.submissionTime
+        completionTime &amp;lt;- job.completionTime
+      } {
+        source.JOB_DURATION.value.set(completionTime.getTime() - submissionTime.getTime())
+      }&lt;br/&gt;
+&lt;br/&gt;
+      // update global app status counters&lt;br/&gt;
+      appStatusSource.foreach { source =&amp;gt;
+        source.COMPLETED_STAGES.inc(job.completedStages.size)
+        source.FAILED_STAGES.inc(job.failedStages)
+        source.COMPLETED_TASKS.inc(job.completedTasks)
+        source.FAILED_TASKS.inc(job.failedTasks)
+        source.KILLED_TASKS.inc(job.killedTasks)
+        source.SKIPPED_TASKS.inc(job.skippedTasks)
+        source.SKIPPED_STAGES.inc(job.skippedStages.size)
+      }&lt;br/&gt;
       update(job, now, last = true)&lt;br/&gt;
+      if (job.status == JobExecutionStatus.SUCCEEDED) {
+        appSummary = new AppSummary(appSummary.numCompletedJobs + 1, appSummary.numCompletedStages)
+        kvstore.write(appSummary)
+      }&lt;br/&gt;
     }&lt;br/&gt;
-&lt;br/&gt;
-    appSummary = new AppSummary(appSummary.numCompletedJobs + 1, appSummary.numCompletedStages)&lt;br/&gt;
-    kvstore.write(appSummary)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   override def onStageSubmitted(event: SparkListenerStageSubmitted): Unit = {&lt;br/&gt;
@@ -430,6 +473,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
       val locality = event.taskInfo.taskLocality.toString()&lt;br/&gt;
       val count = stage.localitySummary.getOrElse(locality, 0L) + 1L&lt;br/&gt;
       stage.localitySummary = stage.localitySummary ++ Map(locality -&amp;gt; count)&lt;br/&gt;
+      stage.activeTasksPerExecutor(event.taskInfo.executorId) += 1&lt;br/&gt;
       maybeUpdate(stage, now)&lt;br/&gt;
 &lt;br/&gt;
       stage.jobs.foreach { job =&amp;gt;&lt;br/&gt;
@@ -515,6 +559,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
       if (killedDelta &amp;gt; 0) {
         stage.killedSummary = killedTasksSummary(event.reason, stage.killedSummary)
       }&lt;br/&gt;
+      stage.activeTasksPerExecutor(event.taskInfo.executorId) -= 1&lt;br/&gt;
       // &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-24415&quot; title=&quot;Stage page aggregated executor metrics wrong when failures &quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-24415&quot;&gt;&lt;del&gt;SPARK-24415&lt;/del&gt;&lt;/a&gt; Wait for all tasks to finish before removing stage from live list&lt;br/&gt;
       val removeStage =&lt;br/&gt;
         stage.activeTasks == 0 &amp;amp;&amp;amp;&lt;br/&gt;
@@ -539,7 +584,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
         if (killedDelta &amp;gt; 0) {
           job.killedSummary = killedTasksSummary(event.reason, job.killedSummary)
         }&lt;br/&gt;
-        conditionalLiveUpdate(job, now, removeStage)&lt;br/&gt;
+        if (removeStage) {
+          update(job, now)
+        } else {
+          maybeUpdate(job, now)
+        }&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
       val esummary = stage.executorSummary(event.taskInfo.executorId)&lt;br/&gt;
@@ -550,7 +599,16 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
       if (metricsDelta != null) {
         esummary.metrics = LiveEntityHelpers.addMetrics(esummary.metrics, metricsDelta)
       }&lt;br/&gt;
-      conditionalLiveUpdate(esummary, now, removeStage)&lt;br/&gt;
+&lt;br/&gt;
+      val isLastTask = stage.activeTasksPerExecutor(event.taskInfo.executorId) == 0&lt;br/&gt;
+&lt;br/&gt;
+      // If the last task of the executor finished, then update the esummary&lt;br/&gt;
+      // for both live and history events.&lt;br/&gt;
+      if (isLastTask) {
+        update(esummary, now)
+      } else {
+        maybeUpdate(esummary, now)
+      }&lt;br/&gt;
 &lt;br/&gt;
       if (!stage.cleaning &amp;amp;&amp;amp; stage.savedTasks.get() &amp;gt; maxTasksPerStage) {
         stage.cleaning = true
@@ -583,9 +641,14 @@ private[spark] class AppStatusListener(
         }&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
-      // Force an update on live applications when the number of active tasks reaches 0. This is&lt;br/&gt;
-      // checked in some tests (e.g. SQLTestUtilsBase) so it needs to be reliably up to date.&lt;br/&gt;
-      conditionalLiveUpdate(exec, now, exec.activeTasks == 0)&lt;br/&gt;
+      // Force an update on both live and history applications when the number of active tasks&lt;br/&gt;
+      // reaches 0. This is checked in some tests (e.g. SQLTestUtilsBase) so it needs to be&lt;br/&gt;
+      // reliably up to date.&lt;br/&gt;
+      if (exec.activeTasks == 0) {
+        update(exec, now)
+      } else {
+        maybeUpdate(exec, now)
+      }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -639,10 +702,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
       if (removeStage) {
         liveStages.remove((event.stageInfo.stageId, event.stageInfo.attemptNumber))
       }&lt;br/&gt;
+      if (stage.status == v1.StageStatus.COMPLETE) {
+        appSummary = new AppSummary(appSummary.numCompletedJobs, appSummary.numCompletedStages + 1)
+        kvstore.write(appSummary)
+      }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    appSummary = new AppSummary(appSummary.numCompletedJobs, appSummary.numCompletedStages + 1)&lt;br/&gt;
-    kvstore.write(appSummary)&lt;br/&gt;
+    // remove any dead executors that were not running for any currently active stages&lt;br/&gt;
+    deadExecutors.retain((execId, exec) =&amp;gt; isExecutorActiveForLiveStages(exec))&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def removeBlackListedStageFrom(exec: LiveExecutor, stageId: Int, now: Long) = {
@@ -669,7 +736,37 @@ private[spark] class AppStatusListener(
   }&lt;br/&gt;
 &lt;br/&gt;
   override def onUnpersistRDD(event: SparkListenerUnpersistRDD): Unit = {&lt;br/&gt;
-    liveRDDs.remove(event.rddId)&lt;br/&gt;
+    liveRDDs.remove(event.rddId).foreach { liveRDD =&amp;gt;&lt;br/&gt;
+      val storageLevel = liveRDD.info.storageLevel&lt;br/&gt;
+&lt;br/&gt;
+      // Use RDD partition info to update executor block info.&lt;br/&gt;
+      liveRDD.getPartitions().foreach { case (_, part) =&amp;gt;&lt;br/&gt;
+        part.executors.foreach { executorId =&amp;gt;&lt;br/&gt;
+          liveExecutors.get(executorId).foreach { exec =&amp;gt;
+            exec.rddBlocks = exec.rddBlocks - 1
+          }&lt;br/&gt;
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+&lt;br/&gt;
+      val now = System.nanoTime()&lt;br/&gt;
+&lt;br/&gt;
+      // Use RDD distribution to update executor memory and disk usage info.&lt;br/&gt;
+      liveRDD.getDistributions().foreach { case (executorId, rddDist) =&amp;gt;&lt;br/&gt;
+        liveExecutors.get(executorId).foreach { exec =&amp;gt;&lt;br/&gt;
+          if (exec.hasMemoryInfo) {&lt;br/&gt;
+            if (storageLevel.useOffHeap) {
+              exec.usedOffHeap = addDeltaToValue(exec.usedOffHeap, -rddDist.offHeapUsed)
+            } else {
+              exec.usedOnHeap = addDeltaToValue(exec.usedOnHeap, -rddDist.onHeapUsed)
+            }&lt;br/&gt;
+          }&lt;br/&gt;
+          exec.memoryUsed = addDeltaToValue(exec.memoryUsed, -rddDist.memoryUsed)&lt;br/&gt;
+          exec.diskUsed = addDeltaToValue(exec.diskUsed, -rddDist.diskUsed)&lt;br/&gt;
+          maybeUpdate(exec, now)&lt;br/&gt;
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
     kvstore.delete(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;RDDStorageInfoWrapper&amp;#93;&lt;/span&gt;, event.rddId)&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
@@ -692,6 +789,31 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
         }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
+&lt;br/&gt;
+    // check if there is a new peak value for any of the executor level memory metrics&lt;br/&gt;
+    // for the live UI. SparkListenerExecutorMetricsUpdate events are only processed&lt;br/&gt;
+    // for the live UI.&lt;br/&gt;
+    event.executorUpdates.foreach { updates =&amp;gt;&lt;br/&gt;
+      liveExecutors.get(event.execId).foreach { exec =&amp;gt;&lt;br/&gt;
+        if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(updates)) {
+          maybeUpdate(exec, now)
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  override def onStageExecutorMetrics(executorMetrics: SparkListenerStageExecutorMetrics): Unit = {&lt;br/&gt;
+    val now = System.nanoTime()&lt;br/&gt;
+&lt;br/&gt;
+    // check if there is a new peak value for any of the executor level memory metrics,&lt;br/&gt;
+    // while reading from the log. SparkListenerStageExecutorMetrics are only processed&lt;br/&gt;
+    // when reading logs.&lt;br/&gt;
+    liveExecutors.get(executorMetrics.execId)&lt;br/&gt;
+      .orElse(deadExecutors.get(executorMetrics.execId)).map { exec =&amp;gt;&lt;br/&gt;
+      if (exec.peakExecutorMetrics.compareAndUpdatePeakValues(executorMetrics.executorMetrics)) {+        update(exec, now)+      }&lt;br/&gt;
+    }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   override def onBlockUpdated(event: SparkListenerBlockUpdated): Unit = {
@@ -728,6 +850,11 @@ private[spark] class AppStatusListener(
       .sortBy(_.stageId)
   }&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * Apply a delta to a value, but ensure that it doesn&apos;t go negative.&lt;br/&gt;
+   */&lt;br/&gt;
+  private def addDeltaToValue(old: Long, delta: Long): Long = math.max(0, old + delta)&lt;br/&gt;
+&lt;br/&gt;
   private def updateRDDBlock(event: SparkListenerBlockUpdated, block: RDDBlockId): Unit = {&lt;br/&gt;
     val now = System.nanoTime()&lt;br/&gt;
     val executorId = event.blockUpdatedInfo.blockManagerId.executorId&lt;br/&gt;
@@ -737,9 +864,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
     val diskDelta = event.blockUpdatedInfo.diskSize * (if (storageLevel.useDisk) 1 else -1)&lt;br/&gt;
     val memoryDelta = event.blockUpdatedInfo.memSize * (if (storageLevel.useMemory) 1 else -1)&lt;br/&gt;
 &lt;br/&gt;
-    // Function to apply a delta to a value, but ensure that it doesn&apos;t go negative.&lt;br/&gt;
-    def newValue(old: Long, delta: Long): Long = math.max(0, old + delta)&lt;br/&gt;
-&lt;br/&gt;
     val updatedStorageLevel = if (storageLevel.isValid) {
       Some(storageLevel.description)
     } else {&lt;br/&gt;
@@ -756,13 +880,13 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
     maybeExec.foreach { exec =&amp;gt;&lt;br/&gt;
       if (exec.hasMemoryInfo) {&lt;br/&gt;
         if (storageLevel.useOffHeap) {
-          exec.usedOffHeap = newValue(exec.usedOffHeap, memoryDelta)
+          exec.usedOffHeap = addDeltaToValue(exec.usedOffHeap, memoryDelta)
         } else {
-          exec.usedOnHeap = newValue(exec.usedOnHeap, memoryDelta)
+          exec.usedOnHeap = addDeltaToValue(exec.usedOnHeap, memoryDelta)
         }&lt;br/&gt;
       }&lt;br/&gt;
-      exec.memoryUsed = newValue(exec.memoryUsed, memoryDelta)&lt;br/&gt;
-      exec.diskUsed = newValue(exec.diskUsed, diskDelta)&lt;br/&gt;
+      exec.memoryUsed = addDeltaToValue(exec.memoryUsed, memoryDelta)&lt;br/&gt;
+      exec.diskUsed = addDeltaToValue(exec.diskUsed, diskDelta)&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     // Update the block entry in the RDD info, keeping track of the deltas above so that we&lt;br/&gt;
@@ -790,8 +914,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
       // Only update the partition if it&apos;s still stored in some executor, otherwise get rid of it.&lt;br/&gt;
       if (executors.nonEmpty) {
         partition.update(executors, rdd.storageLevel,
-          newValue(partition.memoryUsed, memoryDelta),
-          newValue(partition.diskUsed, diskDelta))
+          addDeltaToValue(partition.memoryUsed, memoryDelta),
+          addDeltaToValue(partition.diskUsed, diskDelta))
       } else {
         rdd.removePartition(block.name)
       }&lt;br/&gt;
@@ -799,14 +923,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
       maybeExec.foreach { exec =&amp;gt;&lt;br/&gt;
         if (exec.rddBlocks + rddBlocksDelta &amp;gt; 0) {&lt;br/&gt;
           val dist = rdd.distribution(exec)&lt;br/&gt;
-          dist.memoryUsed = newValue(dist.memoryUsed, memoryDelta)&lt;br/&gt;
-          dist.diskUsed = newValue(dist.diskUsed, diskDelta)&lt;br/&gt;
+          dist.memoryUsed = addDeltaToValue(dist.memoryUsed, memoryDelta)&lt;br/&gt;
+          dist.diskUsed = addDeltaToValue(dist.diskUsed, diskDelta)&lt;br/&gt;
 &lt;br/&gt;
           if (exec.hasMemoryInfo) {&lt;br/&gt;
             if (storageLevel.useOffHeap) {
-              dist.offHeapUsed = newValue(dist.offHeapUsed, memoryDelta)
+              dist.offHeapUsed = addDeltaToValue(dist.offHeapUsed, memoryDelta)
             } else {
-              dist.onHeapUsed = newValue(dist.onHeapUsed, memoryDelta)
+              dist.onHeapUsed = addDeltaToValue(dist.onHeapUsed, memoryDelta)
             }&lt;br/&gt;
           }&lt;br/&gt;
           dist.lastUpdate = null&lt;br/&gt;
@@ -825,8 +949,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
         }&lt;br/&gt;
       }&lt;br/&gt;
 &lt;br/&gt;
-      rdd.memoryUsed = newValue(rdd.memoryUsed, memoryDelta)&lt;br/&gt;
-      rdd.diskUsed = newValue(rdd.diskUsed, diskDelta)&lt;br/&gt;
+      rdd.memoryUsed = addDeltaToValue(rdd.memoryUsed, memoryDelta)&lt;br/&gt;
+      rdd.diskUsed = addDeltaToValue(rdd.diskUsed, diskDelta)&lt;br/&gt;
       update(rdd, now)&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -905,14 +1029,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  private def conditionalLiveUpdate(entity: LiveEntity, now: Long, condition: Boolean): Unit = {&lt;br/&gt;
-    if (condition) {
-      liveUpdate(entity, now)
-    } else {
-      maybeUpdate(entity, now)
-    }&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
   private def cleanupExecutors(count: Long): Unit = {
     // Because the limit is on the number of *dead* executors, we need to calculate whether
     // there are actually enough dead executors to be deleted.
@@ -968,16 +1084,6 @@ private[spark] class AppStatusListener(
         kvstore.delete(e.getClass(), e.id)
       }&lt;br/&gt;
 &lt;br/&gt;
-      val tasks = kvstore.view(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskDataWrapper&amp;#93;&lt;/span&gt;)&lt;br/&gt;
-        .index(&quot;stage&quot;)&lt;br/&gt;
-        .first(key)&lt;br/&gt;
-        .last(key)&lt;br/&gt;
-        .asScala&lt;br/&gt;
-&lt;br/&gt;
-      tasks.foreach { t =&amp;gt;
-        kvstore.delete(t.getClass(), t.taskId)
-      }&lt;br/&gt;
-&lt;br/&gt;
       // Check whether there are remaining attempts for the same stage. If there aren&apos;t, then&lt;br/&gt;
       // also delete the RDD graph data.&lt;br/&gt;
       val remainingAttempts = kvstore.view(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;StageDataWrapper&amp;#93;&lt;/span&gt;)&lt;br/&gt;
@@ -1000,6 +1106,15 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusListener(&lt;br/&gt;
 &lt;br/&gt;
       cleanupCachedQuantiles(key)&lt;br/&gt;
     }&lt;br/&gt;
+&lt;br/&gt;
+    // Delete tasks for all stages in one pass, as deleting them for each stage individually is slow&lt;br/&gt;
+    val tasks = kvstore.view(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskDataWrapper&amp;#93;&lt;/span&gt;).asScala&lt;br/&gt;
+    val keys = stages.map { s =&amp;gt; (s.info.stageId, s.info.attemptId) }.toSet&lt;br/&gt;
+    tasks.foreach { t =&amp;gt;&lt;br/&gt;
+      if (keys.contains((t.stageId, t.stageAttemptId))) {
+        kvstore.delete(t.getClass(), t.taskId)
+      }&lt;br/&gt;
+    }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   private def cleanupTasks(stage: LiveStage): Unit = {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/AppStatusSource.scala b/core/src/main/scala/org/apache/spark/status/AppStatusSource.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 0000000000000..f6a21578ff499&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/AppStatusSource.scala&lt;br/&gt;
@@ -0,0 +1,78 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
+ * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
+ * this work for additional information regarding copyright ownership.&lt;br/&gt;
+ * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
+ * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
+ * the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+package org.apache.spark.status&lt;br/&gt;
+&lt;br/&gt;
+import java.util.concurrent.atomic.AtomicLong&lt;br/&gt;
+&lt;br/&gt;
+import AppStatusSource.getCounter&lt;br/&gt;
+import com.codahale.metrics.{Counter, Gauge, MetricRegistry}&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.spark.SparkConf&lt;br/&gt;
+import org.apache.spark.internal.config.Status.APP_STATUS_METRICS_ENABLED&lt;br/&gt;
+import org.apache.spark.metrics.source.Source&lt;br/&gt;
+&lt;br/&gt;
+private &lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class JobDuration(val value: AtomicLong) extends Gauge&lt;span class=&quot;error&quot;&gt;&amp;#91;Long&amp;#93;&lt;/span&gt; {
+  override def getValue: Long = value.get()
+}&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusSource extends Source {
+
+  override implicit val metricRegistry = new MetricRegistry()
+
+  override val sourceName = &quot;appStatus&quot;
+
+  val jobDuration = new JobDuration(new AtomicLong(0L))
+
+  // Duration of each job in milliseconds
+  val JOB_DURATION = metricRegistry
+    .register(MetricRegistry.name(&quot;jobDuration&quot;), jobDuration)
+
+  val FAILED_STAGES = getCounter(&quot;stages&quot;, &quot;failedStages&quot;)
+
+  val SKIPPED_STAGES = getCounter(&quot;stages&quot;, &quot;skippedStages&quot;)
+
+  val COMPLETED_STAGES = getCounter(&quot;stages&quot;, &quot;completedStages&quot;)
+
+  val SUCCEEDED_JOBS = getCounter(&quot;jobs&quot;, &quot;succeededJobs&quot;)
+
+  val FAILED_JOBS = getCounter(&quot;jobs&quot;, &quot;failedJobs&quot;)
+
+  val COMPLETED_TASKS = getCounter(&quot;tasks&quot;, &quot;completedTasks&quot;)
+
+  val FAILED_TASKS = getCounter(&quot;tasks&quot;, &quot;failedTasks&quot;)
+
+  val KILLED_TASKS = getCounter(&quot;tasks&quot;, &quot;killedTasks&quot;)
+
+  val SKIPPED_TASKS = getCounter(&quot;tasks&quot;, &quot;skippedTasks&quot;)
+
+  val BLACKLISTED_EXECUTORS = getCounter(&quot;tasks&quot;, &quot;blackListedExecutors&quot;)
+
+  val UNBLACKLISTED_EXECUTORS = getCounter(&quot;tasks&quot;, &quot;unblackListedExecutors&quot;)
+}&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object AppStatusSource {&lt;br/&gt;
+&lt;br/&gt;
+  def getCounter(prefix: String, name: String)(implicit metricRegistry: MetricRegistry): Counter = {
+    metricRegistry.counter(MetricRegistry.name(prefix, name))
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def createSource(conf: SparkConf): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;AppStatusSource&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    Option(conf.get(APP_STATUS_METRICS_ENABLED))&lt;br/&gt;
+      .filter(identity)&lt;br/&gt;
+      .map { _ =&amp;gt; new AppStatusSource() }&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala b/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala&lt;br/&gt;
index e237281c552b1..5c0ed4d5d8f4c 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/AppStatusStore.scala&lt;br/&gt;
@@ -112,10 +112,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusStore(&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  def stageAttempt(stageId: Int, stageAttemptId: Int, details: Boolean = false): v1.StageData = {&lt;br/&gt;
+  def stageAttempt(stageId: Int, stageAttemptId: Int,&lt;br/&gt;
+      details: Boolean = false): (v1.StageData, Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;) = {
     val stageKey = Array(stageId, stageAttemptId)
-    val stage = store.read(classOf[StageDataWrapper], stageKey).info
-    if (details) stageWithDetails(stage) else stage
+    val stageDataWrapper = store.read(classOf[StageDataWrapper], stageKey)
+    val stage = if (details) stageWithDetails(stageDataWrapper.info) else stageDataWrapper.info
+    (stage, stageDataWrapper.jobIds.toSeq)
   }&lt;br/&gt;
 &lt;br/&gt;
   def taskCount(stageId: Int, stageAttemptId: Int): Long = {&lt;br/&gt;
@@ -349,7 +351,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusStore(&lt;br/&gt;
   def taskList(stageId: Int, stageAttemptId: Int, maxTasks: Int): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;v1.TaskData&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     val stageKey = Array(stageId, stageAttemptId)&lt;br/&gt;
     store.view(classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskDataWrapper&amp;#93;&lt;/span&gt;).index(&quot;stage&quot;).first(stageKey).last(stageKey).reverse()&lt;br/&gt;
-      .max(maxTasks).asScala.map(_.toApi).toSeq.reverse&lt;br/&gt;
+      .max(maxTasks).asScala.map { taskDataWrapper =&amp;gt;
+      constructTaskData(taskDataWrapper)
+    }.toSeq.reverse&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   def taskList(&lt;br/&gt;
@@ -388,7 +392,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class AppStatusStore(&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     val ordered = if (ascending) indexed else indexed.reverse()&lt;br/&gt;
-    ordered.skip(offset).max(length).asScala.map(_.toApi).toSeq&lt;br/&gt;
+    ordered.skip(offset).max(length).asScala.map { taskDataWrapper =&amp;gt;+      constructTaskData(taskDataWrapper)+    }.toSeq&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   def executorSummary(stageId: Int, attemptId: Int): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, v1.ExecutorStageSummary&amp;#93;&lt;/span&gt; = {
@@ -494,6 +500,24 @@ private[spark] class AppStatusStore(
     store.close()
   }&lt;br/&gt;
 &lt;br/&gt;
+  def constructTaskData(taskDataWrapper: TaskDataWrapper) : v1.TaskData = {&lt;br/&gt;
+    val taskDataOld: v1.TaskData = taskDataWrapper.toApi&lt;br/&gt;
+    val executorLogs: Option[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;] = try {
+      Some(executorSummary(taskDataOld.executorId).executorLogs)
+    } catch {
+      case e: NoSuchElementException =&amp;gt; e.getMessage
+        None
+    }&lt;br/&gt;
+    new v1.TaskData(taskDataOld.taskId, taskDataOld.index,&lt;br/&gt;
+      taskDataOld.attempt, taskDataOld.launchTime, taskDataOld.resultFetchStart,&lt;br/&gt;
+      taskDataOld.duration, taskDataOld.executorId, taskDataOld.host, taskDataOld.status,&lt;br/&gt;
+      taskDataOld.taskLocality, taskDataOld.speculative, taskDataOld.accumulatorUpdates,&lt;br/&gt;
+      taskDataOld.errorMessage, taskDataOld.taskMetrics,&lt;br/&gt;
+      executorLogs.getOrElse(Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;()),&lt;br/&gt;
+      AppStatusUtils.schedulerDelay(taskDataOld),&lt;br/&gt;
+      AppStatusUtils.gettingResultTime(taskDataOld))&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
 }&lt;br/&gt;
 &lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object AppStatusStore {&lt;br/&gt;
@@ -503,10 +527,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object AppStatusStore {&lt;br/&gt;
   /**&lt;br/&gt;
    * Create an in-memory store for a live application.&lt;br/&gt;
    */&lt;br/&gt;
-  def createLiveStore(conf: SparkConf): AppStatusStore = {&lt;br/&gt;
+  def createLiveStore(&lt;br/&gt;
+      conf: SparkConf,&lt;br/&gt;
+      appStatusSource: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;AppStatusSource&amp;#93;&lt;/span&gt; = None): AppStatusStore = {
     val store = new ElementTrackingStore(new InMemoryStore(), conf)
-    val listener = new AppStatusListener(store, conf, true)
+    val listener = new AppStatusListener(store, conf, true, appStatusSource)
     new AppStatusStore(store, listener = Some(listener))
   }&lt;br/&gt;
-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala b/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala&lt;br/&gt;
index 863b0967f765e..5ec7d90bfaaba 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/ElementTrackingStore.scala&lt;br/&gt;
@@ -24,6 +24,7 @@ import scala.collection.mutable.{HashMap, ListBuffer}&lt;br/&gt;
 import com.google.common.util.concurrent.MoreExecutors&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.SparkConf&lt;br/&gt;
+import org.apache.spark.internal.config.Status._&lt;br/&gt;
 import org.apache.spark.util.{ThreadUtils, Utils}
&lt;p&gt; import org.apache.spark.util.kvstore._&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -45,8 +46,6 @@ import org.apache.spark.util.kvstore._&lt;br/&gt;
  */&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class ElementTrackingStore(store: KVStore, conf: SparkConf) extends KVStore {&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;import config._&lt;br/&gt;
-&lt;br/&gt;
   private val triggers = new HashMap[Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;, Seq[Trigger&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;]]()&lt;br/&gt;
   private val flushTriggers = new ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;() =&amp;gt; Unit&amp;#93;&lt;/span&gt;()&lt;br/&gt;
   private val executor = if (conf.get(ASYNC_TRACKING_ENABLED)) {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/KVUtils.scala b/core/src/main/scala/org/apache/spark/status/KVUtils.scala&lt;br/&gt;
index 99b1843d8e1c0..45348be5c98b9 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/status/KVUtils.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/KVUtils.scala&lt;br/&gt;
@@ -42,7 +42,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object KVUtils extends Logging {&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class KVStoreScalaSerializer extends KVStoreSerializer 
{
 
     mapper.registerModule(DefaultScalaModule)
-    mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL)
+    mapper.setSerializationInclusion(JsonInclude.Include.NON_ABSENT)
 
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/core/src/main/scala/org/apache/spark/status/LiveEntity.scala b/core/src/main/scala/org/apache/spark/status/LiveEntity.scala&lt;br/&gt;
index 79e3f13b826ce..47e45a66ecccb 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/status/LiveEntity.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/LiveEntity.scala&lt;br/&gt;
@@ -26,14 +26,13 @@ import scala.collection.mutable.HashMap&lt;br/&gt;
 import com.google.common.collect.Interners&lt;/p&gt;

&lt;p&gt; import org.apache.spark.JobExecutionStatus&lt;br/&gt;
-import org.apache.spark.executor.TaskMetrics&lt;br/&gt;
+import org.apache.spark.executor.&lt;/p&gt;
{ExecutorMetrics, TaskMetrics}
&lt;p&gt; import org.apache.spark.scheduler.&lt;/p&gt;
{AccumulableInfo, StageInfo, TaskInfo}
&lt;p&gt; import org.apache.spark.status.api.v1&lt;br/&gt;
 import org.apache.spark.storage.RDDInfo&lt;br/&gt;
 import org.apache.spark.ui.SparkUI&lt;br/&gt;
 import org.apache.spark.util.AccumulatorContext&lt;br/&gt;
 import org.apache.spark.util.collection.OpenHashSet&lt;br/&gt;
-import org.apache.spark.util.kvstore.KVStore&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A mutable representation of a live entity in Spark (jobs, stages, tasks, et al). Every live&lt;br/&gt;
@@ -62,7 +61,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class LiveEntity {&lt;br/&gt;
 private class LiveJob(&lt;br/&gt;
     val jobId: Int,&lt;br/&gt;
     name: String,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;submissionTime: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Date&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    val submissionTime: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Date&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     val stageIds: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     jobGroup: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     numTasks: Int) extends LiveEntity {&lt;br/&gt;
@@ -268,6 +267,9 @@ private class LiveExecutor(val executorId: String, _addTime: Long) extends LiveE&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def hasMemoryInfo: Boolean = totalOnHeap &amp;gt;= 0L&lt;/p&gt;

&lt;p&gt;+  // peak values for executor level metrics&lt;br/&gt;
+  val peakExecutorMetrics = new ExecutorMetrics()&lt;br/&gt;
+&lt;br/&gt;
   def hostname: String = if (host != null) host else hostPort.split(&quot;:&quot;)(0)&lt;/p&gt;

&lt;p&gt;   override protected def doUpdate(): Any = &lt;/p&gt;
{
@@ -302,10 +304,10 @@ private class LiveExecutor(val executorId: String, _addTime: Long) extends LiveE
       Option(removeReason),
       executorLogs,
       memoryMetrics,
-      blacklistedInStages)
+      blacklistedInStages,
+      Some(peakExecutorMetrics).filter(_.isSet))
     new ExecutorSummaryWrapper(info)
   }
&lt;p&gt;-&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt; private class LiveExecutorStageSummary(&lt;br/&gt;
@@ -374,6 +376,8 @@ private class LiveStage extends LiveEntity {&lt;/p&gt;

&lt;p&gt;   val executorSummaries = new HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, LiveExecutorStageSummary&amp;#93;&lt;/span&gt;()&lt;/p&gt;

&lt;p&gt;+  val activeTasksPerExecutor = new HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Int&amp;#93;&lt;/span&gt;().withDefaultValue(0)&lt;br/&gt;
+&lt;br/&gt;
   var blackListedExecutors = new HashSet&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;()&lt;/p&gt;

&lt;p&gt;   // Used for cleanup of tasks after they reach the configured limit. Not written to the store.&lt;br/&gt;
@@ -538,6 +542,10 @@ private class LiveRDD(val info: RDDInfo) extends LiveEntity &lt;/p&gt;
{
     distributions.get(exec.executorId)
   }

&lt;p&gt;+  def getPartitions(): scala.collection.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, LiveRDDPartition&amp;#93;&lt;/span&gt; = partitions&lt;br/&gt;
+&lt;br/&gt;
+  def getDistributions(): scala.collection.Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, LiveRDDDistribution&amp;#93;&lt;/span&gt; = distributions&lt;br/&gt;
+&lt;br/&gt;
   override protected def doUpdate(): Any = {&lt;br/&gt;
     val dists = if (distributions.nonEmpty) {&lt;br/&gt;
       Some(distributions.values.map(_.toApi()).toSeq)&lt;br/&gt;
@@ -581,8 +589,7 @@ private object LiveEntityHelpers {&lt;br/&gt;
       .filter &lt;/p&gt;
{ acc =&amp;gt;
         // We don&apos;t need to store internal or SQL accumulables as their values will be shown in
         // other places, so drop them to reduce the memory usage.
-        !acc.internal &amp;amp;&amp;amp; (!acc.metadata.isDefined ||
-          acc.metadata.get != Some(AccumulatorContext.SQL_ACCUM_IDENTIFIER))
+        !acc.internal &amp;amp;&amp;amp; acc.metadata != Some(AccumulatorContext.SQL_ACCUM_IDENTIFIER)
       }
&lt;p&gt;       .map { acc =&amp;gt;&lt;br/&gt;
         new v1.AccumulableInfo(&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala b/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala&lt;br/&gt;
index 4560d300cb0c8..50a286d0d3b0f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/api/v1/JacksonMessageWriter.scala&lt;br/&gt;
@@ -49,7 +49,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;v1&amp;#93;&lt;/span&gt; class JacksonMessageWriter extends MessageBodyWriter&lt;span class=&quot;error&quot;&gt;&amp;#91;Object&amp;#93;&lt;/span&gt;{&lt;br/&gt;
   }&lt;br/&gt;
   mapper.registerModule(com.fasterxml.jackson.module.scala.DefaultScalaModule)&lt;br/&gt;
   mapper.enable(SerializationFeature.INDENT_OUTPUT)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;mapper.setSerializationInclusion(JsonInclude.Include.NON_NULL)&lt;br/&gt;
+  mapper.setSerializationInclusion(JsonInclude.Include.NON_ABSENT)&lt;br/&gt;
   mapper.setDateFormat(JacksonMessageWriter.makeISODateFormat)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   override def isWriteable(&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala b/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala&lt;br/&gt;
index 32100c5704538..1f4082cac8f75 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/api/v1/OneApplicationResource.scala&lt;br/&gt;
@@ -175,7 +175,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;v1&amp;#93;&lt;/span&gt; class OneApplicationAttemptResource extends AbstractApplicationResou&lt;br/&gt;
   def getAttempt(): ApplicationAttemptInfo = {&lt;br/&gt;
     uiRoot.getApplicationInfo(appId)&lt;br/&gt;
       .flatMap &lt;/p&gt;
{ app =&amp;gt;
-        app.attempts.filter(_.attemptId == attemptId).headOption
+        app.attempts.find(_.attemptId.contains(attemptId))
       }
&lt;p&gt;       .getOrElse {&lt;br/&gt;
         throw new NotFoundException(s&quot;unknown app $appId, attempt $attemptId&quot;)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala b/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala&lt;br/&gt;
index 96249e4bfd5fa..f81892734c2de 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/api/v1/StagesResource.scala&lt;br/&gt;
@@ -16,15 +16,16 @@&lt;br/&gt;
  */&lt;br/&gt;
 package org.apache.spark.status.api.v1&lt;/p&gt;

&lt;p&gt;-import java.util.&lt;/p&gt;
{List =&amp;gt; JList}
&lt;p&gt;+import java.util.&lt;/p&gt;
{HashMap, List =&amp;gt; JList, Locale}
&lt;p&gt; import javax.ws.rs._&lt;br/&gt;
-import javax.ws.rs.core.MediaType&lt;br/&gt;
+import javax.ws.rs.core.&lt;/p&gt;
{Context, MediaType, MultivaluedMap, UriInfo}

&lt;p&gt; import org.apache.spark.SparkException&lt;br/&gt;
 import org.apache.spark.scheduler.StageInfo&lt;br/&gt;
 import org.apache.spark.status.api.v1.StageStatus._&lt;br/&gt;
 import org.apache.spark.status.api.v1.TaskSorting._&lt;br/&gt;
 import org.apache.spark.ui.SparkUI&lt;br/&gt;
+import org.apache.spark.ui.jobs.ApiHelper._&lt;/p&gt;

&lt;p&gt; @Produces(Array(MediaType.APPLICATION_JSON))&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;v1&amp;#93;&lt;/span&gt; class StagesResource extends BaseAppResource {&lt;br/&gt;
@@ -56,7 +57,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;v1&amp;#93;&lt;/span&gt; class StagesResource extends BaseAppResource {&lt;br/&gt;
       @PathParam(&quot;stageAttemptId&quot;) stageAttemptId: Int,&lt;br/&gt;
       @QueryParam(&quot;details&quot;) @DefaultValue(&quot;true&quot;) details: Boolean): StageData = withUI { ui =&amp;gt;&lt;br/&gt;
     try &lt;/p&gt;
{
-      ui.store.stageAttempt(stageId, stageAttemptId, details = details)
+      ui.store.stageAttempt(stageId, stageAttemptId, details = details)._1
     }
&lt;p&gt; catch {&lt;br/&gt;
       case _: NoSuchElementException =&amp;gt;&lt;br/&gt;
         // Change the message depending on whether there are any attempts for the requested stage.&lt;br/&gt;
@@ -102,4 +103,120 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;v1&amp;#93;&lt;/span&gt; class StagesResource extends BaseAppResource &lt;/p&gt;
{
     withUI(_.store.taskList(stageId, stageAttemptId, offset, length, sortBy))
   }

&lt;p&gt;+  // This api needs to stay formatted exactly as it is below, since, it is being used by the&lt;br/&gt;
+  // datatables for the stages page.&lt;br/&gt;
+  @GET&lt;br/&gt;
+  @Path(&quot;&lt;/p&gt;
{stageId: \\d+}
&lt;p&gt;/&lt;/p&gt;
{stageAttemptId: \\d+}
&lt;p&gt;/taskTable&quot;)&lt;br/&gt;
+  def taskTable(&lt;br/&gt;
+    @PathParam(&quot;stageId&quot;) stageId: Int,&lt;br/&gt;
+    @PathParam(&quot;stageAttemptId&quot;) stageAttemptId: Int,&lt;br/&gt;
+    @QueryParam(&quot;details&quot;) @DefaultValue(&quot;true&quot;) details: Boolean,&lt;br/&gt;
+    @Context uriInfo: UriInfo):&lt;br/&gt;
+  HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Object&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    withUI { ui =&amp;gt;&lt;br/&gt;
+      val uriQueryParameters = uriInfo.getQueryParameters(true)&lt;br/&gt;
+      val totalRecords = uriQueryParameters.getFirst(&quot;numTasks&quot;)&lt;br/&gt;
+      var isSearch = false&lt;br/&gt;
+      var searchValue: String = null&lt;br/&gt;
+      var filteredRecords = totalRecords&lt;br/&gt;
+      // The datatables client API sends a list of query parameters to the server which contain&lt;br/&gt;
+      // information like the columns to be sorted, search value typed by the user in the search&lt;br/&gt;
+      // box, pagination index etc. For more information on these query parameters,&lt;br/&gt;
+      // refer &lt;a href=&quot;https://datatables.net/manual/server-side&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://datatables.net/manual/server-side&lt;/a&gt;.&lt;br/&gt;
+      if (uriQueryParameters.getFirst(&quot;search&lt;span class=&quot;error&quot;&gt;&amp;#91;value&amp;#93;&lt;/span&gt;&quot;) != null &amp;amp;&amp;amp;&lt;br/&gt;
+        uriQueryParameters.getFirst(&quot;search&lt;span class=&quot;error&quot;&gt;&amp;#91;value&amp;#93;&lt;/span&gt;&quot;).length &amp;gt; 0) &lt;/p&gt;
{
+        isSearch = true
+        searchValue = uriQueryParameters.getFirst(&quot;search[value]&quot;)
+      }
&lt;p&gt;+      val _tasksToShow: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskData&amp;#93;&lt;/span&gt; = doPagination(uriQueryParameters, stageId, stageAttemptId,&lt;br/&gt;
+        isSearch, totalRecords.toInt)&lt;br/&gt;
+      val ret = new HashMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Object&amp;#93;&lt;/span&gt;()&lt;br/&gt;
+      if (_tasksToShow.nonEmpty) {&lt;br/&gt;
+        // Performs server-side search based on input from user&lt;br/&gt;
+        if (isSearch) {&lt;br/&gt;
+          val filteredTaskList = filterTaskList(_tasksToShow, searchValue)&lt;br/&gt;
+          filteredRecords = filteredTaskList.length.toString&lt;br/&gt;
+          if (filteredTaskList.length &amp;gt; 0) &lt;/p&gt;
{
+            val pageStartIndex = uriQueryParameters.getFirst(&quot;start&quot;).toInt
+            val pageLength = uriQueryParameters.getFirst(&quot;length&quot;).toInt
+            ret.put(&quot;aaData&quot;, filteredTaskList.slice(
+              pageStartIndex, pageStartIndex + pageLength))
+          }
&lt;p&gt; else &lt;/p&gt;
{
+            ret.put(&quot;aaData&quot;, filteredTaskList)
+          }
&lt;p&gt;+        } else &lt;/p&gt;
{
+          ret.put(&quot;aaData&quot;, _tasksToShow)
+        }
&lt;p&gt;+      } else &lt;/p&gt;
{
+        ret.put(&quot;aaData&quot;, _tasksToShow)
+      }
&lt;p&gt;+      ret.put(&quot;recordsTotal&quot;, totalRecords)&lt;br/&gt;
+      ret.put(&quot;recordsFiltered&quot;, filteredRecords)&lt;br/&gt;
+      ret&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  // Performs pagination on the server side&lt;br/&gt;
+  def doPagination(queryParameters: MultivaluedMap&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;, stageId: Int,&lt;br/&gt;
+    stageAttemptId: Int, isSearch: Boolean, totalRecords: Int): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskData&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    var columnNameToSort = queryParameters.getFirst(&quot;columnNameToSort&quot;)&lt;br/&gt;
+    // Sorting on Logs column will default to Index column sort&lt;br/&gt;
+    if (columnNameToSort.equalsIgnoreCase(&quot;Logs&quot;)) &lt;/p&gt;
{
+      columnNameToSort = &quot;Index&quot;
+    }
&lt;p&gt;+    val isAscendingStr = queryParameters.getFirst(&quot;order&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;dir&amp;#93;&lt;/span&gt;&quot;)&lt;br/&gt;
+    var pageStartIndex = 0&lt;br/&gt;
+    var pageLength = totalRecords&lt;br/&gt;
+    // We fetch only the desired rows upto the specified page length for all cases except when a&lt;br/&gt;
+    // search query is present, in that case, we need to fetch all the rows to perform the search&lt;br/&gt;
+    // on the entire table&lt;br/&gt;
+    if (!isSearch) &lt;/p&gt;
{
+      pageStartIndex = queryParameters.getFirst(&quot;start&quot;).toInt
+      pageLength = queryParameters.getFirst(&quot;length&quot;).toInt
+    }
&lt;p&gt;+    withUI(_.store.taskList(stageId, stageAttemptId, pageStartIndex, pageLength,&lt;br/&gt;
+      indexName(columnNameToSort), isAscendingStr.equalsIgnoreCase(&quot;asc&quot;)))&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  // Filters task list based on search parameter&lt;br/&gt;
+  def filterTaskList(&lt;br/&gt;
+    taskDataList: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskData&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    searchValue: String): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskData&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    val defaultOptionString: String = &quot;d&quot;&lt;br/&gt;
+    val searchValueLowerCase = searchValue.toLowerCase(Locale.ROOT)&lt;br/&gt;
+    val containsValue = (taskDataParams: Any) =&amp;gt; taskDataParams.toString.toLowerCase(&lt;br/&gt;
+      Locale.ROOT).contains(searchValueLowerCase)&lt;br/&gt;
+    val taskMetricsContainsValue = (task: TaskData) =&amp;gt; task.taskMetrics match &lt;/p&gt;
{
+      case None =&amp;gt; false
+      case Some(metrics) =&amp;gt;
+        (containsValue(task.taskMetrics.get.executorDeserializeTime)
+        || containsValue(task.taskMetrics.get.executorRunTime)
+        || containsValue(task.taskMetrics.get.jvmGcTime)
+        || containsValue(task.taskMetrics.get.resultSerializationTime)
+        || containsValue(task.taskMetrics.get.memoryBytesSpilled)
+        || containsValue(task.taskMetrics.get.diskBytesSpilled)
+        || containsValue(task.taskMetrics.get.peakExecutionMemory)
+        || containsValue(task.taskMetrics.get.inputMetrics.bytesRead)
+        || containsValue(task.taskMetrics.get.inputMetrics.recordsRead)
+        || containsValue(task.taskMetrics.get.outputMetrics.bytesWritten)
+        || containsValue(task.taskMetrics.get.outputMetrics.recordsWritten)
+        || containsValue(task.taskMetrics.get.shuffleReadMetrics.fetchWaitTime)
+        || containsValue(task.taskMetrics.get.shuffleReadMetrics.recordsRead)
+        || containsValue(task.taskMetrics.get.shuffleWriteMetrics.bytesWritten)
+        || containsValue(task.taskMetrics.get.shuffleWriteMetrics.recordsWritten)
+        || containsValue(task.taskMetrics.get.shuffleWriteMetrics.writeTime))
+    }
&lt;p&gt;+    val filteredTaskDataSequence: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskData&amp;#93;&lt;/span&gt; = taskDataList.filter(f =&amp;gt;&lt;br/&gt;
+      (containsValue(f.taskId) || containsValue(f.index) || containsValue(f.attempt)&lt;br/&gt;
+        || containsValue(f.launchTime)&lt;br/&gt;
+        || containsValue(f.resultFetchStart.getOrElse(defaultOptionString))&lt;br/&gt;
+        || containsValue(f.duration.getOrElse(defaultOptionString))&lt;br/&gt;
+        || containsValue(f.executorId) || containsValue(f.host) || containsValue(f.status)&lt;br/&gt;
+        || containsValue(f.taskLocality) || containsValue(f.speculative)&lt;br/&gt;
+        || containsValue(f.errorMessage.getOrElse(defaultOptionString))&lt;br/&gt;
+        || taskMetricsContainsValue(f)&lt;br/&gt;
+        || containsValue(f.schedulerDelay) || containsValue(f.gettingResultTime)))&lt;br/&gt;
+    filteredTaskDataSequence&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/api/v1/api.scala b/core/src/main/scala/org/apache/spark/status/api/v1/api.scala&lt;br/&gt;
index 971d7e90fa7b8..aa21da2b66ab2 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/status/api/v1/api.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/api/v1/api.scala&lt;br/&gt;
@@ -22,9 +22,14 @@ import java.util.Date&lt;br/&gt;
 import scala.xml.&lt;/p&gt;
{NodeSeq, Text}

&lt;p&gt; import com.fasterxml.jackson.annotation.JsonIgnoreProperties&lt;br/&gt;
-import com.fasterxml.jackson.databind.annotation.JsonDeserialize&lt;br/&gt;
+import com.fasterxml.jackson.core.&lt;/p&gt;
{JsonGenerator, JsonParser}
&lt;p&gt;+import com.fasterxml.jackson.core.`type`.TypeReference&lt;br/&gt;
+import com.fasterxml.jackson.databind.&lt;/p&gt;
{DeserializationContext, JsonDeserializer, JsonSerializer, SerializerProvider}
&lt;p&gt;+import com.fasterxml.jackson.databind.annotation.&lt;/p&gt;
{JsonDeserialize, JsonSerialize}

&lt;p&gt; import org.apache.spark.JobExecutionStatus&lt;br/&gt;
+import org.apache.spark.executor.ExecutorMetrics&lt;br/&gt;
+import org.apache.spark.metrics.ExecutorMetricType&lt;/p&gt;

&lt;p&gt; case class ApplicationInfo private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     id: String,&lt;br/&gt;
@@ -98,7 +103,10 @@ class ExecutorSummary private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     val removeReason: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     val executorLogs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     val memoryMetrics: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;MemoryMetrics&amp;#93;&lt;/span&gt;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val blacklistedInStages: Set&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val blacklistedInStages: Set&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    @JsonSerialize(using = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorMetricsJsonSerializer&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    @JsonDeserialize(using = classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorMetricsJsonDeserializer&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+    val peakMemoryMetrics: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorMetrics&amp;#93;&lt;/span&gt;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; class MemoryMetrics private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     val usedOnHeapStorageMemory: Long,&lt;br/&gt;
@@ -106,6 +114,36 @@ class MemoryMetrics private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     val totalOnHeapStorageMemory: Long,&lt;br/&gt;
     val totalOffHeapStorageMemory: Long)&lt;/p&gt;

&lt;p&gt;+/** deserializer for peakMemoryMetrics: convert map to ExecutorMetrics */&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class ExecutorMetricsJsonDeserializer&lt;br/&gt;
+    extends JsonDeserializer[Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorMetrics&amp;#93;&lt;/span&gt;] {&lt;br/&gt;
+  override def deserialize(&lt;br/&gt;
+      jsonParser: JsonParser,&lt;br/&gt;
+      deserializationContext: DeserializationContext): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorMetrics&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    val metricsMap = jsonParser.readValueAs[Option[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, Long&amp;#93;&lt;/span&gt;]](&lt;br/&gt;
+      new TypeReference[Option[Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, java.lang.Long&amp;#93;&lt;/span&gt;]] {})&lt;br/&gt;
+    metricsMap.map(metrics =&amp;gt; new ExecutorMetrics(metrics))&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
+/** serializer for peakMemoryMetrics: convert ExecutorMetrics to map with metric name as key */&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class ExecutorMetricsJsonSerializer&lt;br/&gt;
+    extends JsonSerializer[Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorMetrics&amp;#93;&lt;/span&gt;] {&lt;br/&gt;
+  override def serialize(&lt;br/&gt;
+      metrics: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorMetrics&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+      jsonGenerator: JsonGenerator,&lt;br/&gt;
+      serializerProvider: SerializerProvider): Unit = {&lt;br/&gt;
+    metrics.foreach { m: ExecutorMetrics =&amp;gt;&lt;br/&gt;
+      val metricsMap = ExecutorMetricType.values.map &lt;/p&gt;
{ metricType =&amp;gt;
+            metricType.name -&amp;gt; m.getMetricValue(metricType)
+      }
&lt;p&gt;.toMap&lt;br/&gt;
+      jsonGenerator.writeObject(metricsMap)&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  override def isEmpty(provider: SerializerProvider, value: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ExecutorMetrics&amp;#93;&lt;/span&gt;): Boolean =&lt;br/&gt;
+    value.isEmpty&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
 class JobData private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     val jobId: Int,&lt;br/&gt;
     val name: String,&lt;br/&gt;
@@ -215,7 +253,10 @@ class TaskData private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     val speculative: Boolean,&lt;br/&gt;
     val accumulatorUpdates: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;AccumulableInfo&amp;#93;&lt;/span&gt;,&lt;br/&gt;
     val errorMessage: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val taskMetrics: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskMetrics&amp;#93;&lt;/span&gt; = None)&lt;br/&gt;
+    val taskMetrics: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskMetrics&amp;#93;&lt;/span&gt; = None,&lt;br/&gt;
+    val executorLogs: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    val schedulerDelay: Long,&lt;br/&gt;
+    val gettingResultTime: Long)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; class TaskMetrics private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     val executorDeserializeTime: Long,&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/status/storeTypes.scala b/core/src/main/scala/org/apache/spark/status/storeTypes.scala&lt;br/&gt;
index 646cf25880e37..ef19e86f3135f 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/status/storeTypes.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/status/storeTypes.scala&lt;br/&gt;
@@ -283,7 +283,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class TaskDataWrapper(&lt;br/&gt;
       speculative,&lt;br/&gt;
       accumulatorUpdates,&lt;br/&gt;
       errorMessage,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;metrics)&lt;br/&gt;
+      metrics,&lt;br/&gt;
+      executorLogs = null,&lt;br/&gt;
+      schedulerDelay = 0L,&lt;br/&gt;
+      gettingResultTime = 0L)&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   @JsonIgnore @KVIndex(TaskIndexNames.STAGE)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/storage/BlockManager.scala b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala&lt;br/&gt;
index f5c69ad241e3a..1dfbc6effb346 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/storage/BlockManager.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/storage/BlockManager.scala&lt;br/&gt;
@@ -35,7 +35,7 @@ import scala.util.control.NonFatal&lt;br/&gt;
 import com.codahale.metrics.&lt;/p&gt;
{MetricRegistry, MetricSet}

&lt;p&gt; import org.apache.spark._&lt;br/&gt;
-import org.apache.spark.executor.&lt;/p&gt;
{DataReadMethod, ShuffleWriteMetrics}
&lt;p&gt;+import org.apache.spark.executor.DataReadMethod&lt;br/&gt;
 import org.apache.spark.internal.&lt;/p&gt;
{config, Logging}&lt;br/&gt;
 import org.apache.spark.memory.{MemoryManager, MemoryMode}&lt;br/&gt;
 import org.apache.spark.metrics.source.Source&lt;br/&gt;
@@ -43,12 +43,13 @@ import org.apache.spark.network._&lt;br/&gt;
 import org.apache.spark.network.buffer.ManagedBuffer&lt;br/&gt;
 import org.apache.spark.network.client.StreamCallbackWithID&lt;br/&gt;
 import org.apache.spark.network.netty.SparkTransportConf&lt;br/&gt;
-import org.apache.spark.network.shuffle.{ExternalShuffleClient, TempFileManager}&lt;br/&gt;
+import org.apache.spark.network.shuffle._&lt;br/&gt;
 import org.apache.spark.network.shuffle.protocol.ExecutorShuffleInfo&lt;br/&gt;
+import org.apache.spark.network.util.TransportConf&lt;br/&gt;
 import org.apache.spark.rpc.RpcEnv&lt;br/&gt;
 import org.apache.spark.scheduler.ExecutorCacheTaskLocation&lt;br/&gt;
 import org.apache.spark.serializer.{SerializerInstance, SerializerManager}&lt;br/&gt;
-import org.apache.spark.shuffle.ShuffleManager&lt;br/&gt;
+import org.apache.spark.shuffle.{ShuffleManager, ShuffleWriteMetricsReporter}&lt;br/&gt;
 import org.apache.spark.storage.memory._&lt;br/&gt;
 import org.apache.spark.unsafe.Platform&lt;br/&gt;
 import org.apache.spark.util._&lt;br/&gt;
@@ -131,8 +132,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class BlockManager(&lt;br/&gt;
 &lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; val externalShuffleServiceEnabled =&lt;br/&gt;
     conf.get(config.SHUFFLE_SERVICE_ENABLED)&lt;br/&gt;
-  private val chunkSize =&lt;br/&gt;
-    conf.getSizeAsBytes(&quot;spark.storage.memoryMapLimitForTests&quot;, Int.MaxValue.toString).toInt&lt;br/&gt;
   private val remoteReadNioBufferConversion =&lt;br/&gt;
     conf.getBoolean(&quot;spark.network.remoteReadNioBufferConversion&quot;, false)&lt;br/&gt;
 &lt;br/&gt;
@@ -213,11 +212,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class BlockManager(&lt;br/&gt;
 &lt;br/&gt;
   private var blockReplicationPolicy: BlockReplicationPolicy = _&lt;br/&gt;
 &lt;br/&gt;
-  // A TempFileManager used to track all the files of remote blocks which above the&lt;br/&gt;
+  // A DownloadFileManager used to track all the files of remote blocks which are above the&lt;br/&gt;
   // specified memory threshold. Files will be deleted automatically based on weak reference.&lt;br/&gt;
   // Exposed for test&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;storage&amp;#93;&lt;/span&gt; val remoteBlockTempFileManager =&lt;br/&gt;
-    new BlockManager.RemoteBlockTempFileManager(this)&lt;br/&gt;
+    new BlockManager.RemoteBlockDownloadFileManager(this)&lt;br/&gt;
   private val maxRemoteBlockToMem = conf.get(config.MAX_REMOTE_BLOCK_SIZE_FETCH_TO_MEM)&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
@@ -237,7 +236,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class BlockManager(&lt;br/&gt;
       val priorityClass = conf.get(&lt;br/&gt;
         &quot;spark.storage.replication.policy&quot;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;RandomBlockReplicationPolicy&amp;#93;&lt;/span&gt;.getName)&lt;br/&gt;
       val clazz = Utils.classForName(priorityClass)&lt;br/&gt;
-      val ret = clazz.newInstance.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BlockReplicationPolicy&amp;#93;&lt;/span&gt;&lt;br/&gt;
+      val ret = clazz.getConstructor().newInstance().asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BlockReplicationPolicy&amp;#93;&lt;/span&gt;&lt;br/&gt;
       logInfo(s&quot;Using $priorityClass for block replication policy&quot;)&lt;br/&gt;
       ret&lt;br/&gt;
     }&lt;br/&gt;
@@ -436,10 +435,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class BlockManager(&lt;br/&gt;
         // stream.&lt;br/&gt;
         channel.close()&lt;br/&gt;
         // TODO &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25035&quot; title=&quot;Replicating disk-stored blocks should avoid memory mapping&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25035&quot;&gt;&lt;del&gt;SPARK-25035&lt;/del&gt;&lt;/a&gt; Even if we&apos;re only going to write the data to disk after this, we end up&lt;br/&gt;
-        // using a lot of memory here.  With encryption, we&apos;ll read the whole file into a regular&lt;br/&gt;
-        // byte buffer and OOM.  Without encryption, we&apos;ll memory map the file and won&apos;t get a jvm&lt;br/&gt;
-        // OOM, but might get killed by the OS / cluster manager.  We could at least read the tmp&lt;br/&gt;
-        // file as a stream in both cases.&lt;br/&gt;
+        // using a lot of memory here. We&apos;ll read the whole file into a regular&lt;br/&gt;
+        // byte buffer and OOM.  We could at least read the tmp file as a stream.&lt;br/&gt;
         val buffer = securityManager.getIOEncryptionKey() match {
           case Some(key) =&amp;gt;
             // we need to pass in the size of the unencrypted block
@@ -451,7 +448,7 @@ private[spark] class BlockManager(
             new EncryptedBlockData(tmpFile, blockSize, conf, key).toChunkedByteBuffer(allocator)
 
           case None =&amp;gt;
-            ChunkedByteBuffer.map(tmpFile, conf.get(config.MEMORY_MAP_LIMIT_FOR_TESTS).toInt)
+            ChunkedByteBuffer.fromFile(tmpFile)
         }&lt;br/&gt;
         putBytes(blockId, buffer, level)(classTag)&lt;br/&gt;
         tmpFile.delete()&lt;br/&gt;
@@ -695,9 +692,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class BlockManager(&lt;br/&gt;
    */&lt;br/&gt;
   private def getRemoteValues&lt;span class=&quot;error&quot;&gt;&amp;#91;T: ClassTag&amp;#93;&lt;/span&gt;(blockId: BlockId): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;BlockResult&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     val ct = implicitly[ClassTag&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;]&lt;br/&gt;
-    getRemoteBytes(blockId).map { data =&amp;gt;&lt;br/&gt;
+    getRemoteManagedBuffer(blockId).map { data =&amp;gt;
       val values =
-        serializerManager.dataDeserializeStream(blockId, data.toInputStream(dispose = true))(ct)
+        serializerManager.dataDeserializeStream(blockId, data.createInputStream())(ct)
       new BlockResult(values, DataReadMethod.Network, data.size)
     }&lt;br/&gt;
   }&lt;br/&gt;
@@ -720,14 +717,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class BlockManager(&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
-   * Get block from remote block managers as serialized bytes.&lt;br/&gt;
+   * Get block from remote block managers as a ManagedBuffer.&lt;br/&gt;
    */&lt;br/&gt;
-  def getRemoteBytes(blockId: BlockId): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ChunkedByteBuffer&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    // TODO if we change this method to return the ManagedBuffer, then getRemoteValues&lt;br/&gt;
-    // could just use the inputStream on the temp file, rather than memory-mapping the file.&lt;br/&gt;
-    // Until then, replication can cause the process to use too much memory and get killed&lt;br/&gt;
-    // by the OS / cluster manager (not a java OOM, since it&apos;s a memory-mapped file) even though&lt;br/&gt;
-    // we&apos;ve read the data to disk.&lt;br/&gt;
+  private def getRemoteManagedBuffer(blockId: BlockId): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ManagedBuffer&amp;#93;&lt;/span&gt; = {
     logDebug(s&quot;Getting remote block $blockId&quot;)
     require(blockId != null, &quot;BlockId is null&quot;)
     var runningFailureCount = 0
@@ -792,14 +784,13 @@ private[spark] class BlockManager(
       }&lt;br/&gt;
 &lt;br/&gt;
       if (data != null) {&lt;br/&gt;
-        // &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-24307&quot; title=&quot;Support sending messages over 2GB from memory&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-24307&quot;&gt;&lt;del&gt;SPARK-24307&lt;/del&gt;&lt;/a&gt; undocumented &quot;escape-hatch&quot; in case there are any issues in converting to&lt;br/&gt;
-        // ChunkedByteBuffer, to go back to old code-path.  Can be removed post Spark 2.4 if&lt;br/&gt;
-        // new path is stable.&lt;br/&gt;
-        if (remoteReadNioBufferConversion) {
-          return Some(new ChunkedByteBuffer(data.nioByteBuffer()))
-        } else {
-          return Some(ChunkedByteBuffer.fromManagedBuffer(data, chunkSize))
-        }&lt;br/&gt;
+        // If the ManagedBuffer is a BlockManagerManagedBuffer, the disposal of the&lt;br/&gt;
+        // byte buffers backing it may need to be handled after reading the bytes.&lt;br/&gt;
+        // In this case, since we just fetched the bytes remotely, we do not have&lt;br/&gt;
+        // a BlockManagerManagedBuffer. The assert here is to ensure that this holds&lt;br/&gt;
+        // true (or the disposal is handled).&lt;br/&gt;
+        assert(!data.isInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;BlockManagerManagedBuffer&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+        return Some(data)&lt;br/&gt;
       }&lt;br/&gt;
       logDebug(s&quot;The value of block $blockId is null&quot;)&lt;br/&gt;
     }&lt;br/&gt;
@@ -807,6 +798,22 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class BlockManager(&lt;br/&gt;
     None&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
+  /**&lt;br/&gt;
+   * Get block from remote block managers as serialized bytes.&lt;br/&gt;
+   */&lt;br/&gt;
+  def getRemoteBytes(blockId: BlockId): Option&lt;span class=&quot;error&quot;&gt;&amp;#91;ChunkedByteBuffer&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+    getRemoteManagedBuffer(blockId).map { data =&amp;gt;&lt;br/&gt;
+      // &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-24307&quot; title=&quot;Support sending messages over 2GB from memory&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-24307&quot;&gt;&lt;del&gt;SPARK-24307&lt;/del&gt;&lt;/a&gt; undocumented &quot;escape-hatch&quot; in case there are any issues in converting to&lt;br/&gt;
+      // ChunkedByteBuffer, to go back to old code-path.  Can be removed post Spark 2.4 if&lt;br/&gt;
+      // new path is stable.&lt;br/&gt;
+      if (remoteReadNioBufferConversion) {
+        new ChunkedByteBuffer(data.nioByteBuffer())
+      } else {
+        ChunkedByteBuffer.fromManagedBuffer(data)
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;br/&gt;
    * Get a block from the block manager (either local or remote).&lt;br/&gt;
    *&lt;br/&gt;
@@ -935,7 +942,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class BlockManager(&lt;br/&gt;
       file: File,&lt;br/&gt;
       serializerInstance: SerializerInstance,&lt;br/&gt;
       bufferSize: Int,&lt;br/&gt;
-      writeMetrics: ShuffleWriteMetrics): DiskBlockObjectWriter = {&lt;br/&gt;
+      writeMetrics: ShuffleWriteMetricsReporter): DiskBlockObjectWriter = {&lt;br/&gt;
     val syncWrites = conf.getBoolean(&quot;spark.shuffle.sync&quot;, false)&lt;br/&gt;
     new DiskBlockObjectWriter(file, serializerManager, serializerInstance, bufferSize,&lt;br/&gt;
       syncWrites, writeMetrics, blockId)&lt;br/&gt;
@@ -1664,23 +1671,28 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object BlockManager {
     metricRegistry.registerAll(metricSet)
   }&lt;br/&gt;
 &lt;br/&gt;
-  class RemoteBlockTempFileManager(blockManager: BlockManager)&lt;br/&gt;
-      extends TempFileManager with Logging {&lt;br/&gt;
+  class RemoteBlockDownloadFileManager(blockManager: BlockManager)&lt;br/&gt;
+      extends DownloadFileManager with Logging {&lt;br/&gt;
+    // lazy because SparkEnv is set after this&lt;br/&gt;
+    lazy val encryptionKey = SparkEnv.get.securityManager.getIOEncryptionKey()&lt;br/&gt;
 &lt;br/&gt;
-    private class ReferenceWithCleanup(file: File, referenceQueue: JReferenceQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;)&lt;br/&gt;
-        extends WeakReference&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;(file, referenceQueue) {&lt;br/&gt;
-      private val filePath = file.getAbsolutePath&lt;br/&gt;
+    private class ReferenceWithCleanup(&lt;br/&gt;
+        file: DownloadFile,&lt;br/&gt;
+        referenceQueue: JReferenceQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;DownloadFile&amp;#93;&lt;/span&gt;&lt;br/&gt;
+        ) extends WeakReference&lt;span class=&quot;error&quot;&gt;&amp;#91;DownloadFile&amp;#93;&lt;/span&gt;(file, referenceQueue) {&lt;br/&gt;
+&lt;br/&gt;
+      val filePath = file.path()&lt;br/&gt;
 &lt;br/&gt;
       def cleanUp(): Unit = {&lt;br/&gt;
         logDebug(s&quot;Clean up file $filePath&quot;)&lt;br/&gt;
 &lt;br/&gt;
-        if (!new File(filePath).delete()) {&lt;br/&gt;
+        if (!file.delete()) {
           logDebug(s&quot;Fail to delete file $filePath&quot;)
         }&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    private val referenceQueue = new JReferenceQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;File&amp;#93;&lt;/span&gt;&lt;br/&gt;
+    private val referenceQueue = new JReferenceQueue&lt;span class=&quot;error&quot;&gt;&amp;#91;DownloadFile&amp;#93;&lt;/span&gt;&lt;br/&gt;
     private val referenceBuffer = Collections.newSetFromMap&lt;span class=&quot;error&quot;&gt;&amp;#91;ReferenceWithCleanup&amp;#93;&lt;/span&gt;(&lt;br/&gt;
       new ConcurrentHashMap)&lt;br/&gt;
 &lt;br/&gt;
@@ -1692,11 +1704,21 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object BlockManager {&lt;br/&gt;
     cleaningThread.setName(&quot;RemoteBlock-temp-file-clean-thread&quot;)&lt;br/&gt;
     cleaningThread.start()&lt;br/&gt;
 &lt;br/&gt;
-    override def createTempFile(): File = {&lt;br/&gt;
-      blockManager.diskBlockManager.createTempLocalBlock()._2&lt;br/&gt;
+    override def createTempFile(transportConf: TransportConf): DownloadFile = {&lt;br/&gt;
+      val file = blockManager.diskBlockManager.createTempLocalBlock()._2&lt;br/&gt;
+      encryptionKey match {
+        case Some(key) =&amp;gt;
+          // encryption is enabled, so when we read the decrypted data off the network, we need to
+          // encrypt it when writing to disk.  Note that the data may have been encrypted when it
+          // was cached on disk on the remote side, but it was already decrypted by now (see
+          // EncryptedBlockData).
+          new EncryptedDownloadFile(file, key)
+        case None =&amp;gt;
+          new SimpleDownloadFile(file, transportConf)
+      }&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
-    override def registerTempFileToClean(file: File): Boolean = {&lt;br/&gt;
+    override def registerTempFileToClean(file: DownloadFile): Boolean = {
       referenceBuffer.add(new ReferenceWithCleanup(file, referenceQueue))
     }&lt;br/&gt;
 &lt;br/&gt;
@@ -1724,4 +1746,39 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object BlockManager {&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * A DownloadFile that encrypts data when it is written, and decrypts when it&apos;s read.&lt;br/&gt;
+   */&lt;br/&gt;
+  private class EncryptedDownloadFile(&lt;br/&gt;
+      file: File,&lt;br/&gt;
+      key: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;) extends DownloadFile {&lt;br/&gt;
+&lt;br/&gt;
+    private val env = SparkEnv.get&lt;br/&gt;
+&lt;br/&gt;
+    override def delete(): Boolean = file.delete()&lt;br/&gt;
+&lt;br/&gt;
+    override def openForWriting(): DownloadFileWritableChannel = {
+      new EncryptedDownloadWritableChannel()
+    }&lt;br/&gt;
+&lt;br/&gt;
+    override def path(): String = file.getAbsolutePath&lt;br/&gt;
+&lt;br/&gt;
+    private class EncryptedDownloadWritableChannel extends DownloadFileWritableChannel {&lt;br/&gt;
+      private val countingOutput: CountingWritableChannel = new CountingWritableChannel(&lt;br/&gt;
+        Channels.newChannel(env.serializerManager.wrapForEncryption(new FileOutputStream(file))))&lt;br/&gt;
+&lt;br/&gt;
+      override def closeAndRead(): ManagedBuffer = {
+        countingOutput.close()
+        val size = countingOutput.getCount
+        new EncryptedManagedBuffer(new EncryptedBlockData(file, size, env.conf, key))
+      }&lt;br/&gt;
+&lt;br/&gt;
+      override def write(src: ByteBuffer): Int = countingOutput.write(src)&lt;br/&gt;
+&lt;br/&gt;
+      override def isOpen: Boolean = countingOutput.isOpen()&lt;br/&gt;
+&lt;br/&gt;
+      override def close(): Unit = countingOutput.close()&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala b/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala&lt;br/&gt;
index a024c83d8d8b7..17390f9c60e79 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/storage/DiskBlockObjectWriter.scala&lt;br/&gt;
@@ -20,9 +20,9 @@ package org.apache.spark.storage&lt;br/&gt;
 import java.io.{BufferedOutputStream, File, FileOutputStream, OutputStream}&lt;br/&gt;
 import java.nio.channels.FileChannel&lt;br/&gt;
 &lt;br/&gt;
-import org.apache.spark.executor.ShuffleWriteMetrics&lt;br/&gt;
 import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.serializer.{SerializationStream, SerializerInstance, SerializerManager}&lt;br/&gt;
+import org.apache.spark.shuffle.ShuffleWriteMetricsReporter&lt;br/&gt;
 import org.apache.spark.util.Utils&lt;br/&gt;
 &lt;br/&gt;
 /**&lt;br/&gt;
@@ -43,7 +43,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class DiskBlockObjectWriter(&lt;br/&gt;
     syncWrites: Boolean,&lt;br/&gt;
     // These write metrics concurrently shared with other active DiskBlockObjectWriters who&lt;br/&gt;
     // are themselves performing writes. All updates must be relative.&lt;br/&gt;
-    writeMetrics: ShuffleWriteMetrics,&lt;br/&gt;
+    writeMetrics: ShuffleWriteMetricsReporter,&lt;br/&gt;
     val blockId: BlockId = null)&lt;br/&gt;
   extends OutputStream&lt;br/&gt;
   with Logging {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/storage/DiskStore.scala b/core/src/main/scala/org/apache/spark/storage/DiskStore.scala&lt;br/&gt;
index a820bc70b33b2..29963a95cb074 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/storage/DiskStore.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/storage/DiskStore.scala&lt;br/&gt;
@@ -30,8 +30,10 @@ import io.netty.channel.DefaultFileRegion&lt;br/&gt;
 &lt;br/&gt;
 import org.apache.spark.{SecurityManager, SparkConf}&lt;br/&gt;
 import org.apache.spark.internal.{config, Logging}
&lt;p&gt;+import org.apache.spark.network.buffer.ManagedBuffer&lt;br/&gt;
 import org.apache.spark.network.util.&lt;/p&gt;
{AbstractFileRegion, JavaUtils}
&lt;p&gt; import org.apache.spark.security.CryptoStreamUtils&lt;br/&gt;
+import org.apache.spark.unsafe.array.ByteArrayMethods&lt;br/&gt;
 import org.apache.spark.util.Utils&lt;br/&gt;
 import org.apache.spark.util.io.ChunkedByteBuffer&lt;/p&gt;

&lt;p&gt;@@ -200,7 +202,7 @@ private class DiskBlockData(&lt;br/&gt;
   private def open() = new FileInputStream(file).getChannel&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt;-private class EncryptedBlockData(&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class EncryptedBlockData(&lt;br/&gt;
     file: File,&lt;br/&gt;
     blockSize: Long,&lt;br/&gt;
     conf: SparkConf,&lt;br/&gt;
@@ -216,7 +218,7 @@ private class EncryptedBlockData(&lt;br/&gt;
       var remaining = blockSize&lt;br/&gt;
       val chunks = new ListBuffer&lt;span class=&quot;error&quot;&gt;&amp;#91;ByteBuffer&amp;#93;&lt;/span&gt;()&lt;br/&gt;
       while (remaining &amp;gt; 0) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val chunkSize = math.min(remaining, Int.MaxValue)&lt;br/&gt;
+        val chunkSize = math.min(remaining, ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)&lt;br/&gt;
         val chunk = allocator(chunkSize.toInt)&lt;br/&gt;
         remaining -= chunkSize&lt;br/&gt;
         JavaUtils.readFully(source, chunk)&lt;br/&gt;
@@ -234,7 +236,8 @@ private class EncryptedBlockData(&lt;br/&gt;
     // This is used by the block transfer service to replicate blocks. The upload code reads&lt;br/&gt;
     // all bytes into memory to send the block to the remote executor, so it&apos;s ok to do this&lt;br/&gt;
     // as long as the block fits in a Java array.&lt;/li&gt;
	&lt;li&gt;assert(blockSize &amp;lt;= Int.MaxValue, &quot;Block is too large to be wrapped in a byte buffer.&quot;)&lt;br/&gt;
+    assert(blockSize &amp;lt;= ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH,&lt;br/&gt;
+      &quot;Block is too large to be wrapped in a byte buffer.&quot;)&lt;br/&gt;
     val dst = ByteBuffer.allocate(blockSize.toInt)&lt;br/&gt;
     val in = open()&lt;br/&gt;
     try 
{
@@ -260,7 +263,23 @@ private class EncryptedBlockData(
         throw e
     }
&lt;p&gt;   }&lt;br/&gt;
+}&lt;br/&gt;
+&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class EncryptedManagedBuffer(&lt;br/&gt;
+    val blockData: EncryptedBlockData) extends ManagedBuffer &lt;/p&gt;
{
+
+  // This is the size of the decrypted data
+  override def size(): Long = blockData.size
+
+  override def nioByteBuffer(): ByteBuffer = blockData.toByteBuffer()
+
+  override def convertToNetty(): AnyRef = blockData.toNetty()
+
+  override def createInputStream(): InputStream = blockData.toInputStream()
+
+  override def retain(): ManagedBuffer = this
 
+  override def release(): ManagedBuffer = this
 }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; private class ReadableChannelFileRegion(source: ReadableByteChannel, blockSize: Long)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala b/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala&lt;br/&gt;
index 9ccc8f9cc585b..917cfab1c699a 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/storage/RDDInfo.scala&lt;br/&gt;
@@ -55,14 +55,17 @@ class RDDInfo(&lt;br/&gt;
 }&lt;/p&gt;

&lt;p&gt; private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object RDDInfo {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private val callsiteForm = SparkEnv.get.conf.get(EVENT_LOG_CALLSITE_FORM)&lt;br/&gt;
-&lt;br/&gt;
   def fromRdd(rdd: RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;): RDDInfo = {&lt;br/&gt;
     val rddName = Option(rdd.name).getOrElse(Utils.getFormattedClassName(rdd))&lt;br/&gt;
     val parentIds = rdd.dependencies.map(_.rdd.id)&lt;/li&gt;
	&lt;li&gt;val callSite = callsiteForm match {&lt;/li&gt;
	&lt;li&gt;case &quot;short&quot; =&amp;gt; rdd.creationSite.shortForm&lt;/li&gt;
	&lt;li&gt;case &quot;long&quot; =&amp;gt; rdd.creationSite.longForm&lt;br/&gt;
+    val callsiteLongForm = Option(SparkEnv.get)&lt;br/&gt;
+      .map(_.conf.get(EVENT_LOG_CALLSITE_LONG_FORM))&lt;br/&gt;
+      .getOrElse(false)&lt;br/&gt;
+&lt;br/&gt;
+    val callSite = if (callsiteLongForm) 
{
+      rdd.creationSite.longForm
+    }
&lt;p&gt; else &lt;/p&gt;
{
+      rdd.creationSite.shortForm
     }
&lt;p&gt;     new RDDInfo(rdd.id, rddName, rdd.partitions.length,&lt;br/&gt;
       rdd.getStorageLevel, parentIds, callSite, rdd.scope)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala b/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala&lt;br/&gt;
index 00d01dd28afb5..86f7c08eddcb5 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/storage/ShuffleBlockFetcherIterator.scala&lt;br/&gt;
@@ -17,7 +17,7 @@&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; package org.apache.spark.storage&lt;/p&gt;

&lt;p&gt;-import java.io.&lt;/p&gt;
{File, InputStream, IOException}
&lt;p&gt;+import java.io.&lt;/p&gt;
{InputStream, IOException}
&lt;p&gt; import java.nio.ByteBuffer&lt;br/&gt;
 import java.util.concurrent.LinkedBlockingQueue&lt;br/&gt;
 import javax.annotation.concurrent.GuardedBy&lt;br/&gt;
@@ -28,8 +28,9 @@ import scala.collection.mutable.&lt;/p&gt;
{ArrayBuffer, HashMap, HashSet, Queue}
&lt;p&gt; import org.apache.spark.&lt;/p&gt;
{SparkException, TaskContext}
&lt;p&gt; import org.apache.spark.internal.Logging&lt;br/&gt;
 import org.apache.spark.network.buffer.&lt;/p&gt;
{FileSegmentManagedBuffer, ManagedBuffer}
&lt;p&gt;-import org.apache.spark.network.shuffle.&lt;/p&gt;
{BlockFetchingListener, ShuffleClient, TempFileManager}
&lt;p&gt;-import org.apache.spark.shuffle.FetchFailedException&lt;br/&gt;
+import org.apache.spark.network.shuffle._&lt;br/&gt;
+import org.apache.spark.network.util.TransportConf&lt;br/&gt;
+import org.apache.spark.shuffle.&lt;/p&gt;
{FetchFailedException, ShuffleReadMetricsReporter}
&lt;p&gt; import org.apache.spark.util.Utils&lt;br/&gt;
 import org.apache.spark.util.io.ChunkedByteBufferOutputStream&lt;/p&gt;

&lt;p&gt;@@ -50,7 +51,7 @@ import org.apache.spark.util.io.ChunkedByteBufferOutputStream&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;For each block we also require the size (in bytes as a long field) in&lt;/li&gt;
	&lt;li&gt;order to throttle the memory usage. Note that zero-sized blocks are&lt;/li&gt;
	&lt;li&gt;already excluded, which happened in&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;*                        [&lt;span class=&quot;error&quot;&gt;&amp;#91;MapOutputTracker.convertMapStatuses&amp;#93;&lt;/span&gt;].&lt;br/&gt;
+ *                        [&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.MapOutputTracker.convertMapStatuses&amp;#93;&lt;/span&gt;].&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param streamWrapper A function to wrap the returned input stream.&lt;/li&gt;
	&lt;li&gt;@param maxBytesInFlight max size (in bytes) of remote blocks to fetch at any given point.&lt;/li&gt;
	&lt;li&gt;@param maxReqsInFlight max number of remote requests to fetch blocks at any given point.&lt;br/&gt;
@@ -58,6 +59,7 @@ import org.apache.spark.util.io.ChunkedByteBufferOutputStream&lt;/li&gt;
	&lt;li&gt;for a given remote host:port.&lt;/li&gt;
	&lt;li&gt;@param maxReqSizeShuffleToMem max size (in bytes) of a request that can be shuffled to memory.&lt;/li&gt;
	&lt;li&gt;@param detectCorrupt whether to detect any corruption in fetched blocks.&lt;br/&gt;
+ * @param shuffleMetrics used to report shuffle metrics.&lt;br/&gt;
  */&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;&lt;br/&gt;
 final class ShuffleBlockFetcherIterator(&lt;br/&gt;
@@ -70,8 +72,9 @@ final class ShuffleBlockFetcherIterator(&lt;br/&gt;
     maxReqsInFlight: Int,&lt;br/&gt;
     maxBlocksInFlightPerAddress: Int,&lt;br/&gt;
     maxReqSizeShuffleToMem: Long,&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;detectCorrupt: Boolean)&lt;/li&gt;
	&lt;li&gt;extends Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;(BlockId, InputStream)&amp;#93;&lt;/span&gt; with TempFileManager with Logging {&lt;br/&gt;
+    detectCorrupt: Boolean,&lt;br/&gt;
+    shuffleMetrics: ShuffleReadMetricsReporter)&lt;br/&gt;
+  extends Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;(BlockId, InputStream)&amp;#93;&lt;/span&gt; with DownloadFileManager with Logging 
{
 
   import ShuffleBlockFetcherIterator._
 
@@ -136,8 +139,6 @@ final class ShuffleBlockFetcherIterator(
    */
   private[this] val corruptedBlocks = mutable.HashSet[BlockId]()
 
-  private[this] val shuffleMetrics = context.taskMetrics().createTempShuffleReadMetrics()
-
   /**
    * Whether the iterator is still active. If isZombie is true, the callback interface will no
    * longer place fetched blocks into [[results]].
@@ -150,7 +151,7 @@ final class ShuffleBlockFetcherIterator(
    * deleted when cleanup. This is a layer of defensiveness against disk file leaks.
    */
   @GuardedBy(&quot;this&quot;)
-  private[this] val shuffleFilesSet = mutable.HashSet[File]()
+  private[this] val shuffleFilesSet = mutable.HashSet[DownloadFile]()
 
   initialize()
 
@@ -164,11 +165,15 @@ final class ShuffleBlockFetcherIterator(
     currentResult = null
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def createTempFile(): File = {&lt;/li&gt;
	&lt;li&gt;blockManager.diskBlockManager.createTempLocalBlock()._2&lt;br/&gt;
+  override def createTempFile(transportConf: TransportConf): DownloadFile = 
{
+    // we never need to do any encryption or decryption here, regardless of configs, because that
+    // is handled at another layer in the code.  When encryption is enabled, shuffle data is written
+    // to disk encrypted in the first place, and sent over the network still encrypted.
+    new SimpleDownloadFile(
+      blockManager.diskBlockManager.createTempLocalBlock()._2, transportConf)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def registerTempFileToClean(file: File): Boolean = synchronized {&lt;br/&gt;
+  override def registerTempFileToClean(file: DownloadFile): Boolean = synchronized {&lt;br/&gt;
     if (isZombie) 
{
       false
     }
&lt;p&gt; else &lt;/p&gt;
{
@@ -204,7 +209,7 @@ final class ShuffleBlockFetcherIterator(
     }
&lt;p&gt;     shuffleFilesSet.foreach &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: { file =&amp;gt;       if (!file.delete()) {
-        logWarning(&quot;Failed to cleanup shuffle fetch temp file &quot; + file.getAbsolutePath())
+        logWarning(&quot;Failed to cleanup shuffle fetch temp file &quot; + file.path())
       }     }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;   }&lt;br/&gt;
@@ -443,35 +448,35 @@ final class ShuffleBlockFetcherIterator(&lt;br/&gt;
               buf.release()&lt;br/&gt;
               throwFetchFailedException(blockId, address, e)&lt;br/&gt;
           }&lt;br/&gt;
-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;input = streamWrapper(blockId, in)&lt;/li&gt;
	&lt;li&gt;// Only copy the stream if it&apos;s wrapped by compression or encryption, also the size of&lt;/li&gt;
	&lt;li&gt;// block is small (the decompressed block is smaller than maxBytesInFlight)&lt;/li&gt;
	&lt;li&gt;if (detectCorrupt &amp;amp;&amp;amp; !input.eq(in) &amp;amp;&amp;amp; size &amp;lt; maxBytesInFlight / 3) {&lt;/li&gt;
	&lt;li&gt;val originalInput = input&lt;/li&gt;
	&lt;li&gt;val out = new ChunkedByteBufferOutputStream(64 * 1024, ByteBuffer.allocate)&lt;/li&gt;
	&lt;li&gt;try {&lt;br/&gt;
+          var isStreamCopied: Boolean = false&lt;br/&gt;
+          try {&lt;br/&gt;
+            input = streamWrapper(blockId, in)&lt;br/&gt;
+            // Only copy the stream if it&apos;s wrapped by compression or encryption, also the size of&lt;br/&gt;
+            // block is small (the decompressed block is smaller than maxBytesInFlight)&lt;br/&gt;
+            if (detectCorrupt &amp;amp;&amp;amp; !input.eq(in) &amp;amp;&amp;amp; size &amp;lt; maxBytesInFlight / 3) 
{
+              isStreamCopied = true
+              val out = new ChunkedByteBufferOutputStream(64 * 1024, ByteBuffer.allocate)
               // Decompress the whole block at once to detect any corruption, which could increase
               // the memory usage tne potential increase the chance of OOM.
               // TODO: manage the memory used here, and spill it into disk in case of OOM.
-              Utils.copyStream(input, out)
-              out.close()
+              Utils.copyStream(input, out, closeStreams = true)
               input = out.toChunkedByteBuffer.toInputStream(dispose = true)
-            }
&lt;p&gt; catch {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;case e: IOException =&amp;gt;&lt;/li&gt;
	&lt;li&gt;buf.release()&lt;/li&gt;
	&lt;li&gt;if (buf.isInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;FileSegmentManagedBuffer&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; corruptedBlocks.contains(blockId)) 
{
-                  throwFetchFailedException(blockId, address, e)
-                }
&lt;p&gt; else &lt;/p&gt;
{
-                  logWarning(s&quot;got an corrupted block $blockId from $address, fetch again&quot;, e)
-                  corruptedBlocks += blockId
-                  fetchRequests += FetchRequest(address, Array((blockId, size)))
-                  result = null
-                }&lt;/th&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
&lt;/li&gt;
	&lt;li&gt;} finally 
{
-              // TODO: release the buf here to free memory earlier
-              originalInput.close()
+            }
&lt;p&gt;+          } catch &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            case e}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt; finally &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+            // TODO}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;diff --git a/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala b/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala&lt;br/&gt;
index adc406bb1c441..1c9ea1dba97d7 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/storage/StorageUtils.scala&lt;br/&gt;
@@ -22,9 +22,12 @@ import java.nio.
{ByteBuffer, MappedByteBuffer}
&lt;p&gt; import scala.collection.Map&lt;br/&gt;
 import scala.collection.mutable&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+import org.apache.commons.lang3.&lt;/p&gt;
{JavaVersion, SystemUtils}
&lt;p&gt;+import sun.misc.Unsafe&lt;br/&gt;
 import sun.nio.ch.DirectBuffer&lt;/p&gt;

&lt;p&gt; import org.apache.spark.internal.Logging&lt;br/&gt;
+import org.apache.spark.util.Utils&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Storage information for each BlockManager.&lt;br/&gt;
@@ -193,6 +196,31 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class StorageStatus(&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; /** Helper methods for storage-related objects. */&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object StorageUtils extends Logging {&lt;br/&gt;
+&lt;br/&gt;
+  // In Java 8, the type of DirectBuffer.cleaner() was sun.misc.Cleaner, and it was possible&lt;br/&gt;
+  // to access the method sun.misc.Cleaner.clean() to invoke it. The type changed to&lt;br/&gt;
+  // jdk.internal.ref.Cleaner in later JDKs, and the .clean() method is not accessible even with&lt;br/&gt;
+  // reflection. However sun.misc.Unsafe added a invokeCleaner() method in JDK 9+ and this is&lt;br/&gt;
+  // still accessible with reflection.&lt;br/&gt;
+  private val bufferCleaner: DirectBuffer =&amp;gt; Unit =&lt;br/&gt;
+    if (SystemUtils.isJavaVersionAtLeast(JavaVersion.JAVA_9)) &lt;/p&gt;
{
+      val cleanerMethod =
+        Utils.classForName(&quot;sun.misc.Unsafe&quot;).getMethod(&quot;invokeCleaner&quot;, classOf[ByteBuffer])
+      val unsafeField = classOf[Unsafe].getDeclaredField(&quot;theUnsafe&quot;)
+      unsafeField.setAccessible(true)
+      val unsafe = unsafeField.get(null).asInstanceOf[Unsafe]
+      buffer: DirectBuffer =&amp;gt; cleanerMethod.invoke(unsafe, buffer)
+    }
&lt;p&gt; else {&lt;br/&gt;
+      val cleanerMethod = Utils.classForName(&quot;sun.misc.Cleaner&quot;).getMethod(&quot;clean&quot;)&lt;br/&gt;
+      buffer: DirectBuffer =&amp;gt; {&lt;br/&gt;
+        // Careful to avoid the return type of .cleaner(), which changes with JDK&lt;br/&gt;
+        val cleaner: AnyRef = buffer.cleaner()&lt;br/&gt;
+        if (cleaner != null) &lt;/p&gt;
{
+          cleanerMethod.invoke(cleaner)
+        }
&lt;p&gt;+      }&lt;br/&gt;
+    }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Attempt to clean up a ByteBuffer if it is direct or memory-mapped. This uses an &lt;b&gt;unsafe&lt;/b&gt; Sun&lt;/li&gt;
	&lt;li&gt;API that will cause errors if one attempts to read from the disposed buffer. However, neither&lt;br/&gt;
@@ -204,14 +232,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object StorageUtils extends Logging {&lt;br/&gt;
   def dispose(buffer: ByteBuffer): Unit = 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {     if (buffer != null &amp;amp;&amp;amp; buffer.isInstanceOf[MappedByteBuffer]) {
       logTrace(s&quot;Disposing of $buffer&quot;)
-      cleanDirectBuffer(buffer.asInstanceOf[DirectBuffer])
+      bufferCleaner(buffer.asInstanceOf[DirectBuffer])
     }   }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def cleanDirectBuffer(buffer: DirectBuffer) = {&lt;/li&gt;
	&lt;li&gt;val cleaner = buffer.cleaner()&lt;/li&gt;
	&lt;li&gt;if (cleaner != null) 
{
-      cleaner.clean()
-    }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala b/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala&lt;br/&gt;
index 06fd56e54d9c8..8513359934bec 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/storage/memory/MemoryStore.scala&lt;br/&gt;
@@ -34,6 +34,7 @@ import org.apache.spark.memory.
{MemoryManager, MemoryMode}
&lt;p&gt; import org.apache.spark.serializer.&lt;/p&gt;
{SerializationStream, SerializerManager}
&lt;p&gt; import org.apache.spark.storage._&lt;br/&gt;
 import org.apache.spark.unsafe.Platform&lt;br/&gt;
+import org.apache.spark.unsafe.array.ByteArrayMethods&lt;br/&gt;
 import org.apache.spark.util.&lt;/p&gt;
{SizeEstimator, Utils}
&lt;p&gt; import org.apache.spark.util.collection.SizeTrackingVector&lt;br/&gt;
 import org.apache.spark.util.io.&lt;/p&gt;
{ChunkedByteBuffer, ChunkedByteBufferOutputStream}
&lt;p&gt;@@ -333,11 +334,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class MemoryStore(&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Initial per-task memory to request for unrolling blocks (bytes).&lt;br/&gt;
     val initialMemoryThreshold = unrollMemoryThreshold&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val chunkSize = if (initialMemoryThreshold &amp;gt; Int.MaxValue) {&lt;br/&gt;
+    val chunkSize = if (initialMemoryThreshold &amp;gt; ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH) {&lt;br/&gt;
       logWarning(s&quot;Initial memory threshold of ${Utils.bytesToString(initialMemoryThreshold)} &quot; +&lt;br/&gt;
         s&quot;is too large to be set as chunk size. Chunk size has been capped to &quot; +&lt;/li&gt;
	&lt;li&gt;s&quot;${Utils.bytesToString(Int.MaxValue)}&quot;)&lt;/li&gt;
	&lt;li&gt;Int.MaxValue&lt;br/&gt;
+        s&quot;${Utils.bytesToString(ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH)}&quot;)&lt;br/&gt;
+      ByteArrayMethods.MAX_ROUNDED_ARRAY_LENGTH&lt;br/&gt;
     } else 
{
       initialMemoryThreshold.toInt
     }
&lt;p&gt;diff --git a/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala b/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala&lt;br/&gt;
index 52a955111231a..316af9b79d286 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/ui/JettyUtils.scala&lt;br/&gt;
@@ -356,13 +356,15 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object JettyUtils extends Logging 
{
 
         (connector, connector.getLocalPort())
       }
&lt;p&gt;+      val httpConfig = new HttpConfiguration()&lt;br/&gt;
+      httpConfig.setRequestHeaderSize(conf.get(UI_REQUEST_HEADER_SIZE).toInt)&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // If SSL is configured, create the secure connector first.&lt;br/&gt;
       val securePort = sslOptions.createJettySslContextFactory().map { factory =&amp;gt;&lt;br/&gt;
         val securePort = sslOptions.port.getOrElse(if (port &amp;gt; 0) Utils.userPort(port, 400) else 0)&lt;br/&gt;
         val secureServerName = if (serverName.nonEmpty) s&quot;$serverName (HTTPS)&quot; else serverName&lt;br/&gt;
         val connectionFactories = AbstractConnectionFactory.getFactories(factory,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new HttpConnectionFactory())&lt;br/&gt;
+          new HttpConnectionFactory(httpConfig))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         def sslConnect(currentPort: Int): (ServerConnector, Int) = {&lt;br/&gt;
           newConnector(connectionFactories, currentPort)&lt;br/&gt;
@@ -377,7 +379,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object JettyUtils extends Logging {&lt;/p&gt;

&lt;p&gt;       // Bind the HTTP port.&lt;br/&gt;
       def httpConnect(currentPort: Int): (ServerConnector, Int) = &lt;/p&gt;
{
-        newConnector(Array(new HttpConnectionFactory()), currentPort)
+        newConnector(Array(new HttpConnectionFactory(httpConfig)), currentPort)
       }

&lt;p&gt;       val (httpConnector, httpPort) = Utils.startServiceOnPort&lt;span class=&quot;error&quot;&gt;&amp;#91;ServerConnector&amp;#93;&lt;/span&gt;(port, httpConnect,&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/ui/PagedTable.scala b/core/src/main/scala/org/apache/spark/ui/PagedTable.scala&lt;br/&gt;
index 65fa38387b9ee..6c2c1f6827948 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/ui/PagedTable.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/ui/PagedTable.scala&lt;br/&gt;
@@ -31,11 +31,7 @@ import org.apache.spark.util.Utils&lt;br/&gt;
  *&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param pageSize the number of rows in a page&lt;br/&gt;
  */&lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; abstract class PagedDataSource&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(val pageSize: Int) {&lt;br/&gt;
-&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (pageSize &amp;lt;= 0) 
{
-    throw new IllegalArgumentException(&quot;Page size must be positive&quot;)
-  }
&lt;p&gt;+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; abstract class PagedDataSource&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(val pageSize: Int) {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Return the size of all data.&lt;br/&gt;
@@ -51,13 +47,24 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; abstract class PagedDataSource&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(val pageSize: Int) {&lt;/li&gt;
	&lt;li&gt;Slice the data for this page&lt;br/&gt;
    */&lt;br/&gt;
   def pageData(page: Int): PageData&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; = {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val totalPages = (dataSize + pageSize - 1) / pageSize&lt;/li&gt;
	&lt;li&gt;if (page &amp;lt;= 0 || page &amp;gt; totalPages) {&lt;/li&gt;
	&lt;li&gt;throw new IndexOutOfBoundsException(&lt;/li&gt;
	&lt;li&gt;s&quot;Page $page is out of range. Please select a page number between 1 and $totalPages.&quot;)&lt;br/&gt;
+    // Display all the data in one page, if the pageSize is less than or equal to zero.&lt;br/&gt;
+    val pageTableSize = if (pageSize &amp;lt;= 0) 
{
+      dataSize
+    }
&lt;p&gt; else &lt;/p&gt;
{
+      pageSize
+    }
&lt;p&gt;+    val totalPages = (dataSize + pageTableSize - 1) / pageTableSize&lt;br/&gt;
+&lt;br/&gt;
+    val pageToShow = if (page &amp;lt;= 0) &lt;/p&gt;
{
+      1
+    }
&lt;p&gt; else if (page &amp;gt; totalPages) &lt;/p&gt;
{
+      totalPages
+    }
&lt;p&gt; else &lt;/p&gt;
{
+      page
     }&lt;/li&gt;
	&lt;li&gt;val from = (page - 1) * pageSize&lt;/li&gt;
	&lt;li&gt;val to = dataSize.min(page * pageSize)&lt;br/&gt;
+&lt;br/&gt;
+    val (from, to) = ((pageToShow - 1) * pageSize, dataSize.min(pageToShow * pageTableSize))&lt;br/&gt;
+&lt;br/&gt;
     PageData(totalPages, sliceData(from, to))&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -72,7 +79,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; case class PageData&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(totalPage: Int, data: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;)&lt;br/&gt;
 /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A paged table that will generate a HTML table for a specified page and also the page navigation.&lt;br/&gt;
  */&lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; trait PagedTable&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; {&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; trait PagedTable&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def tableId: String&lt;/p&gt;

&lt;p&gt;@@ -80,8 +87,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; trait PagedTable&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; {&lt;/p&gt;

&lt;p&gt;   def pageSizeFormField: String&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def prevPageSizeFormField: String&lt;br/&gt;
-&lt;br/&gt;
   def pageNumberFormField: String&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def dataSource: PagedDataSource&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;&lt;br/&gt;
@@ -94,7 +99,23 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; trait PagedTable&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; {&lt;br/&gt;
     val _dataSource = dataSource&lt;br/&gt;
     try {&lt;br/&gt;
       val PageData(totalPages, data) = _dataSource.pageData(page)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val pageNavi = pageNavigation(page, _dataSource.pageSize, totalPages)&lt;br/&gt;
+&lt;br/&gt;
+      val pageToShow = if (page &amp;lt;= 0) 
{
+        1
+      }
&lt;p&gt; else if (page &amp;gt; totalPages) &lt;/p&gt;
{
+        totalPages
+      }
&lt;p&gt; else &lt;/p&gt;
{
+        page
+      }
&lt;p&gt;+      // Display all the data in one page, if the pageSize is less than or equal to zero.&lt;br/&gt;
+      val pageSize = if (_dataSource.pageSize &amp;lt;= 0) &lt;/p&gt;
{
+        data.size
+      }
&lt;p&gt; else &lt;/p&gt;
{
+        _dataSource.pageSize
+      }
&lt;p&gt;+&lt;br/&gt;
+      val pageNavi = pageNavigation(pageToShow, pageSize, totalPages)&lt;br/&gt;
+&lt;br/&gt;
       &amp;lt;div&amp;gt;&lt;/p&gt;
         {pageNavi}
&lt;p&gt;         &amp;lt;table class=&lt;/p&gt;
{tableCssClass}
&lt;p&gt; id=&lt;/p&gt;
{tableId}
&lt;p&gt;&amp;gt;&lt;br/&gt;
@@ -122,13 +143,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; trait PagedTable&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Return a page navigation.&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* &amp;lt;ul&amp;gt;&lt;/li&gt;
	&lt;li&gt;*   &amp;lt;li&amp;gt;If the totalPages is 1, the page navigation will be empty&amp;lt;/li&amp;gt;&lt;/li&gt;
	&lt;li&gt;*   &amp;lt;li&amp;gt;&lt;/li&gt;
	&lt;li&gt;*     If the totalPages is more than 1, it will create a page navigation including a group of&lt;/li&gt;
	&lt;li&gt;*     page numbers and a form to submit the page number.&lt;/li&gt;
	&lt;li&gt;*   &amp;lt;/li&amp;gt;&lt;/li&gt;
	&lt;li&gt;* &amp;lt;/ul&amp;gt;&lt;br/&gt;
+   *&lt;br/&gt;
+   * It will create a page navigation including a group of page numbers and a form&lt;br/&gt;
+   * to submit the page number.&lt;br/&gt;
    *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;Here are some examples of the page navigation:&lt;/li&gt;
	&lt;li&gt;{{{&lt;br/&gt;
@@ -154,120 +171,112 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; trait PagedTable&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; 
{
    * }
&lt;p&gt;}}&lt;br/&gt;
    */&lt;br/&gt;
   private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; def pageNavigation(page: Int, pageSize: Int, totalPages: Int): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (totalPages == 1) 
{
-      Nil
-    }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// A group includes all page numbers will be shown in the page navigation.&lt;/li&gt;
	&lt;li&gt;// The size of group is 10 means there are 10 page numbers will be shown.&lt;/li&gt;
	&lt;li&gt;// The first group is 1 to 10, the second is 2 to 20, and so on&lt;/li&gt;
	&lt;li&gt;val groupSize = 10&lt;/li&gt;
	&lt;li&gt;val firstGroup = 0&lt;/li&gt;
	&lt;li&gt;val lastGroup = (totalPages - 1) / groupSize&lt;/li&gt;
	&lt;li&gt;val currentGroup = (page - 1) / groupSize&lt;/li&gt;
	&lt;li&gt;val startPage = currentGroup * groupSize + 1&lt;/li&gt;
	&lt;li&gt;val endPage = totalPages.min(startPage + groupSize - 1)&lt;/li&gt;
	&lt;li&gt;val pageTags = (startPage to endPage).map { p =&amp;gt;&lt;/li&gt;
	&lt;li&gt;if (p == page) {&lt;/li&gt;
	&lt;li&gt;// The current page should be disabled so that it cannot be clicked.&lt;/li&gt;
	&lt;li&gt;&amp;lt;li class=&quot;disabled&quot;&amp;gt;&amp;lt;a href=&quot;#&quot;&amp;gt;
{p}&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&lt;br/&gt;
-        } else {&lt;br/&gt;
-          &amp;lt;li&amp;gt;&amp;lt;a href={Unparsed(pageLink(p))}&amp;gt;{p}
&lt;p&gt;&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
+    // A group includes all page numbers will be shown in the page navigation.&lt;br/&gt;
+    // The size of group is 10 means there are 10 page numbers will be shown.&lt;br/&gt;
+    // The first group is 1 to 10, the second is 2 to 20, and so on&lt;br/&gt;
+    val groupSize = 10&lt;br/&gt;
+    val firstGroup = 0&lt;br/&gt;
+    val lastGroup = (totalPages - 1) / groupSize&lt;br/&gt;
+    val currentGroup = (page - 1) / groupSize&lt;br/&gt;
+    val startPage = currentGroup * groupSize + 1&lt;br/&gt;
+    val endPage = totalPages.min(startPage + groupSize - 1)&lt;br/&gt;
+    val pageTags = (startPage to endPage).map { p =&amp;gt;&lt;br/&gt;
+      if (p == page) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+        // The current page should be disabled so that it cannot be clicked.+        &amp;lt;li class=&amp;quot;disabled&amp;quot;&amp;gt;&amp;lt;a href=&amp;quot;#&amp;quot;&amp;gt;{p}&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;&lt;br/&gt;
+      } else {&lt;br/&gt;
+        &amp;lt;li&amp;gt;&amp;lt;a href={Unparsed(pageLink(p))}&amp;gt;{p}&amp;lt;/a&amp;gt;&amp;lt;/li&amp;gt;       }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;+    }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val hiddenFormFields = {&lt;/li&gt;
	&lt;li&gt;if (goButtonFormPath.contains(&apos;?&apos;)) {&lt;/li&gt;
	&lt;li&gt;val queryString = goButtonFormPath.split(&quot;&lt;br class=&quot;atl-forced-newline&quot; /&gt;?&quot;, 2)(1)&lt;/li&gt;
	&lt;li&gt;val search = queryString.split(&quot;#&quot;)(0)&lt;/li&gt;
	&lt;li&gt;Splitter&lt;/li&gt;
	&lt;li&gt;.on(&apos;&amp;amp;&apos;)&lt;/li&gt;
	&lt;li&gt;.trimResults()&lt;/li&gt;
	&lt;li&gt;.omitEmptyStrings()&lt;/li&gt;
	&lt;li&gt;.withKeyValueSeparator(&quot;=&quot;)&lt;/li&gt;
	&lt;li&gt;.split(search)&lt;/li&gt;
	&lt;li&gt;.asScala&lt;/li&gt;
	&lt;li&gt;.filterKeys(_ != pageSizeFormField)&lt;/li&gt;
	&lt;li&gt;.filterKeys(_ != prevPageSizeFormField)&lt;/li&gt;
	&lt;li&gt;.filterKeys(_ != pageNumberFormField)&lt;/li&gt;
	&lt;li&gt;.mapValues(URLDecoder.decode(_, &quot;UTF-8&quot;))&lt;/li&gt;
	&lt;li&gt;.map { case (k, v) =&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;input type=&quot;hidden&quot; name=
{k} value={v} /&amp;gt;&lt;br/&gt;
-            }&lt;br/&gt;
-        } else {
-          Seq.empty
-        }&lt;br/&gt;
+    val hiddenFormFields = {&lt;br/&gt;
+      if (goButtonFormPath.contains(&apos;?&apos;)) {&lt;br/&gt;
+        val queryString = goButtonFormPath.split(&quot;&lt;br class=&quot;atl-forced-newline&quot; /&gt;?&quot;, 2)(1)&lt;br/&gt;
+        val search = queryString.split(&quot;#&quot;)(0)&lt;br/&gt;
+        Splitter&lt;br/&gt;
+          .on(&apos;&amp;amp;&apos;)&lt;br/&gt;
+          .trimResults()&lt;br/&gt;
+          .omitEmptyStrings()&lt;br/&gt;
+          .withKeyValueSeparator(&quot;=&quot;)&lt;br/&gt;
+          .split(search)&lt;br/&gt;
+          .asScala&lt;br/&gt;
+          .filterKeys(_ != pageSizeFormField)&lt;br/&gt;
+          .filterKeys(_ != pageNumberFormField)&lt;br/&gt;
+          .mapValues(URLDecoder.decode(_, &quot;UTF-8&quot;))&lt;br/&gt;
+          .map { case (k, v) =&amp;gt;&lt;br/&gt;
+            &amp;lt;input type=&quot;hidden&quot; name={k}
&lt;p&gt; value=&lt;/p&gt;
{v}
&lt;p&gt; /&amp;gt;&lt;br/&gt;
+          }&lt;br/&gt;
+      } else &lt;/p&gt;
{
+        Seq.empty
       }
&lt;p&gt;+    }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    &amp;lt;div&amp;gt;&lt;br/&gt;
       &amp;lt;div&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;div&amp;gt;&lt;/li&gt;
	&lt;li&gt;&amp;lt;form id=
{s&quot;form-$tableId-page&quot;}&lt;br/&gt;
-                method=&quot;get&quot;&lt;br/&gt;
-                action={Unparsed(goButtonFormPath)}&lt;br/&gt;
-                class=&quot;form-inline pull-right&quot;&lt;br/&gt;
-                style=&quot;margin-bottom: 0px;&quot;&amp;gt;&lt;br/&gt;
-            &amp;lt;input type=&quot;hidden&quot;&lt;br/&gt;
-                   name={prevPageSizeFormField}&lt;br/&gt;
-                   value={pageSize.toString} /&amp;gt;&lt;br/&gt;
-            {hiddenFormFields}&lt;br/&gt;
-            &amp;lt;label&amp;gt;{totalPages} Pages. Jump to&amp;lt;/label&amp;gt;&lt;br/&gt;
-            &amp;lt;input type=&quot;text&quot;&lt;br/&gt;
-                   name={pageNumberFormField}&lt;br/&gt;
-                   id={s&quot;form-$tableId-page-no&quot;}&lt;br/&gt;
-                   value={page.toString} class=&quot;span1&quot; /&amp;gt;&lt;br/&gt;
-&lt;br/&gt;
-            &amp;lt;label&amp;gt;. Show &amp;lt;/label&amp;gt;&lt;br/&gt;
-            &amp;lt;input type=&quot;text&quot;&lt;br/&gt;
-                   id={s&quot;form-$tableId-page-size&quot;}&lt;br/&gt;
-                   name={pageSizeFormField}&lt;br/&gt;
-                   value={pageSize.toString}&lt;br/&gt;
-                   class=&quot;span1&quot; /&amp;gt;&lt;br/&gt;
-            &amp;lt;label&amp;gt;items in a page.&amp;lt;/label&amp;gt;&lt;br/&gt;
-&lt;br/&gt;
-            &amp;lt;button type=&quot;submit&quot; class=&quot;btn&quot;&amp;gt;Go&amp;lt;/button&amp;gt;&lt;br/&gt;
-          &amp;lt;/form&amp;gt;&lt;br/&gt;
-        &amp;lt;/div&amp;gt;&lt;br/&gt;
-        &amp;lt;div class=&quot;pagination&quot; style=&quot;margin-bottom: 0px;&quot;&amp;gt;&lt;br/&gt;
-          &amp;lt;span style=&quot;float: left; padding-top: 4px; padding-right: 4px;&quot;&amp;gt;Page: &amp;lt;/span&amp;gt;&lt;br/&gt;
-          &amp;lt;ul&amp;gt;&lt;br/&gt;
-            {if (currentGroup &amp;gt; firstGroup) {&lt;br/&gt;
-            &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;a href={Unparsed(pageLink(startPage - groupSize))} aria-label=&quot;Previous Group&quot;&amp;gt;&lt;br/&gt;
-                &amp;lt;span aria-hidden=&quot;true&quot;&amp;gt;&lt;br/&gt;
-                  &amp;lt;&amp;lt;&lt;br/&gt;
-                &amp;lt;/span&amp;gt;&lt;br/&gt;
-              &amp;lt;/a&amp;gt;&lt;br/&gt;
-            &amp;lt;/li&amp;gt;&lt;br/&gt;
-            }}&lt;br/&gt;
-            {if (page &amp;gt; 1) {&lt;br/&gt;
-            &amp;lt;li&amp;gt;&lt;br/&gt;
-            &amp;lt;a href={Unparsed(pageLink(page - 1))} aria-label=&quot;Previous&quot;&amp;gt;&lt;br/&gt;
+        &amp;lt;form id={s&quot;form-$tableId-page&quot;}
&lt;p&gt;+              method=&quot;get&quot;&lt;br/&gt;
+              action=&lt;/p&gt;
{Unparsed(goButtonFormPath)}
&lt;p&gt;+              class=&quot;form-inline pull-right&quot;&lt;br/&gt;
+              style=&quot;margin-bottom: 0px;&quot;&amp;gt;&lt;br/&gt;
+          &lt;/p&gt;
{hiddenFormFields}
&lt;p&gt;+          &amp;lt;label&amp;gt;&lt;/p&gt;
{totalPages} Pages. Jump to&amp;lt;/label&amp;gt;&lt;br/&gt;
+          &amp;lt;input type=&quot;text&quot;&lt;br/&gt;
+                 name={pageNumberFormField}&lt;br/&gt;
+                 id={s&quot;form-$tableId-page-no&quot;}&lt;br/&gt;
+                 value={page.toString} class=&quot;span1&quot; /&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+          &amp;lt;label&amp;gt;. Show &amp;lt;/label&amp;gt;&lt;br/&gt;
+          &amp;lt;input type=&quot;text&quot;&lt;br/&gt;
+                 id={s&quot;form-$tableId-page-size&quot;}&lt;br/&gt;
+                 name={pageSizeFormField}&lt;br/&gt;
+                 value={pageSize.toString}&lt;br/&gt;
+                 class=&quot;span1&quot; /&amp;gt;&lt;br/&gt;
+          &amp;lt;label&amp;gt;items in a page.&amp;lt;/label&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+          &amp;lt;button type=&quot;submit&quot; class=&quot;btn&quot;&amp;gt;Go&amp;lt;/button&amp;gt;&lt;br/&gt;
+        &amp;lt;/form&amp;gt;&lt;br/&gt;
+      &amp;lt;/div&amp;gt;&lt;br/&gt;
+      &amp;lt;div class=&quot;pagination&quot; style=&quot;margin-bottom: 0px;&quot;&amp;gt;&lt;br/&gt;
+        &amp;lt;span style=&quot;float: left; padding-top: 4px; padding-right: 4px;&quot;&amp;gt;Page: &amp;lt;/span&amp;gt;&lt;br/&gt;
+        &amp;lt;ul&amp;gt;&lt;br/&gt;
+          {if (currentGroup &amp;gt; firstGroup) {&lt;br/&gt;
+          &amp;lt;li&amp;gt;&lt;br/&gt;
+            &amp;lt;a href={Unparsed(pageLink(startPage - groupSize))} aria-label=&quot;Previous Group&quot;&amp;gt;&lt;br/&gt;
               &amp;lt;span aria-hidden=&quot;true&quot;&amp;gt;&lt;br/&gt;
-                &amp;lt;&lt;br/&gt;
+                &amp;lt;&amp;lt;&lt;br/&gt;
               &amp;lt;/span&amp;gt;&lt;br/&gt;
             &amp;lt;/a&amp;gt;&lt;br/&gt;
-            &amp;lt;/li&amp;gt;&lt;br/&gt;
-            }}&lt;br/&gt;
-            {pageTags}&lt;br/&gt;
-            {if (page &amp;lt; totalPages) {&lt;br/&gt;
-            &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;a href={Unparsed(pageLink(page + 1))} aria-label=&quot;Next&quot;&amp;gt;&lt;br/&gt;
-                &amp;lt;span aria-hidden=&quot;true&quot;&amp;gt;&amp;gt;&amp;lt;/span&amp;gt;&lt;br/&gt;
-              &amp;lt;/a&amp;gt;&lt;br/&gt;
-            &amp;lt;/li&amp;gt;&lt;br/&gt;
-            }}&lt;br/&gt;
-            {if (currentGroup &amp;lt; lastGroup) {&lt;br/&gt;
-            &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;a href={Unparsed(pageLink(startPage + groupSize))} aria-label=&quot;Next Group&quot;&amp;gt;&lt;br/&gt;
-                &amp;lt;span aria-hidden=&quot;true&quot;&amp;gt;&lt;br/&gt;
-                  &amp;gt;&amp;gt;&lt;br/&gt;
-                &amp;lt;/span&amp;gt;&lt;br/&gt;
-              &amp;lt;/a&amp;gt;&lt;br/&gt;
-            &amp;lt;/li&amp;gt;&lt;br/&gt;
+          &amp;lt;/li&amp;gt;&lt;br/&gt;
           }}&lt;br/&gt;
-          &amp;lt;/ul&amp;gt;&lt;br/&gt;
-        &amp;lt;/div&amp;gt;&lt;br/&gt;
+          {if (page &amp;gt; 1) {&lt;br/&gt;
+          &amp;lt;li&amp;gt;&lt;br/&gt;
+          &amp;lt;a href={Unparsed(pageLink(page - 1))} aria-label=&quot;Previous&quot;&amp;gt;&lt;br/&gt;
+            &amp;lt;span aria-hidden=&quot;true&quot;&amp;gt;&lt;br/&gt;
+              &amp;lt;&lt;br/&gt;
+            &amp;lt;/span&amp;gt;&lt;br/&gt;
+          &amp;lt;/a&amp;gt;&lt;br/&gt;
+          &amp;lt;/li&amp;gt;&lt;br/&gt;
+          }}&lt;br/&gt;
+          {pageTags}&lt;br/&gt;
+          {if (page &amp;lt; totalPages) {&lt;br/&gt;
+          &amp;lt;li&amp;gt;&lt;br/&gt;
+            &amp;lt;a href={Unparsed(pageLink(page + 1))} aria-label=&quot;Next&quot;&amp;gt;&lt;br/&gt;
+              &amp;lt;span aria-hidden=&quot;true&quot;&amp;gt;&amp;gt;&amp;lt;/span&amp;gt;&lt;br/&gt;
+            &amp;lt;/a&amp;gt;&lt;br/&gt;
+          &amp;lt;/li&amp;gt;&lt;br/&gt;
+          }}&lt;br/&gt;
+          {if (currentGroup &amp;lt; lastGroup) {&lt;br/&gt;
+          &amp;lt;li&amp;gt;&lt;br/&gt;
+            &amp;lt;a href={Unparsed(pageLink(startPage + groupSize))} aria-label=&quot;Next Group&quot;&amp;gt;&lt;br/&gt;
+              &amp;lt;span aria-hidden=&quot;true&quot;&amp;gt;&lt;br/&gt;
+                &amp;gt;&amp;gt;&lt;br/&gt;
+              &amp;lt;/span&amp;gt;&lt;br/&gt;
+            &amp;lt;/a&amp;gt;&lt;br/&gt;
+          &amp;lt;/li&amp;gt;&lt;br/&gt;
+        }}&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
       &amp;lt;/div&amp;gt;&lt;br/&gt;
-    }&lt;br/&gt;
+    &amp;lt;/div&amp;gt;&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
   /**&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/ui/UIUtils.scala b/core/src/main/scala/org/apache/spark/ui/UIUtils.scala&lt;br/&gt;
index 732b7528f499e..60a929375baae 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/ui/UIUtils.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/ui/UIUtils.scala&lt;br/&gt;
@@ -204,6 +204,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object UIUtils extends Logging {&lt;br/&gt;
           href={prependBaseUri(request, &quot;/static/dataTables.bootstrap.css&quot;)} type=&quot;text/css&quot;/&amp;gt;&lt;br/&gt;
     &amp;lt;link rel=&quot;stylesheet&quot;&lt;br/&gt;
           href={prependBaseUri(request, &quot;/static/jsonFormatter.min.css&quot;)} type=&quot;text/css&quot;/&amp;gt;&lt;br/&gt;
+    &amp;lt;link rel=&quot;stylesheet&quot;&lt;br/&gt;
+          href={prependBaseUri(request, &quot;/static/webui-dataTables.css&quot;)} type=&quot;text/css&quot;/&amp;gt;&lt;br/&gt;
     &amp;lt;script src={prependBaseUri(request, &quot;/static/jquery.dataTables.1.10.4.min.js&quot;)}&amp;gt;&amp;lt;/script&amp;gt;&lt;br/&gt;
     &amp;lt;script src={prependBaseUri(request, &quot;/static/jquery.cookies.2.2.0.min.js&quot;)}&amp;gt;&amp;lt;/script&amp;gt;&lt;br/&gt;
     &amp;lt;script src={prependBaseUri(request, &quot;/static/jquery.blockUI.min.js&quot;)}&amp;gt;&amp;lt;/script&amp;gt;&lt;br/&gt;
@@ -218,7 +220,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object UIUtils extends Logging {&lt;br/&gt;
       title: String,&lt;br/&gt;
       content: =&amp;gt; Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       activeTab: SparkUITab,&lt;br/&gt;
-      refreshInterval: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt; = None,&lt;br/&gt;
       helpText: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None,&lt;br/&gt;
       showVisualization: Boolean = false,&lt;br/&gt;
       useDataTables: Boolean = false): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala b/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala&lt;br/&gt;
index 90e9a7a3630cf..2c22e0555fcb8 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/ui/jobs/AllJobsPage.scala&lt;br/&gt;
@@ -220,7 +220,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class AllJobsPage(parent: JobsTab, store: AppStatusStore) extends We&lt;br/&gt;
     val parameterJobSortColumn = UIUtils.stripXSS(request.getParameter(jobTag + &quot;.sort&quot;))&lt;br/&gt;
     val parameterJobSortDesc = UIUtils.stripXSS(request.getParameter(jobTag + &quot;.desc&quot;))&lt;br/&gt;
     val parameterJobPageSize = UIUtils.stripXSS(request.getParameter(jobTag + &quot;.pageSize&quot;))&lt;br/&gt;
-    val parameterJobPrevPageSize = UIUtils.stripXSS(request.getParameter(jobTag + &quot;.prevPageSize&quot;))&lt;br/&gt;
 &lt;br/&gt;
     val jobPage = Option(parameterJobPage).map(_.toInt).getOrElse(1)&lt;br/&gt;
     val jobSortColumn = Option(parameterJobSortColumn).map { sortColumn =&amp;gt;&lt;br/&gt;
@@ -231,17 +230,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class AllJobsPage(parent: JobsTab, store: AppStatusStore) extends We&lt;br/&gt;
       jobSortColumn == jobIdTitle&lt;br/&gt;
     )&lt;br/&gt;
     val jobPageSize = Option(parameterJobPageSize).map(_.toInt).getOrElse(100)&lt;br/&gt;
-    val jobPrevPageSize = Option(parameterJobPrevPageSize).map(_.toInt).getOrElse(jobPageSize)&lt;br/&gt;
-&lt;br/&gt;
-    val page: Int = {&lt;br/&gt;
-      // If the user has changed to a larger page size, then go to page 1 in order to avoid&lt;br/&gt;
-      // IndexOutOfBoundsException.&lt;br/&gt;
-      if (jobPageSize &amp;lt;= jobPrevPageSize) {
-        jobPage
-      } else {
-        1
-      }&lt;br/&gt;
-    }&lt;br/&gt;
+&lt;br/&gt;
     val currentTime = System.currentTimeMillis()&lt;br/&gt;
 &lt;br/&gt;
     try {
@@ -259,7 +248,7 @@ private[ui] class AllJobsPage(parent: JobsTab, store: AppStatusStore) extends We
         pageSize = jobPageSize,
         sortColumn = jobSortColumn,
         desc = jobSortDesc
-      ).table(page)
+      ).table(jobPage)
     } catch {&lt;br/&gt;
       case e @ (_ : IllegalArgumentException | _ : IndexOutOfBoundsException) =&amp;gt;&lt;br/&gt;
         &amp;lt;div class=&quot;alert alert-error&quot;&amp;gt;&lt;br/&gt;
@@ -526,8 +515,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class JobPagedTable(&lt;br/&gt;
 &lt;br/&gt;
   override def pageSizeFormField: String = jobTag + &quot;.pageSize&quot;&lt;br/&gt;
 &lt;br/&gt;
-  override def prevPageSizeFormField: String = jobTag + &quot;.prevPageSize&quot;&lt;br/&gt;
-&lt;br/&gt;
   override def pageNumberFormField: String = jobTag + &quot;.page&quot;&lt;br/&gt;
 &lt;br/&gt;
   override val dataSource = new JobDataSource(&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala b/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala&lt;br/&gt;
deleted file mode 100644&lt;br/&gt;
index 0ff64f053f371..0000000000000&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/ui/jobs/ExecutorTable.scala&lt;br/&gt;
+++ /dev/null&lt;br/&gt;
@@ -1,152 +0,0 @@&lt;br/&gt;
-/*&lt;br/&gt;
- * Licensed to the Apache Software Foundation (ASF) under one or more&lt;br/&gt;
- * contributor license agreements.  See the NOTICE file distributed with&lt;br/&gt;
- * this work for additional information regarding copyright ownership.&lt;br/&gt;
- * The ASF licenses this file to You under the Apache License, Version 2.0&lt;br/&gt;
- * (the &quot;License&quot;); you may not use this file except in compliance with&lt;br/&gt;
- * the License.  You may obtain a copy of the License at&lt;br/&gt;
- *&lt;br/&gt;
- *    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
- *&lt;br/&gt;
- * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
- * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
- * See the License for the specific language governing permissions and&lt;br/&gt;
- * limitations under the License.&lt;br/&gt;
- */&lt;br/&gt;
-&lt;br/&gt;
-package org.apache.spark.ui.jobs&lt;br/&gt;
-&lt;br/&gt;
-import scala.xml.{Node, Unparsed}&lt;br/&gt;
-&lt;br/&gt;
-import org.apache.spark.status.AppStatusStore&lt;br/&gt;
-import org.apache.spark.status.api.v1.StageData&lt;br/&gt;
-import org.apache.spark.ui.{ToolTips, UIUtils}&lt;br/&gt;
-import org.apache.spark.util.Utils&lt;br/&gt;
-&lt;br/&gt;
-/** Stage summary grouped by executors. */&lt;br/&gt;
-private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class ExecutorTable(stage: StageData, store: AppStatusStore) {&lt;br/&gt;
-&lt;br/&gt;
-  import ApiHelper._&lt;br/&gt;
-&lt;br/&gt;
-  def toNodeSeq: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    &amp;lt;table class={UIUtils.TABLE_CLASS_STRIPED_SORTABLE}&amp;gt;&lt;br/&gt;
-      &amp;lt;thead&amp;gt;&lt;br/&gt;
-        &amp;lt;th id=&quot;executorid&quot;&amp;gt;Executor ID&amp;lt;/th&amp;gt;&lt;br/&gt;
-        &amp;lt;th&amp;gt;Address&amp;lt;/th&amp;gt;&lt;br/&gt;
-        &amp;lt;th&amp;gt;Task Time&amp;lt;/th&amp;gt;&lt;br/&gt;
-        &amp;lt;th&amp;gt;Total Tasks&amp;lt;/th&amp;gt;&lt;br/&gt;
-        &amp;lt;th&amp;gt;Failed Tasks&amp;lt;/th&amp;gt;&lt;br/&gt;
-        &amp;lt;th&amp;gt;Killed Tasks&amp;lt;/th&amp;gt;&lt;br/&gt;
-        &amp;lt;th&amp;gt;Succeeded Tasks&amp;lt;/th&amp;gt;&lt;br/&gt;
-        {if (hasInput(stage)) {&lt;br/&gt;
-          &amp;lt;th&amp;gt;&lt;br/&gt;
-            &amp;lt;span data-toggle=&quot;tooltip&quot; title={ToolTips.INPUT}&amp;gt;Input Size / Records&amp;lt;/span&amp;gt;&lt;br/&gt;
-          &amp;lt;/th&amp;gt;&lt;br/&gt;
-        }}&lt;br/&gt;
-        {if (hasOutput(stage)) {&lt;br/&gt;
-          &amp;lt;th&amp;gt;&lt;br/&gt;
-            &amp;lt;span data-toggle=&quot;tooltip&quot; title={ToolTips.OUTPUT}&amp;gt;Output Size / Records&amp;lt;/span&amp;gt;&lt;br/&gt;
-          &amp;lt;/th&amp;gt;&lt;br/&gt;
-        }}&lt;br/&gt;
-        {if (hasShuffleRead(stage)) {&lt;br/&gt;
-          &amp;lt;th&amp;gt;&lt;br/&gt;
-            &amp;lt;span data-toggle=&quot;tooltip&quot; title={ToolTips.SHUFFLE_READ}&amp;gt;&lt;br/&gt;
-            Shuffle Read Size / Records&amp;lt;/span&amp;gt;&lt;br/&gt;
-          &amp;lt;/th&amp;gt;&lt;br/&gt;
-        }}&lt;br/&gt;
-        {if (hasShuffleWrite(stage)) {&lt;br/&gt;
-          &amp;lt;th&amp;gt;&lt;br/&gt;
-            &amp;lt;span data-toggle=&quot;tooltip&quot; title={ToolTips.SHUFFLE_WRITE}&amp;gt;&lt;br/&gt;
-            Shuffle Write Size / Records&amp;lt;/span&amp;gt;&lt;br/&gt;
-          &amp;lt;/th&amp;gt;&lt;br/&gt;
-        }}&lt;br/&gt;
-        {if (hasBytesSpilled(stage)) {&lt;br/&gt;
-          &amp;lt;th&amp;gt;Shuffle Spill (Memory)&amp;lt;/th&amp;gt;&lt;br/&gt;
-          &amp;lt;th&amp;gt;Shuffle Spill (Disk)&amp;lt;/th&amp;gt;&lt;br/&gt;
-        }}&lt;br/&gt;
-        &amp;lt;th&amp;gt;&lt;br/&gt;
-          &amp;lt;span data-toggle=&quot;tooltip&quot; title={ToolTips.BLACKLISTED}&amp;gt;&lt;br/&gt;
-          Blacklisted&lt;br/&gt;
-          &amp;lt;/span&amp;gt;&lt;br/&gt;
-        &amp;lt;/th&amp;gt;&lt;br/&gt;
-      &amp;lt;/thead&amp;gt;&lt;br/&gt;
-      &amp;lt;tbody&amp;gt;&lt;br/&gt;
-        {createExecutorTable(stage)}&lt;br/&gt;
-      &amp;lt;/tbody&amp;gt;&lt;br/&gt;
-    &amp;lt;/table&amp;gt;&lt;br/&gt;
-    &amp;lt;script&amp;gt;&lt;br/&gt;
-      {Unparsed {&lt;br/&gt;
-        &quot;&quot;&quot;&lt;br/&gt;
-          |      window.onload = function() {
-          |        sorttable.innerSortFunction.apply(document.getElementById(&apos;executorid&apos;), [])
-          |      };&lt;br/&gt;
-        &quot;&quot;&quot;.stripMargin&lt;br/&gt;
-      }}&lt;br/&gt;
-    &amp;lt;/script&amp;gt;&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
-  private def createExecutorTable(stage: StageData) : Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-    val executorSummary = store.executorSummary(stage.stageId, stage.attemptId)&lt;br/&gt;
-&lt;br/&gt;
-    executorSummary.toSeq.sortBy(_._1).map { case (k, v) =&amp;gt;&lt;br/&gt;
-      val executor = store.asOption(store.executorSummary(k))&lt;br/&gt;
-      &amp;lt;tr&amp;gt;&lt;br/&gt;
-        &amp;lt;td&amp;gt;&lt;br/&gt;
-          &amp;lt;div style=&quot;float: left&quot;&amp;gt;{k}&amp;lt;/div&amp;gt;&lt;br/&gt;
-          &amp;lt;div style=&quot;float: right&quot;&amp;gt;&lt;br/&gt;
-          {&lt;br/&gt;
-            executor.map(_.executorLogs).getOrElse(Map.empty).map {&lt;br/&gt;
-              case (logName, logUrl) =&amp;gt; &amp;lt;div&amp;gt;&amp;lt;a href={logUrl}&amp;gt;{logName}&amp;lt;/a&amp;gt;&amp;lt;/div&amp;gt;&lt;br/&gt;
-            }&lt;br/&gt;
-          }&lt;br/&gt;
-          &amp;lt;/div&amp;gt;&lt;br/&gt;
-        &amp;lt;/td&amp;gt;&lt;br/&gt;
-        &amp;lt;td&amp;gt;{executor.map { e =&amp;gt; e.hostPort }.getOrElse(&quot;CANNOT FIND ADDRESS&quot;)}&amp;lt;/td&amp;gt;&lt;br/&gt;
-        &amp;lt;td sorttable_customkey={v.taskTime.toString}&amp;gt;{UIUtils.formatDuration(v.taskTime)}&amp;lt;/td&amp;gt;&lt;br/&gt;
-        &amp;lt;td&amp;gt;{v.failedTasks + v.succeededTasks + v.killedTasks}&amp;lt;/td&amp;gt;&lt;br/&gt;
-        &amp;lt;td&amp;gt;{v.failedTasks}&amp;lt;/td&amp;gt;&lt;br/&gt;
-        &amp;lt;td&amp;gt;{v.killedTasks}&amp;lt;/td&amp;gt;&lt;br/&gt;
-        &amp;lt;td&amp;gt;{v.succeededTasks}&amp;lt;/td&amp;gt;&lt;br/&gt;
-        {if (hasInput(stage)) {&lt;br/&gt;
-          &amp;lt;td sorttable_customkey={v.inputBytes.toString}&amp;gt;&lt;br/&gt;
-            {s&quot;${Utils.bytesToString(v.inputBytes)} / ${v.inputRecords}&quot;}&lt;br/&gt;
-          &amp;lt;/td&amp;gt;&lt;br/&gt;
-        }}&lt;br/&gt;
-        {if (hasOutput(stage)) {&lt;br/&gt;
-          &amp;lt;td sorttable_customkey={v.outputBytes.toString}&amp;gt;&lt;br/&gt;
-            {s&quot;${Utils.bytesToString(v.outputBytes)} / ${v.outputRecords}&quot;}&lt;br/&gt;
-          &amp;lt;/td&amp;gt;&lt;br/&gt;
-        }}&lt;br/&gt;
-        {if (hasShuffleRead(stage)) {&lt;br/&gt;
-          &amp;lt;td sorttable_customkey={v.shuffleRead.toString}&amp;gt;&lt;br/&gt;
-            {s&quot;${Utils.bytesToString(v.shuffleRead)} / ${v.shuffleReadRecords}&quot;}&lt;br/&gt;
-          &amp;lt;/td&amp;gt;&lt;br/&gt;
-        }}&lt;br/&gt;
-        {if (hasShuffleWrite(stage)) {&lt;br/&gt;
-          &amp;lt;td sorttable_customkey={v.shuffleWrite.toString}&amp;gt;&lt;br/&gt;
-            {s&quot;${Utils.bytesToString(v.shuffleWrite)} / ${v.shuffleWriteRecords}&quot;}&lt;br/&gt;
-          &amp;lt;/td&amp;gt;&lt;br/&gt;
-        }}&lt;br/&gt;
-        {if (hasBytesSpilled(stage)) {&lt;br/&gt;
-          &amp;lt;td sorttable_customkey={v.memoryBytesSpilled.toString}&amp;gt;&lt;br/&gt;
-            {Utils.bytesToString(v.memoryBytesSpilled)}&lt;br/&gt;
-          &amp;lt;/td&amp;gt;&lt;br/&gt;
-          &amp;lt;td sorttable_customkey={v.diskBytesSpilled.toString}&amp;gt;&lt;br/&gt;
-            {Utils.bytesToString(v.diskBytesSpilled)}&lt;br/&gt;
-          &amp;lt;/td&amp;gt;&lt;br/&gt;
-        }}&lt;br/&gt;
-        {&lt;br/&gt;
-          if (executor.map(_.isBlacklisted).getOrElse(false)) {
-            &amp;lt;td&amp;gt;for application&amp;lt;/td&amp;gt;
-          } else if (v.isBlacklistedForStage) {
-            &amp;lt;td&amp;gt;for stage&amp;lt;/td&amp;gt;
-          } else {
-            &amp;lt;td&amp;gt;false&amp;lt;/td&amp;gt;
-          }&lt;br/&gt;
-        }&lt;br/&gt;
-      &amp;lt;/tr&amp;gt;&lt;br/&gt;
-    }&lt;br/&gt;
-  }&lt;br/&gt;
-&lt;br/&gt;
-}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala b/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala&lt;br/&gt;
index 55eb989962668..a213b764abea7 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/ui/jobs/StagePage.scala&lt;br/&gt;
@@ -91,7 +91,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StagePage(parent: StagesTab, store: AppStatusStore) extends We&lt;br/&gt;
     val parameterTaskSortColumn = UIUtils.stripXSS(request.getParameter(&quot;task.sort&quot;))&lt;br/&gt;
     val parameterTaskSortDesc = UIUtils.stripXSS(request.getParameter(&quot;task.desc&quot;))&lt;br/&gt;
     val parameterTaskPageSize = UIUtils.stripXSS(request.getParameter(&quot;task.pageSize&quot;))&lt;br/&gt;
-    val parameterTaskPrevPageSize = UIUtils.stripXSS(request.getParameter(&quot;task.prevPageSize&quot;))&lt;br/&gt;
+&lt;br/&gt;
+    val eventTimelineParameterTaskPage = UIUtils.stripXSS(&lt;br/&gt;
+      request.getParameter(&quot;task.eventTimelinePageNumber&quot;))&lt;br/&gt;
+    val eventTimelineParameterTaskPageSize = UIUtils.stripXSS(&lt;br/&gt;
+      request.getParameter(&quot;task.eventTimelinePageSize&quot;))&lt;br/&gt;
+    var eventTimelineTaskPage = Option(eventTimelineParameterTaskPage).map(_.toInt).getOrElse(1)&lt;br/&gt;
+    var eventTimelineTaskPageSize = Option(&lt;br/&gt;
+      eventTimelineParameterTaskPageSize).map(_.toInt).getOrElse(100)&lt;br/&gt;
 &lt;br/&gt;
     val taskPage = Option(parameterTaskPage).map(_.toInt).getOrElse(1)&lt;br/&gt;
     val taskSortColumn = Option(parameterTaskSortColumn).map { sortColumn =&amp;gt;
@@ -99,13 +106,11 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
     }.getOrElse(&quot;Index&quot;)&lt;br/&gt;
     val taskSortDesc = Option(parameterTaskSortDesc).map(_.toBoolean).getOrElse(false)&lt;br/&gt;
     val taskPageSize = Option(parameterTaskPageSize).map(_.toInt).getOrElse(100)&lt;br/&gt;
-    val taskPrevPageSize = Option(parameterTaskPrevPageSize).map(_.toInt).getOrElse(taskPageSize)&lt;br/&gt;
-&lt;br/&gt;
     val stageId = parameterId.toInt&lt;br/&gt;
     val stageAttemptId = parameterAttempt.toInt&lt;br/&gt;
 &lt;br/&gt;
     val stageHeader = s&quot;Details for Stage $stageId (Attempt $stageAttemptId)&quot;&lt;br/&gt;
-    val stageData = parent.store&lt;br/&gt;
+    val (stageData, stageJobIds) = parent.store&lt;br/&gt;
       .asOption(parent.store.stageAttempt(stageId, stageAttemptId, details = false))&lt;br/&gt;
       .getOrElse {&lt;br/&gt;
         val content =&lt;br/&gt;
@@ -117,7 +122,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StagePage(parent: StagesTab, store: AppStatusStore) extends We&lt;br/&gt;
 &lt;br/&gt;
     val localitySummary = store.localitySummary(stageData.stageId, stageData.attemptId)&lt;br/&gt;
 &lt;br/&gt;
-    val totalTasks = taskCount(stageData)&lt;br/&gt;
+    val totalTasks = stageData.numActiveTasks + stageData.numCompleteTasks +&lt;br/&gt;
+      stageData.numFailedTasks + stageData.numKilledTasks&lt;br/&gt;
     if (totalTasks == 0) {&lt;br/&gt;
       val content =&lt;br/&gt;
         &amp;lt;div&amp;gt;&lt;br/&gt;
@@ -132,7 +138,15 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StagePage(parent: StagesTab, store: AppStatusStore) extends We&lt;br/&gt;
     val totalTasksNumStr = if (totalTasks == storedTasks) {
       s&quot;$totalTasks&quot;
     } else {&lt;br/&gt;
-      s&quot;$storedTasks, showing ${totalTasks}&quot;&lt;br/&gt;
+      s&quot;$totalTasks, showing $storedTasks&quot;&lt;br/&gt;
+    }&lt;br/&gt;
+    if (eventTimelineTaskPageSize &amp;lt; 1 || eventTimelineTaskPageSize &amp;gt; totalTasks) {
+      eventTimelineTaskPageSize = totalTasks
+    }&lt;br/&gt;
+    val eventTimelineTotalPages =&lt;br/&gt;
+      (totalTasks + eventTimelineTaskPageSize - 1) / eventTimelineTaskPageSize&lt;br/&gt;
+    if (eventTimelineTaskPage &amp;lt; 1 || eventTimelineTaskPage &amp;gt; eventTimelineTotalPages) {
+      eventTimelineTaskPage = 1
     }&lt;br/&gt;
 &lt;br/&gt;
     val summary =&lt;br/&gt;
@@ -154,20 +168,20 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StagePage(parent: StagesTab, store: AppStatusStore) extends We&lt;br/&gt;
           }}&lt;br/&gt;
           {if (hasOutput(stageData)) {&lt;br/&gt;
             &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;strong&amp;gt;Output: &amp;lt;/strong&amp;gt;&lt;br/&gt;
+              &amp;lt;strong&amp;gt;Output Size / Records: &amp;lt;/strong&amp;gt;&lt;br/&gt;
               {s&quot;${Utils.bytesToString(stageData.outputBytes)} / ${stageData.outputRecords}&quot;}&lt;br/&gt;
             &amp;lt;/li&amp;gt;&lt;br/&gt;
           }}&lt;br/&gt;
           {if (hasShuffleRead(stageData)) {&lt;br/&gt;
             &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;strong&amp;gt;Shuffle Read: &amp;lt;/strong&amp;gt;&lt;br/&gt;
+              &amp;lt;strong&amp;gt;Shuffle Read Size / Records: &amp;lt;/strong&amp;gt;&lt;br/&gt;
               {s&quot;${Utils.bytesToString(stageData.shuffleReadBytes)} / &quot; +&lt;br/&gt;
                s&quot;${stageData.shuffleReadRecords}&quot;}&lt;br/&gt;
             &amp;lt;/li&amp;gt;&lt;br/&gt;
           }}&lt;br/&gt;
           {if (hasShuffleWrite(stageData)) {&lt;br/&gt;
             &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;strong&amp;gt;Shuffle Write: &amp;lt;/strong&amp;gt;&lt;br/&gt;
+              &amp;lt;strong&amp;gt;Shuffle Write Size / Records: &amp;lt;/strong&amp;gt;&lt;br/&gt;
                {s&quot;${Utils.bytesToString(stageData.shuffleWriteBytes)} / &quot; +&lt;br/&gt;
                s&quot;${stageData.shuffleWriteRecords}&quot;}&lt;br/&gt;
             &amp;lt;/li&amp;gt;&lt;br/&gt;
@@ -182,74 +196,17 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StagePage(parent: StagesTab, store: AppStatusStore) extends We&lt;br/&gt;
               {Utils.bytesToString(stageData.diskBytesSpilled)}&lt;br/&gt;
             &amp;lt;/li&amp;gt;&lt;br/&gt;
           }}&lt;br/&gt;
-        &amp;lt;/ul&amp;gt;&lt;br/&gt;
-      &amp;lt;/div&amp;gt;&lt;br/&gt;
-&lt;br/&gt;
-    val showAdditionalMetrics =&lt;br/&gt;
-      &amp;lt;div&amp;gt;&lt;br/&gt;
-        &amp;lt;span class=&quot;expand-additional-metrics&quot;&amp;gt;&lt;br/&gt;
-          &amp;lt;span class=&quot;expand-additional-metrics-arrow arrow-closed&quot;&amp;gt;&amp;lt;/span&amp;gt;&lt;br/&gt;
-          &amp;lt;a&amp;gt;Show Additional Metrics&amp;lt;/a&amp;gt;&lt;br/&gt;
-        &amp;lt;/span&amp;gt;&lt;br/&gt;
-        &amp;lt;div class=&quot;additional-metrics collapsed&quot;&amp;gt;&lt;br/&gt;
-          &amp;lt;ul&amp;gt;&lt;br/&gt;
-            &amp;lt;li&amp;gt;&lt;br/&gt;
-                &amp;lt;input type=&quot;checkbox&quot; id=&quot;select-all-metrics&quot;/&amp;gt;&lt;br/&gt;
-                &amp;lt;span class=&quot;additional-metric-title&quot;&amp;gt;&amp;lt;em&amp;gt;(De)select All&amp;lt;/em&amp;gt;&amp;lt;/span&amp;gt;&lt;br/&gt;
-            &amp;lt;/li&amp;gt;&lt;br/&gt;
+          {if (!stageJobIds.isEmpty) {&lt;br/&gt;
             &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;span data-toggle=&quot;tooltip&quot;&lt;br/&gt;
-                    title={ToolTips.SCHEDULER_DELAY} data-placement=&quot;right&quot;&amp;gt;&lt;br/&gt;
-                &amp;lt;input type=&quot;checkbox&quot; name={TaskDetailsClassNames.SCHEDULER_DELAY}/&amp;gt;&lt;br/&gt;
-                &amp;lt;span class=&quot;additional-metric-title&quot;&amp;gt;Scheduler Delay&amp;lt;/span&amp;gt;&lt;br/&gt;
-              &amp;lt;/span&amp;gt;&lt;br/&gt;
+              &amp;lt;strong&amp;gt;Associated Job Ids: &amp;lt;/strong&amp;gt;&lt;br/&gt;
+              {stageJobIds.sorted.map { jobId =&amp;gt;&lt;br/&gt;
+                val jobURL = &quot;%s/jobs/job/?id=%s&quot;&lt;br/&gt;
+                  .format(UIUtils.prependBaseUri(request, parent.basePath), jobId)&lt;br/&gt;
+                &amp;lt;a href={jobURL}&amp;gt;{jobId.toString}&amp;lt;/a&amp;gt;&amp;lt;span&amp;gt;&amp;nbsp;&amp;lt;/span&amp;gt;&lt;br/&gt;
+              }}&lt;br/&gt;
             &amp;lt;/li&amp;gt;&lt;br/&gt;
-            &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;span data-toggle=&quot;tooltip&quot;&lt;br/&gt;
-                    title={ToolTips.TASK_DESERIALIZATION_TIME} data-placement=&quot;right&quot;&amp;gt;&lt;br/&gt;
-                &amp;lt;input type=&quot;checkbox&quot; name={TaskDetailsClassNames.TASK_DESERIALIZATION_TIME}/&amp;gt;&lt;br/&gt;
-                &amp;lt;span class=&quot;additional-metric-title&quot;&amp;gt;Task Deserialization Time&amp;lt;/span&amp;gt;&lt;br/&gt;
-              &amp;lt;/span&amp;gt;&lt;br/&gt;
-            &amp;lt;/li&amp;gt;&lt;br/&gt;
-            {if (stageData.shuffleReadBytes &amp;gt; 0) {&lt;br/&gt;
-              &amp;lt;li&amp;gt;&lt;br/&gt;
-                &amp;lt;span data-toggle=&quot;tooltip&quot;&lt;br/&gt;
-                      title={ToolTips.SHUFFLE_READ_BLOCKED_TIME} data-placement=&quot;right&quot;&amp;gt;&lt;br/&gt;
-                  &amp;lt;input type=&quot;checkbox&quot; name={TaskDetailsClassNames.SHUFFLE_READ_BLOCKED_TIME}/&amp;gt;&lt;br/&gt;
-                  &amp;lt;span class=&quot;additional-metric-title&quot;&amp;gt;Shuffle Read Blocked Time&amp;lt;/span&amp;gt;&lt;br/&gt;
-                &amp;lt;/span&amp;gt;&lt;br/&gt;
-              &amp;lt;/li&amp;gt;&lt;br/&gt;
-              &amp;lt;li&amp;gt;&lt;br/&gt;
-                &amp;lt;span data-toggle=&quot;tooltip&quot;&lt;br/&gt;
-                      title={ToolTips.SHUFFLE_READ_REMOTE_SIZE} data-placement=&quot;right&quot;&amp;gt;&lt;br/&gt;
-                  &amp;lt;input type=&quot;checkbox&quot; name={TaskDetailsClassNames.SHUFFLE_READ_REMOTE_SIZE}/&amp;gt;&lt;br/&gt;
-                  &amp;lt;span class=&quot;additional-metric-title&quot;&amp;gt;Shuffle Remote Reads&amp;lt;/span&amp;gt;&lt;br/&gt;
-                &amp;lt;/span&amp;gt;&lt;br/&gt;
-              &amp;lt;/li&amp;gt;&lt;br/&gt;
-            }}&lt;br/&gt;
-            &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;span data-toggle=&quot;tooltip&quot;&lt;br/&gt;
-                    title={ToolTips.RESULT_SERIALIZATION_TIME} data-placement=&quot;right&quot;&amp;gt;&lt;br/&gt;
-                &amp;lt;input type=&quot;checkbox&quot; name={TaskDetailsClassNames.RESULT_SERIALIZATION_TIME}/&amp;gt;&lt;br/&gt;
-                &amp;lt;span class=&quot;additional-metric-title&quot;&amp;gt;Result Serialization Time&amp;lt;/span&amp;gt;&lt;br/&gt;
-              &amp;lt;/span&amp;gt;&lt;br/&gt;
-            &amp;lt;/li&amp;gt;&lt;br/&gt;
-            &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;span data-toggle=&quot;tooltip&quot;&lt;br/&gt;
-                    title={ToolTips.GETTING_RESULT_TIME} data-placement=&quot;right&quot;&amp;gt;&lt;br/&gt;
-                &amp;lt;input type=&quot;checkbox&quot; name={TaskDetailsClassNames.GETTING_RESULT_TIME}/&amp;gt;&lt;br/&gt;
-                &amp;lt;span class=&quot;additional-metric-title&quot;&amp;gt;Getting Result Time&amp;lt;/span&amp;gt;&lt;br/&gt;
-              &amp;lt;/span&amp;gt;&lt;br/&gt;
-            &amp;lt;/li&amp;gt;&lt;br/&gt;
-            &amp;lt;li&amp;gt;&lt;br/&gt;
-              &amp;lt;span data-toggle=&quot;tooltip&quot;&lt;br/&gt;
-                    title={ToolTips.PEAK_EXECUTION_MEMORY} data-placement=&quot;right&quot;&amp;gt;&lt;br/&gt;
-                &amp;lt;input type=&quot;checkbox&quot; name={TaskDetailsClassNames.PEAK_EXECUTION_MEMORY}/&amp;gt;&lt;br/&gt;
-                &amp;lt;span class=&quot;additional-metric-title&quot;&amp;gt;Peak Execution Memory&amp;lt;/span&amp;gt;&lt;br/&gt;
-              &amp;lt;/span&amp;gt;&lt;br/&gt;
-            &amp;lt;/li&amp;gt;&lt;br/&gt;
-          &amp;lt;/ul&amp;gt;&lt;br/&gt;
-        &amp;lt;/div&amp;gt;&lt;br/&gt;
+          }}&lt;br/&gt;
+        &amp;lt;/ul&amp;gt;&lt;br/&gt;
       &amp;lt;/div&amp;gt;&lt;br/&gt;
 &lt;br/&gt;
     val stageGraph = parent.store.asOption(parent.store.operationGraphForStage(stageId))&lt;br/&gt;
@@ -268,17 +225,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StagePage(parent: StagesTab, store: AppStatusStore) extends We&lt;br/&gt;
       accumulableRow,&lt;br/&gt;
       stageData.accumulatorUpdates.toSeq)&lt;br/&gt;
 &lt;br/&gt;
-    val page: Int = {&lt;br/&gt;
-      // If the user has changed to a larger page size, then go to page 1 in order to avoid&lt;br/&gt;
-      // IndexOutOfBoundsException.&lt;br/&gt;
-      if (taskPageSize &amp;lt;= taskPrevPageSize) {
-        taskPage
-      } else {-        1-      }&lt;br/&gt;
-    }&lt;br/&gt;
     val currentTime = System.currentTimeMillis()&lt;br/&gt;
-    val (taskTable, taskTableHTML) = try {&lt;br/&gt;
+    val taskTable = try {
       val _taskTable = new TaskPagedTable(
         stageData,
         UIUtils.prependBaseUri(request, parent.basePath) +
@@ -289,17 +237,10 @@ private[ui] class StagePage(parent: StagesTab, store: AppStatusStore) extends We
         desc = taskSortDesc,
         store = parent.store
       )
-      (_taskTable, _taskTable.table(page))
+      _taskTable
     } catch {&lt;br/&gt;
       case e @ (_ : IllegalArgumentException | _ : IndexOutOfBoundsException) =&amp;gt;&lt;br/&gt;
-        val errorMessage =&lt;br/&gt;
-          &amp;lt;div class=&quot;alert alert-error&quot;&amp;gt;&lt;br/&gt;
-            &amp;lt;p&amp;gt;Error while rendering stage table:&amp;lt;/p&amp;gt;&lt;br/&gt;
-            &amp;lt;pre&amp;gt;&lt;br/&gt;
-              {Utils.exceptionString(e)}&lt;br/&gt;
-            &amp;lt;/pre&amp;gt;&lt;br/&gt;
-          &amp;lt;/div&amp;gt;&lt;br/&gt;
-        (null, errorMessage)&lt;br/&gt;
+        null&lt;br/&gt;
     }&lt;br/&gt;
 &lt;br/&gt;
     val jsForScrollingDownToTaskTable =&lt;br/&gt;
@@ -317,190 +258,36 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StagePage(parent: StagesTab, store: AppStatusStore) extends We&lt;br/&gt;
         }&lt;br/&gt;
       &amp;lt;/script&amp;gt;&lt;br/&gt;
 &lt;br/&gt;
-    val metricsSummary = store.taskSummary(stageData.stageId, stageData.attemptId,&lt;br/&gt;
-      Array(0, 0.25, 0.5, 0.75, 1.0))&lt;br/&gt;
-&lt;br/&gt;
-    val summaryTable = metricsSummary.map { metrics =&amp;gt;&lt;br/&gt;
-      def timeQuantiles(data: IndexedSeq&lt;span class=&quot;error&quot;&gt;&amp;#91;Double&amp;#93;&lt;/span&gt;): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-        data.map { millis =&amp;gt;&lt;br/&gt;
-          &amp;lt;td&amp;gt;{UIUtils.formatDuration(millis.toLong)}&amp;lt;/td&amp;gt;&lt;br/&gt;
-        }&lt;br/&gt;
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def sizeQuantiles(data: IndexedSeq&lt;span class=&quot;error&quot;&gt;&amp;#91;Double&amp;#93;&lt;/span&gt;): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-        data.map { size =&amp;gt;&lt;br/&gt;
-          &amp;lt;td&amp;gt;{Utils.bytesToString(size.toLong)}&amp;lt;/td&amp;gt;&lt;br/&gt;
-        }&lt;br/&gt;
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def sizeQuantilesWithRecords(&lt;br/&gt;
-          data: IndexedSeq&lt;span class=&quot;error&quot;&gt;&amp;#91;Double&amp;#93;&lt;/span&gt;,&lt;br/&gt;
-          records: IndexedSeq&lt;span class=&quot;error&quot;&gt;&amp;#91;Double&amp;#93;&lt;/span&gt;) : Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-        data.zip(records).map { case (d, r) =&amp;gt;&lt;br/&gt;
-          &amp;lt;td&amp;gt;{s&quot;${Utils.bytesToString(d.toLong)} / ${r.toLong}&quot;}&amp;lt;/td&amp;gt;&lt;br/&gt;
-        }&lt;br/&gt;
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def titleCell(title: String, tooltip: String): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
-        &amp;lt;td&amp;gt;&lt;br/&gt;
-          &amp;lt;span data-toggle=&quot;tooltip&quot; title={tooltip} data-placement=&quot;right&quot;&amp;gt;&lt;br/&gt;
-            {title}&lt;br/&gt;
-          &amp;lt;/span&amp;gt;&lt;br/&gt;
-        &amp;lt;/td&amp;gt;&lt;br/&gt;
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def simpleTitleCell(title: String): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = &amp;lt;td&amp;gt;{title}&amp;lt;/td&amp;gt;&lt;br/&gt;
-&lt;br/&gt;
-      val deserializationQuantiles = titleCell(&quot;Task Deserialization Time&quot;,&lt;br/&gt;
-        ToolTips.TASK_DESERIALIZATION_TIME) ++ timeQuantiles(metrics.executorDeserializeTime)&lt;br/&gt;
-&lt;br/&gt;
-      val serviceQuantiles = simpleTitleCell(&quot;Duration&quot;) ++ timeQuantiles(metrics.executorRunTime)&lt;br/&gt;
-&lt;br/&gt;
-      val gcQuantiles = titleCell(&quot;GC Time&quot;, ToolTips.GC_TIME) ++ timeQuantiles(metrics.jvmGcTime)&lt;br/&gt;
-&lt;br/&gt;
-      val serializationQuantiles = titleCell(&quot;Result Serialization Time&quot;,&lt;br/&gt;
-        ToolTips.RESULT_SERIALIZATION_TIME) ++ timeQuantiles(metrics.resultSerializationTime)&lt;br/&gt;
-&lt;br/&gt;
-      val gettingResultQuantiles = titleCell(&quot;Getting Result Time&quot;, ToolTips.GETTING_RESULT_TIME) ++&lt;br/&gt;
-        timeQuantiles(metrics.gettingResultTime)&lt;br/&gt;
-&lt;br/&gt;
-      val peakExecutionMemoryQuantiles = titleCell(&quot;Peak Execution Memory&quot;,&lt;br/&gt;
-        ToolTips.PEAK_EXECUTION_MEMORY) ++ sizeQuantiles(metrics.peakExecutionMemory)&lt;br/&gt;
-&lt;br/&gt;
-      // The scheduler delay includes the network delay to send the task to the worker&lt;br/&gt;
-      // machine and to send back the result (but not the time to fetch the task result,&lt;br/&gt;
-      // if it needed to be fetched from the block manager on the worker).&lt;br/&gt;
-      val schedulerDelayQuantiles = titleCell(&quot;Scheduler Delay&quot;, ToolTips.SCHEDULER_DELAY) ++&lt;br/&gt;
-        timeQuantiles(metrics.schedulerDelay)&lt;br/&gt;
-&lt;br/&gt;
-      def inputQuantiles: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {
-        simpleTitleCell(&quot;Input Size / Records&quot;) ++
-          sizeQuantilesWithRecords(metrics.inputMetrics.bytesRead, metrics.inputMetrics.recordsRead)
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def outputQuantiles: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {
-        simpleTitleCell(&quot;Output Size / Records&quot;) ++
-          sizeQuantilesWithRecords(metrics.outputMetrics.bytesWritten,
-            metrics.outputMetrics.recordsWritten)
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def shuffleReadBlockedQuantiles: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {
-        titleCell(&quot;Shuffle Read Blocked Time&quot;, ToolTips.SHUFFLE_READ_BLOCKED_TIME) ++
-          timeQuantiles(metrics.shuffleReadMetrics.fetchWaitTime)
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def shuffleReadTotalQuantiles: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {
-        titleCell(&quot;Shuffle Read Size / Records&quot;, ToolTips.SHUFFLE_READ) ++
-          sizeQuantilesWithRecords(metrics.shuffleReadMetrics.readBytes,
-            metrics.shuffleReadMetrics.readRecords)
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def shuffleReadRemoteQuantiles: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {
-        titleCell(&quot;Shuffle Remote Reads&quot;, ToolTips.SHUFFLE_READ_REMOTE_SIZE) ++
-          sizeQuantiles(metrics.shuffleReadMetrics.remoteBytesRead)
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def shuffleWriteQuantiles: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {
-        simpleTitleCell(&quot;Shuffle Write Size / Records&quot;) ++
-          sizeQuantilesWithRecords(metrics.shuffleWriteMetrics.writeBytes,
-            metrics.shuffleWriteMetrics.writeRecords)
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def memoryBytesSpilledQuantiles: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {
-        simpleTitleCell(&quot;Shuffle spill (memory)&quot;) ++ sizeQuantiles(metrics.memoryBytesSpilled)
-      }&lt;br/&gt;
-&lt;br/&gt;
-      def diskBytesSpilledQuantiles: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {
-        simpleTitleCell(&quot;Shuffle spill (disk)&quot;) ++ sizeQuantiles(metrics.diskBytesSpilled)
-      }&lt;br/&gt;
-&lt;br/&gt;
-      val listings: Seq[Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt;] = Seq(&lt;br/&gt;
-        &amp;lt;tr&amp;gt;{serviceQuantiles}&amp;lt;/tr&amp;gt;,&lt;br/&gt;
-        &amp;lt;tr class={TaskDetailsClassNames.SCHEDULER_DELAY}&amp;gt;{schedulerDelayQuantiles}&amp;lt;/tr&amp;gt;,&lt;br/&gt;
-        &amp;lt;tr class={TaskDetailsClassNames.TASK_DESERIALIZATION_TIME}&amp;gt;&lt;br/&gt;
-          {deserializationQuantiles}&lt;br/&gt;
-        &amp;lt;/tr&amp;gt;&lt;br/&gt;
-        &amp;lt;tr&amp;gt;{gcQuantiles}&amp;lt;/tr&amp;gt;,&lt;br/&gt;
-        &amp;lt;tr class={TaskDetailsClassNames.RESULT_SERIALIZATION_TIME}&amp;gt;&lt;br/&gt;
-          {serializationQuantiles}&lt;br/&gt;
-        &amp;lt;/tr&amp;gt;,&lt;br/&gt;
-        &amp;lt;tr class={TaskDetailsClassNames.GETTING_RESULT_TIME}&amp;gt;{gettingResultQuantiles}&amp;lt;/tr&amp;gt;,&lt;br/&gt;
-        &amp;lt;tr class={TaskDetailsClassNames.PEAK_EXECUTION_MEMORY}&amp;gt;&lt;br/&gt;
-          {peakExecutionMemoryQuantiles}&lt;br/&gt;
-        &amp;lt;/tr&amp;gt;,&lt;br/&gt;
-        if (hasInput(stageData)) &amp;lt;tr&amp;gt;{inputQuantiles}&amp;lt;/tr&amp;gt; else Nil,&lt;br/&gt;
-        if (hasOutput(stageData)) &amp;lt;tr&amp;gt;{outputQuantiles}&amp;lt;/tr&amp;gt; else Nil,&lt;br/&gt;
-        if (hasShuffleRead(stageData)) {&lt;br/&gt;
-          &amp;lt;tr class={TaskDetailsClassNames.SHUFFLE_READ_BLOCKED_TIME}&amp;gt;&lt;br/&gt;
-            {shuffleReadBlockedQuantiles}&lt;br/&gt;
-          &amp;lt;/tr&amp;gt;&lt;br/&gt;
-          &amp;lt;tr&amp;gt;{shuffleReadTotalQuantiles}&amp;lt;/tr&amp;gt;&lt;br/&gt;
-          &amp;lt;tr class={TaskDetailsClassNames.SHUFFLE_READ_REMOTE_SIZE}&amp;gt;&lt;br/&gt;
-            {shuffleReadRemoteQuantiles}&lt;br/&gt;
-          &amp;lt;/tr&amp;gt;&lt;br/&gt;
-        } else {
-          Nil
-        },&lt;br/&gt;
-        if (hasShuffleWrite(stageData)) &amp;lt;tr&amp;gt;{shuffleWriteQuantiles}&amp;lt;/tr&amp;gt; else Nil,&lt;br/&gt;
-        if (hasBytesSpilled(stageData)) &amp;lt;tr&amp;gt;{memoryBytesSpilledQuantiles}&amp;lt;/tr&amp;gt; else Nil,&lt;br/&gt;
-        if (hasBytesSpilled(stageData)) &amp;lt;tr&amp;gt;{diskBytesSpilledQuantiles}&amp;lt;/tr&amp;gt; else Nil)&lt;br/&gt;
-&lt;br/&gt;
-      val quantileHeaders = Seq(&quot;Metric&quot;, &quot;Min&quot;, &quot;25th percentile&quot;, &quot;Median&quot;, &quot;75th percentile&quot;,&lt;br/&gt;
-        &quot;Max&quot;)&lt;br/&gt;
-      // The summary table does not use CSS to stripe rows, which doesn&apos;t work with hidden&lt;br/&gt;
-      // rows (instead, JavaScript in table.js is used to stripe the non-hidden rows).&lt;br/&gt;
-      UIUtils.listingTable(&lt;br/&gt;
-        quantileHeaders,&lt;br/&gt;
-        identity[Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt;],&lt;br/&gt;
-        listings,&lt;br/&gt;
-        fixedWidth = true,&lt;br/&gt;
-        id = Some(&quot;task-summary-table&quot;),&lt;br/&gt;
-        stripeRowsWithCss = false)&lt;br/&gt;
-    }&lt;br/&gt;
-&lt;br/&gt;
-    val executorTable = new ExecutorTable(stageData, parent.store)&lt;br/&gt;
-&lt;br/&gt;
-    val maybeAccumulableTable: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; =&lt;br/&gt;
-      if (hasAccumulators(stageData)) { &amp;lt;h4&amp;gt;Accumulators&amp;lt;/h4&amp;gt; ++ accumulableTable } else Seq()&lt;br/&gt;
-&lt;br/&gt;
-    val aggMetrics =&lt;br/&gt;
-      &amp;lt;span class=&quot;collapse-aggregated-metrics collapse-table&quot;&lt;br/&gt;
-            onClick=&quot;collapseTable(&apos;collapse-aggregated-metrics&apos;,&apos;aggregated-metrics&apos;)&quot;&amp;gt;&lt;br/&gt;
-        &amp;lt;h4&amp;gt;&lt;br/&gt;
-          &amp;lt;span class=&quot;collapse-table-arrow arrow-open&quot;&amp;gt;&amp;lt;/span&amp;gt;&lt;br/&gt;
-          &amp;lt;a&amp;gt;Aggregated Metrics by Executor&amp;lt;/a&amp;gt;&lt;br/&gt;
-        &amp;lt;/h4&amp;gt;&lt;br/&gt;
-      &amp;lt;/span&amp;gt;&lt;br/&gt;
-      &amp;lt;div class=&quot;aggregated-metrics collapsible-table&quot;&amp;gt;&lt;br/&gt;
-        {executorTable.toNodeSeq}&lt;br/&gt;
-      &amp;lt;/div&amp;gt;&lt;br/&gt;
-&lt;br/&gt;
     val content =&lt;br/&gt;
       summary ++&lt;br/&gt;
-      dagViz ++&lt;br/&gt;
-      showAdditionalMetrics ++&lt;br/&gt;
+      dagViz ++ &amp;lt;div id=&quot;showAdditionalMetrics&quot;&amp;gt;&amp;lt;/div&amp;gt; ++&lt;br/&gt;
       makeTimeline(&lt;br/&gt;
         // Only show the tasks in the table&lt;br/&gt;
-        Option(taskTable).map(_.dataSource.tasks).getOrElse(Nil),&lt;br/&gt;
-        currentTime) ++&lt;br/&gt;
-      &amp;lt;h4&amp;gt;Summary Metrics for &amp;lt;a href=&quot;#tasks-section&quot;&amp;gt;{numCompleted} Completed Tasks&amp;lt;/a&amp;gt;&amp;lt;/h4&amp;gt; ++&lt;br/&gt;
-      &amp;lt;div&amp;gt;{summaryTable.getOrElse(&quot;No tasks have reported metrics yet.&quot;)}&amp;lt;/div&amp;gt; ++&lt;br/&gt;
-      aggMetrics ++&lt;br/&gt;
-      maybeAccumulableTable ++&lt;br/&gt;
-      &amp;lt;span id=&quot;tasks-section&quot; class=&quot;collapse-aggregated-tasks collapse-table&quot;&lt;br/&gt;
-          onClick=&quot;collapseTable(&apos;collapse-aggregated-tasks&apos;,&apos;aggregated-tasks&apos;)&quot;&amp;gt;&lt;br/&gt;
-        &amp;lt;h4&amp;gt;&lt;br/&gt;
-          &amp;lt;span class=&quot;collapse-table-arrow arrow-open&quot;&amp;gt;&amp;lt;/span&amp;gt;&lt;br/&gt;
-          &amp;lt;a&amp;gt;Tasks ({totalTasksNumStr})&amp;lt;/a&amp;gt;&lt;br/&gt;
-        &amp;lt;/h4&amp;gt;&lt;br/&gt;
-      &amp;lt;/span&amp;gt; ++&lt;br/&gt;
-      &amp;lt;div class=&quot;aggregated-tasks collapsible-table&quot;&amp;gt;&lt;br/&gt;
-        {taskTableHTML ++ jsForScrollingDownToTaskTable}&lt;br/&gt;
-      &amp;lt;/div&amp;gt;&lt;br/&gt;
-    UIUtils.headerSparkPage(request, stageHeader, content, parent, showVisualization = true)&lt;br/&gt;
+        Option(taskTable).map({ taskPagedTable =&amp;gt;
+          val from = (eventTimelineTaskPage - 1) * eventTimelineTaskPageSize
+          val to = taskPagedTable.dataSource.dataSize.min(
+            eventTimelineTaskPage * eventTimelineTaskPageSize)
+          taskPagedTable.dataSource.sliceData(from, to)}).getOrElse(Nil), currentTime,&lt;br/&gt;
+        eventTimelineTaskPage, eventTimelineTaskPageSize, eventTimelineTotalPages, stageId,&lt;br/&gt;
+        stageAttemptId, totalTasks) ++&lt;br/&gt;
+        &amp;lt;div id=&quot;parent-container&quot;&amp;gt;&lt;br/&gt;
+          &amp;lt;script src={UIUtils.prependBaseUri(request, &quot;/static/utils.js&quot;)}&amp;gt;&amp;lt;/script&amp;gt;&lt;br/&gt;
+          &amp;lt;script src={UIUtils.prependBaseUri(request, &quot;/static/stagepage.js&quot;)}&amp;gt;&amp;lt;/script&amp;gt;&lt;br/&gt;
+        &amp;lt;/div&amp;gt;&lt;br/&gt;
+        UIUtils.headerSparkPage(request, stageHeader, content, parent, showVisualization = true,&lt;br/&gt;
+          useDataTables = true)&lt;br/&gt;
+&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  def makeTimeline(tasks: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskData&amp;#93;&lt;/span&gt;, currentTime: Long): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
+  def makeTimeline(&lt;br/&gt;
+      tasks: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskData&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+      currentTime: Long,&lt;br/&gt;
+      page: Int,&lt;br/&gt;
+      pageSize: Int,&lt;br/&gt;
+      totalPages: Int,&lt;br/&gt;
+      stageId: Int,&lt;br/&gt;
+      stageAttemptId: Int,&lt;br/&gt;
+      totalTasks: Int): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Node&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     val executorsSet = new HashSet&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, String)&amp;#93;&lt;/span&gt;&lt;br/&gt;
     var minLaunchTime = Long.MaxValue&lt;br/&gt;
     var maxFinishTime = Long.MinValue&lt;br/&gt;
@@ -659,6 +446,31 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StagePage(parent: StagesTab, store: AppStatusStore) extends We&lt;br/&gt;
           &amp;lt;input type=&quot;checkbox&quot;&amp;gt;&amp;lt;/input&amp;gt;&lt;br/&gt;
           &amp;lt;span&amp;gt;Enable zooming&amp;lt;/span&amp;gt;&lt;br/&gt;
         &amp;lt;/div&amp;gt;&lt;br/&gt;
+        &amp;lt;div&amp;gt;&lt;br/&gt;
+          &amp;lt;form id={s&quot;form-event-timeline-page&quot;}&lt;br/&gt;
+                method=&quot;get&quot;&lt;br/&gt;
+                action=&quot;&quot;&lt;br/&gt;
+                class=&quot;form-inline pull-right&quot;&lt;br/&gt;
+                style=&quot;margin-bottom: 0px;&quot;&amp;gt;&lt;br/&gt;
+            &amp;lt;label&amp;gt;Tasks: {totalTasks}. {totalPages}
&lt;p&gt; Pages. Jump to&amp;lt;/label&amp;gt;&lt;br/&gt;
+            &amp;lt;input type=&quot;hidden&quot; name=&quot;id&quot; value=&lt;/p&gt;
{stageId.toString}
&lt;p&gt; /&amp;gt;&lt;br/&gt;
+            &amp;lt;input type=&quot;hidden&quot; name=&quot;attempt&quot; value=&lt;/p&gt;
{stageAttemptId.toString}
&lt;p&gt; /&amp;gt;&lt;br/&gt;
+            &amp;lt;input type=&quot;text&quot;&lt;br/&gt;
+                   name=&quot;task.eventTimelinePageNumber&quot;&lt;br/&gt;
+                   id=&lt;/p&gt;
{s&quot;form-event-timeline-page-no&quot;}
&lt;p&gt;+                   value=&lt;/p&gt;
{page.toString}
&lt;p&gt; class=&quot;span1&quot; /&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+            &amp;lt;label&amp;gt;. Show &amp;lt;/label&amp;gt;&lt;br/&gt;
+            &amp;lt;input type=&quot;text&quot;&lt;br/&gt;
+                   id=&lt;/p&gt;
{s&quot;form-event-timeline-page-size&quot;}
&lt;p&gt;+                   name=&quot;task.eventTimelinePageSize&quot;&lt;br/&gt;
+                   value=&lt;/p&gt;
{pageSize.toString}
&lt;p&gt;+                   class=&quot;span1&quot; /&amp;gt;&lt;br/&gt;
+            &amp;lt;label&amp;gt;items in a page.&amp;lt;/label&amp;gt;&lt;br/&gt;
+&lt;br/&gt;
+            &amp;lt;button type=&quot;submit&quot; class=&quot;btn&quot;&amp;gt;Go&amp;lt;/button&amp;gt;&lt;br/&gt;
+          &amp;lt;/form&amp;gt;&lt;br/&gt;
+        &amp;lt;/div&amp;gt;&lt;br/&gt;
       &amp;lt;/div&amp;gt;&lt;/p&gt;
       {TIMELINE_LEGEND}
&lt;p&gt;     &amp;lt;/div&amp;gt; ++&lt;br/&gt;
@@ -685,7 +497,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class TaskDataSource(&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private var _tasksToShow: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskData&amp;#93;&lt;/span&gt; = null&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def dataSize: Int = taskCount(stage)&lt;br/&gt;
+  override def dataSize: Int = store.taskCount(stage.stageId, stage.attemptId).toInt&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   override def sliceData(from: Int, to: Int): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;TaskData&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     if (_tasksToShow == null) {&lt;br/&gt;
@@ -722,8 +534,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class TaskPagedTable(&lt;/p&gt;

&lt;p&gt;   override def pageSizeFormField: String = &quot;task.pageSize&quot;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def prevPageSizeFormField: String = &quot;task.prevPageSize&quot;&lt;br/&gt;
-&lt;br/&gt;
   override def pageNumberFormField: String = &quot;task.page&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   override val dataSource: TaskDataSource = new TaskDataSource(&lt;br/&gt;
@@ -847,7 +657,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class TaskPagedTable(&lt;br/&gt;
         &amp;lt;/div&amp;gt;&lt;br/&gt;
       &amp;lt;/td&amp;gt;&lt;br/&gt;
       &amp;lt;td&amp;gt;&lt;/p&gt;
{UIUtils.formatDate(task.launchTime)}
&lt;p&gt;&amp;lt;/td&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;td&amp;gt;
{formatDuration(task.duration)}
&lt;p&gt;&amp;lt;/td&amp;gt;&lt;br/&gt;
+      &amp;lt;td&amp;gt;&lt;/p&gt;
{formatDuration(task.taskMetrics.map(_.executorRunTime))}
&lt;p&gt;&amp;lt;/td&amp;gt;&lt;br/&gt;
       &amp;lt;td class=&lt;/p&gt;
{TaskDetailsClassNames.SCHEDULER_DELAY}
&lt;p&gt;&amp;gt;&lt;/p&gt;
         {UIUtils.formatDuration(AppStatusUtils.schedulerDelay(task))}
&lt;p&gt;       &amp;lt;/td&amp;gt;&lt;br/&gt;
@@ -962,7 +772,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class TaskPagedTable(&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; object ApiHelper {&lt;br/&gt;
+private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object ApiHelper {&lt;/p&gt;

&lt;p&gt;   val HEADER_ID = &quot;ID&quot;&lt;br/&gt;
   val HEADER_TASK_INDEX = &quot;Index&quot;&lt;br/&gt;
@@ -1000,7 +810,9 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; object ApiHelper {&lt;br/&gt;
     HEADER_EXECUTOR -&amp;gt; TaskIndexNames.EXECUTOR,&lt;br/&gt;
     HEADER_HOST -&amp;gt; TaskIndexNames.HOST,&lt;br/&gt;
     HEADER_LAUNCH_TIME -&amp;gt; TaskIndexNames.LAUNCH_TIME,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;HEADER_DURATION -&amp;gt; TaskIndexNames.DURATION,&lt;br/&gt;
+    // &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26109&quot; title=&quot;Duration in the task summary metrics table and the task table are different&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26109&quot;&gt;&lt;del&gt;SPARK-26109&lt;/del&gt;&lt;/a&gt;: Duration of task as executorRunTime to make it consistent with the&lt;br/&gt;
+    // aggregated tasks summary metrics table and the previous versions of Spark.&lt;br/&gt;
+    HEADER_DURATION -&amp;gt; TaskIndexNames.EXEC_RUN_TIME,&lt;br/&gt;
     HEADER_SCHEDULER_DELAY -&amp;gt; TaskIndexNames.SCHEDULER_DELAY,&lt;br/&gt;
     HEADER_DESER_TIME -&amp;gt; TaskIndexNames.DESER_TIME,&lt;br/&gt;
     HEADER_GC_TIME -&amp;gt; TaskIndexNames.GC_TIME,&lt;br/&gt;
@@ -1047,13 +859,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; object ApiHelper {&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def lastStageNameAndDescription(store: AppStatusStore, job: JobData): (String, String) = &lt;/p&gt;
{
-    val stage = store.asOption(store.stageAttempt(job.stageIds.max, 0))
+    val stage = store.asOption(store.stageAttempt(job.stageIds.max, 0)._1)
     (stage.map(_.name).getOrElse(&quot;&quot;), stage.flatMap(_.description).getOrElse(job.name))
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def taskCount(stageData: StageData): Int = 
{
-    stageData.numActiveTasks + stageData.numCompleteTasks + stageData.numFailedTasks +
-      stageData.numKilledTasks
-  }
&lt;p&gt;-&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala b/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala&lt;br/&gt;
index d01acdae59c9f..766efc15e26ba 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/ui/jobs/StageTable.scala&lt;br/&gt;
@@ -53,8 +53,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StageTableBase(&lt;br/&gt;
   val parameterStageSortColumn = UIUtils.stripXSS(request.getParameter(stageTag + &quot;.sort&quot;))&lt;br/&gt;
   val parameterStageSortDesc = UIUtils.stripXSS(request.getParameter(stageTag + &quot;.desc&quot;))&lt;br/&gt;
   val parameterStagePageSize = UIUtils.stripXSS(request.getParameter(stageTag + &quot;.pageSize&quot;))&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;val parameterStagePrevPageSize =&lt;/li&gt;
	&lt;li&gt;UIUtils.stripXSS(request.getParameter(stageTag + &quot;.prevPageSize&quot;))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   val stagePage = Option(parameterStagePage).map(_.toInt).getOrElse(1)&lt;br/&gt;
   val stageSortColumn = Option(parameterStageSortColumn).map { sortColumn =&amp;gt;&lt;br/&gt;
@@ -65,18 +63,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StageTableBase(&lt;br/&gt;
     stageSortColumn == &quot;Stage Id&quot;&lt;br/&gt;
   )&lt;br/&gt;
   val stagePageSize = Option(parameterStagePageSize).map(_.toInt).getOrElse(100)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val stagePrevPageSize = Option(parameterStagePrevPageSize).map(_.toInt)&lt;/li&gt;
	&lt;li&gt;.getOrElse(stagePageSize)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;val page: Int = {&lt;/li&gt;
	&lt;li&gt;// If the user has changed to a larger page size, then go to page 1 in order to avoid&lt;/li&gt;
	&lt;li&gt;// IndexOutOfBoundsException.&lt;/li&gt;
	&lt;li&gt;if (stagePageSize &amp;lt;= stagePrevPageSize) 
{
-      stagePage
-    }
&lt;p&gt; else &lt;/p&gt;
{
-      1
-    }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
+&lt;br/&gt;
   val currentTime = System.currentTimeMillis()&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   val toNodeSeq = try &lt;/p&gt;
{
@@ -96,7 +83,7 @@ private[ui] class StageTableBase(
       isFailedStage,
       parameterOtherTable,
       request
-    ).table(page)
+    ).table(stagePage)
   }
&lt;p&gt; catch {&lt;br/&gt;
     case e @ (_ : IllegalArgumentException | _ : IndexOutOfBoundsException) =&amp;gt;&lt;br/&gt;
       &amp;lt;div class=&quot;alert alert-error&quot;&amp;gt;&lt;br/&gt;
@@ -161,8 +148,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StagePagedTable(&lt;/p&gt;

&lt;p&gt;   override def pageSizeFormField: String = stageTag + &quot;.pageSize&quot;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def prevPageSizeFormField: String = stageTag + &quot;.prevPageSize&quot;&lt;br/&gt;
-&lt;br/&gt;
   override def pageNumberFormField: String = stageTag + &quot;.page&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   val parameterPath = UIUtils.prependBaseUri(request, basePath) + s&quot;/$subPath/?&quot; +&lt;br/&gt;
@@ -383,7 +368,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StagePagedTable(&lt;br/&gt;
         {if (cachedRddInfos.nonEmpty) {&lt;br/&gt;
           Text(&quot;RDD: &quot;) ++&lt;br/&gt;
           cachedRddInfos.map { i =&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;a href={s&quot;$basePathUri/storage/rdd?id=${i.id}&quot;}&amp;gt;
{i.name}&amp;lt;/a&amp;gt;&lt;br/&gt;
+            &amp;lt;a href={s&quot;$basePathUri/storage/rdd/?id=${i.id}&quot;}&amp;gt;{i.name}
&lt;p&gt;&amp;lt;/a&amp;gt;&lt;br/&gt;
           }&lt;br/&gt;
         }}&lt;br/&gt;
         &amp;lt;pre&amp;gt;&lt;/p&gt;
{s.details}
&lt;p&gt;&amp;lt;/pre&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala b/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala&lt;br/&gt;
index 238cd31433660..87da290c83057 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/ui/storage/RDDPage.scala&lt;br/&gt;
@@ -39,13 +39,11 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class RDDPage(parent: SparkUITab, store: AppStatusStore) extends Web&lt;br/&gt;
     val parameterBlockSortColumn = UIUtils.stripXSS(request.getParameter(&quot;block.sort&quot;))&lt;br/&gt;
     val parameterBlockSortDesc = UIUtils.stripXSS(request.getParameter(&quot;block.desc&quot;))&lt;br/&gt;
     val parameterBlockPageSize = UIUtils.stripXSS(request.getParameter(&quot;block.pageSize&quot;))&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;val parameterBlockPrevPageSize = UIUtils.stripXSS(request.getParameter(&quot;block.prevPageSize&quot;))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val blockPage = Option(parameterBlockPage).map(_.toInt).getOrElse(1)&lt;br/&gt;
     val blockSortColumn = Option(parameterBlockSortColumn).getOrElse(&quot;Block Name&quot;)&lt;br/&gt;
     val blockSortDesc = Option(parameterBlockSortDesc).map(_.toBoolean).getOrElse(false)&lt;br/&gt;
     val blockPageSize = Option(parameterBlockPageSize).map(_.toInt).getOrElse(100)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val blockPrevPageSize = Option(parameterBlockPrevPageSize).map(_.toInt).getOrElse(blockPageSize)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val rddId = parameterId.toInt&lt;br/&gt;
     val rddStorageInfo = try {&lt;br/&gt;
@@ -60,16 +58,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class RDDPage(parent: SparkUITab, store: AppStatusStore) extends Web&lt;br/&gt;
     val workerTable = UIUtils.listingTable(workerHeader, workerRow,&lt;br/&gt;
       rddStorageInfo.dataDistribution.get, id = Some(&quot;rdd-storage-by-worker-table&quot;))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Block table&lt;/li&gt;
	&lt;li&gt;val page: Int = {&lt;/li&gt;
	&lt;li&gt;// If the user has changed to a larger page size, then go to page 1 in order to avoid&lt;/li&gt;
	&lt;li&gt;// IndexOutOfBoundsException.&lt;/li&gt;
	&lt;li&gt;if (blockPageSize &amp;lt;= blockPrevPageSize) 
{
-        blockPage
-      }
&lt;p&gt; else &lt;/p&gt;
{
-        1
-      }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
     val blockTableHTML = try {&lt;br/&gt;
       val _blockTable = new BlockPagedTable(&lt;br/&gt;
         UIUtils.prependBaseUri(request, parent.basePath) + s&quot;/storage/rdd/?id=${rddId}&quot;,&lt;br/&gt;
@@ -78,7 +66,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class RDDPage(parent: SparkUITab, store: AppStatusStore) extends Web&lt;br/&gt;
         blockSortColumn,&lt;br/&gt;
         blockSortDesc,&lt;br/&gt;
         store.executorList(true))&lt;/li&gt;
	&lt;li&gt;_blockTable.table(page)&lt;br/&gt;
+      _blockTable.table(blockPage)&lt;br/&gt;
     } catch {&lt;br/&gt;
       case e @ (_ : IllegalArgumentException | _ : IndexOutOfBoundsException) =&amp;gt;&lt;br/&gt;
         &amp;lt;div class=&quot;alert alert-error&quot;&amp;gt;
{e.getMessage}
&lt;p&gt;&amp;lt;/div&amp;gt;&lt;br/&gt;
@@ -242,8 +230,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class BlockPagedTable(&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   override def pageSizeFormField: String = &quot;block.pageSize&quot;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;override def prevPageSizeFormField: String = &quot;block.prevPageSize&quot;&lt;br/&gt;
-&lt;br/&gt;
   override def pageNumberFormField: String = &quot;block.page&quot;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   override val dataSource: BlockDataSource = new BlockDataSource(&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala b/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala&lt;br/&gt;
index 3eb546e336e99..2488197814ffd 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/ui/storage/StoragePage.scala&lt;br/&gt;
@@ -78,7 +78,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;ui&amp;#93;&lt;/span&gt; class StoragePage(parent: SparkUITab, store: AppStatusStore) extends&lt;br/&gt;
     &amp;lt;tr&amp;gt;&lt;br/&gt;
       &amp;lt;td&amp;gt;&lt;/p&gt;
{rdd.id}
&lt;p&gt;&amp;lt;/td&amp;gt;&lt;br/&gt;
       &amp;lt;td&amp;gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&amp;lt;a href={&quot;%s/storage/rdd?id=%s&quot;.format(&lt;br/&gt;
+        &amp;lt;a href=
{&quot;%s/storage/rdd/?id=%s&quot;.format(
           UIUtils.prependBaseUri(request, parent.basePath), rdd.id)}
&lt;p&gt;&amp;gt;&lt;/p&gt;
           {rdd.name}
&lt;p&gt;         &amp;lt;/a&amp;gt;&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala b/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala&lt;br/&gt;
index bf618b4afbce0..d5b3ce36e742a 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/util/AccumulatorV2.scala&lt;br/&gt;
@@ -485,34 +485,3 @@ class CollectionAccumulator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt; extends AccumulatorV2[T, java.util.List&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;] 
{
     _list.addAll(newValue)
   }
&lt;p&gt; }&lt;br/&gt;
-&lt;br/&gt;
-&lt;br/&gt;
-class LegacyAccumulatorWrapper&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt;(&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;initialValue: R,&lt;/li&gt;
	&lt;li&gt;param: org.apache.spark.AccumulableParam&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt;) extends AccumulatorV2&lt;span class=&quot;error&quot;&gt;&amp;#91;T, R&amp;#93;&lt;/span&gt; {&lt;/li&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; var _value = initialValue  // Current value on driver&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;@transient private lazy val _zero = param.zero(initialValue)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;override def isZero: Boolean = _value.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;AnyRef&amp;#93;&lt;/span&gt;.eq(_zero.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;AnyRef&amp;#93;&lt;/span&gt;)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;override def copy(): LegacyAccumulatorWrapper&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt; = 
{
-    val acc = new LegacyAccumulatorWrapper(initialValue, param)
-    acc._value = _value
-    acc
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;override def reset(): Unit = 
{
-    _value = _zero
-  }
&lt;p&gt;-&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;override def add(v: T): Unit = _value = param.addAccumulator(_value, v)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;override def merge(other: AccumulatorV2&lt;span class=&quot;error&quot;&gt;&amp;#91;T, R&amp;#93;&lt;/span&gt;): Unit = other match {&lt;/li&gt;
	&lt;li&gt;case o: LegacyAccumulatorWrapper&lt;span class=&quot;error&quot;&gt;&amp;#91;R, T&amp;#93;&lt;/span&gt; =&amp;gt; _value = param.addInPlace(_value, o.value)&lt;/li&gt;
	&lt;li&gt;case _ =&amp;gt; throw new UnsupportedOperationException(&lt;/li&gt;
	&lt;li&gt;s&quot;Cannot merge ${this.getClass.getName} with ${other.getClass.getName}&quot;)&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;override def value: R = _value&lt;br/&gt;
-}&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala b/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala&lt;br/&gt;
index b6c300c4778b1..1b3e525644f00 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala&lt;br/&gt;
@@ -23,8 +23,8 @@ import java.lang.invoke.SerializedLambda&lt;br/&gt;
 import scala.collection.mutable.
{Map, Set, Stack}
&lt;p&gt; import scala.language.existentials&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-import org.apache.xbean.asm6.&lt;/p&gt;
{ClassReader, ClassVisitor, MethodVisitor, Type}&lt;br/&gt;
-import org.apache.xbean.asm6.Opcodes._&lt;br/&gt;
+import org.apache.xbean.asm7.{ClassReader, ClassVisitor, MethodVisitor, Type}
&lt;p&gt;+import org.apache.xbean.asm7.Opcodes._&lt;/p&gt;

&lt;p&gt; import org.apache.spark.&lt;/p&gt;
{SparkEnv, SparkException}
&lt;p&gt; import org.apache.spark.internal.Logging&lt;br/&gt;
@@ -175,7 +175,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object ClosureCleaner extends Logging {&lt;br/&gt;
       closure.getClass.isSynthetic &amp;amp;&amp;amp;&lt;br/&gt;
         closure&lt;br/&gt;
           .getClass&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;.getInterfaces.exists(_.getName.equals(&quot;scala.Serializable&quot;))&lt;br/&gt;
+          .getInterfaces.exists(_.getName == &quot;scala.Serializable&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     if (isClosureCandidate) {&lt;br/&gt;
       try {&lt;br/&gt;
@@ -285,8 +285,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object ClosureCleaner extends Logging {&lt;br/&gt;
         innerClasses.foreach { c =&amp;gt; logDebug(s&quot;     ${c.getName}&quot;) }&lt;br/&gt;
         logDebug(s&quot; + outer classes: ${outerClasses.size}&quot; )&lt;br/&gt;
         outerClasses.foreach { c =&amp;gt; logDebug(s&quot;     ${c.getName}&quot;) }&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;logDebug(s&quot; + outer objects: ${outerObjects.size}&quot;)&lt;/li&gt;
	&lt;li&gt;outerObjects.foreach 
{ o =&amp;gt; logDebug(s&quot;     $o&quot;) }
&lt;p&gt;       }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;       // Fail fast if we detect return statements in closures&lt;br/&gt;
@@ -318,19 +316,20 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object ClosureCleaner extends Logging {&lt;br/&gt;
       if (outerPairs.nonEmpty) {&lt;br/&gt;
         val (outermostClass, outermostObject) = outerPairs.head&lt;br/&gt;
         if (isClosure(outermostClass)) {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;logDebug(s&quot; + outermost object is a closure, so we clone it: ${outerPairs.head}&quot;)&lt;br/&gt;
+          logDebug(s&quot; + outermost object is a closure, so we clone it: ${outermostClass}&quot;)&lt;br/&gt;
         } else if (outermostClass.getName.startsWith(&quot;$line&quot;)) {&lt;br/&gt;
           // &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-14558&quot; title=&quot;In ClosureCleaner, clean the outer pointer if it&amp;#39;s a REPL line object&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-14558&quot;&gt;&lt;del&gt;SPARK-14558&lt;/del&gt;&lt;/a&gt;: if the outermost object is a REPL line object, we should clone&lt;br/&gt;
           // and clean it as it may carray a lot of unnecessary information,&lt;br/&gt;
           // e.g. hadoop conf, spark conf, etc.&lt;/li&gt;
	&lt;li&gt;logDebug(s&quot; + outermost object is a REPL line object, so we clone it: ${outerPairs.head}&quot;)&lt;br/&gt;
+          logDebug(s&quot; + outermost object is a REPL line object, so we clone it:&quot; +&lt;br/&gt;
+            s&quot; ${outermostClass}&quot;)&lt;br/&gt;
         } else {&lt;br/&gt;
           // The closure is ultimately nested inside a class; keep the object of that&lt;br/&gt;
           // class without cloning it since we don&apos;t want to clone the user&apos;s objects.&lt;br/&gt;
           // Note that we still need to keep around the outermost object itself because&lt;br/&gt;
           // we need it to clone its child closure later (see below).&lt;/li&gt;
	&lt;li&gt;logDebug(&quot; + outermost object is not a closure or REPL line object,&quot; +&lt;/li&gt;
	&lt;li&gt;&quot;so do not clone it: &quot; +  outerPairs.head)&lt;br/&gt;
+          logDebug(s&quot; + outermost object is not a closure or REPL line object,&quot; +&lt;br/&gt;
+            s&quot; so do not clone it: ${outermostClass}&quot;)&lt;br/&gt;
           parent = outermostObject // e.g. SparkContext&lt;br/&gt;
           outerPairs = outerPairs.tail&lt;br/&gt;
         }&lt;br/&gt;
@@ -341,7 +340,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object ClosureCleaner extends Logging {&lt;br/&gt;
       // Clone the closure objects themselves, nulling out any fields that are not&lt;br/&gt;
       // used in the closure we&apos;re working on or any of its inner closures.&lt;br/&gt;
       for ((cls, obj) &amp;lt;- outerPairs) {&lt;/li&gt;
	&lt;li&gt;logDebug(s&quot; + cloning the object $obj of class ${cls.getName}&quot;)&lt;br/&gt;
+        logDebug(s&quot; + cloning instance of class ${cls.getName}&quot;)&lt;br/&gt;
         // We null out these unused references by cloning each object and then filling in all&lt;br/&gt;
         // required fields from the original object. We need the parent here because the Java&lt;br/&gt;
         // language specification requires the first constructor parameter of any closure to be&lt;br/&gt;
@@ -351,7 +350,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object ClosureCleaner extends Logging {&lt;br/&gt;
         // If transitive cleaning is enabled, we recursively clean any enclosing closure using&lt;br/&gt;
         // the already populated accessed fields map of the starting closure&lt;br/&gt;
         if (cleanTransitively &amp;amp;&amp;amp; isClosure(clone.getClass)) {&lt;/li&gt;
	&lt;li&gt;logDebug(s&quot; + cleaning cloned closure $clone recursively (${cls.getName})&quot;)&lt;br/&gt;
+          logDebug(s&quot; + cleaning cloned closure recursively (${cls.getName})&quot;)&lt;br/&gt;
           // No need to check serializable here for the outer closures because we&apos;re&lt;br/&gt;
           // only interested in the serializability of the starting closure&lt;br/&gt;
           clean(clone, checkSerializable = false, cleanTransitively, accessedFields)&lt;br/&gt;
@@ -425,7 +424,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class ReturnStatementInClosureException&lt;br/&gt;
   extends SparkException(&quot;Return statements aren&apos;t allowed in Spark closures&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; private class ReturnStatementFinder(targetMethodName: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;extends ClassVisitor(ASM6) {&lt;br/&gt;
+  extends ClassVisitor(ASM7) {&lt;br/&gt;
   override def visitMethod(access: Int, name: String, desc: String,&lt;br/&gt;
       sig: String, exceptions: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): MethodVisitor = {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;@@ -439,7 +438,7 @@ private class ReturnStatementFinder(targetMethodName: Option&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = None)&lt;br/&gt;
       val isTargetMethod = targetMethodName.isEmpty ||&lt;br/&gt;
         name == targetMethodName.get || name == targetMethodName.get.stripSuffix(&quot;$adapted&quot;)&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new MethodVisitor(ASM6) {&lt;br/&gt;
+      new MethodVisitor(ASM7) {&lt;br/&gt;
         override def visitTypeInsn(op: Int, tp: String) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {           if (op == NEW &amp;amp;&amp;amp; tp.contains(&amp;quot;scala/runtime/NonLocalReturnControl&amp;quot;) &amp;amp;&amp;amp; isTargetMethod) {
             throw new ReturnStatementInClosureException
@@ -447,7 +446,7 @@ private class ReturnStatementFinder(targetMethodName: Option[String] = None)
         }       }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;     } else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;new MethodVisitor(ASM6) {}&lt;br/&gt;
+      new MethodVisitor(ASM7) {}&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
@@ -471,7 +470,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;util&amp;#93;&lt;/span&gt; class FieldAccessFinder(&lt;br/&gt;
     findTransitively: Boolean,&lt;br/&gt;
     specificMethod: Option[MethodIdentifier&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;] = None,&lt;br/&gt;
     visitedMethods: Set[MethodIdentifier&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;] = Set.empty)&lt;/li&gt;
	&lt;li&gt;extends ClassVisitor(ASM6) {&lt;br/&gt;
+  extends ClassVisitor(ASM7) 
{
 
   override def visitMethod(
       access: Int,
@@ -486,7 +485,7 @@ private[util] class FieldAccessFinder(
       return null
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new MethodVisitor(ASM6) {&lt;br/&gt;
+    new MethodVisitor(ASM7) {&lt;br/&gt;
       override def visitFieldInsn(op: Int, owner: String, name: String, desc: String) {&lt;br/&gt;
         if (op == GETFIELD) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {           for (cl &amp;lt;- fields.keys if cl.getName == owner.replace(&amp;#39;/&amp;#39;, &amp;#39;.&amp;#39;)) {
@@ -526,7 +525,7 @@ private[util] class FieldAccessFinder(
   } }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;-private class InnerClosureFinder(output: Set[Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;]) extends ClassVisitor(ASM6) {&lt;br/&gt;
+private class InnerClosureFinder(output: Set[Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;]) extends ClassVisitor(ASM7) {&lt;br/&gt;
   var myName: String = null&lt;/p&gt;

&lt;p&gt;   // TODO: Recursively find inner closures that we indirectly reference, e.g.&lt;br/&gt;
@@ -541,7 +540,7 @@ private class InnerClosureFinder(output: Set[Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;]) extends ClassVisitor(ASM&lt;/p&gt;

&lt;p&gt;   override def visitMethod(access: Int, name: String, desc: String,&lt;br/&gt;
       sig: String, exceptions: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;): MethodVisitor = {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;new MethodVisitor(ASM6) {&lt;br/&gt;
+    new MethodVisitor(ASM7) {&lt;br/&gt;
       override def visitMethodInsn(&lt;br/&gt;
           op: Int, owner: String, name: String, desc: String, itf: Boolean) {&lt;br/&gt;
         val argTypes = Type.getArgumentTypes(desc)&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala b/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala&lt;br/&gt;
index 21acaa95c5645..f4d6c7a28d2e4 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/util/CompletionIterator.scala&lt;br/&gt;
@@ -25,11 +25,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt;&lt;br/&gt;
 abstract class CompletionIterator[ +A, +I &amp;lt;: Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;A&amp;#93;&lt;/span&gt;](sub: I) extends Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;A&amp;#93;&lt;/span&gt; {&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var completed = false&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def next(): A = sub.next()&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;this&amp;#93;&lt;/span&gt; var iter = sub&lt;br/&gt;
+  def next(): A = iter.next()&lt;br/&gt;
   def hasNext: Boolean = {&lt;/li&gt;
	&lt;li&gt;val r = sub.hasNext&lt;br/&gt;
+    val r = iter.hasNext&lt;br/&gt;
     if (!r &amp;amp;&amp;amp; !completed) 
{
       completed = true
+      // reassign to release resources of highly resource consuming iterators early
+      iter = Iterator.empty.asInstanceOf[I]
       completion()
     }
&lt;p&gt;     r&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala b/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala&lt;br/&gt;
index 50c6461373dee..0cd8612b8fd1c 100644&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala&lt;br/&gt;
@@ -31,6 +31,7 @@ import org.json4s.jackson.JsonMethods._&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import org.apache.spark._&lt;br/&gt;
 import org.apache.spark.executor._&lt;br/&gt;
+import org.apache.spark.metrics.ExecutorMetricType&lt;br/&gt;
 import org.apache.spark.rdd.RDDOperationScope&lt;br/&gt;
 import org.apache.spark.scheduler._&lt;br/&gt;
 import org.apache.spark.scheduler.cluster.ExecutorInfo&lt;br/&gt;
@@ -98,6 +99,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object JsonProtocol {&lt;br/&gt;
         logStartToJson(logStart)&lt;br/&gt;
       case metricsUpdate: SparkListenerExecutorMetricsUpdate =&amp;gt;&lt;br/&gt;
         executorMetricsUpdateToJson(metricsUpdate)&lt;br/&gt;
+      case stageExecutorMetrics: SparkListenerStageExecutorMetrics =&amp;gt;&lt;br/&gt;
+        stageExecutorMetricsToJson(stageExecutorMetrics)&lt;br/&gt;
       case blockUpdate: SparkListenerBlockUpdated =&amp;gt;&lt;br/&gt;
         blockUpdateToJson(blockUpdate)&lt;br/&gt;
       case _ =&amp;gt; parse(mapper.writeValueAsString(event))&lt;br/&gt;
@@ -236,6 +239,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object JsonProtocol {&lt;br/&gt;
   def executorMetricsUpdateToJson(metricsUpdate: SparkListenerExecutorMetricsUpdate): JValue = {&lt;br/&gt;
     val execId = metricsUpdate.execId&lt;br/&gt;
     val accumUpdates = metricsUpdate.accumUpdates&lt;br/&gt;
+    val executorMetrics = metricsUpdate.executorUpdates.map(executorMetricsToJson(_))&lt;br/&gt;
     (&quot;Event&quot; -&amp;gt; SPARK_LISTENER_EVENT_FORMATTED_CLASS_NAMES.metricsUpdate) ~&lt;br/&gt;
     (&quot;Executor ID&quot; -&amp;gt; execId) ~&lt;br/&gt;
     (&quot;Metrics Updated&quot; -&amp;gt; accumUpdates.map { case (taskId, stageId, stageAttemptId, updates) =&amp;gt;&lt;br/&gt;
@@ -243,7 +247,16 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object JsonProtocol &lt;/p&gt;
{
       (&quot;Stage ID&quot; -&amp;gt; stageId) ~
       (&quot;Stage Attempt ID&quot; -&amp;gt; stageAttemptId) ~
       (&quot;Accumulator Updates&quot; -&amp;gt; JArray(updates.map(accumulableInfoToJson).toList))
-    }
&lt;p&gt;)&lt;br/&gt;
+    }) ~&lt;br/&gt;
+    (&quot;Executor Metrics Updated&quot; -&amp;gt; executorMetrics)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def stageExecutorMetricsToJson(metrics: SparkListenerStageExecutorMetrics): JValue = &lt;/p&gt;
{
+    (&quot;Event&quot; -&amp;gt; SPARK_LISTENER_EVENT_FORMATTED_CLASS_NAMES.stageExecutorMetrics) ~
+    (&quot;Executor ID&quot; -&amp;gt; metrics.execId) ~
+    (&quot;Stage ID&quot; -&amp;gt; metrics.stageId) ~
+    (&quot;Stage Attempt ID&quot; -&amp;gt; metrics.stageAttemptId) ~
+    (&quot;Executor Metrics&quot; -&amp;gt; executorMetricsToJson(metrics.executorMetrics))
   }

&lt;p&gt;   def blockUpdateToJson(blockUpdate: SparkListenerBlockUpdated): JValue = {&lt;br/&gt;
@@ -379,6 +392,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object JsonProtocol &lt;/p&gt;
{
     (&quot;Updated Blocks&quot; -&amp;gt; updatedBlocks)
   }

&lt;p&gt;+  /** Convert executor metrics to JSON. */&lt;br/&gt;
+  def executorMetricsToJson(executorMetrics: ExecutorMetrics): JValue = {&lt;br/&gt;
+    val metrics = ExecutorMetricType.values.map&lt;/p&gt;
{ metricType =&amp;gt;
+      JField(metricType.name, executorMetrics.getMetricValue(metricType))
+     }
&lt;p&gt;+    JObject(metrics: _*)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   def taskEndReasonToJson(taskEndReason: TaskEndReason): JValue = {&lt;br/&gt;
     val reason = Utils.getFormattedClassName(taskEndReason)&lt;br/&gt;
     val json: JObject = taskEndReason match {&lt;br/&gt;
@@ -531,6 +552,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object JsonProtocol &lt;/p&gt;
{
     val executorRemoved = Utils.getFormattedClassName(SparkListenerExecutorRemoved)
     val logStart = Utils.getFormattedClassName(SparkListenerLogStart)
     val metricsUpdate = Utils.getFormattedClassName(SparkListenerExecutorMetricsUpdate)
+    val stageExecutorMetrics = Utils.getFormattedClassName(SparkListenerStageExecutorMetrics)
     val blockUpdate = Utils.getFormattedClassName(SparkListenerBlockUpdated)
   }

&lt;p&gt;@@ -555,6 +577,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object JsonProtocol {&lt;br/&gt;
       case `executorRemoved` =&amp;gt; executorRemovedFromJson(json)&lt;br/&gt;
       case `logStart` =&amp;gt; logStartFromJson(json)&lt;br/&gt;
       case `metricsUpdate` =&amp;gt; executorMetricsUpdateFromJson(json)&lt;br/&gt;
+      case `stageExecutorMetrics` =&amp;gt; stageExecutorMetricsFromJson(json)&lt;br/&gt;
       case `blockUpdate` =&amp;gt; blockUpdateFromJson(json)&lt;br/&gt;
       case other =&amp;gt; mapper.readValue(compact(render(json)), Utils.classForName(other))&lt;br/&gt;
         .asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;SparkListenerEvent&amp;#93;&lt;/span&gt;&lt;br/&gt;
@@ -585,6 +608,15 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object JsonProtocol &lt;/p&gt;
{
     SparkListenerTaskGettingResult(taskInfo)
   }

&lt;p&gt;+  /** Extract the executor metrics from JSON. */&lt;br/&gt;
+  def executorMetricsFromJson(json: JValue): ExecutorMetrics = {&lt;br/&gt;
+    val metrics =&lt;br/&gt;
+      ExecutorMetricType.values.map &lt;/p&gt;
{ metric =&amp;gt;
+        metric.name -&amp;gt; jsonOption(json \ metric.name).map(_.extract[Long]).getOrElse(0L)
+      }
&lt;p&gt;.toMap&lt;br/&gt;
+    new ExecutorMetrics(metrics)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
   def taskEndFromJson(json: JValue): SparkListenerTaskEnd = {&lt;br/&gt;
     val stageId = (json \ &quot;Stage ID&quot;).extract&lt;span class=&quot;error&quot;&gt;&amp;#91;Int&amp;#93;&lt;/span&gt;&lt;br/&gt;
     val stageAttemptId =&lt;br/&gt;
@@ -691,7 +723,18 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object JsonProtocol &lt;/p&gt;
{
         (json \ &quot;Accumulator Updates&quot;).extract[List[JValue]].map(accumulableInfoFromJson)
       (taskId, stageId, stageAttemptId, updates)
     }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;SparkListenerExecutorMetricsUpdate(execInfo, accumUpdates)&lt;br/&gt;
+    val executorUpdates = jsonOption(json \ &quot;Executor Metrics Updated&quot;).map 
{
+      executorUpdate =&amp;gt; executorMetricsFromJson(executorUpdate)
+    }
&lt;p&gt;+    SparkListenerExecutorMetricsUpdate(execInfo, accumUpdates, executorUpdates)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def stageExecutorMetricsFromJson(json: JValue): SparkListenerStageExecutorMetrics = &lt;/p&gt;
{
+    val execId = (json \ &quot;Executor ID&quot;).extract[String]
+    val stageId = (json \ &quot;Stage ID&quot;).extract[Int]
+    val stageAttemptId = (json \ &quot;Stage Attempt ID&quot;).extract[Int]
+    val executorMetrics = executorMetricsFromJson(json \ &quot;Executor Metrics&quot;)
+    SparkListenerStageExecutorMetrics(execId, stageId, stageAttemptId, executorMetrics)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   def blockUpdateFromJson(json: JValue): SparkListenerBlockUpdated = {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/util/ListenerBus.scala b/core/src/main/scala/org/apache/spark/util/ListenerBus.scala&lt;br/&gt;
index a8f10684d5a2c..2e517707ff774 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/util/ListenerBus.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/util/ListenerBus.scala&lt;br/&gt;
@@ -60,6 +60,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; trait ListenerBus&lt;span class=&quot;error&quot;&gt;&amp;#91;L &amp;lt;: AnyRef, E&amp;#93;&lt;/span&gt; extends Logging {&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  /**&lt;br/&gt;
+   * Remove all listeners and they won&apos;t receive any events. This method is thread-safe and can be&lt;br/&gt;
+   * called in any thread.&lt;br/&gt;
+   */&lt;br/&gt;
+  final def removeAllListeners(): Unit = &lt;/p&gt;
{
+    listenersPlusTimers.clear()
+  }
&lt;p&gt;+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This can be overridden by subclasses if there is any extra cleanup to do when removing a&lt;/li&gt;
	&lt;li&gt;listener.  In particular AsyncEventQueues can clean up queues in the LiveListenerBus.&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/util/Utils.scala b/core/src/main/scala/org/apache/spark/util/Utils.scala&lt;br/&gt;
index 15c958d3f511e..227c9e734f0af 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/util/Utils.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/util/Utils.scala&lt;br/&gt;
@@ -19,7 +19,6 @@ package org.apache.spark.util&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; import java.io._&lt;br/&gt;
 import java.lang.&lt;/p&gt;
{Byte =&amp;gt; JByte}
&lt;p&gt;-import java.lang.InternalError&lt;br/&gt;
 import java.lang.management.&lt;/p&gt;
{LockInfo, ManagementFactory, MonitorInfo, ThreadInfo}
&lt;p&gt; import java.lang.reflect.InvocationTargetException&lt;br/&gt;
 import java.math.&lt;/p&gt;
{MathContext, RoundingMode}
&lt;p&gt;@@ -32,7 +31,6 @@ import java.security.SecureRandom&lt;br/&gt;
 import java.util.&lt;/p&gt;
{Locale, Properties, Random, UUID}
&lt;p&gt; import java.util.concurrent._&lt;br/&gt;
 import java.util.concurrent.TimeUnit.NANOSECONDS&lt;br/&gt;
-import java.util.concurrent.atomic.AtomicBoolean&lt;br/&gt;
 import java.util.zip.GZIPInputStream&lt;/p&gt;

&lt;p&gt; import scala.annotation.tailrec&lt;br/&gt;
@@ -94,53 +92,6 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Utils extends Logging {&lt;br/&gt;
   private val MAX_DIR_CREATION_ATTEMPTS: Int = 10&lt;br/&gt;
   @volatile private var localRootDirs: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt; = null&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* The performance overhead of creating and logging strings for wide schemas can be large. To&lt;/li&gt;
	&lt;li&gt;* limit the impact, we bound the number of fields to include by default. This can be overridden&lt;/li&gt;
	&lt;li&gt;* by setting the &apos;spark.debug.maxToStringFields&apos; conf in SparkEnv.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;val DEFAULT_MAX_TO_STRING_FIELDS = 25&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; def maxNumToStringFields = {&lt;/li&gt;
	&lt;li&gt;if (SparkEnv.get != null) 
{
-      SparkEnv.get.conf.getInt(&quot;spark.debug.maxToStringFields&quot;, DEFAULT_MAX_TO_STRING_FIELDS)
-    }
&lt;p&gt; else &lt;/p&gt;
{
-      DEFAULT_MAX_TO_STRING_FIELDS
-    }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/** Whether we have warned about plan string truncation yet. */&lt;/li&gt;
	&lt;li&gt;private val truncationWarningPrinted = new AtomicBoolean(false)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/**&lt;/li&gt;
	&lt;li&gt;* Format a sequence with semantics similar to calling .mkString(). Any elements beyond&lt;/li&gt;
	&lt;li&gt;* maxNumToStringFields will be dropped and replaced by a &quot;... N more fields&quot; placeholder.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @return the trimmed and formatted string.&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;def truncatedString&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(&lt;/li&gt;
	&lt;li&gt;seq: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;,&lt;/li&gt;
	&lt;li&gt;start: String,&lt;/li&gt;
	&lt;li&gt;sep: String,&lt;/li&gt;
	&lt;li&gt;end: String,&lt;/li&gt;
	&lt;li&gt;maxNumFields: Int = maxNumToStringFields): String = {&lt;/li&gt;
	&lt;li&gt;if (seq.length &amp;gt; maxNumFields) {&lt;/li&gt;
	&lt;li&gt;if (truncationWarningPrinted.compareAndSet(false, true)) 
{
-        logWarning(
-          &quot;Truncated the string representation of a plan since it was too large. This &quot; +
-          &quot;behavior can be adjusted by setting &apos;spark.debug.maxToStringFields&apos; in SparkEnv.conf.&quot;)
-      }&lt;/li&gt;
	&lt;li&gt;val numFields = math.max(0, maxNumFields - 1)&lt;/li&gt;
	&lt;li&gt;seq.take(numFields).mkString(&lt;/li&gt;
	&lt;li&gt;start, sep, sep + &quot;... &quot; + (seq.length - numFields) + &quot; more fields&quot; + end)&lt;/li&gt;
	&lt;li&gt;} else 
{
-      seq.mkString(start, sep, end)
-    }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;/** Shorthand for calling truncatedString() without start or end strings. */&lt;/li&gt;
	&lt;li&gt;def truncatedString&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(seq: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;, sep: String): String = truncatedString(seq, &quot;&quot;, sep, &quot;&quot;)&lt;br/&gt;
-&lt;br/&gt;
   /** Serialize an object using Java serialization */&lt;br/&gt;
   def serialize&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(o: T): Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     val bos = new ByteArrayOutputStream()&lt;br/&gt;
@@ -240,6 +191,19 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Utils extends Logging 
{
     // scalastyle:on classforname
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  /**&lt;br/&gt;
+   * Run a segment of code using a different context class loader in the current thread&lt;br/&gt;
+   */&lt;br/&gt;
+  def withContextClassLoader&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(ctxClassLoader: ClassLoader)(fn: =&amp;gt; T): T = {&lt;br/&gt;
+    val oldClassLoader = Thread.currentThread().getContextClassLoader()&lt;br/&gt;
+    try &lt;/p&gt;
{
+      Thread.currentThread().setContextClassLoader(ctxClassLoader)
+      fn
+    }
&lt;p&gt; finally &lt;/p&gt;
{
+      Thread.currentThread().setContextClassLoader(oldClassLoader)
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Primitive often used when writing [&lt;span class=&quot;error&quot;&gt;&amp;#91;java.nio.ByteBuffer&amp;#93;&lt;/span&gt;] to [&lt;span class=&quot;error&quot;&gt;&amp;#91;java.io.DataOutput&amp;#93;&lt;/span&gt;]&lt;br/&gt;
    */&lt;br/&gt;
@@ -2052,6 +2016,30 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Utils extends Logging {&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  /**&lt;br/&gt;
+   * Implements the same logic as JDK `java.lang.String#trim` by removing leading and trailing&lt;br/&gt;
+   * non-printable characters less or equal to &apos;\u0020&apos; (SPACE) but preserves natural line&lt;br/&gt;
+   * delimiters according to [&lt;span class=&quot;error&quot;&gt;&amp;#91;java.util.Properties&amp;#93;&lt;/span&gt;] load method. The natural line delimiters are&lt;br/&gt;
+   * removed by JDK during load. Therefore any remaining ones have been specifically provided and&lt;br/&gt;
+   * escaped by the user, and must not be ignored&lt;br/&gt;
+   *&lt;br/&gt;
+   * @param str&lt;br/&gt;
+   * @return the trimmed value of str&lt;br/&gt;
+   */&lt;br/&gt;
+  private&lt;span class=&quot;error&quot;&gt;&amp;#91;util&amp;#93;&lt;/span&gt; def trimExceptCRLF(str: String): String = {&lt;br/&gt;
+    val nonSpaceOrNaturalLineDelimiter: Char =&amp;gt; Boolean = &lt;/p&gt;
{ ch =&amp;gt;
+      ch &amp;gt; &apos; &apos; || ch == &apos;\r&apos; || ch == &apos;\n&apos;
+    }
&lt;p&gt;+&lt;br/&gt;
+    val firstPos = str.indexWhere(nonSpaceOrNaturalLineDelimiter)&lt;br/&gt;
+    val lastPos = str.lastIndexWhere(nonSpaceOrNaturalLineDelimiter)&lt;br/&gt;
+    if (firstPos &amp;gt;= 0 &amp;amp;&amp;amp; lastPos &amp;gt;= 0) &lt;/p&gt;
{
+      str.substring(firstPos, lastPos + 1)
+    }
&lt;p&gt; else &lt;/p&gt;
{
+      &quot;&quot;
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
   /** Load properties present in the given file. */&lt;br/&gt;
   def getPropertiesFromFile(filename: String): Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     val file = new File(filename)&lt;br/&gt;
@@ -2062,8 +2050,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Utils extends Logging {&lt;br/&gt;
     try {&lt;br/&gt;
       val properties = new Properties()&lt;br/&gt;
       properties.load(inReader)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;properties.stringPropertyNames().asScala.map(&lt;/li&gt;
	&lt;li&gt;k =&amp;gt; (k, properties.getProperty(k).trim)).toMap&lt;br/&gt;
+      properties.stringPropertyNames().asScala&lt;br/&gt;
+        .map 
{ k =&amp;gt; (k, trimExceptCRLF(properties.getProperty(k))) }
&lt;p&gt;+        .toMap&lt;br/&gt;
+&lt;br/&gt;
     } catch {&lt;br/&gt;
       case e: IOException =&amp;gt;&lt;br/&gt;
         throw new SparkException(s&quot;Failed when loading Spark properties from $filename&quot;, e)&lt;br/&gt;
@@ -2290,7 +2280,12 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Utils extends Logging {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;configure a new log4j level&lt;br/&gt;
    */&lt;br/&gt;
   def setLogLevel(l: org.apache.log4j.Level) {&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;org.apache.log4j.Logger.getRootLogger().setLevel(l)&lt;br/&gt;
+    val rootLogger = org.apache.log4j.Logger.getRootLogger()&lt;br/&gt;
+    rootLogger.setLevel(l)&lt;br/&gt;
+    rootLogger.getAllAppenders().asScala.foreach 
{
+      case ca: org.apache.log4j.ConsoleAppender =&amp;gt; ca.setThreshold(l)
+      case _ =&amp;gt; // no-op
+    }
&lt;p&gt;   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
@@ -2392,7 +2387,8 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Utils extends Logging {&lt;br/&gt;
       &quot;org.apache.spark.security.ShellBasedGroupsMappingProvider&quot;)&lt;br/&gt;
     if (groupProviderClassName != &quot;&quot;) {&lt;br/&gt;
       try {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val groupMappingServiceProvider = classForName(groupProviderClassName).newInstance.&lt;br/&gt;
+        val groupMappingServiceProvider = classForName(groupProviderClassName).&lt;br/&gt;
+          getConstructor().newInstance().&lt;br/&gt;
           asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;org.apache.spark.security.GroupMappingServiceProvider&amp;#93;&lt;/span&gt;&lt;br/&gt;
         val currentUserGroups = groupMappingServiceProvider.getGroups(username)&lt;br/&gt;
         return currentUserGroups&lt;br/&gt;
@@ -2698,7 +2694,7 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Utils extends Logging {&lt;br/&gt;
     }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     val masterScheme = new URI(masterWithoutK8sPrefix).getScheme&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;val resolvedURL = masterScheme.toLowerCase match {&lt;br/&gt;
+    val resolvedURL = masterScheme.toLowerCase(Locale.ROOT) match {&lt;br/&gt;
       case &quot;https&quot; =&amp;gt;&lt;br/&gt;
         masterWithoutK8sPrefix&lt;br/&gt;
       case &quot;http&quot; =&amp;gt;&lt;br/&gt;
@@ -2795,6 +2791,44 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object Utils extends Logging {&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Regular expression matching full width characters.&lt;br/&gt;
+   *&lt;br/&gt;
+   * Looked at all the 0x0000-0xFFFF characters (unicode) and showed them under Xshell.&lt;br/&gt;
+   * Found all the full width characters, then get the regular expression.&lt;br/&gt;
+   */&lt;br/&gt;
+  private val fullWidthRegex = (&quot;&quot;&quot;[&quot;&quot;&quot; +&lt;br/&gt;
+    // scalastyle:off nonascii&lt;br/&gt;
+    &quot;&quot;&quot;\u1100-\u115F&quot;&quot;&quot; +&lt;br/&gt;
+    &quot;&quot;&quot;\u2E80-\uA4CF&quot;&quot;&quot; +&lt;br/&gt;
+    &quot;&quot;&quot;\uAC00-\uD7A3&quot;&quot;&quot; +&lt;br/&gt;
+    &quot;&quot;&quot;\uF900-\uFAFF&quot;&quot;&quot; +&lt;br/&gt;
+    &quot;&quot;&quot;\uFE10-\uFE19&quot;&quot;&quot; +&lt;br/&gt;
+    &quot;&quot;&quot;\uFE30-\uFE6F&quot;&quot;&quot; +&lt;br/&gt;
+    &quot;&quot;&quot;\uFF00-\uFF60&quot;&quot;&quot; +&lt;br/&gt;
+    &quot;&quot;&quot;\uFFE0-\uFFE6&quot;&quot;&quot; +&lt;br/&gt;
+    // scalastyle:on nonascii&lt;br/&gt;
+    &quot;&quot;&quot;]&quot;&quot;&quot;).r&lt;br/&gt;
+&lt;br/&gt;
+  /**&lt;br/&gt;
+   * Return the number of half widths in a given string. Note that a full width character&lt;br/&gt;
+   * occupies two half widths.&lt;br/&gt;
+   *&lt;br/&gt;
+   * For a string consisting of 1 million characters, the execution of this method requires&lt;br/&gt;
+   * about 50ms.&lt;br/&gt;
+   */&lt;br/&gt;
+  def stringHalfWidth(str: String): Int = 
{
+    if (str == null) 0 else str.length + fullWidthRegex.findAllIn(str).size
+  }
&lt;p&gt;+&lt;br/&gt;
+  def sanitizeDirName(str: String): String = {&lt;br/&gt;
+    str.replaceAll(&quot;[ :/]&quot;, &quot;-&quot;).replaceAll(&quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;.${}&amp;#39;\&amp;quot;&amp;#93;&lt;/span&gt;&quot;, &quot;_&quot;).toLowerCase(Locale.ROOT)&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  def isClientMode(conf: SparkConf): Boolean = &lt;/p&gt;
{
+    &quot;client&quot;.equals(conf.get(SparkLauncher.DEPLOY_MODE, &quot;client&quot;))
+  }
&lt;p&gt; }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; private&lt;span class=&quot;error&quot;&gt;&amp;#91;util&amp;#93;&lt;/span&gt; object CallerContext extends Logging {&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/util/VersionUtils.scala b/core/src/main/scala/org/apache/spark/util/VersionUtils.scala&lt;br/&gt;
index 828153b868420..c0f8866dd58dc 100644&lt;br/&gt;
&amp;#8212; a/core/src/main/scala/org/apache/spark/util/VersionUtils.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/util/VersionUtils.scala&lt;br/&gt;
@@ -23,6 +23,7 @@ package org.apache.spark.util&lt;br/&gt;
 private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object VersionUtils {&lt;/p&gt;

&lt;p&gt;   private val majorMinorRegex = &quot;&quot;&quot;^(\d+)\.(\d+)(\..*)?$&quot;&quot;&quot;.r&lt;br/&gt;
+  private val shortVersionRegex = &quot;&quot;&quot;^(\d+\.\d+\.\d+)(.*)?$&quot;&quot;&quot;.r&lt;/p&gt;

&lt;p&gt;   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Given a Spark version string, return the major version number.&lt;br/&gt;
@@ -36,6 +37,19 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object VersionUtils {&lt;br/&gt;
    */&lt;br/&gt;
   def minorVersion(sparkVersion: String): Int = majorMinorVersion(sparkVersion)._2&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  /**&lt;br/&gt;
+   * Given a Spark version string, return the short version string.&lt;br/&gt;
+   * E.g., for 3.0.0-SNAPSHOT, return &apos;3.0.0&apos;.&lt;br/&gt;
+   */&lt;br/&gt;
+  def shortVersion(sparkVersion: String): String = {&lt;br/&gt;
+    shortVersionRegex.findFirstMatchIn(sparkVersion) match &lt;/p&gt;
{
+      case Some(m) =&amp;gt; m.group(1)
+      case None =&amp;gt;
+        throw new IllegalArgumentException(s&quot;Spark tried to parse &apos;$sparkVersion&apos; as a Spark&quot; +
+          s&quot; version string, but it could not find the major/minor/maintenance version numbers.&quot;)
+    }
&lt;p&gt;+  }&lt;br/&gt;
+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Given a Spark version string, return the (major version number, minor version number).&lt;/li&gt;
	&lt;li&gt;E.g., for 2.0.1-SNAPSHOT, return (2, 0).&lt;br/&gt;
diff --git a/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala b/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala&lt;br/&gt;
index b159200d79222..46279e79d78db 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala&lt;br/&gt;
+++ b/core/src/main/scala/org/apache/spark/util/collection/ExternalSorter.scala&lt;br/&gt;
@@ -727,9 +727,10 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; class ExternalSorter&lt;span class=&quot;error&quot;&gt;&amp;#91;K, V, C&amp;#93;&lt;/span&gt;(&lt;br/&gt;
     spills.clear()&lt;br/&gt;
     forceSpillFiles.foreach(s =&amp;gt; s.file.delete())&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;  (This diff was longer than 20,000 lines, and has been truncated...)&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16727179" author="githubbot" created="Sat, 22 Dec 2018 02:24:13 +0000"  >&lt;p&gt;Ngone51 opened a new pull request #23368: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; title=&quot;YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26269&quot;&gt;&lt;del&gt;SPARK-26269&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;YARN&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;BRANCH-2.4&amp;#93;&lt;/span&gt; Yarnallocator should have same blacklist behaviour with yarn to maxmize use of cluster resource&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23368&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23368&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What changes were proposed in this pull request?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   As I mentioned in jira &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; title=&quot;YarnAllocator should have same blacklist behaviour with YARN to maxmize use of cluster resource&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26269&quot;&gt;&lt;del&gt;SPARK-26269&lt;/del&gt;&lt;/a&gt;(&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26269&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-26269&lt;/a&gt;), in order to maxmize the use of cluster resource,  this pr try to make `YarnAllocator` have the same blacklist behaviour with YARN.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;How was this patch tested?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Added.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 47 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|s016gg:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12342385">2.4.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>