<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:54:13 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-21159] Cluster mode, driver throws connection refused exception submitted by SparkLauncher</title>
                <link>https://issues.apache.org/jira/browse/SPARK-21159</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When an spark application submitted by SparkLauncher#startApplication method, this will get a SparkAppHandle. In the test environment, the launcher runs on server A, if it runs in Client mode, everything is ok. In cluster mode, the launcher will run on Server A, and the driver will be run on Server B, in this scenario, when initialize SparkContext, a LauncherBackend will try to connect to the launcher application via specified port and ip address. the problem is the implementation of LauncherBackend uses loopback ip to connect which is 127.0.0.1. this will cause the connection refused as server B never ran the launcher. &lt;/p&gt;

&lt;p&gt;The expected behavior is the LauncherBackend should use Server A&apos;s Ip address to connect for reporting the running status.&lt;/p&gt;

&lt;p&gt;Below is the stacktrace:&lt;br/&gt;
17/06/20 17:24:37 ERROR SparkContext: Error initializing SparkContext.&lt;br/&gt;
java.net.ConnectException: Connection refused&lt;br/&gt;
	at java.net.PlainSocketImpl.socketConnect(Native Method)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)&lt;br/&gt;
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)&lt;br/&gt;
	at java.net.Socket.connect(Socket.java:589)&lt;br/&gt;
	at java.net.Socket.connect(Socket.java:538)&lt;br/&gt;
	at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:434)&lt;br/&gt;
	at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:244)&lt;br/&gt;
	at org.apache.spark.launcher.LauncherBackend.connect(LauncherBackend.scala:43)&lt;br/&gt;
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.start(StandaloneSchedulerBackend.scala:60)&lt;br/&gt;
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:156)&lt;br/&gt;
	at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:509)&lt;br/&gt;
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2313)&lt;br/&gt;
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)&lt;br/&gt;
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)&lt;br/&gt;
	at scala.Option.getOrElse(Option.scala:121)&lt;br/&gt;
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)&lt;br/&gt;
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.executeSparkJob(AbstractCommonSparkTask.scala:91)&lt;br/&gt;
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.runSparkJob(AbstractCommonSparkTask.scala:25)&lt;br/&gt;
	at com.asura.grinder.datatask.main.TaskMain$.main(TaskMain.scala:61)&lt;br/&gt;
	at com.asura.grinder.datatask.main.TaskMain.main(TaskMain.scala)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)&lt;br/&gt;
	at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)&lt;br/&gt;
17/06/20 17:24:37 INFO SparkUI: Stopped Spark web UI at &lt;a href=&quot;http://172.25.108.62:4040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://172.25.108.62:4040&lt;/a&gt;&lt;br/&gt;
17/06/20 17:24:37 INFO StandaloneSchedulerBackend: Shutting down all executors&lt;br/&gt;
17/06/20 17:24:37 INFO CoarseGrainedSchedulerBackend$DriverEndpoint: Asking each executor to shut down&lt;br/&gt;
17/06/20 17:24:37 ERROR Utils: Uncaught exception in thread main&lt;br/&gt;
java.lang.NullPointerException&lt;br/&gt;
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.org$apache$spark$scheduler$cluster$StandaloneSchedulerBackend$$stop(StandaloneSchedulerBackend.scala:214)&lt;br/&gt;
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.stop(StandaloneSchedulerBackend.scala:116)&lt;br/&gt;
	at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:467)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1588)&lt;br/&gt;
	at org.apache.spark.SparkContext$$anonfun$stop$8.apply$mcV$sp(SparkContext.scala:1826)&lt;br/&gt;
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1283)&lt;br/&gt;
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1825)&lt;br/&gt;
	at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:587)&lt;br/&gt;
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2313)&lt;br/&gt;
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)&lt;br/&gt;
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)&lt;br/&gt;
	at scala.Option.getOrElse(Option.scala:121)&lt;br/&gt;
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)&lt;br/&gt;
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.executeSparkJob(AbstractCommonSparkTask.scala:91)&lt;br/&gt;
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.runSparkJob(AbstractCommonSparkTask.scala:25)&lt;br/&gt;
	at com.asura.grinder.datatask.main.TaskMain$.main(TaskMain.scala:61)&lt;br/&gt;
	at com.asura.grinder.datatask.main.TaskMain.main(TaskMain.scala)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)&lt;br/&gt;
	at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)&lt;br/&gt;
17/06/20 17:24:37 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!&lt;br/&gt;
17/06/20 17:24:37 INFO MemoryStore: MemoryStore cleared&lt;br/&gt;
17/06/20 17:24:37 INFO BlockManager: BlockManager stopped&lt;br/&gt;
17/06/20 17:24:37 INFO BlockManagerMaster: BlockManagerMaster stopped&lt;br/&gt;
17/06/20 17:24:37 WARN MetricsSystem: Stopping a MetricsSystem that is not running&lt;br/&gt;
17/06/20 17:24:37 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!&lt;br/&gt;
17/06/20 17:24:37 INFO SparkContext: Successfully stopped SparkContext&lt;br/&gt;
17/06/20 17:24:37 ERROR MongoPilotTask: error occurred group&lt;/p&gt;
{2}
&lt;p&gt;:task(222)&lt;br/&gt;
java.net.ConnectException: Connection refused&lt;br/&gt;
	at java.net.PlainSocketImpl.socketConnect(Native Method)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:350)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:206)&lt;br/&gt;
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:188)&lt;br/&gt;
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)&lt;br/&gt;
	at java.net.Socket.connect(Socket.java:589)&lt;br/&gt;
	at java.net.Socket.connect(Socket.java:538)&lt;br/&gt;
	at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:434)&lt;br/&gt;
	at java.net.Socket.&amp;lt;init&amp;gt;(Socket.java:244)&lt;br/&gt;
	at org.apache.spark.launcher.LauncherBackend.connect(LauncherBackend.scala:43)&lt;br/&gt;
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.start(StandaloneSchedulerBackend.scala:60)&lt;br/&gt;
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:156)&lt;br/&gt;
	at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:509)&lt;br/&gt;
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2313)&lt;br/&gt;
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:868)&lt;br/&gt;
	at org.apache.spark.sql.SparkSession$Builder$$anonfun$6.apply(SparkSession.scala:860)&lt;br/&gt;
	at scala.Option.getOrElse(Option.scala:121)&lt;br/&gt;
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:860)&lt;br/&gt;
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.executeSparkJob(AbstractCommonSparkTask.scala:91)&lt;br/&gt;
	at com.asura.grinder.datatask.task.AbstractCommonSparkTask.runSparkJob(AbstractCommonSparkTask.scala:25)&lt;br/&gt;
	at com.asura.grinder.datatask.main.TaskMain$.main(TaskMain.scala:61)&lt;br/&gt;
	at com.asura.grinder.datatask.main.TaskMain.main(TaskMain.scala)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:58)&lt;br/&gt;
	at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)&lt;/p&gt;
</description>
                <environment>&lt;p&gt;Server A-Master&lt;br/&gt;
Server B-Slave&lt;/p&gt;</environment>
        <key id="13081294">SPARK-21159</key>
            <summary>Cluster mode, driver throws connection refused exception submitted by SparkLauncher</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="vanzin">Marcelo Masiero Vanzin</assignee>
                                    <reporter username="teclusky@gmail.com">niefei</reporter>
                        <labels>
                    </labels>
                <created>Wed, 21 Jun 2017 02:26:34 +0000</created>
                <updated>Sat, 24 Jun 2017 05:37:44 +0000</updated>
                            <resolved>Sat, 24 Jun 2017 05:24:27 +0000</resolved>
                                    <version>2.1.0</version>
                                    <fixVersion>2.1.2</fixVersion>
                    <fixVersion>2.2.0</fixVersion>
                                    <component>Spark Core</component>
                    <component>Spark Submit</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16056872" author="vanzin" created="Wed, 21 Jun 2017 02:33:20 +0000"  >&lt;p&gt;standalone cluster mode applications are not supported through &lt;tt&gt;startApplication&lt;/tt&gt; yet. See &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-11033&quot; title=&quot;Launcher: add support for monitoring standalone/cluster apps&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-11033&quot;&gt;&lt;del&gt;SPARK-11033&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I don&apos;t think they should fail, so I&apos;m not going to make this a duplicate yet, in case there&apos;s something to fix here.&lt;/p&gt;</comment>
                            <comment id="16057117" author="teclusky@gmail.com" created="Wed, 21 Jun 2017 07:52:18 +0000"  >&lt;p&gt;thank you for your reply. it should use launcher&apos;s IP address to connect rather than driver&apos;s IP address, as the launcher and driver will not run on the same server in cluster mode. &lt;/p&gt;</comment>
                            <comment id="16057767" author="vanzin" created="Wed, 21 Jun 2017 16:16:12 +0000"  >&lt;p&gt;No, that should not be it. That&apos;s not how the launcher works internally.&lt;/p&gt;

&lt;p&gt;I&apos;m only keeping this open because app&apos;s shouldn&apos;t fail because this feature is not implemented.&lt;/p&gt;</comment>
                            <comment id="16060208" author="apachespark" created="Thu, 22 Jun 2017 23:54:04 +0000"  >&lt;p&gt;User &apos;vanzin&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/18397&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/18397&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16060453" author="teclusky@gmail.com" created="Fri, 23 Jun 2017 06:10:59 +0000"  >&lt;p&gt;Great, thank you.&lt;/p&gt;</comment>
                            <comment id="16061809" author="cloud_fan" created="Sat, 24 Jun 2017 05:24:27 +0000"  >&lt;p&gt;Issue resolved by pull request 18397&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/18397&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/18397&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 21 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3giw7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>