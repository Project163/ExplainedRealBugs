<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:23:28 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4900] MLlib SingularValueDecomposition ARPACK IllegalStateException </title>
                <link>https://issues.apache.org/jira/browse/SPARK-4900</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;java.lang.reflect.InvocationTargetException&lt;br/&gt;
        ...&lt;br/&gt;
Caused by: java.lang.IllegalStateException: ARPACK returns non-zero info = 3 Please refer ARPACK user guide for error message.&lt;br/&gt;
        at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:120)&lt;br/&gt;
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:235)&lt;br/&gt;
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:171)&lt;br/&gt;
		...&lt;/p&gt;</description>
                <environment>&lt;p&gt;Ubuntu 1410, Java HotSpot(TM) 64-Bit Server VM (build 25.25-b02, mixed mode)&lt;br/&gt;
spark local mode&lt;/p&gt;</environment>
        <key id="12762822">SPARK-4900</key>
            <summary>MLlib SingularValueDecomposition ARPACK IllegalStateException </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srowen">Sean R. Owen</assignee>
                                    <reporter username="mbofb">Mike Beyer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 19 Dec 2014 11:23:04 +0000</created>
                <updated>Sat, 4 Apr 2015 15:55:04 +0000</updated>
                            <resolved>Sat, 4 Apr 2015 15:54:51 +0000</resolved>
                                    <version>1.1.1</version>
                    <version>1.2.0</version>
                    <version>1.2.1</version>
                                    <fixVersion>1.3.0</fixVersion>
                                    <component>MLlib</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="14254644" author="mbofb" created="Sat, 20 Dec 2014 11:01:15 +0000"  >&lt;p&gt;this exception occurs for various numbers of rows, columns, and k&lt;/p&gt;</comment>
                            <comment id="14309981" author="srowen" created="Fri, 6 Feb 2015 21:43:46 +0000"  >&lt;p&gt;Do you have any more info, like how to reproduce this? what were you computing?&lt;/p&gt;</comment>
                            <comment id="14312769" author="mbofb" created="Mon, 9 Feb 2015 20:05:07 +0000"  >&lt;p&gt;put a snapshot test data 1000x1000 matrix to &lt;a href=&quot;https://dl.dropboxusercontent.com/u/8489998/test_matrix_1.zip&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://dl.dropboxusercontent.com/u/8489998/test_matrix_1.zip&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;calling: &lt;br/&gt;
			String filename = &quot;/custompath/27637/test_matrix_1&quot;;&lt;br/&gt;
			RDD&amp;lt;Vector&amp;gt; vectorRDD = MLUtils.loadVectors(javaSparkContext.sc(), filename);&lt;br/&gt;
			vectorRDD.cache();&lt;br/&gt;
			System.out.println(&quot;trtRowRDD.count():\t&quot; + vectorRDD.count());&lt;br/&gt;
			RowMatrix rowMatrix = new RowMatrix(vectorRDD);&lt;br/&gt;
			System.out.println(&quot;rowMatrix.numRows():\t&quot; + rowMatrix.numRows());&lt;br/&gt;
			System.out.println(&quot;rowMatrix.numCols():\t&quot; + rowMatrix.numCols());&lt;/p&gt;
			{
				int k = 10;
				boolean computeU = true;
				double rCond = 1.0E-9d;
				SingularValueDecomposition&amp;lt;RowMatrix, Matrix&amp;gt; svd = rowMatrix.computeSVD(k, computeU, rCond);
				RowMatrix u = svd.U();
				RDD&amp;lt;Vector&amp;gt; uRowsRDD = u.rows();
				System.out.println(&quot;uRowsRDD.count():\t&quot; + uRowsRDD.count());
				Vector s = svd.s();
				System.out.println(&quot;s.size():\t&quot; + s.size());
				Matrix v = svd.V();
				System.out.println(&quot;v.numRows():\t&quot; + v.numRows());
				System.out.println(&quot;v.numCols():\t&quot; + v.numCols());
			}

&lt;p&gt;results in:&lt;/p&gt;


&lt;p&gt;maxFeatureSpaceTermNumber:      1000&lt;br/&gt;
trtRowRDD.count():      1000&lt;br/&gt;
rowMatrix.numRows():    1000&lt;br/&gt;
rowMatrix.numCols():    1000&lt;br/&gt;
15/02/09 19:56:59 WARN PrimaryRunnerSpark:&lt;br/&gt;
java.lang.IllegalStateException: ARPACK returns non-zero info = 3 Please refer ARPACK user guide for error message.&lt;br/&gt;
        at org.apache.spark.mllib.linalg.EigenValueDecomposition$.symmetricEigs(EigenValueDecomposition.scala:120)&lt;br/&gt;
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:258)&lt;br/&gt;
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computeSVD(RowMatrix.scala:190)&lt;br/&gt;
        at com.example.processing.spark.SVDProcessing2.createSVD_2(SVDProcessing2.java:184)&lt;br/&gt;
        at com.example.processing.spark.RunnerSpark.main(PrimaryRunnerSpark.java:27)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:483)&lt;br/&gt;
        at sbt.Run.invokeMain(Run.scala:67)&lt;br/&gt;
        at sbt.Run.run0(Run.scala:61)&lt;br/&gt;
        at sbt.Run.sbt$Run$$execute$1(Run.scala:51)&lt;br/&gt;
        at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)&lt;br/&gt;
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)&lt;br/&gt;
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)&lt;br/&gt;
        at sbt.Logger$$anon$4.apply(Logger.scala:85)&lt;br/&gt;
        at sbt.TrapExit$App.run(TrapExit.scala:248)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
15/02/09 19:56:59 INFO TimeCounter: TIMER &lt;span class=&quot;error&quot;&gt;&amp;#91;com.example.processing.spark.PrimaryRunnerSpark&amp;#93;&lt;/span&gt; : 13.0 Seconds&lt;br/&gt;
TIMER &lt;span class=&quot;error&quot;&gt;&amp;#91;com.example.processing.spark.PrimaryRunnerSpark&amp;#93;&lt;/span&gt; : 13.0 Seconds&lt;br/&gt;
15/02/09 19:56:59 ERROR ContextCleaner: Error cleaning broadcast 20&lt;br/&gt;
java.lang.InterruptedException&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1039)&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1328)&lt;br/&gt;
        at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)&lt;br/&gt;
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)&lt;br/&gt;
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)&lt;br/&gt;
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:107)&lt;br/&gt;
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)&lt;br/&gt;
        at scala.concurrent.Await$.result(package.scala:107)&lt;br/&gt;
        at org.apache.spark.storage.BlockManagerMaster.removeBroadcast(BlockManagerMaster.scala:137)&lt;br/&gt;
        at org.apache.spark.broadcast.TorrentBroadcast$.unpersist(TorrentBroadcast.scala:227)&lt;br/&gt;
        at org.apache.spark.broadcast.TorrentBroadcastFactory.unbroadcast(TorrentBroadcastFactory.scala:45)&lt;br/&gt;
        at org.apache.spark.broadcast.BroadcastManager.unbroadcast(BroadcastManager.scala:66)&lt;br/&gt;
        at org.apache.spark.ContextCleaner.doCleanupBroadcast(ContextCleaner.scala:185)&lt;br/&gt;
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(&lt;br/&gt;
ContextCleaner.scala:147)&lt;br/&gt;
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1$$anonfun$apply$mcV$sp$2.apply(&lt;br/&gt;
ContextCleaner.scala:138)&lt;br/&gt;
        at scala.Option.foreach(Option.scala:236)&lt;br/&gt;
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply$mcV$sp(ContextCleaner.sc&lt;br/&gt;
ala:138)&lt;br/&gt;
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134&lt;br/&gt;
)&lt;br/&gt;
        at org.apache.spark.ContextCleaner$$anonfun$org$apache$spark$ContextCleaner$$keepCleaning$1.apply(ContextCleaner.scala:134&lt;br/&gt;
)&lt;br/&gt;
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1550)&lt;br/&gt;
        at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:133)&lt;br/&gt;
        at org.apache.spark.ContextCleaner$$anon$3.run(ContextCleaner.scala:65)&lt;br/&gt;
15/02/09 19:56:59 ERROR Utils: Uncaught exception in thread SparkListenerBus&lt;br/&gt;
java.lang.InterruptedException&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)&lt;br/&gt;
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)&lt;br/&gt;
        at java.util.concurrent.Semaphore.acquire(Semaphore.java:312)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply$mcV$sp(LiveListenerBus.scala:48)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1$$anonfun$run$1.apply(LiveListenerBus.scala:47)&lt;br/&gt;
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1550)&lt;br/&gt;
        at org.apache.spark.scheduler.LiveListenerBus$$anon$1.run(LiveListenerBus.scala:46)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;success&amp;#93;&lt;/span&gt; Total time: 20 s, completed Feb 9, 2015 7:56:59 PM&lt;/p&gt;</comment>
                            <comment id="14312779" author="mbofb" created="Mon, 9 Feb 2015 20:11:17 +0000"  >&lt;p&gt;BTW: Matrix pc = rowMatrix.computePrincipalComponents(3); yields also an exception:&lt;/p&gt;

&lt;p&gt;breeze.linalg.NotConvergedException:&lt;br/&gt;
        at breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:60)&lt;br/&gt;
        at breeze.linalg.svd$Svd_DM_Impl$.apply(svd.scala:32)&lt;br/&gt;
        at breeze.generic.UFunc$class.apply(UFunc.scala:48)&lt;br/&gt;
        at breeze.linalg.svd$.apply(svd.scala:17)&lt;br/&gt;
        at org.apache.spark.mllib.linalg.distributed.RowMatrix.computePrincipalComponents(RowMatrix.scala:375)&lt;br/&gt;
        at com.example.processing.spark.SVDProcessing2.createSVD_2(SVDProcessing2.java:236)&lt;br/&gt;
        at com.example.processing.spark.RunnerSpark.main(PrimaryRunnerSpark.java:27)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:483)&lt;br/&gt;
        at sbt.Run.invokeMain(Run.scala:67)&lt;br/&gt;
        at sbt.Run.run0(Run.scala:61)&lt;br/&gt;
        at sbt.Run.sbt$Run$$execute$1(Run.scala:51)&lt;br/&gt;
        at sbt.Run$$anonfun$run$1.apply$mcV$sp(Run.scala:55)&lt;br/&gt;
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)&lt;br/&gt;
        at sbt.Run$$anonfun$run$1.apply(Run.scala:55)&lt;br/&gt;
        at sbt.Logger$$anon$4.apply(Logger.scala:85)&lt;br/&gt;
        at sbt.TrapExit$App.run(TrapExit.scala:248)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;</comment>
                            <comment id="14312795" author="srowen" created="Mon, 9 Feb 2015 20:19:16 +0000"  >&lt;p&gt;So I think there is at least a small problem in the error reporting:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;      info.`val` match {
        &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; 1 =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IllegalStateException(&lt;span class=&quot;code-quote&quot;&gt;&quot;ARPACK returns non-zero info = &quot;&lt;/span&gt; + info.`val` +
            &lt;span class=&quot;code-quote&quot;&gt;&quot; Maximum number of iterations taken. (Refer ARPACK user guide &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; details)&quot;&lt;/span&gt;)
        &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; 2 =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IllegalStateException(&lt;span class=&quot;code-quote&quot;&gt;&quot;ARPACK returns non-zero info = &quot;&lt;/span&gt; + info.`val` +
            &lt;span class=&quot;code-quote&quot;&gt;&quot; No shifts could be applied. Try to increase NCV. &quot;&lt;/span&gt; +
            &lt;span class=&quot;code-quote&quot;&gt;&quot;(Refer ARPACK user guide &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; details)&quot;&lt;/span&gt;)
        &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; _ =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IllegalStateException(&lt;span class=&quot;code-quote&quot;&gt;&quot;ARPACK returns non-zero info = &quot;&lt;/span&gt; + info.`val` +
            &lt;span class=&quot;code-quote&quot;&gt;&quot; Please refer ARPACK user guide &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; error message.&quot;&lt;/span&gt;)
      }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Really, what&apos;s called case 2 here corresponds to return value 3, which is what you get.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;            =  0: Normal exit.
            =  1: Maximum number of iterations taken.
                  All possible eigenvalues of OP has been found. IPARAM(5)  
                  returns the number of wanted converged Ritz values.
            =  2: No longer an informational error. Deprecated starting
                  with release 2 of ARPACK.
            =  3: No shifts could be applied during a cycle of the 
                  Implicitly restarted Arnoldi iteration. One possibility 
                  is to increase the size of NCV relative to NEV. 
                  See remark 4 below.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I can fix the error message. Remark 4 that it refers to is:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    4. At present there is no a-priori analysis to guide the selection
       of NCV relative to NEV.  The only formal requrement is that NCV &amp;gt; NEV.
       However, it is recommended that NCV .ge. 2*NEV.  If many problems of
       the same type are to be solved, one should experiment with increasing
       NCV &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; keeping NEV fixed &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; a given test problem.  This will 
       usually decrease the required number of OP*x operations but it
       also increases the work and storage required to maintain the orthogonal
       basis vectors.   The optimal &lt;span class=&quot;code-quote&quot;&gt;&quot;cross-over&quot;&lt;/span&gt; with respect to CPU time
       is problem dependent and must be determined empirically.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So I think that translates to &quot;k is too big&quot;. Is the matrix low-rank?&lt;br/&gt;
In any event this is ultimately breeze code and I&apos;m not sure if there&apos;s much that will done in Spark itself.&lt;/p&gt;</comment>
                            <comment id="14312810" author="srowen" created="Mon, 9 Feb 2015 20:26:38 +0000"  >&lt;p&gt;I think the other message means what it says; the SVD couldn&apos;t finish finding the 3rd principal component. The implementation you&apos;re calling for PCA is simple but may not be the most accurate. It&apos;s either a function of your input, or that you&apos;d need a different algo or set of settings when calling into Breeze to run PCA.&lt;/p&gt;</comment>
                            <comment id="14313132" author="apachespark" created="Mon, 9 Feb 2015 23:15:37 +0000"  >&lt;p&gt;User &apos;srowen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4485&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4485&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14313586" author="mengxr" created="Tue, 10 Feb 2015 05:14:16 +0000"  >&lt;p&gt;Issue resolved by pull request 4485&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4485&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4485&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14313863" author="srowen" created="Tue, 10 Feb 2015 09:12:28 +0000"  >&lt;p&gt;Yeah, that fixes the error at least. I imagine the rest is a function of the input and ARPACK, but if there&apos;s evidence that something is wrong in the Spark in between, you can reopen.&lt;/p&gt;</comment>
                            <comment id="14340197" author="mbofb" created="Fri, 27 Feb 2015 14:35:09 +0000"  >&lt;p&gt;i traced the problem back due to Double.NaN, Double.POSITIVE_INFINITY or  Double.NEGATIVE_INFINITY values of the java double values. In my opinion the user should care to have not such values in the data, but when an exception is thrown it should give a hint that this is a likely cause. If also other allgorithms in MLLib are not pretecting against failing due to Double.NaN, Double.POSITIVE_INFINITY or  Double.NEGATIVE_INFINITY it should be mentioned in the MLlib doc that the users has to ensure only proper double numbers.&lt;/p&gt;</comment>
                            <comment id="14340202" author="srowen" created="Fri, 27 Feb 2015 14:39:08 +0000"  >&lt;p&gt;Hm, I tend to agree that upfront error checking is good, and infinite / NaN values are not valid as input. There&apos;s a bit of a question of where you check, and whether it slows things down a lot to be always checking. Would you care to propose error checking for this case? I suppose at least the error message for this ARPACK error could hint at this possible cause.&lt;/p&gt;</comment>
                            <comment id="14340208" author="mbofb" created="Fri, 27 Feb 2015 14:47:09 +0000"  >&lt;p&gt;in my opinion there should be no automatic error checking but only hints on exception messages and in general MLlib Vector and Matrix doc. Maybe this should hint to an utility method (to be put somewhere) which would do such an check on vectors and matrix and which can be invoked by someone getting exceptions on his data.&lt;/p&gt;</comment>
                            <comment id="14340229" author="srowen" created="Fri, 27 Feb 2015 15:03:07 +0000"  >&lt;p&gt;This is still mostly counting on the user to validate their input. How about augmenting the error message here to suggest this as a cause - would that resolve the proximate issue more completely?&lt;/p&gt;</comment>
                            <comment id="14340235" author="mbofb" created="Fri, 27 Feb 2015 15:07:10 +0000"  >
&lt;p&gt;I would suggest a printValueStatistics(Vector/Matrix) method e.g. on MLUtils which should be hinted at the SVD exeption message (and on all exception methods from algorithms which are not able to process data with such double values - maybee also the PCA exception ?)&lt;/p&gt;

&lt;p&gt;which says something like&lt;/p&gt;

&lt;p&gt;values of the vector/matrix: &lt;br/&gt;
x 0 values&lt;br/&gt;
y values are different from 0&lt;br/&gt;
sparsity is z %;&lt;/p&gt;

&lt;p&gt;(an enhanced method could report more detailed stats one the distribution which can be costly to obtain but helpfull to grasp what one actully wants to process)&lt;/p&gt;

&lt;p&gt;Non finite values (might cause problems on some (all?) algorithms):&lt;br/&gt;
m values are Double.NaN &lt;br/&gt;
n values are Double.POSITIVE_INFINITY&lt;br/&gt;
o values are Double.NEGATIVE_INFINITY&lt;/p&gt;</comment>
                            <comment id="14340278" author="srowen" created="Fri, 27 Feb 2015 15:31:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mengxr&quot; class=&quot;user-hover&quot; rel=&quot;mengxr&quot;&gt;mengxr&lt;/a&gt; what do you think on this one? I/we could implement whatever sounds good. We could keep this narrower by just implementing an explicit check on the argument to LAPACK, and also expanding its error message. I hesitate to build out more debugging utilities, even if they sound a bit useful.&lt;/p&gt;</comment>
                            <comment id="14395788" author="srowen" created="Sat, 4 Apr 2015 15:54:51 +0000"  >&lt;p&gt;Given lack of follow-up I think this is as resolved as we&apos;re going to make it.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12713392">SPARK-1782</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 33 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i23m8f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>