<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:52:55 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-16548] java.io.CharConversionException: Invalid UTF-32 character  prevents me from querying my data</title>
                <link>https://issues.apache.org/jira/browse/SPARK-16548</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Basically, when I query my json data I get &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.io.CharConversionException: Invalid UTF-32 character 0x7b2265(above 10ffff)  at &lt;span class=&quot;code-object&quot;&gt;char&lt;/span&gt; #192, &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; #771)
	at com.fasterxml.jackson.core.io.UTF32Reader.reportInvalid(UTF32Reader.java:189)
	at com.fasterxml.jackson.core.io.UTF32Reader.read(UTF32Reader.java:150)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.loadMore(ReaderBasedJsonParser.java:153)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:1855)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:571)
	at org.apache.spark.sql.catalyst.expressions.GetJsonObject$$anonfun$eval$2$$anonfun$4.apply(jsonExpressions.scala:142)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I do not like it. If you can not process one json among 100500 please return null, do not fail everything. I have dirty one line fix, and I understand how I can make it more reasonable. What is our position - what behaviour we wanna get?&lt;/p&gt;</description>
                <environment></environment>
        <key id="12989510">SPARK-16548</key>
            <summary>java.io.CharConversionException: Invalid UTF-32 character  prevents me from querying my data</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="epahomov">Egor Pahomov</reporter>
                        <labels>
                    </labels>
                <created>Thu, 14 Jul 2016 17:05:20 +0000</created>
                <updated>Wed, 24 Jul 2019 19:03:59 +0000</updated>
                            <resolved>Wed, 26 Apr 2017 03:44:48 +0000</resolved>
                                    <version>1.6.1</version>
                                    <fixVersion>2.2.0</fixVersion>
                    <fixVersion>2.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="15377352" author="srowen" created="Thu, 14 Jul 2016 17:43:41 +0000"  >&lt;p&gt;Tough call, just because returning &apos;null&apos; is also arguably wrong, silently swallowing the error. In your own code you could handle this, but, I know this comes from inside Spark SQL. You can pre-process your input for invalid data. That is likely a good idea. I am not sure I would change Spark here.&lt;/p&gt;</comment>
                            <comment id="15378112" author="epahomov" created="Thu, 14 Jul 2016 18:47:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sowen&quot; class=&quot;user-hover&quot; rel=&quot;sowen&quot;&gt;sowen&lt;/a&gt; if your concern is &quot;silently&quot; we can wright errors to logs. WDYT?&lt;/p&gt;</comment>
                            <comment id="15378123" author="srowen" created="Thu, 14 Jul 2016 18:51:39 +0000"  >&lt;p&gt;Yeah sure, but is that much better? the job continues and is missing some data. Maybe that&apos;s OK, maybe not, but it&apos;s also something you can handle in preprocessing&lt;/p&gt;</comment>
                            <comment id="15975735" author="marmbrus" created="Wed, 19 Apr 2017 23:20:34 +0000"  >&lt;p&gt;I&apos;m not sure I agree.  The default behavior for parsing corrupted JSON is to return &lt;tt&gt;null&lt;/tt&gt; and fill in the column &lt;tt&gt;_corrupt_record&lt;/tt&gt; (same for casts that fail or other error cases).  It&apos;s weird that this one case is different.&lt;/p&gt;</comment>
                            <comment id="15979188" author="apachespark" created="Fri, 21 Apr 2017 18:28:03 +0000"  >&lt;p&gt;User &apos;ewasserman&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/17693&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/17693&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16811206" author="bijithkumar" created="Fri, 5 Apr 2019 19:48:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt;&#160;I am getting the same Exception in Spark 2.3.2. Wondering why would that happen since this is fixed in 2.3.0&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.io.CharConversionException: Invalid UTF-32 character 0x4d89aa(above 10ffff) at &lt;span class=&quot;code-object&quot;&gt;char&lt;/span&gt; #63, &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; #255) at com.fasterxml.jackson.core.io.UTF32Reader.reportInvalid(UTF32Reader.java:189) at com.fasterxml.jackson.core.io.UTF32Reader.read(UTF32Reader.java:150) at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.loadMore(ReaderBasedJsonParser.java:153) at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2017) at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:577) at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:350) at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:347) at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2589) at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:347) at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$3.apply(JsonDataSource.scala:128) at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$3.apply(JsonDataSource.scala:128) at org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:61) at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:132) at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:132) at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:109) at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source) at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408) at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439) at org.apache.spark.util.collection.ExternalSorter.insertAll(ExternalSorter.scala:191) at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:63) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96) at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53) at org.apache.spark.scheduler.Task.run(Task.scala:109) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16812469" author="cloud_fan" created="Mon, 8 Apr 2019 14:28:35 +0000"  >&lt;p&gt;Do you have a small dateset to reproduce it?&lt;/p&gt;</comment>
                            <comment id="16812858" author="bijithkumar" created="Mon, 8 Apr 2019 22:46:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt;, I&#160;couldn&apos;t&#160;find the&#160;specific&#160;character of the corrupted data that is causing the issue.&#160;However,&#160;here is the corrupted section from&#160;file to reproduce the issue. Please see attached -&#160;&lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12965255/12965255_corrupted.json&quot; title=&quot;corrupted.json attached to SPARK-16548&quot;&gt;corrupted.json&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;.&#160;&lt;/p&gt;</comment>
                            <comment id="16892098" author="shivakumar.ss" created="Wed, 24 Jul 2019 19:03:59 +0000"  >&lt;p&gt;Hello guys,&#160;&lt;/p&gt;

&lt;p&gt;I am using Spark&#160;2.3.0 and facing the same issue, i have to process huge amount of files and job is getting failed due to 1 or 2 files.&#160;&lt;/p&gt;

&lt;p&gt;I have following code snippet to read the json files&#160;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;val df = sqlContext.read.schema(Schemas.request_01)&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;.json(inputPath1,inputPath2)&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;df.show(10)&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Following is the exception which i am receiving.&#160;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;}}{{19/07/25 00:17:50 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, 192.168.1.100, executor 0): java.io.CharConversionException: Invalid UTF-32 character 0x4d89aa(above 10ffff) at char #63, byte #255)&lt;/tt&gt;&lt;br/&gt;
{{ at com.fasterxml.jackson.core.io.UTF32Reader.reportInvalid(UTF32Reader.java:189)}}&lt;br/&gt;
{{ at com.fasterxml.jackson.core.io.UTF32Reader.read(UTF32Reader.java:150)}}&lt;br/&gt;
{{ at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.loadMore(ReaderBasedJsonParser.java:153)}}&lt;br/&gt;
{{ at com.fasterxml.jackson.core.json.ReaderBasedJsonParser._skipWSOrEnd(ReaderBasedJsonParser.java:2017)}}&lt;br/&gt;
{{ at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:577)}}&lt;br/&gt;
{{ at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:350)}}&lt;br/&gt;
{{ at org.apache.spark.sql.catalyst.json.JacksonParser$$anonfun$parse$2.apply(JacksonParser.scala:347)}}&lt;br/&gt;
{{ at org.apache.spark.util.Utils$.tryWithResource(Utils.scala:2585)}}&lt;br/&gt;
{{ at org.apache.spark.sql.catalyst.json.JacksonParser.parse(JacksonParser.scala:347)}}&lt;br/&gt;
{{ at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$3.apply(JsonDataSource.scala:126)}}&lt;br/&gt;
{{ at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$3.apply(JsonDataSource.scala:126)}}&lt;br/&gt;
{{ at org.apache.spark.sql.execution.datasources.FailureSafeParser.parse(FailureSafeParser.scala:61)}}&lt;br/&gt;
{{ at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:130)}}&lt;br/&gt;
{{ at org.apache.spark.sql.execution.datasources.json.TextInputJsonDataSource$$anonfun$readFile$2.apply(JsonDataSource.scala:130)}}&lt;br/&gt;
{{ at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)}}&lt;br/&gt;
{{ at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)}}&lt;br/&gt;
{{ at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)}}&lt;br/&gt;
{{ at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:106)}}&lt;br/&gt;
{{ at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)}}&lt;br/&gt;
{{ at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)}}&lt;br/&gt;
{{ at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$10$$anon$1.hasNext(WholeStageCodegenExec.scala:614)}}&lt;br/&gt;
{{ at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:253)}}&lt;br/&gt;
{{ at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:247)}}&lt;br/&gt;
{{ at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)}}&lt;br/&gt;
{{ at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:830)}}&lt;br/&gt;
{{ at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)}}&lt;br/&gt;
{{ at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)}}&lt;br/&gt;
{{ at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)}}&lt;br/&gt;
{{ at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)}}&lt;br/&gt;
{{ at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:324)}}&lt;br/&gt;
{{ at org.apache.spark.rdd.RDD.iterator(RDD.scala:288)}}&lt;br/&gt;
{{ at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)}}&lt;br/&gt;
{{ at org.apache.spark.scheduler.Task.run(Task.scala:109)}}&lt;br/&gt;
{{ at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)}}&lt;br/&gt;
{{ at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)}}&lt;br/&gt;
{{ at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)}}&lt;br/&gt;
{{ at java.lang.Thread.run(Thread.java:748)}}&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Can any of you help me to ignore these json files and process with other valid files.&#160; ?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;

&lt;p&gt;Shiva Kumar SS&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13063657">SPARK-20314</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13131346">SPARK-23094</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12965255" name="corrupted.json" size="3398697" author="bijithkumar" created="Mon, 8 Apr 2019 22:43:21 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 16 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i30zhb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>