<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:14:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-2251] MLLib Naive Bayes Example SparkException: Can only zip RDDs with same number of elements in each partition</title>
                <link>https://issues.apache.org/jira/browse/SPARK-2251</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I follow the exact code from Naive Bayes Example (&lt;a href=&quot;http://spark.apache.org/docs/latest/mllib-naive-bayes.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/docs/latest/mllib-naive-bayes.html&lt;/a&gt;) of MLLib.&lt;/p&gt;

&lt;p&gt;When I executed the final command: &lt;br/&gt;
val accuracy = 1.0 * predictionAndLabel.filter(x =&amp;gt; x._1 == x._2).count() / test.count()&lt;/p&gt;

&lt;p&gt;It complains &quot;Can only zip RDDs with same number of elements in each partition&quot;.&lt;/p&gt;

&lt;p&gt;I got the following exception:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;14/06/23 19:39:23 INFO SparkContext: Starting job: count at &amp;lt;console&amp;gt;:31
14/06/23 19:39:23 INFO DAGScheduler: Got job 3 (count at &amp;lt;console&amp;gt;:31) with 2 output partitions (allowLocal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
14/06/23 19:39:23 INFO DAGScheduler: Final stage: Stage 4(count at &amp;lt;console&amp;gt;:31)
14/06/23 19:39:23 INFO DAGScheduler: Parents of &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; stage: List()
14/06/23 19:39:23 INFO DAGScheduler: Missing parents: List()
14/06/23 19:39:23 INFO DAGScheduler: Submitting Stage 4 (FilteredRDD[14] at filter at &amp;lt;console&amp;gt;:31), which has no missing parents
14/06/23 19:39:23 INFO DAGScheduler: Submitting 2 missing tasks from Stage 4 (FilteredRDD[14] at filter at &amp;lt;console&amp;gt;:31)
14/06/23 19:39:23 INFO TaskSchedulerImpl: Adding task set 4.0 with 2 tasks
14/06/23 19:39:23 INFO TaskSetManager: Starting task 4.0:0 as TID 8 on executor localhost: localhost (PROCESS_LOCAL)
14/06/23 19:39:23 INFO TaskSetManager: Serialized task 4.0:0 as 3410 bytes in 0 ms
14/06/23 19:39:23 INFO TaskSetManager: Starting task 4.0:1 as TID 9 on executor localhost: localhost (PROCESS_LOCAL)
14/06/23 19:39:23 INFO TaskSetManager: Serialized task 4.0:1 as 3410 bytes in 1 ms
14/06/23 19:39:23 INFO Executor: Running task ID 8
14/06/23 19:39:23 INFO Executor: Running task ID 9
14/06/23 19:39:23 INFO BlockManager: Found block broadcast_0 locally
14/06/23 19:39:23 INFO BlockManager: Found block broadcast_0 locally
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:0+24
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:24+24
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:0+24
14/06/23 19:39:23 INFO HadoopRDD: Input split: file:/home/jun/open_source/spark/mllib/data/sample_naive_bayes_data.txt:24+24
14/06/23 19:39:23 ERROR Executor: Exception in task ID 9
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:724)
14/06/23 19:39:23 ERROR Executor: Exception in task ID 8
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:724)
14/06/23 19:39:23 WARN TaskSetManager: Lost TID 8 (task 4.0:0)
14/06/23 19:39:23 WARN TaskSetManager: Loss was due to org.apache.spark.SparkException
org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
	at org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:724)
14/06/23 19:39:23 ERROR TaskSetManager: Task 4.0:0 failed 1 times; aborting job
14/06/23 19:39:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
14/06/23 19:39:23 INFO DAGScheduler: Failed to run count at &amp;lt;console&amp;gt;:31
14/06/23 19:39:23 INFO TaskSetManager: Loss was due to org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition [duplicate 1]
14/06/23 19:39:23 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
14/06/23 19:39:23 INFO TaskSchedulerImpl: Cancelling stage 4
org.apache.spark.SparkException: Job aborted due to stage failure: Task 4.0:0 failed 1 times, most recent failure: Exception failure in TID 8 on host localhost: org.apache.spark.SparkException: Can only zip RDDs with same number of elements in each partition
        org.apache.spark.rdd.RDD$$anonfun$zip$1$$anon$1.hasNext(RDD.scala:663)
        scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:388)
        org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1067)
        org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
        org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:858)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
        org.apache.spark.SparkContext$$anonfun$runJob$4.apply(SparkContext.scala:1079)
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
        org.apache.spark.scheduler.Task.run(Task.scala:51)
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
        java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:724)
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1038)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1022)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1020)
	at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1020)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:638)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:638)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:638)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1212)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:498)
	at akka.actor.ActorCell.invoke(ActorCell.scala:456)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:237)
	at akka.dispatch.Mailbox.run(Mailbox.scala:219)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment>&lt;p&gt;OS: Fedora Linux&lt;br/&gt;
Spark Version: 1.0.0. Git clone from the Spark Repository&lt;/p&gt;</environment>
        <key id="12723262">SPARK-2251</key>
            <summary>MLLib Naive Bayes Example SparkException: Can only zip RDDs with same number of elements in each partition</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mengxr">Xiangrui Meng</assignee>
                                    <reporter username="xiejuncs">Jun Xie</reporter>
                        <labels>
                            <label>Naive-Bayes</label>
                    </labels>
                <created>Tue, 24 Jun 2014 02:46:58 +0000</created>
                <updated>Sun, 11 Oct 2015 18:22:02 +0000</updated>
                            <resolved>Fri, 27 Jun 2014 04:47:24 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.0.1</fixVersion>
                    <fixVersion>1.1.0</fixVersion>
                                    <component>MLlib</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="14042738" author="srowen" created="Tue, 24 Jun 2014 21:43:49 +0000"  >&lt;p&gt;For what it&apos;s worth, I can reproduce this. In the sample, the &quot;test&quot; RDD has 2 partitions, containing 2 and 1 examples. The &quot;prediction&quot; RDD has 2 partitions, containing 1 and 2 examples respectively. So they aren&apos;t matched up, even though one is a 1-1 map() of the other. &lt;/p&gt;

&lt;p&gt;That seems like it shouldn&apos;t happen? maybe someone more knowledgeable can say whether that itself should occur. &quot;test&quot; is a PartitionwiseSampledRDD and &quot;prediction&quot; is a MappedRDD of course. &lt;/p&gt;

&lt;p&gt;If it is allowed to happen, then the example should be fixed, and I could easily supply a patch. It can be done without having to zip up RDDs to begin with.&lt;/p&gt;</comment>
                            <comment id="14043052" author="xiejuncs" created="Wed, 25 Jun 2014 05:27:15 +0000"  >&lt;p&gt;Hi, Sean. Thanks very much for your insight. I am new to Spark. So if you can easily supply a patch. Please. Really appreciate it.&lt;/p&gt;

&lt;p&gt;I am digging it according to your suggestion to see what is going on. At the same time, familiar myself with Spark. &lt;/p&gt;</comment>
                            <comment id="14043169" author="srowen" created="Wed, 25 Jun 2014 08:07:09 +0000"  >&lt;p&gt;Well the change to the examples is pretty straightforward. Instead of separately computing &quot;predictions&quot;, you just:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val predictionAndLabel = test.map(x =&amp;gt; (model.predict(x.features), x.label))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;... and similarly for other languages, and other examples. In fact it seems more straightforward.&lt;/p&gt;

&lt;p&gt;But I am wondering if this is actually a bug in PartitionwiseSampledRDD. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mengxr&quot; class=&quot;user-hover&quot; rel=&quot;mengxr&quot;&gt;mengxr&lt;/a&gt; is this a bit of code you wrote or are familiar with?&lt;/p&gt;</comment>
                            <comment id="14044391" author="mengxr" created="Thu, 26 Jun 2014 06:47:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xiejuncs&quot; class=&quot;user-hover&quot; rel=&quot;xiejuncs&quot;&gt;xiejuncs&lt;/a&gt; Are you running the example on the latest master or v1.0.0? I tested it on v1.0.0 and it worked well. But it did fail in the latest master. `RDD.zip` was modified after v1.0.0. So could you confirm the version you are running? &lt;/p&gt;</comment>
                            <comment id="14044490" author="mengxr" created="Thu, 26 Jun 2014 09:05:20 +0000"  >&lt;p&gt;Found a bug introduced by me in random sampler. PR: &lt;a href=&quot;https://github.com/apache/spark/pull/1229&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1229&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14045120" author="pwendell" created="Thu, 26 Jun 2014 20:33:35 +0000"  >&lt;p&gt;This is fixed in 1.0.1 via:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/1234/files&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1234/files&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14045462" author="xiejuncs" created="Fri, 27 Jun 2014 02:49:23 +0000"  >&lt;p&gt;I use the following command:&lt;/p&gt;

&lt;p&gt;git log&lt;/p&gt;

&lt;p&gt;The first entry is &lt;br/&gt;
commit 601032f5bfe2dcdc240bfcc553f401e6facbf5ec&lt;br/&gt;
Author: Zongheng Yang &amp;lt;zongheng.y@gmail.com&amp;gt;&lt;br/&gt;
Date:   Tue Jun 10 21:59:01 2014 -0700&lt;/p&gt;

&lt;p&gt;How to find out the current version of my branch? My current branch is in master, and I add Apache/Spark as the remote upstream.&lt;/p&gt;

&lt;p&gt;Jun&lt;/p&gt;</comment>
                            <comment id="14045515" author="pwendell" created="Fri, 27 Jun 2014 04:47:24 +0000"  >&lt;p&gt;Issue resolved by pull request 1229&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/1229&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1229&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14952374" author="apachespark" created="Sun, 11 Oct 2015 18:22:02 +0000"  >&lt;p&gt;User &apos;mengxr&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/1229&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1229&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>401449</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 6 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1x3mv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>401522</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>