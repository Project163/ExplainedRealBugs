<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:41:06 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-14675] ClassFormatError in codegen when using Aggregator</title>
                <link>https://issues.apache.org/jira/browse/SPARK-14675</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;code:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  val toList = new Aggregator[(String, Int), Seq[Int], Seq[Int]] {
    def bufferEncoder: Encoder[Seq[Int]] = implicitly[Encoder[Seq[Int]]]
    def finish(reduction: Seq[Int]): Seq[Int] = reduction
    def merge(b1: Seq[Int],b2: Seq[Int]): Seq[Int] = b1 ++ b2
    def outputEncoder: Encoder[Seq[Int]] = implicitly[Encoder[Seq[Int]]]
    def reduce(b: Seq[Int],a: (String, Int)): Seq[Int] = b :+ a._2
    def zero: Seq[Int] = Seq.empty[Int]
  }

  val ds1 = List((&quot;a&quot;, 1), (&quot;a&quot;, 2), (&quot;a&quot;, 3)).toDS
  val ds2 = ds1.groupByKey(_._1).agg(toList.toColumn)
  ds2.show
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;this gives me:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;6/04/15 18:31:22 WARN TaskSetManager: Lost task 1.0 in stage 3.0 (TID 7, localhost): java.lang.ClassFormatError: Duplicate field name&amp;amp;signature in class file org/apache/spark/sql/catalyst/expressions/GeneratedClass$SpecificMutableProjection
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:800)
	at org.codehaus.janino.ByteArrayClassLoader.findClass(ByteArrayClassLoader.java:66)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass.generate(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$create$2.apply(GenerateMutableProjection.scala:140)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$$anonfun$create$2.apply(GenerateMutableProjection.scala:139)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.generateProcessRow(AggregationIterator.scala:178)
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.&amp;lt;init&amp;gt;(AggregationIterator.scala:197)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.&amp;lt;init&amp;gt;(SortBasedAggregationIterator.scala:39)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregate$$anonfun$doExecute$1$$anonfun$3.apply(SortBasedAggregate.scala:80)
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregate$$anonfun$doExecute$1$$anonfun$3.apply(SortBasedAggregate.scala:71)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:768)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:768)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:72)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:239)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;when i do:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; ds2.queryExecution.debug.codegen()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;i get:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Found 2 WholeStageCodegen subtrees.
== Subtree 1 / 2 ==
WholeStageCodegen
:  +- Sort [value#6 ASC], false, 0
:     +- INPUT
+- AppendColumns &amp;lt;function1&amp;gt;, newInstance(class scala.Tuple2), [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String], true) AS value#6]
   +- LocalTableScan [_1#2,_2#3], [[0,1800000001,1,61],[0,1800000001,2,61],[0,1800000001,3,61]]

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */ 
/* 005 */ /** Codegened pipeline for:
/* 006 */ * Sort [value#6 ASC], false, 0
/* 007 */ +- INPUT
/* 008 */ */
/* 009 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 010 */   private Object[] references;
/* 011 */   private boolean sort_needToSort;
/* 012 */   private org.apache.spark.sql.execution.Sort sort_plan;
/* 013 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter;
/* 014 */   private org.apache.spark.executor.TaskMetrics sort_metrics;
/* 015 */   private scala.collection.Iterator&amp;lt;UnsafeRow&amp;gt; sort_sortedIter;
/* 016 */   private scala.collection.Iterator inputadapter_input;
/* 017 */   private org.apache.spark.sql.execution.metric.LongSQLMetric sort_dataSize;
/* 018 */   private org.apache.spark.sql.execution.metric.LongSQLMetricValue sort_metricValue;
/* 019 */   private org.apache.spark.sql.execution.metric.LongSQLMetric sort_spillSize;
/* 020 */   private org.apache.spark.sql.execution.metric.LongSQLMetricValue sort_metricValue1;
/* 021 */   
/* 022 */   public GeneratedIterator(Object[] references) {
/* 023 */     this.references = references;
/* 024 */   }
/* 025 */   
/* 026 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 027 */     partitionIndex = index;
/* 028 */     sort_needToSort = true;
/* 029 */     this.sort_plan = (org.apache.spark.sql.execution.Sort) references[0];
/* 030 */     sort_sorter = sort_plan.createSorter();
/* 031 */     sort_metrics = org.apache.spark.TaskContext.get().taskMetrics();
/* 032 */     
/* 033 */     inputadapter_input = inputs[0];
/* 034 */     this.sort_dataSize = (org.apache.spark.sql.execution.metric.LongSQLMetric) references[1];
/* 035 */     sort_metricValue = (org.apache.spark.sql.execution.metric.LongSQLMetricValue) sort_dataSize.localValue();
/* 036 */     this.sort_spillSize = (org.apache.spark.sql.execution.metric.LongSQLMetric) references[2];
/* 037 */     sort_metricValue1 = (org.apache.spark.sql.execution.metric.LongSQLMetricValue) sort_spillSize.localValue();
/* 038 */   }
/* 039 */   
/* 040 */   private void sort_addToSorter() throws java.io.IOException {
/* 041 */     /*** PRODUCE: INPUT */
/* 042 */     
/* 043 */     while (inputadapter_input.hasNext()) {
/* 044 */       InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();
/* 045 */       /*** CONSUME: Sort [value#6 ASC], false, 0 */
/* 046 */       
/* 047 */       sort_sorter.insertRow((UnsafeRow)inputadapter_row);
/* 048 */       if (shouldStop()) return;
/* 049 */     }
/* 050 */     
/* 051 */   }
/* 052 */   
/* 053 */   protected void processNext() throws java.io.IOException {
/* 054 */     /*** PRODUCE: Sort [value#6 ASC], false, 0 */
/* 055 */     if (sort_needToSort) {
/* 056 */       sort_addToSorter();
/* 057 */       Long sort_spillSizeBefore = sort_metrics.memoryBytesSpilled();
/* 058 */       sort_sortedIter = sort_sorter.sort();
/* 059 */       sort_metricValue.add(sort_sorter.getPeakMemoryUsage());
/* 060 */       sort_metricValue1.add(sort_metrics.memoryBytesSpilled() - sort_spillSizeBefore);
/* 061 */       sort_metrics.incPeakExecutionMemory(sort_sorter.getPeakMemoryUsage());
/* 062 */       sort_needToSort = false;
/* 063 */     }
/* 064 */     
/* 065 */     while (sort_sortedIter.hasNext()) {
/* 066 */       UnsafeRow sort_outputRow = (UnsafeRow)sort_sortedIter.next();
/* 067 */       
/* 068 */       /*** CONSUME: WholeStageCodegen */
/* 069 */       
/* 070 */       append(sort_outputRow);
/* 071 */       
/* 072 */       if (shouldStop()) return;
/* 073 */     }
/* 074 */   }
/* 075 */ }

== Subtree 2 / 2 ==
WholeStageCodegen
:  +- Sort [value#6 ASC], false, 0
:     +- INPUT
+- Exchange hashpartitioning(value#6, 4), None
   +- SortBasedAggregate(key=[value#6], functions=[(anon$1(scala.Tuple2),mode=Partial,isDistinct=false)], output=[value#6,value#15])
      +- WholeStageCodegen
         :  +- Sort [value#6 ASC], false, 0
         :     +- INPUT
         +- AppendColumns &amp;lt;function1&amp;gt;, newInstance(class scala.Tuple2), [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String], true) AS value#6]
            +- LocalTableScan [_1#2,_2#3], [[0,1800000001,1,61],[0,1800000001,2,61],[0,1800000001,3,61]]

Generated code:
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIterator(references);
/* 003 */ }
/* 004 */ 
/* 005 */ /** Codegened pipeline for:
/* 006 */ * Sort [value#6 ASC], false, 0
/* 007 */ +- INPUT
/* 008 */ */
/* 009 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 010 */   private Object[] references;
/* 011 */   private boolean sort_needToSort;
/* 012 */   private org.apache.spark.sql.execution.Sort sort_plan;
/* 013 */   private org.apache.spark.sql.execution.UnsafeExternalRowSorter sort_sorter;
/* 014 */   private org.apache.spark.executor.TaskMetrics sort_metrics;
/* 015 */   private scala.collection.Iterator&amp;lt;UnsafeRow&amp;gt; sort_sortedIter;
/* 016 */   private scala.collection.Iterator inputadapter_input;
/* 017 */   private org.apache.spark.sql.execution.metric.LongSQLMetric sort_dataSize;
/* 018 */   private org.apache.spark.sql.execution.metric.LongSQLMetricValue sort_metricValue;
/* 019 */   private org.apache.spark.sql.execution.metric.LongSQLMetric sort_spillSize;
/* 020 */   private org.apache.spark.sql.execution.metric.LongSQLMetricValue sort_metricValue1;
/* 021 */   
/* 022 */   public GeneratedIterator(Object[] references) {
/* 023 */     this.references = references;
/* 024 */   }
/* 025 */   
/* 026 */   public void init(int index, scala.collection.Iterator inputs[]) {
/* 027 */     partitionIndex = index;
/* 028 */     sort_needToSort = true;
/* 029 */     this.sort_plan = (org.apache.spark.sql.execution.Sort) references[0];
/* 030 */     sort_sorter = sort_plan.createSorter();
/* 031 */     sort_metrics = org.apache.spark.TaskContext.get().taskMetrics();
/* 032 */     
/* 033 */     inputadapter_input = inputs[0];
/* 034 */     this.sort_dataSize = (org.apache.spark.sql.execution.metric.LongSQLMetric) references[1];
/* 035 */     sort_metricValue = (org.apache.spark.sql.execution.metric.LongSQLMetricValue) sort_dataSize.localValue();
/* 036 */     this.sort_spillSize = (org.apache.spark.sql.execution.metric.LongSQLMetric) references[2];
/* 037 */     sort_metricValue1 = (org.apache.spark.sql.execution.metric.LongSQLMetricValue) sort_spillSize.localValue();
/* 038 */   }
/* 039 */   
/* 040 */   private void sort_addToSorter() throws java.io.IOException {
/* 041 */     /*** PRODUCE: INPUT */
/* 042 */     
/* 043 */     while (inputadapter_input.hasNext()) {
/* 044 */       InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();
/* 045 */       /*** CONSUME: Sort [value#6 ASC], false, 0 */
/* 046 */       
/* 047 */       sort_sorter.insertRow((UnsafeRow)inputadapter_row);
/* 048 */       if (shouldStop()) return;
/* 049 */     }
/* 050 */     
/* 051 */   }
/* 052 */   
/* 053 */   protected void processNext() throws java.io.IOException {
/* 054 */     /*** PRODUCE: Sort [value#6 ASC], false, 0 */
/* 055 */     if (sort_needToSort) {
/* 056 */       sort_addToSorter();
/* 057 */       Long sort_spillSizeBefore = sort_metrics.memoryBytesSpilled();
/* 058 */       sort_sortedIter = sort_sorter.sort();
/* 059 */       sort_metricValue.add(sort_sorter.getPeakMemoryUsage());
/* 060 */       sort_metricValue1.add(sort_metrics.memoryBytesSpilled() - sort_spillSizeBefore);
/* 061 */       sort_metrics.incPeakExecutionMemory(sort_sorter.getPeakMemoryUsage());
/* 062 */       sort_needToSort = false;
/* 063 */     }
/* 064 */     
/* 065 */     while (sort_sortedIter.hasNext()) {
/* 066 */       UnsafeRow sort_outputRow = (UnsafeRow)sort_sortedIter.next();
/* 067 */       
/* 068 */       /*** CONSUME: WholeStageCodegen */
/* 069 */       
/* 070 */       append(sort_outputRow);
/* 071 */       
/* 072 */       if (shouldStop()) return;
/* 073 */     }
/* 074 */   }
/* 075 */ }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment>&lt;p&gt;spark 2.0.0-SNAPSHOT&lt;/p&gt;</environment>
        <key id="12959308">SPARK-14675</key>
            <summary>ClassFormatError in codegen when using Aggregator</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cloud_fan">Wenchen Fan</assignee>
                                    <reporter username="koert">koert kuipers</reporter>
                        <labels>
                    </labels>
                <created>Fri, 15 Apr 2016 22:35:44 +0000</created>
                <updated>Tue, 19 Apr 2016 17:53:12 +0000</updated>
                            <resolved>Tue, 19 Apr 2016 17:53:12 +0000</resolved>
                                                    <fixVersion>2.0.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="15243770" author="rxin" created="Fri, 15 Apr 2016 22:38:18 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15243780" author="koert" created="Fri, 15 Apr 2016 22:53:05 +0000"  >&lt;p&gt;it works fine for other similar Aggregators, i think the main difference is that with this one i am using a Seq for the buffer type...&lt;/p&gt;</comment>
                            <comment id="15243788" author="koert" created="Fri, 15 Apr 2016 22:58:20 +0000"  >&lt;p&gt;for example it works fine if i use this aggregator instead:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  val sum = new Aggregator[(String, Int), Int, Int] {
    def bufferEncoder: Encoder[Int] = implicitly[Encoder[Int]]
    def finish(reduction: Int): Int = reduction
    def merge(b1: Int,b2: Int): Int = b1 + b2
    def outputEncoder: Encoder[Int] = implicitly[Encoder[Int]]
    def reduce(b: Int,a: (String, Int)): Int = b + a._2
    def zero: Int = 0
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15245330" author="apachespark" created="Mon, 18 Apr 2016 09:02:05 +0000"  >&lt;p&gt;User &apos;cloud-fan&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/12468&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/12468&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15248285" author="yhuai" created="Tue, 19 Apr 2016 17:53:12 +0000"  >&lt;p&gt;This issue has been resolved by &lt;a href=&quot;https://github.com/apache/spark/pull/12468&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/12468&lt;/a&gt;.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 31 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2w7b3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>