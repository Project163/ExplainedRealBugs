<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:51:02 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-19407] defaultFS is used FileSystem.get instead of getting it from uri scheme</title>
                <link>https://issues.apache.org/jira/browse/SPARK-19407</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Caused by: java.lang.IllegalArgumentException: Wrong FS: s3a://**************/checkpoint/7b2231a3-d845-4740-bfa3-681850e5987f/metadata, expected: &lt;a href=&quot;file:///&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;file:///&lt;/a&gt;&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:649)&lt;br/&gt;
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:82)&lt;br/&gt;
	at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:606)&lt;br/&gt;
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:824)&lt;br/&gt;
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:601)&lt;br/&gt;
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:421)&lt;br/&gt;
	at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.StreamMetadata$.read(StreamMetadata.scala:51)&lt;br/&gt;
	at org.apache.spark.sql.execution.streaming.StreamExecution.&amp;lt;init&amp;gt;(StreamExecution.scala:100)&lt;br/&gt;
	at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:232)&lt;br/&gt;
	at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:269)&lt;br/&gt;
	at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:262)&lt;/p&gt;

&lt;p&gt;Can easily replicate on spark standalone cluster by providing checkpoint location uri scheme anything other than &quot;file://&quot; and not overriding in config.&lt;/p&gt;

&lt;p&gt;WorkAround  --conf spark.hadoop.fs.defaultFS=s3a://somebucket or set it in sparkConf or spark-default.conf&lt;/p&gt;</description>
                <environment></environment>
        <key id="13039018">SPARK-19407</key>
            <summary>defaultFS is used FileSystem.get instead of getting it from uri scheme</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="uncleGen">Genmao Yu</assignee>
                                    <reporter username="amit.assudani@gmail.com">Amit Assudani</reporter>
                        <labels>
                            <label>checkpoint</label>
                            <label>filesystem</label>
                            <label>starter</label>
                            <label>streaming</label>
                    </labels>
                <created>Mon, 30 Jan 2017 22:24:08 +0000</created>
                <updated>Wed, 6 Nov 2019 09:41:32 +0000</updated>
                            <resolved>Tue, 7 Feb 2017 05:03:52 +0000</resolved>
                                    <version>2.1.0</version>
                                    <fixVersion>2.1.1</fixVersion>
                    <fixVersion>2.2.0</fixVersion>
                                    <component>Structured Streaming</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="15846188" author="zsxwing" created="Tue, 31 Jan 2017 00:07:06 +0000"  >&lt;p&gt;Good catch. Do you want to submit a PR to fix it? It&apos;s just replacing &quot;FileSystem.get(hadoopConf)&quot; with &quot;metadataFile.getFileSystem(hadoopConf)&quot;.&lt;/p&gt;</comment>
                            <comment id="15846930" author="amit.assudani@gmail.com" created="Tue, 31 Jan 2017 15:01:18 +0000"  >&lt;p&gt;I&apos;ll do it over the weekend, you may assign it to me. I may need some logistics support as it will be my first PR.&lt;/p&gt;</comment>
                            <comment id="15851290" author="stevel@apache.org" created="Fri, 3 Feb 2017 10:16:55 +0000"  >&lt;p&gt;Yes, looks like  &lt;tt&gt;StreamMetadata.read()&lt;/tt&gt; is getting it wrong. Which is funny, as &lt;tt&gt;StreamingQueryManager.createQuery()&lt;/tt&gt; gets it right&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &lt;span class=&quot;code-comment&quot;&gt;/** Read the metadata from file &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; it exists */&lt;/span&gt;
  def read(metadataFile: Path, hadoopConf: Configuration): Option[StreamMetadata] = {
    val fs = FileSystem.get(hadoopConf)  &lt;span class=&quot;code-comment&quot;&gt;/* HERE */&lt;/span&gt;
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (fs.exists(metadataFile)) {
      &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; input: FSDataInputStream = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
      &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;when it should be&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val fs = FileSystem.get(metadataFile, hadoopConf)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The hard part will be testing this&lt;/p&gt;</comment>
                            <comment id="15853213" author="srowen" created="Sun, 5 Feb 2017 11:55:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aassudani&quot; class=&quot;user-hover&quot; rel=&quot;aassudani&quot;&gt;aassudani&lt;/a&gt; are you making a pull request?&lt;/p&gt;</comment>
                            <comment id="15853426" author="unclegen" created="Mon, 6 Feb 2017 01:27:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aassudani&quot; class=&quot;user-hover&quot; rel=&quot;aassudani&quot;&gt;aassudani&lt;/a&gt; Are you still working on this? As this issue is clear and easy to fix, I will making a pr later if it is busy for you to work on this.&lt;/p&gt;</comment>
                            <comment id="15853431" author="apachespark" created="Mon, 6 Feb 2017 01:40:03 +0000"  >&lt;p&gt;User &apos;uncleGen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16815&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16815&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16968221" author="hryhoriev.nick" created="Wed, 6 Nov 2019 09:35:36 +0000"  >&lt;p&gt;I have the same issue with spark 2.4.4.&lt;br/&gt;
 When I Use spark on YARN in Client mode.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
val sparkConf = SparkOnYarnAppController.sparkHadoopKeys(configuration)
 .foldLeft(&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf()
 .setAppName(appName)
 .setIfMissing(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.ui.enabled&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&quot;&lt;/span&gt;)
 .setMaster(&lt;span class=&quot;code-quote&quot;&gt;&quot;yarn&quot;&lt;/span&gt;)
 .setIfMissing(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.hadoop.yarn.resourcemanager.hostname&quot;&lt;/span&gt;,s&lt;span class=&quot;code-quote&quot;&gt;&quot;hadoop-$cluster.com&quot;&lt;/span&gt;)
 .setIfMissing(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.yarn.archive&quot;&lt;/span&gt;, s&lt;span class=&quot;code-quote&quot;&gt;&quot;hdfs:&lt;span class=&quot;code-comment&quot;&gt;///sparkDistributions/$distribution.tgz&quot;&lt;/span&gt;)
&lt;/span&gt; .setIfMissing(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.dynamicAllocation.enabled&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&quot;&lt;/span&gt;)
 .setIfMissing(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.driver.memory&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1g&quot;&lt;/span&gt;)
 .setIfMissing(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.driver.cores&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;)
 .setIfMissing(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.executor.memory&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1g&quot;&lt;/span&gt;)
 .setIfMissing(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.executor.instances&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;)
 .setIfMissing(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.executor.cores&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;)
 .setIfMissing(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.yarn.maxAppAttempts&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;)
 )((sparkConf, hadoopProps) =&amp;gt; sparkConf.set(hadoopProps._1, hadoopProps._2))
SparkSession.builder().config(sparkConf).getOrCreate()
&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;TestRecord(partition: &lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;, value: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;)
object CommonTools {

implicit &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;MockDataframe(session: SparkSession) {
implicit val sqlContext: SQLContext = session.sqlContext
def mockDataFrame[T: Encoder](mockData: Seq[T]): DataFrame ={ 
 val mockStream = MemoryStream[T]
 mockStream.addData(mockData)
 mockStream.toDF() }
}
implicit &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;StreamSinkToHadoopFileSystem(dataFrame: DataFrame) {
 def sinkToS3(s3path: Str ing, format: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, checkpointDir: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, trigger:  Trigger): StreamingQuery ={
 dataFrame.writeStream
 .format(&lt;span class=&quot;code-quote&quot;&gt;&quot;parquet&quot;&lt;/span&gt;)
 .queryName(&lt;span class=&quot;code-quote&quot;&gt;&quot;Test-TooManyVersionPerRootPrefixInS3&quot;&lt;/span&gt;)
 .trigger(trigger)
 .option(&lt;span class=&quot;code-quote&quot;&gt;&quot;checkpointLocation&quot;&lt;/span&gt;, checkpointDir)
 .format(format)
 .partitionBy(&lt;span class=&quot;code-quote&quot;&gt;&quot;partition&quot;&lt;/span&gt;)
 .option(&lt;span class=&quot;code-quote&quot;&gt;&quot;path&quot;&lt;/span&gt;, s3path) .start() }
 }
}
&#160;
val stream = sparkSession
 .mockDataFrame[TestRecord]((0 to 100).map { i =&amp;gt; TestRecord(i, s&lt;span class=&quot;code-quote&quot;&gt;&quot;i-${UUID.randomUUID().toString}&quot;&lt;/span&gt;) })
 .sinkToS3(
 s3path = s&lt;span class=&quot;code-quote&quot;&gt;&quot;$outputDir/TooManyVersionPerRootPrefixInS3/&quot;&lt;/span&gt;,
 format = &lt;span class=&quot;code-quote&quot;&gt;&quot;parquet&quot;&lt;/span&gt;,
 checkpointDir = s&lt;span class=&quot;code-quote&quot;&gt;&quot;$checkpointDir/TooManyVersionPerRootPrefixInS3-checkpoint&quot;&lt;/span&gt;,
 trigger = Trigger.ProcessingTime(5.seconds)
 )&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;exception&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2019-11-06 11:31:47 ERROR org.apache.spark.sql.execution.streaming.StreamMetadata:91 - Error writing stream metadata StreamMetadata(bc32a9a9-8328-406d-8b37-30770e10962b) to s3a:&lt;span class=&quot;code-comment&quot;&gt;//bukcet/mhr/TooManyVersionPerRootPrefixInS3-checkpoint/metadata2019-11-06 11:31:47 ERROR org.apache.spark.sql.execution.streaming.StreamMetadata:91 - Error writing stream metadata StreamMetadata(bc32a9a9-8328-406d-8b37-30770e10962b) to s3a://af-eu-west-1-stg-data-lake-orc-with-crr/mhr/TooManyVersionPerRootPrefixInS3-checkpoint/metadataorg.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories. at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:400) at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:461) at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:200) at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:475) at org.apache.hadoop.fs.s3a.S3AOutputStream.&amp;lt;init&amp;gt;(S3AOutputStream.java:66) at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:663) at org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1177) at org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100) at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605) at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:703) at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:699) at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90) at org.apache.hadoop.fs.FileContext.create(FileContext.java:699) at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:311)Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.&amp;lt;init&amp;gt;(CheckpointFileManager.scala:133) at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.&amp;lt;init&amp;gt;(CheckpointFileManager.scala:136) at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:318) at org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:78) at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.apply(StreamExecution.scala:125) at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.apply(StreamExecution.scala:123) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.streaming.StreamExecution.&amp;lt;init&amp;gt;(StreamExecution.scala:123) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.&amp;lt;init&amp;gt;(MicroBatchExecution.scala:48) at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:275) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:316) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)org.apache.hadoop.util.DiskChecker$DiskErrorException: No space available in any of the local directories. at com.appsflyer.spark.s3.it.CommonTools$StreamSinkToHadoopFileSystem.sinkToS3(TooManyVersionPerRootPrefixInS3.scala:45) at com.appsflyer.spark.s3.it.TooManyVersionPerRootPrefixInS3$.main(TooManyVersionPerRootPrefixInS3.scala:80) at com.appsflyer.spark.s3.it.TooManyVersionPerRootPrefixInS3.main(TooManyVersionPerRootPrefixInS3.scala) at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.getLocalPathForWrite(LocalDirAllocator.java:400) at org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext.createTmpFileForWrite(LocalDirAllocator.java:461) at org.apache.hadoop.fs.LocalDirAllocator.createTmpFileForWrite(LocalDirAllocator.java:200) at org.apache.hadoop.fs.s3a.S3AFileSystem.createTmpFileForWrite(S3AFileSystem.java:475) at org.apache.hadoop.fs.s3a.S3AOutputStream.&amp;lt;init&amp;gt;(S3AOutputStream.java:66) at org.apache.hadoop.fs.s3a.S3AFileSystem.create(S3AFileSystem.java:663) at org.apache.hadoop.fs.FileSystem.primitiveCreate(FileSystem.java:1177) at org.apache.hadoop.fs.DelegateToFileSystem.createInternal(DelegateToFileSystem.java:100) at org.apache.hadoop.fs.AbstractFileSystem.create(AbstractFileSystem.java:605) at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:703) at org.apache.hadoop.fs.FileContext$3.next(FileContext.java:699) at org.apache.hadoop.fs.FSLinkResolver.resolve(FSLinkResolver.java:90) at org.apache.hadoop.fs.FileContext.create(FileContext.java:699) at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createTempFile(CheckpointFileManager.scala:311) at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.&amp;lt;init&amp;gt;(CheckpointFileManager.scala:133) at org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.&amp;lt;init&amp;gt;(CheckpointFileManager.scala:136) at org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.createAtomic(CheckpointFileManager.scala:318) at org.apache.spark.sql.execution.streaming.StreamMetadata$.write(StreamMetadata.scala:78) at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.apply(StreamExecution.scala:125) at org.apache.spark.sql.execution.streaming.StreamExecution$$anonfun$2.apply(StreamExecution.scala:123) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.execution.streaming.StreamExecution.&amp;lt;init&amp;gt;(StreamExecution.scala:123) at org.apache.spark.sql.execution.streaming.MicroBatchExecution.&amp;lt;init&amp;gt;(MicroBatchExecution.scala:48) at org.apache.spark.sql.streaming.StreamingQueryManager.createQuery(StreamingQueryManager.scala:275) at org.apache.spark.sql.streaming.StreamingQueryManager.startQuery(StreamingQueryManager.scala:316) at org.apache.spark.sql.streaming.DataStreamWriter.start(DataStreamWriter.scala:325)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16968226" author="hryhoriev.nick" created="Wed, 6 Nov 2019 09:41:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=uncleGen&quot; class=&quot;user-hover&quot; rel=&quot;uncleGen&quot;&gt;uncleGen&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310250" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10431"><![CDATA[Important]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 1 week, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i39e1b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12338779">2.1.1</customfieldvalue>
    <customfieldvalue id="12338275">2.2.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>