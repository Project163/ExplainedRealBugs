<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:45:26 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-16233] test_sparkSQL.R is failing</title>
                <link>https://issues.apache.org/jira/browse/SPARK-16233</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;By running &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;./R/run-tests.sh 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Getting error:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;xin:spark xr$ ./R/run-tests.sh
Warning: Ignoring non-spark config property: SPARK_SCALA_VERSION=2.11
Loading required &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;: methods

Attaching &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;: &#8216;SparkR&#8217;

The following object is masked from &#8216;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;:testthat&#8217;:

    describe

The following objects are masked from &#8216;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;:stats&#8217;:

    cov, filter, lag, na.omit, predict, sd, &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;, window

The following objects are masked from &#8216;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;:base&#8217;:

    as.data.frame, colnames, colnames&amp;lt;-, drop, endsWith, intersect,
    rank, rbind, sample, startsWith, subset, summary, transform, union

binary functions: ...........
functions on binary files: ....
broadcast variables: ..
functions in client.R: .....
test functions in sparkR.R: .....Re-using existing Spark Context. Call sparkR.session.stop() or restart R to create a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Spark Context
....Re-using existing Spark Context. Call sparkR.session.stop() or restart R to create a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Spark Context
...........
include an external JAR in SparkContext: Warning: Ignoring non-spark config property: SPARK_SCALA_VERSION=2.11
..
include R packages:
MLlib functions: .........................SLF4J: Failed to load class &lt;span class=&quot;code-quote&quot;&gt;&quot;org.slf4j.impl.StaticLoggerBinder&quot;&lt;/span&gt;.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http:&lt;span class=&quot;code-comment&quot;&gt;//www.slf4j.org/codes.html#StaticLoggerBinder &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; further details.
&lt;/span&gt;.27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 65,622
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [label] BINARY: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [terms, list, element, list, element] BINARY: 2 values, 42B raw, 43B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [hasIntercept] BOOLEAN: 1 values, 1B raw, 3B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 49
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 90B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [labels, list, element] BINARY: 3 values, 50B raw, 50B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 92
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [vectorCol] BINARY: 1 values, 18B raw, 20B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 126B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [prefixesToRewrite, key_value, key] BINARY: 2 values, 61B raw, 61B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 58B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [prefixesToRewrite, key_value, value] BINARY: 2 values, 15B raw, 17B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY], dic { 1 entries, 12B raw, 1B comp}
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 54
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 122B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [columnsToPrune, list, element] BINARY: 2 values, 59B raw, 59B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 56
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [intercept] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [coefficients, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [coefficients, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [coefficients, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [coefficients, values, list, element] DOUBLE: 3 values, 37B raw, 38B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer ver.................................................................W..........
parallelize() and collect(): .............................
...................................................................................................................................................................................................................................................................
SerDe functionality: ...................
partitionBy, groupByKey, reduceByKey etc.: ....................
SparkSQL functions: .........................................................S................................................................................................................................................................................................................................................................S......................................................................................................................................................................1.....................................S                                                        .....................
tests RDD function take(): ................
the textFile() function: .............
functions in utils.R: ....................................
Windows-specific tests: S

Skipped ------------------------------------------------------------------------
1. create DataFrame from RDD (@test_sparkSQL.R#200) - Hive is not build with SparkSQL, skipped

2. test HiveContext (@test_sparkSQL.R#1003) - Hive is not build with SparkSQL, skipped

3. enableHiveSupport on SparkSession (@test_sparkSQL.R#2395) - Hive is not build with SparkSQL, skipped

4. sparkJars tag in SparkContext (@test_Windows.R#21) - This test is only &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Windows, skipped

Warnings -----------------------------------------------------------------------
1. spark.naiveBayes (@test_mllib.R#390) - `not()` is deprecated.

Failed -------------------------------------------------------------------------
1. Error: read/write ORC files (@test_sparkSQL.R#1705) -------------------------
org.apache.spark.sql.AnalysisException: The ORC data source must be used with Hive support enabled;
	at org.apache.spark.sql.execution.datasources.DataSource.lookupDataSource(DataSource.scala:137)
	at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:78)
	at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:78)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:414)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:141)
	at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:86)
	at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
1: write.df(df, orcPath, &lt;span class=&quot;code-quote&quot;&gt;&quot;orc&quot;&lt;/span&gt;, mode = &lt;span class=&quot;code-quote&quot;&gt;&quot;overwrite&quot;&lt;/span&gt;) at /Users/xin/workspace/spark/R/lib/SparkR/tests/testthat/test_sparkSQL.R:1705
2: write.df(df, orcPath, &lt;span class=&quot;code-quote&quot;&gt;&quot;orc&quot;&lt;/span&gt;, mode = &lt;span class=&quot;code-quote&quot;&gt;&quot;overwrite&quot;&lt;/span&gt;)
3: .local(df, path, ...)
4: callJMethod(write, &lt;span class=&quot;code-quote&quot;&gt;&quot;save&quot;&lt;/span&gt;, path)
5: invokeJava(isStatic = FALSE, objId$id, methodName, ...)
6: stop(readString(conn))

DONE ===========================================================================
Error: Test failures
Execution halted
Warning: Ignoring non-spark config property: SPARK_SCALA_VERSION=2.11
Loading required &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;: methods

Attaching &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;: &#8216;SparkR&#8217;

The following object is masked from &#8216;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;:testthat&#8217;:

    describe

The following objects are masked from &#8216;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;:stats&#8217;:

    cov, filter, lag, na.omit, predict, sd, &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;, window

The following objects are masked from &#8216;&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;:base&#8217;:

    as.data.frame, colnames, colnames&amp;lt;-, drop, endsWith, intersect,
    rank, rbind, sample, startsWith, subset, summary, transform, union

binary functions: ...........
functions on binary files: ....
broadcast variables: ..
functions in client.R: .....
test functions in sparkR.R: .....Re-using existing Spark Context. Call sparkR.session.stop() or restart R to create a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Spark Context
....Re-using existing Spark Context. Call sparkR.session.stop() or restart R to create a &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Spark Context
...........
include an external JAR in SparkContext: Warning: Ignoring non-spark config property: SPARK_SCALA_VERSION=2.11
..
include R packages:
MLlib functions: .........................SLF4J: Failed to load class &lt;span class=&quot;code-quote&quot;&gt;&quot;org.slf4j.impl.StaticLoggerBinder&quot;&lt;/span&gt;.
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http:&lt;span class=&quot;code-comment&quot;&gt;//www.slf4j.org/codes.html#StaticLoggerBinder &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; further details.
&lt;/span&gt;.27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 65,622
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 70B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [label] BINARY: 1 values, 21B raw, 23B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 87B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [terms, list, element, list, element] BINARY: 2 values, 42B raw, 43B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:25 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [hasIntercept] BOOLEAN: 1 values, 1B raw, 3B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 49
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 90B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [labels, list, element] BINARY: 3 values, 50B raw, 50B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 92
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 61B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [vectorCol] BINARY: 1 values, 18B raw, 20B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 126B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [prefixesToRewrite, key_value, key] BINARY: 2 values, 61B raw, 61B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:26 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 58B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [prefixesToRewrite, key_value, value] BINARY: 2 values, 15B raw, 17B comp, 1 pages, encodings: [RLE, PLAIN_DICTIONARY], dic { 1 entries, 12B raw, 1B comp}
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 54
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 122B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [columnsToPrune, list, element] BINARY: 2 values, 59B raw, 59B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer version is: PARQUET_1_0
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Maximum row group padding size is 0 bytes
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.InternalParquetRecordWriter: Flushing mem columnStore to file. allocated memory: 56
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 51B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [intercept] DOUBLE: 1 values, 8B raw, 10B comp, 1 pages, encodings: [PLAIN, BIT_PACKED]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 45B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [coefficients, type] INT32: 1 values, 10B raw, 12B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 30B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [coefficients, size] INT32: 1 values, 7B raw, 9B comp, 1 pages, encodings: [PLAIN, RLE, BIT_PACKED]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 36B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [coefficients, indices, list, element] INT32: 1 values, 13B raw, 15B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ColumnChunkPageWriteStore: written 79B &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [coefficients, values, list, element] DOUBLE: 3 values, 37B raw, 38B comp, 1 pages, encodings: [PLAIN, RLE]
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.codec.CodecConfig: Compression: SNAPPY
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet block size to 134217728
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Parquet dictionary page size to 1048576
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Dictionary is on
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Validation is off
27-Jun-2016 1:51:27 PM INFO: org.apache.parquet.hadoop.ParquetOutputFormat: Writer ver.................................................................W..........
parallelize() and collect(): .............................
...................................................................................................................................................................................................................................................................
SerDe functionality: ...................
partitionBy, groupByKey, reduceByKey etc.: ....................
SparkSQL functions: .........................................................S................................................................................................................................................................................................................................................................S......................................................................................................................................................................1.....................................S                                                        .....................
tests RDD function take(): ................
the textFile() function: .............
functions in utils.R: ....................................
Windows-specific tests: S

Skipped ------------------------------------------------------------------------
1. create DataFrame from RDD (@test_sparkSQL.R#200) - Hive is not build with SparkSQL, skipped

2. test HiveContext (@test_sparkSQL.R#1003) - Hive is not build with SparkSQL, skipped

3. enableHiveSupport on SparkSession (@test_sparkSQL.R#2395) - Hive is not build with SparkSQL, skipped

4. sparkJars tag in SparkContext (@test_Windows.R#21) - This test is only &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Windows, skipped

Warnings -----------------------------------------------------------------------
1. spark.naiveBayes (@test_mllib.R#390) - `not()` is deprecated.

Failed -------------------------------------------------------------------------
1. Error: read/write ORC files (@test_sparkSQL.R#1705) -------------------------
org.apache.spark.sql.AnalysisException: The ORC data source must be used with Hive support enabled;
	at org.apache.spark.sql.execution.datasources.DataSource.lookupDataSource(DataSource.scala:137)
	at org.apache.spark.sql.execution.datasources.DataSource.providingClass$lzycompute(DataSource.scala:78)
	at org.apache.spark.sql.execution.datasources.DataSource.providingClass(DataSource.scala:78)
	at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:414)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:211)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:483)
	at org.apache.spark.api.r.RBackendHandler.handleMethodCall(RBackendHandler.scala:141)
	at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:86)
	at org.apache.spark.api.r.RBackendHandler.channelRead0(RBackendHandler.scala:38)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
1: write.df(df, orcPath, &lt;span class=&quot;code-quote&quot;&gt;&quot;orc&quot;&lt;/span&gt;, mode = &lt;span class=&quot;code-quote&quot;&gt;&quot;overwrite&quot;&lt;/span&gt;) at /Users/xin/workspace/spark/R/lib/SparkR/tests/testthat/test_sparkSQL.R:1705
2: write.df(df, orcPath, &lt;span class=&quot;code-quote&quot;&gt;&quot;orc&quot;&lt;/span&gt;, mode = &lt;span class=&quot;code-quote&quot;&gt;&quot;overwrite&quot;&lt;/span&gt;)
3: .local(df, path, ...)
4: callJMethod(write, &lt;span class=&quot;code-quote&quot;&gt;&quot;save&quot;&lt;/span&gt;, path)
5: invokeJava(isStatic = FALSE, objId$id, methodName, ...)
6: stop(readString(conn))

DONE ===========================================================================
Error: Test failures
Execution halted
Had test failures; see logs.&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Cause: most probably these tests are using &apos;createDataFrame(sqlContext...)&apos; which is deprecated. Should update tests method invocations. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12983920">SPARK-16233</key>
            <summary>test_sparkSQL.R is failing</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dongjoon">Dongjoon Hyun</assignee>
                                    <reporter username="iamshrek">Xin Ren</reporter>
                        <labels>
                    </labels>
                <created>Mon, 27 Jun 2016 20:01:33 +0000</created>
                <updated>Sat, 2 Jul 2016 19:28:06 +0000</updated>
                            <resolved>Fri, 1 Jul 2016 22:35:40 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>SparkR</component>
                    <component>Tests</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="15351703" author="iamshrek" created="Mon, 27 Jun 2016 20:01:46 +0000"  >&lt;p&gt;I&apos;m working on this&lt;/p&gt;</comment>
                            <comment id="15352885" author="prashant_" created="Tue, 28 Jun 2016 12:11:04 +0000"  >&lt;p&gt;Did you build spark with -Phive ?&lt;/p&gt;</comment>
                            <comment id="15353340" author="iamshrek" created="Tue, 28 Jun 2016 16:36:42 +0000"  >&lt;p&gt;this is what I used to build sparkR, should I add &quot;-Phive&quot;? sorry I&apos;m new to this part.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;build/mvn -DskipTests -Psparkr &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15353344" author="shivaram" created="Tue, 28 Jun 2016 16:40:14 +0000"  >&lt;p&gt;Ideally we should skip the tests which depend on Hive if SparkR was built without Hive. Was there some recent change in how ORC files are handled ? &lt;/p&gt;

&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dongjoon&quot; class=&quot;user-hover&quot; rel=&quot;dongjoon&quot;&gt;dongjoon&lt;/a&gt; who added read.orc, write.orc support&lt;/p&gt;</comment>
                            <comment id="15353352" author="iamshrek" created="Tue, 28 Jun 2016 16:45:47 +0000"  >&lt;p&gt;Actually I was just following the docs here &lt;a href=&quot;https://github.com/keypointt/spark/tree/master/R#examples-unit-tests&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/keypointt/spark/tree/master/R#examples-unit-tests&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Maybe we should update the docs here to point it out that &quot;-Phive&quot; could be needed?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;build/mvn -DskipTests -Psparkr &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;You can also run the unit tests &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; SparkR by running. You need to install the testthat &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt; first:

R -e &lt;span class=&quot;code-quote&quot;&gt;&apos;install.packages(&lt;span class=&quot;code-quote&quot;&gt;&quot;testthat&quot;&lt;/span&gt;, repos=&lt;span class=&quot;code-quote&quot;&gt;&quot;http:&lt;span class=&quot;code-comment&quot;&gt;//cran.us.r-project.org&quot;&lt;/span&gt;)&apos;&lt;/span&gt;
&lt;/span&gt;./R/run-tests.sh
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15353388" author="dongjoon" created="Tue, 28 Jun 2016 17:11:06 +0000"  >&lt;p&gt;Oh, thank you for pinging me. For this issue, correctly, your concern is right.&lt;br/&gt;
My bads. That will be the root cause.&lt;/p&gt;</comment>
                            <comment id="15353405" author="dongjoon" created="Tue, 28 Jun 2016 17:20:12 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shivaram&quot; class=&quot;user-hover&quot; rel=&quot;shivaram&quot;&gt;shivaram&lt;/a&gt;.&lt;br/&gt;
Actually, in `DataFrameReaderWriterSuite.scala`, the ORC test is also marked `ignored`.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/test/DataFrameReaderWriterSuite.scala#L353&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/core/src/test/scala/org/apache/spark/sql/test/DataFrameReaderWriterSuite.scala#L353&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;All the other ORC tests are inside `spark/sql/hive` package. I think we can simply make that test just as `ignored` like Scala testsuite.&lt;/p&gt;</comment>
                            <comment id="15359661" author="shivaram" created="Fri, 1 Jul 2016 21:27:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dongjoon&quot; class=&quot;user-hover&quot; rel=&quot;dongjoon&quot;&gt;dongjoon&lt;/a&gt; Thanks for taking a look. Can you send a PR to ignore this test for now ?&lt;/p&gt;</comment>
                            <comment id="15359683" author="dongjoon" created="Fri, 1 Jul 2016 21:38:09 +0000"  >&lt;p&gt;Oh, Sure.&lt;br/&gt;
I thought &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=iamshrek&quot; class=&quot;user-hover&quot; rel=&quot;iamshrek&quot;&gt;iamshrek&lt;/a&gt; was woking on this issue.&lt;br/&gt;
I&apos;ll make a PR now.&lt;/p&gt;</comment>
                            <comment id="15359717" author="dongjoon" created="Fri, 1 Jul 2016 22:01:41 +0000"  >&lt;p&gt;hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shivaram&quot; class=&quot;user-hover&quot; rel=&quot;shivaram&quot;&gt;shivaram&lt;/a&gt;.&lt;br/&gt;
I made a PR for this.&lt;/p&gt;</comment>
                            <comment id="15359725" author="apachespark" created="Fri, 1 Jul 2016 22:12:04 +0000"  >&lt;p&gt;User &apos;dongjoon-hyun&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14019&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14019&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15359726" author="dongjoon" created="Fri, 1 Jul 2016 22:12:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/14019&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14019&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15359763" author="shivaram" created="Fri, 1 Jul 2016 22:35:40 +0000"  >&lt;p&gt;Issue resolved by pull request 14019&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14019&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14019&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15360058" author="iamshrek" created="Sat, 2 Jul 2016 08:07:53 +0000"  >&lt;p&gt;Oh sorry I thought you guys would take over so I stopped working on this one.&lt;/p&gt;

&lt;p&gt;Thanks a lot resolving this &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_up.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15360299" author="dongjoon" created="Sat, 2 Jul 2016 19:28:06 +0000"  >&lt;p&gt;Sorry for not being clear!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 20 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i307jr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>