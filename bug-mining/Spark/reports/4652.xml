<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:52:00 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-19872] UnicodeDecodeError in Pyspark on sc.textFile read with repartition</title>
                <link>https://issues.apache.org/jira/browse/SPARK-19872</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I&apos;m receiving the following traceback:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt;&amp;gt;&amp;gt; sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&apos;test.txt&apos;&lt;/span&gt;).repartition(10).collect()
Traceback (most recent call last):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;stdin&amp;gt;&quot;&lt;/span&gt;, line 1, in &amp;lt;module&amp;gt;
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/rdd.py&quot;&lt;/span&gt;, line 810, in collect
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; list(_load_from_socket(port, self._jrdd_deserializer))
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/rdd.py&quot;&lt;/span&gt;, line 140, in _load_from_socket
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; item in serializer.load_stream(rf):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/serializers.py&quot;&lt;/span&gt;, line 539, in load_stream
    yield self.loads(stream)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/serializers.py&quot;&lt;/span&gt;, line 534, in loads
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; s.decode(&lt;span class=&quot;code-quote&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; self.use_unicode &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; s
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/Users/brianbruggeman/.envs/dg/lib/python2.7/encodings/utf_8.py&quot;&lt;/span&gt;, line 16, in decode
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: &lt;span class=&quot;code-quote&quot;&gt;&apos;utf8&apos;&lt;/span&gt; codec can&apos;t decode &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; 0x80 in position 0: invalid start &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I created a textfile (text.txt) with standard linux newlines:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;a
b

d
e
f
g
h
i
j
k
l

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think ran pyspark:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ pyspark
Python 2.7.13 (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, Dec 18 2016, 07:03:39)
[GCC 4.2.1 Compatible Apple LLVM 8.0.0 (clang-800.0.42.1)] on darwin
Type &lt;span class=&quot;code-quote&quot;&gt;&quot;help&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;copyright&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;credits&quot;&lt;/span&gt; or &lt;span class=&quot;code-quote&quot;&gt;&quot;license&quot;&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; more information.
Using Spark&apos;s &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; log4j profile: org/apache/spark/log4j-defaults.properties
Setting &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; log level to &lt;span class=&quot;code-quote&quot;&gt;&quot;WARN&quot;&lt;/span&gt;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/03/08 13:59:27 WARN NativeCodeLoader: Unable to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your platform... using builtin-java classes where applicable
17/03/08 13:59:32 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &apos;_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.0
      /_/

Using Python version 2.7.13 (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, Dec 18 2016 07:03:39)
SparkSession available as &lt;span class=&quot;code-quote&quot;&gt;&apos;spark&apos;&lt;/span&gt;.
&amp;gt;&amp;gt;&amp;gt; sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&apos;test.txt&apos;&lt;/span&gt;).collect()
[u&lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;c&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;d&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;e&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;f&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;g&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;h&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;i&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;j&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;k&apos;&lt;/span&gt;, u&lt;span class=&quot;code-quote&quot;&gt;&apos;l&apos;&lt;/span&gt;]
&amp;gt;&amp;gt;&amp;gt; sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&apos;test.txt&apos;&lt;/span&gt;, use_unicode=False).collect()
[&lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;c&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;d&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;e&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;f&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;g&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;h&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;i&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;j&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;k&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;l&apos;&lt;/span&gt;]
&amp;gt;&amp;gt;&amp;gt; sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&apos;test.txt&apos;&lt;/span&gt;, use_unicode=False).repartition(10).collect()
[&lt;span class=&quot;code-quote&quot;&gt;&apos;\x80\x02]q\x01(U\x01aU\x01bU\x01cU\x01dU\x01eU\x01fU\x01ge.&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;\x80\x02]q\x01(U\x01hU\x01iU\x01jU\x01kU\x01le.&apos;&lt;/span&gt;]
&amp;gt;&amp;gt;&amp;gt; sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&apos;test.txt&apos;&lt;/span&gt;).repartition(10).collect()
Traceback (most recent call last):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;stdin&amp;gt;&quot;&lt;/span&gt;, line 1, in &amp;lt;module&amp;gt;
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/rdd.py&quot;&lt;/span&gt;, line 810, in collect
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; list(_load_from_socket(port, self._jrdd_deserializer))
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/rdd.py&quot;&lt;/span&gt;, line 140, in _load_from_socket
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; item in serializer.load_stream(rf):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/serializers.py&quot;&lt;/span&gt;, line 539, in load_stream
    yield self.loads(stream)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/Cellar/apache-spark/2.1.0/libexec/python/pyspark/serializers.py&quot;&lt;/span&gt;, line 534, in loads
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; s.decode(&lt;span class=&quot;code-quote&quot;&gt;&quot;utf-8&quot;&lt;/span&gt;) &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; self.use_unicode &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; s
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/Users/brianbruggeman/.envs/dg/lib/python2.7/encodings/utf_8.py&quot;&lt;/span&gt;, line 16, in decode
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; codecs.utf_8_decode(input, errors, True)
UnicodeDecodeError: &lt;span class=&quot;code-quote&quot;&gt;&apos;utf8&apos;&lt;/span&gt; codec can&apos;t decode &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt; 0x80 in position 0: invalid start &lt;span class=&quot;code-object&quot;&gt;byte&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This really looks like a bug in the `serializers.py` code.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Mac and EC2&lt;/p&gt;</environment>
        <key id="13049391">SPARK-19872</key>
            <summary>UnicodeDecodeError in Pyspark on sc.textFile read with repartition</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gurwls223">Hyukjin Kwon</assignee>
                                    <reporter username="bbruggeman">Brian Bruggeman</reporter>
                        <labels>
                    </labels>
                <created>Wed, 8 Mar 2017 20:50:32 +0000</created>
                <updated>Mon, 12 Dec 2022 18:10:34 +0000</updated>
                            <resolved>Wed, 15 Mar 2017 17:14:32 +0000</resolved>
                                    <version>2.1.0</version>
                                    <fixVersion>2.1.1</fixVersion>
                    <fixVersion>2.2.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="15901974" author="bbruggeman" created="Wed, 8 Mar 2017 21:09:11 +0000"  >&lt;p&gt;This is a regression from spark 2.0.x.&lt;/p&gt;</comment>
                            <comment id="15901997" author="bbruggeman" created="Wed, 8 Mar 2017 21:23:43 +0000"  >&lt;p&gt;I reverted `rdd.py` and `serializers.py` to the 2.0.2 branch in github and the code above works without an error.&lt;/p&gt;

&lt;p&gt;rdd.py link: &lt;a href=&quot;https://github.com/apache/spark/blob/branch-2.0/python/pyspark/rdd.py&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/branch-2.0/python/pyspark/rdd.py&lt;/a&gt;&lt;br/&gt;
serializers.py link: &lt;a href=&quot;https://github.com/apache/spark/blob/branch-2.0/python/pyspark/serializers.py&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/branch-2.0/python/pyspark/serializers.py&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;rdd diff:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;--- tmp2.py 2017-03-08 15:17:45.000000000 -0600
+++ saved2.py   2017-03-08 15:17:59.000000000 -0600
@@ -52,8 +52,6 @@
     get_used_memory, ExternalSorter, ExternalGroupBy
 from pyspark.traceback_utils &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SCCallSiteSync

-from py4j.java_collections &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; ListConverter, MapConverter
-

 __all__ = [&lt;span class=&quot;code-quote&quot;&gt;&quot;RDD&quot;&lt;/span&gt;]

@@ -137,11 +135,12 @@
         &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt;
     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; not sock:
         raise Exception(&lt;span class=&quot;code-quote&quot;&gt;&quot;could not open socket&quot;&lt;/span&gt;)
-    # The RDD materialization time is unpredicable, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; we set a timeout &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; socket reading
-    # operation, it will very possibly fail. See SPARK-18281.
-    sock.settimeout(None)
-    # The socket will be automatically closed when garbage-collected.
-    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; serializer.load_stream(sock.makefile(&lt;span class=&quot;code-quote&quot;&gt;&quot;rb&quot;&lt;/span&gt;, 65536))
+    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
+        rf = sock.makefile(&lt;span class=&quot;code-quote&quot;&gt;&quot;rb&quot;&lt;/span&gt;, 65536)
+        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; item in serializer.load_stream(rf):
+            yield item
+    &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt;:
+        sock.close()


 def ignore_unicode_prefix(f):
@@ -264,13 +263,44 @@

     def isCheckpointed(self):
         &quot;&quot;&quot;
-        Return whether &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; RDD has been checkpointed or not
+        Return whether &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; RDD is checkpointed and materialized, either reliably or locally.
         &quot;&quot;&quot;
         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; self._jrdd.rdd().isCheckpointed()

+    def localCheckpoint(self):
+        &quot;&quot;&quot;
+        Mark &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; RDD &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; local checkpointing using Spark&apos;s existing caching layer.
+
+        This method is &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; users who wish to truncate RDD lineages &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; skipping the expensive
+        step of replicating the materialized data in a reliable distributed file system. This is
+        useful &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; RDDs with &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; lineages that need to be truncated periodically (e.g. GraphX).
+
+        Local checkpointing sacrifices fault-tolerance &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; performance. In particular, checkpointed
+        data is written to ephemeral local storage in the executors instead of to a reliable,
+        fault-tolerant storage. The effect is that &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; an executor fails during the computation,
+        the checkpointed data may no longer be accessible, causing an irrecoverable job failure.
+
+        This is NOT safe to use with dynamic allocation, which removes executors along
+        with their cached blocks. If you must use both features, you are advised to set
+        L{spark.dynamicAllocation.cachedExecutorIdleTimeout} to a high value.
+
+        The checkpoint directory set through L{SparkContext.setCheckpointDir()} is not used.
+        &quot;&quot;&quot;
+        self._jrdd.rdd().localCheckpoint()
+
+    def isLocallyCheckpointed(self):
+        &quot;&quot;&quot;
+        Return whether &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; RDD is marked &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; local checkpointing.
+
+        Exposed &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; testing.
+        &quot;&quot;&quot;
+        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; self._jrdd.rdd().isLocallyCheckpointed()
+
     def getCheckpointFile(self):
         &quot;&quot;&quot;
         Gets the name of the file to which &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; RDD was checkpointed
+
+        Not defined &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; RDD is checkpointed locally.
         &quot;&quot;&quot;
         checkpointFile = self._jrdd.rdd().getCheckpointFile()
         &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; checkpointFile.isDefined():
@@ -387,6 +417,9 @@
             with replacement: expected number of times each element is chosen; fraction must be &amp;gt;= 0
         :param seed: seed &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the random number generator

+        .. note:: This is not guaranteed to provide exactly the fraction specified of the total
+            count of the given :class:`DataFrame`.
+
         &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(100), 4)
         &amp;gt;&amp;gt;&amp;gt; 6 &amp;lt;= rdd.sample(False, 0.1, 81).count() &amp;lt;= 14
         True
@@ -425,8 +458,8 @@
         &quot;&quot;&quot;
         Return a fixed-size sampled subset of &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; RDD.

-        Note that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting array is expected
-        to be small, as all the data is loaded into the driver&apos;s memory.
+        .. note:: This method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting array is expected
+            to be small, as all the data is loaded into the driver&apos;s memory.

         &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize(range(0, 10))
         &amp;gt;&amp;gt;&amp;gt; len(rdd.takeSample(True, 20, 1))
@@ -537,7 +570,7 @@
         Return the intersection of &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; RDD and another one. The output will
         not contain any duplicate elements, even &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the input RDDs did.

-        Note that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method performs a shuffle internally.
+        .. note:: This method performs a shuffle internally.

         &amp;gt;&amp;gt;&amp;gt; rdd1 = sc.parallelize([1, 10, 2, 3, 4, 5])
         &amp;gt;&amp;gt;&amp;gt; rdd2 = sc.parallelize([1, 6, 2, 3, 7, 8])
@@ -768,8 +801,9 @@
     def collect(self):
         &quot;&quot;&quot;
         Return a list that contains all of the elements in &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; RDD.
-        Note that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting array is expected
-        to be small, as all the data is loaded into the driver&apos;s memory.
+
+        .. note:: This method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting array is expected
+            to be small, as all the data is loaded into the driver&apos;s memory.
         &quot;&quot;&quot;
         with SCCallSiteSync(self.context) as css:
             port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())
@@ -1214,12 +1248,12 @@

     def top(self, num, key=None):
         &quot;&quot;&quot;
-        Get the top N elements from a RDD.
+        Get the top N elements from an RDD.

-        Note that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting array is expected
-        to be small, as all the data is loaded into the driver&apos;s memory.
+        .. note:: This method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting array is expected
+            to be small, as all the data is loaded into the driver&apos;s memory.

-        Note: It returns the list sorted in descending order.
+        .. note:: It returns the list sorted in descending order.

         &amp;gt;&amp;gt;&amp;gt; sc.parallelize([10, 4, 2, 12, 3]).top(1)
         [12]
@@ -1238,11 +1272,11 @@

     def takeOrdered(self, num, key=None):
         &quot;&quot;&quot;
-        Get the N elements from a RDD ordered in ascending order or as
+        Get the N elements from an RDD ordered in ascending order or as
         specified by the optional key function.

-        Note that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting array is expected
-        to be small, as all the data is loaded into the driver&apos;s memory.
+        .. note:: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting array is expected
+            to be small, as all the data is loaded into the driver&apos;s memory.

         &amp;gt;&amp;gt;&amp;gt; sc.parallelize([10, 1, 2, 9, 3, 4, 5, 6, 7]).takeOrdered(6)
         [1, 2, 3, 4, 5, 6]
@@ -1263,11 +1297,11 @@
         that partition to estimate the number of additional partitions needed
         to satisfy the limit.

-        Note that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting array is expected
-        to be small, as all the data is loaded into the driver&apos;s memory.
-
         Translated from the Scala implementation in RDD#take().

+        .. note:: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting array is expected
+            to be small, as all the data is loaded into the driver&apos;s memory.
+
         &amp;gt;&amp;gt;&amp;gt; sc.parallelize([2, 3, 4, 5, 6]).cache().take(2)
         [2, 3]
         &amp;gt;&amp;gt;&amp;gt; sc.parallelize([2, 3, 4, 5, 6]).take(10)
@@ -1331,8 +1365,9 @@

     def isEmpty(self):
         &quot;&quot;&quot;
-        Returns &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; and only &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the RDD contains no elements at all. Note that an RDD
-        may be empty even when it has at least 1 partition.
+        Returns &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; and only &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the RDD contains no elements at all.
+
+        .. note:: an RDD may be empty even when it has at least 1 partition.

         &amp;gt;&amp;gt;&amp;gt; sc.parallelize([]).isEmpty()
         True
@@ -1523,8 +1558,8 @@
         &quot;&quot;&quot;
         Return the key-value pairs in &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; RDD to the master as a dictionary.

-        Note that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting data is expected
-        to be small, as all the data is loaded into the driver&apos;s memory.
+        .. note:: &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; method should only be used &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the resulting data is expected
+            to be small, as all the data is loaded into the driver&apos;s memory.

         &amp;gt;&amp;gt;&amp;gt; m = sc.parallelize([(1, 2), (3, 4)]).collectAsMap()
         &amp;gt;&amp;gt;&amp;gt; m[1]
@@ -1761,8 +1796,7 @@
         set of aggregation functions.

         Turns an RDD[(K, V)] into a result of type RDD[(K, C)], &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; a &quot;combined
-        type&quot; C.  Note that V and C can be different -- &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; example, one might
-        group an RDD of type (Int, Int) into an RDD of type (Int, List[Int]).
+        type&quot; C.

         Users provide three functions:

@@ -1774,6 +1808,9 @@

         In addition, users can control the partitioning of the output RDD.

+        .. note:: V and C can be different -- &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; example, one might group an RDD of type
+            (Int, Int) into an RDD of type (Int, List[Int]).
+
         &amp;gt;&amp;gt;&amp;gt; x = sc.parallelize([(&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;, 1), (&lt;span class=&quot;code-quote&quot;&gt;&quot;b&quot;&lt;/span&gt;, 1), (&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;, 1)])
         &amp;gt;&amp;gt;&amp;gt; def add(a, b): &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; a + str(b)
         &amp;gt;&amp;gt;&amp;gt; sorted(x.combineByKey(str, add, add).collect())
@@ -1845,9 +1882,9 @@
         Group the values &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; each key in the RDD into a single sequence.
         Hash-partitions the resulting RDD with numPartitions partitions.

-        Note: If you are grouping in order to perform an aggregation (such as a
-        sum or average) over each key, using reduceByKey or aggregateByKey will
-        provide much better performance.
+        .. note:: If you are grouping in order to perform an aggregation (such as a
+            sum or average) over each key, using reduceByKey or aggregateByKey will
+            provide much better performance.

         &amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([(&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;, 1), (&lt;span class=&quot;code-quote&quot;&gt;&quot;b&quot;&lt;/span&gt;, 1), (&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;, 1)])
         &amp;gt;&amp;gt;&amp;gt; sorted(rdd.groupByKey().mapValues(len).collect())
@@ -2018,8 +2055,7 @@
          &amp;gt;&amp;gt;&amp;gt; len(rdd.repartition(10).glom().collect())
          10
         &quot;&quot;&quot;
-        jrdd = self._jrdd.repartition(numPartitions)
-        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; RDD(jrdd, self.ctx, self._jrdd_deserializer)
+        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; self.coalesce(numPartitions, shuffle=True)

     def coalesce(self, numPartitions, shuffle=False):
         &quot;&quot;&quot;
@@ -2030,7 +2066,15 @@
         &amp;gt;&amp;gt;&amp;gt; sc.parallelize([1, 2, 3, 4, 5], 3).coalesce(1).glom().collect()
         [[1, 2, 3, 4, 5]]
         &quot;&quot;&quot;
-        jrdd = self._jrdd.coalesce(numPartitions, shuffle)
+        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; shuffle:
+            # Decrease the batch size in order to distribute evenly the elements across output
+            # partitions. Otherwise, repartition will possibly produce highly skewed partitions.
+            batchSize = min(10, self.ctx._batchSize or 1024)
+            ser = BatchedSerializer(PickleSerializer(), batchSize)
+            selfCopy = self._reserialize(ser)
+            jrdd = selfCopy._jrdd.coalesce(numPartitions, shuffle)
+        &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;:
+            jrdd = self._jrdd.coalesce(numPartitions, shuffle)
         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; RDD(jrdd, self.ctx, self._jrdd_deserializer)

     def zip(self, other):
@@ -2316,16 +2360,9 @@
         # The broadcast will have same life cycle as created PythonRDD
         broadcast = sc.broadcast(pickled_command)
         pickled_command = ser.dumps(broadcast)
-    # There is a bug in py4j.java_gateway.JavaClass with auto_convert
-    # https:&lt;span class=&quot;code-comment&quot;&gt;//github.com/bartdag/py4j/issues/161
&lt;/span&gt;-    # TODO: use auto_convert once py4j fix the bug
-    broadcast_vars = ListConverter().convert(
-        [x._jbroadcast &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; x in sc._pickled_broadcast_vars],
-        sc._gateway._gateway_client)
+    broadcast_vars = [x._jbroadcast &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; x in sc._pickled_broadcast_vars]
     sc._pickled_broadcast_vars.clear()
-    env = MapConverter().convert(sc.environment, sc._gateway._gateway_client)
-    includes = ListConverter().convert(sc._python_includes, sc._gateway._gateway_client)
-    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; pickled_command, broadcast_vars, env, includes
+    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; pickled_command, broadcast_vars, sc.environment, sc._python_includes


 def _wrap_function(sc, func, deserializer, serializer, profiler=None):
@@ -2433,4 +2470,4 @@


 &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; __name__ == &lt;span class=&quot;code-quote&quot;&gt;&quot;__main__&quot;&lt;/span&gt;:
-    _test()
\ No newline at end of file
+    _test()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;serializers diff:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;--- tmp.py  2017-03-08 15:13:45.000000000 -0600
+++ &amp;lt;redacted&amp;gt;/lib/python2.7/site-packages/pyspark/serializers.py   2017-03-08 15:13:03.000000000 -0600
@@ -61,7 +61,7 @@
 &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; sys.version &amp;lt; &lt;span class=&quot;code-quote&quot;&gt;&apos;3&apos;&lt;/span&gt;:
     &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; cPickle as pickle
     protocol = 2
-    from itertools &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; izip as zip
+    from itertools &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; izip as zip, imap as map
 &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;:
     &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; pickle
     protocol = 3
@@ -96,7 +96,12 @@
         raise NotImplementedError

     def _load_stream_without_unbatching(self, stream):
-        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; self.load_stream(stream)
+        &quot;&quot;&quot;
+        Return an iterator of deserialized batches (lists) of objects from the input stream.
+        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; the serializer does not operate on batches the &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; implementation returns an
+        iterator of single element lists.
+        &quot;&quot;&quot;
+        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; map(lambda x: [x], self.load_stream(stream))

     # Note: our notion of &lt;span class=&quot;code-quote&quot;&gt;&quot;equality&quot;&lt;/span&gt; is that output generated by
     # equal serializers can be deserialized using the same serializer.
@@ -278,50 +283,57 @@
         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;AutoBatchedSerializer(%s)&quot;&lt;/span&gt; % self.serializer


-&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;CartesianDeserializer(FramedSerializer):
+&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;CartesianDeserializer(Serializer):

     &quot;&quot;&quot;
     Deserializes the JavaRDD cartesian() of two PythonRDDs.
+    Due to pyspark batching we cannot simply use the result of the Java RDD cartesian,
+    we additionally need to &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; the cartesian within each pair of batches.
     &quot;&quot;&quot;

     def __init__(self, key_ser, val_ser):
-        FramedSerializer.__init__(self)
         self.key_ser = key_ser
         self.val_ser = val_ser

-    def prepare_keys_values(self, stream):
-        key_stream = self.key_ser._load_stream_without_unbatching(stream)
-        val_stream = self.val_ser._load_stream_without_unbatching(stream)
-        key_is_batched = isinstance(self.key_ser, BatchedSerializer)
-        val_is_batched = isinstance(self.val_ser, BatchedSerializer)
-        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (keys, vals) in zip(key_stream, val_stream):
-            keys = keys &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; key_is_batched &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; [keys]
-            vals = vals &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; val_is_batched &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; [vals]
-            yield (keys, vals)
+    def _load_stream_without_unbatching(self, stream):
+        key_batch_stream = self.key_ser._load_stream_without_unbatching(stream)
+        val_batch_stream = self.val_ser._load_stream_without_unbatching(stream)
+        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (key_batch, val_batch) in zip(key_batch_stream, val_batch_stream):
+            # &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; correctness with repeated cartesian/zip &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; must be returned as one batch
+            yield product(key_batch, val_batch)

     def load_stream(self, stream):
-        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (keys, vals) in self.prepare_keys_values(stream):
-            &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; pair in product(keys, vals):
-                yield pair
+        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; chain.from_iterable(self._load_stream_without_unbatching(stream))

     def __repr__(self):
         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;CartesianDeserializer(%s, %s)&quot;&lt;/span&gt; % \
                (str(self.key_ser), str(self.val_ser))


-&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;PairDeserializer(CartesianDeserializer):
+&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;PairDeserializer(Serializer):

     &quot;&quot;&quot;
     Deserializes the JavaRDD zip() of two PythonRDDs.
+    Due to pyspark batching we cannot simply use the result of the Java RDD zip,
+    we additionally need to &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; the zip within each pair of batches.
     &quot;&quot;&quot;

+    def __init__(self, key_ser, val_ser):
+        self.key_ser = key_ser
+        self.val_ser = val_ser
+
+    def _load_stream_without_unbatching(self, stream):
+        key_batch_stream = self.key_ser._load_stream_without_unbatching(stream)
+        val_batch_stream = self.val_ser._load_stream_without_unbatching(stream)
+        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (key_batch, val_batch) in zip(key_batch_stream, val_batch_stream):
+            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; len(key_batch) != len(val_batch):
+                raise ValueError(&lt;span class=&quot;code-quote&quot;&gt;&quot;Can not deserialize PairRDD with different number of items&quot;&lt;/span&gt;
+                                 &lt;span class=&quot;code-quote&quot;&gt;&quot; in batches: (%d, %d)&quot;&lt;/span&gt; % (len(key_batch), len(val_batch)))
+            # &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; correctness with repeated cartesian/zip &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; must be returned as one batch
+            yield zip(key_batch, val_batch)
+
     def load_stream(self, stream):
-        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (keys, vals) in self.prepare_keys_values(stream):
-            &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; len(keys) != len(vals):
-                raise ValueError(&lt;span class=&quot;code-quote&quot;&gt;&quot;Can not deserialize RDD with different number of items&quot;&lt;/span&gt;
-                                 &lt;span class=&quot;code-quote&quot;&gt;&quot; in pair: (%d, %d)&quot;&lt;/span&gt; % (len(keys), len(vals)))
-            &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; pair in zip(keys, vals):
-                yield pair
+        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; chain.from_iterable(self._load_stream_without_unbatching(stream))

     def __repr__(self):
         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;PairDeserializer(%s, %s)&quot;&lt;/span&gt; % (str(self.key_ser), str(self.val_ser))
@@ -378,6 +390,16 @@
     _old_namedtuple = _copy_func(collections.namedtuple)

     def namedtuple(*args, **kwargs):
+        &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; sys
+        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; sys.version.startswith(&lt;span class=&quot;code-quote&quot;&gt;&apos;3&apos;&lt;/span&gt;):
+            defaults = {
+                &lt;span class=&quot;code-quote&quot;&gt;&apos;verbose&apos;&lt;/span&gt;: False,
+                &lt;span class=&quot;code-quote&quot;&gt;&apos;rename&apos;&lt;/span&gt;: False,
+                &lt;span class=&quot;code-quote&quot;&gt;&apos;module&apos;&lt;/span&gt;: None,
+            }
+            &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; key, value in defaults.items():
+                &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; key not in kwargs:
+                    kwargs[key] = value
         cls = _old_namedtuple(*args, **kwargs)
         &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; _hack_namedtuple(cls)

@@ -559,4 +581,4 @@
     &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; doctest
     (failure_count, test_count) = doctest.testmod()
     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; failure_count:
-        exit(-1)
\ No newline at end of file
+        exit(-1)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15902023" author="bbruggeman" created="Wed, 8 Mar 2017 21:45:54 +0000"  >&lt;p&gt;Using the Spark 2.1.0 serializers.py and the Spark 2.0.2 rdd.py, the code runs.&lt;/p&gt;</comment>
                            <comment id="15902684" author="srowen" created="Thu, 9 Mar 2017 08:42:57 +0000"  >&lt;p&gt;(Blocker is for committers to determine)&lt;/p&gt;</comment>
                            <comment id="15922777" author="apachespark" created="Mon, 13 Mar 2017 19:14:03 +0000"  >&lt;p&gt;User &apos;HyukjinKwon&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/17282&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/17282&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15926992" author="bbruggeman" created="Wed, 15 Mar 2017 21:00:42 +0000"  >&lt;p&gt;Wondering if a test will be added to prevent future regressions.&lt;/p&gt;</comment>
                            <comment id="15927122" author="gurwls223" created="Wed, 15 Mar 2017 22:45:23 +0000"  >&lt;p&gt;Yup, test was added.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 35 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3b533:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>