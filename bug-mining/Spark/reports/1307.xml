<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:22:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-5395] Large number of Python workers causing resource depletion</title>
                <link>https://issues.apache.org/jira/browse/SPARK-5395</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;During job execution a large number of Python worker accumulates eventually causing YARN to kill containers for being over their memory allocation (in the case below that is about 8G for executors plus 6G for overhead per container). &lt;/p&gt;

&lt;p&gt;In this instance, at the time of killing the container 97 pyspark.daemon processes had accumulated.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2015-01-23 15:36:53,654 INFO [Reporter] yarn.YarnAllocationHandler (Logging.scala:logInfo(59)) - Container marked as failed: container_1421692415636_0052_01_000030. Exit status: 143. Diagnostics: Container [pid=35211,containerID=container_1421692415636_0052_01_000030] is running beyond physical memory limits. Current usage: 14.9 GB of 14.5 GB physical memory used; 41.3 GB of 72.5 GB virtual memory used. Killing container.
Dump of the process-tree for container_1421692415636_0052_01_000030 :
|- PID PPID PGRPID SESSID CMD_NAME USER_MODE_TIME(MILLIS) SYSTEM_TIME(MILLIS) VMEM_USAGE(BYTES) RSSMEM_USAGE(PAGES) FULL_CMD_LINE
|- 54101 36625 36625 35211 (python) 78 1 332730368 16834 python -m pyspark.daemon
|- 52140 36625 36625 35211 (python) 58 1 332730368 16837 python -m pyspark.daemon
|- 36625 35228 36625 35211 (python) 65 604 331685888 17694 python -m pyspark.daemon
	[...]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The configuration used uses 64 containers with 2 cores each.&lt;/p&gt;

&lt;p&gt;Full output here: &lt;a href=&quot;https://gist.github.com/skrasser/e3e2ee8dede5ef6b082c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/skrasser/e3e2ee8dede5ef6b082c&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Mailinglist discussion: &lt;a href=&quot;https://www.mail-archive.com/user@spark.apache.org/msg20102.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.mail-archive.com/user@spark.apache.org/msg20102.html&lt;/a&gt;&lt;/p&gt;</description>
                <environment>&lt;p&gt;AWS ElasticMapReduce&lt;/p&gt;</environment>
        <key id="12769827">SPARK-5395</key>
            <summary>Large number of Python workers causing resource depletion</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="davies">Davies Liu</assignee>
                                    <reporter username="skrasser">Sven Krasser</reporter>
                        <labels>
                    </labels>
                <created>Sat, 24 Jan 2015 08:19:40 +0000</created>
                <updated>Tue, 17 Feb 2015 04:36:04 +0000</updated>
                            <resolved>Tue, 17 Feb 2015 04:35:59 +0000</resolved>
                                    <version>1.2.0</version>
                    <version>1.3.0</version>
                                    <fixVersion>1.2.2</fixVersion>
                    <fixVersion>1.3.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>3</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="14292121" author="mkman84" created="Mon, 26 Jan 2015 17:21:27 +0000"  >&lt;p&gt;Having the same issue in standalone deployment mode. A single spark-submitted job is spawning a ton of pyspark.daemon instances and depleting the cluster memory even though the appropriate environment variables have been set.&lt;/p&gt;</comment>
                            <comment id="14292630" author="skrasser" created="Mon, 26 Jan 2015 23:36:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mkman84&quot; class=&quot;user-hover&quot; rel=&quot;mkman84&quot;&gt;mkman84&lt;/a&gt;, do you also see this for both spark.python.worker.reuse false/true? (FWIW, the run I pasted above had reuse disabled.)&lt;/p&gt;

&lt;p&gt;Also, do you happen to have a small job that can be used as a repro (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=davies&quot; class=&quot;user-hover&quot; rel=&quot;davies&quot;&gt;davies&lt;/a&gt; was asking for one, so far I only managed to trigger this condition using production data).&lt;/p&gt;</comment>
                            <comment id="14292668" author="mkman84" created="Mon, 26 Jan 2015 23:56:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=skrasser&quot; class=&quot;user-hover&quot; rel=&quot;skrasser&quot;&gt;skrasser&lt;/a&gt;, I actually only managed to have this reproduced using production data as well (so far). I&apos;ll try to write a simple version tomorrow but it seems that it&apos;s a mix of both python worker processes not being killed after it&apos;s no longer running (causing build up), as well as the python worker exceeding the allocated memory limit. &lt;/p&gt;

&lt;p&gt;I think it &lt;b&gt;may&lt;/b&gt; be related to a couple of specific actions such as groupByKey/cogroup, though I&apos;ll still need to do some tests to be sure what&apos;s causing this.&lt;/p&gt;

&lt;p&gt;I should also add that we haven&apos;t modified the default for the python.worker.reuse variable, so in our case it should be using the default of True.&lt;/p&gt;</comment>
                            <comment id="14292769" author="mkman84" created="Tue, 27 Jan 2015 01:04:21 +0000"  >&lt;p&gt;This may prove to be useful...&lt;/p&gt;

&lt;p&gt;I&apos;m watching a presently running spark-submitted job, while watching the pyspark.daemon processes. The framework is permitted to only use 8 cores on each node with the default python worker memory of 512mb per node (not the executor memory which is set to higher than this).&lt;/p&gt;

&lt;p&gt;Ignoring the exact RDD actions for a moment, it looks like while it transitions from Stage 1 -&amp;gt; Stage 2, it spawned up 8-10 additional pyspark.daemon processes making the box use more cores than it was even allowed to... A few seconds after that, the other 8 processes entered a sleeping state while still holding onto the physical memory it ate up in Stage 1. As soon as Stage 2 finished, practically all of the pyspark.daemons vanished and freed up the memory usage. I was keeping an eye on 2 random nodes and the exact same thing occurred on both. It was also the only currently executing job at the time so there was really no other interference/contention for resources.&lt;/p&gt;

&lt;p&gt;I will try to provide a bit more detail on the exact transformations/actions occurring between the 2 stages, although I know a PartionBy and cogroup are occurring at the very least without inspecting the spark-submitted code directly.&lt;/p&gt;</comment>
                            <comment id="14292913" author="skrasser" created="Tue, 27 Jan 2015 03:14:33 +0000"  >&lt;p&gt;Some additional findings from my side: I&apos;ve managed to trigger the problem using a simpler job on production data that basically does a reduceByKey followed by a count action. I get &amp;gt;20 workers (2 cores per executor) before any tasks in the first stage (reduceByKey) complete (i.e. different from the stage transition behavior you noticed). However, this doesn&apos;t occur if I run over a smaller data set, i.e. fewer production data files.&lt;/p&gt;

&lt;p&gt;Before calling reduceByKey I have a coalesce call. Without that the error does not occur (at least in this smaller script). This at first glance looked potentially spilling related (more data per task), but attempting to force spills by setting the worker memory very low did not help with my attempts to get a repro on test data.&lt;/p&gt;</comment>
                            <comment id="14293653" author="mkman84" created="Tue, 27 Jan 2015 15:05:23 +0000"  >&lt;p&gt;Actually I think I know why this happens... I&apos;m thinking the problem really occurs due to the way auto-persistence of specific actions occur. &lt;/p&gt;

&lt;p&gt;ReduceByKey, GroupByKey, cogroup, etc, are typically heavy actions that get auto-persisted for the reason that the resulting RDD&apos;s will most likely be used for something right after. &lt;/p&gt;

&lt;p&gt;The interesting thing is that this memory is outside of the executor memory for the framework (it&apos;s what goes into these pyspark daemons that get spawned up temporarily). The other interesting fact is that let&apos;s say we leave the default python worker memory set to 512MB, and you have a framework that uses 8 cores on each executor, it spawns up 8 * 512MB (4GB) of python workers while the stage is running. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=skrasser&quot; class=&quot;user-hover&quot; rel=&quot;skrasser&quot;&gt;skrasser&lt;/a&gt; In your case, if you chain a bunch of auto-persisting actions (which I believe coalesce is a part of, since instead of dealing with a shuffle read, it instead builds a potentially large array of partitions on the executor), it will spawn an additional 2 python workers per executor for that separate task, while the previous tasks&apos; python workers are left in a sleeping state, waiting for the results of the subsequent task to complete... &lt;/p&gt;

&lt;p&gt;If that&apos;s the case, then it should actually be a bit easier showing how a single framework can nuke a single host by creating a crazy chain of coalescing/reduceByKey/GroupByKey/cogrouping actions (which I&apos;m off to try out now haha)&lt;/p&gt;

&lt;p&gt;EDIT: I&apos;m almost positive this is what&apos;s causing this to occur now. Unfortunately there is no easy way to prevent a single framework from wiping out all of the memory on a single box if it does a huge amount of shuffle writing, with the combination of auto-persisting and chained RDD actions which depend on previous RDD computations... You COULD break up the chain by forcing an intermediate step to DISK using (saveAsPickleFile/saveAsTextFile perhaps), and then in the next step reading it back in. At least that would force the previous python worker daemons to be cleaned up before potentially spawning new ones...&lt;/p&gt;

&lt;p&gt;Ideally there should be an environment variable for the max number of python workers allowed to be spawned per executor, because it looks like that doesn&apos;t exist as of yet! &lt;/p&gt;</comment>
                            <comment id="14293843" author="skrasser" created="Tue, 27 Jan 2015 17:31:45 +0000"  >&lt;p&gt;I&apos;ve definitely seen this behavior when adding additional reduceByKey operations that could execute in parallel (around 2 workers per such operation), but in my small test script it&apos;s a single reduceByKey operation followed by a single coalesce statement. I&apos;m running it right now, and it already spiked up to over 30 Python workers per executor, so there must be something else going on on top of this.&lt;/p&gt;</comment>
                            <comment id="14294570" author="skrasser" created="Wed, 28 Jan 2015 01:43:49 +0000"  >&lt;p&gt;Some new findings: I can trigger the problem now just using the &lt;tt&gt;coalesce&lt;/tt&gt; call. My job now looks like this:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;sc.newAPIHadoopFile().map().map().coalesce().count()&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the 64 executor case, this occurs when processing 1TB in 1500 files. If I go down to 2 executors, 200GB in 305 files make the worker count go up to 9 (higher as I add more files). With less data, things appear normal.&lt;/p&gt;

&lt;p&gt;That raises the question about what &lt;tt&gt;coalesce()&lt;/tt&gt; is doing that causes new workers to spawn.&lt;/p&gt;</comment>
                            <comment id="14294820" author="apachespark" created="Wed, 28 Jan 2015 07:26:01 +0000"  >&lt;p&gt;User &apos;davies&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4238&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4238&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14295409" author="skrasser" created="Wed, 28 Jan 2015 17:15:18 +0000"  >&lt;p&gt;Thanks Davies!&lt;/p&gt;</comment>
                            <comment id="14295416" author="mkman84" created="Wed, 28 Jan 2015 17:20:08 +0000"  >&lt;p&gt;Thanks! Hopefully that tackles the same problem I was seeing!&lt;/p&gt;</comment>
                            <comment id="14298040" author="joshrosen" created="Fri, 30 Jan 2015 01:30:22 +0000"  >&lt;p&gt;I&apos;ve committed Davies&apos; patch (&lt;a href=&quot;https://github.com/apache/spark/pull/4238&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4238&lt;/a&gt;) to &lt;tt&gt;master&lt;/tt&gt; for inclusion in Spark 1.3.0 and tagged it for later backport to Spark 1.2.2. (I&apos;ll cherry-pick the commit after we close the 1.2.1 vote).&lt;/p&gt;</comment>
                            <comment id="14323636" author="joshrosen" created="Tue, 17 Feb 2015 04:35:59 +0000"  >&lt;p&gt;I&apos;ve merged this into `branch-1.2` (1.2.2), so I&apos;m marking this as fixed.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 40 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i24r73:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12329321">1.2.2</customfieldvalue>
    <customfieldvalue id="12327642">1.3.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>