<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:28:36 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-37728] reading nested columns with ORC vectorized reader can cause ArrayIndexOutOfBoundsException</title>
                <link>https://issues.apache.org/jira/browse/SPARK-37728</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When spark.sql.orc.enableNestedColumnVectorizedReader is set to true, reading nested columns of ORC files can cause ArrayIndexOutOfBoundsException. Here is a simple reproduction:&lt;/p&gt;

&lt;p&gt;1) create an ORC file which contains records of type Array&amp;lt;Array&amp;lt;String&amp;gt;&amp;gt;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
./bin/spark-shell &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;Item(record: Array[Array[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]])

val data = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Array[Array[Array[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]]](100)
&#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (i &amp;lt;- 0 to 99) {
&#160; &#160; &#160; val temp = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Array[Array[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]](50)
&#160; &#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (j &amp;lt;- 0 to 49) {
&#160; &#160; &#160; &#160; temp(j) = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Array[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;](1000)
&#160; &#160; &#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (k &amp;lt;- 0 to 999) {
&#160; &#160; &#160; &#160; &#160; temp(j)(k) = k.toString
&#160; &#160; &#160; &#160; }
&#160; &#160; &#160; }
&#160; &#160; &#160; data(i) = temp
&#160; &#160; }
val rdd = spark.sparkContext.parallelize(data, 1)
val df = rdd.map(x =&amp;gt; Item(x)).toDF
df.write.orc(&lt;span class=&quot;code-quote&quot;&gt;&quot;file:&lt;span class=&quot;code-comment&quot;&gt;///home/user_name/data&quot;&lt;/span&gt;) &lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;2) read the orc with spark.sql.orc.enableNestedColumnVectorizedReader=true&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
./bin/spark-shell --conf spark.sql.orc.enableVectorizedReader=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; --conf spark.sql.codegen.wholeStage=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; --conf spark.sql.orc.enableNestedColumnVectorizedReader=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt; --conf spark.sql.orc.columnarReaderBatchSize=4096 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
val df = spark.read.orc(&lt;span class=&quot;code-quote&quot;&gt;&quot;file:&lt;span class=&quot;code-comment&quot;&gt;///home/user_name/data&quot;&lt;/span&gt;)
&lt;/span&gt;df.show(100) &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Then Spark threw ArrayIndexOutOfBoundsException:&lt;/p&gt;

&lt;p&gt;Driver stacktrace:&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2455)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2404)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2403)&lt;br/&gt;
&#160; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)&lt;br/&gt;
&#160; at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)&lt;br/&gt;
&#160; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2403)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1162)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1162)&lt;br/&gt;
&#160; at scala.Option.foreach(Option.scala:407)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1162)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2643)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2585)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2574)&lt;br/&gt;
&#160; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:940)&lt;br/&gt;
&#160; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2227)&lt;br/&gt;
&#160; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2248)&lt;br/&gt;
&#160; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:490)&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:443)&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:48)&lt;br/&gt;
&#160; at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3833)&lt;br/&gt;
&#160; at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2832)&lt;br/&gt;
&#160; at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3824)&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)&lt;br/&gt;
&#160; at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)&lt;br/&gt;
&#160; at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3822)&lt;br/&gt;
&#160; at org.apache.spark.sql.Dataset.head(Dataset.scala:2832)&lt;br/&gt;
&#160; at org.apache.spark.sql.Dataset.take(Dataset.scala:3053)&lt;br/&gt;
&#160; at org.apache.spark.sql.Dataset.getRows(Dataset.scala:288)&lt;br/&gt;
&#160; at org.apache.spark.sql.Dataset.showString(Dataset.scala:327)&lt;br/&gt;
&#160; at org.apache.spark.sql.Dataset.show(Dataset.scala:807)&lt;br/&gt;
&#160; at org.apache.spark.sql.Dataset.show(Dataset.scala:766)&lt;br/&gt;
&#160; ... 47 elided&lt;br/&gt;
Caused by: java.lang.ArrayIndexOutOfBoundsException: 4096&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.datasources.orc.OrcArrayColumnVector.getArray(OrcArrayColumnVector.java:53)&lt;br/&gt;
&#160; at org.apache.spark.sql.vectorized.ColumnarArray.getArray(ColumnarArray.java:170)&lt;br/&gt;
&#160; at org.apache.spark.sql.vectorized.ColumnarArray.getArray(ColumnarArray.java:31)&lt;br/&gt;
&#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)&lt;br/&gt;
&#160; at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:363)&lt;br/&gt;
&#160; at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)&lt;br/&gt;
&#160; at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)&lt;br/&gt;
&#160; at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)&lt;br/&gt;
&#160; at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:365)&lt;br/&gt;
&#160; at org.apache.spark.rdd.RDD.iterator(RDD.scala:329)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)&lt;br/&gt;
&#160; at org.apache.spark.scheduler.Task.run(Task.scala:136)&lt;br/&gt;
&#160; at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:507)&lt;br/&gt;
&#160; at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1468)&lt;br/&gt;
&#160; at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:510)&lt;br/&gt;
&#160; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
&#160; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
&#160; at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13419202">SPARK-37728</key>
            <summary>reading nested columns with ORC vectorized reader can cause ArrayIndexOutOfBoundsException</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yangyimin">Yimin Yang</assignee>
                                    <reporter username="yangyimin">Yimin Yang</reporter>
                        <labels>
                    </labels>
                <created>Thu, 23 Dec 2021 11:58:51 +0000</created>
                <updated>Tue, 4 Jan 2022 17:27:09 +0000</updated>
                            <resolved>Tue, 28 Dec 2021 06:11:18 +0000</resolved>
                                    <version>3.2.0</version>
                                    <fixVersion>3.2.1</fixVersion>
                    <fixVersion>3.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="17464553" author="apachespark" created="Thu, 23 Dec 2021 13:09:30 +0000"  >&lt;p&gt;User &apos;yym1995&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/35002&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/35002&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17464554" author="apachespark" created="Thu, 23 Dec 2021 13:10:00 +0000"  >&lt;p&gt;User &apos;yym1995&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/35002&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/35002&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17465953" author="dongjoon" created="Tue, 28 Dec 2021 06:11:35 +0000"  >&lt;p&gt;This is resolved via &lt;a href=&quot;https://github.com/apache/spark/pull/35002&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/35002&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17465992" author="apachespark" created="Tue, 28 Dec 2021 08:05:14 +0000"  >&lt;p&gt;User &apos;yym1995&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/35038&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/35038&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17465994" author="apachespark" created="Tue, 28 Dec 2021 08:06:28 +0000"  >&lt;p&gt;User &apos;yym1995&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/35038&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/35038&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 46 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0xzrc:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>