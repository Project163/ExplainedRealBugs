<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:03:04 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-26293] Cast exception when having python udf in subquery</title>
                <link>https://issues.apache.org/jira/browse/SPARK-26293</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description></description>
                <environment></environment>
        <key id="13202760">SPARK-26293</key>
            <summary>Cast exception when having python udf in subquery</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cloud_fan">Wenchen Fan</assignee>
                                    <reporter username="cloud_fan">Wenchen Fan</reporter>
                        <labels>
                    </labels>
                <created>Thu, 6 Dec 2018 11:18:49 +0000</created>
                <updated>Fri, 20 Mar 2020 03:18:18 +0000</updated>
                            <resolved>Tue, 11 Dec 2018 06:35:55 +0000</resolved>
                                    <version>2.4.0</version>
                                    <fixVersion>2.4.6</fixVersion>
                    <fixVersion>3.0.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16711360" author="apachespark" created="Thu, 6 Dec 2018 12:10:35 +0000"  >&lt;p&gt;User &apos;cloud-fan&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/23248&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23248&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16714936" author="githubbot" created="Mon, 10 Dec 2018 15:35:17 +0000"  >&lt;p&gt;AdolphKK commented on issue #23248: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26293&quot; title=&quot;Cast exception when having python udf in subquery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26293&quot;&gt;&lt;del&gt;SPARK-26293&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Cast exception when having python udf in subquery&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23248#issuecomment-445858370&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23248#issuecomment-445858370&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   looks good for me, +1 :+1: &lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716329" author="githubbot" created="Tue, 11 Dec 2018 06:21:41 +0000"  >&lt;p&gt;asfgit closed pull request #23248: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26293&quot; title=&quot;Cast exception when having python udf in subquery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26293&quot;&gt;&lt;del&gt;SPARK-26293&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Cast exception when having python udf in subquery&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23248&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23248&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/python/pyspark/sql/tests/test_udf.py b/python/pyspark/sql/tests/test_udf.py&lt;br/&gt;
index ed298f724d551..12cf8c7de1dad 100644&lt;br/&gt;
&amp;#8212; a/python/pyspark/sql/tests/test_udf.py&lt;br/&gt;
+++ b/python/pyspark/sql/tests/test_udf.py&lt;br/&gt;
@@ -23,7 +23,7 @@&lt;/p&gt;

&lt;p&gt; from pyspark import SparkContext&lt;br/&gt;
 from pyspark.sql import SparkSession, Column, Row&lt;br/&gt;
-from pyspark.sql.functions import UserDefinedFunction&lt;br/&gt;
+from pyspark.sql.functions import UserDefinedFunction, udf&lt;br/&gt;
 from pyspark.sql.types import *&lt;br/&gt;
 from pyspark.sql.utils import AnalysisException&lt;br/&gt;
 from pyspark.testing.sqlutils import ReusedSQLTestCase, test_compiled, test_not_compiled_message&lt;br/&gt;
@@ -102,7 +102,6 @@ def test_udf_registration_return_type_not_none(self):&lt;/p&gt;

&lt;p&gt;     def test_nondeterministic_udf(self):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Test that nondeterministic UDFs are evaluated only once in chained UDF evaluations&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         import random&lt;br/&gt;
         udf_random_col = udf(lambda: int(100 * random.random()), IntegerType()).asNondeterministic()&lt;br/&gt;
         self.assertEqual(udf_random_col.deterministic, False)&lt;br/&gt;
@@ -113,7 +112,6 @@ def test_nondeterministic_udf(self):&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_nondeterministic_udf2(self):&lt;br/&gt;
         import random&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         random_udf = udf(lambda: random.randint(6, 6), IntegerType()).asNondeterministic()&lt;br/&gt;
         self.assertEqual(random_udf.deterministic, False)&lt;br/&gt;
         random_udf1 = self.spark.catalog.registerFunction(&quot;randInt&quot;, random_udf)&lt;br/&gt;
@@ -132,7 +130,6 @@ def test_nondeterministic_udf2(self):&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_nondeterministic_udf3(self):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;regression test for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-23233&quot; title=&quot;asNondeterministic in Python UDF not being set when the UDF is called at least once&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-23233&quot;&gt;&lt;del&gt;SPARK-23233&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         f = udf(lambda x: x)&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;Here we cache the JVM UDF instance.&lt;br/&gt;
         self.spark.range(1).select(f(&quot;id&quot;))&lt;br/&gt;
@@ -144,7 +141,7 @@ def test_nondeterministic_udf3(self):&lt;br/&gt;
         self.assertFalse(deterministic)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;     def test_nondeterministic_udf_in_aggregate(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf, sum&lt;br/&gt;
+        from pyspark.sql.functions import sum&lt;br/&gt;
         import random&lt;br/&gt;
         udf_random_col = udf(lambda: int(100 * random.random()), &apos;int&apos;).asNondeterministic()&lt;br/&gt;
         df = self.spark.range(10)&lt;br/&gt;
@@ -181,7 +178,6 @@ def test_multiple_udfs(self):&lt;br/&gt;
         self.assertEqual(tuple(row), (6, 5))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_in_filter_on_top_of_outer_join(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         left = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(a=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         right = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(a=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         df = left.join(right, on=&apos;a&apos;, how=&apos;left_outer&apos;)&lt;br/&gt;
@@ -190,7 +186,6 @@ def test_udf_in_filter_on_top_of_outer_join(self):&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_in_filter_on_top_of_join(self):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;regression test for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-18589&quot; title=&quot;persist() resolves &amp;quot;java.lang.RuntimeException: Invalid PythonUDF &amp;lt;lambda&amp;gt;(...), requires attributes from more than one child&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-18589&quot;&gt;&lt;del&gt;SPARK-18589&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         left = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(a=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         right = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(b=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         f = udf(lambda a, b: a == b, BooleanType())&lt;br/&gt;
@@ -199,7 +194,6 @@ def test_udf_in_filter_on_top_of_join(self):&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_in_join_condition(self):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;regression test for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25314&quot; title=&quot;Invalid PythonUDF - requires attributes from more than one child - in &amp;quot;on&amp;quot; join condition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25314&quot;&gt;&lt;del&gt;SPARK-25314&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         left = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(a=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         right = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(b=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         f = udf(lambda a, b: a == b, BooleanType())&lt;br/&gt;
@@ -211,7 +205,7 @@ def test_udf_in_join_condition(self):&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_in_left_outer_join_condition(self):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;regression test for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26147&quot; title=&quot;Python UDFs in join condition fail even when using columns from only one side of join&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26147&quot;&gt;&lt;del&gt;SPARK-26147&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf, col&lt;br/&gt;
+        from pyspark.sql.functions import col&lt;br/&gt;
         left = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(a=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         right = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(b=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         f = udf(lambda a: str(a), StringType())&lt;br/&gt;
@@ -223,7 +217,6 @@ def test_udf_in_left_outer_join_condition(self):&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_in_left_semi_join_condition(self):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;regression test for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25314&quot; title=&quot;Invalid PythonUDF - requires attributes from more than one child - in &amp;quot;on&amp;quot; join condition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25314&quot;&gt;&lt;del&gt;SPARK-25314&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         left = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(a=1, a1=1, a2=1), Row(a=2, a1=2, a2=2)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         right = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(b=1, b1=1, b2=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         f = udf(lambda a, b: a == b, BooleanType())&lt;br/&gt;
@@ -236,7 +229,6 @@ def test_udf_in_left_semi_join_condition(self):&lt;br/&gt;
     def test_udf_and_common_filter_in_join_condition(self):&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;regression test for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25314&quot; title=&quot;Invalid PythonUDF - requires attributes from more than one child - in &amp;quot;on&amp;quot; join condition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25314&quot;&gt;&lt;del&gt;SPARK-25314&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;test the complex scenario with both udf and common filter&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         left = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(a=1, a1=1, a2=1), Row(a=2, a1=2, a2=2)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         right = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(b=1, b1=1, b2=1), Row(b=1, b1=3, b2=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         f = udf(lambda a, b: a == b, BooleanType())&lt;br/&gt;
@@ -247,7 +239,6 @@ def test_udf_and_common_filter_in_join_condition(self):&lt;br/&gt;
     def test_udf_and_common_filter_in_left_semi_join_condition(self):&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;regression test for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25314&quot; title=&quot;Invalid PythonUDF - requires attributes from more than one child - in &amp;quot;on&amp;quot; join condition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25314&quot;&gt;&lt;del&gt;SPARK-25314&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;test the complex scenario with both udf and common filter&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         left = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(a=1, a1=1, a2=1), Row(a=2, a1=2, a2=2)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         right = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(b=1, b1=1, b2=1), Row(b=1, b1=3, b2=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         f = udf(lambda a, b: a == b, BooleanType())&lt;br/&gt;
@@ -258,7 +249,6 @@ def test_udf_and_common_filter_in_left_semi_join_condition(self):&lt;br/&gt;
     def test_udf_not_supported_in_join_condition(self):&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;regression test for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25314&quot; title=&quot;Invalid PythonUDF - requires attributes from more than one child - in &amp;quot;on&amp;quot; join condition&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25314&quot;&gt;&lt;del&gt;SPARK-25314&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;test python udf is not supported in join type besides left_semi and inner join.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         left = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(a=1, a1=1, a2=1), Row(a=2, a1=2, a2=2)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         right = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;Row(b=1, b1=1, b2=1), Row(b=1, b1=3, b2=1)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
         f = udf(lambda a, b: a == b, BooleanType())&lt;br/&gt;
@@ -301,7 +291,7 @@ def test_broadcast_in_udf(self):&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_with_filter_function(self):&lt;br/&gt;
         df = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;(1, &amp;quot;1&amp;quot;), (2, &amp;quot;2&amp;quot;), (1, &amp;quot;2&amp;quot;), (1, &amp;quot;2&amp;quot;)&amp;#93;&lt;/span&gt;, &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;key&amp;quot;, &amp;quot;value&amp;quot;&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf, col&lt;br/&gt;
+        from pyspark.sql.functions import col&lt;br/&gt;
         from pyspark.sql.types import BooleanType&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         my_filter = udf(lambda a: a &amp;lt; 2, BooleanType())&lt;br/&gt;
@@ -310,7 +300,7 @@ def test_udf_with_filter_function(self):&lt;/p&gt;

&lt;p&gt;     def test_udf_with_aggregate_function(self):&lt;br/&gt;
         df = self.spark.createDataFrame(&lt;span class=&quot;error&quot;&gt;&amp;#91;(1, &amp;quot;1&amp;quot;), (2, &amp;quot;2&amp;quot;), (1, &amp;quot;2&amp;quot;), (1, &amp;quot;2&amp;quot;)&amp;#93;&lt;/span&gt;, &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;key&amp;quot;, &amp;quot;value&amp;quot;&amp;#93;&lt;/span&gt;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf, col, sum&lt;br/&gt;
+        from pyspark.sql.functions import col, sum&lt;br/&gt;
         from pyspark.sql.types import BooleanType&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         my_filter = udf(lambda a: a == 1, BooleanType())&lt;br/&gt;
@@ -326,7 +316,7 @@ def test_udf_with_aggregate_function(self):&lt;br/&gt;
         self.assertEqual(sel.collect(), &lt;span class=&quot;error&quot;&gt;&amp;#91;Row(t=4), Row(t=3)&amp;#93;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;     def test_udf_in_generate(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf, explode&lt;br/&gt;
+        from pyspark.sql.functions import explode&lt;br/&gt;
         df = self.spark.range(5)&lt;br/&gt;
         f = udf(lambda x: list(range&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;), ArrayType(LongType()))&lt;br/&gt;
         row = df.select(explode(f(*df))).groupBy().sum().first()&lt;br/&gt;
@@ -353,7 +343,6 @@ def test_udf_in_generate(self):&lt;br/&gt;
         self.assertEqual(res&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, 1)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_with_order_by_and_limit(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         my_copy = udf(lambda x: x, IntegerType())&lt;br/&gt;
         df = self.spark.range(10).orderBy(&quot;id&quot;)&lt;br/&gt;
         res = df.select(df.id, my_copy(df.id).alias(&quot;copy&quot;)).limit(1)&lt;br/&gt;
@@ -394,14 +383,14 @@ def test_non_existed_udaf(self):&lt;br/&gt;
                                 lambda: spark.udf.registerJavaUDAF(&quot;udaf1&quot;, &quot;non_existed_udaf&quot;))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_with_input_file_name(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf, input_file_name&lt;br/&gt;
+        from pyspark.sql.functions import input_file_name&lt;br/&gt;
         sourceFile = udf(lambda path: path, StringType())&lt;br/&gt;
         filePath = &quot;python/test_support/sql/people1.json&quot;&lt;br/&gt;
         row = self.spark.read.json(filePath).select(sourceFile(input_file_name())).first()&lt;br/&gt;
         self.assertTrue(row&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;.find(&quot;people1.json&quot;) != -1)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_with_input_file_name_for_hadooprdd(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf, input_file_name&lt;br/&gt;
+        from pyspark.sql.functions import input_file_name&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         def filename(path):&lt;br/&gt;
             return path&lt;br/&gt;
@@ -427,9 +416,6 @@ def test_udf_defers_judf_initialization(self):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;This is separate of  UDFInitializationTests&lt;/li&gt;
	&lt;li&gt;to avoid context initialization&lt;/li&gt;
	&lt;li&gt;when udf is called&lt;br/&gt;
-&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import UserDefinedFunction&lt;br/&gt;
-&lt;br/&gt;
         f = UserDefinedFunction(lambda x: x, StringType())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         self.assertIsNone(&lt;br/&gt;
@@ -445,8 +431,6 @@ def test_udf_defers_judf_initialization(self):&lt;br/&gt;
         )&lt;/p&gt;

&lt;p&gt;     def test_udf_with_string_return_type(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import UserDefinedFunction&lt;br/&gt;
-&lt;br/&gt;
         add_one = UserDefinedFunction(lambda x: x + 1, &quot;integer&quot;)&lt;br/&gt;
         make_pair = UserDefinedFunction(lambda x: (-x, x), &quot;struct&amp;lt;x:integer,y:integer&amp;gt;&quot;)&lt;br/&gt;
         make_array = UserDefinedFunction(&lt;br/&gt;
@@ -460,13 +444,11 @@ def test_udf_with_string_return_type(self):&lt;br/&gt;
         self.assertTupleEqual(expected, actual)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_shouldnt_accept_noncallable_object(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import UserDefinedFunction&lt;br/&gt;
-&lt;br/&gt;
         non_callable = None&lt;br/&gt;
         self.assertRaises(TypeError, UserDefinedFunction, non_callable, StringType())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     def test_udf_with_decorator(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import lit, udf&lt;br/&gt;
+        from pyspark.sql.functions import lit&lt;br/&gt;
         from pyspark.sql.types import IntegerType, DoubleType&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         @udf(IntegerType())&lt;br/&gt;
@@ -523,7 +505,6 @@ def as_double&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;:&lt;br/&gt;
         )&lt;/p&gt;

&lt;p&gt;     def test_udf_wrapper(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
         from pyspark.sql.types import IntegerType&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         def f&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;:&lt;br/&gt;
@@ -569,7 +550,7 @@ def test_nonparam_udf_with_aggregate(self):&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-24721&quot; title=&quot;Failed to use PythonUDF with literal inputs in filter with data sources&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-24721&quot;&gt;&lt;del&gt;SPARK-24721&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
     @unittest.skipIf(not test_compiled, test_not_compiled_message)&lt;br/&gt;
     def test_datasource_with_udf(self):&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf, lit, col&lt;br/&gt;
+        from pyspark.sql.functions import lit, col&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         path = tempfile.mkdtemp()&lt;br/&gt;
         shutil.rmtree(path)&lt;br/&gt;
@@ -609,8 +590,6 @@ def test_datasource_with_udf(self):&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25591&quot; title=&quot;PySpark Accumulators with multiple PythonUDFs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25591&quot;&gt;&lt;del&gt;SPARK-25591&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
     def test_same_accumulator_in_udfs(self):&lt;/li&gt;
&lt;/ol&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import udf&lt;br/&gt;
-&lt;br/&gt;
         data_schema = StructType([StructField(&quot;a&quot;, IntegerType(), True),&lt;br/&gt;
                                   StructField(&quot;b&quot;, IntegerType(), True)])&lt;br/&gt;
         data = self.spark.createDataFrame([&lt;span class=&quot;error&quot;&gt;&amp;#91;1, 2&amp;#93;&lt;/span&gt;], schema=data_schema)&lt;br/&gt;
@@ -632,6 +611,15 @@ def second_udf&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;:&lt;br/&gt;
         data.collect()&lt;br/&gt;
         self.assertEqual(test_accum.value, 101)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+    # &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26293&quot; title=&quot;Cast exception when having python udf in subquery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26293&quot;&gt;&lt;del&gt;SPARK-26293&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
+    def test_udf_in_subquery(self):&lt;br/&gt;
+        f = udf(lambda x: x, &quot;long&quot;)&lt;br/&gt;
+        with self.tempView(&quot;v&quot;):&lt;br/&gt;
+            self.spark.range(1).filter(f(&quot;id&quot;) &amp;gt;= 0).createTempView(&quot;v&quot;)&lt;br/&gt;
+            sql = self.spark.sql&lt;br/&gt;
+            result = sql(&quot;select i from values(0L) as data&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; where i in (select id from v)&quot;)&lt;br/&gt;
+            self.assertEqual(result.collect(), &lt;span class=&quot;error&quot;&gt;&amp;#91;Row(i=0)&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+&lt;/p&gt;

&lt;p&gt; class UDFInitializationTests(unittest.TestCase):&lt;br/&gt;
     def tearDown(self):&lt;br/&gt;
@@ -642,8 +630,6 @@ def tearDown(self):&lt;br/&gt;
             SparkContext._active_spark_context.stop()&lt;/p&gt;

&lt;p&gt;     def test_udf_init_shouldnt_initialize_context(self):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;from pyspark.sql.functions import UserDefinedFunction&lt;br/&gt;
-&lt;br/&gt;
         UserDefinedFunction(lambda x: x, StringType())&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;         self.assertIsNone(&lt;br/&gt;
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala&lt;br/&gt;
index 2b87796dc6833..a5203daea9cd0 100644&lt;br/&gt;
&amp;#8212; a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala&lt;br/&gt;
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ArrowEvalPythonExec.scala&lt;br/&gt;
@@ -60,8 +60,12 @@ private class BatchIterator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;(iter: Iterator&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;, batchSize: Int)&lt;br/&gt;
 /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A logical plan that evaluates a [&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonUDF&amp;#93;&lt;/span&gt;].&lt;br/&gt;
  */&lt;br/&gt;
-case class ArrowEvalPython(udfs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonUDF&amp;#93;&lt;/span&gt;, output: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Attribute&amp;#93;&lt;/span&gt;, child: LogicalPlan)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;extends UnaryNode&lt;br/&gt;
+case class ArrowEvalPython(&lt;br/&gt;
+    udfs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonUDF&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    output: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Attribute&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    child: LogicalPlan) extends UnaryNode 
{
+  override def producedAttributes: AttributeSet = AttributeSet(output.drop(child.output.length))
+}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A physical plan that evaluates a [&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonUDF&amp;#93;&lt;/span&gt;].&lt;br/&gt;
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala&lt;br/&gt;
index b08b7e60e130b..d3736d24e5019 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala&lt;br/&gt;
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/BatchEvalPythonExec.scala&lt;br/&gt;
@@ -32,8 +32,12 @@ import org.apache.spark.sql.types.
{StructField, StructType}
&lt;p&gt; /**&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;A logical plan that evaluates a [&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonUDF&amp;#93;&lt;/span&gt;]&lt;br/&gt;
  */&lt;br/&gt;
-case class BatchEvalPython(udfs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonUDF&amp;#93;&lt;/span&gt;, output: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Attribute&amp;#93;&lt;/span&gt;, child: LogicalPlan)&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;extends UnaryNode&lt;br/&gt;
+case class BatchEvalPython(&lt;br/&gt;
+    udfs: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonUDF&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    output: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Attribute&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    child: LogicalPlan) extends UnaryNode 
{
+  override def producedAttributes: AttributeSet = AttributeSet(output.drop(child.output.length))
+}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A physical plan that evaluates a [&lt;span class=&quot;error&quot;&gt;&amp;#91;PythonUDF&amp;#93;&lt;/span&gt;]&lt;br/&gt;
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala&lt;br/&gt;
index 90b5325919e96..380c31baa6213 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala&lt;br/&gt;
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/python/ExtractPythonUDFs.scala&lt;br/&gt;
@@ -24,7 +24,7 @@ import org.apache.spark.api.python.PythonEvalType&lt;br/&gt;
 import org.apache.spark.sql.AnalysisException&lt;br/&gt;
 import org.apache.spark.sql.catalyst.expressions._&lt;br/&gt;
 import org.apache.spark.sql.catalyst.expressions.aggregate.AggregateExpression&lt;br/&gt;
-import org.apache.spark.sql.catalyst.plans.logical.
{Aggregate, Filter, LogicalPlan, Project}
&lt;p&gt;+import org.apache.spark.sql.catalyst.plans.logical._&lt;br/&gt;
 import org.apache.spark.sql.catalyst.rules.Rule&lt;/p&gt;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;@@ -131,8 +131,20 @@ object ExtractPythonUDFs extends Rule&lt;span class=&quot;error&quot;&gt;&amp;#91;LogicalPlan&amp;#93;&lt;/span&gt; with PredicateHelper &lt;/p&gt;
{
     expressions.flatMap(collectEvaluableUDFs)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;def apply(plan: LogicalPlan): LogicalPlan = plan transformUp {&lt;/li&gt;
	&lt;li&gt;case plan: LogicalPlan =&amp;gt; extract(plan)&lt;br/&gt;
+  def apply(plan: LogicalPlan): LogicalPlan = plan match 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {+    // SPARK-26293}&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716332" author="githubbot" created="Tue, 11 Dec 2018 06:22:37 +0000"  >&lt;p&gt;cloud-fan commented on issue #23248: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26293&quot; title=&quot;Cast exception when having python udf in subquery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26293&quot;&gt;&lt;del&gt;SPARK-26293&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Cast exception when having python udf in subquery&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23248#issuecomment-446086659&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23248#issuecomment-446086659&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   thanks, merging to master/2.4!&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716341" author="cloud_fan" created="Tue, 11 Dec 2018 06:35:55 +0000"  >&lt;p&gt;Issue resolved by pull request 23248&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/23248&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23248&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16716548" author="githubbot" created="Tue, 11 Dec 2018 08:32:04 +0000"  >&lt;p&gt;HyukjinKwon commented on issue #23248: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26293&quot; title=&quot;Cast exception when having python udf in subquery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26293&quot;&gt;&lt;del&gt;SPARK-26293&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Cast exception when having python udf in subquery&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23248#issuecomment-446115111&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23248#issuecomment-446115111&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   BTW, @cloud-fan, I think it&apos;s going to be a considerable conflict against branch-2.4 ... If the conflict is considerable, might better to open a PR.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716564" author="githubbot" created="Tue, 11 Dec 2018 08:36:34 +0000"  >&lt;p&gt;cloud-fan commented on issue #23248: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26293&quot; title=&quot;Cast exception when having python udf in subquery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26293&quot;&gt;&lt;del&gt;SPARK-26293&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Cast exception when having python udf in subquery&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23248#issuecomment-446116345&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23248#issuecomment-446116345&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   @HyukjinKwon the conflict is only the test. I just moved the test (without those cleanups) to the giant `tests.py` in 2.4. &lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716569" author="githubbot" created="Tue, 11 Dec 2018 08:40:42 +0000"  >&lt;p&gt;HyukjinKwon commented on issue #23248: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26293&quot; title=&quot;Cast exception when having python udf in subquery&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26293&quot;&gt;&lt;del&gt;SPARK-26293&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Cast exception when having python udf in subquery&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23248#issuecomment-446117533&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23248#issuecomment-446117533&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Ah, sounds good!&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 49 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|s018go:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>