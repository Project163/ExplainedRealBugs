<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:41:36 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-13289] Word2Vec generate infinite distances when numIterations&gt;5</title>
                <link>https://issues.apache.org/jira/browse/SPARK-13289</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I recently ran some word2vec experiments on a cluster with 50 executors on some large text dataset but find out that when number of iterations is larger than 5 the distance between words will be all infinite. My code looks like this:&lt;/p&gt;

&lt;p&gt;val text = sc.textFile(&quot;/project/NLP/1_biliion_words/train&quot;).map(_.split(&quot; &quot;).toSeq)&lt;br/&gt;
import org.apache.spark.mllib.feature.&lt;/p&gt;
{Word2Vec, Word2VecModel}
&lt;p&gt;val word2vec = new Word2Vec().setMinCount(25).setVectorSize(96).setNumPartitions(99).setNumIterations(10).setWindowSize(5)&lt;br/&gt;
val model = word2vec.fit(text)&lt;br/&gt;
val synonyms = model.findSynonyms(&quot;who&quot;, 40)&lt;br/&gt;
for((synonym, cosineSimilarity) &amp;lt;- synonyms) {&lt;br/&gt;
  println(s&quot;$synonym $cosineSimilarity&quot;)&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;The results are: &lt;br/&gt;
to Infinity&lt;br/&gt;
and Infinity&lt;br/&gt;
that Infinity&lt;br/&gt;
with Infinity&lt;br/&gt;
said Infinity&lt;br/&gt;
it Infinity&lt;br/&gt;
by Infinity&lt;br/&gt;
be Infinity&lt;br/&gt;
have Infinity&lt;br/&gt;
he Infinity&lt;br/&gt;
has Infinity&lt;br/&gt;
his Infinity&lt;br/&gt;
an Infinity&lt;br/&gt;
) Infinity&lt;br/&gt;
not Infinity&lt;br/&gt;
who Infinity&lt;br/&gt;
I Infinity&lt;br/&gt;
had Infinity&lt;br/&gt;
their Infinity&lt;br/&gt;
were Infinity&lt;br/&gt;
they Infinity&lt;br/&gt;
but Infinity&lt;br/&gt;
been Infinity&lt;/p&gt;

&lt;p&gt;I tried many different datasets and different words for finding synonyms.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Linux, Scala&lt;/p&gt;</environment>
        <key id="12938623">SPARK-13289</key>
            <summary>Word2Vec generate infinite distances when numIterations&gt;5</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="flysjy">Junyang Shen</assignee>
                                    <reporter username="daiqi5477">Qi Dai</reporter>
                        <labels>
                            <label>features</label>
                    </labels>
                <created>Thu, 11 Feb 2016 21:11:11 +0000</created>
                <updated>Sun, 1 May 2016 15:35:15 +0000</updated>
                            <resolved>Sat, 30 Apr 2016 09:16:38 +0000</resolved>
                                    <version>1.6.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>MLlib</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="15144383" author="srowen" created="Fri, 12 Feb 2016 10:33:41 +0000"  >&lt;p&gt;THere isn&apos;t enough info to reproduce anything here. We don&apos;t have your data. You&apos;d have to supply a much more specific lead on what the issue is.&lt;/p&gt;</comment>
                            <comment id="15144685" author="daiqi5477" created="Fri, 12 Feb 2016 15:16:19 +0000"  >&lt;p&gt;I&apos;m using the &quot;One Billion Words Language Modeling&quot; dataset available at: &lt;a href=&quot;http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.statmt.org/lm-benchmark/1-billion-word-language-modeling-benchmark-r13output.tar.gz&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;As for the code, it&apos;s very simple and you can run it in spark-shell:&lt;br/&gt;
val text = sc.textFile(&quot;/project/NLP/1_biliion_words/train&quot;).map(_.split(&quot; &quot;).toSeq)&lt;br/&gt;
import org.apache.spark.mllib.feature.&lt;/p&gt;
{Word2Vec, Word2VecModel}
&lt;p&gt;val word2vec = new Word2Vec().setMinCount(25).setVectorSize(96).setNumPartitions(99).setNumIterations(10).setWindowSize(5)&lt;br/&gt;
val model = word2vec.fit(text)&lt;br/&gt;
val synonyms = model.findSynonyms(&quot;who&quot;, 40)&lt;br/&gt;
for((synonym, cosineSimilarity) &amp;lt;- synonyms)&lt;/p&gt;
{ println(s&quot;$synonym $cosineSimilarity&quot;) }

&lt;p&gt;For environment, I believe you can reproduce it with any cluster that can run spark 1.6 (I didn&apos;t try local mode). &lt;/p&gt;

&lt;p&gt;Please note, the problem happens with multiple iterations. When you are trying to reproduce it, please set iterations&amp;gt;5, e.g. 10 &lt;/p&gt;</comment>
                            <comment id="15148872" author="daiqi5477" created="Tue, 16 Feb 2016 16:34:13 +0000"  >&lt;p&gt;Hi Sean, Are you able to reproduce the issue? Do you need any other details? (Maybe the reply I made few days ago didn&apos;t send to you. If so, could you take a look at the details I provided below?) I tried some other parameters. It looks like it&apos;s more likely to fail with larger dataset, more partitions and more iterations. &lt;/p&gt;</comment>
                            <comment id="15148876" author="srowen" created="Tue, 16 Feb 2016 16:41:16 +0000"  >&lt;p&gt;I haven&apos;t run it. It&apos;d be great if you can run with this and propose a fix?&lt;/p&gt;</comment>
                            <comment id="15148920" author="daiqi5477" created="Tue, 16 Feb 2016 17:09:22 +0000"  >&lt;p&gt;I&apos;m not familiar with the algorithm and implementation. Maybe we need to wait for some other people in the community who involved in the implementation to take a look at the issue.&lt;/p&gt;</comment>
                            <comment id="15148923" author="srowen" created="Tue, 16 Feb 2016 17:11:44 +0000"  >&lt;p&gt;That&apos;s not usually how it works; I wouldn&apos;t expect others to debug for you. Just step through the code?&lt;/p&gt;</comment>
                            <comment id="15148937" author="daiqi5477" created="Tue, 16 Feb 2016 17:18:00 +0000"  >&lt;p&gt;Yes, just download the data from the url. Unzip it and change the path the the training data folder. Then, step through the rest parts and the issue should be able to reproduced. (probably it&apos;s better to run on a cluster because the dataset is big) &lt;/p&gt;</comment>
                            <comment id="15156785" author="mlnick" created="Mon, 22 Feb 2016 11:16:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=daiqi5477&quot; class=&quot;user-hover&quot; rel=&quot;daiqi5477&quot;&gt;daiqi5477&lt;/a&gt; could you try your experiments again against the latest master, and see if you run into the same issue?&lt;/p&gt;</comment>
                            <comment id="15157401" author="daiqi5477" created="Mon, 22 Feb 2016 18:10:31 +0000"  >&lt;p&gt;Do you have a runable distribution of the latest master somewhere? I tried to build it but didn&apos;t get through. It failed at building spark-catalyst_2.11 with the following error: Failed to execute goal org.apache.maven.plugins:maven-shade-plugin:2.4.3:shade (default) on project spark-catalyst_2.11: Error creating shaded jar: Method code too large! -&amp;gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Help 1&amp;#93;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;I was building with &quot;build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean package&quot;&lt;/p&gt;

&lt;p&gt;I tried &quot;export MAVEN_OPTS=&quot;-Xmx4g -XX:MaxPermSize=4g -XX:ReservedCodeCacheSize=2g&quot;&quot; and &quot;export MAVEN_OPTS=&quot;-Xmx2g -XX:MaxPermSize=512M -XX:ReservedCodeCacheSize=512m&quot;&quot; &lt;/p&gt;

&lt;p&gt;I&apos;m using java 1.7.80 on Ubuntu 14.04.3&lt;/p&gt;</comment>
                            <comment id="15158167" author="wziyong" created="Tue, 23 Feb 2016 02:23:25 +0000"  >&lt;p&gt;I pull the latest code,and I also met the problem like yours.Did you solve it?Thanks!&lt;/p&gt;</comment>
                            <comment id="15158213" author="rspitzer" created="Tue, 23 Feb 2016 03:18:57 +0000"  >&lt;p&gt;same&lt;/p&gt;</comment>
                            <comment id="15158355" author="vectorijk" created="Tue, 23 Feb 2016 06:10:10 +0000"  >&lt;p&gt;same. Spark-13431 mentioned this.&lt;/p&gt;</comment>
                            <comment id="15158446" author="mlnick" created="Tue, 23 Feb 2016 07:18:22 +0000"  >&lt;p&gt;Yes the master build is currently failing as detailed in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13431&quot; title=&quot;Maven build fails due to: Method code too large! in Catalyst&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-13431&quot;&gt;&lt;del&gt;SPARK-13431&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15168619" author="mlnick" created="Fri, 26 Feb 2016 07:54:02 +0000"  >&lt;p&gt;Master branch should be building now. Can you try again?&lt;/p&gt;</comment>
                            <comment id="15176296" author="daiqi5477" created="Wed, 2 Mar 2016 19:27:07 +0000"  >&lt;p&gt;I tried &quot;build/mvn -Pyarn -Phadoop-2.6 -Dhadoop.version=2.6.0 -Phive -Phive-thriftserver -DskipTests clean package&quot;. I looks successful but I can&apos;t run it in yarn-client mode. Then I turned to &quot;./make-distribution.sh --name spark210 --tgz -Psparkr -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn&quot; but it can&apos;t go through. I also tried the compiled one at: &lt;a href=&quot;http://people.apache.org/~pwendell/spark-nightly/spark-master-bin/latest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://people.apache.org/~pwendell/spark-nightly/spark-master-bin/latest/&lt;/a&gt; and it also can&apos;t run with yarn-client mode. It showed some error related with yarn:&lt;/p&gt;

&lt;p&gt;16/03/02 14:22:52 ERROR SparkContext: Error initializing SparkContext.&lt;br/&gt;
java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.history.YarnHistoryService&lt;br/&gt;
        at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)&lt;br/&gt;
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&lt;br/&gt;
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&lt;br/&gt;
        at java.lang.Class.forName0(Native Method)&lt;br/&gt;
        at java.lang.Class.forName(Class.java:348)&lt;br/&gt;
        at org.apache.spark.util.Utils$.classForName(Utils.scala:174)&lt;br/&gt;
        at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5$$anonfun$apply$4.apply(SchedulerExtensionService.scala:111)&lt;br/&gt;
        at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5$$anonfun$apply$4.apply(SchedulerExtensionService.scala:110)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)&lt;br/&gt;
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)&lt;br/&gt;
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&lt;br/&gt;
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)&lt;br/&gt;
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)&lt;br/&gt;
        at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)&lt;br/&gt;
        at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:110)&lt;br/&gt;
        at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:108)&lt;br/&gt;
        at scala.Option.map(Option.scala:146)&lt;br/&gt;
        at org.apache.spark.scheduler.cluster.SchedulerExtensionServices.start(SchedulerExtensionService.scala:108)&lt;br/&gt;
        at org.apache.spark.scheduler.cluster.YarnSchedulerBackend.start(YarnSchedulerBackend.scala:80)&lt;br/&gt;
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:61)&lt;br/&gt;
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:143)&lt;br/&gt;
        at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:501)&lt;br/&gt;
        at org.apache.spark.repl.Main$.createSparkContext(Main.scala:98)&lt;br/&gt;
        at $line3.$read$$iw$$iw.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:12)&lt;br/&gt;
        at $line3.$read$$iw.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:22)&lt;br/&gt;
        at $line3.$read.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:24)&lt;br/&gt;
        at $line3.$read$.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:28)&lt;br/&gt;
        at $line3.$read$.&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)&lt;br/&gt;
        at $line3.$eval$.$print$lzycompute(&amp;lt;console&amp;gt;:7)&lt;br/&gt;
        at $line3.$eval$.$print(&amp;lt;console&amp;gt;:6)&lt;br/&gt;
        at $line3.$eval.$print(&amp;lt;console&amp;gt;)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:497)&lt;br/&gt;
        at scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:784)&lt;br/&gt;
        at scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1039)&lt;br/&gt;
        at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:636)&lt;br/&gt;
        at scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:635)&lt;br/&gt;
        at scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)&lt;br/&gt;
        at scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)&lt;br/&gt;
        at scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:635)&lt;br/&gt;
        at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:567)&lt;br/&gt;
        at scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:563)&lt;br/&gt;
        at scala.tools.nsc.interpreter.ILoop.reallyInterpret$1(ILoop.scala:802)&lt;br/&gt;
        at scala.tools.nsc.interpreter.ILoop.interpretStartingWith(ILoop.scala:836)&lt;br/&gt;
        at scala.tools.nsc.interpreter.ILoop.command(ILoop.scala:694)&lt;br/&gt;
        at scala.tools.nsc.interpreter.ILoop.processLine(ILoop.scala:404)&lt;br/&gt;
        at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply$mcZ$sp(SparkILoop.scala:39)&lt;br/&gt;
        at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:38)&lt;br/&gt;
        at org.apache.spark.repl.SparkILoop$$anonfun$initializeSpark$1.apply(SparkILoop.scala:38)&lt;br/&gt;
        at scala.tools.nsc.interpreter.IMain.beQuietDuring(IMain.scala:213)&lt;br/&gt;
        at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:38)&lt;br/&gt;
        at org.apache.spark.repl.SparkILoop.loadFiles(SparkILoop.scala:95)&lt;br/&gt;
        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply$mcZ$sp(ILoop.scala:922)&lt;br/&gt;
        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:911)&lt;br/&gt;
        at scala.tools.nsc.interpreter.ILoop$$anonfun$process$1.apply(ILoop.scala:911)&lt;br/&gt;
        at scala.reflect.internal.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:97)&lt;br/&gt;
        at scala.tools.nsc.interpreter.ILoop.process(ILoop.scala:911)&lt;br/&gt;
        at org.apache.spark.repl.Main$.doMain(Main.scala:64)&lt;br/&gt;
        at org.apache.spark.repl.Main$.main(Main.scala:47)&lt;br/&gt;
        at org.apache.spark.repl.Main.main(Main.scala)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
        at java.lang.reflect.Method.invoke(Method.java:497)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:734)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)&lt;br/&gt;
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;br/&gt;
16/03/02 14:22:52 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!&lt;br/&gt;
16/03/02 14:22:52 WARN MetricsSystem: Stopping a MetricsSystem that is not running&lt;br/&gt;
java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.history.YarnHistoryService&lt;br/&gt;
  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)&lt;br/&gt;
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&lt;br/&gt;
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&lt;br/&gt;
  at java.lang.Class.forName0(Native Method)&lt;br/&gt;
  at java.lang.Class.forName(Class.java:348)&lt;br/&gt;
  at org.apache.spark.util.Utils$.classForName(Utils.scala:174)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5$$anonfun$apply$4.apply(SchedulerExtensionService.scala:111)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5$$anonfun$apply$4.apply(SchedulerExtensionService.scala:110)&lt;br/&gt;
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)&lt;br/&gt;
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)&lt;br/&gt;
  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&lt;br/&gt;
  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)&lt;br/&gt;
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)&lt;br/&gt;
  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:110)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:108)&lt;br/&gt;
  at scala.Option.map(Option.scala:146)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices.start(SchedulerExtensionService.scala:108)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.YarnSchedulerBackend.start(YarnSchedulerBackend.scala:80)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:61)&lt;br/&gt;
  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:143)&lt;br/&gt;
  at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:501)&lt;br/&gt;
  at org.apache.spark.repl.Main$.createSparkContext(Main.scala:98)&lt;br/&gt;
  ... 48 elided&lt;br/&gt;
java.lang.NullPointerException&lt;br/&gt;
  at org.apache.spark.sql.SQLContext$.createListenerAndUI(SQLContext.scala:1033)&lt;br/&gt;
  at org.apache.spark.sql.hive.HiveContext.&amp;lt;init&amp;gt;(HiveContext.scala:88)&lt;br/&gt;
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&lt;br/&gt;
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
  at java.lang.reflect.Constructor.newInstance(Constructor.java:422)&lt;br/&gt;
  at org.apache.spark.repl.Main$.createSQLContext(Main.scala:108)&lt;br/&gt;
  ... 48 elided&lt;br/&gt;
&amp;lt;console&amp;gt;:13: error: not found: value sqlContext&lt;br/&gt;
       import sqlContext.implicits._&lt;br/&gt;
              ^&lt;br/&gt;
&amp;lt;console&amp;gt;:13: error: not found: value sqlContext&lt;br/&gt;
       import sqlContext.sql&lt;br/&gt;
              ^&lt;br/&gt;
Probably it&apos;s better to wait for the newer version released. &lt;/p&gt;</comment>
                            <comment id="15200958" author="apachespark" created="Fri, 18 Mar 2016 03:51:03 +0000"  >&lt;p&gt;User &apos;flyjy&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/11812&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/11812&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15200962" author="flysjy" created="Fri, 18 Mar 2016 03:58:59 +0000"  >&lt;p&gt;This PR gives the distance values between 0 and 1.&lt;/p&gt;

&lt;p&gt;scala&amp;gt; model.findSynonyms(&quot;who&quot;, 10)&lt;br/&gt;
res0: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, Double)&amp;#93;&lt;/span&gt; = Array((Harvard-educated,0.5253688097000122), (ex-SAS,0.5213794708251953), (McMutrie,0.5187736749649048), (fellow,0.5166833400726318), (businessman,0.5145374536514282), (American-born,0.5127736330032349), (British-born,0.5062344074249268), (gray-bearded,0.5047978162765503), (American-educated,0.5035858750343323), (mentored,0.49849334359169006))&lt;/p&gt;

&lt;p&gt;scala&amp;gt; model.findSynonyms(&quot;king&quot;, 10)&lt;br/&gt;
res1: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, Double)&amp;#93;&lt;/span&gt; = Array((queen,0.6787897944450378), (prince,0.6786158084869385), (monarch,0.659771203994751), (emperor,0.6490438580513), (goddess,0.643266499042511), (dynasty,0.635733425617218), (sultan,0.6166239380836487), (pharaoh,0.6150713562965393), (birthplace,0.6143025159835815), (empress,0.6109727025032043))&lt;/p&gt;

&lt;p&gt;scala&amp;gt; model.findSynonyms(&quot;queen&quot;, 10)&lt;br/&gt;
res2: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, Double)&amp;#93;&lt;/span&gt; = Array((princess,0.7670737504959106), (godmother,0.6982434988021851), (raven-haired,0.6877717971801758), (swan,0.684934139251709), (hunky,0.6816608309745789), (Titania,0.6808111071586609), (heroine,0.6794036030769348), (king,0.6787897944450378), (diva,0.67848801612854), (lip-synching,0.6731793284416199))&lt;/p&gt;</comment>
                            <comment id="15204983" author="daiqi5477" created="Mon, 21 Mar 2016 19:57:10 +0000"  >&lt;p&gt;I&apos;m trying to test it with the current master branch and nightly build with yarn, but spark always fail to start with the java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.history.YarnHistoryService issue. Does anyone have any idea about this? Looks like no one is reporting this issue. Should I raise another new issue about this?&lt;/p&gt;

&lt;p&gt;The stack is like this:&lt;br/&gt;
java.lang.ClassNotFoundException: org.apache.spark.deploy.yarn.history.YarnHistoryService&lt;br/&gt;
  at scala.reflect.internal.util.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:62)&lt;br/&gt;
  at java.lang.ClassLoader.loadClass(ClassLoader.java:424)&lt;br/&gt;
  at java.lang.ClassLoader.loadClass(ClassLoader.java:357)&lt;br/&gt;
  at java.lang.Class.forName0(Native Method)&lt;br/&gt;
  at java.lang.Class.forName(Class.java:348)&lt;br/&gt;
  at org.apache.spark.util.Utils$.classForName(Utils.scala:177)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:109)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices$$anonfun$start$5.apply(SchedulerExtensionService.scala:108)&lt;br/&gt;
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)&lt;br/&gt;
  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:245)&lt;br/&gt;
  at scala.collection.mutable.ArraySeq.foreach(ArraySeq.scala:74)&lt;br/&gt;
  at scala.collection.TraversableLike$class.map(TraversableLike.scala:245)&lt;br/&gt;
  at scala.collection.AbstractTraversable.map(Traversable.scala:104)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.SchedulerExtensionServices.start(SchedulerExtensionService.scala:108)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.YarnSchedulerBackend.start(YarnSchedulerBackend.scala:81)&lt;br/&gt;
  at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:62)&lt;br/&gt;
  at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144)&lt;br/&gt;
  at org.apache.spark.SparkContext.&amp;lt;init&amp;gt;(SparkContext.scala:501)&lt;br/&gt;
  at org.apache.spark.repl.Main$.createSparkContext(Main.scala:89)&lt;br/&gt;
  ... 48 elided&lt;br/&gt;
java.lang.NullPointerException&lt;br/&gt;
  at org.apache.spark.sql.SQLContext$.createListenerAndUI(SQLContext.scala:1036)&lt;br/&gt;
  at org.apache.spark.sql.hive.HiveContext.&amp;lt;init&amp;gt;(HiveContext.scala:91)&lt;br/&gt;
  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)&lt;br/&gt;
  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)&lt;br/&gt;
  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)&lt;br/&gt;
  at java.lang.reflect.Constructor.newInstance(Constructor.java:422)&lt;br/&gt;
  at org.apache.spark.repl.Main$.createSQLContext(Main.scala:99)&lt;br/&gt;
  ... 48 elided&lt;br/&gt;
&amp;lt;console&amp;gt;:13: error: not found: value sqlContext&lt;br/&gt;
       import sqlContext.implicits._&lt;br/&gt;
              ^&lt;br/&gt;
&amp;lt;console&amp;gt;:13: error: not found: value sqlContext&lt;br/&gt;
       import sqlContext.sql&lt;br/&gt;
              ^&lt;/p&gt;</comment>
                            <comment id="15208646" author="flysjy" created="Wed, 23 Mar 2016 15:59:18 +0000"  >&lt;p&gt;Can you run the SparkContext? The above logs show that sqlContext is not good, but, you should be able to run the word2vec scripts.&lt;/p&gt;</comment>
                            <comment id="15208665" author="daiqi5477" created="Wed, 23 Mar 2016 16:07:12 +0000"  >&lt;p&gt;sc also failed to start&lt;/p&gt;</comment>
                            <comment id="15213778" author="daiqi5477" created="Mon, 28 Mar 2016 04:12:12 +0000"  >&lt;p&gt;I tested this commit on the &quot;One Billion Words Language Modeling&quot; dataset with 72 partitions and 15 iterations. It works well. (word2vec.scala has been changed recently. This PR might need to be updated accordingly.)&lt;/p&gt;</comment>
                            <comment id="15213781" author="daiqi5477" created="Mon, 28 Mar 2016 04:14:15 +0000"  >&lt;p&gt;I figured out the issue. It&apos;s caused by the SPARK_CONF set in my .bashrc for the 1.x spark.&lt;/p&gt;</comment>
                            <comment id="15251415" author="flysjy" created="Thu, 21 Apr 2016 06:58:57 +0000"  >&lt;p&gt;The latest PR should have fixed the issue, and is ready to be merged. Please let me know if there is anything that I can do.&lt;/p&gt;</comment>
                            <comment id="15265262" author="srowen" created="Sat, 30 Apr 2016 09:16:38 +0000"  >&lt;p&gt;Issue resolved by pull request 11812&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/11812&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/11812&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 29 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2sqpj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311620" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Shepherd</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>mlnick</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>