<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:20:20 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-33571] Handling of hybrid to proleptic calendar when reading and writing Parquet data not working correctly</title>
                <link>https://issues.apache.org/jira/browse/SPARK-33571</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;The handling of old dates written with older Spark versions (&amp;lt;2.4.6) using the hybrid calendar in Spark 3.0.0 and 3.0.1 seems to be broken/not working correctly.&lt;/p&gt;

&lt;p&gt;From what I understand it should work like this:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Only relevant for `DateType` before 1582-10-15 or `TimestampType` before 1900-01-01T00:00:00Z&lt;/li&gt;
	&lt;li&gt;Only applies when reading or writing parquet files&lt;/li&gt;
	&lt;li&gt;When reading parquet files written with Spark &amp;lt; 2.4.6 which contain dates or timestamps before the above mentioned moments in time a `SparkUpgradeException` should be raised informing the user to choose either `LEGACY` or `CORRECTED` for the `datetimeRebaseModeInRead`&lt;/li&gt;
	&lt;li&gt;When reading parquet files written with Spark &amp;lt; 2.4.6 which contain dates or timestamps before the above mentioned moments in time and `datetimeRebaseModeInRead` is set to `LEGACY` the dates and timestamps should show the same values in Spark 3.0.1. with for example `df.show()` as they did in Spark 2.4.5&lt;/li&gt;
	&lt;li&gt;When reading parquet files written with Spark &amp;lt; 2.4.6 which contain dates or timestamps before the above mentioned moments in time and `datetimeRebaseModeInRead` is set to `CORRECTED` the dates and timestamps should show different values in Spark 3.0.1. with for example `df.show()` as they did in Spark 2.4.5&lt;/li&gt;
	&lt;li&gt;When writing parqet files with Spark &amp;gt; 3.0.0 which contain dates or timestamps before the above mentioned moment in time a `SparkUpgradeException` should be raised informing the user to choose either `LEGACY` or `CORRECTED` for the `datetimeRebaseModeInWrite`&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;First of all I&apos;m not 100% sure all of this is correct. I&apos;ve been unable to find any clear documentation on the expected behavior. The understanding I have was pieced together from the mailing list (&lt;a href=&quot;http://apache-spark-user-list.1001560.n3.nabble.com/Spark-3-0-1-new-Proleptic-Gregorian-calendar-td38914.html)&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-spark-user-list.1001560.n3.nabble.com/Spark-3-0-1-new-Proleptic-Gregorian-calendar-td38914.html)&lt;/a&gt; the blog post linked there and looking at the Spark code.&lt;/p&gt;

&lt;p&gt;From our testing we&apos;re seeing several issues:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Reading parquet data with Spark 3.0.1 that was written with Spark 2.4.5. that contains fields of type `TimestampType` which contain timestamps before the above mentioned moments in time without `datetimeRebaseModeInRead` set doesn&apos;t raise the `SparkUpgradeException`, it succeeds without any changes to the resulting dataframe compared to that dataframe in Spark 2.4.5&lt;/li&gt;
	&lt;li&gt;Reading parquet data with Spark 3.0.1 that was written with Spark 2.4.5. that contains fields of type `TimestampType` or `DateType` which contain dates or timestamps before the above mentioned moments in time with `datetimeRebaseModeInRead` set to `LEGACY` results in the same values in the dataframe as when using `CORRECTED`, so it seems like no rebasing is happening.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ve made some scripts to help with testing/show the behavior, it uses pyspark 2.4.5, 2.4.6 and 3.0.1. You can find them here &lt;a href=&quot;https://github.com/simonvanderveldt/spark3-rebasemode-issue&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/simonvanderveldt/spark3-rebasemode-issue&lt;/a&gt;. I&apos;ll post the outputs in a comment below as well.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13342812">SPARK-33571</key>
            <summary>Handling of hybrid to proleptic calendar when reading and writing Parquet data not working correctly</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="maxgekk">Max Gekk</assignee>
                                    <reporter username="simonvanderveldt">Simon</reporter>
                        <labels>
                    </labels>
                <created>Thu, 26 Nov 2020 10:54:36 +0000</created>
                <updated>Mon, 12 Dec 2022 18:10:50 +0000</updated>
                            <resolved>Fri, 4 Dec 2020 07:28:22 +0000</resolved>
                                    <version>3.0.0</version>
                    <version>3.0.1</version>
                                    <fixVersion>3.1.0</fixVersion>
                                    <component>PySpark</component>
                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="17239193" author="simonvanderveldt" created="Thu, 26 Nov 2020 11:03:22 +0000"  >&lt;p&gt;Below the output of the timestamp test script with the noise removed&lt;br/&gt;
&lt;b&gt;Writing:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Spark version: 2.4.5
Spark conf [(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;local[*]&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.deployMode&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;client&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.app.name&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;generate-timestamp-data&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.ui.showConsoleProgress&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;&lt;/span&gt;)]
root
 |-- row: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- timestamp: timestamp (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

Writing parquet files to output/timestampspark245/
done
...
Spark version: 2.4.6
Spark conf [(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;local[*]&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.deployMode&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;client&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.app.name&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;generate-timestamp-data&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.ui.showConsoleProgress&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;&lt;/span&gt;)]
root
 |-- row: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- timestamp: timestamp (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

Writing parquet files to output/timestampspark246/
done
...
Spark version: 3.0.1
Spark conf [(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;local[*]&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.pyFiles&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos;), (&apos;&lt;/span&gt;spark.submit.deployMode&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;client&lt;span class=&quot;code-quote&quot;&gt;&apos;), (&apos;&lt;/span&gt;spark.app.name&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;generate-timestamp-data&lt;span class=&quot;code-quote&quot;&gt;&apos;), (&apos;&lt;/span&gt;spark.ui.showConsoleProgress&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;)]
root
 |-- row: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- timestamp: timestamp (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

+---+-------------------+                                                       
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:10:10|
|  2|1880-10-01 10:10:10|
|  3|2020-10-01 10:10:10|
+---+-------------------+

Writing parquet files to output/timestampspark301/
done
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note no exception was raised when writing old timestamps to parquet in spark 3.0.1&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Reading&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Spark version: 3.0.1
Spark conf [(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.app.name&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;read-data&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;local[*]&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.pyFiles&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos;), (&apos;&lt;/span&gt;spark.submit.deployMode&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;client&lt;span class=&quot;code-quote&quot;&gt;&apos;), (&apos;&lt;/span&gt;spark.ui.showConsoleProgress&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;)]
Reading parquet files from output/timestampspark245/*.parquet
root
 |-- row: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- timestamp: timestamp (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

done
...
Spark version: 3.0.1
Spark conf [(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.app.name&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;read-data&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;local[*]&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.pyFiles&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos;), (&apos;&lt;/span&gt;spark.submit.deployMode&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;client&lt;span class=&quot;code-quote&quot;&gt;&apos;), (&apos;&lt;/span&gt;spark.ui.showConsoleProgress&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;)]
Reading parquet files from output/timestampspark246/*.parquet
root
 |-- row: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- timestamp: timestamp (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

done
...
Spark version: 3.0.1
Spark conf [(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.app.name&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;read-data&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;local[*]&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.pyFiles&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos;), (&apos;&lt;/span&gt;spark.submit.deployMode&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;client&lt;span class=&quot;code-quote&quot;&gt;&apos;), (&apos;&lt;/span&gt;spark.ui.showConsoleProgress&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;)]
Reading parquet files from output/timestampspark301/*.parquet
root
 |-- row: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- timestamp: timestamp (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:10:10|
|  2|1880-10-01 10:10:10|
|  3|2020-10-01 10:10:10|
+---+-------------------+

done
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note no exception was raised when reading parquet files written with Spark 2.4.5 containing old timestamps&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Reading using the two different datetimeRebaseModeInRead modes&lt;/b&gt;&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Spark version: 3.0.1
Spark conf [(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.app.name&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;read-data&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.sql.legacy.parquet.datetimeRebaseModeInRead&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;LEGACY&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;local[*]&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.pyFiles&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos;), (&apos;&lt;/span&gt;spark.submit.deployMode&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;client&lt;span class=&quot;code-quote&quot;&gt;&apos;), (&apos;&lt;/span&gt;spark.ui.showConsoleProgress&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;)]
Reading parquet files from output/timestampspark245/*.parquet
root
 |-- row: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- timestamp: timestamp (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

done
...
Spark version: 3.0.1
Spark conf [(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.app.name&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;read-data&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.sql.legacy.parquet.datetimeRebaseModeInRead&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;CORRECTED&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;local[*]&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.pyFiles&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos;), (&apos;&lt;/span&gt;spark.submit.deployMode&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;client&lt;span class=&quot;code-quote&quot;&gt;&apos;), (&apos;&lt;/span&gt;spark.ui.showConsoleProgress&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;)]
Reading parquet files from output/timestampspark245/*.parquet
root
 |-- row: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- timestamp: timestamp (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

+---+-------------------+
|row|          timestamp|
+---+-------------------+
|  1|0220-10-01 10:50:38|
|  2|1880-10-01 10:50:38|
|  3|2020-10-01 10:10:10|
+---+-------------------+

done
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note no difference in the timestamps shown&lt;/p&gt;</comment>
                            <comment id="17239204" author="simonvanderveldt" created="Thu, 26 Nov 2020 11:16:14 +0000"  >&lt;p&gt;Below the output of the date testscript with the noise removed&lt;br/&gt;
Writing without additional config works as expected. Spark 3.0.1. throws a `SparkUpgradeException` when writing to parquet and the dataframe contains old dates.&lt;br/&gt;
Reading without additional config works as expected. Spark 3.0.1. throws a `SparkUpgradeException` when reading parquet files written with Spark 2.4.5 in Spark 3.0.1.&lt;/p&gt;

&lt;p&gt;Reading using the two different `datetimeRebaseModeInRead` modes doesn&apos;t work though, it shows no difference&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Spark version: 3.0.1
Spark conf [(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.app.name&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;read-data&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.sql.legacy.parquet.datetimeRebaseModeInRead&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;LEGACY&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;local[*]&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.pyFiles&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos;), (&apos;&lt;/span&gt;spark.submit.deployMode&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;client&lt;span class=&quot;code-quote&quot;&gt;&apos;), (&apos;&lt;/span&gt;spark.ui.showConsoleProgress&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;)]
Reading parquet files from output/datespark245/*.parquet
root
 |-- row: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- date: date (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

+---+----------+
|row|      date|
+---+----------+
|  1|0220-10-01|
|  2|1880-10-01|
|  3|2020-10-01|
+---+----------+

done
...
Spark version: 3.0.1
Spark conf [(&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.app.name&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;read-data&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.sql.legacy.parquet.datetimeRebaseModeInRead&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;CORRECTED&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.master&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;local[*]&apos;&lt;/span&gt;), (&lt;span class=&quot;code-quote&quot;&gt;&apos;spark.submit.pyFiles&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos;), (&apos;&lt;/span&gt;spark.submit.deployMode&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;client&lt;span class=&quot;code-quote&quot;&gt;&apos;), (&apos;&lt;/span&gt;spark.ui.showConsoleProgress&lt;span class=&quot;code-quote&quot;&gt;&apos;, &apos;&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;)]
Reading parquet files from output/datespark245/*.parquet
root
 |-- row: string (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
 |-- date: date (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)

+---+----------+
|row|      date|
+---+----------+
|  1|0220-10-01|
|  2|1880-10-01|
|  3|2020-10-01|
+---+----------+

done
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Note no difference in the dates shown&lt;/p&gt;</comment>
                            <comment id="17240456" author="gurwls223" created="Mon, 30 Nov 2020 03:07:28 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=maxgekk&quot; class=&quot;user-hover&quot; rel=&quot;maxgekk&quot;&gt;maxgekk&lt;/a&gt; FYI&lt;/p&gt;</comment>
                            <comment id="17241339" author="maxgekk" created="Tue, 1 Dec 2020 08:27:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=simonvanderveldt&quot; class=&quot;user-hover&quot; rel=&quot;simonvanderveldt&quot;&gt;simonvanderveldt&lt;/a&gt;&#160;Thank you for the detailed description and your investigation. Let me clarify a few things:&lt;/p&gt;

&lt;p&gt;&amp;gt; From our testing we&apos;re seeing several issues:&lt;br/&gt;
&amp;gt; Reading parquet data with Spark 3.0.1 that was written with Spark 2.4.5. that contains fields of type `TimestampType` which contain timestamps before the above mentioned moments in time without `datetimeRebaseModeInRead` set doesn&apos;t raise the `SparkUpgradeException`, it succeeds without any changes to the resulting dataframe compares to that dataframe in Spark 2.4.5&lt;/p&gt;

&lt;p&gt;Spark 2.4.5 writes timestamps as parquet INT96 type. The SQL config `datetimeRebaseModeInRead` does not influence on reading such types in Spark 3.0.1, so, Spark performs rebasing always (LEGACY mode). We recently added separate configs for INT96:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/30056&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/30056&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/30121&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/30121&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The changes will be released with Spark 3.1.0.&lt;/p&gt;

&lt;p&gt;&amp;gt; Reading parquet data with Spark 3.0.1 that was written with Spark 2.4.5. that contains fields of type `TimestampType` or `DateType` which contain dates or timestamps before the above mentioned moments in time with `datetimeRebaseModeInRead` set to `LEGACY` results in the same values in the dataframe as when using `CORRECTED`, so it seems like no rebasing is happening.&lt;/p&gt;

&lt;p&gt;For INT96, it seems it is correct behavior. We should observe different results for TIMESTAMP_MICROS and TIMESTAMP_MILLIS types, see the SQL config spark.sql.parquet.outputTimestampType.&lt;/p&gt;

&lt;p&gt;The DATE case is more interesting as we must see a difference in results for ancient dates. I will investigate this case. &lt;/p&gt;

</comment>
                            <comment id="17241379" author="maxgekk" created="Tue, 1 Dec 2020 09:21:38 +0000"  >&lt;p&gt;I have tried to reproduce the issue on the master branch by reading the file saved by Spark 2.4.5 (&lt;a href=&quot;https://github.com/apache/spark/tree/master/sql/core/src/test/resources/test-data):&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/tree/master/sql/core/src/test/resources/test-data):&lt;/a&gt;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-scala&quot;&gt;
  test(&lt;span class=&quot;code-quote&quot;&gt;&quot;SPARK-33571: read ancient dates saved by Spark 2.4.5&quot;&lt;/span&gt;) {
    withSQLConf(SQLConf.LEGACY_PARQUET_REBASE_MODE_IN_READ.key -&amp;gt; LEGACY.toString) {
      &lt;span class=&quot;code-keyword&quot;&gt;val&lt;/span&gt; path = getResourceParquetFilePath(&lt;span class=&quot;code-quote&quot;&gt;&quot;test-data/before_1582_date_v2_4_5.snappy.parquet&quot;&lt;/span&gt;)
      &lt;span class=&quot;code-keyword&quot;&gt;val&lt;/span&gt; df = spark.read.parquet(path)
      df.show(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
    }
    withSQLConf(SQLConf.LEGACY_PARQUET_REBASE_MODE_IN_READ.key -&amp;gt; CORRECTED.toString) {
      &lt;span class=&quot;code-keyword&quot;&gt;val&lt;/span&gt; path = getResourceParquetFilePath(&lt;span class=&quot;code-quote&quot;&gt;&quot;test-data/before_1582_date_v2_4_5.snappy.parquet&quot;&lt;/span&gt;)
      &lt;span class=&quot;code-keyword&quot;&gt;val&lt;/span&gt; df = spark.read.parquet(path)
      df.show(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The results are different in LEGACY and in CORRECTED modes:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+----------+----------+
|dict      |plain     |
+----------+----------+
|1001-01-01|1001-01-01|
|1001-01-01|1001-01-02|
|1001-01-01|1001-01-03|
|1001-01-01|1001-01-04|
|1001-01-01|1001-01-05|
|1001-01-01|1001-01-06|
|1001-01-01|1001-01-07|
|1001-01-01|1001-01-08|
+----------+----------+

+----------+----------+
|dict      |plain     |
+----------+----------+
|1001-01-07|1001-01-07|
|1001-01-07|1001-01-08|
|1001-01-07|1001-01-09|
|1001-01-07|1001-01-10|
|1001-01-07|1001-01-11|
|1001-01-07|1001-01-12|
|1001-01-07|1001-01-13|
|1001-01-07|1001-01-14|
+----------+----------+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17241400" author="maxgekk" created="Tue, 1 Dec 2020 09:35:33 +0000"  >&lt;p&gt;Spark 3.0.1 shows different results as well:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-scala&quot;&gt;
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &apos;_/
   /___/ .__/\_,_/_/ /_/\_\   version 3.0.1
      /_/

Using Scala version 2.12.10 (OpenJDK 64-Bit Server VM, Java 1.8.0_275)
scala&amp;gt; spark.read.parquet(&lt;span class=&quot;code-quote&quot;&gt;&quot;/Users/maximgekk/proj/parquet-read-2_4_5_files/sql/core/src/test/resources/test-data/before_1582_date_v2_4_5.snappy.parquet&quot;&lt;/span&gt;).show(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
20/12/01 12:31:59 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 1)
org.apache.spark.SparkUpgradeException: You may get a different result due to the upgrading of Spark 3.0: reading dates before 1582-10-15 or timestamps before 1900-01-01T00:00:00Z from Parquet files can be ambiguous, as the files may be written by Spark 2.x or legacy versions of Hive, which uses a legacy hybrid calendar that is different from Spark 3.0+&apos;s Proleptic Gregorian calendar. See more details in SPARK-31404. You can set spark.sql.legacy.parquet.datetimeRebaseModeInRead to &apos;LEGACY&apos; to rebase the datetime values w.r.t. the calendar difference during reading. Or set spark.sql.legacy.parquet.datetimeRebaseModeInRead to &apos;CORRECTED&apos; to read the datetime values as it is.

scala&amp;gt; spark.conf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.legacy.parquet.datetimeRebaseModeInRead&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;LEGACY&quot;&lt;/span&gt;)

scala&amp;gt; spark.read.parquet(&lt;span class=&quot;code-quote&quot;&gt;&quot;/Users/maximgekk/proj/parquet-read-2_4_5_files/sql/core/src/test/resources/test-data/before_1582_date_v2_4_5.snappy.parquet&quot;&lt;/span&gt;).show(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
+----------+----------+
|dict      |plain     |
+----------+----------+
|1001-01-01|1001-01-01|
|1001-01-01|1001-01-02|
|1001-01-01|1001-01-03|
|1001-01-01|1001-01-04|
|1001-01-01|1001-01-05|
|1001-01-01|1001-01-06|
|1001-01-01|1001-01-07|
|1001-01-01|1001-01-08|
+----------+----------+


scala&amp;gt; spark.conf.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.legacy.parquet.datetimeRebaseModeInRead&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;CORRECTED&quot;&lt;/span&gt;)

scala&amp;gt; spark.read.parquet(&lt;span class=&quot;code-quote&quot;&gt;&quot;/Users/maximgekk/proj/parquet-read-2_4_5_files/sql/core/src/test/resources/test-data/before_1582_date_v2_4_5.snappy.parquet&quot;&lt;/span&gt;).show(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
+----------+----------+
|dict      |plain     |
+----------+----------+
|1001-01-07|1001-01-07|
|1001-01-07|1001-01-08|
|1001-01-07|1001-01-09|
|1001-01-07|1001-01-10|
|1001-01-07|1001-01-11|
|1001-01-07|1001-01-12|
|1001-01-07|1001-01-13|
|1001-01-07|1001-01-14|
+----------+----------+

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17241408" author="maxgekk" created="Tue, 1 Dec 2020 09:51:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=simonvanderveldt&quot; class=&quot;user-hover&quot; rel=&quot;simonvanderveldt&quot;&gt;simonvanderveldt&lt;/a&gt; Looking at the dates, you tested, both dates 1880-10-01 and 2020-10-01 belong to the Gregorian calendar, so, should be no diffs.&lt;/p&gt;

&lt;p&gt;For the date 0220-10-01, please, have a look at the table which I built in the PR: &lt;a href=&quot;https://github.com/apache/spark/pull/28067&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/28067&lt;/a&gt; . The table shows that there is no diffs between 2 calendars for the year.&lt;/p&gt;</comment>
                            <comment id="17243440" author="apachespark" created="Thu, 3 Dec 2020 18:43:56 +0000"  >&lt;p&gt;User &apos;MaxGekk&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/30596&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/30596&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17243441" author="maxgekk" created="Thu, 3 Dec 2020 18:45:43 +0000"  >&lt;p&gt;I opened the PR &lt;a href=&quot;https://github.com/apache/spark/pull/30596&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/30596&lt;/a&gt;&#160;with some improvements for config docs. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt;&#160;could you review it, please.&lt;/p&gt;</comment>
                            <comment id="17243789" author="gurwls223" created="Fri, 4 Dec 2020 07:28:22 +0000"  >&lt;p&gt;Fixed in &lt;a href=&quot;https://github.com/apache/spark/pull/30596&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/30596&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17243856" author="apachespark" created="Fri, 4 Dec 2020 08:56:26 +0000"  >&lt;p&gt;User &apos;MaxGekk&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/30604&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/30604&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17245969" author="simonvanderveldt" created="Tue, 8 Dec 2020 16:12:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=maxgekk&quot; class=&quot;user-hover&quot; rel=&quot;maxgekk&quot;&gt;maxgekk&lt;/a&gt; Thanks for taking the time to look into this, for the updates to the documentation and for the explanation! I think the docs with regards to dates are clear now.&lt;br/&gt;
The actual data I ran into this issue with used the year 220 so that&apos;s why I used it, of course that&apos;s the one century with a 0 day diff &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/tongue.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; The table with the different diffs between the two calendars cleared it up a lot, I used some different dates and can now also see the differences between the two read modes.&lt;/p&gt;

&lt;p&gt;If you don&apos;t mind I have two additional questions:&lt;br/&gt;
&amp;gt; Spark 2.4.5 writes timestamps as parquet INT96 type. The SQL config `datetimeRebaseModeInRead` does not influence on reading such types in Spark 3.0.1, so, Spark performs rebasing always (LEGACY mode). We recently added separate configs for INT96...&lt;/p&gt;


&lt;p&gt;The behavior of the to be introduced in Spark 3.1 `spark.sql.legacy.parquet.int96RebaseModeIn*` is the same as for `datetimeRebaseModeIn*`? So Spark will check the parquet metadata for Spark version and the `datetimeRebaseModeInRead` metadata key and use the correct behavior. If those are not set it will raise an exception and ask the user to define the mode. Is that correct?&lt;/p&gt;

&lt;p&gt;(P.S. You explicitly mention Spark 2.4.5 writes timestamps as INT96, but from my testing Spark 3 does the same by default, not sure if that aligns with your findings?)&lt;/p&gt;

&lt;p&gt;&amp;gt; For INT96, it seems it is correct behavior. We should observe different results for TIMESTAMP_MICROS and TIMESTAMP_MILLIS types, see the SQL config spark.sql.parquet.outputTimestampType.&lt;/p&gt;

&lt;p&gt;What is the expected behavior for TIMESTAMP_MICROS and TIMESTAMP_MILLIS with regards to this?&lt;/p&gt;</comment>
                            <comment id="17246717" author="maxgekk" created="Wed, 9 Dec 2020 17:48:36 +0000"  >&lt;p&gt;&amp;gt; The behavior of the to be introduced in Spark 3.1 `spark.sql.legacy.parquet.int96RebaseModeIn*` is the same as for `datetimeRebaseModeIn*`?&lt;/p&gt;

&lt;p&gt;Yes.&lt;/p&gt;

&lt;p&gt;&amp;gt; So Spark will check the parquet metadata for Spark version and the `datetimeRebaseModeInRead` metadata key and use the correct behavior.&lt;/p&gt;

&lt;p&gt;Correct, except of names of metadata keys. Spark checks , see &lt;a href=&quot;https://github.com/MaxGekk/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L58-L68&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/MaxGekk/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/package.scala#L58-L68&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&amp;gt; If those are not set it will raise an exception and ask the user to define the mode. Is that correct?&lt;/p&gt;

&lt;p&gt;Yes. Spark should raise the exception if it is not clear which calendar the writer used.&lt;/p&gt;

&lt;p&gt;&amp;gt; but from my testing Spark 3 does the same by default, not sure if that aligns with your findings?&lt;/p&gt;

&lt;p&gt;Spark 3.0.0-SNAPSHOT saved timestamps as TIMESTAMP_MICROS in parquet till &lt;a href=&quot;https://github.com/apache/spark/pull/28450&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/28450&lt;/a&gt; . I just wanted to say that the configs datetimeRebaseModeIn* you pointed out don&apos;t impact on INT96 in Spark 3.0.&lt;/p&gt;

&lt;p&gt;&amp;gt; What is the expected behavior for TIMESTAMP_MICROS and TIMESTAMP_MILLIS with regards to this?&lt;/p&gt;

&lt;p&gt;The same as for DATE type. Spark takes into account the same SQL configs and metdata keys from parquet files.&lt;/p&gt;
</comment>
                            <comment id="17248899" author="simonvanderveldt" created="Mon, 14 Dec 2020 10:51:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=maxgekk&quot; class=&quot;user-hover&quot; rel=&quot;maxgekk&quot;&gt;maxgekk&lt;/a&gt; OK, all clear. Thanks again for the clarifications!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 48 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0kyq8:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>