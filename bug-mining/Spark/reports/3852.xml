<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:46:08 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-16664] Spark 1.6.2 - Persist call on Data frames with more than 200 columns is wiping out the data.</title>
                <link>https://issues.apache.org/jira/browse/SPARK-16664</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Calling persist on a data frame with more than 200 columns is removing the data from the data frame. This is an issue in Spark 1.6.2. Works with out any issues in Spark 1.6.1&lt;/p&gt;

&lt;p&gt;Following test case demonstrates problem. Please let me know if you need any additional information. Thanks.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark._
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.rdd.RDD
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.types._
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.{Row, SQLContext}
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.scalatest.FunSuite

&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;TestSpark162_1 &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; FunSuite {

  test(&lt;span class=&quot;code-quote&quot;&gt;&quot;test data frame with 200 columns&quot;&lt;/span&gt;) {
    val sparkConfig = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf()
    val parallelism = 5
    sparkConfig.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.parallelism&quot;&lt;/span&gt;, s&lt;span class=&quot;code-quote&quot;&gt;&quot;$parallelism&quot;&lt;/span&gt;)
    sparkConfig.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.shuffle.partitions&quot;&lt;/span&gt;, s&lt;span class=&quot;code-quote&quot;&gt;&quot;$parallelism&quot;&lt;/span&gt;)

    val sc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkContext(s&lt;span class=&quot;code-quote&quot;&gt;&quot;local[3]&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;TestNestedJson&quot;&lt;/span&gt;, sparkConfig)
    val sqlContext = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SQLContext(sc)

    &lt;span class=&quot;code-comment&quot;&gt;// create dataframe with 200 columns and fake 200 values
&lt;/span&gt;    val size = 200
    val rdd: RDD[Seq[&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;]] = sc.parallelize(Seq(Seq.range(0, size)))
    val rowRdd: RDD[Row] = rdd.map(d =&amp;gt; Row.fromSeq(d))

    val schemas = List.range(0, size).map(a =&amp;gt; StructField(&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;+ a, LongType, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;))
    val testSchema = StructType(schemas)
    val testDf = sqlContext.createDataFrame(rowRdd, testSchema)

    &lt;span class=&quot;code-comment&quot;&gt;// test value
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;(testDf.persist.take(1).apply(0).toSeq(100).asInstanceOf[&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;] == 100)
    sc.stop()
  }

  test(&lt;span class=&quot;code-quote&quot;&gt;&quot;test data frame with 201 columns&quot;&lt;/span&gt;) {
    val sparkConfig = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf()
    val parallelism = 5
    sparkConfig.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.parallelism&quot;&lt;/span&gt;, s&lt;span class=&quot;code-quote&quot;&gt;&quot;$parallelism&quot;&lt;/span&gt;)
    sparkConfig.set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.sql.shuffle.partitions&quot;&lt;/span&gt;, s&lt;span class=&quot;code-quote&quot;&gt;&quot;$parallelism&quot;&lt;/span&gt;)

    val sc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkContext(s&lt;span class=&quot;code-quote&quot;&gt;&quot;local[3]&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;TestNestedJson&quot;&lt;/span&gt;, sparkConfig)
    val sqlContext = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SQLContext(sc)

    &lt;span class=&quot;code-comment&quot;&gt;// create dataframe with 201 columns and fake 201 values
&lt;/span&gt;    val size = 201
    val rdd: RDD[Seq[&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;]] = sc.parallelize(Seq(Seq.range(0, size)))
    val rowRdd: RDD[Row] = rdd.map(d =&amp;gt; Row.fromSeq(d))

    val schemas = List.range(0, size).map(a =&amp;gt; StructField(&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;+ a, LongType, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;))
    val testSchema = StructType(schemas)
    val testDf = sqlContext.createDataFrame(rowRdd, testSchema)


    &lt;span class=&quot;code-comment&quot;&gt;// test value
&lt;/span&gt;    &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;(testDf.persist.take(1).apply(0).toSeq(100).asInstanceOf[&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;] == 100)
    sc.stop()
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12991445">SPARK-16664</key>
            <summary>Spark 1.6.2 - Persist call on Data frames with more than 200 columns is wiping out the data.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="darkcaller">Wesley Tang</assignee>
                                    <reporter username="skolli">Satish Kolli</reporter>
                        <labels>
                    </labels>
                <created>Thu, 21 Jul 2016 13:22:10 +0000</created>
                <updated>Mon, 28 Aug 2017 21:56:56 +0000</updated>
                            <resolved>Fri, 29 Jul 2016 11:28:11 +0000</resolved>
                                    <version>1.6.2</version>
                                    <fixVersion>1.6.3</fixVersion>
                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>16</watches>
                                                                                                                <comments>
                            <comment id="15387717" author="srowen" created="Thu, 21 Jul 2016 13:49:18 +0000"  >&lt;p&gt;What does &quot;wipe out data&quot; mean here? your query is for 100 elements, not 200.&lt;/p&gt;</comment>
                            <comment id="15387745" author="yzhou30" created="Thu, 21 Jul 2016 14:10:36 +0000"  >&lt;p&gt;Hi there,&lt;/p&gt;

&lt;p&gt;The first test case creates a dataframe with a 1 row dataframe, fill 200 columns from 0 to 199, then persist, and then get the 1 row back, and verify that 100th column value is 100.&lt;br/&gt;
The second test case doing the same thing but creates a dataframe with 201 columns.&lt;/p&gt;

&lt;p&gt;In 2nd case, the entire dataframe became &apos;empty&apos; after persist() call.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;

&lt;p&gt;-ying&lt;/p&gt;</comment>
                            <comment id="15387858" author="skolli" created="Thu, 21 Jul 2016 15:06:18 +0000"  >&lt;p&gt;Here is a demonstration from the spark shell: &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$SPARK_HOME/bin/spark-shell --master local[4]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt;

scala&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark._
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark._

scala&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.rdd.RDD
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.rdd.RDD

scala&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.types._
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.types._

scala&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.{Row, SQLContext}
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.{Row, SQLContext}

scala&amp;gt;

scala&amp;gt; val size = 201
size: Int = 201

scala&amp;gt; val rdd: RDD[Seq[&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;]] = sc.parallelize(Seq(Seq.range(0, size)))
rdd: org.apache.spark.rdd.RDD[Seq[&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;]] = ParallelCollectionRDD[36] at parallelize at &amp;lt;console&amp;gt;:53

scala&amp;gt; val rowRdd: RDD[Row] = rdd.map(d =&amp;gt; Row.fromSeq(d))
rowRdd: org.apache.spark.rdd.RDD[org.apache.spark.sql.Row] = MapPartitionsRDD[37] at map at &amp;lt;console&amp;gt;:55

scala&amp;gt;

scala&amp;gt; val schemas = List.range(0, size).map(a =&amp;gt; StructField(&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;+ a, LongType, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;))
schemas: List[org.apache.spark.sql.types.StructField] = List(StructField(name0,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name1,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name2,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name3,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(na                                                                       me4,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name5,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name6,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name7,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name8,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name9,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name10,Lo                                                                       ngType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name11,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name12,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name13,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name14,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name15,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name16,Lon                                                                       gType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name17,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name18,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name19,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name20,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField...
scala&amp;gt; val testSchema = StructType(schemas)
testSchema: org.apache.spark.sql.types.StructType = StructType(StructField(name0,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name1,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name2,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name3,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(                                                                       name4,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name5,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name6,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name7,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name8,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name9,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name10,                                                                       LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name11,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name12,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name13,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name14,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name15,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name16,L                                                                       ongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name17,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name18,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name19,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructField(name20,LongType,&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;), StructFie...
scala&amp;gt; val testDf = sqlContext.createDataFrame(rowRdd, testSchema)
testDf: org.apache.spark.sql.DataFrame = [name0: bigint, name1: bigint, name2: bigint, name3: bigint, name4: bigint, name5: bigint, name6: bigint, name7: bigint, name8: bigint, name9: bigint, name10: bigint, nam                                                                       e11: bigint, name12: bigint, name13: bigint, name14: bigint, name15: bigint, name16: bigint, name17: bigint, name18: bigint, name19: bigint, name20: bigint, name21: bigint, name22: bigint, name23: bigint, name24                                                                       : bigint, name25: bigint, name26: bigint, name27: bigint, name28: bigint, name29: bigint, name30: bigint, name31: bigint, name32: bigint, name33: bigint, name34: bigint, name35: bigint, name36: bigint, name37: b                                                                       igint, name38: bigint, name39: bigint, name40: bigint, name41: bigint, name42: bigint, name43: bigint, name44: bigint, name45: bigint, name46: bigint, name47: bigin...
scala&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;&lt;font color=&quot;green&quot;&gt;&lt;b&gt;Take the first row from the data frame before calling persist:&lt;/b&gt;&lt;/font&gt;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; testDf.take(1)
res9: Array[org.apache.spark.sql.Row] = Array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,                                                                       58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,1                                                                       21,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,                                                                       174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200])
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;font color=&quot;red&quot;&gt;&lt;b&gt;Take the first row from the data frame after calling persist:&lt;/b&gt;&lt;/font&gt;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; testDf.persist.take(1)
res10: Array[org.apache.spark.sql.Row] = Array([0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0                                                                      ,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0])
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15388209" author="dongjoon" created="Thu, 21 Jul 2016 18:44:16 +0000"  >&lt;p&gt;It looks like that. FYI, here is the result of current master. As you said, it&apos;s really 1.6.2 specific.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.types._
scala&amp;gt; val size = 200
scala&amp;gt; val rdd: org.apache.spark.rdd.RDD[Seq[&lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;]] = spark.sparkContext.parallelize(Seq(Seq.range(0, size)))
scala&amp;gt; val rowRdd = rdd.map(d =&amp;gt; org.apache.spark.sql.Row.fromSeq(d))
scala&amp;gt; val schemas = List.range(0, size).map(a =&amp;gt; StructField(&lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt; + a, LongType, &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;))
scala&amp;gt; val testDf = spark.createDataFrame(rowRdd, StructType(schemas))
scala&amp;gt; testDf.persist().take(1)
res0: Array[org.apache.spark.sql.Row] = Array([0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199])
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15388219" author="dongjoon" created="Thu, 21 Jul 2016 18:50:39 +0000"  >&lt;p&gt;Also, FYI, RC5 is the same with the current master.&lt;/p&gt;</comment>
                            <comment id="15389367" author="darkcaller" created="Fri, 22 Jul 2016 11:53:49 +0000"  >&lt;p&gt;According to the original post, the size should be 201 to prove master works&lt;/p&gt;</comment>
                            <comment id="15389397" author="skolli" created="Fri, 22 Jul 2016 12:23:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dongjoon&quot; class=&quot;user-hover&quot; rel=&quot;dongjoon&quot;&gt;dongjoon&lt;/a&gt; Problem exists in master also. I tried a nightly build from the following and it failed.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;http://people.apache.org/~pwendell/spark-nightly/spark-master-bin/latest/

spark-2.1.0-SNAPSHOT-bin-hadoop2.4.tgz	2016-07-22 08:15	175M
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15390525" author="proflin" created="Sat, 23 Jul 2016 04:19:49 +0000"  >&lt;p&gt;I think I&apos;ve found the root cause and I&apos;m going to submit a patch shortly; thanks!&lt;/p&gt;</comment>
                            <comment id="15390534" author="apachespark" created="Sat, 23 Jul 2016 04:47:05 +0000"  >&lt;p&gt;User &apos;breakdawn&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14324&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14324&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15390562" author="dongjoon" created="Sat, 23 Jul 2016 05:57:17 +0000"  >&lt;p&gt;Oh, I missed the 201 cases. Sorry.&lt;/p&gt;</comment>
                            <comment id="15397259" author="jchamp" created="Thu, 28 Jul 2016 08:37:06 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We are experiencing the same problem here with Spark 1.6.2 on Ubuntu / CentOS / MacOS when using a DataFrame with more than 200 columns :&lt;br/&gt;
-&amp;gt; Data is Wiped ( This was working in Spark 1.6.1 )&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;And with more than 8117 cols, it&apos;s not working, but there is an exception :&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:555)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:575)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:572)&lt;br/&gt;
	at org.spark-project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)&lt;br/&gt;
	at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)&lt;br/&gt;
	... 28 more&lt;br/&gt;
Caused by: org.codehaus.janino.JaninoRuntimeException: Code of method &quot;()Z&quot; of class &quot;org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificColumnarIterator&quot; grows beyond 64 KB&lt;/li&gt;
&lt;/ul&gt;



</comment>
                            <comment id="15399166" author="srowen" created="Fri, 29 Jul 2016 11:28:12 +0000"  >&lt;p&gt;Issue resolved by pull request 14324&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14324&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14324&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15399168" author="tgraves" created="Fri, 29 Jul 2016 11:30:41 +0000"  >&lt;p&gt;I am out of the office until 8/8. Please contact my manager or file support request if you need immediate help.&lt;/p&gt;</comment>
                            <comment id="15399754" author="apachespark" created="Fri, 29 Jul 2016 17:52:06 +0000"  >&lt;p&gt;User &apos;breakdawn&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14404&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14404&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12997311">SPARK-17061</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12997009">SPARK-17043</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12999705">SPARK-17218</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13000999">SPARK-17294</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13097997">SPARK-21851</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12992392">SPARK-16716</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310250" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10431"><![CDATA[Important]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 16 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i31bev:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12336857">2.0.1</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>