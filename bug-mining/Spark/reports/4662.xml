<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:52:04 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-16599] java.util.NoSuchElementException: None.get  at at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)</title>
                <link>https://issues.apache.org/jira/browse/SPARK-16599</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;run a spark job with spark 2.0, error message&lt;/p&gt;

&lt;p&gt;Job aborted due to stage failure: Task 0 in stage 821.0 failed 4 times, most recent failure: Lost task 0.3 in stage 821.0 (TID 1480, e103): java.util.NoSuchElementException: None.get&lt;br/&gt;
	at scala.None$.get(Option.scala:347)&lt;br/&gt;
	at scala.None$.get(Option.scala:345)&lt;br/&gt;
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:644)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;
</description>
                <environment>&lt;p&gt;centos 6.7   spark 2.0&lt;/p&gt;</environment>
        <key id="12990289">SPARK-16599</key>
            <summary>java.util.NoSuchElementException: None.get  at at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="8">Not A Problem</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="zhudebin">devinzhu</reporter>
                        <labels>
                    </labels>
                <created>Mon, 18 Jul 2016 08:16:50 +0000</created>
                <updated>Fri, 8 Dec 2017 20:45:00 +0000</updated>
                            <resolved>Fri, 8 Dec 2017 20:45:00 +0000</resolved>
                                    <version>2.0.0</version>
                                                        <due></due>
                            <votes>4</votes>
                                    <watches>20</watches>
                                                                                                                <comments>
                            <comment id="15381894" author="srowen" created="Mon, 18 Jul 2016 08:32:46 +0000"  >&lt;p&gt;Any more info like how to reproduce this?&lt;/p&gt;</comment>
                            <comment id="15474055" author="naegelejd" created="Thu, 8 Sep 2016 14:46:34 +0000"  >&lt;p&gt;I believe I can reproduce this. I have a singleton &quot;Engine&quot; object which instantiates a single &lt;tt&gt;SparkSession&lt;/tt&gt; for my entire app.&lt;/p&gt;

&lt;p&gt;Here is the relevant, &lt;b&gt;working&lt;/b&gt; excerpt:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;object Engine {
  ...
  &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; currentSession: SparkSession = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;

  def session: SparkSession = {
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (currentSession == &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
      Log.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Creating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkSession&quot;&lt;/span&gt;)
      currentSession = SparkSession
        .builder()
        .appName(appName)
        .master(Settings.SparkMaster)
        .getOrCreate()

      currentSession.udf.register(...)
    }
    currentSession
  }

  def someQuery(): Dataset[Foo] = {
    val s = session
    &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; s.implicits._

    s.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from foo&quot;&lt;/span&gt;).as[Foo]
  }

}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If I simplify it to the following, I see the reported error:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;object Engine {
  ...

  Log.debug(&lt;span class=&quot;code-quote&quot;&gt;&quot;Creating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkSession&quot;&lt;/span&gt;)
  val session = SparkSession
    .builder()
    .appName(appName)
    .master(Settings.SparkMaster)
    .getOrCreate()

  session.udf.register(...)

  def allFoos(): Dataset[Foo] = {
    session.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from foo&quot;&lt;/span&gt;).as[Foo]
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The exception occurs when I attempt to write a Dataset to disk (using spark-avro):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  val foos = Engine.allFoos()
  &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; com.databricks.spark.avro._
  foos.write.mode(SaveMode.Overwrite).avro(&lt;span class=&quot;code-quote&quot;&gt;&quot;hdfs:&lt;span class=&quot;code-comment&quot;&gt;///foos&quot;&lt;/span&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here&apos;s the stacktrace beginning from the &lt;tt&gt;write&lt;/tt&gt; call above:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;[Stage 2:&amp;gt;                                                          (0 + 2) / 7]16/09/08 14:24:39 WARN TaskSetManager: Lost task 2.0 in stage 2.0 (TID 5, 0.0.0.0): java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:347)
        at scala.None$.get(Option.scala:345)
        at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
        at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

[Stage 2:================&amp;gt;                                          (2 + 2) / 7]16/09/08 14:24:39 ERROR TaskSetManager: Task 3 in stage 2.0 failed 4 times; aborting job
16/09/08 14:24:39 ERROR InsertIntoHadoopFsRelationCommand: Aborting job.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 2.0 failed 4 times, most recent failure: Lost task 3.3 in stage 2.0 (TID 13, 0.0.0.0): java.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:347)
        at scala.None$.get(Option.scala:345)
        at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
        at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1429)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1417)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1416)
        at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1416)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1640)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1599)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1588)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1886)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1899)
        at org.apache.spark.SparkContext.runJob(SparkContext.scala:1919)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelationCommand.scala:143)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$$anonfun$run$1.apply(InsertIntoHadoopFsRelationCommand.scala:115)
        at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
        at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:115)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:60)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:58)
        at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
        at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
        at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
        at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
        at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:88)
        at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:88)
        at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:495)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:213)
        at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:196)
        at com.databricks.spark.avro.&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;$AvroDataFrameWriter$$anonfun$avro$1.apply(&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;.scala:26)
        at com.databricks.spark.avro.&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;$AvroDataFrameWriter$$anonfun$avro$1.apply(&lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt;.scala:26)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15476972" author="srowen" created="Fri, 9 Sep 2016 12:39:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; do you think it&apos;s as simple as changing BlockInfoManager here ...&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    val readLocks = &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; {
      readLocksByTask.remove(taskAttemptId).get
    }
    val writeLocks = &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; {
      writeLocksByTask.remove(taskAttemptId).getOrElse(Seq.empty)
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;to call getOrElse(Seq.empty) for readLocks too? Of course that fixes the immediate problem but didn&apos;t know if there was a reason this shouldn&apos;t be possible.&lt;/p&gt;</comment>
                            <comment id="15477378" author="joshrosen" created="Fri, 9 Sep 2016 15:45:59 +0000"  >&lt;p&gt;I&apos;m not quite sure why this asymmetry between &lt;tt&gt;readLocksByTask&lt;/tt&gt; and &lt;tt&gt;writeLocksByTask&lt;/tt&gt; exists. However, the scenario that occurred here is supposed to be impossible because the {[readLocksByTask}} entry should have been created when &lt;tt&gt;BlockInfoManager.regsiterTask&lt;/tt&gt; as called:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  /**
   * Called at the start of a task in order to register that task with &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; [[BlockInfoManager]].
   * This must be called prior to calling any other BlockInfoManager methods from that task.
   */
  def registerTask(taskAttemptId: TaskAttemptId): Unit = &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; {
    require(!readLocksByTask.contains(taskAttemptId),
      s&lt;span class=&quot;code-quote&quot;&gt;&quot;Task attempt $taskAttemptId is already registered&quot;&lt;/span&gt;)
    readLocksByTask(taskAttemptId) = ConcurrentHashMultiset.create()
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My hunch is that there&apos;s some other bug here which is manifesting itself through this error.&lt;/p&gt;</comment>
                            <comment id="15479567" author="srowen" created="Sat, 10 Sep 2016 10:20:38 +0000"  >&lt;p&gt; The only thing I can see is that somehow a task starts and is waiting on the BlockInfoManager lock to register the attempt, and is interrupted or something during shutdown, and so tries to cleanup without having actually registered any task info. &lt;/p&gt;

&lt;p&gt;Or shutdown clears the state, and then in the process of cleaning up it sees no task attempts at all. But both of those scenarios mean some error caused the app to shutdown, and I don&apos;t know if that&apos;s the case here. Still, this seems like a potentially possible scenario we might have to guard against.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=naegelejd&quot; class=&quot;user-hover&quot; rel=&quot;naegelejd&quot;&gt;naegelejd&lt;/a&gt; are you in a position to build Spark with a modification and try again locally? or is there any self-contained reproduction you can find? Because I&apos;d like to get more info by modifying that bit of code to this, temporarily, to gain more info:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    val readLocks = &lt;span class=&quot;code-keyword&quot;&gt;synchronized&lt;/span&gt; {
      val maybeReadLocks = readLocksByTask.remove(taskAttemptId)
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!maybeReadLocks.isDefined) {
        logWarning(s&lt;span class=&quot;code-quote&quot;&gt;&quot;No read locks registered &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; task attempt $taskAttemptId ?&quot;&lt;/span&gt;)
      }
      maybeReadLocks.getOrElse(Seq.empty)
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If this occurs while an exception was thrown it may let the underlying exception bubble up too, to help understand the scenario.&lt;/p&gt;</comment>
                            <comment id="15570826" author="shiv4nsh" created="Thu, 13 Oct 2016 04:44:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt;: Any updates on this issue ? We are also facing the same issue here. Can you please let us know what is the exact problem ?? We are using Cassandra as a store .&lt;/p&gt;</comment>
                            <comment id="15571478" author="srowen" created="Thu, 13 Oct 2016 10:02:29 +0000"  >&lt;p&gt;What You See Is What You Get here. Nobody is working on this AFAICT, and there was no reply to my last question.&lt;br/&gt;
If you want to jump in, can you try the change I suggested above?&lt;/p&gt;</comment>
                            <comment id="15571885" author="naegelejd" created="Thu, 13 Oct 2016 13:18:29 +0000"  >&lt;p&gt;I apologize for not replying. I don&apos;t have any time in the short-term to dig into this and I can&apos;t reproduce it with a stripped-down example. We had to revert to 1.6.2 in production for a handful of reasons.&lt;/p&gt;</comment>
                            <comment id="15733363" author="mnagle" created="Thu, 8 Dec 2016 20:52:30 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I get a similar error. In my code i also try to create a singleton session and a singleton object. i get this error when i try using the map function of my dataframe and pass each &quot;row&quot; in the singleton object&apos;s method.&lt;/p&gt;

&lt;p&gt;I get this error when i run my code on EMR cluster&lt;/p&gt;

&lt;p&gt;my code :&lt;/p&gt;

&lt;p&gt;object SparkSessionHolder{&lt;br/&gt;
  val spark = SparkSession.builder().appName(&quot;my Spark App&quot;).master(&quot;yarn&quot;).getOrCreate()&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;object MyObjectHolder{&lt;br/&gt;
  val dMakerClient: MyObject = initializeClient(SparkSessionHolder.spark, CONTRACT_FILE,FACT_HEADER,RESULT_HEADER, RULES_FILE);&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;object MyObjectWithMain {&lt;/p&gt;

&lt;p&gt;  def main(args: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;) = &lt;/p&gt;
{

    val cliArgs = new CommandLineArguments(MySparkOptions.getOptionsForDMaker, args)
    /* executing spark jobs */
    executeSparkJob(cliArgs)
  }

&lt;p&gt;  def executeSparkJob(cliArgs: CommandLineArguments): Unit = {&lt;/p&gt;

&lt;p&gt;   val spark = SparkSessionHolder.spark&lt;/p&gt;

&lt;p&gt;    try &lt;/p&gt;
{
     /// when i iterate this DF i get the exception
      val finalFactDF = getFactData(spark, some_arguments);

      // exception thrown in this method
      val resultDF = getResultData(spark, finalFactDF)
      resultDF.foreach(row =&amp;gt; println(&quot;row &quot;+row))
    
    }
&lt;p&gt;catch&lt;/p&gt;
{
      case e: Exception =&amp;gt; e.printStackTrace()
    }
&lt;p&gt;    finally &lt;/p&gt;
{
      spark.stop()
    }
&lt;p&gt;  }&lt;/p&gt;


&lt;p&gt;  private def getFactData(spark: SparkSession, some_arguments): DataFrame = &lt;/p&gt;
{


    /// some logic


  }


&lt;p&gt;  private def getResultData(spark: SparkSession, factDF: DataFrame): DataFrame = &lt;/p&gt;
{

    import spark.implicits._

   
/// when i call makedecision method outside the map function it returns the results but throws exception when called inside the map function
    val firstRow = factDF.first().toSeq.mkString(&quot;,&quot;)
    println(&quot;first row &quot;+firstRow)
    val dResult = MyObjectHolder.MyObject.makeDecision(firstRow)
    println(&quot;dResult &quot;+dResult)
  
    // iterate each fact row, convert it to comma separated string and pass it to myObject
    val resultDF = factDF.map(row =&amp;gt; MyObjectHolder.myObject.makeDecision(row.toSeq.mkString(DMakerConstants.CSV_DELIMITER))).toDF(RESULT_COLUMN);
    )
    resultDF

  }

&lt;p&gt;}&lt;/p&gt;


&lt;p&gt;Exception :&lt;br/&gt;
aborting job&lt;br/&gt;
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 26.0 failed 4 times, most recent failure: Lost task 0.3 in stage 26.0 (TID 1464, ip-172-31-29-201.ec2.internal): java.util.NoSuchElementException: None.get&lt;br/&gt;
	at scala.None$.get(Option.scala:347)&lt;br/&gt;
	at scala.None$.get(Option.scala:345)&lt;br/&gt;
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;Driver stacktrace:&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1454)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1442)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1441)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1441)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)&lt;br/&gt;
	at scala.Option.foreach(Option.scala:257)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1667)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1622)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1611)&lt;br/&gt;
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1890)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1903)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1916)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1930)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:894)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:892)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:358)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.foreach(RDD.scala:892)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$foreach$1.apply$mcV$sp(Dataset.scala:2108)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$foreach$1.apply(Dataset.scala:2108)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$foreach$1.apply(Dataset.scala:2108)&lt;br/&gt;
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.foreach(Dataset.scala:2107)&lt;br/&gt;
	at com.amazon.ppr.amu.etl.monthly.MonthlyPremPsmOverrideDMaker$.executeSparkJob(MonthlyPremPsmOverrideDMaker.scala:108)&lt;br/&gt;
	at com.amazon.ppr.amu.etl.monthly.MonthlyPremPsmOverrideDMaker$.main(MonthlyPremPsmOverrideDMaker.scala:50)&lt;br/&gt;
	at com.amazon.ppr.amu.etl.monthly.MonthlyPremPsmOverrideDMaker.main(MonthlyPremPsmOverrideDMaker.scala)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:627)&lt;br/&gt;
Caused by: java.util.NoSuchElementException: None.get&lt;br/&gt;
	at scala.None$.get(Option.scala:347)&lt;br/&gt;
	at scala.None$.get(Option.scala:345)&lt;br/&gt;
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;</comment>
                            <comment id="15845639" author="dubovsky" created="Mon, 30 Jan 2017 18:33:07 +0000"  >&lt;p&gt;I have a piece of code in &lt;a href=&quot;https://github.com/andypetrella/spark-notebook/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;spark notebook&lt;/a&gt; which throws exactly this stack trace consistently.&lt;br/&gt;
The same code run through spark-submit runs just fine. I use spark 2.1.0 for this.&lt;/p&gt;

&lt;p&gt;I wonder if I can be of some help to you solving this having this code at hand. I see the suggestion to use custom compiled spark with that log mentioned above. But I would need an assistance of author of spark notebook with this...&lt;/p&gt;</comment>
                            <comment id="15847889" author="yetsun" created="Wed, 1 Feb 2017 02:21:33 +0000"  >&lt;p&gt;I got the same exception with the following code: &lt;/p&gt;

&lt;p&gt;(My Spark version is 2.1.0. Scala version: 2.11.8. Hadoop version: 2.7.3)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;MyClass(id: Int, name: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;)

val myDF = sparkSession.sparkContext
.textFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;s3a:&lt;span class=&quot;code-comment&quot;&gt;//myS3Bucket/myTextFile.txt&quot;&lt;/span&gt;)
&lt;/span&gt;.map(_.split(&lt;span class=&quot;code-quote&quot;&gt;&quot;\t&quot;&lt;/span&gt;))
.map(_.map(_.trim))
.map(a =&amp;gt; MyClass(a(0).toInt, a(1)))
.toDF
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The exception only happened at the following line:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;.map(a =&amp;gt; MyClass(a(0).toInt, a(1)))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If I removed this line, there is no exception. &lt;/p&gt;

&lt;p&gt;So I changed it to the following to bypass this exception.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val myDF = sparkSession.sparkContext
.textFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;s3a:&lt;span class=&quot;code-comment&quot;&gt;//myS3Bucket/myTextFile.txt&quot;&lt;/span&gt;)
&lt;/span&gt;.map(_.split(&lt;span class=&quot;code-quote&quot;&gt;&quot;\t&quot;&lt;/span&gt;))
.map(_.map(_.trim))
.map {
  &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; a: Array[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;] =&amp;gt; (a(0).toInt, a(1))
}.toDF(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;name&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It works for me.&lt;/p&gt;

</comment>
                            <comment id="15848138" author="dubovsky" created="Wed, 1 Feb 2017 08:53:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yetsun&quot; class=&quot;user-hover&quot; rel=&quot;yetsun&quot;&gt;yetsun&lt;/a&gt; Have you run this in spark-shell or by spark-submit?&lt;/p&gt;

&lt;p&gt;I still do not have minimal example to post here. But my code also involves usage of custom case class in Dataset. It works when I spark-submit it or when typed directly in spark-shell. But it fails when run in spark-shell through &lt;a href=&quot;https://github.com/andypetrella/spark-notebook&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;sparkNotebook&lt;/a&gt;. I do not know yet what difference does that make. See &lt;a href=&quot;https://github.com/andypetrella/spark-notebook/issues/807&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;sparkNB issue here&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15849063" author="yetsun" created="Wed, 1 Feb 2017 22:49:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dubovsky&quot; class=&quot;user-hover&quot; rel=&quot;dubovsky&quot;&gt;dubovsky&lt;/a&gt; No, the exception I talked about happened in Spark Notebook. Thanks for letting me know. After reading your comment, I tried in Spark Shell, the case class doesn&apos;t have exception there. It seems an issue in Spark Notebook.&lt;/p&gt;</comment>
                            <comment id="15849732" author="dubovsky" created="Thu, 2 Feb 2017 09:48:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yetsun&quot; class=&quot;user-hover&quot; rel=&quot;yetsun&quot;&gt;yetsun&lt;/a&gt; Good. Do you use the same notebook?&lt;/p&gt;

&lt;p&gt;Spark guys above said that this NoSuchElementException shouldn&apos;t be happening. So while this looks like issue in sparkNB it might be something not good in spark as well. Exception from BlockInfoManager is not expected way how missing class definition (or similar) issue should manifest itself.&lt;/p&gt;</comment>
                            <comment id="15894402" author="dubovsky" created="Fri, 3 Mar 2017 13:58:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; I tried to create a custom spark build with the change you suggested above. But I am unable to install it locally (see below). I asked on spark dev mailing list but nobody really helped. So I try to post it here. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gist.github.com/james64/cc158bdb81bc1828937c757fde94ce82&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;This is a change&lt;/a&gt; I did to spark on v2.1.0 tag. And &lt;a href=&quot;https://gist.github.com/james64/85b3bf4613e7105bebd687502258a518&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this is a build output&lt;/a&gt; I got when tried to run this:&lt;/p&gt;

&lt;p&gt;./build/mvn -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.7.1 clean install&lt;/p&gt;

&lt;p&gt;I believe the profile selection and versions are right because this was successful:&lt;/p&gt;

&lt;p&gt;./dev/make-distribution.sh --name spark-custom-lock --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn -Dhadoop.version=2.6.0-cdh5.7.1&lt;/p&gt;</comment>
                            <comment id="15924043" author="knight76" created="Tue, 14 Mar 2017 11:42:52 +0000"  >&lt;p&gt;I have same problem too. I installed spark-2.1.0-bin-hadoop2.7 on Mac.&lt;/p&gt;

&lt;p&gt;My example code is below, very simple. But sometimes &quot;java.util.NoSuchElementException: None.get&quot; was throwned.&lt;/p&gt;

&lt;p&gt;val nums = 1 to 300000&lt;br/&gt;
val powerfulRdd = sc.parallelize(nums)&lt;br/&gt;
powerfulRdd.filter(_ % 2 == 0).collect()&lt;/p&gt;


&lt;p&gt;Error is below.&lt;br/&gt;
17/03/14 20:33:06 ERROR Executor: Exception in task 3.0 in stage 4.0 (TID 35)&lt;br/&gt;
java.util.NoSuchElementException: None.get&lt;br/&gt;
	at scala.None$.get(Option.scala:347)&lt;br/&gt;
	at scala.None$.get(Option.scala:345)&lt;br/&gt;
	at org.apa&lt;/p&gt;


&lt;p&gt;17/03/14 20:33:06 ERROR Executor: Exception in task 1.0 in stage 4.0 (TID 33)&lt;br/&gt;
java.util.NoSuchElementException: None.get&lt;br/&gt;
	at scala.None$.get(Option.scala:347)&lt;br/&gt;
	at scala.None$.get(Option.scala:345)&lt;br/&gt;
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:670)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:289)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 4.0 failed 1 times, most recent failure: Lost task 7.0 in stage 4.0 (TID 39, localhost, executor driver): java.util.NoSuchElementException: None.get&lt;br/&gt;
	at scala.None$.get(Option.scala:347)&lt;br/&gt;
	at scala.None$.get(Option.scala:345)&lt;br/&gt;
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:670)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:289)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;Driver stacktrace:&lt;br/&gt;
  at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1435)&lt;br/&gt;
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1423)&lt;br/&gt;
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1422)&lt;br/&gt;
  at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&lt;br/&gt;
  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1422)&lt;br/&gt;
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)&lt;br/&gt;
  at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:802)&lt;br/&gt;
  at scala.Option.foreach(Option.scala:257)&lt;br/&gt;
  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:802)&lt;br/&gt;
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1650)&lt;br/&gt;
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1605)&lt;br/&gt;
  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1594)&lt;br/&gt;
  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&lt;br/&gt;
  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:628)&lt;br/&gt;
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1918)&lt;br/&gt;
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1931)&lt;br/&gt;
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1944)&lt;br/&gt;
  at org.apache.spark.SparkContext.runJob(SparkContext.scala:1958)&lt;br/&gt;
  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:917)&lt;br/&gt;
  at org.apache.spark.rdd.RDD$$anonfun$foreach$1.apply(RDD.scala:915)&lt;br/&gt;
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)&lt;br/&gt;
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)&lt;br/&gt;
  at org.apache.spark.rdd.RDD.foreach(RDD.scala:915)&lt;br/&gt;
  ... 52 elided&lt;br/&gt;
Caused by: java.util.NoSuchElementException: None.get&lt;br/&gt;
  at scala.None$.get(Option.scala:347)&lt;br/&gt;
  at scala.None$.get(Option.scala:345)&lt;br/&gt;
  at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)&lt;br/&gt;
  at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:670)&lt;br/&gt;
  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:289)&lt;br/&gt;
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
  at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;</comment>
                            <comment id="15924119" author="srowen" created="Tue, 14 Mar 2017 12:55:29 +0000"  >&lt;p&gt;While I can&apos;t reproduce this, it seems like it&apos;s still and issue and is reproducible in some contexts.&lt;br/&gt;
I&apos;m leaning towards applying the simple change I suggested above because it can make the situation no worse, I think. I don&apos;t believe it would necessarily resolve it though.&lt;/p&gt;</comment>
                            <comment id="15924216" author="apachespark" created="Tue, 14 Mar 2017 13:47:03 +0000"  >&lt;p&gt;User &apos;srowen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/17290&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/17290&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15971081" author="cenyuhai" created="Mon, 17 Apr 2017 14:00:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; I also encounter this problem&lt;/p&gt;</comment>
                            <comment id="15975490" author="iozsaracoglu" created="Wed, 19 Apr 2017 20:49:15 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sowen&quot; class=&quot;user-hover&quot; rel=&quot;sowen&quot;&gt;sowen&lt;/a&gt;, I get this error consistently. I am currently on 2.1 but I had the same experience on 2.0. Error points to &quot;foreach&quot; step.&lt;/p&gt;

&lt;p&gt;This is the case (with collect) that I do NOT experience the problem regardless of my job submit type (Local, YARN-client, or YARN-cluster)&lt;br/&gt;
DFnodeGroup.collect().foreach(r=&amp;gt; {  &lt;br/&gt;
...&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;This is the case (without collect) that I DO experience the problem every time when submitting job to YARN (client or cluster), but not Local &lt;br/&gt;
DFnodeGroup.foreach(r=&amp;gt; {  &lt;br/&gt;
...&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;The difference might be that if tasks are running in the same JVM or not. I tried workarounds including the one suggested by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=naegelejd&quot; class=&quot;user-hover&quot; rel=&quot;naegelejd&quot;&gt;naegelejd&lt;/a&gt; above on Sep 8 with no success.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;

&lt;p&gt;executor 1): java.util.NoSuchElementException: None.get&lt;br/&gt;
	at scala.None$.get(Option.scala:347)&lt;br/&gt;
	at scala.None$.get(Option.scala:345)&lt;br/&gt;
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)&lt;br/&gt;
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:670)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:289)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;</comment>
                            <comment id="16038185" author="rake" created="Tue, 6 Jun 2017 05:00:36 +0000"  >&lt;p&gt;I&apos;ve been trying to resolve this error for past couple of days. Figured I must just be doing something wrong, but after seeing the messages here I now think I&apos;m hitting a bug.&lt;/p&gt;

&lt;p&gt;I&apos;m running a version of 2.2.1-SNAPSHOT from 2-4 weeks ago.&lt;br/&gt;
Not using a Spark Notebook.&lt;br/&gt;
Running in IntelliJ in a Scala console on a MacBook Pro.&lt;br/&gt;
I read in two simple Parquet files into DataFrames.  They are vertices and edges from which I will create a GraphFrame.&lt;br/&gt;
All DataFrame fields are Strings.&lt;br/&gt;
After reading in the &apos;vertices&apos; DataFrame I can call show() and count() on it and it works fine.&lt;br/&gt;
I create a GraphFrame with `val gf = GraphFrame( vertices, edges )`&lt;br/&gt;
I can call `gf.vertices.count()` and `gf..edges.count()` and it works.&lt;br/&gt;
Then I do a `gf.persist()`.&lt;br/&gt;
Then if I do &apos;gf.vertices.count()` (the same statement that just worked!) I get an error.&lt;br/&gt;
```&lt;br/&gt;
org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.util.NoSuchElementException: None.get&lt;br/&gt;
java.util.NoSuchElementException: None.get&lt;br/&gt;
	at scala.None$.get(Option.scala:347)&lt;br/&gt;
	at scala.None$.get(Option.scala:345)&lt;br/&gt;
	at org.apache.spark.sql.execution.DataSourceScanExec$class.org$apache$spark$sql$execution$DataSourceScanExec$$redact(DataSourceScanExec.scala:70)&lt;br/&gt;
	at org.apache.spark.sql.execution.DataSourceScanExec$$anonfun$4.apply(DataSourceScanExec.scala:54)&lt;br/&gt;
	at org.apache.spark.sql.execution.DataSourceScanExec$$anonfun$4.apply(DataSourceScanExec.scala:52)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&lt;br/&gt;
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)&lt;br/&gt;
	at org.apache.spark.sql.execution.DataSourceScanExec$class.simpleString(DataSourceScanExec.scala:52)&lt;br/&gt;
	at org.apache.spark.sql.execution.FileSourceScanExec.simpleString(DataSourceScanExec.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.plans.QueryPlan.verboseString(QueryPlan.scala:349)&lt;br/&gt;
	at org.apache.spark.sql.execution.FileSourceScanExec.org$apache$spark$sql$execution$DataSourceScanExec$$super$verboseString(DataSourceScanExec.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.execution.DataSourceScanExec$class.verboseString(DataSourceScanExec.scala:60)&lt;br/&gt;
	at org.apache.spark.sql.execution.FileSourceScanExec.verboseString(DataSourceScanExec.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:546)&lt;br/&gt;
	at org.apache.spark.sql.execution.WholeStageCodegenExec.generateTreeString(WholeStageCodegenExec.scala:451)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.generateTreeString(TreeNode.scala:558)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.treeString(TreeNode.scala:470)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.treeString(TreeNode.scala:467)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.toString(TreeNode.scala:464)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1421)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)&lt;br/&gt;
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)&lt;br/&gt;
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1509)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1432)&lt;br/&gt;
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)&lt;br/&gt;
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1548)&lt;br/&gt;
	...&lt;br/&gt;
```&lt;br/&gt;
Not seeing it this time, but on previous runs have seen messages that make it sound like a new SparkSession is being created somewhere.&lt;/p&gt;</comment>
                            <comment id="16097070" author="rdub" created="Sat, 22 Jul 2017 02:27:04 +0000"  >&lt;p&gt;I&apos;ve been running into this lately, and I just &lt;a href=&quot;https://github.com/ryan-williams/spark-bugs/tree/msc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;made a minimal repro here&lt;/a&gt;; &lt;a href=&quot;https://github.com/ryan-williams/spark-bugs/blob/msc/src/main/scala/com/foo/Main.scala&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;direct link to source file here&lt;/a&gt;.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I enable &lt;tt&gt;spark.driver.allowMultipleContexts&lt;/tt&gt; (I often enable it in tests)&lt;/li&gt;
	&lt;li&gt;make my app&apos;s &lt;tt&gt;SparkContext&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;cause a second &lt;tt&gt;SparkContext&lt;/tt&gt; to be created by referencing a singleton which creates a &lt;tt&gt;SparkContext&lt;/tt&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;aside: this second &lt;tt&gt;SparkContext&lt;/tt&gt;-creation was accidental in my real code that triggered this bug&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;make a &lt;tt&gt;Broadcast&lt;/tt&gt; and &lt;tt&gt;RDD&lt;/tt&gt; with the first &lt;tt&gt;SparkContext&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;map&lt;/tt&gt; over that &lt;tt&gt;RDD&lt;/tt&gt; referencing the &lt;tt&gt;Broadcast&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;when this job is run, I get fatal BlockManager errors:&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.apache.spark.SparkException: Failed to get broadcast_0_piece0 of broadcast_0
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:178)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:150)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:150)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:218)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1269)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;17/07/22 02:08:11 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:670)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:289)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I haven&apos;t really tried to track down what interaction with multiple &lt;tt&gt;SparkContext&lt;/tt&gt;&apos;s might be broken, but I suppose something in the block-management plumbing is holding on to a reference to the last-created &lt;tt&gt;SparkContext&lt;/tt&gt;, or something like that?&lt;/p&gt;</comment>
                            <comment id="16097302" author="rdub" created="Sat, 22 Jul 2017 12:36:03 +0000"  >&lt;p&gt;Update: I think the real story here is the first stack trace above: &lt;tt&gt;Failed to get broadcast_0_piece0 of broadcast_0&lt;/tt&gt;; in my real application, I see that error causing jobs to fail without the &lt;tt&gt;None.get&lt;/tt&gt; occurring.&lt;/p&gt;

&lt;p&gt;Curiously, setting &lt;tt&gt;log4j.logger.org.apache.spark=DEBUG&lt;/tt&gt; causes the error to go away in my real application; setting &lt;tt&gt;log4j.logger.org.apache.spark=DEBUG&lt;/tt&gt; in my repro causes the &lt;tt&gt;None.get&lt;/tt&gt; to go away, but the job still fails due to &lt;tt&gt;Failed to get broadcast_0_piece0 of broadcast_0&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gist.github.com/ryan-williams/564069ba1bd2c052c64be68f7d86c0c5&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Here is full output of such a repro run with debug logging enabled&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The error seems sensitive to the order of creation of:&lt;/p&gt;

&lt;p&gt;1. &lt;tt&gt;Broadcast&lt;/tt&gt; from first &lt;tt&gt;SparkContext&lt;/tt&gt;&lt;br/&gt;
2. Second &lt;tt&gt;SparkContext&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;The bug occurs when the &lt;tt&gt;Broadcast&lt;/tt&gt; comes first:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-comment&quot;&gt;// Make a Broadcast
&lt;/span&gt;    val bs = sc.broadcast(Set(1, 2, 3))

    &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-quote&quot;&gt;&quot;Accidentally&quot;&lt;/span&gt; create second SparkContext
&lt;/span&gt;    println(Foo.foo)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;but not when the order is reversed:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-quote&quot;&gt;&quot;Accidentally&quot;&lt;/span&gt; create second SparkContext
&lt;/span&gt;    println(Foo.foo)

    &lt;span class=&quot;code-comment&quot;&gt;// Make a Broadcast
&lt;/span&gt;    val bs = sc.broadcast(Set(1, 2, 3))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This also raises a question about whether everyone in this thread is seeing the same issue/exception, since my &lt;tt&gt;None.get&lt;/tt&gt; seems to be caused by some other issue regarding block-management in the presence of multiple &lt;tt&gt;SparkContext&lt;/tt&gt;&apos;s.&lt;/p&gt;</comment>
                            <comment id="16097318" author="rdub" created="Sat, 22 Jul 2017 13:22:27 +0000"  >&lt;p&gt;Seems like the second &lt;tt&gt;SparkContext&lt;/tt&gt;&apos;s &lt;tt&gt;BlockManager&lt;/tt&gt;/&lt;tt&gt;NettyBlockTransferService&lt;/tt&gt; starts at the same address as the first&apos;s, intercepts subsequent block requests, and doesn&apos;t know about blocks that were created with the first &lt;tt&gt;SparkContext&lt;/tt&gt;/&lt;tt&gt;BlockManager&lt;/tt&gt;. &amp;lt;edit&amp;gt; this doesn&apos;t explain why switching to &lt;tt&gt;DEBUG&lt;/tt&gt; logging made the issue go away in my application though; hmm&#8230; &amp;lt;/edit&amp;gt;&lt;/p&gt;

&lt;p&gt;If anyone else who saw this issue wants to chime in about whether they may have &#8211; or definitely did not have &#8211; multiple concurrently-active &lt;tt&gt;SparkContext&lt;/tt&gt;&apos;s in the same JVM when they saw the issue, that might be useful!&lt;/p&gt;
</comment>
                            <comment id="16207204" author="aphasingnirvana" created="Tue, 17 Oct 2017 09:07:35 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; I have observed this happening with my code, but one common observation was that, it always occurred with the last job which failed every single time, if that could help you understand the cause of the bug.&lt;/p&gt;</comment>
                            <comment id="16208925" author="aphasingnirvana" created="Wed, 18 Oct 2017 07:26:51 +0000"  >&lt;p&gt;More on the error here from the particular node where the exception occurred:&lt;/p&gt;

&lt;p&gt;It fails to get a particular broadcast piece:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
17/10/18 00:09:48 INFO Executor: Running task 216.0 in stage 715.0 (TID 1645)
17/10/18 00:09:48 INFO TorrentBroadcast: Started reading broadcast variable 554
17/10/18 00:09:48 INFO log: Logging initialized @532075ms
17/10/18 00:09:48 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_554_piece0 of broadcast_554
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:186)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:67)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 ERROR Executor: Exception in task 216.0 in stage 715.0 (TID 1645)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 INFO CoarseGrainedExecutorBackend: Got assigned task 1656
17/10/18 00:09:48 INFO Executor: Running task 109.0 in stage 715.0 (TID 1656)
17/10/18 00:09:48 INFO TorrentBroadcast: Started reading broadcast variable 554
17/10/18 00:09:48 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_554_piece0 of broadcast_554
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:186)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:67)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 ERROR Executor: Exception in task 109.0 in stage 715.0 (TID 1656)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 INFO CoarseGrainedExecutorBackend: Got assigned task 1663
17/10/18 00:09:48 INFO Executor: Running task 216.1 in stage 715.0 (TID 1663)
17/10/18 00:09:48 INFO TorrentBroadcast: Started reading broadcast variable 554
17/10/18 00:09:48 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_554_piece0 of broadcast_554
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:186)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:67)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 ERROR Executor: Exception in task 216.1 in stage 715.0 (TID 1663)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 INFO CoarseGrainedExecutorBackend: Got assigned task 1668
17/10/18 00:09:48 INFO Executor: Running task 105.1 in stage 715.0 (TID 1668)
17/10/18 00:09:48 INFO TorrentBroadcast: Started reading broadcast variable 554
17/10/18 00:09:48 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_554_piece0 of broadcast_554
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:186)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:67)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 ERROR Executor: Exception in task 105.1 in stage 715.0 (TID 1668)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 INFO CoarseGrainedExecutorBackend: Got assigned task 1676
17/10/18 00:09:48 INFO Executor: Running task 216.2 in stage 715.0 (TID 1676)
17/10/18 00:09:48 INFO TorrentBroadcast: Started reading broadcast variable 554
17/10/18 00:09:48 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_554_piece0 of broadcast_554
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:186)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:67)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 ERROR Executor: Exception in task 216.2 in stage 715.0 (TID 1676)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 INFO CoarseGrainedExecutorBackend: Got assigned task 1686
17/10/18 00:09:48 INFO Executor: Running task 120.0 in stage 715.0 (TID 1686)
17/10/18 00:09:48 INFO TorrentBroadcast: Started reading broadcast variable 554
17/10/18 00:09:48 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_554_piece0 of broadcast_554
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:186)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:67)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 ERROR Executor: Exception in task 120.0 in stage 715.0 (TID 1686)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 INFO CoarseGrainedExecutorBackend: Got assigned task 1697
17/10/18 00:09:48 INFO Server: jetty-9.2.z-SNAPSHOT
17/10/18 00:09:48 INFO Executor: Running task 216.3 in stage 715.0 (TID 1697)
17/10/18 00:09:48 INFO TorrentBroadcast: Started reading broadcast variable 554
17/10/18 00:09:48 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_554_piece0 of broadcast_554
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:186)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:67)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 ERROR Executor: Exception in task 216.3 in stage 715.0 (TID 1697)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 INFO CoarseGrainedExecutorBackend: Got assigned task 1705
17/10/18 00:09:48 INFO Executor: Running task 134.0 in stage 715.0 (TID 1705)
17/10/18 00:09:48 INFO TorrentBroadcast: Started reading broadcast variable 554
17/10/18 00:09:48 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_554_piece0 of broadcast_554
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:186)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:67)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 ERROR Executor: Exception in task 134.0 in stage 715.0 (TID 1705)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 INFO CoarseGrainedExecutorBackend: Got assigned task 1710
17/10/18 00:09:48 INFO Executor: Running task 126.1 in stage 715.0 (TID 1710)
17/10/18 00:09:48 INFO TorrentBroadcast: Started reading broadcast variable 554
17/10/18 00:09:48 ERROR Utils: Exception encountered
org.apache.spark.SparkException: Failed to get broadcast_554_piece0 of broadcast_554
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp(TorrentBroadcast.scala:146)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply(TorrentBroadcast.scala:125)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks(TorrentBroadcast.scala:125)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:186)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1273)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:174)
	at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:65)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:89)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:67)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/10/18 00:09:48 ERROR Executor: Exception in task 126.1 in stage 715.0 (TID 1710)
java.util.NoSuchElementException: None.get
	at scala.None$.get(Option.scala:347)
	at scala.None$.get(Option.scala:345)
	at org.apache.spark.storage.BlockInfoManager.releaseAllLocksForTask(BlockInfoManager.scala:343)
	at org.apache.spark.storage.BlockManager.releaseAllLocksForTask(BlockManager.scala:646)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:281)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="16216862" author="aphasingnirvana" created="Tue, 24 Oct 2017 13:06:02 +0000"  >&lt;p&gt;I&apos;ve been temporarily able to avoid the error using a local SparkSession declared in the function rather than declaring it globally accessible to all the functions in my object/class. If required in other functions, passed it as a parameter to the particular function. Hope it helps to resolve the bug.&lt;/p&gt;

&lt;p&gt;Cheers!&lt;/p&gt;</comment>
                            <comment id="16284163" author="terrasect" created="Fri, 8 Dec 2017 20:31:14 +0000"  >&lt;p&gt;Ran into this while unit testing and can confirm it&apos;s related to multiple SparkContexts. (Was trying to have one in the test and one in the app.) The solution I used is to have a single SparkContext in the test and pass it into the app.&lt;/p&gt;</comment>
                            <comment id="16284176" author="srowen" created="Fri, 8 Dec 2017 20:45:00 +0000"  >&lt;p&gt;OK. I added a minor improvement to the error handling, and haven&apos;t had consistent reproductions otherwise. If it&apos;s likely due to multiple SparkContexts, I&apos;d declare it not an issue, as there aren&apos;t supposed to be multiple contexts.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 49 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i314af:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>