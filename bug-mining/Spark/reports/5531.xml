<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:58:36 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-22036] BigDecimal multiplication sometimes returns null</title>
                <link>https://issues.apache.org/jira/browse/SPARK-22036</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;The multiplication of two BigDecimal numbers sometimes returns null. Here is a minimal reproduction:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;object Main &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; App {
  &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.{SparkConf, SparkContext}
  &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.SparkSession
  &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; spark.implicits._
  val conf = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf().setMaster(&lt;span class=&quot;code-quote&quot;&gt;&quot;local[*]&quot;&lt;/span&gt;).setAppName(&lt;span class=&quot;code-quote&quot;&gt;&quot;REPL&quot;&lt;/span&gt;).set(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.ui.enabled&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&quot;&lt;/span&gt;)
  val spark = SparkSession.builder().config(conf).appName(&lt;span class=&quot;code-quote&quot;&gt;&quot;REPL&quot;&lt;/span&gt;).getOrCreate()
  implicit val sqlContext = spark.sqlContext

  &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;X2(a: BigDecimal, b: BigDecimal)
  val ds = sqlContext.createDataset(List(X2(BigDecimal(-0.1267333984375), BigDecimal(-1000.1))))
  val result = ds.select(ds(&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;) * ds(&lt;span class=&quot;code-quote&quot;&gt;&quot;b&quot;&lt;/span&gt;)).collect.head
  println(result) &lt;span class=&quot;code-comment&quot;&gt;// [&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;]
&lt;/span&gt;}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13102759">SPARK-22036</key>
            <summary>BigDecimal multiplication sometimes returns null</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mgaido">Marco Gaido</assignee>
                                    <reporter username="OlivierBlanvillain">Olivier Blanvillain</reporter>
                        <labels>
                    </labels>
                <created>Sat, 16 Sep 2017 11:12:06 +0000</created>
                <updated>Thu, 20 Sep 2018 23:40:29 +0000</updated>
                            <resolved>Thu, 18 Jan 2018 13:30:38 +0000</resolved>
                                    <version>2.2.0</version>
                                    <fixVersion>2.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>13</watches>
                                                                                                                <comments>
                            <comment id="16168969" author="mgaido" created="Sat, 16 Sep 2017 16:39:32 +0000"  >&lt;p&gt;This happens because there is an overflow in the operation. I am not sure of what should be done in this case. The current implementation returns null when an operation cause a loss of precision.&lt;/p&gt;</comment>
                            <comment id="16168989" author="olivierblanvillain" created="Sat, 16 Sep 2017 17:19:02 +0000"  >&lt;p&gt;It&apos;s surprising because in this case the resulting value seems to fit within the range of representable values:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; val result = BigDecimal(-0.1267333984375) * BigDecimal(-1000.1)
result: scala.math.BigDecimal = 126.74607177734375

scala&amp;gt; sqlContenxt.createDataset(List(result)).head == result
res10: &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt; = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also Spark will silently loses BigDecimal precision in other circumstances:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; val tooPrecise = BigDecimal(&lt;span class=&quot;code-quote&quot;&gt;&quot;126.74607177734375111111111&quot;&lt;/span&gt;)
tooPrecise: scala.math.BigDecimal = 126.74607177734375111111111

scala&amp;gt; val ds = sqlContenxt.createDataset(List(tooPrecise))
ds: org.apache.spark.sql.Dataset[scala.math.BigDecimal] = [value: decimal(38,18)]

scala&amp;gt; ds.head
res14: scala.math.BigDecimal = 126.746071777343751111

scala&amp;gt; ds.select(ds(&lt;span class=&quot;code-quote&quot;&gt;&quot;value&quot;&lt;/span&gt;) * BigDecimal(1)).head
res15: org.apache.spark.sql.Row = [126.746071777343751111]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&amp;gt; I am not sure of what should be done in this case&lt;/p&gt;

&lt;p&gt;Given that Sparks&apos; BigDecimal have bounded precision I would consider following what is done for other numeric representations and return the closest representable value in case of overflow.&lt;/p&gt;</comment>
                            <comment id="16168994" author="olivierblanvillain" created="Sat, 16 Sep 2017 17:36:04 +0000"  >&lt;p&gt;This seems to be a multiplication only thing, for instance Spark silently loses precision with addition:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; val a = BigDecimal(&lt;span class=&quot;code-quote&quot;&gt;&quot;43.65&quot;&lt;/span&gt;)
a: scala.math.BigDecimal = 43.65

scala&amp;gt; val b = BigDecimal(&lt;span class=&quot;code-quote&quot;&gt;&quot;61.11&quot;&lt;/span&gt;)
b: scala.math.BigDecimal = 61.11

scala&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;X2(a: BigDecimal, b: BigDecimal)
defined &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;X2

scala&amp;gt; val ds = sqlContenxt.createDataset(List(X2(a, b)))
ds: org.apache.spark.sql.Dataset[X2] = [a: decimal(38,18), b: decimal(38,18)]

scala&amp;gt; val res = ds.select(ds(&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;) + ds(&lt;span class=&quot;code-quote&quot;&gt;&quot;b&quot;&lt;/span&gt;)).head.get(0).asInstanceOf[java.math.BigDecimal]
res: java.math.BigDecimal = 104.760000000000000000

scala&amp;gt; res.subtract((a + b).underlying)
res3: java.math.BigDecimal = 0E-18
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16169023" author="mgaido" created="Sat, 16 Sep 2017 19:08:44 +0000"  >&lt;p&gt;Yes, it is only for multiplications. The reason is that for the multiplication it expects the result to have a scale which is the sum of the two scales of the operands. When there is an overflow in the result of the operations, the result is rounded up and the scale is one less than the expected. In this situation, the result is set to null.&lt;/p&gt;</comment>
                            <comment id="16169050" author="olivierblanvillain" created="Sat, 16 Sep 2017 20:45:07 +0000"  >&lt;p&gt;I understand. Is this working as intended / fulfils a specification or is it just an artifact of the current implementation?&lt;/p&gt;</comment>
                            <comment id="16169059" author="mgaido" created="Sat, 16 Sep 2017 20:59:11 +0000"  >&lt;p&gt;Honestly I don&apos;t know, that is why I said that I don&apos;t know what should be done.&lt;/p&gt;</comment>
                            <comment id="16169074" author="mgaido" created="Sat, 16 Sep 2017 22:14:33 +0000"  >&lt;p&gt;Maybe the &quot;bad&quot; part is that by default spark creates the columns as &lt;tt&gt;Decimal(38, 18)&lt;/tt&gt;. This is the problem. With a multiplication this leads to a &lt;tt&gt;Decimal(38, 36)&lt;/tt&gt;, which as you can easily understand is the root of the problem of your operation. If you cast the two columns before the multiplication, like &lt;tt&gt;ds(&quot;a&quot;).cast(DecimalType(20,14))&lt;/tt&gt;, you won&apos;t have any problem anymore.&lt;br/&gt;
Currently you should suggest Spark which are the right values to use.&lt;/p&gt;</comment>
                            <comment id="16169129" author="olivierblanvillain" created="Sat, 16 Sep 2017 23:05:38 +0000"  >&lt;p&gt;Adding a cast indeed prevents getting null values. However this solution is less than satisfactory as the resulting multiplications are &lt;em&gt;less&lt;/em&gt; precise on &lt;tt&gt;BigDecimal&lt;/tt&gt; than on &lt;tt&gt;Double&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Here is an example. We compute the product of two numbers: &lt;tt&gt;0.0000199735164642333984375&lt;/tt&gt; and &lt;tt&gt;-0.000010430812835693359375&lt;/tt&gt;. Below are the result of this multiplication by the JVM as &lt;tt&gt;java.lang.Double&lt;/tt&gt; (double), by Spark using &lt;tt&gt;.cast(DecimalType(20,14))&lt;/tt&gt; (casted), and finally using unlimited precision arithmetic (actual):&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;double: -2.083400119090584E-10&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;casted: -2.083400119509193464E-10&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;actual: -2.083400119090583757497370243072509765625E-10&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="16287025" author="taklwu" created="Tue, 12 Dec 2017 03:02:01 +0000"  >&lt;p&gt;&lt;ins&gt;1, we met the similar issue when multiplying 3&lt;/ins&gt; BigDecimal, although we have a workaound to cast them into a smaller number, it&apos;s not easy to tell when it will fail especially we have more complicated query.&lt;/p&gt;</comment>
                            <comment id="16291805" author="annunarcist" created="Thu, 14 Dec 2017 23:49:49 +0000"  >&lt;p&gt;+1 Issue reproduced on spark-2.2.0 : &lt;/p&gt;

&lt;p&gt;Data at s3 location - s3://bucket/spark-sql-jira/ :&lt;br/&gt;
---------------------------------------------&lt;br/&gt;
100|99999&lt;/p&gt;

&lt;p&gt;drop table if exists test;&lt;br/&gt;
CREATE EXTERNAL TABLE `test` (&lt;br/&gt;
a    decimal(38,10),&lt;br/&gt;
b    decimal(38,10)&lt;br/&gt;
)&lt;br/&gt;
ROW FORMAT DELIMITED FIELDS TERMINATED BY &apos;|&apos;&lt;br/&gt;
STORED AS TEXTFILE&lt;br/&gt;
LOCATION &apos;s3://bucket/spark-sql-jira/&apos;;&lt;/p&gt;

&lt;p&gt;spark-sql&amp;gt; select a,(a*b*0.98765432100) from test;&lt;br/&gt;
100	9876444.4445679&lt;br/&gt;
Time taken: 11.033 seconds, Fetched 1 row(s)&lt;/p&gt;

&lt;p&gt;spark-sql&amp;gt; select a,(a*b*0.987654321000) from test;&lt;br/&gt;
100	NULL&lt;br/&gt;
Time taken: 0.523 seconds, Fetched 1 row(s)&lt;/p&gt;

&lt;p&gt;Changing a column&apos;s scale from decimal(38,10) to decimal(38,9) also helped but we would loose precision. &lt;/p&gt;</comment>
                            <comment id="16297056" author="apachespark" created="Tue, 19 Dec 2017 16:42:04 +0000"  >&lt;p&gt;User &apos;mgaido91&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20023&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20023&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16330506" author="cloud_fan" created="Thu, 18 Jan 2018 13:30:38 +0000"  >&lt;p&gt;Issue resolved by pull request 20023&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20023&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20023&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16351366" author="apachespark" created="Sat, 3 Feb 2018 11:24:03 +0000"  >&lt;p&gt;User &apos;wangyum&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20498&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20498&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16618104" author="bersprockets" created="Mon, 17 Sep 2018 20:18:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mgaido&quot; class=&quot;user-hover&quot; rel=&quot;mgaido&quot;&gt;mgaido&lt;/a&gt; In this change, you modified how&#160;precision and scale are&#160;determined when&#160;literals are promoted to decimal. For example, before the change, an integer literal&apos;s precision and scale would be hardcoded to DecimalType(10, 0). After the change, it&apos;s based on the number of digits in the literal.&lt;/p&gt;

&lt;p&gt;However, that new behavior for literals is not toggled by &lt;tt&gt;spark.sql.decimalOperations.allowPrecisionLoss&lt;/tt&gt; like the other changes in behavior introduced by the PR.&lt;/p&gt;

&lt;p&gt;As a result, there are cases where we see truncation and rounding in 2.3/2.4 that we don&apos;t see in 2.2, and this change in behavior is not controllable via the configuration setting. E.g,:&lt;/p&gt;

&lt;p&gt;In 2.2:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;scala&amp;gt; sql(&quot;select 26393499451/(1e6 * 1000) as c1&quot;).printSchema
root
 |-- c1: decimal(27,13) (nullable = true) &amp;lt;== 13 decimal digits
scala&amp;gt; sql(&quot;select 26393499451/(1e6 * 1000) as c1&quot;).show
+----------------+
|              c1|
+----------------+
|26.3934994510000|
+----------------+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In 2.3 and up:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;scala&amp;gt; sql(&quot;set spark.sql.decimalOperations.allowPrecisionLoss&quot;).show
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.decimal...| true|
+--------------------+-----+
scala&amp;gt; sql(&quot;select 26393499451/(1e6 * 1000) as c1&quot;).printSchema
root
 |-- c1: decimal(12,7) (nullable = true)
scala&amp;gt; sql(&quot;select 26393499451/(1e6 * 1000) as c1&quot;).show
+----------+
|        c1|
+----------+
|26.3934995| &amp;lt;== result is truncated and rounded up.
+----------+
scala&amp;gt; sql(&quot;set spark.sql.decimalOperations.allowPrecisionLoss=false&quot;).show
+--------------------+-----+
|                 key|value|
+--------------------+-----+
|spark.sql.decimal...|false|
+--------------------+-----+
scala&amp;gt; sql(&quot;select 26393499451/(1e6 * 1000) as c1&quot;).printSchema
root
 |-- c1: decimal(12,7) (nullable = true)
scala&amp;gt; sql(&quot;select 26393499451/(1e6 * 1000) as c1&quot;).show
+----------+
|        c1|
+----------+
|26.3934995| &amp;lt;== result is still truncated and rounded up.
+----------+
scala&amp;gt; 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I can force it to behave the old way, at least for this case, by explicitly casting the literal:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;scala&amp;gt; sql(&quot;select 26393499451/(1e6 * cast(1000 as decimal(10, 0))) as c1&quot;).show
+----------------+
|              c1|
+----------------+
|26.3934994510000|
+----------------+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Do you think it makes sense for &lt;tt&gt;spark.sql.decimalOperations.allowPrecisionLoss&lt;/tt&gt; to also toggle how literal promotion happens (the old way vs.&#160;the new way)?&lt;/p&gt;</comment>
                            <comment id="16618892" author="mgaido" created="Tue, 18 Sep 2018 10:34:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bersprockets&quot; class=&quot;user-hover&quot; rel=&quot;bersprockets&quot;&gt;bersprockets&lt;/a&gt; first of all thank you for reporting this and sorry for my mistake on this.&lt;/p&gt;

&lt;p&gt;I think the solution you are suggesting isn&apos;t the right one. Also the result in the case allowPrecisionLoss=true should not have any truncation here. The problem is the way we handle negative scale. So this issue I think is related to &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-24468&quot; title=&quot;DecimalType `adjustPrecisionScale` might fail when scale is negative&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-24468&quot;&gt;&lt;del&gt;SPARK-24468&lt;/del&gt;&lt;/a&gt;. The problem is that Hive and MSSQL we are taking our rules from are not allowing negative scale, while we do. So this has to be revisited. May you please submit a new JIRA for this? Meanwhile I am starting working on it and I&apos;ll submit a fix ASAP. Sorry for the trouble. Thanks.&lt;/p&gt;</comment>
                            <comment id="16619129" author="mgaido" created="Tue, 18 Sep 2018 13:45:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bersprockets&quot; class=&quot;user-hover&quot; rel=&quot;bersprockets&quot;&gt;bersprockets&lt;/a&gt; I created &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25454&quot; title=&quot;Division between operands with negative scale can cause precision loss&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25454&quot;&gt;&lt;del&gt;SPARK-25454&lt;/del&gt;&lt;/a&gt; for tracking since I have a path for this and it might be considered as a blocker for 2.4, so I wanted to expedite it. I am submitting a patch for this soon. Sorry for the problem again. Thanks.&lt;/p&gt;</comment>
                            <comment id="16622877" author="apachespark" created="Thu, 20 Sep 2018 23:40:29 +0000"  >&lt;p&gt;User &apos;cloud-fan&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/22494&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22494&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13167160">SPARK-24606</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13024866">HIVE-15331</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13132731">SPARK-23179</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 8 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3k5kn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>