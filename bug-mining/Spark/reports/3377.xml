<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:41:20 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-14699] Driver is marked as failed even it runs successfully</title>
                <link>https://issues.apache.org/jira/browse/SPARK-14699</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;We recently upgraded Spark from 1.5.2 to 1.6.0 and found that all batch jobs are marked as failed.&lt;br/&gt;
To address this issue, we wrote a simple test application which just sum up from 1 to 10000 and it is marked as failed even though its result was correct.&lt;br/&gt;
Here is the typical stderr message and there is &quot;ERROR worker.WorkerWatcher: Lost connection to worker rpc&quot; when driver exits.&lt;/p&gt;

&lt;p&gt;16/04/14 06:20:41 INFO scheduler.DAGScheduler: ResultStage 1 (sum at SparkBatchTest.scala:19) finished in 0.052 s&lt;br/&gt;
16/04/14 06:20:41 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool&lt;br/&gt;
16/04/14 06:20:41 INFO scheduler.DAGScheduler: Job 1 finished: sum at SparkBatchTest.scala:19, took 0.061177 s&lt;br/&gt;
16/04/14 06:20:41 ERROR worker.WorkerWatcher: Lost connection to worker rpc endpoint spark://Worker@spark-worker-ltv-prod-006.prod.vungle.com:7078. Exiting.&lt;br/&gt;
16/04/14 06:20:41 INFO spark.SparkContext: Invoking stop() from shutdown hook&lt;br/&gt;
16/04/14 06:20:41 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 172.16.33.187:36442 in memory (size: 1452.0 B, free: 511.1 MB)&lt;br/&gt;
16/04/14 06:20:41 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on ip-172-16-31-86.ec2.internal:29708 in memory (size: 1452.0 B, free: 511.1 MB)&lt;br/&gt;
16/04/14 06:20:41 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on ip-172-16-32-207.ec2.internal:21259 in memory (size: 1452.0 B, free: 511.1 MB)&lt;br/&gt;
16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/metrics/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/stages/stage/kill,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/api,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/static,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/executors/threadDump/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/executors/threadDump,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/executors/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/executors,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/environment/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/environment,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/storage/rdd/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/storage/rdd,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/storage/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/storage,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/stages/pool/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/stages/pool,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/stages/stage/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/stages/stage,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/stages/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/stages,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/jobs/job/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/jobs/job,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/jobs/json,null}
&lt;p&gt;16/04/14 06:20:41 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler&lt;/p&gt;
{/jobs,null}
&lt;p&gt;16/04/14 06:20:41 INFO spark.ContextCleaner: Cleaned accumulator 2&lt;br/&gt;
16/04/14 06:20:41 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 172.16.33.187:36442 in memory (size: 804.0 B, free: 511.1 MB)&lt;br/&gt;
16/04/14 06:20:41 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-16-31-86.ec2.internal:29708 in memory (size: 804.0 B, free: 511.1 MB)&lt;br/&gt;
16/04/14 06:20:41 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on ip-172-16-32-207.ec2.internal:21259 in memory (size: 804.0 B, free: 511.1 MB)&lt;br/&gt;
16/04/14 06:20:41 INFO spark.ContextCleaner: Cleaned accumulator 1&lt;br/&gt;
16/04/14 06:20:41 INFO ui.SparkUI: Stopped Spark web UI at &lt;a href=&quot;http://172.16.31.56:4040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://172.16.31.56:4040&lt;/a&gt;&lt;br/&gt;
16/04/14 06:20:41 INFO cluster.SparkDeploySchedulerBackend: Shutting down all executors&lt;br/&gt;
16/04/14 06:20:41 INFO cluster.SparkDeploySchedulerBackend: Asking each executor to shut down&lt;br/&gt;
16/04/14 06:20:41 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!&lt;br/&gt;
16/04/14 06:20:41 INFO storage.MemoryStore: MemoryStore cleared&lt;br/&gt;
16/04/14 06:20:41 INFO storage.BlockManager: BlockManager stopped&lt;br/&gt;
16/04/14 06:20:41 INFO storage.BlockManagerMaster: BlockManagerMaster stopped&lt;br/&gt;
16/04/14 06:20:41 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!&lt;br/&gt;
16/04/14 06:20:41 INFO spark.SparkContext: Successfully stopped SparkContext&lt;br/&gt;
16/04/14 06:20:41 INFO util.ShutdownHookManager: Shutdown hook called&lt;br/&gt;
16/04/14 06:20:41 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-a4c7f9ac-bf40-4924-b977-ec0be4c5fe93&lt;/p&gt;

&lt;p&gt;Since it looks like something related with rpc, we tested with Akka which was default before 1.6.0 by setting &quot;spark.rpc=akka&quot;.&lt;br/&gt;
Then the error message dismisses and a warning complains the same thing about disassociation with worker.&lt;br/&gt;
It seems with Akka it waits another 5 seconds and that makes driver exists gracefully.&lt;br/&gt;
16/04/12 15:41:28 INFO DAGScheduler: ResultStage 1 (sum at SparkBatchTest.scala:19) finished in 0.053 s&lt;br/&gt;
16/04/12 15:41:28 INFO DAGScheduler: Job 1 finished: sum at SparkBatchTest.scala:19, took 0.060660 s&lt;br/&gt;
16/04/12 15:41:28 INFO SparkContext: Invoking stop() from shutdown hook&lt;br/&gt;
16/04/12 15:41:28 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.&lt;br/&gt;
16/04/12 15:41:28 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.&lt;br/&gt;
16/04/12 15:41:28 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.&lt;br/&gt;
16/04/12 15:41:28 INFO SparkUI: Stopped Spark web UI at &lt;a href=&quot;http://sparkbox:8830&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://sparkbox:8830&lt;/a&gt;&lt;br/&gt;
16/04/12 15:41:28 INFO SparkDeploySchedulerBackend: Shutting down all executors&lt;br/&gt;
16/04/12 15:41:28 INFO SparkDeploySchedulerBackend: Asking each executor to shut down&lt;br/&gt;
16/04/12 15:41:28 WARN ReliableDeliverySupervisor: Association with remote system &lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://sparkExecutor@sparkbox:58832&amp;#93;&lt;/span&gt; has failed, address is now gated for &lt;span class=&quot;error&quot;&gt;&amp;#91;5000&amp;#93;&lt;/span&gt; ms. Reason: &lt;span class=&quot;error&quot;&gt;&amp;#91;Disassociated&amp;#93;&lt;/span&gt;&lt;br/&gt;
16/04/12 15:41:28 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!&lt;br/&gt;
16/04/12 15:41:28 INFO MemoryStore: MemoryStore cleared&lt;br/&gt;
16/04/12 15:41:28 INFO BlockManager: BlockManager stopped&lt;br/&gt;
16/04/12 15:41:28 INFO BlockManagerMaster: BlockManagerMaster stopped&lt;br/&gt;
16/04/12 15:41:28 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!&lt;br/&gt;
16/04/12 15:41:28 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.&lt;br/&gt;
16/04/12 15:41:28 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.&lt;br/&gt;
16/04/12 15:41:28 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.&lt;br/&gt;
16/04/12 15:41:28 INFO SparkContext: Successfully stopped SparkContext&lt;br/&gt;
16/04/12 15:41:28 INFO ShutdownHookManager: Shutdown hook called&lt;br/&gt;
16/04/12 15:41:28 INFO ShutdownHookManager: Deleting directory /tmp/spark-138182c2-f1bc-4ea1-b5b8-2adceb27c083&lt;/p&gt;

&lt;p&gt;This only occurs with standalone deployment since it is said DriverWrapper is only used in that cluster mode.&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala#L43&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/DriverWrapper.scala#L43&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Typically it affects the &apos;--supervise&apos; usage for batch jobs, since they will be always considered as failed and get restarted again and again.&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala#L214&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/DriverRunner.scala#L214&lt;/a&gt;&lt;/p&gt;</description>
                <environment>&lt;p&gt;Standalone deployment&lt;/p&gt;</environment>
        <key id="12959644">SPARK-14699</key>
            <summary>Driver is marked as failed even it runs successfully</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zsxwing">Shixiong Zhu</assignee>
                                    <reporter username="liutzvin">Huiqiang Liu</reporter>
                        <labels>
                    </labels>
                <created>Mon, 18 Apr 2016 13:46:09 +0000</created>
                <updated>Wed, 22 Mar 2017 08:54:01 +0000</updated>
                            <resolved>Thu, 21 Apr 2016 19:04:29 +0000</resolved>
                                    <version>1.6.0</version>
                    <version>1.6.1</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="15246609" author="apachespark" created="Mon, 18 Apr 2016 21:41:03 +0000"  >&lt;p&gt;User &apos;zsxwing&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/12481&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/12481&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15382470" author="bruckner" created="Mon, 18 Jul 2016 15:37:40 +0000"  >&lt;p&gt;Any plans to backport to 1.6?  Was resolved in April but fix isn&apos;t in 1.6.2&lt;/p&gt;</comment>
                            <comment id="15935954" author="rohpandi" created="Wed, 22 Mar 2017 08:54:01 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;We plan to use DSE Cassandra 5.0.5 which uses Apache spark 1.6.3.1 and need this fix. Is there any plan to backport it to 1.6.x?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://docs.datastax.com/en/latest-dse/datastax_enterprise/RNdse.html#RNdse__505Comp&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://docs.datastax.com/en/latest-dse/datastax_enterprise/RNdse.html#RNdse__505Comp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;thanks,&lt;br/&gt;
Rohan.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 34 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2w9dr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>