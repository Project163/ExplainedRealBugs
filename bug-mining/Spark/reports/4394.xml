<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:50:10 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-18281] toLocalIterator yields time out error on pyspark2</title>
                <link>https://issues.apache.org/jira/browse/SPARK-18281</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I run the example straight out of the api docs for toLocalIterator and it gives a time out exception:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;from pyspark &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkContext
sc = SparkContext()
rdd = sc.parallelize(range(10))
[x &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; x in rdd.toLocalIterator()]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;conf file:&lt;br/&gt;
spark.driver.maxResultSize 6G&lt;br/&gt;
spark.executor.extraJavaOptions -XX:+UseG1GC -XX:MaxPermSize=1G -XX:+HeapDumpOnOutOfMemoryError&lt;br/&gt;
spark.executor.memory   16G&lt;br/&gt;
spark.executor.uri  foo/spark-2.0.1-bin-hadoop2.7.tgz&lt;br/&gt;
spark.hadoop.fs.s3a.impl org.apache.hadoop.fs.s3a.S3AFileSystem&lt;br/&gt;
spark.hadoop.fs.s3a.buffer.dir  /raid0/spark&lt;br/&gt;
spark.hadoop.fs.s3n.buffer.dir  /raid0/spark&lt;br/&gt;
spark.hadoop.fs.s3a.connection.timeout 500000&lt;br/&gt;
spark.hadoop.fs.s3n.multipart.uploads.enabled   true&lt;br/&gt;
spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version 2&lt;br/&gt;
spark.hadoop.parquet.block.size 2147483648&lt;br/&gt;
spark.hadoop.parquet.enable.summary-metadata    false&lt;br/&gt;
spark.jars.packages com.databricks:spark-avro_2.11:3.0.1,com.amazonaws:aws-java-sdk-pom:1.10.34&lt;br/&gt;
spark.local.dir /raid0/spark&lt;br/&gt;
spark.mesos.coarse  false&lt;br/&gt;
spark.mesos.constraints  priority:1&lt;br/&gt;
spark.network.timeout   600&lt;br/&gt;
spark.rpc.message.maxSize    500&lt;br/&gt;
spark.speculation   false&lt;br/&gt;
spark.sql.parquet.mergeSchema   false&lt;br/&gt;
spark.sql.planner.externalSort  true&lt;br/&gt;
spark.submit.deployMode client&lt;br/&gt;
spark.task.cpus 1&lt;/p&gt;

&lt;p&gt;Exception here:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;---------------------------------------------------------------------------
timeout                                   Traceback (most recent call last)
&amp;lt;ipython-input-1-6319dd276401&amp;gt; in &amp;lt;module&amp;gt;()
      2 sc = SparkContext()
      3 rdd = sc.parallelize(range(10))
----&amp;gt; 4 [x &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; x in rdd.toLocalIterator()]

/foo/spark-2.0.1-bin-hadoop2.7/python/pyspark/rdd.pyc in _load_from_socket(port, serializer)
    140     &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
    141         rf = sock.makefile(&lt;span class=&quot;code-quote&quot;&gt;&quot;rb&quot;&lt;/span&gt;, 65536)
--&amp;gt; 142         &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; item in serializer.load_stream(rf):
    143             yield item
    144     &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt;:

/foo/spark-2.0.1-bin-hadoop2.7/python/pyspark/serializers.pyc in load_stream(self, stream)
    137         &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; True:
    138             &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
--&amp;gt; 139                 yield self._read_with_length(stream)
    140             except EOFError:
    141                 &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;

/foo/spark-2.0.1-bin-hadoop2.7/python/pyspark/serializers.pyc in _read_with_length(self, stream)
    154 
    155     def _read_with_length(self, stream):
--&amp;gt; 156         length = read_int(stream)
    157         &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; length == SpecialLengths.END_OF_DATA_SECTION:
    158             raise EOFError

/foo/spark-2.0.1-bin-hadoop2.7/python/pyspark/serializers.pyc in read_int(stream)
    541 
    542 def read_int(stream):
--&amp;gt; 543     length = stream.read(4)
    544     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; not length:
    545         raise EOFError

/usr/lib/python2.7/socket.pyc in read(self, size)
    378                 # fragmentation issues on many platforms.
    379                 &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
--&amp;gt; 380                     data = self._sock.recv(left)
    381                 except error, e:
    382                     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; e.args[0] == EINTR:

timeout: timed out
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</description>
                <environment>&lt;p&gt;Ubuntu 14.04.5 LTS&lt;br/&gt;
Driver: AWS M4.XLARGE&lt;br/&gt;
Slaves: AWS M4.4.XLARGE&lt;br/&gt;
mesos 1.0.1&lt;br/&gt;
spark 2.0.1&lt;br/&gt;
pyspark&lt;/p&gt;</environment>
        <key id="13018312">SPARK-18281</key>
            <summary>toLocalIterator yields time out error on pyspark2</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="viirya">L. C. Hsieh</assignee>
                                    <reporter username="lminer">Luke Miner</reporter>
                        <labels>
                    </labels>
                <created>Fri, 4 Nov 2016 23:38:31 +0000</created>
                <updated>Wed, 27 Sep 2017 13:46:10 +0000</updated>
                            <resolved>Tue, 20 Dec 2016 21:13:08 +0000</resolved>
                                    <version>2.0.1</version>
                                    <fixVersion>2.0.3</fixVersion>
                    <fixVersion>2.1.1</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="15743524" author="mwdusenb@us.ibm.com" created="Mon, 12 Dec 2016 23:34:45 +0000"  >&lt;p&gt;I&apos;m also seeing the same error with both Python 2.7 and Python 3.5 on Spark 2.0.2 and the Git master when using &lt;tt&gt;rdd.toLocalIterator()&lt;/tt&gt; or &lt;tt&gt;df.toLocalIterator()&lt;/tt&gt; for a PySpark RDD and DataFrame, respectively.&lt;/p&gt;

&lt;p&gt;On Spark 1.6.x, &lt;tt&gt;rdd.toLocalIterator()&lt;/tt&gt; worked correctly.&lt;/p&gt;

&lt;p&gt;Here&apos;s another example using DataFrames:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;df = spark.createDataFrame([[1],[2],[3]])
it = df.toLocalIterator()   # should timeout here with an &lt;span class=&quot;code-quote&quot;&gt;&quot;java.net.SocketTimeoutException: Accept timed out&quot;&lt;/span&gt; error
row = next(it)   # &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; an &lt;span class=&quot;code-quote&quot;&gt;&quot;Exception: could not open socket&quot;&lt;/span&gt; error
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Result:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;ERROR PythonRDD: Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; sending iterator
java.net.SocketTimeoutException: Accept timed out
	at java.net.PlainSocketImpl.socketAccept(Native Method)
	at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)
	at java.net.ServerSocket.implAccept(ServerSocket.java:545)
	at java.net.ServerSocket.accept(ServerSocket.java:513)
	at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:697)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=davies&quot; class=&quot;user-hover&quot; rel=&quot;davies&quot;&gt;davies&lt;/a&gt; I see that &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-14334&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;SPARK-14334 &lt;/a&gt; re-engineered and expanded the &lt;tt&gt;toLocalIterator&lt;/tt&gt; functionality to DataSets/DataFrames for both Scala/Java &amp;amp; Python.  Do you have any thoughts on the issue that is arising now?&lt;/p&gt;</comment>
                            <comment id="15744047" author="viirya" created="Tue, 13 Dec 2016 03:46:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mwdusenb%40us.ibm.com&quot; class=&quot;user-hover&quot; rel=&quot;mwdusenb@us.ibm.com&quot;&gt;mwdusenb@us.ibm.com&lt;/a&gt; I can reproduce your issue. I already have the fixing too. If you are not working on this, I will submit a PR for it.&lt;/p&gt;

&lt;p&gt;BTW, I can&apos;t exactly reproduce the issue reported by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lminer&quot; class=&quot;user-hover&quot; rel=&quot;lminer&quot;&gt;lminer&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;from pyspark &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkContext
sc = SparkContext()
rdd = sc.parallelize(range(10))
[x &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; x in rdd.toLocalIterator()]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;But the following one will be failed:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;from pyspark &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkContext
sc = SparkContext()
rdd = sc.parallelize(range(10))
it = rdd.toLocalIterator()
next(it)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;They are caused by the same bug. I&apos;d fix them together.&lt;/p&gt;</comment>
                            <comment id="15744271" author="apachespark" created="Tue, 13 Dec 2016 05:57:05 +0000"  >&lt;p&gt;User &apos;viirya&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16263&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16263&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15746002" author="mwdusenb@us.ibm.com" created="Tue, 13 Dec 2016 19:20:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=viirya&quot; class=&quot;user-hover&quot; rel=&quot;viirya&quot;&gt;viirya&lt;/a&gt; Thanks for taking on this bug!  I tried out PR, and I&apos;m still running into a socket timeout error for the example I gave above:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Traceback (most recent call last):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;stdin&amp;gt;&quot;&lt;/span&gt;, line 1, in &amp;lt;module&amp;gt;
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/mwdusenb/spark/python/pyspark/sql/dataframe.py&quot;&lt;/span&gt;, line 416, in toLocalIterator
    peek = next(iter)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/mwdusenb/spark/python/pyspark/rdd.py&quot;&lt;/span&gt;, line 140, in _load_from_socket
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; item in serializer.load_stream(rf):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/mwdusenb/spark/python/pyspark/serializers.py&quot;&lt;/span&gt;, line 144, in load_stream
    yield self._read_with_length(stream)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/mwdusenb/spark/python/pyspark/serializers.py&quot;&lt;/span&gt;, line 161, in _read_with_length
    length = read_int(stream)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/home/mwdusenb/spark/python/pyspark/serializers.py&quot;&lt;/span&gt;, line 555, in read_int
    length = stream.read(4)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/opt/anaconda3/lib/python3.5/socket.py&quot;&lt;/span&gt;, line 575, in readinto
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; self._sock.recv_into(b)
socket.timeout: timed out
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Interestingly, it looks like the &lt;tt&gt;it = df.toLocalIterator()&lt;/tt&gt; line launches a very large number of &lt;tt&gt;toLocalIterator&lt;/tt&gt; jobs, and then the Python socket times out while those jobs are running.&lt;/p&gt;</comment>
                            <comment id="15746023" author="mwdusenb@us.ibm.com" created="Tue, 13 Dec 2016 19:29:02 +0000"  >&lt;p&gt;Here&apos;s another interesting finding.  The first (original) example fails with the timeout.  However, if you create the DataFrame, do something with it, and then create the iterator, it will work.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;df = spark.createDataFrame([[1],[2],[3]])
it = df.toLocalIterator()  # FAILS HERE
row = next(it)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;df = spark.createDataFrame([[1],[2],[3]])
df.count()
it = df.toLocalIterator()  # No longer fails
row = next(it)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This leads me to believe there may be something wrong with the creation of the DataFrame.&lt;/p&gt;</comment>
                            <comment id="15747120" author="viirya" created="Wed, 14 Dec 2016 03:42:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mwdusenb%40us.ibm.com&quot; class=&quot;user-hover&quot; rel=&quot;mwdusenb@us.ibm.com&quot;&gt;mwdusenb@us.ibm.com&lt;/a&gt; Thanks for reporting this again! I can&apos;t reproduce this after applying the PR. However, I think the remaining issue is similar to the change in the PR.&lt;/p&gt;

&lt;p&gt;In JVM side, we just get an iterator of the RDD partitioned results. Once the connection is established, we begin to write elements through the socket. However, if the RDD is not materialized before, the materialization time + network cost might exceed the timeout setting before serving the first element to Python.&lt;/p&gt;

&lt;p&gt;That is why when you materialize the RDD by running &lt;tt&gt;df.count&lt;/tt&gt;, it will not fail.&lt;/p&gt;

&lt;p&gt;I&apos;d change the PR accordingly. May you try it again and see if it solves your tests? Thanks.&lt;/p&gt;</comment>
                            <comment id="15749711" author="mwdusenb@us.ibm.com" created="Wed, 14 Dec 2016 22:30:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=viirya&quot; class=&quot;user-hover&quot; rel=&quot;viirya&quot;&gt;viirya&lt;/a&gt; Thanks for continuing to work on this!  With the latest PR update, the original example now runs correctly when running PySpark in local mode (&lt;tt&gt;./bin/pyspark&lt;/tt&gt;).  However, I then tried the same example  on Yarn again (&lt;tt&gt;./bin/pyspark --master yarn --deploy-mode client&lt;/tt&gt;) and ran into the same socket timeout issues.&lt;/p&gt;

&lt;p&gt;I looked into it further and found some interesting findings.  In local mode execution, the number of partitions for the DataFrame was &lt;tt&gt;48&lt;/tt&gt;, while in Yarn execution mode, the same DataFrame started at &lt;tt&gt;665&lt;/tt&gt; partitions.  Looking at the UI, as soon as &lt;tt&gt;it = df.toLocalIterator()&lt;/tt&gt; is called, it will launch a number of &lt;tt&gt;toLocalIterator&lt;/tt&gt; jobs equal to the number of partitions, so &lt;tt&gt;48&lt;/tt&gt; jobs in local mode, and &lt;tt&gt;665&lt;/tt&gt; in Yarn mode.  Currently, the &lt;tt&gt;it = df.toLocalIterator()&lt;/tt&gt; line blocks until all of those jobs finish.  So, if the number of partitions is high enough, the socket timeout will still be triggered.&lt;/p&gt;

&lt;p&gt;Below is a reproducible example (I hope!) in local mode (&lt;tt&gt;./bin/pyspark&lt;/tt&gt;):&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;df = spark.createDataFrame([[1],[2],[3]])
it = df.toLocalIterator()
row = next(it)   # &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; should work
df.rdd.getNumPartitions()  # returns `48`

# Now let&apos;s &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt; it
df2 = df.repartition(700)  # increase number of partitions
it2 = df2.toLocalIterator()  # THIS FAILS -&amp;gt; `socket.timeout: timed out`
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15749830" author="holdenk" created="Wed, 14 Dec 2016 23:31:27 +0000"  >&lt;p&gt;For what its worth I can repro on top of the PR with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mwdusenb%40us.ibm.com&quot; class=&quot;user-hover&quot; rel=&quot;mwdusenb@us.ibm.com&quot;&gt;mwdusenb@us.ibm.com&lt;/a&gt;&apos;s example. I think we might be going about fixing this in somewhat of an odd way - the current Python behaviour of toLocalIterator is pretty different than that of Scala, we immediately do a foreach on the Scala iterator which is somewhat strange.&lt;/p&gt;

&lt;p&gt;Maybe we could change this to behave more like the Scala toLocalIterator and get rid of these timeouts/hacks around the timeouts. What do people think?&lt;/p&gt;</comment>
                            <comment id="15750158" author="viirya" created="Thu, 15 Dec 2016 02:31:39 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=holdenk&quot; class=&quot;user-hover&quot; rel=&quot;holdenk&quot;&gt;holdenk&lt;/a&gt;, what you meant for &quot;we immediately do a foreach on the Scala iterator which is somewhat strange.&quot;?&lt;/p&gt;</comment>
                            <comment id="15750246" author="viirya" created="Thu, 15 Dec 2016 03:20:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mwdusenb%40us.ibm.com&quot; class=&quot;user-hover&quot; rel=&quot;mwdusenb@us.ibm.com&quot;&gt;mwdusenb@us.ibm.com&lt;/a&gt; Thanks for this test case! It is useful to me. However I need to increase the partition number to 1000 to reproduce this issue.&lt;/p&gt;

&lt;p&gt;The additional partitions will increase the time to materialize RDD elements and so cause timeout.&lt;/p&gt;

&lt;p&gt;I think we can&apos;t set a timeout to the socket reading operation like currently doing as the RDD materialization time is unpredictable. I will keep the connection timeout untouched but unset timeout for socket reading. &lt;/p&gt;</comment>
                            <comment id="15750281" author="viirya" created="Thu, 15 Dec 2016 03:43:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mwdusenb%40us.ibm.com&quot; class=&quot;user-hover&quot; rel=&quot;mwdusenb@us.ibm.com&quot;&gt;mwdusenb@us.ibm.com&lt;/a&gt; BTW, I updated the fixing and if you have time to test it again, that would be great. Thank you.&lt;/p&gt;</comment>
                            <comment id="15765260" author="davies" created="Tue, 20 Dec 2016 21:13:08 +0000"  >&lt;p&gt;Issue resolved by pull request 16263&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16263&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16263&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15884007" author="realno" created="Sat, 25 Feb 2017 04:13:21 +0000"  >&lt;p&gt;Is this bug really resolved? I am using the latest 2.1.0 release and having the same timeout behaviour as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mwdusenb%40us.ibm.com&quot; class=&quot;user-hover&quot; rel=&quot;mwdusenb@us.ibm.com&quot;&gt;mwdusenb@us.ibm.com&lt;/a&gt; described. When using the iterator in local mode it works fine but as soon as moving to cluster it will timeout. I also tested with Mike&apos;s example and was able to validate it. Can someone point me to a fix or an alternative with similar functionality?&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                            <comment id="15906497" author="lebigot" created="Sun, 12 Mar 2017 11:31:38 +0000"  >&lt;p&gt;Same here: I see the problem with the latest version too (2.1.0)! The problem appears randomly, for me (I haven&apos;t built any minimal example, but the problem looks very much like the one reported here: toLocalIterator used, etc.).&lt;/p&gt;</comment>
                            <comment id="15906498" author="viirya" created="Sun, 12 Mar 2017 11:41:25 +0000"  >&lt;p&gt;Can you provide some info about your environment? Few reproducible examples we used before are:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;df = spark.createDataFrame([[1],[2],[3]])
it = df.toLocalIterator()
row = next(it)   # &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; should work
df.rdd.getNumPartitions()  # returns `48`

# Now let&apos;s &lt;span class=&quot;code-keyword&quot;&gt;break&lt;/span&gt; it
df2 = df.repartition(700)  # increase number of partitions
it2 = df2.toLocalIterator()  # THIS FAILS -&amp;gt; `socket.timeout: timed out`
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;df = spark.createDataFrame([[1],[2],[3]])
it = df.toLocalIterator()  # FAILS HERE
row = next(it)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Can you run this examples without failure?&lt;/p&gt;</comment>
                            <comment id="15906499" author="viirya" created="Sun, 12 Mar 2017 11:42:09 +0000"  >&lt;p&gt;Or you have other reproducible examples to test?&lt;/p&gt;</comment>
                            <comment id="15906501" author="lebigot" created="Sun, 12 Mar 2017 11:50:51 +0000"  >&lt;p&gt;Thanks Liang-Chi. Now I do have a minimal example: the small example which is marked above as working is not working on my machine:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;~/Downloads/spark-2.1.0-bin-hadoop2.7/bin % PYSPARK_DRIVER_PYTHON=ipython2 ./pyspark

Python 2.7.13 (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, Dec 23 2016, 05:05:58)
Type &lt;span class=&quot;code-quote&quot;&gt;&quot;copyright&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;credits&quot;&lt;/span&gt; or &lt;span class=&quot;code-quote&quot;&gt;&quot;license&quot;&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; more information.

IPython 5.3.0 -- An enhanced Interactive Python.
?         -&amp;gt; Introduction and overview of IPython&apos;s features.
%quickref -&amp;gt; Quick reference.
help      -&amp;gt; Python&apos;s own help system.
object?   -&amp;gt; Details about &lt;span class=&quot;code-quote&quot;&gt;&apos;object&apos;&lt;/span&gt;, use &lt;span class=&quot;code-quote&quot;&gt;&apos;object??&apos;&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; extra details.
17/03/12 12:46:29 WARN SparkContext: Support &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Java 7 is deprecated as of Spark 2.0.0
2017-03-12 12:46:30.538 java[75598:10832148] Unable to load realm info from SCDynamicStore
17/03/12 12:46:30 WARN NativeCodeLoader: Unable to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your platform... using builtin-java classes where applicable
17/03/12 12:46:33 WARN Utils: Service &lt;span class=&quot;code-quote&quot;&gt;&apos;SparkUI&apos;&lt;/span&gt; could not bind on port 4040. Attempting port 4041.
17/03/12 12:46:48 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &apos;_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.0
      /_/

Using Python version 2.7.13 (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, Dec 23 2016 05:05:58)
SparkSession available as &lt;span class=&quot;code-quote&quot;&gt;&apos;spark&apos;&lt;/span&gt;.
In [1]: df = spark.createDataFrame([[1],[2],[3]])
   ...: it = df.toLocalIterator()
   ...: row = next(it)   # &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; should work
   ...: df.rdd.getNumPartitions()  # returns `48`
   ...: 

---------------------------------------------------------------------------
timeout                                   Traceback (most recent call last)
&amp;lt;ipython-input-1-828d0f5b5ce8&amp;gt; in &amp;lt;module&amp;gt;()
      1 df = spark.createDataFrame([[1],[2],[3]])
      2 it = df.toLocalIterator()
----&amp;gt; 3 row = next(it)   # &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; should work
      4 df.rdd.getNumPartitions()  # returns `48`

/Users/lebigot/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc in _load_from_socket(port, serializer)
    138     &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
    139         rf = sock.makefile(&lt;span class=&quot;code-quote&quot;&gt;&quot;rb&quot;&lt;/span&gt;, 65536)
--&amp;gt; 140         &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; item in serializer.load_stream(rf):
    141             yield item
    142     &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt;:

/Users/lebigot/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/serializers.pyc in load_stream(self, stream)
    142         &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; True:
    143             &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
--&amp;gt; 144                 yield self._read_with_length(stream)
    145             except EOFError:
    146                 &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;

/Users/lebigot/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/serializers.pyc in _read_with_length(self, stream)
    159
    160     def _read_with_length(self, stream):
--&amp;gt; 161         length = read_int(stream)
    162         &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; length == SpecialLengths.END_OF_DATA_SECTION:
    163             raise EOFError

/Users/lebigot/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/serializers.pyc in read_int(stream)
    553
    554 def read_int(stream):
--&amp;gt; 555     length = stream.read(4)
    556     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; not length:
    557         raise EOFError

/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/socket.pyc in read(self, size)
    382                 # fragmentation issues on many platforms.
    383                 &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
--&amp;gt; 384                     data = self._sock.recv(left)
    385                 except error, e:
    386                     &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; e.args[0] == EINTR:

timeout: timed out
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The number of partitions is 4.&lt;/p&gt;

&lt;p&gt;Configuration:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;latest macOS Sierra (10.12.3),&lt;/li&gt;
	&lt;li&gt;IPython, etc. provided through MacPorts,&lt;/li&gt;
	&lt;li&gt;no special Spark configuration except for the verbosity level,&lt;/li&gt;
	&lt;li&gt;nothing else running on my machine (MacBook early 2015).&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15906512" author="lebigot" created="Sun, 12 Mar 2017 12:54:47 +0000"  >&lt;p&gt;The second example also fails, but differently:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;~/Downloads/spark-2.1.0-bin-hadoop2.7/bin % PYSPARK_DRIVER_PYTHON=ipython2 ./pyspark

Python 2.7.13 (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, Dec 23 2016, 05:05:58)
Type &lt;span class=&quot;code-quote&quot;&gt;&quot;copyright&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;credits&quot;&lt;/span&gt; or &lt;span class=&quot;code-quote&quot;&gt;&quot;license&quot;&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; more information.

IPython 5.3.0 -- An enhanced Interactive Python.
?         -&amp;gt; Introduction and overview of IPython&apos;s features.
%quickref -&amp;gt; Quick reference.
help      -&amp;gt; Python&apos;s own help system.
object?   -&amp;gt; Details about &lt;span class=&quot;code-quote&quot;&gt;&apos;object&apos;&lt;/span&gt;, use &lt;span class=&quot;code-quote&quot;&gt;&apos;object??&apos;&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; extra details.
17/03/12 13:52:32 WARN SparkContext: Support &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; Java 7 is deprecated as of Spark 2.0.0
2017-03-12 13:52:32.493 java[79268:10882992] Unable to load realm info from SCDynamicStore
17/03/12 13:52:32 WARN NativeCodeLoader: Unable to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your platform... using builtin-java classes where applicable
17/03/12 13:52:42 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &apos;_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.1.0
      /_/

Using Python version 2.7.13 (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, Dec 23 2016 05:05:58)
SparkSession available as &lt;span class=&quot;code-quote&quot;&gt;&apos;spark&apos;&lt;/span&gt;.
(&#8230;)
In [2]: df = spark.createDataFrame([[1],[2],[3]])
   ...: 
In [3]: df.rdd.getNumPartitions()
Out[3]: 4
In [4]: df2 = df.repartition(700)  # increase number of partitions
   ...: it2 = df2.toLocalIterator()  # THIS FAILS -&amp;gt; `socket.timeout: timed out`
   ...: 
   ...: 
In [5]: 17/03/12 13:53:20 ERROR PythonRDD: Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; sending iterator
java.net.SocketTimeoutException: Accept timed out
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java)
        at java.net.ServerSocket.implAccept(ServerSocket.java:530)
        at java.net.ServerSocket.accept(ServerSocket.java:498)
        at org.apache.spark.api.python.PythonRDD$$anon$2.run(PythonRDD.scala:69)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15906519" author="viirya" created="Sun, 12 Mar 2017 13:15:16 +0000"  >&lt;p&gt;Besides, can you also provide the error log?&lt;/p&gt;</comment>
                            <comment id="15906521" author="viirya" created="Sun, 12 Mar 2017 13:25:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lebigot&quot; class=&quot;user-hover&quot; rel=&quot;lebigot&quot;&gt;lebigot&lt;/a&gt; Thanks for the error log! It is weird because looks like you run the following old code which is fixed by the PR submitted for this issue: &lt;a href=&quot;https://github.com/apache/spark/pull/16263&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16263&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;/Users/lebigot/Downloads/spark-2.1.0-bin-hadoop2.7/python/pyspark/rdd.pyc in _load_from_socket(port, serializer)
    138     &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
    139         rf = sock.makefile(&lt;span class=&quot;code-quote&quot;&gt;&quot;rb&quot;&lt;/span&gt;, 65536)
--&amp;gt; 140         &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; item in serializer.load_stream(rf):
    141             yield item
    142     &lt;span class=&quot;code-keyword&quot;&gt;finally&lt;/span&gt;:
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I go to check the detailed changes in Spark 2.1.0: &lt;a href=&quot;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315420&amp;amp;version=12335644&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/ReleaseNote.jspa?projectId=12315420&amp;amp;version=12335644&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I don&apos;t find this issue in the JIRA list. So I think this fixing is not included in Spark 2.1.0 release. &lt;/p&gt;
</comment>
                            <comment id="15906523" author="viirya" created="Sun, 12 Mar 2017 13:27:32 +0000"  >&lt;p&gt;Oh. Btw, you can see the Fix Version/s of this JIRA is 2.0.3, 2.1.1.&lt;/p&gt;</comment>
                            <comment id="15906565" author="lebigot" created="Sun, 12 Mar 2017 15:56:47 +0000"  >&lt;p&gt;Thanks Liang-Chi. I naively thought that if version 2.0.3 was listed in the fixed version it implied that 2.1 had the fix. I&apos;m looking forward to using the fixed version, then (not sure yet how to do this right now without compiling anything, though).&lt;/p&gt;</comment>
                            <comment id="15906568" author="srowen" created="Sun, 12 Mar 2017 16:05:40 +0000"  >&lt;p&gt;2.1.1 means 2.1.1 is the first 2.1.x version that has the fix, so, not 2.1.0. 2.0.3 comes after 2.1.0 chronologically.&lt;/p&gt;</comment>
                            <comment id="15906773" author="viirya" created="Mon, 13 Mar 2017 01:54:04 +0000"  >&lt;p&gt;That is right. So you can try 2.1.1 or latest codebase to test it. Please let me know if this issue happens still. Thanks.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12955412">SPARK-14334</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13024440">SPARK-18649</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 36 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i35wxr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>