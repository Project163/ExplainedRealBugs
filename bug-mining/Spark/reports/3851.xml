<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:46:08 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-16740] joins.LongToUnsafeRowMap crashes with NegativeArraySizeException</title>
                <link>https://issues.apache.org/jira/browse/SPARK-16740</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;Here is a crash in Spark SQL joins, with a minimal reproducible test case. Interestingly, it only seems to happen when reading Parquet data (I added a &lt;tt&gt;crash = True&lt;/tt&gt; variable to show it)&lt;/p&gt;

&lt;p&gt;This is an &lt;tt&gt;left_outer&lt;/tt&gt; example, but it also crashes with a regular &lt;tt&gt;inner&lt;/tt&gt; join.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; os

from pyspark &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkContext
from pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; types as SparkTypes
from pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SQLContext

sc = SparkContext()
sqlc = SQLContext(sc)

schema1 = SparkTypes.StructType([
    SparkTypes.StructField(&lt;span class=&quot;code-quote&quot;&gt;&quot;id1&quot;&lt;/span&gt;, SparkTypes.LongType(), nullable=True)
])
schema2 = SparkTypes.StructType([
    SparkTypes.StructField(&lt;span class=&quot;code-quote&quot;&gt;&quot;id2&quot;&lt;/span&gt;, SparkTypes.LongType(), nullable=True)
])

# Valid &lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt; values (-9223372036854775808 &amp;lt; -5543241376386463808 , 4661454128115150227 &amp;lt; 9223372036854775807)
data1 = [(4661454128115150227,), (-5543241376386463808,)]
data2 = [(650460285, )]

df1 = sqlc.createDataFrame(sc.parallelize(data1), schema1)
df2 = sqlc.createDataFrame(sc.parallelize(data2), schema2)

crash = True
&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; crash:
    os.system(&lt;span class=&quot;code-quote&quot;&gt;&quot;rm -rf /tmp/sparkbug&quot;&lt;/span&gt;)
    df1.write.parquet(&lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/sparkbug/vertex&quot;&lt;/span&gt;)
    df2.write.parquet(&lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/sparkbug/edge&quot;&lt;/span&gt;)

    df1 = sqlc.read.load(&lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/sparkbug/vertex&quot;&lt;/span&gt;)
    df2 = sqlc.read.load(&lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/sparkbug/edge&quot;&lt;/span&gt;)

result_df = df2.join(df1, on=(df1.id1 == df2.id2), how=&lt;span class=&quot;code-quote&quot;&gt;&quot;left_outer&quot;&lt;/span&gt;)

# Should print [Row(id2=650460285, id1=None)]
print result_df.collect()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When ran with &lt;tt&gt;spark-submit&lt;/tt&gt;, the final &lt;tt&gt;collect()&lt;/tt&gt; call crashes with this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;py4j.protocol.Py4JJavaError: An error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; calling o61.collectToPython.
: org.apache.spark.SparkException: Exception thrown in awaitResult:
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:194)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:120)
	at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:229)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:125)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:125)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:124)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenOuter(BroadcastHashJoinExec.scala:242)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;consume(WholeStageCodegenExec.scala:153)
	at org.apache.spark.sql.execution.BatchedDataSourceScanExec.consume(ExistingRDD.scala:225)
	at org.apache.spark.sql.execution.BatchedDataSourceScanExec.doProduce(ExistingRDD.scala:328)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.BatchedDataSourceScanExec.produce(ExistingRDD.scala:225)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.CodegenSupport$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;produce(WholeStageCodegenExec.scala:78)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:309)
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:347)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:86)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:86)
	at org.apache.spark.sql.Dataset.javaToPython(Dataset.scala:2507)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply$mcI$sp(Dataset.scala:2513)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2513)
	at org.apache.spark.sql.Dataset$$anonfun$collectToPython$1.apply(Dataset.scala:2513)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2532)
	at org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:2512)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:128)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:211)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.NegativeArraySizeException
	at org.apache.spark.sql.execution.joins.LongToUnsafeRowMap.optimize(HashedRelation.scala:619)
	at org.apache.spark.sql.execution.joins.LongHashedRelation$.apply(HashedRelation.scala:806)
	at org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:105)
	at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:816)
	at org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:812)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:90)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1$$anonfun$apply$1.apply(BroadcastExchangeExec.scala:72)
	at org.apache.spark.sql.execution.SQLExecution$.withExecutionId(SQLExecution.scala:94)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec$$anonfun$relationFuture$1.apply(BroadcastExchangeExec.scala:72)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)
	at scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12992698">SPARK-16740</key>
            <summary>joins.LongToUnsafeRowMap crashes with NegativeArraySizeException</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sylvinus">Sylvain Zimmer</assignee>
                                    <reporter username="sylvinus">Sylvain Zimmer</reporter>
                        <labels>
                    </labels>
                <created>Tue, 26 Jul 2016 19:32:01 +0000</created>
                <updated>Mon, 31 Oct 2016 12:38:33 +0000</updated>
                            <resolved>Thu, 28 Jul 2016 16:52:51 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>PySpark</component>
                    <component>Spark Core</component>
                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="15394437" author="sylvinus" created="Tue, 26 Jul 2016 19:54:22 +0000"  >&lt;p&gt;I&apos;m not a Scala expert but from a quick review of the code, it appears that it&apos;s easy to overflow the &lt;tt&gt;range&lt;/tt&gt; variable in &lt;tt&gt;optimize()&lt;/tt&gt;:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala#L608&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/joins/HashedRelation.scala#L608&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In my example, that would yield&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; val range = 4661454128115150227L - (-5543241376386463808L)
range: &lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt; = -8242048569207937581
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Maybe we should add &lt;tt&gt;range &amp;gt;= 0&lt;/tt&gt; as a condition of doing that optimization?&lt;/p&gt;</comment>
                            <comment id="15394553" author="dongjoon" created="Tue, 26 Jul 2016 21:18:05 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sylvinus&quot; class=&quot;user-hover&quot; rel=&quot;sylvinus&quot;&gt;sylvinus&lt;/a&gt;. &lt;br/&gt;
It looks like that. Could you make a PR for this issue?&lt;/p&gt;</comment>
                            <comment id="15394555" author="dongjoon" created="Tue, 26 Jul 2016 21:19:03 +0000"  >&lt;p&gt;Maybe, you will modify the following line?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val range = maxKey - minKey
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15394624" author="apachespark" created="Tue, 26 Jul 2016 22:00:07 +0000"  >&lt;p&gt;User &apos;sylvinus&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14373&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14373&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15394628" author="sylvinus" created="Tue, 26 Jul 2016 22:00:42 +0000"  >&lt;p&gt;Thanks! I just did. Let me know if that&apos;s okay.&lt;/p&gt;</comment>
                            <comment id="15394634" author="dongjoon" created="Tue, 26 Jul 2016 22:03:20 +0000"  >&lt;p&gt;You had better look up the one who made that code with `git blame` command and ask him after passing Jenkins.&lt;br/&gt;
That is the fastest way to get reviewed. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15394644" author="sylvinus" created="Tue, 26 Jul 2016 22:09:10 +0000"  >&lt;p&gt;OK! Looks like that would be &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=davies&quot; class=&quot;user-hover&quot; rel=&quot;davies&quot;&gt;davies&lt;/a&gt; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15620711" author="harishk15" created="Sun, 30 Oct 2016 22:49:18 +0000"  >&lt;p&gt;is this fix is available in 2.0.2 snapshot?. Please confirm&lt;/p&gt;</comment>
                            <comment id="15620813" author="dongjoon" created="Mon, 31 Oct 2016 00:05:09 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=harishk15&quot; class=&quot;user-hover&quot; rel=&quot;harishk15&quot;&gt;harishk15&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Yep. The patch is still there in branch-2.0. I guess you can test that with Spark 2.0.2-rc1, too.&lt;/p&gt;

&lt;p&gt;If you think you meet some related issue in 2.0.2-rc1, please file a Jira issue.&lt;/p&gt;</comment>
                            <comment id="15620885" author="harishk15" created="Mon, 31 Oct 2016 00:58:56 +0000"  >&lt;p&gt;Thank you. I downloaded the 2.0.2 snapshot with 2.7 Hadoop (i think its on 10/13). I can still reproduce this issue. If the &quot;2.0.2-rc1&quot; was updated after 10/13 then i will take the updates and try. Can you please help me to find the latest download path.?&lt;br/&gt;
I am going to try 2.0.3 snap shot from below location &amp;#8211; any suggestions?&lt;br/&gt;
&lt;a href=&quot;http://people.apache.org/~pwendell/spark-nightly/spark-branch-2.0-bin/latest/spark-2.0.3-SNAPSHOT-bin-hadoop2.7.tgz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://people.apache.org/~pwendell/spark-nightly/spark-branch-2.0-bin/latest/spark-2.0.3-SNAPSHOT-bin-hadoop2.7.tgz&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 3 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i31j5b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>