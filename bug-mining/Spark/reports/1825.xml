<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:27:13 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-7326] Performing window() on a WindowedDStream doesn&apos;t work all the time</title>
                <link>https://issues.apache.org/jira/browse/SPARK-7326</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Someone reported similar issues before but got no response.&lt;br/&gt;
&lt;a href=&quot;http://apache-spark-user-list.1001560.n3.nabble.com/Windows-of-windowed-streams-not-displaying-the-expected-results-td466.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-spark-user-list.1001560.n3.nabble.com/Windows-of-windowed-streams-not-displaying-the-expected-results-td466.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;And I met the same issue recently and it can be reproduced in 1.3.1 by the following piece of code:&lt;/p&gt;

&lt;p&gt;def main(args: Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;) {&lt;/p&gt;

&lt;p&gt;    val batchInterval = &quot;1234&quot;&lt;br/&gt;
    val sparkConf = new SparkConf()&lt;br/&gt;
      .setAppName(&quot;WindowOnWindowedDStream&quot;)&lt;br/&gt;
      .setMaster(&quot;local&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;&quot;)&lt;/p&gt;

&lt;p&gt;    val ssc =  new StreamingContext(sparkConf, Milliseconds(batchInterval.toInt))&lt;br/&gt;
    ssc.checkpoint(&quot;checkpoint&quot;)&lt;/p&gt;

&lt;p&gt;    def createRDD(i: Int) : RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, Int)&amp;#93;&lt;/span&gt; = {&lt;/p&gt;

&lt;p&gt;      val count = 1000&lt;br/&gt;
      val rawLogs = (1 to count).map&lt;/p&gt;
{ _ =&amp;gt;
        val word = &quot;word&quot; + Random.nextInt.abs % 5

        (word, 1)
      }
&lt;p&gt;      ssc.sparkContext.parallelize(rawLogs)&lt;br/&gt;
    }&lt;/p&gt;

&lt;p&gt;    val rddQueue = mutable.Queue[RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;(String, Int)&amp;#93;&lt;/span&gt;]()&lt;br/&gt;
    val rawLogStream = ssc.queueStream(rddQueue)&lt;/p&gt;

&lt;p&gt;    (1 to 300) foreach &lt;/p&gt;
{ i =&amp;gt;
      rddQueue.enqueue(createRDD(i))
    }

&lt;p&gt;    val l1 = rawLogStream.window(Milliseconds(batchInterval.toInt) * 5, Milliseconds(batchInterval.toInt) * 5).reduceByKey(_ + _)&lt;/p&gt;

&lt;p&gt;    val l2 = l1.window(Milliseconds(batchInterval.toInt) * 15, Milliseconds(batchInterval.toInt) * 15).reduceByKey(_ + _)&lt;/p&gt;

&lt;p&gt;    l1.print()&lt;br/&gt;
    l2.print()&lt;/p&gt;

&lt;p&gt;    ssc.start()&lt;br/&gt;
    ssc.awaitTermination()&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;Here we have two windowed DStream instance l1 and l2. &lt;/p&gt;

&lt;p&gt;l1 is the result DStream by performing a window() on the source DStream with both window and sliding duration 5 times the batch internal of the source stream.&lt;/p&gt;

&lt;p&gt;l2 is the result DStream by performing a window() on l1, with both window and sliding duration 3 times l1&apos;s batch interval, which is 15 times of the source stream.&lt;/p&gt;

&lt;p&gt;From the output of this simple streaming app, I can only see print data output from l1 and no data printed from l2.&lt;/p&gt;

&lt;p&gt;Diving into the source code, I found the problem may most likely reside in DStream.slice() implementation, as shown below.&lt;/p&gt;

&lt;p&gt;  def slice(fromTime: Time, toTime: Time): Seq[RDD&lt;span class=&quot;error&quot;&gt;&amp;#91;T&amp;#93;&lt;/span&gt;] = {&lt;br/&gt;
    if (!isInitialized) &lt;/p&gt;
{
      throw new SparkException(this + &quot; has not been initialized&quot;)
    }
&lt;p&gt;    if (!(fromTime - zeroTime).isMultipleOf(slideDuration)) &lt;/p&gt;
{
      logWarning(&quot;fromTime (&quot; + fromTime + &quot;) is not a multiple of slideDuration (&quot;
        + slideDuration + &quot;)&quot;)
    }
&lt;p&gt;    if (!(toTime - zeroTime).isMultipleOf(slideDuration)) &lt;/p&gt;
{
      logWarning(&quot;toTime (&quot; + fromTime + &quot;) is not a multiple of slideDuration (&quot;
        + slideDuration + &quot;)&quot;)
    }
&lt;p&gt;    val alignedToTime = toTime.floor(slideDuration, zeroTime)&lt;br/&gt;
    val alignedFromTime = fromTime.floor(slideDuration, zeroTime)&lt;/p&gt;

&lt;p&gt;    logInfo(&quot;Slicing from &quot; + fromTime + &quot; to &quot; + toTime +&lt;br/&gt;
      &quot; (aligned to &quot; + alignedFromTime + &quot; and &quot; + alignedToTime + &quot;)&quot;)&lt;/p&gt;

&lt;p&gt;    alignedFromTime.to(alignedToTime, slideDuration).flatMap(time =&amp;gt; &lt;/p&gt;
{
      if (time &amp;gt;= zeroTime) getOrCompute(time) else None
    }
&lt;p&gt;)&lt;br/&gt;
  }&lt;/p&gt;

&lt;p&gt;Here after performing floor() on both fromTime and toTime, the result (alignedFromTime - zeroTime) and (alignedToTime - zeroTime) may no longer be multiple of the slidingDuration, thus making isTimeValid() check failed for all the remaining computation.&lt;/p&gt;

&lt;p&gt;The fix would be to add a new floor() function in Time.scala to respect the zeroTime while performing the floor :&lt;/p&gt;

&lt;p&gt;  def floor(that: Duration, zeroTime: Time): Time = &lt;/p&gt;
{
    val t = that.milliseconds
    new Time(((this.millis - zeroTime.milliseconds) / t) * t + zeroTime.milliseconds)
  }
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;And then change the DStream.slice to call this new floor function by passing in its zeroTime.&lt;/p&gt;

&lt;p&gt;    val alignedToTime = toTime.floor(slideDuration, zeroTime)&lt;br/&gt;
    val alignedFromTime = fromTime.floor(slideDuration, zeroTime)&lt;/p&gt;

&lt;p&gt;This way the alignedToTime and alignedFromTime are &lt;b&gt;really&lt;/b&gt; aligned in respect to zeroTime whose value is not really a 0.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12826769">SPARK-7326</key>
            <summary>Performing window() on a WindowedDStream doesn&apos;t work all the time</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="wesleymiao">Wesley Miao</assignee>
                                    <reporter username="wesleymiao">Wesley Miao</reporter>
                        <labels>
                    </labels>
                <created>Sun, 3 May 2015 06:17:35 +0000</created>
                <updated>Mon, 11 May 2015 11:21:03 +0000</updated>
                            <resolved>Mon, 11 May 2015 11:20:33 +0000</resolved>
                                    <version>1.3.1</version>
                                    <fixVersion>1.4.0</fixVersion>
                                    <component>DStreams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="14525808" author="apachespark" created="Sun, 3 May 2015 12:22:04 +0000"  >&lt;p&gt;User &apos;wesleymiao&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/5871&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/5871&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14525815" author="srowen" created="Sun, 3 May 2015 12:48:31 +0000"  >&lt;p&gt;Out of curiosity why do you have a window + slide of 5x the batch duration? that is simply the same as making an underlying stream with 5x the batch duration. Maybe this is just a toy example. I know it&apos;s not directly relevant to what you&apos;re reporting.&lt;/p&gt;</comment>
                            <comment id="14526062" author="wesleymiao" created="Sun, 3 May 2015 22:39:02 +0000"  >&lt;p&gt;What I&apos;d like to achieve is to do multiple-level aggregations on the source of logging stream. For example - let&apos;s say the source logging stream is at interval 1 second. The first level aggregation would be every 1 minute, which is the 60 intervals of the source stream. The second level would be every 1 hour. The third level would be 1 day. And we can do more levels if we want.&lt;/p&gt;

&lt;p&gt;What I hope is that at each level we&apos;ll do reduceByKey(_ + _) so that its aggregation can be done against its immediate previous level, instead of always aggregating against the source stream. Level 3&apos;s reduceByKey will be based on level 2&apos;s result, level 2 is based one level 1 and level 1 is based on the source stream.&lt;/p&gt;

&lt;p&gt;I would think this approach will be more efficient than always reduce over the source stream, particularly for the higher level (like daily and weekly) aggregation.  &lt;/p&gt;</comment>
                            <comment id="14526278" author="srowen" created="Mon, 4 May 2015 05:39:54 +0000"  >&lt;p&gt;Makes sense, just interested in whether this was simplifiable, but it is beside the point.&lt;/p&gt;</comment>
                            <comment id="14537826" author="srowen" created="Mon, 11 May 2015 11:20:33 +0000"  >&lt;p&gt;Issue resolved by pull request 5871&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/5871&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/5871&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 28 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2e7wv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>