<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:44:14 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-15704] TungstenAggregate crashes </title>
                <link>https://issues.apache.org/jira/browse/SPARK-15704</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When I run DatasetBenchmark, the JVM crashes while executing &quot;Dataset complex Aggregator&quot; test case due to IndexOutOfBoundsException.&lt;br/&gt;
The error happens in TungstenAggregate; the mappings between bufferSerializer and bufferDeserializer are broken due to unresolved attribute.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;16/06/02 01:41:05 ERROR Executor: Exception in task 0.0 in stage 67.0 (TID 232)&lt;br/&gt;
java.lang.IndexOutOfBoundsException: -1&lt;br/&gt;
	at scala.collection.LinearSeqOptimized$class.apply(LinearSeqOptimized.scala:65)&lt;br/&gt;
	at scala.collection.immutable.List.apply(List.scala:84)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.aggregate.DeclarativeAggregate$RichAttribute.right(interfaces.scala:389)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.TypedAggregateExpression$$anonfun$3.applyOrElse(TypedAggregateExpression.scala:110)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.TypedAggregateExpression$$anonfun$3.applyOrElse(TypedAggregateExpression.scala:109)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:265)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:265)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:68)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:264)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:270)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:270)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:307)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)&lt;br/&gt;
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&lt;br/&gt;
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)&lt;br/&gt;
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)&lt;br/&gt;
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)&lt;br/&gt;
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:356)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:270)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:270)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:270)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5$$anonfun$apply$11.apply(TreeNode.scala:336)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:381)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&lt;br/&gt;
	at scala.collection.immutable.List.map(List.scala:285)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:334)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)&lt;br/&gt;
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&lt;br/&gt;
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)&lt;br/&gt;
	at scala.collection.AbstractIterator.to(Iterator.scala:1336)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)&lt;br/&gt;
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1336)&lt;br/&gt;
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)&lt;br/&gt;
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1336)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:356)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:270)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:254)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.TypedAggregateExpression.mergeExpressions$lzycompute(TypedAggregateExpression.scala:109)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.TypedAggregateExpression.mergeExpressions(TypedAggregateExpression.scala:105)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$7.apply(AggregationIterator.scala:162)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$7.apply(AggregationIterator.scala:158)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&lt;br/&gt;
	at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)&lt;br/&gt;
	at scala.collection.AbstractTraversable.flatMap(Traversable.scala:104)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.generateProcessRow(AggregationIterator.scala:158)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.&amp;lt;init&amp;gt;(AggregationIterator.scala:197)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.&amp;lt;init&amp;gt;(SortBasedAggregationIterator.scala:39)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortBasedAggregateExec.scala:80)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortBasedAggregateExec.scala:71)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:775)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:775)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:318)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:282)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:85)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;&lt;/blockquote&gt;</description>
                <environment></environment>
        <key id="12974859">SPARK-15704</key>
            <summary>TungstenAggregate crashes </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="inouehrs">Hiroshi Inoue</assignee>
                                    <reporter username="inouehrs">Hiroshi Inoue</reporter>
                        <labels>
                    </labels>
                <created>Wed, 1 Jun 2016 16:50:12 +0000</created>
                <updated>Tue, 21 Jun 2016 14:15:59 +0000</updated>
                            <resolved>Mon, 6 Jun 2016 03:10:50 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="15310650" author="apachespark" created="Wed, 1 Jun 2016 17:04:07 +0000"  >&lt;p&gt;User &apos;inouehrs&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13446&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13446&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15316156" author="cloud_fan" created="Mon, 6 Jun 2016 03:10:50 +0000"  >&lt;p&gt;Issue resolved by pull request 13446&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13446&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13446&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15341478" author="deenar" created="Tue, 21 Jun 2016 10:00:09 +0000"  >&lt;p&gt;Hi guys&lt;/p&gt;

&lt;p&gt;I get a similar error when using complex types in Aggregator. Not sure if this is the same issue or something else.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-style: solid;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeHeader panelHeader&quot; style=&quot;border-bottom-width: 1px;border-bottom-style: solid;&quot;&gt;&lt;b&gt;Agg.scala&lt;/b&gt;&lt;/div&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.functions._
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.TypedColumn
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.catalyst.encoders.ExpressionEncoder
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.expressions.Aggregator
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.{Encoder,Row}
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; sqlContext.implicits._

object CustomSummer &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; Aggregator[Valuation, Map[Int, Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]], Seq[Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]]] with Serializable  {
     def zero: Map[Int, Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]] = Map()
     def reduce(b: Map[Int, Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]], a:Valuation): Map[Int, Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]] = {
       val timeInterval: Int = a.timeInterval
       val currentSum: Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;] = b.get(timeInterval).getOrElse(Nil)
       val currentRow: Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;] = a.pvs
       b.updated(timeInterval, sumArray(currentSum, currentRow))
     }     
    def sumArray(a: Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;], b: Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]): Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;] = Nil
     def merge(b1: Map[Int, Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]], b2: Map[Int, Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]]): Map[Int, Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]] = {
        /* merges two maps together ++ replaces any (k,v) from the map on the left
        side of ++ (here map1) by (k,v) from the right side map, &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (k,_) already
        exists in the left side map (here map1), e.g. Map(1-&amp;gt;1) ++ Map(1-&amp;gt;2) results in Map(1-&amp;gt;2) */
        b1 ++ b2.map { &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; (timeInterval, exposures) =&amp;gt;
          timeInterval -&amp;gt; sumArray(exposures, b1.getOrElse(timeInterval, Nil))
        }
     }
     def finish(exposures: Map[Int, Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]]): Seq[Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]] = 
      {
        exposures.size match {
          &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; 0 =&amp;gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
          &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; _ =&amp;gt; {
            val range = exposures.keySet.max
            &lt;span class=&quot;code-comment&quot;&gt;// convert map to 2 dimensional array, (timeInterval x Seq[expScn1, expScn2, ...]
&lt;/span&gt;            (0 to range).map(x =&amp;gt; exposures.getOrElse(x, Nil))
          }
        }
      }
  override def bufferEncoder: Encoder[Map[Int,Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]]] = ExpressionEncoder()
  override def outputEncoder: Encoder[Seq[Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;]]] = ExpressionEncoder()
   }

&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;Valuation(timeInterval : Int, pvs : Seq[&lt;span class=&quot;code-object&quot;&gt;Double&lt;/span&gt;])
val valns = sc.parallelize(Seq(Valuation(0, Seq(1.0,2.0,3.0)),
  Valuation(2, Seq(1.0,2.0,3.0)),
  Valuation(1, Seq(1.0,2.0,3.0)),Valuation(2, Seq(1.0,2.0,3.0)),Valuation(0, Seq(1.0,2.0,3.0))
  )).toDS

val g_c1 = valns.groupByKey(_.timeInterval).agg(CustomSummer.toColumn).show(&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I get the following error&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 10.0 failed 1 times, most recent failure: Lost task 1.0 in stage 10.0 (TID 19, localhost): java.lang.IndexOutOfBoundsException: 0&lt;br/&gt;
at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:47)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.remove(ArrayBuffer.scala:167)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:244)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:214)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:156)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:318)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:318)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:318)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:318)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.semanticHash(Expression.scala:174)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$Expr.hashCode(EquivalentExpressions.scala:39)&lt;br/&gt;
	at scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:210)&lt;br/&gt;
	at scala.collection.mutable.HashTable$HashUtils$class.elemHashCode(HashTable.scala:398)&lt;br/&gt;
	at scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:39)&lt;br/&gt;
	at scala.collection.mutable.HashTable$class.findEntry(HashTable.scala:130)&lt;br/&gt;
	at scala.collection.mutable.HashMap.findEntry(HashMap.scala:39)&lt;br/&gt;
	at scala.collection.mutable.HashMap.get(HashMap.scala:69)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExpr(EquivalentExpressions.scala:53)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:86)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$subexpressionElimination$1.apply(CodeGenerator.scala:661)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$subexpressionElimination$1.apply(CodeGenerator.scala:661)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.subexpressionElimination(CodeGenerator.scala:661)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:718)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:59)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:44)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:369)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:93)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:92)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.generateProcessRow(AggregationIterator.scala:178)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.&amp;lt;init&amp;gt;(AggregationIterator.scala:197)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.&amp;lt;init&amp;gt;(SortBasedAggregationIterator.scala:29)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:84)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:75)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:780)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:780)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:85)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;Driver stacktrace:&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1450)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1438)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1437)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1437)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:811)&lt;br/&gt;
	at scala.Option.foreach(Option.scala:236)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:811)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1659)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1618)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1607)&lt;br/&gt;
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)&lt;br/&gt;
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:632)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1872)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1885)&lt;br/&gt;
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1898)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:347)&lt;br/&gt;
	at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:39)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2176)&lt;br/&gt;
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2525)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2175)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2182)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1918)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:1917)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.withTypedCallback(Dataset.scala:2555)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.head(Dataset.scala:1917)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2132)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:239)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.show(Dataset.scala:526)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.show(Dataset.scala:506)&lt;br/&gt;
Caused by: java.lang.IndexOutOfBoundsException: 0&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.apply(ResizableArray.scala:43)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.apply(ArrayBuffer.scala:47)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.remove(ArrayBuffer.scala:167)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$2.apply(TreeNode.scala:244)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:179)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.trees.TreeNode.withNewChildren(TreeNode.scala:214)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:156)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:318)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:318)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:318)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$1.apply(Expression.scala:155)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:318)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)&lt;br/&gt;
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized$lzycompute(Expression.scala:155)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.canonicalized(Expression.scala:154)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.Expression.semanticHash(Expression.scala:174)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions$Expr.hashCode(EquivalentExpressions.scala:39)&lt;br/&gt;
	at scala.runtime.ScalaRunTime$.hash(ScalaRunTime.scala:210)&lt;br/&gt;
	at scala.collection.mutable.HashTable$HashUtils$class.elemHashCode(HashTable.scala:398)&lt;br/&gt;
	at scala.collection.mutable.HashMap.elemHashCode(HashMap.scala:39)&lt;br/&gt;
	at scala.collection.mutable.HashTable$class.findEntry(HashTable.scala:130)&lt;br/&gt;
	at scala.collection.mutable.HashMap.findEntry(HashMap.scala:39)&lt;br/&gt;
	at scala.collection.mutable.HashMap.get(HashMap.scala:69)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExpr(EquivalentExpressions.scala:53)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.EquivalentExpressions.addExprTree(EquivalentExpressions.scala:86)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$subexpressionElimination$1.apply(CodeGenerator.scala:661)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext$$anonfun$subexpressionElimination$1.apply(CodeGenerator.scala:661)&lt;br/&gt;
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.subexpressionElimination(CodeGenerator.scala:661)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.CodegenContext.generateExpressions(CodeGenerator.scala:718)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:59)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.generate(GenerateMutableProjection.scala:44)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:369)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:93)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3$$anonfun$4.apply(SortAggregateExec.scala:92)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.generateProcessRow(AggregationIterator.scala:178)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.AggregationIterator.&amp;lt;init&amp;gt;(AggregationIterator.scala:197)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortBasedAggregationIterator.&amp;lt;init&amp;gt;(SortBasedAggregationIterator.scala:29)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:84)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.SortAggregateExec$$anonfun$doExecute$1$$anonfun$3.apply(SortAggregateExec.scala:75)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:780)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:780)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:85)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="15341587" author="inouehrs" created="Tue, 21 Jun 2016 11:20:40 +0000"  >&lt;p&gt;I confirmed the same error by executing Deenar&apos;s code.&lt;br/&gt;
This seems another issue.&lt;/p&gt;</comment>
                            <comment id="15341598" author="deenar" created="Tue, 21 Jun 2016 11:28:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=inouehrs&quot; class=&quot;user-hover&quot; rel=&quot;inouehrs&quot;&gt;inouehrs&lt;/a&gt; thanks for checking this out, Do you want me to raise another JIRA?&lt;/p&gt;</comment>
                            <comment id="15341618" author="inouehrs" created="Tue, 21 Jun 2016 11:40:07 +0000"  >&lt;p&gt;Yes, please. Thank you.&lt;/p&gt;</comment>
                            <comment id="15341844" author="deenar" created="Tue, 21 Jun 2016 14:15:26 +0000"  >&lt;p&gt;done see &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-16100&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-16100&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12981322">SPARK-16100</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 22 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2yu5z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>