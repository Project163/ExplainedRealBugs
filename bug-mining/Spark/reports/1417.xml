<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:23:44 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-5795] api.java.JavaPairDStream.saveAsNewAPIHadoopFiles may not friendly to java</title>
                <link>https://issues.apache.org/jira/browse/SPARK-5795</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;import org.apache.hadoop.mapreduce.lib.output.TextOutputFormat;&lt;/p&gt;

&lt;p&gt;the following code can&apos;t compile on java.&lt;br/&gt;
JavaPairDStream&amp;lt;Integer, Integer&amp;gt; rs =....&lt;br/&gt;
rs.saveAsNewAPIHadoopFiles(&quot;prefix&quot;, &quot;txt&quot;, Integer.class, Integer.class, TextOutputFormat.class, jobConf);&lt;/p&gt;

&lt;p&gt;but similar code in JavaPairRDD works ok.&lt;br/&gt;
JavaPairRDD&amp;lt;String, String&amp;gt; counts =.......&lt;br/&gt;
counts.saveAsNewAPIHadoopFile(&quot;out&quot;, Text.class, Text.class, TextOutputFormat.class, jobConf);&lt;br/&gt;
====================&lt;br/&gt;
mybe the &lt;br/&gt;
  def saveAsNewAPIHadoopFiles(&lt;br/&gt;
      prefix: String,&lt;br/&gt;
      suffix: String,&lt;br/&gt;
      keyClass: Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;,&lt;br/&gt;
      valueClass: Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;,&lt;br/&gt;
      outputFormatClass: Class[_ &amp;lt;: NewOutputFormat&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;],&lt;br/&gt;
      conf: Configuration = new Configuration) &lt;/p&gt;
{
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }&lt;br/&gt;
=====&amp;gt;&lt;br/&gt;
def saveAsNewAPIHadoopFiles[F &amp;lt;: NewOutputFormat&lt;span class=&quot;error&quot;&gt;&amp;#91;_, _&amp;#93;&lt;/span&gt;](&lt;br/&gt;
      prefix: String,&lt;br/&gt;
      suffix: String,&lt;br/&gt;
      keyClass: Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;,&lt;br/&gt;
      valueClass: Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;,&lt;br/&gt;
      outputFormatClass: Class&lt;span class=&quot;error&quot;&gt;&amp;#91;F&amp;#93;&lt;/span&gt;,&lt;br/&gt;
      conf: Configuration = new Configuration) {    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)  }



</description>
                <environment></environment>
        <key id="12774837">SPARK-5795</key>
            <summary>api.java.JavaPairDStream.saveAsNewAPIHadoopFiles may not friendly to java</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srowen">Sean R. Owen</assignee>
                                    <reporter username="cnstar9988">Littlestar</reporter>
                        <labels>
                    </labels>
                <created>Fri, 13 Feb 2015 09:06:43 +0000</created>
                <updated>Mon, 16 Feb 2015 19:33:25 +0000</updated>
                            <resolved>Mon, 16 Feb 2015 19:32:52 +0000</resolved>
                                    <version>1.2.1</version>
                                    <fixVersion>1.3.0</fixVersion>
                                    <component>DStreams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="14319769" author="cnstar9988" created="Fri, 13 Feb 2015 09:13:43 +0000"  >&lt;p&gt;org.apache.spark.api.java.JavaPairRDD&amp;lt;K, V&amp;gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;/** Output the RDD to any Hadoop-supported file system. */
  def saveAsHadoopFile[F &amp;lt;: OutputFormat[_, _]](
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F],
      conf: JobConf) {
    rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass, conf)
  }

  /** Output the RDD to any Hadoop-supported file system. */
  def saveAsHadoopFile[F &amp;lt;: OutputFormat[_, _]](
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F]) {
    rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass)
  }

  /** Output the RDD to any Hadoop-supported file system, compressing with the supplied codec. */
  def saveAsHadoopFile[F &amp;lt;: OutputFormat[_, _]](
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F],
      codec: Class[_ &amp;lt;: CompressionCodec]) {
    rdd.saveAsHadoopFile(path, keyClass, valueClass, outputFormatClass, codec)
  }

  /** Output the RDD to any Hadoop-supported file system. */
  def saveAsNewAPIHadoopFile[F &amp;lt;: NewOutputFormat[_, _]](
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F],
      conf: Configuration) {
    rdd.saveAsNewAPIHadoopFile(path, keyClass, valueClass, outputFormatClass, conf)
  }

  /**
   * Output the RDD to any Hadoop-supported storage system, using
   * a Configuration object for that storage system.
   */
  def saveAsNewAPIHadoopDataset(conf: Configuration) {
    rdd.saveAsNewAPIHadoopDataset(conf)
  }

  /** Output the RDD to any Hadoop-supported file system. */
  def saveAsNewAPIHadoopFile[F &amp;lt;: NewOutputFormat[_, _]](
      path: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[F]) {
    rdd.saveAsNewAPIHadoopFile(path, keyClass, valueClass, outputFormatClass)
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;org.apache.spark.streaming.api.java.JavaPairDStream&amp;lt;K, V&amp;gt;&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;/**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: &quot;prefix-TIME_IN_MS.suffix&quot;.
   */
  def saveAsHadoopFiles[F &amp;lt;: OutputFormat[K, V]](prefix: String, suffix: String) {
    dstream.saveAsHadoopFiles(prefix, suffix)
  }

  /**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: &quot;prefix-TIME_IN_MS.suffix&quot;.
   */
  def saveAsHadoopFiles(
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ &amp;lt;: OutputFormat[_, _]]) {
    dstream.saveAsHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass)
  }

  /**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: &quot;prefix-TIME_IN_MS.suffix&quot;.
   */
  def saveAsHadoopFiles(
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ &amp;lt;: OutputFormat[_, _]],
      conf: JobConf) {
    dstream.saveAsHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }

  /**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: &quot;prefix-TIME_IN_MS.suffix&quot;.
   */
  def saveAsNewAPIHadoopFiles[F &amp;lt;: NewOutputFormat[K, V]](prefix: String, suffix: String) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix)
  }

  /**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: &quot;prefix-TIME_IN_MS.suffix&quot;.
   */
  def saveAsNewAPIHadoopFiles(
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ &amp;lt;: NewOutputFormat[_, _]]) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass)
  }

  /**
   * Save each RDD in `this` DStream as a Hadoop file. The file name at each batch interval is
   * generated based on `prefix` and `suffix`: &quot;prefix-TIME_IN_MS.suffix&quot;.
   */
  def saveAsNewAPIHadoopFiles(
      prefix: String,
      suffix: String,
      keyClass: Class[_],
      valueClass: Class[_],
      outputFormatClass: Class[_ &amp;lt;: NewOutputFormat[_, _]],
      conf: Configuration = new Configuration) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14319830" author="srowen" created="Fri, 13 Feb 2015 10:25:36 +0000"  >&lt;p&gt;When you say &quot;doesn&apos;t compile&quot;, you should show the compilation error. Although I think I know what it is. There&apos;s a workaround but I agree we can look at fixing it. If it breaks binary compatibility, it would have to wait until later.&lt;/p&gt;</comment>
                            <comment id="14320083" author="cnstar9988" created="Fri, 13 Feb 2015 13:50:54 +0000"  >&lt;p&gt;error info...&lt;br/&gt;
The method saveAsNewAPIHadoopFiles(String, String, Class&amp;lt;?&amp;gt;, Class&amp;lt;?&amp;gt;, Class&amp;lt;? extends OutputFormat&amp;lt;?,?&amp;gt;&amp;gt;) in the type JavaPairDStream&amp;lt;Integer,Integer&amp;gt; is not applicable for the arguments (String, String, Class&amp;lt;Integer&amp;gt;, Class&amp;lt;Integer&amp;gt;, Class&amp;lt;TextOutputFormat&amp;gt;)&lt;/p&gt;

</comment>
                            <comment id="14320090" author="cnstar9988" created="Fri, 13 Feb 2015 13:53:08 +0000"  >&lt;p&gt;my testcase on java 1.7 and spark 1.3 trunk.&lt;br/&gt;
Thanks.&lt;/p&gt;</comment>
                            <comment id="14320109" author="cnstar9988" created="Fri, 13 Feb 2015 14:08:52 +0000"  >&lt;p&gt;Does it same problem as &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-5297&quot; title=&quot;JavaStreamingContext.fileStream won&amp;#39;t work because type info isn&amp;#39;t propagated&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-5297&quot;&gt;&lt;del&gt;SPARK-5297&lt;/del&gt;&lt;/a&gt;, thanks.&lt;/p&gt;</comment>
                            <comment id="14321760" author="srowen" created="Sun, 15 Feb 2015 00:15:10 +0000"  >&lt;p&gt;Yes, I&apos;ve seen the same problem and been meaning to do something about it. It makes you do this to use &lt;tt&gt;JavaPairDStream&lt;/tt&gt;:&lt;br/&gt;
&lt;a href=&quot;https://github.com/OryxProject/oryx/blob/master/oryx-lambda/src/main/java/com/cloudera/oryx/lambda/BatchLayer.java#L187&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/OryxProject/oryx/blob/master/oryx-lambda/src/main/java/com/cloudera/oryx/lambda/BatchLayer.java#L187&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;So basically, this is how it&apos;s declared now:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  def saveAsNewAPIHadoopFiles(
      prefix: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;,
      suffix: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;,
      keyClass: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;[_],
      valueClass: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;[_],
      outputFormatClass: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;[_ &amp;lt;: NewOutputFormat[_, _]],
      conf: Configuration = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Configuration) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;but this works, and is how it works in &lt;tt&gt;JavaPairRDD&lt;/tt&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  def saveAsNewAPIHadoopFiles[F &amp;lt;: NewOutputFormat[_, _]](
      prefix: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;,
      suffix: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;,
      keyClass: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;[_],
      valueClass: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;[_],
      outputFormatClass: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;[F],
      conf: Configuration = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Configuration) {
    dstream.saveAsNewAPIHadoopFiles(prefix, suffix, keyClass, valueClass, outputFormatClass, conf)
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I worry about an API change of course, but, I think the current API isn&apos;t directly callable, so seems OK to change.&lt;/p&gt;

&lt;p&gt;For a simple demo, try compiling this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;JavaPairDStream&amp;lt;IntWritable,Text&amp;gt; pds = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;;&#8232;
pds.saveAsNewAPIHadoopFiles(&lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;, &quot;&lt;/span&gt;&quot;, IntWritable.class, IntWritable.class, SequenceFileOutputFormat.class);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The change above makes it work. I&apos;ll open a PR. I bumped the priority based on my understanding of the issue.&lt;/p&gt;</comment>
                            <comment id="14321761" author="apachespark" created="Sun, 15 Feb 2015 00:17:41 +0000"  >&lt;p&gt;User &apos;srowen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4608&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4608&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14321848" author="cnstar9988" created="Sun, 15 Feb 2015 07:42:46 +0000"  >&lt;p&gt;I merge pull/4608 and rebuild, it works for me, thanks.&lt;/p&gt;</comment>
                            <comment id="14323167" author="srowen" created="Mon, 16 Feb 2015 19:32:52 +0000"  >&lt;p&gt;Issue resolved by pull request 4608&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4608&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4608&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12698728" name="TestStreamCompile.java" size="2242" author="cnstar9988" created="Fri, 13 Feb 2015 13:53:08 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 40 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i25l7z:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>