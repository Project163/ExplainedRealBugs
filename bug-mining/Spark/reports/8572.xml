<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:30:18 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-39612] The dataframe returned by exceptAll() can no longer perform operations such as count() or isEmpty(), or an exception will be thrown.</title>
                <link>https://issues.apache.org/jira/browse/SPARK-39612</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;As I said, the dataframe returned by `exceptAll()` can no longer perform operations such as `count()` or `isEmpty()`, or an exception will be thrown.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&amp;gt;&amp;gt;&amp;gt; d1 = spark.createDataFrame([(&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;)], &lt;span class=&quot;code-quote&quot;&gt;&apos;STRING&apos;&lt;/span&gt;)
&amp;gt;&amp;gt;&amp;gt; d1.show()
+-----+
|value|
+-----+
| &#160; &#160;a|
+-----+
&amp;gt;&amp;gt;&amp;gt; d2 = d1.exceptAll(d1)
&amp;gt;&amp;gt;&amp;gt; d2.show()
+-----+
|value|
+-----+
+-----+
&amp;gt;&amp;gt;&amp;gt; d2.count()
22/06/27 11:22:15 ERROR Executor: Exception in task 0.0 in stage 113.0 (TID 525)
java.lang.IllegalStateException: Couldn&apos;t find value#465 in [sum#494L]
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
&#160; &#160; at scala.collection.immutable.List.map(List.scala:297)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:698)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:528)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
&#160; &#160; at org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)
&#160; &#160; at org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)
&#160; &#160; at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$10(GenerateExec.scala:114)
&#160; &#160; at org.apache.spark.sql.execution.LazyIterator.results$lzycompute(GenerateExec.scala:36)
&#160; &#160; at org.apache.spark.sql.execution.LazyIterator.results(GenerateExec.scala:36)
&#160; &#160; at org.apache.spark.sql.execution.LazyIterator.hasNext(GenerateExec.scala:37)
&#160; &#160; at scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)
&#160; &#160; at scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)
&#160; &#160; at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
&#160; &#160; at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
&#160; &#160; at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
&#160; &#160; at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
&#160; &#160; at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
&#160; &#160; at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
&#160; &#160; at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
&#160; &#160; at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
&#160; &#160; at org.apache.spark.scheduler.Task.run(Task.scala:136)
&#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
&#160; &#160; at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
&#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
&#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
&#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
&#160; &#160; at java.base/java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:829)
22/06/27 11:22:15 ERROR TaskSetManager: Task 0 in stage 113.0 failed 1 times; aborting job
Traceback (most recent call last):
&#160; File &lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;stdin&amp;gt;&quot;&lt;/span&gt;, line 1, in &amp;lt;module&amp;gt;
&#160; File &lt;span class=&quot;code-quote&quot;&gt;&quot;/opt/downloads/spark-3.3.0-bin-hadoop3/python/pyspark/sql/dataframe.py&quot;&lt;/span&gt;, line 804, in count
&#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;(self._jdf.count())
&#160; File &lt;span class=&quot;code-quote&quot;&gt;&quot;/root/miniconda3/lib/python3.8/site-packages/py4j/java_gateway.py&quot;&lt;/span&gt;, line 1304, in __call__
&#160; &#160; return_value = get_return_value(
&#160; File &lt;span class=&quot;code-quote&quot;&gt;&quot;/opt/downloads/spark-3.3.0-bin-hadoop3/python/pyspark/sql/utils.py&quot;&lt;/span&gt;, line 190, in deco
&#160; &#160; &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; f(*a, **kw)
&#160; File &lt;span class=&quot;code-quote&quot;&gt;&quot;/root/miniconda3/lib/python3.8/site-packages/py4j/protocol.py&quot;&lt;/span&gt;, line 326, in get_return_value
&#160; &#160; raise Py4JJavaError(
py4j.protocol.Py4JJavaError: An error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; calling o253.count.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 113.0 failed 1 times, most recent failure: Lost task 0.0 in stage 113.0 (TID 525) (thomaszhu1.fyre.ibm.com executor driver): java.lang.IllegalStateException: Couldn&apos;t find value#465 in [sum#494L]
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
&#160; &#160; at scala.collection.immutable.List.map(List.scala:297)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:698)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:528)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
&#160; &#160; at org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)
&#160; &#160; at org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)
&#160; &#160; at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$10(GenerateExec.scala:114)
&#160; &#160; at org.apache.spark.sql.execution.LazyIterator.results$lzycompute(GenerateExec.scala:36)
&#160; &#160; at org.apache.spark.sql.execution.LazyIterator.results(GenerateExec.scala:36)
&#160; &#160; at org.apache.spark.sql.execution.LazyIterator.hasNext(GenerateExec.scala:37)
&#160; &#160; at scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)
&#160; &#160; at scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)
&#160; &#160; at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
&#160; &#160; at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
&#160; &#160; at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
&#160; &#160; at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
&#160; &#160; at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
&#160; &#160; at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
&#160; &#160; at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
&#160; &#160; at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
&#160; &#160; at org.apache.spark.scheduler.Task.run(Task.scala:136)
&#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
&#160; &#160; at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
&#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
&#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
&#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
&#160; &#160; at java.base/java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:829)Driver stacktrace:
&#160; &#160; at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)
&#160; &#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)
&#160; &#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)
&#160; &#160; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
&#160; &#160; at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
&#160; &#160; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
&#160; &#160; at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)
&#160; &#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)
&#160; &#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)
&#160; &#160; at scala.Option.foreach(Option.scala:407)
&#160; &#160; at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)
&#160; &#160; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)
&#160; &#160; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)
&#160; &#160; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)
&#160; &#160; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
Caused by: java.lang.IllegalStateException: Couldn&apos;t find value#465 in [sum#494L]
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:80)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:73)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$3(TreeNode.scala:589)
&#160; &#160; at scala.collection.immutable.List.map(List.scala:297)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:698)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:589)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)
&#160; &#160; at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:528)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:73)
&#160; &#160; at org.apache.spark.sql.execution.GenerateExec.boundGenerator$lzycompute(GenerateExec.scala:75)
&#160; &#160; at org.apache.spark.sql.execution.GenerateExec.boundGenerator(GenerateExec.scala:75)
&#160; &#160; at org.apache.spark.sql.execution.GenerateExec.$anonfun$doExecute$10(GenerateExec.scala:114)
&#160; &#160; at org.apache.spark.sql.execution.LazyIterator.results$lzycompute(GenerateExec.scala:36)
&#160; &#160; at org.apache.spark.sql.execution.LazyIterator.results(GenerateExec.scala:36)
&#160; &#160; at org.apache.spark.sql.execution.LazyIterator.hasNext(GenerateExec.scala:37)
&#160; &#160; at scala.collection.Iterator$ConcatIterator.advance(Iterator.scala:199)
&#160; &#160; at scala.collection.Iterator$ConcatIterator.hasNext(Iterator.scala:227)
&#160; &#160; at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithoutKey_0$(Unknown Source)
&#160; &#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)
&#160; &#160; at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
&#160; &#160; at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
&#160; &#160; at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)
&#160; &#160; at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)
&#160; &#160; at org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)
&#160; &#160; at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
&#160; &#160; at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)
&#160; &#160; at org.apache.spark.scheduler.Task.run(Task.scala:136)
&#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)
&#160; &#160; at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)
&#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)
&#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
&#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
&#160; &#160; at java.base/java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:829)

 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment>&lt;p&gt;OS: centos stream 8&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$ uname -a
Linux thomaszhu1.fyre.ibm.com 4.18.0-348.7.1.el8_5.x86_64 #1 SMP Wed Dec 22 13:25:12 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux

$ python --version
Python 3.8.13 

$ pyspark --version
Welcome to
&#160; &#160; &#160; ____ &#160; &#160; &#160; &#160; &#160; &#160; &#160;__
&#160; &#160; &#160;/ __/__ &#160;___ _____/ /__
&#160; &#160; _\ \/ _ \/ _ `/ __/ &#160;&apos;_/
&#160; &#160;/___/ .__/\_,_/_/ /_/\_\ &#160; version 3.3.0
&#160; &#160; &#160; /_/
&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;&#160;
Using Scala version 2.12.15, OpenJDK 64-Bit Server VM, 11.0.11
Branch HEAD
Compiled by user ubuntu on 2022-06-09T19:58:58Z
Revision f74867bddfbcdd4d08076db36851e88b15e66556
Url https:&lt;span class=&quot;code-comment&quot;&gt;//github.com/apache/spark
&lt;/span&gt;Type --help &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; more information.

$ java --version
openjdk 11.0.11 2021-04-20
OpenJDK &lt;span class=&quot;code-object&quot;&gt;Runtime&lt;/span&gt; Environment AdoptOpenJDK-11.0.11+9 (build 11.0.11+9)
OpenJDK 64-Bit Server VM AdoptOpenJDK-11.0.11+9 (build 11.0.11+9, mixed mode) &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</environment>
        <key id="13468478">SPARK-39612</key>
            <summary>The dataframe returned by exceptAll() can no longer perform operations such as count() or isEmpty(), or an exception will be thrown.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gurwls223">Hyukjin Kwon</assignee>
                                    <reporter username="zhujunyong">Zhu JunYong</reporter>
                        <labels>
                    </labels>
                <created>Mon, 27 Jun 2022 03:33:08 +0000</created>
                <updated>Mon, 12 Dec 2022 18:11:09 +0000</updated>
                            <resolved>Tue, 5 Jul 2022 11:45:12 +0000</resolved>
                                    <version>3.3.0</version>
                                    <fixVersion>3.3.1</fixVersion>
                    <fixVersion>3.4.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="17562332" author="gurwls223" created="Tue, 5 Jul 2022 02:56:28 +0000"  >&lt;p&gt;Scala reproducer:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
val d1 = Seq(&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;).toDF
d1.exceptAll(d1).count()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17562334" author="gurwls223" created="Tue, 5 Jul 2022 02:57:21 +0000"  >&lt;p&gt;This is a regression from Spark 3.2.0&lt;/p&gt;</comment>
                            <comment id="17562410" author="gurwls223" created="Tue, 5 Jul 2022 07:14:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/35864&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/35864&lt;/a&gt; caused this. I checked that it works if we revert that.&lt;/p&gt;</comment>
                            <comment id="17562511" author="apachespark" created="Tue, 5 Jul 2022 09:35:28 +0000"  >&lt;p&gt;User &apos;HyukjinKwon&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/37084&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/37084&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17562623" author="gurwls223" created="Tue, 5 Jul 2022 11:45:12 +0000"  >&lt;p&gt;Issue resolved by pull request 37084&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/37084&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/37084&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                                                <inwardlinks description="is caused by">
                                        <issuelink>
            <issuekey id="13433427">SPARK-38531</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                            <customfield id="customfield_12314421" key="com.atlassian.jira.plugin.system.customfieldtypes:labels">
                        <customfieldname>Language</customfieldname>
                        <customfieldvalues>
                                        <label>Python</label>
    
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 19 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z16db4:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>