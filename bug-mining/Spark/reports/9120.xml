<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:33:49 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-44079] Json reader crashes when a different schema is present</title>
                <link>https://issues.apache.org/jira/browse/SPARK-44079</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When using pyspark 3.4, we noticed that when reading a json file with a corrupted record the reader crashes. In pyspark 3.3 this worked fine.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Code&lt;/b&gt;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
from pyspark.sql.types &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; StructType, StructField, IntegerType, StringType
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; json


data = &lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;&quot;[{&quot;&lt;/span&gt;a&lt;span class=&quot;code-quote&quot;&gt;&quot;: &quot;&lt;/span&gt;incorrect&lt;span class=&quot;code-quote&quot;&gt;&quot;, &quot;&lt;/span&gt;b&lt;span class=&quot;code-quote&quot;&gt;&quot;: &quot;&lt;/span&gt;correct&lt;span class=&quot;code-quote&quot;&gt;&quot;}]&quot;&lt;/span&gt;&quot;&quot;
schema = StructType([StructField(&lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt;, IntegerType(), True), StructField(&lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt;, StringType(), True), StructField(&lt;span class=&quot;code-quote&quot;&gt;&apos;_corrupt_record&apos;&lt;/span&gt;, StringType(), True)])


spark.read.option(&lt;span class=&quot;code-quote&quot;&gt;&quot;mode&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;PERMISSIVE&quot;&lt;/span&gt;).option(&lt;span class=&quot;code-quote&quot;&gt;&quot;multiline&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&quot;&lt;/span&gt;).schema(schema).json(spark.sparkContext.parallelize([data])).show(truncate=False)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&lt;b&gt;Used packages:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Pyspark==3.4.0&lt;/li&gt;
	&lt;li&gt;python==3.10.0&lt;/li&gt;
	&lt;li&gt;delta-spark==2.4.0&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;br/&gt;
spark_jars=(&lt;br/&gt;
&#160; &quot;org.apache.spark:spark-avro_2.12:3.4.0&quot;&lt;br/&gt;
&#160; &quot;,io.delta:delta-core_2.12:2.4.0&quot;&lt;br/&gt;
&#160; &quot;,com.databricks:spark-xml_2.12:0.16.0&quot;&lt;br/&gt;
)&lt;br/&gt;
&#160;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Expected behaviour&lt;/b&gt;:&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;a&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;b&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;_corrupt_record&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;null&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;null&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;file://{&quot;a&quot;: &quot;incorrect&quot;, &quot;b&quot;: &quot;correct&quot;}&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;\\{&quot;a&quot;: &quot;incorrect&quot;, &quot;b&quot;: &quot;correct&quot;}&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Actual behaviour&lt;/b&gt;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&#160;
*** py4j.protocol.Py4JJavaError: An error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; calling o104.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 4 in stage 2.0 failed 1 times, most recent failure: Lost task 4.0 in stage 2.0 (TID 9) (charlottesmbp2.home executor driver): java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; length 1
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:201)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getAs(rows.scala:35)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get(rows.scala:37)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get$(rows.scala:37)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.get(rows.scala:195)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$2(FailureSafeParser.scala:47)
&#160; &#160; &#160; &#160; at scala.Option.map(Option.scala:230)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$1(FailureSafeParser.scala:47)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:64)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:431)
&#160; &#160; &#160; &#160; at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
&#160; &#160; &#160; &#160; at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
&#160; &#160; &#160; &#160; at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.Task.run(Task.scala:139)
&#160; &#160; &#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
&#160; &#160; &#160; &#160; at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
&#160; &#160; &#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
&#160; &#160; &#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
&#160; &#160; &#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
&#160; &#160; &#160; &#160; at java.base/java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:1589)
Driver stacktrace:
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)
&#160; &#160; &#160; &#160; at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
&#160; &#160; &#160; &#160; at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
&#160; &#160; &#160; &#160; at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)
&#160; &#160; &#160; &#160; at scala.Option.foreach(Option.scala:407)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)
&#160; &#160; &#160; &#160; at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)
&#160; &#160; &#160; &#160; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)
&#160; &#160; &#160; &#160; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)
&#160; &#160; &#160; &#160; at org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4177)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3161)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4167)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4165)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.Dataset.withAction(Dataset.scala:4165)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.Dataset.head(Dataset.scala:3161)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.Dataset.take(Dataset.scala:3382)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.Dataset.getRows(Dataset.scala:284)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.Dataset.showString(Dataset.scala:323)
&#160; &#160; &#160; &#160; at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
&#160; &#160; &#160; &#160; at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:76)
&#160; &#160; &#160; &#160; at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)
&#160; &#160; &#160; &#160; at java.base/java.lang.reflect.Method.invoke(Method.java:578)
&#160; &#160; &#160; &#160; at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
&#160; &#160; &#160; &#160; at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
&#160; &#160; &#160; &#160; at py4j.Gateway.invoke(Gateway.java:282)
&#160; &#160; &#160; &#160; at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
&#160; &#160; &#160; &#160; at py4j.commands.CallCommand.execute(CallCommand.java:79)
&#160; &#160; &#160; &#160; at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
&#160; &#160; &#160; &#160; at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
&#160; &#160; &#160; &#160; at java.base/java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:1589)
Caused by: java.lang.ArrayIndexOutOfBoundsException: Index 1 out of bounds &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; length 1
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.genericGet(rows.scala:201)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.getAs(rows.scala:35)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get(rows.scala:37)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.BaseGenericInternalRow.get$(rows.scala:37)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.GenericInternalRow.get(rows.scala:195)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$2(FailureSafeParser.scala:47)
&#160; &#160; &#160; &#160; at scala.Option.map(Option.scala:230)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.util.FailureSafeParser.$anonfun$toResultRow$1(FailureSafeParser.scala:47)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:64)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.DataFrameReader.$anonfun$json$10(DataFrameReader.scala:431)
&#160; &#160; &#160; &#160; at scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)
&#160; &#160; &#160; &#160; at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)
&#160; &#160; &#160; &#160; at org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)
&#160; &#160; &#160; &#160; at org.apache.spark.rdd.RDD.iterator(RDD.scala:328)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)
&#160; &#160; &#160; &#160; at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)
&#160; &#160; &#160; &#160; at org.apache.spark.scheduler.Task.run(Task.scala:139)
&#160; &#160; &#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)
&#160; &#160; &#160; &#160; at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)
&#160; &#160; &#160; &#160; at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)
&#160; &#160; &#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)
&#160; &#160; &#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)
&#160; &#160; &#160; &#160; ... 1 more &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13540407">SPARK-44079</key>
            <summary>Json reader crashes when a different schema is present</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="fanjia">Jia Fan</assignee>
                                    <reporter username="charlottevdscheun">charlotte van der scheun</reporter>
                        <labels>
                    </labels>
                <created>Fri, 16 Jun 2023 13:20:29 +0000</created>
                <updated>Thu, 29 Jun 2023 13:38:51 +0000</updated>
                            <resolved>Thu, 29 Jun 2023 06:28:28 +0000</resolved>
                                    <version>3.4.0</version>
                                    <fixVersion>3.4.2</fixVersion>
                    <fixVersion>3.5.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="17735199" author="githubbot" created="Tue, 20 Jun 2023 09:15:36 +0000"  >&lt;p&gt;User &apos;Hisoka-X&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/41662&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/41662&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17738374" author="maxgekk" created="Thu, 29 Jun 2023 06:28:28 +0000"  >&lt;p&gt;Issue resolved by pull request 41662&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/41662&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/41662&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 19 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1im28:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>