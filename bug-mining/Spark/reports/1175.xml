<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:21:35 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-2075] Anonymous classes are missing from Spark distribution</title>
                <link>https://issues.apache.org/jira/browse/SPARK-2075</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Running a job built against the Maven dep for 1.0.0 and the hadoop1 distribution produces:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.ClassNotFoundException:
org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here&apos;s what&apos;s in the Maven dep as of 1.0.0:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;jar tvf ~/.m2/repository/org/apache/spark/spark-core_2.10/1.0.0/spark-core_2.10-1.0.0.jar | grep &lt;span class=&quot;code-quote&quot;&gt;&apos;rdd/RDD&apos;&lt;/span&gt; | grep &lt;span class=&quot;code-quote&quot;&gt;&apos;saveAs&apos;&lt;/span&gt;
  1519 Mon May 26 13:57:58 PDT 2014 org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
  1560 Mon May 26 13:57:58 PDT 2014 org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And here&apos;s what&apos;s in the hadoop1 distribution:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;jar tvf spark-assembly-1.0.0-hadoop1.0.4.jar| grep &lt;span class=&quot;code-quote&quot;&gt;&apos;rdd/RDD&apos;&lt;/span&gt; | grep &lt;span class=&quot;code-quote&quot;&gt;&apos;saveAs&apos;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I.e., it&apos;s not there.  It is in the hadoop2 distribution:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;jar tvf spark-assembly-1.0.0-hadoop2.2.0.jar| grep &lt;span class=&quot;code-quote&quot;&gt;&apos;rdd/RDD&apos;&lt;/span&gt; | grep &lt;span class=&quot;code-quote&quot;&gt;&apos;saveAs&apos;&lt;/span&gt;
  1519 Mon May 26 07:29:54 PDT 2014 org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$1.class
  1560 Mon May 26 07:29:54 PDT 2014 org/apache/spark/rdd/RDD$anonfun$saveAsTextFile$2.class
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12719084">SPARK-2075</key>
            <summary>Anonymous classes are missing from Spark distribution</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zsxwing">Shixiong Zhu</assignee>
                                    <reporter username="paulrbrown">Paul R. Brown</reporter>
                        <labels>
                    </labels>
                <created>Sun, 8 Jun 2014 19:44:59 +0000</created>
                <updated>Mon, 22 Dec 2014 10:12:43 +0000</updated>
                            <resolved>Mon, 22 Dec 2014 06:10:28 +0000</resolved>
                                    <version>1.0.0</version>
                                    <fixVersion>1.2.1</fixVersion>
                    <fixVersion>1.3.0</fixVersion>
                                    <component>Build</component>
                    <component>Spark Core</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>14</watches>
                                                                                                                <comments>
                            <comment id="14021465" author="pwendell" created="Sun, 8 Jun 2014 22:11:13 +0000"  >&lt;p&gt;Okay I did some more digging. I think the issue is that the anonymous classes used by saveAsTextFile are not guaranteed to be compiled to the same name every time you compile them in Scala. In the Hadoop 1 build these end up being shortened wheras in the Hadoop 2 build they use the longer names. saveAsTextFile seems to, strangely, be the only affected function. I confirmed this by looking at the difference in the hadoop 1 and 2 jars:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ jar tvf spark-1.0.0-bin-hadoop1/lib/spark-assembly-1.0.0-hadoop*.jar |grep &lt;span class=&quot;code-quote&quot;&gt;&quot;rdd\/RDD\\$&quot;&lt;/span&gt; | awk &lt;span class=&quot;code-quote&quot;&gt;&apos;{ print $8;}&apos;&lt;/span&gt; | sort &amp;gt; hadoop1
$ jar tvf spark-1.0.0-bin-hadoop2/lib/spark-assembly-1.0.0-hadoop*.jar |grep &lt;span class=&quot;code-quote&quot;&gt;&quot;rdd\/RDD\\$&quot;&lt;/span&gt; | awk &lt;span class=&quot;code-quote&quot;&gt;&apos;{ print $8;}&apos;&lt;/span&gt; | sort &amp;gt; hadoop2
$ diff hadoop1 hadoop2
23a24
&amp;gt; org/apache/spark/rdd/RDD$$anonfun$28$$anonfun$apply$13.&lt;span class=&quot;code-keyword&quot;&gt;class
&lt;/span&gt;27,29d27
&amp;lt; org/apache/spark/rdd/RDD$$anonfun$30$$anonfun$apply$13.class
&amp;lt; org/apache/spark/rdd/RDD$$anonfun$30.class
&amp;lt; org/apache/spark/rdd/RDD$$anonfun$31.&lt;span class=&quot;code-keyword&quot;&gt;class
&lt;/span&gt;90a89,90
&amp;gt; org/apache/spark/rdd/RDD$$anonfun$saveAsTextFile$1.class
&amp;gt; org/apache/spark/rdd/RDD$$anonfun$saveAsTextFile$2.class
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This strangely only seems to affect the saveAsTextFile function.&lt;/p&gt;

&lt;p&gt;I&apos;m still a bit confused though because I didn&apos;t think these anonymous classes would show up in the byte code of the user application, so I don&apos;t think it should matter (i.e. this is why Scala probably allows this).&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;javap RDD | grep saveAsText
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void saveAsTextFile(java.lang.&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;);
  &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void saveAsTextFile(java.lang.&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;&amp;lt;? &lt;span class=&quot;code-keyword&quot;&gt;extends&lt;/span&gt; org.apache.hadoop.io.compress.CompressionCodec&amp;gt;);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=paulrbrown&quot; class=&quot;user-hover&quot; rel=&quot;paulrbrown&quot;&gt;paulrbrown&lt;/a&gt; could you explain how you are bundling and submitting your application to the Spark cluster?&lt;/p&gt;</comment>
                            <comment id="14025255" author="paulrbrown" created="Mon, 9 Jun 2014 15:04:53 +0000"  >&lt;p&gt;The job is run by a Java client that connects to the master (using a SparkContext).&lt;/p&gt;

&lt;p&gt;Bundling is performed by a Maven build with two shade plugin invocations, one to package a &quot;driver&quot; uberjar and one to packager a &quot;worker&quot; uberjar.  The worker flavor is sent to the worker nodes, the driver contains the code to connect to the master and run the job.  The Maven build runs against the JAR from Maven Central, and the deployment uses the Spark 1.0.0 hadoop1 download.  (The Spark is staged to S3 once and then downloaded onto master/worker nodes and set up during cluster provisioning.)&lt;/p&gt;

&lt;p&gt;The Maven build uses the usual Scala setup with the library as a dependency and the plugin:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;        &amp;lt;dependency&amp;gt;
            &amp;lt;groupId&amp;gt;org.scala-lang&amp;lt;/groupId&amp;gt;
            &amp;lt;artifactId&amp;gt;scala-library&amp;lt;/artifactId&amp;gt;
            &amp;lt;version&amp;gt;2.10.3&amp;lt;/version&amp;gt;
        &amp;lt;/dependency&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;            &amp;lt;plugin&amp;gt;
                &amp;lt;groupId&amp;gt;net.alchim31.maven&amp;lt;/groupId&amp;gt;
                &amp;lt;artifactId&amp;gt;scala-maven-plugin&amp;lt;/artifactId&amp;gt;
                &amp;lt;executions&amp;gt;
                    &amp;lt;execution&amp;gt;
                        &amp;lt;goals&amp;gt;
                            &amp;lt;goal&amp;gt;compile&amp;lt;/goal&amp;gt;
                            &amp;lt;goal&amp;gt;testCompile&amp;lt;/goal&amp;gt;
                        &amp;lt;/goals&amp;gt;
                    &amp;lt;/execution&amp;gt;
                &amp;lt;/executions&amp;gt;
                &amp;lt;configuration&amp;gt;
                    &amp;lt;scalaVersion&amp;gt;2.10.3&amp;lt;/scalaVersion&amp;gt;
                    &amp;lt;jvmArgs&amp;gt;
                      &amp;lt;jvmArg&amp;gt;-Xms64m&amp;lt;/jvmArg&amp;gt;
                      &amp;lt;jvmArg&amp;gt;-Xmx4096m&amp;lt;/jvmArg&amp;gt;
                    &amp;lt;/jvmArgs&amp;gt;
                &amp;lt;/configuration&amp;gt;
            &amp;lt;/plugin&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14025296" author="paulrbrown" created="Mon, 9 Jun 2014 15:36:29 +0000"  >&lt;p&gt;As food for thought, &lt;a href=&quot;http://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.7.6&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt; is the &lt;tt&gt;InnerClass&lt;/tt&gt; section of the JVM spec.  It looks like there have been some changes from 2.10.3 to 2.10.4 (e.g., &lt;a href=&quot;https://issues.scala-lang.org/browse/SI-6546&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;SI-6546&lt;/a&gt;), but I didn&apos;t dig in.&lt;/p&gt;

&lt;p&gt;I think the thing most likely to work is to ensure that exactly the same bits are used by all of the distributions and posted to Maven Central.  (For some discussion on inner class naming stability, there was quite a bit of it on the Java 8 lambda discussion list, e.g., &lt;a href=&quot;http://mail.openjdk.java.net/pipermail/lambda-spec-experts/2013-July/000316.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this message&lt;/a&gt;.)&lt;/p&gt;</comment>
                            <comment id="14025432" author="pwendell" created="Mon, 9 Jun 2014 17:46:30 +0000"  >&lt;p&gt;I see - so the issue is that your closures are compiled to call that anonymous class in your driver. Then on the cluster the anonymous class is named differently. I wonder if scala can produce stable naming for anonymous classes, that would be the best solution because otherwise it sort of breaks closures. I looked around the compiler flags but I don&apos;t see anything useful.&lt;/p&gt;

&lt;p&gt;One solution here is to see if we can avoid re-compiling spark-core when we make the different builds. The tricky bit though is that if users compile their own downstream version of Spark we&apos;d have to make sure they don&apos;t end up with different versions as well.&lt;/p&gt;</comment>
                            <comment id="14172394" author="dragos" created="Wed, 15 Oct 2014 14:12:51 +0000"  >&lt;p&gt;The Scala compiler produces stable names for anonymous functions. In fact, that&apos;s the reason why the name of the enclosing method is part of the name: so that adding or removing an anonymous function in another method does not change the numbering of the others. Names are assigned by using a per-compilation unit counter and a prefix. Looking at the diff, there&apos;s quite a different picture in the two cases (anonymous functions vs. anonymous classes). Are you sure the two jars are built from the same sources?&lt;/p&gt;

&lt;p&gt;I don&apos;t know how the `assembly` jar is produced, but if it&apos;s using some sort of whole-program analysis and dead-code elimination, it might erroneously remove them. It might help to look at the inputs to the assembly and see if the class is already missing.&lt;/p&gt;

&lt;p&gt;Another possibility is running `scalac -optimize` in only one of the two builds. However, looking at current sources I can&apos;t see why the inliner would remove those closures (the class is not final, and `map` is not final either, so they can&apos;t be resolved and inlined).. &lt;/p&gt;</comment>
                            <comment id="14177748" author="pferrel" created="Tue, 21 Oct 2014 00:55:47 +0000"  >&lt;p&gt;Is there any more on this?&lt;/p&gt;

&lt;p&gt;Building Spark from the 1.1.0 tar for Hadoop 1.2.1--all is well. Trying to upgrade Mahout to use Spark 1.1.0. The Mahout 1.0-snapshot source builds and build tests pass with spark 1.1.0 as a maven dependency. Running the Mahout build on some bigger data using my dev machine as a standalone single node Spark cluster. So the same code is running as executed the build tests, just in single node cluster mode. Also since I built Spark i assume it is using the artifact from my .m2 maven cache, but not 100% on that. Anyway I get the class not found error below.&lt;/p&gt;

&lt;p&gt;I assume the missing function is the anon function passed to the &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    rdd.map(
      {anon function}
    )saveAsTextFile ???? 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;so shouldn&apos;t the function be in the Mahout jar (it isn&apos;t)? Isn&apos;t this function passed in from Mahout so I don&apos;t understand why it matters how Spark was built. &lt;/p&gt;

&lt;p&gt;Several other users are getting this for Spark 1.0.2. If we are doing something wrong in our build process we&apos;d appreciate a pointer.&lt;/p&gt;

&lt;p&gt;Here&apos;s the error I get:&lt;/p&gt;

&lt;p&gt;14/10/20 17:21:36 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 8.0 (TID 16, 192.168.0.2): java.lang.ClassNotFoundException: org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1&lt;br/&gt;
        java.net.URLClassLoader$1.run(URLClassLoader.java:202)&lt;br/&gt;
        java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
        java.net.URLClassLoader.findClass(URLClassLoader.java:190)&lt;br/&gt;
        java.lang.ClassLoader.loadClass(ClassLoader.java:306)&lt;br/&gt;
        java.lang.ClassLoader.loadClass(ClassLoader.java:247)&lt;br/&gt;
        java.lang.Class.forName0(Native Method)&lt;br/&gt;
        java.lang.Class.forName(Class.java:249)&lt;br/&gt;
        org.apache.spark.serializer.JavaDeserializationStream$$anon$1.resolveClass(JavaSerializer.scala:59)&lt;br/&gt;
        java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1591)&lt;br/&gt;
        java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1496)&lt;br/&gt;
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1750)&lt;br/&gt;
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)&lt;br/&gt;
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)&lt;br/&gt;
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)&lt;br/&gt;
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)&lt;br/&gt;
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)&lt;br/&gt;
        java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1970)&lt;br/&gt;
        java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1895)&lt;br/&gt;
        java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1777)&lt;br/&gt;
        java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1329)&lt;br/&gt;
        java.io.ObjectInputStream.readObject(ObjectInputStream.java:349)&lt;br/&gt;
        org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:62)&lt;br/&gt;
        org.apache.spark.serializer.JavaSerializerInstance.deserialize(JavaSerializer.scala:87)&lt;br/&gt;
        org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:57)&lt;br/&gt;
        org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
        org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:177)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)&lt;br/&gt;
        java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)&lt;br/&gt;
        java.lang.Thread.run(Thread.java:695)&lt;/p&gt;
</comment>
                            <comment id="14178350" author="dragos" created="Tue, 21 Oct 2014 12:44:26 +0000"  >&lt;p&gt;The name of the missing class is &lt;tt&gt;org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1&lt;/tt&gt;, so unless Mahout is putting classes inside Spark packages, that class should be somewhere &lt;b&gt;in Spark jars&lt;/b&gt;.&lt;/p&gt;</comment>
                            <comment id="14178688" author="pferrel" created="Tue, 21 Oct 2014 17:30:25 +0000"  >&lt;p&gt;Oops, right. But the function name is being constructed at Mahout build time, right? So the rules for constructing the name are different when building Spark and Mahout OR the function is not being put in the Spark jars?&lt;/p&gt;

&lt;p&gt;Rebuilding yet again so I can&apos;t check the jars just yet. This error was from a clean build of Spark 1.1.0 and Mahout in that sequence. I cleaned the .m2 repo and see that it is filled in only when Mahout is build and is filled in with a repo version of Spark 1.1.0&lt;/p&gt;

&lt;p&gt;Could the problem be related to the fact that Spark as executed on my standalone cluster is from a local build but as Mahout is built it uses the repo version of Spark? Since I am using hadoop 1.2.1 I suspect the repo version may not be exactly what I build.&lt;/p&gt;</comment>
                            <comment id="14178720" author="pferrel" created="Tue, 21 Oct 2014 17:45:30 +0000"  >&lt;p&gt;trying mvn install instead of the documented mvn package to put Spark in the maven cache so that when building Mahout it will get exactly the same bits as will later be run on the cluster--WAG&lt;/p&gt;</comment>
                            <comment id="14178852" author="pferrel" created="Tue, 21 Oct 2014 19:00:16 +0000"  >&lt;p&gt;OK solved. The WAG worked.&lt;/p&gt;

&lt;p&gt;Instead of &apos;mvn package ...&apos; use &apos;mvn install ...&apos; so you&apos;ll get the exact version of Spark needed in your local maven cache at ~/.m2&lt;/p&gt;

&lt;p&gt;This should be changed in the instructions until some way to reliable point to the right version of Spark in the Maven repos is implemented. In my case I had to build Spark to target Hadoop 1.2.1 but the Maven repo was not built for that.&lt;/p&gt;</comment>
                            <comment id="14240741" author="srowen" created="Wed, 10 Dec 2014 06:59:11 +0000"  >&lt;p&gt;It looks like in the end this is a problem caused by mismatching Spark versions at the client/server end, which isn&apos;t supported.&lt;/p&gt;</comment>
                            <comment id="14240838" author="paulrbrown" created="Wed, 10 Dec 2014 09:05:17 +0000"  >&lt;p&gt;It&apos;s not a version of Spark that&apos;s in question; it&apos;s incompatibility between what&apos;s published to Maven Central and what&apos;s offered for download from the Apache site for what is nominally the &lt;b&gt;same&lt;/b&gt; version.&lt;/p&gt;

&lt;p&gt;That said, I don&apos;t object to closing the issue, but I agree with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pferrel&quot; class=&quot;user-hover&quot; rel=&quot;pferrel&quot;&gt;pferrel&lt;/a&gt; above that it&apos;s a documentation issue.  The perfect-world solution would be to publish &lt;b&gt;multiple&lt;/b&gt; Spark artifacts with Maven classifiers identifying the expected backplane.  This issue will continue to afflict people who build Spark applications with Maven Central artifacts and attempt to connect to Spark clusters build against a different backplane.&lt;/p&gt;</comment>
                            <comment id="14240855" author="srowen" created="Wed, 10 Dec 2014 09:33:10 +0000"  >&lt;p&gt;Hm on re-reading more closely, you are right, this is not attributable to version mismatch per se. I think it&apos;s better to leave it open even if I don&apos;t know of any action here. Sorry for the noise.&lt;/p&gt;

&lt;p&gt;The same thing appears to occur in 1.1.1. Maybe this goes away, accidentally, when people all use Hadoop 2.x.&lt;/p&gt;

&lt;p&gt;I don&apos;t think there will be multiple versions deployed to Maven as I think the theory is that the artifacts only exist for the API, and not whatever other libs they happen to link against. This is a leak in that theory though.&lt;/p&gt;</comment>
                            <comment id="14241701" author="pferrel" created="Wed, 10 Dec 2014 20:45:59 +0000"  >&lt;p&gt;If the explanation is correct this needs to be filed against Spark as putting the wrong or not enough artifacts into maven repos. There would need to be a different artifact for every config option that will change internal naming.&lt;/p&gt;

&lt;p&gt;I can&apos;t understand why lots of people aren&apos;t running into this, all it requires is that you link against the repo artifact and run against a user compiled Spark.&lt;/p&gt;</comment>
                            <comment id="14251504" author="sunrui" created="Thu, 18 Dec 2014 11:00:25 +0000"  >&lt;p&gt;I met the same issue. I had a post in the Spark user mailing list but it does not get archived in &lt;a href=&quot;http://apache-spark-user-list.1001560.n3.nabble.com/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-spark-user-list.1001560.n3.nabble.com/&lt;/a&gt;, so I have to describe the issue here:&lt;/p&gt;

&lt;p&gt;Steps to reproduce:&lt;br/&gt;
1.	Download the official pre-built Spark binary 1.1.1 at &lt;a href=&quot;http://d3kbcqa49mib13.cloudfront.net/spark-1.1.1-bin-hadoop1.tgz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://d3kbcqa49mib13.cloudfront.net/spark-1.1.1-bin-hadoop1.tgz&lt;/a&gt; &lt;br/&gt;
2.	Launch the Spark cluster in pseudo cluster mode&lt;br/&gt;
3.	A small scala APP which calls RDD.saveAsObjectFile()&lt;br/&gt;
scalaVersion := &quot;2.10.4&quot;&lt;/p&gt;

&lt;p&gt;libraryDependencies ++= Seq(&lt;br/&gt;
  &quot;org.apache.spark&quot; %% &quot;spark-core&quot; % &quot;1.1.1&quot;&lt;br/&gt;
)&lt;/p&gt;

&lt;p&gt;val sc = new SparkContext(args(0), &quot;test&quot;) //args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt; is the Spark master URI&lt;br/&gt;
  val rdd = sc.parallelize(List(1, 2, 3))&lt;br/&gt;
  rdd.saveAsObjectFile(&quot;/tmp/mysaoftmp&quot;)&lt;br/&gt;
          sc.stop&lt;/p&gt;

&lt;p&gt;throws an exception as follows:&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt; (run-main-0) org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 0.0 failed 4 times, most recent failure: Lost task 1.3 in stage 0.0 (TID 6, ray-desktop.sh.intel.com): java.lang.ClassCastException: scala.Tuple2 cannot be cast to scala.collection.Iterator&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:596)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.rdd.RDD.iterator(RDD.scala:229)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:62)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.scheduler.Task.run(Task.scala:54)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:178)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;error&amp;#93;&lt;/span&gt;         java.lang.Thread.run(Thread.java:701)&lt;/p&gt;

&lt;p&gt;After investigation, I found that this is caused by bytecode incompatibility issue between RDD.class in spark-core_2.10-1.1.1.jar and the pre-built spark assembly respectively.&lt;/p&gt;

&lt;p&gt;This issue also happens with spark 1.1.0.&lt;/p&gt;</comment>
                            <comment id="14251507" author="zsxwing" created="Thu, 18 Dec 2014 11:01:00 +0000"  >&lt;p&gt;Dig deeply and found weird things:&lt;/p&gt;

&lt;p&gt;If I used `mvn -Dhadoop.version=1.2.1 -DskipTests clean package -pl core -am` to compile, the `saveAsTextFile` will be:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;public void saveAsTextFile(java.lang.String);
  Code:
   0:   aload_0
   1:   new     #1577; //class org/apache/spark/rdd/RDD$$anonfun$27
   4:   dup
   5:   aload_0
   6:   invokespecial   #1578; //Method org/apache/spark/rdd/RDD$$anonfun$27.&quot;&amp;lt;init&amp;gt;&quot;:(Lorg/apache/spark/rdd/RDD;)V
   9:   getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   12:  ldc_w   #441; //class scala/Tuple2
   15:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   18:  invokevirtual   #447; //Method map:(Lscala/Function1;Lscala/reflect/ClassTag;)Lorg/apache/spark/rdd/RDD;
   21:  astore_2
   22:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   25:  ldc_w   #1580; //class org/apache/hadoop/io/NullWritable
   28:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   31:  astore_3
   32:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   35:  ldc_w   #1582; //class org/apache/hadoop/io/Text
   38:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   41:  astore  4
   43:  getstatic       #21; //Field org/apache/spark/rdd/RDD$.MODULE$:Lorg/apache/spark/rdd/RDD$;
   46:  aload_2
   47:  invokevirtual   #23; //Method org/apache/spark/rdd/RDD$.rddToPairRDDFunctions$default$4:(Lorg/apache/spark/rdd/RDD;)Lscala/runtime/Null$;
   50:  astore  5
   52:  getstatic       #21; //Field org/apache/spark/rdd/RDD$.MODULE$:Lorg/apache/spark/rdd/RDD$;
   55:  aload_2
   56:  aload_3
   57:  aload   4
   59:  aload   5
   61:  pop
   62:  aconst_null
   63:  invokevirtual   #47; //Method org/apache/spark/rdd/RDD$.rddToPairRDDFunctions:(Lorg/apache/spark/rdd/RDD;Lscala/reflect/ClassTag;Lscala/reflect/ClassTag;Lscala/math/Ordering;)Lorg/apache/spark/rdd/PairRDDFunctions;
   66:  aload_1
   67:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   70:  ldc_w   #1584; //class org/apache/hadoop/mapred/TextOutputFormat
   73:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   76:  invokevirtual   #1588; //Method org/apache/spark/rdd/PairRDDFunctions.saveAsHadoopFile:(Ljava/lang/String;Lscala/reflect/ClassTag;)V
   79:  return
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If I used `mvn -Pyarn -Phadoop-2.2 -Dhadoop.version=2.2.0 -DskipTests clean package -pl core -am` to compile, the `saveAsTextFile` is different:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;public void saveAsTextFile(java.lang.String);
  Code:
   0:   getstatic       #21; //Field org/apache/spark/rdd/RDD$.MODULE$:Lorg/apache/spark/rdd/RDD$;
   3:   aload_0
   4:   new     #1577; //class org/apache/spark/rdd/RDD$$anonfun$saveAsTextFile$1
   7:   dup
   8:   aload_0
   9:   invokespecial   #1578; //Method org/apache/spark/rdd/RDD$$anonfun$saveAsTextFile$1.&quot;&amp;lt;init&amp;gt;&quot;:(Lorg/apache/spark/rdd/RDD;)V
   12:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   15:  ldc_w   #441; //class scala/Tuple2
   18:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   21:  invokevirtual   #447; //Method map:(Lscala/Function1;Lscala/reflect/ClassTag;)Lorg/apache/spark/rdd/RDD;
   24:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   27:  ldc_w   #1580; //class org/apache/hadoop/io/NullWritable
   30:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   33:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   36:  ldc_w   #1582; //class org/apache/hadoop/io/Text
   39:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   42:  getstatic       #1587; //Field scala/math/Ordering$.MODULE$:Lscala/math/Ordering$;
   45:  getstatic       #471; //Field scala/Predef$.MODULE$:Lscala/Predef$;
   48:  invokevirtual   #1591; //Method scala/Predef$.conforms:()Lscala/Predef$$less$colon$less;
   51:  invokevirtual   #1595; //Method scala/math/Ordering$.ordered:(Lscala/Function1;)Lscala/math/Ordering;
   54:  invokevirtual   #47; //Method org/apache/spark/rdd/RDD$.rddToPairRDDFunctions:(Lorg/apache/spark/rdd/RDD;Lscala/reflect/ClassTag;Lscala/reflect/ClassTag;Lscala/math/Ordering;)Lorg/apache/spark/rdd/PairRDDFunctions;
   57:  aload_1
   58:  getstatic       #439; //Field scala/reflect/ClassTag$.MODULE$:Lscala/reflect/ClassTag$;
   61:  ldc_w   #1597; //class org/apache/hadoop/mapred/TextOutputFormat
   64:  invokevirtual   #445; //Method scala/reflect/ClassTag$.apply:(Ljava/lang/Class;)Lscala/reflect/ClassTag;
   67:  invokevirtual   #1601; //Method org/apache/spark/rdd/PairRDDFunctions.saveAsHadoopFile:(Ljava/lang/String;Lscala/reflect/ClassTag;)V
   70:  return
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: in hadoop 1.2.1, saveAsTextFile use the default `Ordering` value `null`, while in hadoop 2.2.0, saveAsTextFile will use `Ordering.ordered` to create a new `Ordering`.&lt;/p&gt;</comment>
                            <comment id="14251519" author="sunrui" created="Thu, 18 Dec 2014 11:11:04 +0000"  >&lt;p&gt;I think we need to address this issue, at least we need to document it.&lt;/p&gt;

&lt;p&gt;In order to guarrantee the binary compatibility, I think we may enforce the release steps to build the assembly as follows for an official release:&lt;br/&gt;
1. Build all module jars except the assembly module&lt;br/&gt;
2. Publish all modules jars to the central maven repository&lt;br/&gt;
3. Then build the assembly module to assemble the module jars into the final assembly by depending on modular jars in the maven repo&lt;br/&gt;
4. May also push the assembly jar to the central maven repository&lt;/p&gt;</comment>
                            <comment id="14251524" author="srowen" created="Thu, 18 Dec 2014 11:16:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt; I don&apos;t expect the byte code to be identical when compiled against different versions of the underlying library, not necessarily. That&apos;s not quite the issue, but rather how these differences change the naming of anonymous functions. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunrui&quot; class=&quot;user-hover&quot; rel=&quot;sunrui&quot;&gt;sunrui&lt;/a&gt; I don&apos;t think that&apos;s quite the issue either. The assembly &lt;b&gt;is&lt;/b&gt; made from the module JARs. All are published to Maven Central; please search &lt;a href=&quot;http://search.maven.org/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://search.maven.org/&lt;/a&gt; . &lt;/p&gt;</comment>
                            <comment id="14251534" author="sunrui" created="Thu, 18 Dec 2014 11:26:50 +0000"  >&lt;p&gt;Owen, if the official assembly is made from the exact published jars, then the binaries should be exactly the same, why we met this issue? You can see that the RDD.class from the maven spark-core 1.1.0 is different from that in the Spark pre-built 1.1.0 assembly (they have different file size, and in-compatible bytecode ).&lt;/p&gt;

&lt;p&gt;So I doubt there is some mistake in the release step of Spark. &lt;/p&gt;</comment>
                            <comment id="14251582" author="srowen" created="Thu, 18 Dec 2014 12:12:54 +0000"  >&lt;p&gt;(Sean) But my understanding is that you are comparing to binaries compiled for a potentially different underlying Hadoop distribution. That is the issue at hand here.&lt;/p&gt;</comment>
                            <comment id="14252053" author="shivaram" created="Thu, 18 Dec 2014 19:03:24 +0000"  >&lt;p&gt;So just to make sure I understand things correctly, is it the case that the jar published to maven (spark-core-1.1.1) is built using Hadoop2 dependencies while the Hadoop1 assembly jar that is distributed is built using Hadoop 1 (obviously...) ?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; While I see that we officially support submitting jobs using spark-submit, it is surprising to me that other deployment methods would fail this way (from the user&apos;s perspective the Spark versions presumably at compile time and run time presumably match up ?). We should at the very least document this, but it would also be good to see if there is a work around.&lt;/p&gt;</comment>
                            <comment id="14252194" author="srowen" created="Thu, 18 Dec 2014 20:31:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shivaram&quot; class=&quot;user-hover&quot; rel=&quot;shivaram&quot;&gt;shivaram&lt;/a&gt; No, I don&apos;t think that&apos;s the case. Certainly not Hadoop 1 vs 2. Here is how the downloadable distros are built:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;make_binary_release &lt;span class=&quot;code-quote&quot;&gt;&quot;hadoop1&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;-Phive -Phive-thriftserver -Dhadoop.version=1.0.4&quot;&lt;/span&gt; &amp;amp;
make_binary_release &lt;span class=&quot;code-quote&quot;&gt;&quot;hadoop1-scala2.11&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;-Phive -Dscala-2.11&quot;&lt;/span&gt; &amp;amp;
make_binary_release &lt;span class=&quot;code-quote&quot;&gt;&quot;cdh4&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;-Phive -Phive-thriftserver -Dhadoop.version=2.0.0-mr1-cdh4.2.0&quot;&lt;/span&gt; &amp;amp;
make_binary_release &lt;span class=&quot;code-quote&quot;&gt;&quot;hadoop2.3&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;-Phadoop-2.3 -Phive -Phive-thriftserver -Pyarn&quot;&lt;/span&gt; &amp;amp;
make_binary_release &lt;span class=&quot;code-quote&quot;&gt;&quot;hadoop2.4&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;-Phadoop-2.4 -Phive -Phive-thriftserver -Pyarn&quot;&lt;/span&gt; &amp;amp;
make_binary_release &lt;span class=&quot;code-quote&quot;&gt;&quot;mapr3&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;-Pmapr3 -Phive -Phive-thriftserver&quot;&lt;/span&gt; &amp;amp;
make_binary_release &lt;span class=&quot;code-quote&quot;&gt;&quot;mapr4&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;-Pmapr4 -Pyarn -Phive -Phive-thriftserver&quot;&lt;/span&gt; &amp;amp;
make_binary_release &lt;span class=&quot;code-quote&quot;&gt;&quot;hadoop2.4-without-hive&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;-Phadoop-2.4 -Pyarn&quot;&lt;/span&gt; &amp;amp;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A default &lt;tt&gt;mvn release&lt;/tt&gt; would use &lt;tt&gt;hadoop.version=1.0.4&lt;/tt&gt;. Somebody can correct me if this isn&apos;t the case, but I assume that this is what goes to Maven Central.&lt;/p&gt;

&lt;p&gt;Of course, this behavior is not desirable and not by design, and worth a mention. As far as I can tell it has only been observed arising in Hadoop 1 vs Hadoop 2-compiled artifacts. Although the intended public API is identical, it&apos;s not actually 100% compatible. Ideally you could get away with using any copy of the public API. &lt;/p&gt;

&lt;p&gt;In these particular cases, the safe practice of always harmonizing binaries on client and server is actually necessary, which is no terrible thing I think. Nothing about this requires using &lt;tt&gt;spark-submit&lt;/tt&gt;, although, that&apos;s a way to make sure you&apos;re using the same Spark in your app and cluster.&lt;/p&gt;</comment>
                            <comment id="14252360" author="shivaram" created="Thu, 18 Dec 2014 21:57:23 +0000"  >&lt;p&gt;Hmm &amp;#8211; looking at the release steps it looks like the release on maven should be from Hadoop 1.0.4 &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pwendell&quot; class=&quot;user-hover&quot; rel=&quot;pwendell&quot;&gt;pwendell&lt;/a&gt; or &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor14&quot; class=&quot;user-hover&quot; rel=&quot;andrewor14&quot;&gt;andrewor14&lt;/a&gt; might be able to throw more light on this. (BTW I wonder if we can trace the source of this mismatch for the case reported by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunrui&quot; class=&quot;user-hover&quot; rel=&quot;sunrui&quot;&gt;sunrui&lt;/a&gt; where the distribution with Hadoop1 of Spark 1.1.1 doesn&apos;t work with the Maven central jar)&lt;/p&gt;

&lt;p&gt;I see your high level point that this is not about spark-submit per se, but about having the exact same binary on the server and as a compile-time dependency. Its just unfortunate that having the same Spark version number isn&apos;t sufficient. Also is the workaround right now to rebuild Spark from source using `make-distribution`, do `mvn install`, rebuild the application and deploy Spark using the assembly jar ?&lt;/p&gt;</comment>
                            <comment id="14252812" author="apachespark" created="Fri, 19 Dec 2014 02:35:25 +0000"  >&lt;p&gt;User &apos;zsxwing&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/3740&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3740&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14252906" author="zsxwing" created="Fri, 19 Dec 2014 04:21:43 +0000"  >&lt;p&gt;Please take a look at my PR&lt;/p&gt;</comment>
                            <comment id="14252979" author="sunrui" created="Fri, 19 Dec 2014 05:32:40 +0000"  >&lt;p&gt;Since `mvn release` is built for Hadoop 1.0.4, I don&apos;t understand the reason why there is difference in RDD.class bytecode from mvn spark-core and the pre-built binary for Hadoop 1.x, because they are both built for Hadoop 1.x and has the same version 1.1.0.&lt;/p&gt;

&lt;p&gt;According to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt;&apos;s PR, it seems that it&apos;s diffcult to guanranttee same bytecode for Hadoop1.x and 2.x. So maybe we need to pubish two versions of a module to mvn, one is for Hadoop 1.x, the other is for Hadoop 2.x, for example, spark-core_2.10-1.1.0-hadoop1.jar and spark-core_2.10-1.1.0-hadoop2.jar?&lt;/p&gt;</comment>
                            <comment id="14253203" author="srowen" created="Fri, 19 Dec 2014 10:05:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunrui&quot; class=&quot;user-hover&quot; rel=&quot;sunrui&quot;&gt;sunrui&lt;/a&gt; From digging in to the various reports of this issue, it seemed to me that in each case the Hadoop version did not match. That is, I do not know that it&apos;s true that the issue manifests when the Hadoop version matches; that would indeed be strange. I could have missed it; this is a bit hard to follow. But do you see evidence of this?&lt;/p&gt;

&lt;p&gt;I don&apos;t think publishing two versions fixes anything, really. The PR might get at the heart of the difference here and resolve it for real. It doesn&apos;t happen if you match binaries, which is good practice anyway.&lt;/p&gt;</comment>
                            <comment id="14254720" author="sunrui" created="Sat, 20 Dec 2014 13:07:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; I assume that mvn jars were built for Hadoop 1.x as you said that &quot;A default mvn release would use hadoop.version=1.0.4. Somebody can correct me if this isn&apos;t the case, but I assume that this is what goes to Maven Central.&quot; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; so it is very possible that mvn jars were actually built for Hadoop 2.x, which is the cause for this issue.&lt;/p&gt;

&lt;p&gt;Yes, I agree that we should using match binaries. The problem is that I thought I was using the matching binaries but actually not. So we can &lt;br/&gt;
1. For exising releases,  document the fact that mvn jars are for Hadoop 2.x. If app depends on mvn jars, it should be       used with Hadoop 2.x. If the app is intended to work with Hadoop 1.x, one way is to rebuild Spark source against Hadoop 1.x and publish the module jars to local mvn repo.&lt;br/&gt;
2. For futrure release, we may eliminate the Spark core bytecode incompatibility between Hadoop1.x and 2.x just as Shixong&apos;s PR is trying to do. &lt;/p&gt;</comment>
                            <comment id="14255115" author="srowen" created="Sun, 21 Dec 2014 10:41:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunrui&quot; class=&quot;user-hover&quot; rel=&quot;sunrui&quot;&gt;sunrui&lt;/a&gt; You are asking if the Maven Central artifacts are built for Hadoop 2? No, why do you say that? It is not true that you need to use Hadoop 2, or need to build a custom version. What you should do ideally is match the version you compile against the version you deploy against &amp;#8211; good practice, even if in reality it should not be so strict. &lt;/p&gt;

&lt;p&gt;But the best outcome indeed is to allow compiling against any artifact, and having it work against any build of the same public API version, no matter what the &apos;backend&apos;. That was intended behavior and hopefully the PRs here get at the nature of the problem.&lt;/p&gt;</comment>
                            <comment id="14255356" author="sunrui" created="Mon, 22 Dec 2014 02:06:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; Yes, I totally agree with you  matching version between compilation and depolyment. That&apos;s my point also. What I am confused is that an app depends on maven spark-core_2.10-1.1.0.jar (suppose it is built for Hadoop 1.x as you said by default is for Hadoop 1.x for mvn release) does not match the spark-core included in the Spark pre-built binary 1.1.0 for hadoop 1.x. I suppose that Spark pre-built binary 1.1.0 for Hadoop 1.x be assembled from MVN jars, so there should be no mismatching, Am I right? If I am wrong, could anyone tell me the reason of the binary mismatch and tell me the best practice for me to solve my problem?&lt;/p&gt;
</comment>
                            <comment id="14255364" author="shivaram" created="Mon, 22 Dec 2014 02:30:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunrui&quot; class=&quot;user-hover&quot; rel=&quot;sunrui&quot;&gt;sunrui&lt;/a&gt; What I can see from this JIRA discussion (and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; please correct me if I am wrong) is that Hadoop 1 vs. Hadoop 2 is one of the causes of incompatibility. It is &lt;em&gt;not the only&lt;/em&gt; reason and I don&apos;t think we exactly know why the pre-built binary for 1.1.0 is different from the maven version.  &lt;/p&gt;

&lt;p&gt;I think the best practice advice is to use the exact same jar in the application and in the runtime. Marking Spark a provided dependency in the application build and using spark-submit is one way of achieving this. Or one can publish a local build to maven and use the same local build to start the cluster etc.&lt;/p&gt;</comment>
                            <comment id="14255477" author="apachespark" created="Mon, 22 Dec 2014 06:21:08 +0000"  >&lt;p&gt;User &apos;zsxwing&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/3758&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3758&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14255606" author="srowen" created="Mon, 22 Dec 2014 10:12:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sunrui&quot; class=&quot;user-hover&quot; rel=&quot;sunrui&quot;&gt;sunrui&lt;/a&gt; Yes, but I am still not clear that anyone has observed the problem with two binaries built for the same version of Hadoop. Most of the situations listed here do not match that description. I might not understand someone&apos;s issue report here. In any event, it sounds like an underlying cause has been fixed already anyway.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12723902">SPARK-2292</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>397283</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 48 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1wecn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>397410</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>