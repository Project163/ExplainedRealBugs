<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:46:26 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-16522] [MESOS] Spark application throws exception on exit</title>
                <link>https://issues.apache.org/jira/browse/SPARK-16522</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Spark applications running on Mesos throw exception upon exit as follows:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;16/07/13 15:20:46 WARN NettyRpcEndpointRef: Error sending message [message = RemoveExecutor(1,Executor finished with state FINISHED)] in 3 attempts
org.apache.spark.SparkException: Exception thrown in awaitResult
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:412)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.executorTerminated(MesosCoarseGrainedSchedulerBackend.scala:555)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.statusUpdate(MesosCoarseGrainedSchedulerBackend.scala:495)
Caused by: org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:127)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	... 4 more
Exception in thread &quot;Thread-47&quot; org.apache.spark.SparkException: Error notifying standalone scheduler&apos;s driver endpoint
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:415)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.executorTerminated(MesosCoarseGrainedSchedulerBackend.scala:555)
	at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.statusUpdate(MesosCoarseGrainedSchedulerBackend.scala:495)
Caused by: org.apache.spark.SparkException: Error sending message [message = RemoveExecutor(1,Executor finished with state FINISHED)]
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:119)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:412)
	... 2 more
Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
	... 4 more
Caused by: org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:127)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
	at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
	... 4 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Applications&apos; result is not affected by this error.&lt;/p&gt;

&lt;p&gt;This issue can be simply reproduced by launching a spark-shell, and exit after running the following commands:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val rdd = sc.parallelize(1 to 10, 10)
rdd.map { _ + 1} collect
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The root cause is that in SparkContext.stop(), MesosCoarseGrainedSchedulerBackend.stop() calls CoarseGrainedSchedulerBackend.stop(). The latter sends messages to stop executors and also stop the driver endpoint without waiting for the actual stop of executors. MesosCoarseGrainedSchedulerBackend.stop() still waits for the executors to stop in a timeout. During the wait, MesosCoarseGrainedSchedulerBackend.statusUpdate() generally will be called to update executors&apos; status, and in turn removeExecutor() is called. But at that time, the driver endpoint is not available.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12989028">SPARK-16522</key>
            <summary>[MESOS] Spark application throws exception on exit</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sunrui">Sun Rui</assignee>
                                    <reporter username="sunrui">Sun Rui</reporter>
                        <labels>
                    </labels>
                <created>Wed, 13 Jul 2016 07:35:18 +0000</created>
                <updated>Mon, 31 Oct 2016 18:36:27 +0000</updated>
                            <resolved>Wed, 10 Aug 2016 09:01:46 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.1</fixVersion>
                    <fixVersion>2.1.0</fixVersion>
                                    <component>Mesos</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="15374583" author="jerryshao" created="Wed, 13 Jul 2016 08:19:19 +0000"  >&lt;p&gt;Perhaps there&apos;s race condition when exiting the Spark application, I remember there&apos;s a similar JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-12967&quot; title=&quot;NettyRPC races with SparkContext.stop() and throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-12967&quot;&gt;&lt;del&gt;SPARK-12967&lt;/del&gt;&lt;/a&gt; to handle it, not sure it is the exactly same scenario.&lt;/p&gt;</comment>
                            <comment id="15374757" author="sunrui" created="Wed, 13 Jul 2016 09:57:59 +0000"  >&lt;p&gt;I checked &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-12967&quot; title=&quot;NettyRPC races with SparkContext.stop() and throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-12967&quot;&gt;&lt;del&gt;SPARK-12967&lt;/del&gt;&lt;/a&gt;, this issue is different. &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-12967&quot; title=&quot;NettyRPC races with SparkContext.stop() and throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-12967&quot;&gt;&lt;del&gt;SPARK-12967&lt;/del&gt;&lt;/a&gt; is for the case that the RpcEnv stopped, but this issue is for the case where a specific endpoint stopped, more specifically, the driver endpoint. When this issue happens, the RpcEnv is still alive.&lt;/p&gt;</comment>
                            <comment id="15374803" author="apachespark" created="Wed, 13 Jul 2016 10:31:07 +0000"  >&lt;p&gt;User &apos;sun-rui&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14175&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14175&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15376125" author="mgummelt" created="Thu, 14 Jul 2016 01:12:27 +0000"  >&lt;p&gt;I&apos;ve seen some stack traces recently that might have been this.  I&apos;m trying to repro now.  Will get back to you.  Which commit/tag are you running?&lt;/p&gt;</comment>
                            <comment id="15376169" author="sunrui" created="Thu, 14 Jul 2016 01:53:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mgummelt&quot; class=&quot;user-hover&quot; rel=&quot;mgummelt&quot;&gt;mgummelt&lt;/a&gt; The commit is e50efd53f073890d789a8448f850cc219cca7708&lt;/p&gt;</comment>
                            <comment id="15377353" author="mgummelt" created="Thu, 14 Jul 2016 17:43:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; I&apos;m going to look into this now and resolve it today.  Can you hold off on the next 2.0 RC until this is resolved?  It&apos;s likely a major bug.&lt;/p&gt;</comment>
                            <comment id="15377361" author="srowen" created="Thu, 14 Jul 2016 17:46:53 +0000"  >&lt;p&gt;I&apos;m not the release guy, but I also saw that RC3 looks like it has already been tagged. Something tells me it won&apos;t be the last though&lt;/p&gt;</comment>
                            <comment id="15378150" author="rxin" created="Thu, 14 Jul 2016 19:06:57 +0000"  >&lt;p&gt;Does this fail the job? If it doesn&apos;t fail the job and simply prints warnings and logs exceptions that it wouldn&apos;t be a release blocker.&lt;/p&gt;</comment>
                            <comment id="15378178" author="mgummelt" created="Thu, 14 Jul 2016 19:23:56 +0000"  >&lt;p&gt;I don&apos;t think so.  Please give me a couple hours to investigate further, though.&lt;/p&gt;</comment>
                            <comment id="15380058" author="rxin" created="Fri, 15 Jul 2016 20:31:52 +0000"  >&lt;p&gt;Any update?&lt;/p&gt;</comment>
                            <comment id="15380130" author="mgummelt" created="Fri, 15 Jul 2016 21:17:37 +0000"  >&lt;p&gt;This shouldn&apos;t affect functionality&lt;/p&gt;</comment>
                            <comment id="15409502" author="laurentcoder" created="Fri, 5 Aug 2016 14:18:00 +0000"  >&lt;p&gt;Is this gonna be fixed soon (2.0.1) ?!&lt;br/&gt;
I just got a stacktrace of a developer here, who&apos;s job &lt;b&gt;failed&lt;/b&gt; .. with following exception&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;2016-08-05 05:43:35,186 WARN  org.apache.spark.rpc.netty.NettyRpcEndpointRef: Error sending message [message = RemoveExecutor(5,Executor finished with state FINISHED)] in 1 attempts
org.apache.spark.SparkException: Exception thrown in awaitResult
        at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:77)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$1.applyOrElse(RpcTimeout.scala:75)
        at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
        at org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:59)
        at scala.PartialFunction$OrElse.apply(PartialFunction.scala:167)
        at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:83)
        at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
        at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:412)
        at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.executorTerminated(MesosCoarseGrainedSchedulerBackend.scala:555)
        at org.apache.spark.scheduler.cluster.mesos.MesosCoarseGrainedSchedulerBackend.statusUpdate(MesosCoarseGrainedSchedulerBackend.scala:495)
Caused by: org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
        at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:152)
        at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:127)
        at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
        at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:508)
        at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:101)
        ... 4 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15409514" author="sunrui" created="Fri, 5 Aug 2016 14:24:02 +0000"  >&lt;p&gt;sure. The PR is under review&lt;/p&gt;</comment>
                            <comment id="15409556" author="mgummelt" created="Fri, 5 Aug 2016 15:00:57 +0000"  >&lt;p&gt;How did the job fail?  Return non-zero exit code?  I think this stack trace is thrown in a non-main thread&lt;/p&gt;</comment>
                            <comment id="15413209" author="srowen" created="Tue, 9 Aug 2016 08:42:35 +0000"  >&lt;p&gt;Resolved by &lt;a href=&quot;https://github.com/apache/spark/pull/14175&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14175&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15414024" author="mgummelt" created="Tue, 9 Aug 2016 18:44:37 +0000"  >&lt;p&gt;Reopening so we can track this until it&apos;s merged into the 2.0 branch.&lt;/p&gt;

&lt;p&gt;Also changed the fix version to 2.0.1&lt;/p&gt;</comment>
                            <comment id="15414044" author="srowen" created="Tue, 9 Aug 2016 18:58:43 +0000"  >&lt;p&gt;OK, I think it&apos;s more accurate to leave it Fixed for 2.1.0, and then also add 2.0.1 later if it&apos;s also fixed for that version, but it won&apos;t matter soon.&lt;/p&gt;</comment>
                            <comment id="15414716" author="apachespark" created="Wed, 10 Aug 2016 05:24:04 +0000"  >&lt;p&gt;User &apos;sun-rui&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14575&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14575&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15414960" author="srowen" created="Wed, 10 Aug 2016 09:01:46 +0000"  >&lt;p&gt;Issue resolved by pull request 14575&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14575&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14575&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15620225" author="harishk15" created="Sun, 30 Oct 2016 16:51:06 +0000"  >&lt;p&gt;I am getting same error in spark 2.0.2 snapshot. Standalone submission.&lt;br/&gt;
 py4j.protocol.Py4JJavaError: An error occurred while calling o37785.count.&lt;br/&gt;
: org.apache.spark.SparkException: Exception thrown in awaitResult: &lt;br/&gt;
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:194)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.doExecuteBroadcast(BroadcastExchangeExec.scala:120)&lt;br/&gt;
	at org.apache.spark.sql.execution.InputAdapter.doExecuteBroadcast(WholeStageCodegenExec.scala:229)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:125)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeBroadcast$1.apply(SparkPlan.scala:125)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeBroadcast(SparkPlan.scala:124)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.prepareBroadcast(BroadcastHashJoinExec.scala:98)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:197)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:30)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:62)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.consume(BroadcastHashJoinExec.scala:38)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.codegenInner(BroadcastHashJoinExec.scala:232)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doConsume(BroadcastHashJoinExec.scala:82)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.consume(basicPhysicalOperators.scala:30)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:62)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)&lt;br/&gt;
	at org.apache.spark.sql.execution.FilterExec.consume(basicPhysicalOperators.scala:79)&lt;br/&gt;
	at org.apache.spark.sql.execution.FilterExec.doConsume(basicPhysicalOperators.scala:194)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.consume(WholeStageCodegenExec.scala:153)&lt;br/&gt;
	at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:218)&lt;br/&gt;
	at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:244)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:218)&lt;br/&gt;
	at org.apache.spark.sql.execution.FilterExec.doProduce(basicPhysicalOperators.scala:113)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.FilterExec.produce(basicPhysicalOperators.scala:79)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:40)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:30)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:40)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:30)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.doProduce(BroadcastHashJoinExec.scala:77)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.BroadcastHashJoinExec.produce(BroadcastHashJoinExec.scala:38)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:40)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:83)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$$anonfun$produce$1.apply(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.CodegenSupport$class.produce(WholeStageCodegenExec.scala:78)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:30)&lt;br/&gt;
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:309)&lt;br/&gt;
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:347)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:471)&lt;br/&gt;
	at org.apache.spark.sql.execution.UnionExec$$anonfun$doExecute$1.apply(basicPhysicalOperators.scala:471)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;br/&gt;
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)&lt;br/&gt;
	at scala.collection.immutable.List.foreach(List.scala:381)&lt;br/&gt;
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)&lt;br/&gt;
	at scala.collection.immutable.List.map(List.scala:285)&lt;br/&gt;
	at org.apache.spark.sql.execution.UnionExec.doExecute(basicPhysicalOperators.scala:471)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:123)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)&lt;br/&gt;
	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:111)&lt;br/&gt;
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.InputAdapter.doExecute(WholeStageCodegenExec.scala:225)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.joins.SortMergeJoinExec.inputRDDs(SortMergeJoinExec.scala:325)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:36)&lt;br/&gt;
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:123)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)&lt;br/&gt;
	at org.apache.spark.sql.execution.SortExec.inputRDDs(SortExec.scala:111)&lt;br/&gt;
	at org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:36)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:138)&lt;br/&gt;
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:123)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:138)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:138)&lt;br/&gt;
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.prepareShuffleDependency(ShuffleExchange.scala:87)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:123)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange$$anonfun$doExecute$1.apply(ShuffleExchange.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)&lt;br/&gt;
	at org.apache.spark.sql.execution.exchange.ShuffleExchange.doExecute(ShuffleExchange.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:233)&lt;br/&gt;
	at org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:138)&lt;br/&gt;
	at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:361)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.getByteArrayRdd(SparkPlan.scala:240)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:287)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$org$apache$spark$sql$Dataset$$execute$1$1.apply(Dataset.scala:2193)&lt;br/&gt;
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:57)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.withNewExecutionId(Dataset.scala:2546)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$execute$1(Dataset.scala:2192)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collect(Dataset.scala:2199)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2227)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$$anonfun$count$1.apply(Dataset.scala:2226)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.withCallback(Dataset.scala:2559)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.count(Dataset.scala:2226)&lt;br/&gt;
	at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:498)&lt;br/&gt;
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:237)&lt;br/&gt;
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)&lt;br/&gt;
	at py4j.Gateway.invoke(Gateway.java:280)&lt;br/&gt;
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)&lt;br/&gt;
	at py4j.commands.CallCommand.execute(CallCommand.java:79)&lt;br/&gt;
	at py4j.GatewayConnection.run(GatewayConnection.java:214)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
Caused by: java.util.concurrent.TimeoutException: Futures timed out after &lt;span class=&quot;error&quot;&gt;&amp;#91;1200 seconds&amp;#93;&lt;/span&gt;&lt;br/&gt;
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:219)&lt;br/&gt;
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)&lt;br/&gt;
	at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)&lt;br/&gt;
	at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)&lt;br/&gt;
	at scala.concurrent.Await$.result(package.scala:190)&lt;br/&gt;
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:190)&lt;br/&gt;
	... 224 more&lt;/p&gt;

&lt;p&gt;16/10/30 16:32:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerStageCompleted(org.apache.spark.scheduler.StageInfo@2079c34a)&lt;br/&gt;
16/10/30 16:32:59 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerJobEnd(121,1477845179288,JobFailed(org.apache.spark.SparkException: Job 121 cancelled because SparkContext was shut down))&lt;/p&gt;</comment>
                            <comment id="15622992" author="mgummelt" created="Mon, 31 Oct 2016 18:36:27 +0000"  >&lt;p&gt;This JIRA was for a bug in Mesos.  If you&apos;re getting this error w/ Standalone, it&apos;s likely a different bug, and you should submit a separate JIRA.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 3 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i30wi7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>