<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:36:12 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-8337] KafkaUtils.createDirectStream for python is lacking API/feature parity with the Scala/Java version</title>
                <link>https://issues.apache.org/jira/browse/SPARK-8337</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;See the following thread for context.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Spark-1-4-Python-API-for-getting-Kafka-offsets-in-direct-mode-tt12714.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Spark-1-4-Python-API-for-getting-Kafka-offsets-in-direct-mode-tt12714.html&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12837549">SPARK-8337</key>
            <summary>KafkaUtils.createDirectStream for python is lacking API/feature parity with the Scala/Java version</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="11">Done</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="aramesh">Amit Ramesh</reporter>
                        <labels>
                    </labels>
                <created>Sat, 13 Jun 2015 00:22:29 +0000</created>
                <updated>Thu, 7 Sep 2017 06:40:57 +0000</updated>
                            <resolved>Thu, 13 Oct 2016 09:26:47 +0000</resolved>
                                    <version>1.4.0</version>
                                                    <component>DStreams</component>
                    <component>PySpark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                                                            <comments>
                            <comment id="14588064" author="juanrh" created="Tue, 16 Jun 2015 13:59:43 +0000"  >&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I&apos;ve made some advances. Due to the limited support for data types in pyspark and org.apache.spark.api.python.PythonRDD, I think adding a function to createDirectStream from MessageAndMetadata to arbitrary values is not such a good idea. In fact currently pyspark communicates with the Scala API by using JavaPairInputDStream[Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;, Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;] and then decoding those arrays of bytes in python. So what I propose is adding an argument to choose between returning a dstream of (key, value) like it is done so far, and a dstream of dictionaries with entries for the key, the value (the message), and also the topic, partition and offset. An approximation to that is implemented in &lt;a href=&quot;https://github.com/juanrh/spark/commit/7a824a814f56f839d2f3fbeda7e9f7467e683c6e&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/juanrh/spark/commit/7a824a814f56f839d2f3fbeda7e9f7467e683c6e&lt;/a&gt; as a python static method  KafkaUtils.createDirectStreamJ, that uses KafkaUtilsPythonHelper.createDirectStreamJ. The following Python code can be used for using it:&lt;/p&gt;

&lt;p&gt;from pyspark.streaming import StreamingContext&lt;br/&gt;
from pyspark.streaming.kafka import KafkaUtils&lt;br/&gt;
ssc = StreamingContext(sc, 1)&lt;br/&gt;
topics = &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;test&amp;quot;&amp;#93;&lt;/span&gt;&lt;br/&gt;
kafkaParams = &lt;/p&gt;
{&quot;metadata.broker.list&quot; : &quot;localhost:9092&quot;}
&lt;p&gt;kafkaStream = KafkaUtils.createDirectStreamJ(ssc, topics, kafkaParams)&lt;br/&gt;
kafkaStream.pprint()&lt;br/&gt;
ssc.start()&lt;br/&gt;
ssc.awaitTermination(timeout=5)&lt;/p&gt;

&lt;p&gt;which gets the following output&lt;br/&gt;
....&lt;br/&gt;
15/06/16 15:31:00 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool&lt;br/&gt;
15/06/16 15:31:00 INFO DAGScheduler: ResultStage 8 (start at NativeMethodAccessorImpl.java:-2) finished&lt;br/&gt;
15/06/16 15:31:00 INFO DAGScheduler: Job 8 finished: start at NativeMethodAccessorImpl.java:-2, took 0,0&lt;br/&gt;
-------------------------------------------&lt;br/&gt;
Time: 2015-06-16 15:31:00&lt;br/&gt;
-------------------------------------------&lt;/p&gt;
{&apos;topic&apos;: u&apos;test&apos;, &apos;partition&apos;: 0, &apos;value&apos;: u&apos;q tal?&apos;, &apos;key&apos;: None, &apos;offset&apos;: 87L}
&lt;p&gt;()&lt;br/&gt;
15/06/16 15:31:00 ....&lt;/p&gt;

&lt;p&gt;I have encoded the dictionary with the following Scala type alias, that uses types that PythonRDD can understand&lt;/p&gt;

&lt;p&gt;/** Using this weird type due to the limited set of types&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;supported by PythonRDD. This corresponds to&lt;br/&gt;
  *&lt;/li&gt;
	&lt;li&gt;((key, message), (topic, (partition, offset)))&lt;br/&gt;
  *&lt;/li&gt;
	&lt;li&gt;where the key and the message are encoded as Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;,&lt;/li&gt;
	&lt;li&gt;and topic, partition and offset are encoded as String.&lt;/li&gt;
	&lt;li&gt;Note we cannot even use triples because only pairs are supported&lt;/li&gt;
	&lt;li&gt;(we get an exception &quot;Unexpected element type class scala.Tuple3&quot;)&lt;br/&gt;
  */&lt;br/&gt;
  type PyKafkaMsgWrapper = ((Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;, Array&lt;span class=&quot;error&quot;&gt;&amp;#91;Byte&amp;#93;&lt;/span&gt;), (String, (String, String)))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If this is enough for you I can refactor thing to join  KafkaUtils.createDirectStreamJ and  KafkaUtils.createDirectStream in a single method, with an additional argument to specify if the meta info is required, with a default value of False so the behaviour is the same as before by default&lt;/p&gt;

&lt;p&gt;Looking forward to hearing your opinions on this.&lt;/p&gt;

&lt;p&gt;Greetings, &lt;/p&gt;

&lt;p&gt;Juan Rodriguez Hortala&lt;/p&gt;</comment>
                            <comment id="14588439" author="aramesh" created="Tue, 16 Jun 2015 17:42:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=juanrh&quot; class=&quot;user-hover&quot; rel=&quot;juanrh&quot;&gt;juanrh&lt;/a&gt; this looks pretty good to me. And from what I can see shouldn&apos;t add much overhead compared to the existing logic. It is perfect in terms of what are in need of &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. One stylistic suggestion is that you could return (key, value, kafka_offsets) where kafka_offsets is a dict of topic, parition and offset. This would keep things a little more consistent with what is returned when meta info is False.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;br/&gt;
Amit&lt;/p&gt;</comment>
                            <comment id="14588517" author="cody@koeninger.org" created="Tue, 16 Jun 2015 18:26:45 +0000"  >&lt;p&gt;So one thing to keep in mind is that if the Kafka project ends up adding more fields to MessageAndMetadata, the scala interface is going to continue to give users access to those fields, without changing anything other than the Kafka version.&lt;/p&gt;

&lt;p&gt;If you go with the approach of building a Python dict, someone&apos;s going to have to remember to go manually change the code to give access to the new fields.&lt;/p&gt;

&lt;p&gt;I don&apos;t have enough Python knowledge to comment on whether the approach of passing a messageHandler function is feasible... I can try to get up to speed on it.  It may be worth trying to get the attention of Davies Liu after the spark conference hubub has died down.&lt;/p&gt;</comment>
                            <comment id="14592354" author="juanrh" created="Thu, 18 Jun 2015 19:11:24 +0000"  >&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I have made some additional experiments:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I have replaced the dictionary with a named tuple, this is just an aesthetic detail. Regarding your comment Amit, what you propose could also be a good option&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;_MessageAndMetadata = namedtuple(&quot;MessageAndMetadata&quot;, &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;key&amp;quot;, &amp;quot;value&amp;quot;, &amp;quot;topic&amp;quot;, &amp;quot;partition&amp;quot;, &amp;quot;offset&amp;quot;&amp;#93;&lt;/span&gt;)&lt;/p&gt;

&lt;p&gt;so we get&lt;/p&gt;

&lt;p&gt;-------------------------------------------&lt;br/&gt;
Time: 2015-06-18 20:38:46&lt;br/&gt;
-------------------------------------------&lt;br/&gt;
MessageAndMetadata(key=None, value=u&apos;hola&apos;, topic=u&apos;test&apos;, partition=0, offset=104L)&lt;br/&gt;
()&lt;br/&gt;
...&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Regarding the message handler approach, I don&apos;t know much about py4j, but from &lt;a href=&quot;http://py4j.sourceforge.net/advanced_topics.html#implementing-java-interfaces-from-python-callback&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://py4j.sourceforge.net/advanced_topics.html#implementing-java-interfaces-from-python-callback&lt;/a&gt; I understand that the limited support py4j offers for calling Java interfaces implemented in Python cannot be used in this situation. That would be necessary to wrap a Python lambda into a org.apache.spark.api.java.function.Function with something like this&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;class JFunction(object):&lt;br/&gt;
    def _&lt;em&gt;init&lt;/em&gt;_(self, f):&lt;br/&gt;
        self._f = f&lt;/p&gt;

&lt;p&gt;    def call(self, v):&lt;br/&gt;
        return self._f(v)&lt;/p&gt;

&lt;p&gt;    class Java:&lt;br/&gt;
        implements = &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;#39;org.apache.spark.api.java.function.Function&amp;#39;&amp;#93;&lt;/span&gt;&lt;/p&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Another option is returning MessageAndMetadata directly instead of encoding them with tuples and then converting to named tuples. But that leads to &quot; Unexpected element type class kafka.message.MessageAndMetadata&quot; in PythonRDD&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;15/06/18 20:45:50 INFO DAGScheduler: Job 9 failed: runJob at PythonRDD.scala:366, took 0,034251 s&lt;br/&gt;
Traceback (most recent call last):&lt;br/&gt;
  File &quot;/home/juanrh/git/spark/python/pyspark/streaming/util.py&quot;, line 57, in call&lt;br/&gt;
    r = self.func(t, *rdds)&lt;br/&gt;
  File &quot;/home/juanrh/git/spark/python/pyspark/streaming/dstream.py&quot;, line 171, in takeAndPrint&lt;br/&gt;
    taken = rdd.take(num + 1)&lt;br/&gt;
  File &quot;/home/juanrh/git/spark/python/pyspark/rdd.py&quot;, line 1265, in take&lt;br/&gt;
    res = self.context.runJob(self, takeUpToNumLeft, p, True)&lt;br/&gt;
  File &quot;/home/juanrh/git/spark/python/pyspark/context.py&quot;, line 891, in runJob&lt;br/&gt;
    allowLocal)&lt;br/&gt;
  File &quot;/home/juanrh/git/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java_gateway.py&quot;, line 538, in _&lt;em&gt;call&lt;/em&gt;_&lt;br/&gt;
    self.target_id, self.name)&lt;br/&gt;
  File &quot;/home/juanrh/git/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py&quot;, line 300, in get_return_value&lt;br/&gt;
    format(target_id, &apos;.&apos;, name), value)&lt;br/&gt;
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.runJob.&lt;br/&gt;
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 9.0 failed 1 times, most recent failure: Lost task 0.0 in stage 9.0 (TID 9, localhost): org.apache.spark.SparkException: Unexpected element type class kafka.message.MessageAndMetadata&lt;br/&gt;
        at org.apache.spark.api.python.PythonRDD$.org$apache$spark$api$python$PythonRDD$$write$1(PythonRDD.scala:422)&lt;br/&gt;
        at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)&lt;br/&gt;
        at org.apache.spark.api.python.PythonRDD$$anonfun$writeIteratorToStream$1.apply(PythonRDD.scala:425)&lt;br/&gt;
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)&lt;br/&gt;
        at org.apache.spark.util.NextIterator.foreach(NextIterator.scala:21)&lt;br/&gt;
        at org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:425)&lt;br/&gt;
        at org.apache.spark.api.python.PythonRDD$WriterThread$$anonfun$run$3.apply(PythonRDD.scala:248)&lt;br/&gt;
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1771)&lt;br/&gt;
        at org.apache.spark.api.python.PythonRDD$WriterThread.run(PythonRDD.scala:208)&lt;/p&gt;

&lt;p&gt;I don&apos;t know the details of pyspark, and the reason why it supports so little data types. On the other hand an approach based on hasoffsets is complicated by the wrapper objects introduced when passing from Scala to Python, but maybe it could be possible to add an OffsetRange object to the _&lt;em&gt;dict&lt;/em&gt;_ of each RDD. Again, as I don&apos;t know about the design of pyspark and its serialization mechanism, I don&apos;t know whether that information is erased or not. &lt;/p&gt;

&lt;p&gt;This is as far as I go with my limited knowledge about pyspark. So maybe, as you suggest Cody, it would be better that another person who knows more about the internals of pyspark takes the baton now. &lt;/p&gt;</comment>
                            <comment id="14592736" author="aramesh" created="Thu, 18 Jun 2015 23:45:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=davies&quot; class=&quot;user-hover&quot; rel=&quot;davies&quot;&gt;davies&lt;/a&gt; can you please chime in here? Thanks!&lt;/p&gt;</comment>
                            <comment id="14598681" author="jerryshao" created="Wed, 24 Jun 2015 01:09:41 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=juanrh&quot; class=&quot;user-hover&quot; rel=&quot;juanrh&quot;&gt;juanrh&lt;/a&gt;, will you also address &lt;tt&gt;OffsetRange&lt;/tt&gt; problem described in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-8389&quot; title=&quot;Expose KafkaRDDs offsetRange in Python&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-8389&quot;&gt;&lt;del&gt;SPARK-8389&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14600010" author="juanrh" created="Wed, 24 Jun 2015 19:42:21 +0000"  >&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;As I said above, I don&apos;t know much about the internals of pyspark, and currently the original RDD from Scala is wrapped by several wrappers for the communication with python, and so the RDD implementing HasOffsetRanges is hidden by those layers. However, after its merge with &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-8389&quot; title=&quot;Expose KafkaRDDs offsetRange in Python&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-8389&quot;&gt;&lt;del&gt;SPARK-8389&lt;/del&gt;&lt;/a&gt;, it looks like this issue has got the attention of several Spark committers, and I&apos;m sure they will be able to come up with a solution that makes OffsetRanges accessible from pyspark.&lt;/p&gt;

&lt;p&gt;Greetings, &lt;/p&gt;

&lt;p&gt;Juan&lt;/p&gt;</comment>
                            <comment id="14600468" author="jerryshao" created="Thu, 25 Jun 2015 01:03:18 +0000"  >&lt;p&gt;OK, well, I&apos;d like to take a crack at it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                            <comment id="14604633" author="juanrh" created="Sun, 28 Jun 2015 10:57:14 +0000"  >&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I have worked a bit on the OffsetRange way, you can access the code at &lt;a href=&quot;https://github.com/juanrh/spark/commit/56fbd5c38bd30b825a7818f1c56abb1f8b2beaff&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/juanrh/spark/commit/56fbd5c38bd30b825a7818f1c56abb1f8b2beaff&lt;/a&gt;. I have added the following method to pyspark KafkaUtils&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;@staticmethod
    def getOffsetRanges(rdd):
        scalaRdd = rdd._jrdd.rdd()
        offsetRangesArray = scalaRdd.offsetRanges()
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; [ OffsetRange(topic = offsetRange.topic(),
                             partition = offsetRange.partition(), 
                             fromOffset = offsetRange.fromOffset(), 
                             untilOffset = offsetRange.untilOffset())
                    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; offsetRange in offsetRangesArray]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This method is used in KafkaUtils.createDirectStreamJB, which is based on the original KafkaUtilsPythonHelper.createDirectStream. The main problem I have is that I don&apos;t  know where to store the OffsetRange objects. The naive trick of adding them to the _&lt;em&gt;dict&lt;/em&gt;_ of each python RDD object doesn&apos;t work, the new field is lost in the pyspark wrappers. So the new method createDirectStreamJB takes two additional options, one for performing an action on the OffsetRange list, and another for adding it to each record of the DStream&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;def createDirectStreamJB(ssc, topics, kafkaParams, fromOffsets={},
                                          keyDecoder=utf8_decoder, valueDecoder=utf8_decoder,       
                                         offsetRangeForeach=None, addOffsetRange=False):
        &quot;&quot;&quot;
        FIXME: temporary working placeholder
        :param offsetRangeForeach: &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; different to None, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; function should be a function from a list of OffsetRange to None, and is applied to the OffsetRange
            list of each rdd
        :param addOffsetRange: &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; False (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;) output records are of the shape (kafkaKey, kafkaValue); &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; True output records are of the shape (offsetRange, (kafkaKey, kafkaValue)) &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; offsetRange the OffsetRange value &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the Spark partition &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the record
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is an example of using createDirectStreamJB:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;from pyspark.streaming &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; StreamingContext
from pyspark.streaming.kafka &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; KafkaUtils
ssc = StreamingContext(sc, 1)
topics = [&lt;span class=&quot;code-quote&quot;&gt;&quot;test&quot;&lt;/span&gt;]
kafkaParams = {&lt;span class=&quot;code-quote&quot;&gt;&quot;metadata.broker.list&quot;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&quot;localhost:9092&quot;&lt;/span&gt;}
def offsetRangeForeach(offsetRangeList):
    print 
    print 
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; offsetRange in offsetRangeList:
        print offsetRange
    print 
    print 

kafkaStream = KafkaUtils.createDirectStreamJB(ssc, topics, kafkaParams, offsetRangeForeach=offsetRangeForeach, addOffsetRange=True)

# OffsetRange printed as &amp;lt;pyspark.streaming.kafka.OffsetRange object at 0x7f2fdc045950&amp;gt;, I guess due to some kind of pyspark proxy 
kafkaStrStream = kafkaStream.map(lambda (offRan, (k, v)) :  str(offRan._fromOffset) + &lt;span class=&quot;code-quote&quot;&gt;&quot; &quot;&lt;/span&gt; + str(offRan._untilOffset) + &lt;span class=&quot;code-quote&quot;&gt;&quot; &quot;&lt;/span&gt; + str(k) + &lt;span class=&quot;code-quote&quot;&gt;&quot; &quot;&lt;/span&gt; + str(v))
# kafkaStream.pprint()
kafkaStrStream.pprint()
ssc.start()
ssc.awaitTermination(timeout=5)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;which gets the following output&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;15/06/28 12:36:03 INFO InputInfoTracker: remove old batch metadata: 1435487761000 ms


OffsetRange(topic=test, partition=0, fromOffset=178, untilOffset=179)


15/06/28 12:36:04 INFO JobScheduler: Added jobs &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; time 1435487764000 ms
...
15/06/28 12:36:04 INFO DAGScheduler: Job 4 finished: runJob at PythonRDD.scala:366, took 0,075387 s
-------------------------------------------
Time: 2015-06-28 12:36:04
-------------------------------------------
178 179 None hola
()
15/06/28 12:36:04 INFO JobScheduler: Finished job streaming job 1435487764000 ms.0 from job set of time 1435487764000 ms
...
15/06/28 12:36:05 INFO BlockManager: Removing RDD 12


OffsetRange(topic=test, partition=0, fromOffset=179, untilOffset=180)


15/06/28 12:36:06 INFO JobScheduler: Starting job streaming job 1435487766000 ms.0 from job set of time 1435487766000 ms
....
15/06/28 12:36:06 INFO DAGScheduler: Job 6 finished: start at NativeMethodAccessorImpl.java:-2, took 0,077993 s
-------------------------------------------
Time: 2015-06-28 12:36:06
-------------------------------------------
179 180 None caracola
()
15/06/28 12:36:06 INFO JobScheduler: Finished job streaming job 1435487766000 ms.0 from job set of time 1435487766000 ms
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Any thoughts on this will be appreciated, in particular about a suitable place to store the list of OffsetRange objects&lt;/p&gt;

&lt;p&gt;Greetings, &lt;/p&gt;

&lt;p&gt;Juan&lt;/p&gt;
</comment>
                            <comment id="14605241" author="jerryshao" created="Mon, 29 Jun 2015 07:47:37 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=juanrh&quot; class=&quot;user-hover&quot; rel=&quot;juanrh&quot;&gt;juanrh&lt;/a&gt;, I think the best choice is to keep the python programming way similar to Scala/Java, here in Java/Scala, we use offsetRange like:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;directKafkaStream.foreachRDD { rdd =&amp;gt; 
     val offsetRanges = rdd.asInstanceOf[HasOffsetRanges]
     &lt;span class=&quot;code-comment&quot;&gt;// offsetRanges.length = # of Kafka partitions being consumed
&lt;/span&gt;     ...
 }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It would be better to keep Python the same programming way. Looks like your implementation is a different way. From my understanding, you will return the offsetRange with each record of KafkaRDD, actually offsetRange is only related to RDD, not records of RDD, so maybe a little strange from my point, you have to serialize the offsetRange from driver to each executor.&lt;/p&gt;

&lt;p&gt;Here is what TD suggested, though still have some details should be figured out. I tried a bit but still have something block on the road.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think the way it works is that the Java/Python friendly DStream returned&lt;br/&gt;
by Java APIs of KafkaUtils, is wrapped in Python&apos;s DStream class in&lt;br/&gt;
dstream.py. The foreachRDD of that class uses another Python class&lt;br/&gt;
TransformFunction to wrap the JavaRDDs into Python&apos;s RDD objects and&lt;br/&gt;
applies the user defined python function on them. To allow the wrapped&lt;br/&gt;
Python RDDs to have a method called &quot;offsetRanges&quot;, you have to&lt;br/&gt;
1. Create a custom KafkaRDD Python class (extending to Python&apos;s RDD class)&lt;br/&gt;
which can wrap a KafkaRDD class. This may actually require defining a&lt;br/&gt;
JavaKafkaRDD class&lt;br/&gt;
2. Create a custom KafkaTransformFunc Python class (extending Python&apos;s&lt;br/&gt;
TransformFunc class) which wraps JavaRDDs into Python&apos;s KafkaRDD classes,&lt;br/&gt;
and applies user&apos;s function on those.&lt;br/&gt;
3. Create a custom KafkaDStream Python class (extending Python&apos;s DStream&lt;br/&gt;
class) which overrides transform() and foreachRDD() to use&lt;br/&gt;
KafkaTransformFunc instead of TransformFunc.&lt;br/&gt;
From my cursory look, this may work. Think about it.&lt;br/&gt;
TD&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="14605588" author="juanrh" created="Mon, 29 Jun 2015 13:12:37 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jerryshao&quot; class=&quot;user-hover&quot; rel=&quot;jerryshao&quot;&gt;jerryshao&lt;/a&gt;, &lt;/p&gt;

&lt;p&gt;That is a good idea, I should had paid more attention to the discussion in the duplicated issue. I will try that way, and tell you how it went. &lt;/p&gt;

&lt;p&gt;Greetings, &lt;/p&gt;

&lt;p&gt;Juan&lt;/p&gt;</comment>
                            <comment id="14609102" author="tdas" created="Tue, 30 Jun 2015 21:25:18 +0000"  >&lt;p&gt;I will let you guys figure out the implementation among yourselves. Let me chime in and say that I am more concerned about exposing offset ranges, than supporting access to MessageAndMetadata. So it would be great if you can make a PR with the first, and then we can focus on the latter in a later iteration.&lt;/p&gt;

&lt;p&gt;In that respect, if you are making a PR, please use the other JIRA - &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-8389&quot; title=&quot;Expose KafkaRDDs offsetRange in Python&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-8389&quot;&gt;&lt;del&gt;SPARK-8389&lt;/del&gt;&lt;/a&gt;, which was specifically for the offset ranges. I am reopening that JIRA and marking it as a sub jira of this one.&lt;/p&gt;</comment>
                            <comment id="14621253" author="tdas" created="Thu, 9 Jul 2015 20:56:55 +0000"  >&lt;p&gt;Now that &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-8389&quot; title=&quot;Expose KafkaRDDs offsetRange in Python&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-8389&quot;&gt;&lt;del&gt;SPARK-8389&lt;/del&gt;&lt;/a&gt; has been fixed, I am open to discussion for the messageHandler function. &lt;/p&gt;</comment>
                            <comment id="14627451" author="apachespark" created="Wed, 15 Jul 2015 03:13:07 +0000"  >&lt;p&gt;User &apos;jerryshao&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/7410&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/7410&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14627722" author="tdas" created="Wed, 15 Jul 2015 08:43:21 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jerryshao&quot; class=&quot;user-hover&quot; rel=&quot;jerryshao&quot;&gt;jerryshao&lt;/a&gt; Could you make a separate sub-task of this JIRA for the message handler API fix (to maintain consistency with other parity related subtask). And update the PR with that JIRA numbers. &lt;/p&gt;</comment>
                            <comment id="14627725" author="tdas" created="Wed, 15 Jul 2015 08:44:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jerryshao&quot; class=&quot;user-hover&quot; rel=&quot;jerryshao&quot;&gt;jerryshao&lt;/a&gt; Never mind, I made it myself.&lt;/p&gt;</comment>
                            <comment id="14627728" author="jerryshao" created="Wed, 15 Jul 2015 08:46:26 +0000"  >&lt;p&gt;OK, thanks TD.&lt;/p&gt;</comment>
                            <comment id="15570186" author="cody@koeninger.org" created="Wed, 12 Oct 2016 23:18:27 +0000"  >&lt;p&gt;Can this be closed, given that the subtasks are resolved and any future discussion of python dstream kafka support seems to be in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-16534&quot; title=&quot;Kafka 0.10 Python support&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-16534&quot;&gt;&lt;del&gt;SPARK-16534&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                            <subtask id="12838096">SPARK-8389</subtask>
                            <subtask id="12845209">SPARK-9065</subtask>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 5 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2fzz3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>