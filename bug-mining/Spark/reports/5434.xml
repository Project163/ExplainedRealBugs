<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:57:54 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-22982] Remove unsafe asynchronous close() call from FileDownloadChannel</title>
                <link>https://issues.apache.org/jira/browse/SPARK-22982</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Spark&apos;s Netty-based file transfer code contains an asynchronous IO bug which may lead to incorrect query results.&lt;/p&gt;

&lt;p&gt;At a high-level, the problem is that an unsafe asynchronous `close()` of a pipe&apos;s source channel creates a race condition where file transfer code closes a file descriptor then attempts to read from it. If the closed file descriptor&apos;s number has been reused by an `open()` call then this invalid read may cause unrelated file operations to return incorrect results due to reading different data than intended.&lt;/p&gt;

&lt;p&gt;I have a small, surgical fix for this bug and will submit a PR with more description on the specific race condition / underlying bug.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13129125">SPARK-22982</key>
            <summary>Remove unsafe asynchronous close() call from FileDownloadChannel</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="joshrosen">Josh Rosen</assignee>
                                    <reporter username="joshrosen">Josh Rosen</reporter>
                        <labels>
                            <label>correctness</label>
                    </labels>
                <created>Sun, 7 Jan 2018 23:09:59 +0000</created>
                <updated>Thu, 18 Jan 2018 18:28:25 +0000</updated>
                            <resolved>Wed, 10 Jan 2018 07:10:03 +0000</resolved>
                                    <version>1.6.0</version>
                    <version>2.0.0</version>
                    <version>2.1.0</version>
                    <version>2.2.0</version>
                                    <fixVersion>2.2.2</fixVersion>
                    <fixVersion>2.3.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="16315514" author="apachespark" created="Sun, 7 Jan 2018 23:31:04 +0000"  >&lt;p&gt;User &apos;JoshRosen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20179&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20179&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16319799" author="cloud_fan" created="Wed, 10 Jan 2018 07:10:03 +0000"  >&lt;p&gt;Issue resolved by pull request 20179&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20179&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20179&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16320203" author="srowen" created="Wed, 10 Jan 2018 13:16:14 +0000"  >&lt;p&gt;Does this affect earlier branches in the same way? Seems important to back port if so. &lt;/p&gt;</comment>
                            <comment id="16320664" author="joshrosen" created="Wed, 10 Jan 2018 17:14:28 +0000"  >&lt;p&gt;In theory this affects all 1.6.0+ versions.&lt;/p&gt;

&lt;p&gt;It&apos;s going to be much harder to trigger in 1.6.0 because we shouldn&apos;t have as many Janino-induced remote ClassNotFoundExceptions to trigger the error path.&lt;/p&gt;

&lt;p&gt;We should probably port to other 2.x branches, with one caveat: we need to make sure that my fix isn&apos;t relying on JRE 8.x functionality because I think at least some of those older branches still have support for Java 7.&lt;/p&gt;</comment>
                            <comment id="16320667" author="srowen" created="Wed, 10 Jan 2018 17:16:19 +0000"  >&lt;p&gt;Agreed. java.nio should be OK as that was introduced in Java 7. Spark 2.2 requires Java 8, so that much is safe.&lt;/p&gt;</comment>
                            <comment id="16330943" author="aash" created="Thu, 18 Jan 2018 18:28:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=joshrosen&quot; class=&quot;user-hover&quot; rel=&quot;joshrosen&quot;&gt;joshrosen&lt;/a&gt; do you have some example stacktraces of what this bug can cause?&#160; Several of our clusters hit what I think is this problem&#160;earlier this month, see below for details.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;For a few days in January (4th through 12th) on our AWS infra, we observed massively degraded disk read throughput (down to 33% of previous peaks).&#160; During this time, we also began observing intermittent exceptions coming from Spark at read time of parquet files that a previous Spark job had written.&#160; When the read throughput recovered on the 12th, we stopped observing the&#160;exceptions and haven&apos;t seen&#160;them since.&lt;/p&gt;

&lt;p&gt;At first&#160;we observed this stacktrace&#160;when reading .snappy.parquet files:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.RuntimeException: java.io.IOException: could not read page Page [bytes.size=1048641, valueCount=29945, uncompressedSize=1048641] in col [my_column] BINARY
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader$1.visit(VectorizedColumnReader.java:493)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader$1.visit(VectorizedColumnReader.java:486)
	at org.apache.parquet.column.page.DataPageV1.accept(DataPageV1.java:96)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readPage(VectorizedColumnReader.java:486)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:157)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:229)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:137)
	at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
	at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:398)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.processInputs(ObjectAggregationIterator.scala:191)
	at org.apache.spark.sql.execution.aggregate.ObjectAggregationIterator.&amp;lt;init&amp;gt;(ObjectAggregationIterator.scala:80)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:109)
	at org.apache.spark.sql.execution.aggregate.ObjectHashAggregateExec$$anonfun$doExecute$1$$anonfun$2.apply(ObjectHashAggregateExec.scala:101)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:827)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:287)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:341)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
Caused by: java.io.IOException: could not read page Page [bytes.size=1048641, valueCount=29945, uncompressedSize=1048641] in col [my_column] BINARY
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readPageV1(VectorizedColumnReader.java:562)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.access$000(VectorizedColumnReader.java:47)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader$1.visit(VectorizedColumnReader.java:490)
	... 31 more
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:98)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.uncompress(Snappy.java:547)
	at org.apache.parquet.hadoop.codec.SnappyDecompressor.decompress(SnappyDecompressor.java:69)
	at org.apache.parquet.hadoop.codec.NonBlockedDecompressorStream.read(NonBlockedDecompressorStream.java:51)
	at java.io.DataInputStream.readFully(DataInputStream.java:195)
	at java.io.DataInputStream.readFully(DataInputStream.java:169)
	at org.apache.parquet.bytes.BytesInput$StreamBytesInput.toByteArray(BytesInput.java:253)
	at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readPageV1(VectorizedColumnReader.java:555)
	... 33 more&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and saw a similar exception when attempting to read the file with parquet-tools instead of Spark, indicating the file itself was corrupted, not the read process:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;parquet.io.ParquetDecodingException: Can not read value at 11074077 in block 1 in file file:/path/to/part-00852-4f6b3ec3-ae6d-41ff-919b-a2ef4ea3dfa0-c000.snappy.parquet
        at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:228)
        at parquet.hadoop.ParquetReader.read(ParquetReader.java:124)
        at parquet.tools.command.CatCommand.execute(CatCommand.java:54)
        at parquet.tools.Main.main(Main.java:219)
Caused by: parquet.io.ParquetDecodingException: could not read page Page [bytes.size=1048598, valueCount=83618, uncompressedSize=1048598] in col [longitude] BINARY
        at parquet.column.impl.ColumnReaderImpl.readPageV1(ColumnReaderImpl.java:568)
        at parquet.column.impl.ColumnReaderImpl.access$300(ColumnReaderImpl.java:57)
        at parquet.column.impl.ColumnReaderImpl$3.visit(ColumnReaderImpl.java:516)
        at parquet.column.impl.ColumnReaderImpl$3.visit(ColumnReaderImpl.java:513)
        at parquet.column.page.DataPageV1.accept(DataPageV1.java:96)
        at parquet.column.impl.ColumnReaderImpl.readPage(ColumnReaderImpl.java:513)
        at parquet.column.impl.ColumnReaderImpl.checkRead(ColumnReaderImpl.java:505)
        at parquet.column.impl.ColumnReaderImpl.consume(ColumnReaderImpl.java:607)
        at parquet.io.RecordReaderImplementation.read(RecordReaderImplementation.java:407)
        at parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:209)
        ... 3 more
Caused by: java.io.IOException: FAILED_TO_UNCOMPRESS(5)
        at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:84)
        at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
        at org.xerial.snappy.Snappy.uncompress(Snappy.java:516)
        at parquet.hadoop.codec.SnappyDecompressor.decompress(SnappyDecompressor.java:69)
        at parquet.hadoop.codec.NonBlockedDecompressorStream.read(NonBlockedDecompressorStream.java:51)
        at java.io.DataInputStream.readFully(DataInputStream.java:195)
        at java.io.DataInputStream.readFully(DataInputStream.java:169)
        at parquet.bytes.BytesInput$StreamBytesInput.toByteArray(BytesInput.java:204)
        at parquet.column.impl.ColumnReaderImpl.readPageV1(ColumnReaderImpl.java:557)
        ... 12 more
Can not read value at 11074077 in block 1 in file file:/path/to/part-00852-4f6b3ec3-ae6d-41ff-919b-a2ef4ea3dfa0-c000.snappy.parquet
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Thinking it might have something to do with snappy, we set &lt;tt&gt;spark.sql.parquet.compression.codec: gzip&lt;/tt&gt; and&#160;then&#160;observed this exception, again intermittently:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.IndexOutOfBoundsException
at java.nio.Buffer.checkIndex(Buffer.java:540)
at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readUnsignedVarInt(VectorizedRleValuesReader.java:532)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readNextGroup(VectorizedRleValuesReader.java:588)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readIntegers(VectorizedRleValuesReader.java:467)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedRleValuesReader.readIntegers(VectorizedRleValuesReader.java:438)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:163)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:229)
at org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:137)
at org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)
at org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:105)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.scan_nextBatch$(Unknown Source)
at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:398)
at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:179)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)
at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)
at org.apache.spark.scheduler.Task.run(Task.scala:108)
at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:341)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
at java.lang.Thread.run(Thread.java:748)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;This occurred in clusters with Spark versions ranging from a few weeks ago on the apache master branch to all the way back to Spark 1.6.1.&#160; All in AWS infra, we did not observe this outside of AWS.&#160; One cluster saw failures in about 1% of&#160;builds.&lt;/p&gt;

&lt;p&gt;My hypothesis is that the Spectre/Meltdown patches on AWS, which in an early&#160;iteration caused large I/O performance degradations, caused Spark to hold file handles open for longer than previously, and subsequently hit this bug more frequently than before.&#160; We hadn&apos;t seen the exceptions previously, and when another silent AWS patch went out&#160;around Jan 12th that fixed I/O performance, we stopped seeing the exceptions.&lt;/p&gt;

&lt;p&gt;Do these observations match some of the observations your team observed while diving into this bug?&#160; Or am I totally off-base and latching onto an unrelated issue hoping it&apos;s the same as mine?&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 43 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1u552:zzy</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12339551">2.3.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>