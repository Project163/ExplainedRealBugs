<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:03:43 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-25992] Accumulators giving KeyError in pyspark</title>
                <link>https://issues.apache.org/jira/browse/SPARK-25992</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I am using accumulators and when I run my code, I sometimes get some warn messages. When I checked, there was nothing accumulated - not sure if I lost info from the accumulator or it worked and I can ignore this error ?&lt;/p&gt;

&lt;p&gt;The message:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Exception happened during processing of request from
(&apos;127.0.0.1&apos;, 62099)
Traceback (most recent call last):
File &quot;/Users/abdealijk/anaconda3/lib/python3.6/socketserver.py&quot;, line 317, in _handle_request_noblock
    self.process_request(request, client_address)
File &quot;/Users/abdealijk/anaconda3/lib/python3.6/socketserver.py&quot;, line 348, in process_request
    self.finish_request(request, client_address)
File &quot;/Users/abdealijk/anaconda3/lib/python3.6/socketserver.py&quot;, line 361, in finish_request
    self.RequestHandlerClass(request, client_address, self)
File &quot;/Users/abdealijk/anaconda3/lib/python3.6/socketserver.py&quot;, line 696, in __init__
    self.handle()
File &quot;/usr/local/hadoop/spark2.3.1/python/pyspark/accumulators.py&quot;, line 238, in handle
    _accumulatorRegistry[aid] += update
KeyError: 0
----------------------------------------
2018-11-09 19:09:08 ERROR DAGScheduler:91 - Failed to update accumulators for task 0
org.apache.spark.SparkException: EOF reached before Python server acknowledged
	at org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:634)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1131)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$updateAccumulators$1.apply(DAGScheduler.scala:1123)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1123)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1206)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1772)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1761)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13197410">SPARK-25992</key>
            <summary>Accumulators giving KeyError in pyspark</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="AbdealiJK">Abdeali Kothari</reporter>
                        <labels>
                    </labels>
                <created>Fri, 9 Nov 2018 13:47:30 +0000</created>
                <updated>Mon, 12 Dec 2022 18:11:11 +0000</updated>
                            <resolved>Wed, 16 Jan 2019 15:30:50 +0000</resolved>
                                    <version>2.3.1</version>
                                    <fixVersion>2.4.1</fixVersion>
                    <fixVersion>3.0.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16682325" author="gurwls223" created="Sat, 10 Nov 2018 10:22:45 +0000"  >&lt;p&gt;Can you share your codes to reproduce? Also please take a look similar JIRAs and describe symptoms and analysis at your best. I at least see one similar JIRA &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2282&quot; title=&quot;PySpark crashes if too many tasks complete quickly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-2282&quot;&gt;&lt;del&gt;SPARK-2282&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="16687726" author="abdealijk" created="Thu, 15 Nov 2018 09:54:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt; I tried a fair bit and was unable to produce a reproducible minimal example.&lt;br/&gt;
I posted it here hoping I could get some understanding when this could occur so I can simplify my code and figure out what is happening.&lt;/p&gt;

&lt;p&gt;I went through the pyspark code and as it occurs in _UpdateRequestHandler - which as I understand it is run only on the applicationMaster and not on workers. So, it looks like what was registered as an accumulator on the master is suddenly not seen in the master anymore. (i.e. the accumulator was lost from accumulatorRegistry)&lt;/p&gt;

&lt;p&gt;I am creating a single spark session and using it with celery - similar to what spark-celery tries to do &lt;a href=&quot;https://github.com/gregbaker/spark-celery&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/gregbaker/spark-celery&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is the celery loader I use if that helps:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;AppLoader(AppLoader):
     def on_worker_init(self):  # Initialize spark &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the worker - once
        &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt;:
            &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; findspark
            findspark.init()
            &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; pyspark
        except ImportError:
            &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; pyspark

        spark_builder = pyspark.sql.SparkSession.builder
        spark_builder = spark_builder.appName(&lt;span class=&quot;code-quote&quot;&gt;&quot;MySpark&quot;&lt;/span&gt;)
        self.spark = spark_builder.getOrCreate()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I am using celery with a single worker: `--concurrency 1`&lt;/p&gt;

&lt;p&gt;I think (and may be mistaken) celery uses subprocess/multiprocessing internally to create a worker thread and runs jobs inside that worker thread. Are there any known issues with: Creating a spark session, Creating a subprocess, Submitting the job (which uses accumulators) in a subprocess.&lt;/p&gt;

&lt;p&gt;I can see that _accumulatorRegistry is a global module level variable in pyspark.accumulators - which means that module reimports can cause a problem (not sure if subprocess does a module reimport)&lt;/p&gt;

&lt;p&gt;Now that I think of it - _start_update_server is called during context creation. So, does that mean that the socket is created in masterThread and if the accumulator is created in my subprocess - the master will receive updates from the workers. So, my masterThread would throw an error that this accum is unknown because it was created in the subprocess ?&lt;/p&gt;


&lt;p&gt;Again: I do not know if celery uses subprocess or something else like asyncio/gevent ... This is pure speculation.&lt;/p&gt;</comment>
                            <comment id="16688931" author="gurwls223" created="Fri, 16 Nov 2018 02:46:13 +0000"  >&lt;p&gt;Sounds unclear if it&apos;s an issue within Spark or not. Would you be interested in continuing investigation?&lt;/p&gt;</comment>
                            <comment id="16695684" author="abdealijk" created="Thu, 22 Nov 2018 08:59:21 +0000"  >&lt;p&gt;I do need to solve this, so I will be looking into it but maybe within the next month.&lt;/p&gt;</comment>
                            <comment id="16713965" author="abdealijk" created="Sun, 9 Dec 2018 13:02:06 +0000"  >&lt;p&gt;Here is a reproducible example in pyspark where using accumulators inside multiprocessing causes errors:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-python&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; pyspark
&lt;span class=&quot;code-keyword&quot;&gt;from&lt;/span&gt; pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; functions &lt;span class=&quot;code-keyword&quot;&gt;as&lt;/span&gt; F
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; multiprocessing

spark = pyspark.sql.SparkSession.builder.getOrCreate()
&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;appId=&quot;&lt;/span&gt;, spark.sparkContext.applicationId)


&lt;span class=&quot;code-keyword&quot;&gt;def&lt;/span&gt; myfunc(x):
    &lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;myfunc({}): appId={}&quot;&lt;/span&gt;.&lt;span class=&quot;code-object&quot;&gt;format&lt;/span&gt;(x, spark.sparkContext.applicationId))
    myaccum = spark.sparkContext.accumulator([], pyspark.accumulators.AddingAccumulatorParam([]))
    df = spark.createDataFrame(
        [[&lt;span class=&quot;code-quote&quot;&gt;&apos;a1&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;b1&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;c1&apos;&lt;/span&gt;, x * 1],
         [&lt;span class=&quot;code-quote&quot;&gt;&apos;a2&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;b2&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;c2&apos;&lt;/span&gt;, x * 2],
         [&lt;span class=&quot;code-quote&quot;&gt;&apos;a3&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;b3&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;c3&apos;&lt;/span&gt;, x * 3]],
        [&lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;c&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;x&apos;&lt;/span&gt;])
    df = df.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&quot;rnd&quot;&lt;/span&gt;, F.rand(42))

    @pyspark.sql.functions.udf
    &lt;span class=&quot;code-keyword&quot;&gt;def&lt;/span&gt; myudf(x):
        &lt;span class=&quot;code-keyword&quot;&gt;nonlocal&lt;/span&gt; myaccum
        myaccum += [&lt;span class=&quot;code-quote&quot;&gt;&quot;myudf({})&quot;&lt;/span&gt;.&lt;span class=&quot;code-object&quot;&gt;format&lt;/span&gt;(x)]
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; x

    df = df.withColumn(&lt;span class=&quot;code-quote&quot;&gt;&quot;x2&quot;&lt;/span&gt;, myudf(df[&lt;span class=&quot;code-quote&quot;&gt;&apos;x&apos;&lt;/span&gt;]))
    df.show()
    &lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;Accum value:&quot;&lt;/span&gt;, myaccum.value)
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; myaccum

&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;Without multiproc:&quot;&lt;/span&gt;)
&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;Got &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; value:&quot;&lt;/span&gt;, myfunc(1).value)
&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;====&quot;&lt;/span&gt;)

&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;With multiproc:&quot;&lt;/span&gt;)
pool = multiprocessing.Pool(1)
&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;Got &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; value:&quot;&lt;/span&gt;, [i.value &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; i &lt;span class=&quot;code-keyword&quot;&gt;in&lt;/span&gt; pool.&lt;span class=&quot;code-object&quot;&gt;map&lt;/span&gt;(myfunc, [1, 2, 3])])
&lt;span class=&quot;code-object&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;====&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this example, I am just using a multiprocPool with 1 worker, so there is no locking issues between threads or concurrency issues between threads or anything of that sort.&lt;/p&gt;</comment>
                            <comment id="16743712" author="gurwls223" created="Wed, 16 Jan 2019 07:15:09 +0000"  >&lt;p&gt;Okay, the problem seems to be because you used multi processing pool. Can you just use thread pool &lt;tt&gt;from multiprocessing.pool import ThreadPool&lt;/tt&gt;?&lt;/p&gt;</comment>
                            <comment id="16744153" author="gurwls223" created="Wed, 16 Jan 2019 15:30:50 +0000"  >&lt;p&gt;Fixed at &lt;a href=&quot;https://github.com/apache/spark/pull/23564&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23564&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I documented the limitation.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 43 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|s00bpc:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>