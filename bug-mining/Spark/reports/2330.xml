<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:31:27 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-7736] Exception not failing Python applications (in yarn cluster mode)</title>
                <link>https://issues.apache.org/jira/browse/SPARK-7736</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;It seems that exceptions thrown in Python spark apps after the SparkContext is instantiated don&apos;t cause the application to fail, at least in Yarn: the application is marked as SUCCEEDED.&lt;/p&gt;

&lt;p&gt;Note that any exception right before the SparkContext correctly places the application in FAILED state.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Spark 1.3.1, Yarn 2.7.0, Ubuntu 14.04&lt;/p&gt;</environment>
        <key id="12831058">SPARK-7736</key>
            <summary>Exception not failing Python applications (in yarn cluster mode)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="vanzin">Marcelo Masiero Vanzin</assignee>
                                    <reporter username="roji">Shay Rojansky</reporter>
                        <labels>
                    </labels>
                <created>Tue, 19 May 2015 16:12:48 +0000</created>
                <updated>Fri, 8 Dec 2017 00:01:12 +0000</updated>
                            <resolved>Wed, 9 Sep 2015 22:50:01 +0000</resolved>
                                                    <fixVersion>1.5.1</fixVersion>
                    <fixVersion>1.6.0</fixVersion>
                                    <component>YARN</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="14601914" author="neelesh77" created="Thu, 25 Jun 2015 20:51:54 +0000"  >&lt;p&gt;Could you add more context to the issue? &lt;br/&gt;
What is the return value / output expected on the applications?&lt;/p&gt;
</comment>
                            <comment id="14601923" author="roji" created="Thu, 25 Jun 2015 20:55:25 +0000"  >&lt;p&gt;The problem is simply with the YARN status for the application. If a Spark application throws an exception after having instantiated the SparkContext, the application obviously terminates but YARN lists the job as SUCCEEDED. This makes it hard for users to see what happened to their jobs in the YARN UI.&lt;/p&gt;

&lt;p&gt;Let me know if this is still unclear.&lt;/p&gt;</comment>
                            <comment id="14618110" author="storpipfugl" created="Wed, 8 Jul 2015 07:06:01 +0000"  >&lt;p&gt;Platform: spark 1.3.0, CDH 5.4.1&lt;/p&gt;

&lt;p&gt;To reproduce with pyspark:&lt;/p&gt;

&lt;p&gt;&amp;#8212;&lt;br/&gt;
from pyspark import SparkContext&lt;br/&gt;
with SparkContext(appName=&quot;raise_uncaught&quot;) as sc:&lt;br/&gt;
    raise Exception(&apos;Fail&apos;)&lt;br/&gt;
&amp;#8212;&lt;br/&gt;
$ spark-submit --master yarn-cluster /path/to/my/pythonscript.py&lt;/p&gt;

&lt;p&gt;This ends up with the following YARN status:&lt;br/&gt;
State:	FINISHED&lt;br/&gt;
FinalStatus:	SUCCEEDED&lt;br/&gt;
Diagnostics:	Shutdown hook called before final status was reported.&lt;/p&gt;

&lt;p&gt;If the exception is thrown before the SparkContext is initialized YARN status displays as expected:&lt;br/&gt;
&amp;#8212;&lt;br/&gt;
from pyspark import SparkContext&lt;br/&gt;
raise Exception(&apos;Fail&apos;)&lt;br/&gt;
with SparkContext(appName=&quot;raise_caught&quot;) as sc:&lt;br/&gt;
    pass&lt;br/&gt;
&amp;#8212;&lt;/p&gt;

&lt;p&gt;This ends up with the following YARN status:&lt;br/&gt;
State:	FAILED&lt;br/&gt;
FinalStatus:	FAILED&lt;br/&gt;
Diagnostics: &amp;lt;trace&amp;gt;&lt;/p&gt;

&lt;p&gt;It seems (from the Diagnostics message) that  &lt;a href=&quot;https://github.com/apache/spark/blob/19834fa9184f0365a160bcb54bcd33eaa87c70dc/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/19834fa9184f0365a160bcb54bcd33eaa87c70dc/yarn/src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala&lt;/a&gt; : L118 is hit when exceptions are raised after initializing SparkContext. This also means applications are not retried when failures happen after SparkContext initialization.&lt;/p&gt;</comment>
                            <comment id="14618885" author="neelesh77" created="Wed, 8 Jul 2015 16:37:01 +0000"  >&lt;p&gt;My 2 cents:&lt;/p&gt;

&lt;p&gt;To have a YARN failed, ApplicationMaster running the driver needs to fail. &lt;/p&gt;

&lt;p&gt;Scenario:&lt;br/&gt;
1) It fails once, YARN retries and succeeds if the exception has been handled correctly. This results in a Successful YARN job (assuming the child tasks (executors) succeeded).&lt;br/&gt;
2) The retries fail and the YARN job fails completely.&lt;br/&gt;
You need the Spark Application to coz a failure in YARN to mark it as a Failure.&lt;/p&gt;

&lt;p&gt;Moreover, the ApplicationMaster.java code from the: &lt;br/&gt;
/hadoop/hadoop-yarn-project/hadoop-yarn/hadoop-yarn-applications/hadoop-yarn-applications-distributedshell/src/main/java/org/apache/hadoop/yarn/applications/distributedshell/ApplicationMaster.java in the Hadoop project should help. &lt;/p&gt;

&lt;p&gt;Reference: &lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html&lt;/a&gt; &lt;/p&gt;

&lt;p&gt;So, I would say this is expected behavior.&lt;br/&gt;
Hope that helps. &lt;/p&gt;

&lt;p&gt;Please add/correct me if needed.&lt;/p&gt;</comment>
                            <comment id="14620021" author="storpipfugl" created="Thu, 9 Jul 2015 07:09:47 +0000"  >&lt;p&gt;Thanks for the comment. I don&apos;t understand how it apply here however as both listed pyspark programs (In my understanding) should result in step 2) of your scenario:&lt;/p&gt;

&lt;p&gt;p1) Unhandled execption raised before SparkContext initialization:&lt;br/&gt;
&amp;#8212;&lt;br/&gt;
from pyspark import SparkContext&lt;br/&gt;
raise Exception(&apos;Fail&apos;)&lt;br/&gt;
sc = SparkContext(appName=&quot;raise_seen_by_yarn&quot;)&lt;br/&gt;
&amp;#8212;&lt;br/&gt;
This results in an AM retry (total 2 AM tries as per YARN default) and subsequent marking of the application YARN status as FAILED. This is what I expect for a &quot;designed to fail AM&quot;.&lt;/p&gt;

&lt;p&gt;p2) Unhandled execption raised after SparkContext initialization:&lt;br/&gt;
&amp;#8212;&lt;br/&gt;
from pyspark import SparkContext&lt;br/&gt;
sc = SparkContext(appName=&quot;raise_not_seen_by_yarn&quot;):&lt;br/&gt;
raise Exception(&apos;Fail&apos;)&lt;br/&gt;
&amp;#8212;&lt;br/&gt;
This results in the the application being marked as SUCCEEDED (total of 1 AM try) which is not what I expect for a &quot;designed to fail AM&quot;.&lt;/p&gt;

&lt;p&gt;I&apos;ve tried to look in the spark documentation if there should be taken special actions to signal failure to YARN but I haven&apos;t found anything? And looking at src/main/scala/org/apache/spark/deploy/yarn/ApplicationMaster.scala : L118 where all sys.exit calls are considered successful termination regardless of exit code I can&apos;t see a way to signal failure to YARN after SparkContext initialization?&lt;/p&gt;

&lt;p&gt;Both p1 and p2 return with non-zero exit code when run with spark-submit --master yarn-client which is what I would expect.&lt;/p&gt;</comment>
                            <comment id="14621832" author="roji" created="Fri, 10 Jul 2015 06:41:53 +0000"  >&lt;p&gt;Neelesh, not sure I understood what you&apos;re saying exactly... I agree with Esben that at the end of the day, if a Spark application fails (by throwing an exception), and does so on all Yarn application attempts, that the Yarn status of that application definitely should be FAILED...&lt;/p&gt;</comment>
                            <comment id="14652429" author="apachespark" created="Mon, 3 Aug 2015 20:23:04 +0000"  >&lt;p&gt;User &apos;vanzin&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/7751&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/7751&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14699894" author="vanzin" created="Mon, 17 Aug 2015 17:35:43 +0000"  >&lt;p&gt;Leaving open for 1.5.1 backport.&lt;/p&gt;</comment>
                            <comment id="14700451" author="apachespark" created="Mon, 17 Aug 2015 23:44:04 +0000"  >&lt;p&gt;User &apos;vanzin&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/8258&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/8258&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14908001" author="ztoth" created="Fri, 25 Sep 2015 13:06:49 +0000"  >&lt;p&gt;As I see, this is also a problem for SparkR applications in yarn-cluster mode. Is there an open JIRA for that?&lt;/p&gt;</comment>
                            <comment id="14908389" author="shivaram" created="Fri, 25 Sep 2015 17:48:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ztoth&quot; class=&quot;user-hover&quot; rel=&quot;ztoth&quot;&gt;ztoth&lt;/a&gt; Could you open a new JIRA for the SparkR problem ?&lt;/p&gt;</comment>
                            <comment id="14910186" author="ztoth" created="Mon, 28 Sep 2015 08:43:47 +0000"  >&lt;p&gt;Created &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-10851&quot; title=&quot;Exception not failing R applications (in yarn cluster mode)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-10851&quot;&gt;&lt;del&gt;SPARK-10851&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14952263" author="roji" created="Sun, 11 Oct 2015 11:31:48 +0000"  >&lt;p&gt;Have just tested this with Spark 1.5.1 on Yarn 2.7.1 and the problem is still there - an exception thrown after the SparkContext has been created terminates the application but Yarn reports it as succeeded.&lt;/p&gt;</comment>
                            <comment id="15927240" author="yash360@gmail.com" created="Thu, 16 Mar 2017 00:18:41 +0000"  >&lt;p&gt;This does not seem Fixed. The application still completes with SUCCESS status even when an exception is thrown from the application.&lt;br/&gt;
Spark version 2.0.2.&lt;/p&gt;</comment>
                            <comment id="16275066" author="dmreshet" created="Fri, 1 Dec 2017 22:08:45 +0000"  >&lt;p&gt;Spark 2.2 still facing that issue.&lt;br/&gt;
In my case Azkaban executes Spark Job and finalStatus of this job in Resource Manager is SUCCESS in anycase.&lt;/p&gt;</comment>
                            <comment id="16282764" author="vanzin" created="Fri, 8 Dec 2017 00:01:12 +0000"  >&lt;p&gt;Make sure all you guys are running apps in cluster mode if you want to see the proper status. I just ran a failing pyspark app in cluster mode to double check, and all seems fine.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12849714">SPARK-9416</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12840393">SPARK-8612</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 49 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2exnr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12333108">1.5.1</customfieldvalue>
    <customfieldvalue id="12333083">1.6.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>