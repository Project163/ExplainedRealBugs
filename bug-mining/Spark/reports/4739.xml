<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:52:39 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-18112] Spark2.x does not support read data from Hive 2.x metastore</title>
                <link>https://issues.apache.org/jira/browse/SPARK-18112</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Hive2.0 has been released in February 2016, after that Hive2.0.1 and Hive2.1.0 have also been released for a long time, but till now spark only support to read hive metastore data from Hive1.2.1 and older version, since Hive2.x has many bugs fixed and performance improvement it&apos;s better and urgent to upgrade to support Hive2.x&lt;/p&gt;

&lt;p&gt;failed to load data from hive2.x metastore:&lt;br/&gt;
Exception in thread &quot;main&quot; java.lang.NoSuchFieldError: HIVE_STATS_JDBC_TIMEOUT&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveUtils$.hiveClientConfigurations(HiveUtils.scala:197)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:262)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveSharedState.metadataHive$lzycompute(HiveSharedState.scala:39)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveSharedState.metadataHive(HiveSharedState.scala:38)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveSharedState.externalCatalog$lzycompute(HiveSharedState.scala:4&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveSharedState.externalCatalog(HiveSharedState.scala:45)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:50)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)&lt;br/&gt;
        at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:31)&lt;br/&gt;
        at org.apache.spark.sql.SparkSession.table(SparkSession.scala:568)&lt;br/&gt;
        at org.apache.spark.sql.SparkSession.table(SparkSession.scala:564)&lt;/p&gt;</description>
                <environment></environment>
        <key id="13015350">SPARK-18112</key>
            <summary>Spark2.x does not support read data from Hive 2.x metastore</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="smilegator">Xiao Li</assignee>
                                    <reporter username="KaiXu">KaiXu</reporter>
                        <labels>
                    </labels>
                <created>Wed, 26 Oct 2016 08:49:31 +0000</created>
                <updated>Mon, 12 Dec 2022 18:11:11 +0000</updated>
                            <resolved>Wed, 15 Mar 2017 02:54:31 +0000</resolved>
                                    <version>2.0.0</version>
                    <version>2.0.1</version>
                                    <fixVersion>2.2.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>19</watches>
                                                                                                                <comments>
                            <comment id="15610286" author="kaixu" created="Thu, 27 Oct 2016 01:27:15 +0000"  >&lt;p&gt;Spark 2.0 removes JavaSparkListener and change SparkListener from a trait to an abstract class: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-14358&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-14358&lt;/a&gt;, but Hive2.x still use JavaSparkListener: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-17563&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-17563&lt;/a&gt;, this Hive patch solved this issue: &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-14029&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-14029&lt;/a&gt;, but from spark side it still does not support read data from Hive2.x metastore.&lt;/p&gt;</comment>
                            <comment id="15621678" author="srowen" created="Mon, 31 Oct 2016 09:07:45 +0000"  >&lt;p&gt;Yes, this may be an instance where Hive 2 will have to shim to be compatible with Spark 1 vs 2 simultaneously.&lt;/p&gt;</comment>
                            <comment id="15814474" author="gurwls223" created="Tue, 10 Jan 2017 09:33:40 +0000"  >&lt;p&gt;Could we resolve this as a duplicate of &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13446&quot; title=&quot;Spark need to support reading data from Hive 2.0.0 metastore&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-13446&quot;&gt;&lt;del&gt;SPARK-13446&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="15904362" author="smilegator" created="Fri, 10 Mar 2017 03:39:58 +0000"  >&lt;p&gt;Let me resolve it for supporting Hive 2.1.0 metastore.&lt;/p&gt;</comment>
                            <comment id="15904429" author="apachespark" created="Fri, 10 Mar 2017 04:41:03 +0000"  >&lt;p&gt;User &apos;gatorsmile&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/17232&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/17232&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15925453" author="cloud_fan" created="Wed, 15 Mar 2017 02:54:31 +0000"  >&lt;p&gt;Issue resolved by pull request 17232&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/17232&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/17232&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16326303" author="jpmoresmau" created="Mon, 15 Jan 2018 14:49:07 +0000"  >&lt;p&gt;I&apos;m using Hive 2.3.2 and Spark 2.2.1, but I still run into this issue. Is there any specific configuration setting I should look for?&lt;/p&gt;</comment>
                            <comment id="16448587" author="tavis" created="Mon, 23 Apr 2018 17:57:05 +0000"  >&lt;p&gt;It looks to me like this issue has actually not been fixed.&#160; As seen in the stack trace, the offending code is in&#160;&lt;/p&gt;

&lt;p&gt;/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala&lt;/p&gt;

&lt;p&gt;line 205, where the method attempts to fetch the value of configuration parameter&#160;HIVE_STATS_JDBC_TIMEOUT&lt;/p&gt;

&lt;p&gt;which was a configuration parameter defined in&#160;import org.apache.hadoop.hive.conf.HiveConf, which is part of hive-common.&#160; However, this configuration parameter was removed in Hive 2, therefore the above code will throw an exception when run with hive-common versions 2.x.&#160; It is possible there are other configuration parameters requested in HiveUtils.scala that have been removed as well; I haven&apos;t checked.&#160; In any event, the above line 205 is still present in the Master branch as of today, so Spark still does not work with Hive 2.x.&lt;/p&gt;</comment>
                            <comment id="16448634" author="tavis" created="Mon, 23 Apr 2018 18:21:48 +0000"  >&lt;p&gt;Sorry, following myself up... the next parameter,&#160;HIVE_STATS_RETRIES_WAIT, also needs to be removed.&#160; A quick search of the code does not find these two parameters used anywhere else in Spark, so I think the two lines can just be deleted without causing any downstream issues.&lt;/p&gt;</comment>
                            <comment id="16589721" author="elgalu" created="Thu, 23 Aug 2018 05:46:21 +0000"  >&lt;p&gt;Same issue. It only gets resolved if I remove spark-hive_2.11-2.3.1.jar but then pyspark and sparklyr stop working.&lt;/p&gt;</comment>
                            <comment id="16604167" author="toopt4" created="Wed, 5 Sep 2018 09:17:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; can this ticket be re-opened?&#160;This code is still in master as mentioned in comments above&lt;/p&gt;</comment>
                            <comment id="16604176" author="gurwls223" created="Wed, 5 Sep 2018 09:26:37 +0000"  >&lt;p&gt;Can you post reproducer step by step? did you set &lt;tt&gt;spark.sql.hive.metastore.version&lt;/tt&gt; and jar properly?&lt;/p&gt;</comment>
                            <comment id="16604391" author="srowen" created="Wed, 5 Sep 2018 13:17:38 +0000"  >&lt;p&gt;I don&apos;t know much about this part, but do we need Hive 2.x on the Spark (client) side in order to read from Hive 2.x metastore? Are you including Hive 2.x in your app? I don&apos;t know if that works.&lt;/p&gt;</comment>
                            <comment id="16605186" author="gurwls223" created="Thu, 6 Sep 2018 02:32:20 +0000"  >&lt;p&gt;We need the metastore jar if I understood correctly. FWIW, I am seeing few tests internally running with different metastore support. I doubt if there&apos;s an issue about its supportability itself.&lt;/p&gt;</comment>
                            <comment id="16628852" author="eugeniuz" created="Wed, 26 Sep 2018 14:22:53 +0000"  >&lt;p&gt;This issue should be reopened.&lt;/p&gt;

&lt;p&gt;As already commented by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Tavis&quot; class=&quot;user-hover&quot; rel=&quot;Tavis&quot;&gt;Tavis&lt;/a&gt; &lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L204&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L204&lt;/a&gt; is referenced but it is not present in HiveConf since branch 2.0&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/hive/blob/branch-1.2/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L1290&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hive/blob/branch-1.2/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L1290&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/hive/blob/branch-2.0/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hive/blob/branch-2.0/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16628865" author="gurwls223" created="Wed, 26 Sep 2018 14:33:25 +0000"  >&lt;p&gt;Can you post reproducer steps please before we open this?&lt;/p&gt;</comment>
                            <comment id="16629000" author="eugeniuz" created="Wed, 26 Sep 2018 16:16:14 +0000"  >&lt;p&gt;I can only describe my situation. I am using AWS EMR 5.17.0 with Hive, Spark, Zeppelin, Hue installed. In Zeppelin the configuration variable for spark interpretter points to /usr/lib/spark. There I found jars/ folder. In jars folder I have the following hive related libraries. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
-rw-r--r-- 1 root root   139044 Aug 15 01:06 hive-beeline-1.2.1-spark2-amzn-0.jar
-rw-r--r-- 1 root root    40850 Aug 15 01:06 hive-cli-1.2.1-spark2-amzn-0.jar
-rw-r--r-- 1 root root 11497847 Aug 15 01:06 hive-exec-1.2.1-spark2-amzn-0.jar
-rw-r--r-- 1 root root   101113 Aug 15 01:06 hive-jdbc-1.2.1-spark2-amzn-0.jar
-rw-r--r-- 1 root root  5472179 Aug 15 01:06 hive-metastore-1.2.1-spark2-amzn-0.jar
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If I replace them with their 2.3.3 equivalents, e.g. hive-exec-1.2.1-spark2-amzn-0.jar -&amp;gt; hive-exec-2.3.3-amzn-1.jar I get the following error when running SQL query in spark:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.NoSuchFieldError: HIVE_STATS_JDBC_TIMEOUT
	at org.apache.spark.sql.hive.HiveUtils$.formatTimeVarsForHiveClient(HiveUtils.scala:205)
	at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:286)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)
	at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:195)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:194)
	at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)
	at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.externalCatalog(HiveSessionStateBuilder.scala:39)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog$lzycompute(HiveSessionStateBuilder.scala:54)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.catalog(HiveSessionStateBuilder.scala:52)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1.&amp;lt;init&amp;gt;(HiveSessionStateBuilder.scala:69)
	at org.apache.spark.sql.hive.HiveSessionStateBuilder.analyzer(HiveSessionStateBuilder.scala:69)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)
	at org.apache.spark.sql.internal.BaseSessionStateBuilder$$anonfun$build$2.apply(BaseSessionStateBuilder.scala:293)
	at org.apache.spark.sql.internal.SessionState.analyzer$lzycompute(SessionState.scala:79)
	at org.apache.spark.sql.internal.SessionState.analyzer(SessionState.scala:79)
	at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)
	at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)
	at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74)
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:641)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:694)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:116)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:97)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:498)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:175)
	at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="16629182" author="elgalu" created="Wed, 26 Sep 2018 17:35:32 +0000"  >&lt;p&gt;And to get things worse Hive is already in version 3. Same with Hadoop, the default Spark+Hadoop distribution comes with Hadoop 2.7 while Hadoop is already 3.1. Is really hard to understand how such a popular open source project like Spark keeps dependencies years old, some are 7 years old or more.&lt;/p&gt;</comment>
                            <comment id="16629420" author="toopt4" created="Wed, 26 Sep 2018 21:13:23 +0000"  >&lt;p&gt;here, here!&lt;/p&gt;</comment>
                            <comment id="16629742" author="gurwls223" created="Thu, 27 Sep 2018 04:30:27 +0000"  >&lt;p&gt;Hadoop 3 profile. See &lt;a href=&quot;https://github.com/apache/spark/pull/21588&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/21588&lt;/a&gt; and please provide some input at &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-20202&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-20202&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16629743" author="gurwls223" created="Thu, 27 Sep 2018 04:33:13 +0000"  >&lt;p&gt;Re: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-18112?focusedCommentId=16629000&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16629000&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-18112?focusedCommentId=16629000&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-16629000&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Did you set &lt;tt&gt;spark.sql.hive.metastore.version&lt;/tt&gt;?&lt;/p&gt;</comment>
                            <comment id="16629749" author="gurwls223" created="Thu, 27 Sep 2018 04:42:40 +0000"  >&lt;p&gt;Hive 3 support. See &lt;a href=&quot;https://github.com/apache/spark/pull/21404&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/21404&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16629946" author="eugeniuz" created="Thu, 27 Sep 2018 08:40:56 +0000"  >&lt;p&gt;RE: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-18112?focusedCommentId=16629743&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16629743&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-18112?focusedCommentId=16629743&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16629743&lt;/a&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt; I set the config value to 2.3.3, didn&apos;t help.&lt;/p&gt;</comment>
                            <comment id="16630002" author="gurwls223" created="Thu, 27 Sep 2018 09:11:35 +0000"  >&lt;p&gt;Have you provided the jars into &lt;tt&gt;spark.sql.hive.metastore.jars&lt;/tt&gt; exclusively?&lt;/p&gt;</comment>
                            <comment id="16630024" author="eugeniuz" created="Thu, 27 Sep 2018 09:16:34 +0000"  >&lt;p&gt;Tried&#160;setting it to &quot;maven&quot; and then to &quot;/usr/lib/hive/lib&quot; from where I copied the 2.3.3 version of hive-*.jar libraries. That didn&apos;t help.&lt;/p&gt;

&lt;p&gt;In any case, how setting hive-* libraries&#160;would help with &lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L204&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala#L204&lt;/a&gt;&#160;referencing a field which doesn&apos;t exist anymore ? The problem is in spark library, isn&apos;t it ?&lt;/p&gt;</comment>
                            <comment id="16630233" author="gurwls223" created="Thu, 27 Sep 2018 11:23:30 +0000"  >&lt;p&gt;It uses Hive fork (1.2.1) jar at that point. When it creates a client, it uses specified metastore jars. &lt;/p&gt;</comment>
                            <comment id="16630236" author="gurwls223" created="Thu, 27 Sep 2018 11:26:27 +0000"  >&lt;p&gt;Such things should be asked to mailing list really. It still sounds like misconfiguration.&lt;/p&gt;</comment>
                            <comment id="16630472" author="tavis" created="Thu, 27 Sep 2018 14:01:54 +0000"  >&lt;p&gt;Why are you even asking these questions?&#160; I have already pointed to the offending lines of code in&#160;/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala that are causing this error and explained why the error is happening (see my comments from April 23rd).&#160; All you have to do is remove those two parameters and push.&lt;/p&gt;</comment>
                            <comment id="16630485" author="gurwls223" created="Thu, 27 Sep 2018 14:09:26 +0000"  >&lt;p&gt;At that code you pointed out, Spark&apos;s Hive fork should be used. Different jar provided in &lt;tt&gt;spark.sql.hive.metastore.jars&lt;/tt&gt; is used to create the correspending client to access different version of Hive via isolated classloader.&lt;br/&gt;
The problem here is, you guys removed hive-1.2.1 in the jars and didn&apos;t provide newer Hive jars in &lt;tt&gt;spark.sql.hive.metastore.jars&lt;/tt&gt; properly.&lt;/p&gt;</comment>
                            <comment id="16630487" author="gurwls223" created="Thu, 27 Sep 2018 14:09:55 +0000"  >&lt;p&gt;I asked questions because i&apos;m pretty sure it&apos;s misconfiguration. That&apos;s why I am asking reproducible steps.&lt;/p&gt;</comment>
                            <comment id="16630542" author="gurwls223" created="Thu, 27 Sep 2018 14:46:52 +0000"  >&lt;p&gt;To be more specific, the code you guys pointed out is executed by Spark&apos;s Hive fork 1.2.1 which contains that configuration (&lt;a href=&quot;https://github.com/apache/hive/blob/branch-1.2/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L1290&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hive/blob/branch-1.2/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java#L1290&lt;/a&gt;) &lt;/p&gt;

&lt;p&gt;That&apos;s meant to be executed with Spark&apos;s Hive fork. So you should leave the jar as is. And then, the higher jars for Hive to create Hive client should be provided to &lt;tt&gt;spark.sql.hive.metastore.jars&lt;/tt&gt; and &lt;tt&gt;spark.sql.hive.metastore.version&lt;/tt&gt; should be set accordingly.&lt;/p&gt;

&lt;p&gt;The problem here looks, you guys completely replaced the jars into higher Hive jars. Therefore, it throws &lt;tt&gt;NoSuchFieldError&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;I recently manually tested 1.2.1, 2.3.0 and 3.0.0 (against &lt;a href=&quot;https://github.com/apache/spark/pull/21404&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/21404&lt;/a&gt;) in few months ago against Apache Spark. I am pretty sure that it works for now.&lt;/p&gt;

&lt;p&gt;If I am mistaken or misunderstood at some points, please provide a reproducible step, or at least why it fails. Let me take a look.&lt;/p&gt;</comment>
                            <comment id="16630543" author="gurwls223" created="Thu, 27 Sep 2018 14:47:55 +0000"  >&lt;p&gt;Also, take a look for the codes, open JIRAs and PRs before complaining next time.&lt;/p&gt;</comment>
                            <comment id="16630689" author="eugeniuz" created="Thu, 27 Sep 2018 16:23:18 +0000"  >&lt;p&gt;&lt;em&gt;&quot;The problem here looks, you guys completely replaced the jars into higher Hive jars. Therefore, it throws&#160;&lt;tt&gt;NoSuchFieldError&lt;/tt&gt;&lt;/em&gt;&quot; - yes you are right. That was my intent. I wanted to&#160;be able to connect to&#160;metastore database created by a Hive client 2.x. If I use that 1.2.1 fork I was getting some query errors due to me using bloom filters on multiple columns of the table. My understanding is that Hive client 1.2.1 is not seeing that information that is why I was trying to replace the jars for a higher version.&lt;/p&gt;</comment>
                            <comment id="16631159" author="gurwls223" created="Thu, 27 Sep 2018 23:06:35 +0000"  >&lt;blockquote&gt;
&lt;p&gt; If I use that 1.2.1 fork I was getting some query errors due to me using bloom filters on multiple columns of the table&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can you file another JIRA then? Some tests were added for that (&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25427&quot; title=&quot;Add BloomFilter creation test cases&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25427&quot;&gt;&lt;del&gt;SPARK-25427&lt;/del&gt;&lt;/a&gt;). If that&apos;s not respected, it sounds another problem.&lt;/p&gt;</comment>
                            <comment id="16631186" author="gurwls223" created="Thu, 27 Sep 2018 23:29:02 +0000"  >&lt;p&gt;To avoid Hive fork 1.2.1 in Spark itself at all, please provide some input at &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-20202&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-20202&lt;/a&gt; to upgrade it to Hive 2.3.2. Now it&apos;s somehow blocked and I need some input there.&lt;/p&gt;</comment>
                            <comment id="16860512" author="honglun" created="Tue, 11 Jun 2019 04:01:08 +0000"  >&lt;p&gt;Why do my issue still exist&#65292;I use the Spark-2.4.3 and Hive-2.3.3.&lt;/p&gt;</comment>
                            <comment id="16860595" author="dongjoon" created="Tue, 11 Jun 2019 06:28:38 +0000"  >&lt;p&gt;What is your issue, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=honglun&quot; class=&quot;user-hover&quot; rel=&quot;honglun&quot;&gt;honglun&lt;/a&gt;? In this issue, `HonglunChen` seems to appear once (two hour ago).&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Why do my issue still exist&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="16860605" author="honglun" created="Tue, 11 Jun 2019 06:57:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dongjoon&quot; class=&quot;user-hover&quot; rel=&quot;dongjoon&quot;&gt;dongjoon&lt;/a&gt;,I replaced the hive-1.x jars(hive-beeline-1.2.1,hive-cli-1.2.1,hive-exec-1.2.1,hive-jdbc-1.2.1,hive-metastore-1.2.1) with the hive-2.3.3 jars,and set &quot;start-thriftserver.sh --conf spark.sql.hive.metastore.jars=/app/spark/jars/* , --conf spark.sql.hive.metastore.version=2.3.3&quot;,but it didn&apos;t work.The error still appears:&#160;&lt;em&gt;Exception in thread &quot;main&quot; java.lang.NoSuchFieldError: HIVE_STATS_JDBC_TIMEOUT.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Did&#160; I misunderstand at some something? I found that HiveConf.java in the&#160;hive-exec-1.2.1 has the field&#160;&lt;em&gt;HIVE_STATS_JDBC_TIMEOUT, but&#160;HiveConf.java is&#160; in the&#160;hive-common-2.3.3 and it has no&#160;field&lt;/em&gt;&#160;&lt;em&gt;HIVE_STATS_JDBC_TIMEOUT.&lt;/em&gt;&lt;/p&gt;</comment>
                            <comment id="16860711" author="gurwls223" created="Tue, 11 Jun 2019 09:08:44 +0000"  >&lt;p&gt;you should place 2.3.3 jars if you want do use 2.3.3.&lt;/p&gt;</comment>
                            <comment id="16860734" author="honglun" created="Tue, 11 Jun 2019 09:26:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt; Thanks, I did not remove the 1.2.1 jars, and put all the 2.3.3 jars in a separate directory, then set &quot;--conf spark.sql.hive.metastore.jars=/path of dir/*&quot;. It works now&#65281;&lt;/p&gt;</comment>
                            <comment id="16862432" author="dongjoon" created="Wed, 12 Jun 2019 20:18:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=honglun&quot; class=&quot;user-hover&quot; rel=&quot;honglun&quot;&gt;honglun&lt;/a&gt;. You should not replace Apache Spark jars in `jars` directory. Those files are used for internal Hive-related &lt;b&gt;execution&lt;/b&gt; inside Spark.&lt;br/&gt;
`--conf spark.sql.hive.metastore.version` is only loading new Hive jars additionally for &lt;b&gt;metastore&lt;/b&gt; access in a isolated class loader separately.&lt;/p&gt;</comment>
                            <comment id="16863598" author="honglun" created="Fri, 14 Jun 2019 01:33:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dongjoon&quot; class=&quot;user-hover&quot; rel=&quot;dongjoon&quot;&gt;dongjoon&lt;/a&gt; Thank you, I get it.&lt;/p&gt;</comment>
                            <comment id="16914192" author="rahulvkulkarni" created="Fri, 23 Aug 2019 11:22:32 +0000"  >&lt;p&gt;Getting this issue on Spark 2.4.3 with Hive 2.3.3. Also tried with Hive 2.3.5. Tried using explicit config variables&#160;spark.sql.hive.metastore.version and&#160;spark.sql.hive.metastore.jars, but no luck.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;cdh235m1:/spark-2.4.3-bin-without-hadoop/conf # spark-shell \&lt;br/&gt;
&amp;gt; --jars /opt/teradata/tdqg/connector/tdqg-spark-connector/02.05.00.04-159/lib/spark-loaderfactory-02.05.00.04-159.jar, \&lt;br/&gt;
&amp;gt; /opt/teradata/tdqg/connector/tdqgspark-connector/02.05.00.04-159/lib/log4j-api-2.7.jar, \&lt;br/&gt;
&amp;gt; /opt/teradata/tdqg/connector/tdqgspark-connector/02.05.00.04-159/lib/log4j-core-2.7.jar, \&lt;br/&gt;
&amp;gt; /opt/teradata/tdqg/connector/tdqgspark-connector/02.05.00.04-159/lib/qgc-spark-02.05.00.04-159.jar, \&lt;br/&gt;
&amp;gt; /opt/teradata/tdqg/connector/tdqgspark-connector/02.05.00.04-159/lib/spark-loader-02.05.00.04-159.jar, \&lt;br/&gt;
&amp;gt; /opt/teradata/tdqg/connector/tdqgspark-connector/02.05.00.04-159/lib/json-simple-1.1.1.jar \&lt;br/&gt;
&amp;gt; --conf spark.sql.hive.metastore.version=2.3.3 \&lt;br/&gt;
&amp;gt; --conf spark.sql.hive.metastore.jars=/apache-hive-2.3.3-bin/lib/* \&lt;br/&gt;
&amp;gt; --master spark://cdh235m1.labs.teradata.com:7077&lt;br/&gt;
SLF4J: Class path contains multiple SLF4J bindings.&lt;br/&gt;
SLF4J: Found binding in &lt;span class=&quot;error&quot;&gt;&amp;#91;jar:file:/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class&amp;#93;&lt;/span&gt;&lt;br/&gt;
SLF4J: Found binding in &lt;span class=&quot;error&quot;&gt;&amp;#91;jar:file:/apache-hive-2.3.3-bin/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class&amp;#93;&lt;/span&gt;&lt;br/&gt;
SLF4J: See &lt;a href=&quot;http://www.slf4j.org/codes.html#multiple_bindings&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.slf4j.org/codes.html#multiple_bindings&lt;/a&gt; for an explanation.&lt;br/&gt;
SLF4J: Actual binding is of type &lt;span class=&quot;error&quot;&gt;&amp;#91;org.slf4j.impl.Log4jLoggerFactory&amp;#93;&lt;/span&gt;&lt;br/&gt;
19/08/23 07:15:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: backward-delete-word&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: backward-delete-word&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: down-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: up-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: up-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: down-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: up-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: down-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: up-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: down-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: up-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: down-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: up-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: down-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: up-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: down-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: up-history&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;INFO&amp;#93;&lt;/span&gt; Unable to bind key for unsupported operation: down-history&lt;br/&gt;
Spark context Web UI available at &lt;a href=&quot;http://cdh235m1.labs.teradata.com:4040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cdh235m1.labs.teradata.com:4040&lt;/a&gt;&lt;br/&gt;
Spark context available as &apos;sc&apos; (master = spark://cdh235m1.labs.teradata.com:7077, app id = app-20190823071550-0000).&lt;br/&gt;
Spark session available as &apos;spark&apos;.&lt;br/&gt;
Welcome to&lt;br/&gt;
 ____ __&lt;br/&gt;
 / _&lt;em&gt;/&lt;/em&gt;_ ___ ____&lt;em&gt;/ /&lt;/em&gt;_&lt;br/&gt;
 &lt;em&gt;\ \/ _ \/ _ `/ __/ &apos;&lt;/em&gt;/&lt;br/&gt;
 /__&lt;em&gt;/ .&lt;/em&gt;&lt;em&gt;/&amp;#95;,&lt;/em&gt;/&lt;em&gt;/ /&lt;/em&gt;/&amp;#95;\ version 2.4.3&lt;br/&gt;
 /_/&lt;/p&gt;

&lt;p&gt;Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_202)&lt;br/&gt;
Type in expressions to have them evaluated.&lt;br/&gt;
Type :help for more information.&lt;/p&gt;

&lt;p&gt;scala&amp;gt; import tdqg.ForeignServer&lt;br/&gt;
import tdqg.ForeignServer&lt;/p&gt;

&lt;p&gt;scala&amp;gt; ForeignServer.getDatasetFromSql(&quot;SELECT * FROM GDCData.Telco_Churn_Anal_Train_V&quot;)&lt;br/&gt;
19/08/23 07:16:03 WARN SparkSession$Builder: Using an existing SparkSession; some configuration may not take effect.&lt;br/&gt;
java.lang.NoSuchFieldError: HIVE_STATS_JDBC_TIMEOUT&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveUtils$.formatTimeVarsForHiveClient(HiveUtils.scala:204)&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:285)&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:66)&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:65)&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:215)&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:215)&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:214)&lt;br/&gt;
 at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:114)&lt;br/&gt;
 at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:102)&lt;br/&gt;
 at org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:141)&lt;br/&gt;
 at org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:136)&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)&lt;br/&gt;
 at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:55)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.catalog.SessionCatalog.isTemporaryTable(SessionCatalog.scala:736)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.isRunningDirectlyOnFiles(Analyzer.scala:749)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:683)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:715)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:708)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:326)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:324)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:29)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:708)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:654)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84)&lt;br/&gt;
 at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)&lt;br/&gt;
 at scala.collection.immutable.List.foldLeft(List.scala:84)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76)&lt;br/&gt;
 at scala.collection.immutable.List.foreach(List.scala:392)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:127)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:121)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:106)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:105)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)&lt;br/&gt;
 at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:105)&lt;br/&gt;
 at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57)&lt;br/&gt;
 at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55)&lt;br/&gt;
 at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47)&lt;br/&gt;
 at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:78)&lt;br/&gt;
 at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:642)&lt;br/&gt;
 at tdqg.ForeignServer.getDatasetFromSql(ForeignServer.java:344)&lt;br/&gt;
 ... 49 elided&lt;/p&gt;

&lt;p&gt;scala&amp;gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="12941306">SPARK-13446</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13031853">SPARK-19076</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 12 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i35eon:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>