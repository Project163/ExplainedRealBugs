<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:03:07 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-26370] Fix resolution of higher-order function for the same identifier.</title>
                <link>https://issues.apache.org/jira/browse/SPARK-26370</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When using a higher-order function with the same variable name as the existing columns in &lt;tt&gt;Filter&lt;/tt&gt; or something which uses &lt;tt&gt;Analyzer.resolveExpressionBottomUp&lt;/tt&gt; during the resolution, e.g.,:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
val df = Seq(
  (Seq(1, 9, 8, 7), 1, 2),
  (Seq(5, 9, 7), 2, 2),
  (Seq.empty, 3, 2),
  (&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;, 4, 2)
).toDF(&lt;span class=&quot;code-quote&quot;&gt;&quot;i&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;x&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;d&quot;&lt;/span&gt;)

checkAnswer(df.filter(&lt;span class=&quot;code-quote&quot;&gt;&quot;exists(i, x -&amp;gt; x % d == 0)&quot;&lt;/span&gt;),
  Seq(Row(Seq(1, 9, 8, 7), 1, 2)))
checkAnswer(df.select(&lt;span class=&quot;code-quote&quot;&gt;&quot;x&quot;&lt;/span&gt;).filter(&lt;span class=&quot;code-quote&quot;&gt;&quot;exists(i, x -&amp;gt; x % d == 0)&quot;&lt;/span&gt;),
  Seq(Row(1)))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;the following exception happens:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.BoundReference cannot be &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt; to org.apache.spark.sql.catalyst.expressions.NamedExpression
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.map(TraversableLike.scala:237)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.$anonfun$functionsForEval$1(higherOrderFunctions.scala:147)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike.map(TraversableLike.scala:237)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:230)
  at scala.collection.immutable.List.map(List.scala:298)
  at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.functionsForEval(higherOrderFunctions.scala:145)
  at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.functionsForEval$(higherOrderFunctions.scala:145)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionsForEval$lzycompute(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionsForEval(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.functionForEval(higherOrderFunctions.scala:176)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.functionForEval$(higherOrderFunctions.scala:176)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionForEval(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.nullSafeEval(higherOrderFunctions.scala:387)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval(higherOrderFunctions.scala:190)
  at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval$(higherOrderFunctions.scala:185)
  at org.apache.spark.sql.catalyst.expressions.ArrayExists.eval(higherOrderFunctions.scala:369)
  at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)
  at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:216)
  at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:215)

...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;because the &lt;tt&gt;UnresolvedAttribute&lt;/tt&gt; s in &lt;tt&gt;LambdaFunction&lt;/tt&gt; are unexpectedly resolved by the rule.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13204417">SPARK-26370</key>
            <summary>Fix resolution of higher-order function for the same identifier.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ueshin">Takuya Ueshin</assignee>
                                    <reporter username="ueshin">Takuya Ueshin</reporter>
                        <labels>
                    </labels>
                <created>Fri, 14 Dec 2018 07:34:18 +0000</created>
                <updated>Fri, 14 Dec 2018 16:30:18 +0000</updated>
                            <resolved>Fri, 14 Dec 2018 16:30:18 +0000</resolved>
                                    <version>2.4.0</version>
                                    <fixVersion>2.4.1</fixVersion>
                    <fixVersion>3.0.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="16721025" author="githubbot" created="Fri, 14 Dec 2018 07:44:56 +0000"  >&lt;p&gt;ueshin opened a new pull request #23320: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26370&quot; title=&quot;Fix resolution of higher-order function for the same identifier.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26370&quot;&gt;&lt;del&gt;SPARK-26370&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Fix resolution of higher-order function for the same identifier.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23320&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23320&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What changes were proposed in this pull request?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   When using a higher-order function with the same variable name as the existing columns in `Filter` or something which uses `Analyzer.resolveExpressionBottomUp` during the resolution, e.g.,:&lt;/p&gt;

&lt;p&gt;   ```scala&lt;br/&gt;
   val df = Seq(&lt;br/&gt;
     (Seq(1, 9, 8, 7), 1, 2),&lt;br/&gt;
     (Seq(5, 9, 7), 2, 2),&lt;br/&gt;
     (Seq.empty, 3, 2),&lt;br/&gt;
     (null, 4, 2)&lt;br/&gt;
   ).toDF(&quot;i&quot;, &quot;x&quot;, &quot;d&quot;)&lt;/p&gt;

&lt;p&gt;   checkAnswer(df.filter(&quot;exists(i, x -&amp;gt; x % d == 0)&quot;),&lt;br/&gt;
     Seq(Row(Seq(1, 9, 8, 7), 1, 2)))&lt;br/&gt;
   checkAnswer(df.select(&quot;x&quot;).filter(&quot;exists(i, x -&amp;gt; x % d == 0)&quot;),&lt;br/&gt;
     Seq(Row(1)))&lt;br/&gt;
   ```&lt;/p&gt;

&lt;p&gt;   the following exception happens:&lt;/p&gt;

&lt;p&gt;   ```&lt;br/&gt;
   java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.BoundReference cannot be cast to org.apache.spark.sql.catalyst.expressions.NamedExpression&lt;br/&gt;
     at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)&lt;br/&gt;
     at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)&lt;br/&gt;
     at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)&lt;br/&gt;
     at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)&lt;br/&gt;
     at scala.collection.TraversableLike.map(TraversableLike.scala:237)&lt;br/&gt;
     at scala.collection.TraversableLike.map$(TraversableLike.scala:230)&lt;br/&gt;
     at scala.collection.AbstractTraversable.map(Traversable.scala:108)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.$anonfun$functionsForEval$1(higherOrderFunctions.scala:147)&lt;br/&gt;
     at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)&lt;br/&gt;
     at scala.collection.immutable.List.foreach(List.scala:392)&lt;br/&gt;
     at scala.collection.TraversableLike.map(TraversableLike.scala:237)&lt;br/&gt;
     at scala.collection.TraversableLike.map$(TraversableLike.scala:230)&lt;br/&gt;
     at scala.collection.immutable.List.map(List.scala:298)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.functionsForEval(higherOrderFunctions.scala:145)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.HigherOrderFunction.functionsForEval$(higherOrderFunctions.scala:145)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionsForEval$lzycompute(higherOrderFunctions.scala:369)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionsForEval(higherOrderFunctions.scala:369)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.functionForEval(higherOrderFunctions.scala:176)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.functionForEval$(higherOrderFunctions.scala:176)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.ArrayExists.functionForEval(higherOrderFunctions.scala:369)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.ArrayExists.nullSafeEval(higherOrderFunctions.scala:387)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval(higherOrderFunctions.scala:190)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.SimpleHigherOrderFunction.eval$(higherOrderFunctions.scala:185)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.ArrayExists.eval(higherOrderFunctions.scala:369)&lt;br/&gt;
     at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificPredicate.eval(Unknown Source)&lt;br/&gt;
     at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3(basicPhysicalOperators.scala:216)&lt;br/&gt;
     at org.apache.spark.sql.execution.FilterExec.$anonfun$doExecute$3$adapted(basicPhysicalOperators.scala:215)&lt;/p&gt;

&lt;p&gt;   ...&lt;br/&gt;
   ```&lt;/p&gt;

&lt;p&gt;   because the `UnresolvedAttribute`s in `LambdaFunction` are unexpectedly resolved by the rule.&lt;/p&gt;

&lt;p&gt;   This pr modified to use a placeholder `UnresolvedNamedLambdaVariable` to prevent unexpected resolution.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;How was this patch tested?&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Added a test and modified some tests.&lt;/p&gt;


&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16721575" author="githubbot" created="Fri, 14 Dec 2018 16:27:46 +0000"  >&lt;p&gt;asfgit closed pull request #23320: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26370&quot; title=&quot;Fix resolution of higher-order function for the same identifier.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26370&quot;&gt;&lt;del&gt;SPARK-26370&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Fix resolution of higher-order function for the same identifier.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/23320&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23320&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala&lt;br/&gt;
index a8a7bbd9f9cd0..1cd7f412bb678 100644&lt;br/&gt;
&amp;#8212; a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala&lt;br/&gt;
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/higherOrderFunctions.scala&lt;br/&gt;
@@ -150,13 +150,14 @@ case class ResolveLambdaVariables(conf: SQLConf) extends Rule&lt;span class=&quot;error&quot;&gt;&amp;#91;LogicalPlan&amp;#93;&lt;/span&gt; {&lt;br/&gt;
       val lambdaMap = l.arguments.map(v =&amp;gt; canonicalizer(v.name) -&amp;gt; v).toMap&lt;br/&gt;
       l.mapChildren(resolve(_, parentLambdaMap ++ lambdaMap))&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;case u @ UnresolvedAttribute(name +: nestedFields) =&amp;gt;&lt;br/&gt;
+    case u @ UnresolvedNamedLambdaVariable(name +: nestedFields) =&amp;gt;&lt;br/&gt;
       parentLambdaMap.get(canonicalizer(name)) match {&lt;br/&gt;
         case Some(lambda) =&amp;gt;&lt;br/&gt;
           nestedFields.foldLeft(lambda: Expression) 
{ (expr, fieldName) =&amp;gt;
             ExtractValue(expr, Literal(fieldName), conf.resolver)
           }&lt;/li&gt;
	&lt;li&gt;case None =&amp;gt; u&lt;br/&gt;
+        case None =&amp;gt;&lt;br/&gt;
+          UnresolvedAttribute(u.nameParts)&lt;br/&gt;
       }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     case _ =&amp;gt;&lt;br/&gt;
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala&lt;br/&gt;
index a8639d29f964d..7141b6e996389 100644&lt;br/&gt;
&amp;#8212; a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala&lt;br/&gt;
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/higherOrderFunctions.scala&lt;br/&gt;
@@ -22,12 +22,34 @@ import java.util.concurrent.atomic.AtomicReference&lt;br/&gt;
 import scala.collection.mutable&lt;/p&gt;

&lt;p&gt; import org.apache.spark.sql.catalyst.InternalRow&lt;br/&gt;
-import org.apache.spark.sql.catalyst.analysis.&lt;/p&gt;
{TypeCheckResult, TypeCoercion, UnresolvedAttribute}
&lt;p&gt;+import org.apache.spark.sql.catalyst.analysis.&lt;/p&gt;
{TypeCheckResult, TypeCoercion, UnresolvedAttribute, UnresolvedException}
&lt;p&gt; import org.apache.spark.sql.catalyst.expressions.codegen._&lt;br/&gt;
 import org.apache.spark.sql.catalyst.util._&lt;br/&gt;
 import org.apache.spark.sql.types._&lt;br/&gt;
 import org.apache.spark.unsafe.array.ByteArrayMethods&lt;/p&gt;

&lt;p&gt;+/**&lt;br/&gt;
+ * A placeholder of lambda variables to prevent unexpected resolution of [&lt;span class=&quot;error&quot;&gt;&amp;#91;LambdaFunction&amp;#93;&lt;/span&gt;].&lt;br/&gt;
+ */&lt;br/&gt;
+case class UnresolvedNamedLambdaVariable(nameParts: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;)&lt;br/&gt;
+  extends LeafExpression with NamedExpression with Unevaluable &lt;/p&gt;
{
+
+  override def name: String =
+    nameParts.map(n =&amp;gt; if (n.contains(&quot;.&quot;)) s&quot;`$n`&quot; else n).mkString(&quot;.&quot;)
+
+  override def exprId: ExprId = throw new UnresolvedException(this, &quot;exprId&quot;)
+  override def dataType: DataType = throw new UnresolvedException(this, &quot;dataType&quot;)
+  override def nullable: Boolean = throw new UnresolvedException(this, &quot;nullable&quot;)
+  override def qualifier: Seq[String] = throw new UnresolvedException(this, &quot;qualifier&quot;)
+  override def toAttribute: Attribute = throw new UnresolvedException(this, &quot;toAttribute&quot;)
+  override def newInstance(): NamedExpression = throw new UnresolvedException(this, &quot;newInstance&quot;)
+  override lazy val resolved = false
+
+  override def toString: String = s&quot;lambda &apos;$name&quot;
+
+  override def sql: String = name
+}
&lt;p&gt;+&lt;br/&gt;
 /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A named lambda variable.&lt;br/&gt;
  */&lt;br/&gt;
@@ -79,7 +101,7 @@ case class LambdaFunction(&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; object LambdaFunction {&lt;br/&gt;
   val identity: LambdaFunction = &lt;/p&gt;
{
-    val id = UnresolvedAttribute.quoted(&quot;id&quot;)
+    val id = UnresolvedNamedLambdaVariable(Seq(&quot;id&quot;))
     LambdaFunction(id, Seq(id))
   }
&lt;p&gt; }&lt;br/&gt;
diff --git a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala&lt;br/&gt;
index 672bffcfc0cad..8959f78b656d2 100644&lt;br/&gt;
&amp;#8212; a/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala&lt;br/&gt;
+++ b/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/parser/AstBuilder.scala&lt;br/&gt;
@@ -1338,9 +1338,12 @@ class AstBuilder(conf: SQLConf) extends SqlBaseBaseVisitor&lt;span class=&quot;error&quot;&gt;&amp;#91;AnyRef&amp;#93;&lt;/span&gt; with Logging&lt;br/&gt;
    */&lt;br/&gt;
   override def visitLambda(ctx: LambdaContext): Expression = withOrigin(ctx) {&lt;br/&gt;
     val arguments = ctx.IDENTIFIER().asScala.map &lt;/p&gt;
{ name =&amp;gt;
-      UnresolvedAttribute.quoted(name.getText)
+      UnresolvedNamedLambdaVariable(UnresolvedAttribute.quoted(name.getText).nameParts)
     }
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LambdaFunction(expression(ctx.expression), arguments)&lt;br/&gt;
+    val function = expression(ctx.expression).transformUp 
{
+      case a: UnresolvedAttribute =&amp;gt; UnresolvedNamedLambdaVariable(a.nameParts)
+    }
&lt;p&gt;+    LambdaFunction(function, arguments)&lt;br/&gt;
   }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   /**&lt;br/&gt;
diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveLambdaVariablesSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveLambdaVariablesSuite.scala&lt;br/&gt;
index c4171c75ecd03..a5847ba7c522d 100644&lt;br/&gt;
&amp;#8212; a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveLambdaVariablesSuite.scala&lt;br/&gt;
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/analysis/ResolveLambdaVariablesSuite.scala&lt;br/&gt;
@@ -49,19 +49,21 @@ class ResolveLambdaVariablesSuite extends PlanTest &lt;/p&gt;
{
     comparePlans(Analyzer.execute(plan(e1)), plan(e2))
   }

&lt;p&gt;+  private def lv(s: Symbol) = UnresolvedNamedLambdaVariable(Seq(s.name))&lt;br/&gt;
+&lt;br/&gt;
   test(&quot;resolution - no op&quot;) &lt;/p&gt;
{
     checkExpression(key, key)
   }

&lt;p&gt;   test(&quot;resolution - simple&quot;) &lt;/p&gt;
{
-    val in = ArrayTransform(values1, LambdaFunction(&apos;x.attr + 1, &apos;x.attr :: Nil))
+    val in = ArrayTransform(values1, LambdaFunction(lv(&apos;x) + 1, lv(&apos;x) :: Nil))
     val out = ArrayTransform(values1, LambdaFunction(lvInt + 1, lvInt :: Nil))
     checkExpression(in, out)
   }

&lt;p&gt;   test(&quot;resolution - nested&quot;) {&lt;br/&gt;
     val in = ArrayTransform(values2, LambdaFunction(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;ArrayTransform(&apos;x.attr, LambdaFunction(&apos;x.attr + 1, &apos;x.attr :: Nil)), &apos;x.attr :: Nil))&lt;br/&gt;
+      ArrayTransform(lv(&apos;x), LambdaFunction(lv(&apos;x) + 1, lv(&apos;x) :: Nil)), lv(&apos;x) :: Nil))&lt;br/&gt;
     val out = ArrayTransform(values2, LambdaFunction(&lt;br/&gt;
       ArrayTransform(lvArray, LambdaFunction(lvInt + 1, lvInt :: Nil)), lvArray :: Nil))&lt;br/&gt;
     checkExpression(in, out)&lt;br/&gt;
@@ -75,14 +77,14 @@ class ResolveLambdaVariablesSuite extends PlanTest {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   test(&quot;fail - name collisions&quot;) &lt;/p&gt;
{
     val p = plan(ArrayTransform(values1,
-      LambdaFunction(&apos;x.attr + &apos;X.attr, &apos;x.attr :: &apos;X.attr :: Nil)))
+      LambdaFunction(lv(&apos;x) + lv(&apos;X), lv(&apos;x) :: lv(&apos;X) :: Nil)))
     val msg = intercept[AnalysisException](Analyzer.execute(p)).getMessage
     assert(msg.contains(&quot;arguments should not have names that are semantically the same&quot;))
   }

&lt;p&gt;   test(&quot;fail - lambda arguments&quot;) &lt;/p&gt;
{
     val p = plan(ArrayTransform(values1,
-      LambdaFunction(&apos;x.attr + &apos;y.attr + &apos;z.attr, &apos;x.attr :: &apos;y.attr :: &apos;z.attr :: Nil)))
+      LambdaFunction(lv(&apos;x) + lv(&apos;y) + lv(&apos;z), lv(&apos;x) :: lv(&apos;y) :: lv(&apos;z) :: Nil)))
     val msg = intercept[AnalysisException](Analyzer.execute(p)).getMessage
     assert(msg.contains(&quot;does not match the number of arguments expected&quot;))
   }
&lt;p&gt;diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala&lt;br/&gt;
index ee0d04da3e46c..748075bfd6a68 100644&lt;br/&gt;
&amp;#8212; a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala&lt;br/&gt;
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/optimizer/ReplaceNullWithFalseInPredicateSuite.scala&lt;br/&gt;
@@ -20,7 +20,7 @@ package org.apache.spark.sql.catalyst.optimizer&lt;br/&gt;
 import org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute&lt;br/&gt;
 import org.apache.spark.sql.catalyst.dsl.expressions._&lt;br/&gt;
 import org.apache.spark.sql.catalyst.dsl.plans._&lt;br/&gt;
-import org.apache.spark.sql.catalyst.expressions.&lt;/p&gt;
{And, ArrayExists, ArrayFilter, ArrayTransform, CaseWhen, Expression, GreaterThan, If, LambdaFunction, Literal, MapFilter, NamedExpression, Or}
&lt;p&gt;+import org.apache.spark.sql.catalyst.expressions.&lt;/p&gt;
{And, ArrayExists, ArrayFilter, ArrayTransform, CaseWhen, Expression, GreaterThan, If, LambdaFunction, Literal, MapFilter, NamedExpression, Or, UnresolvedNamedLambdaVariable}
&lt;p&gt; import org.apache.spark.sql.catalyst.expressions.Literal.&lt;/p&gt;
{FalseLiteral, TrueLiteral}
&lt;p&gt; import org.apache.spark.sql.catalyst.plans.&lt;/p&gt;
{Inner, PlanTest}
&lt;p&gt; import org.apache.spark.sql.catalyst.plans.logical.&lt;/p&gt;
{LocalRelation, LogicalPlan}
&lt;p&gt;@@ -306,22 +306,24 @@ class ReplaceNullWithFalseInPredicateSuite extends PlanTest &lt;/p&gt;
{
     testProjection(originalExpr = column, expectedExpr = column)
   }

&lt;p&gt;+  private def lv(s: Symbol) = UnresolvedNamedLambdaVariable(Seq(s.name))&lt;br/&gt;
+&lt;br/&gt;
   test(&quot;replace nulls in lambda function of ArrayFilter&quot;) &lt;/p&gt;
{
-    testHigherOrderFunc(&apos;a, ArrayFilter, Seq(&apos;e))
+    testHigherOrderFunc(&apos;a, ArrayFilter, Seq(lv(&apos;e)))
   }

&lt;p&gt;   test(&quot;replace nulls in lambda function of ArrayExists&quot;) &lt;/p&gt;
{
-    testHigherOrderFunc(&apos;a, ArrayExists, Seq(&apos;e))
+    testHigherOrderFunc(&apos;a, ArrayExists, Seq(lv(&apos;e)))
   }

&lt;p&gt;   test(&quot;replace nulls in lambda function of MapFilter&quot;) &lt;/p&gt;
{
-    testHigherOrderFunc(&apos;m, MapFilter, Seq(&apos;k, &apos;v))
+    testHigherOrderFunc(&apos;m, MapFilter, Seq(lv(&apos;k), lv(&apos;v)))
   }

&lt;p&gt;   test(&quot;inability to replace nulls in arbitrary higher-order function&quot;) &lt;/p&gt;
{
     val lambdaFunc = LambdaFunction(
-      function = If(&apos;e &amp;gt; 0, Literal(null, BooleanType), TrueLiteral),
-      arguments = Seq[NamedExpression](&apos;e))
+      function = If(lv(&apos;e) &amp;gt; 0, Literal(null, BooleanType), TrueLiteral),
+      arguments = Seq[NamedExpression](lv(&apos;e)))
     val column = ArrayTransform(&apos;a, lambdaFunc)
     testProjection(originalExpr = column, expectedExpr = column)
   }
&lt;p&gt;diff --git a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala&lt;br/&gt;
index b4df22c5b29fa..8bcc69d580d83 100644&lt;br/&gt;
&amp;#8212; a/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala&lt;br/&gt;
+++ b/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/parser/ExpressionParserSuite.scala&lt;br/&gt;
@@ -246,9 +246,11 @@ class ExpressionParserSuite extends PlanTest &lt;/p&gt;
{
     intercept(&quot;foo(a x)&quot;, &quot;extraneous input &apos;x&apos;&quot;)
   }

&lt;p&gt;+  private def lv(s: Symbol) = UnresolvedNamedLambdaVariable(Seq(s.name))&lt;br/&gt;
+&lt;br/&gt;
   test(&quot;lambda functions&quot;) &lt;/p&gt;
{
-    assertEqual(&quot;x -&amp;gt; x + 1&quot;, LambdaFunction(&apos;x + 1, Seq(&apos;x.attr)))
-    assertEqual(&quot;(x, y) -&amp;gt; x + y&quot;, LambdaFunction(&apos;x + &apos;y, Seq(&apos;x.attr, &apos;y.attr)))
+    assertEqual(&quot;x -&amp;gt; x + 1&quot;, LambdaFunction(lv(&apos;x) + 1, Seq(lv(&apos;x))))
+    assertEqual(&quot;(x, y) -&amp;gt; x + y&quot;, LambdaFunction(lv(&apos;x) + lv(&apos;y), Seq(lv(&apos;x), lv(&apos;y))))
   }

&lt;p&gt;   test(&quot;window function expressions&quot;) {&lt;br/&gt;
diff --git a/sql/core/src/test/resources/sql-tests/results/typeCoercion/native/mapZipWith.sql.out b/sql/core/src/test/resources/sql-tests/results/typeCoercion/native/mapZipWith.sql.out&lt;br/&gt;
index 35740094ba53e..86a578ca013df 100644&lt;br/&gt;
&amp;#8212; a/sql/core/src/test/resources/sql-tests/results/typeCoercion/native/mapZipWith.sql.out&lt;br/&gt;
+++ b/sql/core/src/test/resources/sql-tests/results/typeCoercion/native/mapZipWith.sql.out&lt;br/&gt;
@@ -85,7 +85,7 @@ FROM various_maps&lt;br/&gt;
 struct&amp;lt;&amp;gt;&lt;br/&gt;
 &amp;#8211; !query 5 output&lt;br/&gt;
 org.apache.spark.sql.AnalysisException&lt;br/&gt;
-cannot resolve &apos;map_zip_with(various_maps.`decimal_map1`, various_maps.`decimal_map2`, lambdafunction(named_struct(NamePlaceholder(), `k`, NamePlaceholder(), `v1`, NamePlaceholder(), `v2`), `k`, `v1`, `v2`))&apos; due to argument data type mismatch: The input to function map_zip_with should have been two maps with compatible key types, but the key types are &lt;span class=&quot;error&quot;&gt;&amp;#91;decimal(36,0), decimal(36,35)&amp;#93;&lt;/span&gt;.; line 1 pos 7&lt;br/&gt;
+cannot resolve &apos;map_zip_with(various_maps.`decimal_map1`, various_maps.`decimal_map2`, lambdafunction(named_struct(NamePlaceholder(), k, NamePlaceholder(), v1, NamePlaceholder(), v2), k, v1, v2))&apos; due to argument data type mismatch: The input to function map_zip_with should have been two maps with compatible key types, but the key types are &lt;span class=&quot;error&quot;&gt;&amp;#91;decimal(36,0), decimal(36,35)&amp;#93;&lt;/span&gt;.; line 1 pos 7&lt;/p&gt;


&lt;p&gt; &amp;#8211; !query 6&lt;br/&gt;
@@ -113,7 +113,7 @@ FROM various_maps&lt;br/&gt;
 struct&amp;lt;&amp;gt;&lt;br/&gt;
 &amp;#8211; !query 8 output&lt;br/&gt;
 org.apache.spark.sql.AnalysisException&lt;br/&gt;
-cannot resolve &apos;map_zip_with(various_maps.`decimal_map2`, various_maps.`int_map`, lambdafunction(named_struct(NamePlaceholder(), `k`, NamePlaceholder(), `v1`, NamePlaceholder(), `v2`), `k`, `v1`, `v2`))&apos; due to argument data type mismatch: The input to function map_zip_with should have been two maps with compatible key types, but the key types are &lt;span class=&quot;error&quot;&gt;&amp;#91;decimal(36,35), int&amp;#93;&lt;/span&gt;.; line 1 pos 7&lt;br/&gt;
+cannot resolve &apos;map_zip_with(various_maps.`decimal_map2`, various_maps.`int_map`, lambdafunction(named_struct(NamePlaceholder(), k, NamePlaceholder(), v1, NamePlaceholder(), v2), k, v1, v2))&apos; due to argument data type mismatch: The input to function map_zip_with should have been two maps with compatible key types, but the key types are &lt;span class=&quot;error&quot;&gt;&amp;#91;decimal(36,35), int&amp;#93;&lt;/span&gt;.; line 1 pos 7&lt;/p&gt;


&lt;p&gt; &amp;#8211; !query 9&lt;br/&gt;
diff --git a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala&lt;br/&gt;
index e6d1a038a5918..b7fc9570af919 100644&lt;br/&gt;
&amp;#8212; a/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala&lt;br/&gt;
+++ b/sql/core/src/test/scala/org/apache/spark/sql/DataFrameFunctionsSuite.scala&lt;br/&gt;
@@ -2908,6 +2908,26 @@ class DataFrameFunctionsSuite extends QueryTest with SharedSQLContext {&lt;br/&gt;
     }&lt;br/&gt;
     assert(ex.getMessage.contains(&quot;Cannot use null as map key&quot;))&lt;br/&gt;
   }&lt;br/&gt;
+&lt;br/&gt;
+  test(&quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26370&quot; title=&quot;Fix resolution of higher-order function for the same identifier.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26370&quot;&gt;&lt;del&gt;SPARK-26370&lt;/del&gt;&lt;/a&gt;: Fix resolution of higher-order function for the same identifier&quot;) &lt;/p&gt;
{
+    val df = Seq(
+      (Seq(1, 9, 8, 7), 1, 2),
+      (Seq(5, 9, 7), 2, 2),
+      (Seq.empty, 3, 2),
+      (null, 4, 2)
+    ).toDF(&quot;i&quot;, &quot;x&quot;, &quot;d&quot;)
+
+    checkAnswer(df.selectExpr(&quot;x&quot;, &quot;exists(i, x -&amp;gt; x % d == 0)&quot;),
+      Seq(
+        Row(1, true),
+        Row(2, false),
+        Row(3, false),
+        Row(4, null)))
+    checkAnswer(df.filter(&quot;exists(i, x -&amp;gt; x % d == 0)&quot;),
+      Seq(Row(Seq(1, 9, 8, 7), 1, 2)))
+    checkAnswer(df.select(&quot;x&quot;).filter(&quot;exists(i, x -&amp;gt; x % d == 0)&quot;),
+      Seq(Row(1)))
+  }
&lt;p&gt; }&lt;/p&gt;

&lt;p&gt; object DataFrameFunctionsSuite {&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16721578" author="cloud_fan" created="Fri, 14 Dec 2018 16:30:18 +0000"  >&lt;p&gt;Issue resolved by pull request 23320&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/23320&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23320&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 48 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|s01im8:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>