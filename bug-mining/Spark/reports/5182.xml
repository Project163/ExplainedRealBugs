<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:55:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-21928] ClassNotFoundException for custom Kryo registrator class during serde in netty threads</title>
                <link>https://issues.apache.org/jira/browse/SPARK-21928</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;From &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13990&quot; title=&quot;Automatically pick serializer when caching RDDs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-13990&quot;&gt;&lt;del&gt;SPARK-13990&lt;/del&gt;&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13926&quot; title=&quot;Automatically use Kryo serializer when shuffling RDDs with simple types&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-13926&quot;&gt;&lt;del&gt;SPARK-13926&lt;/del&gt;&lt;/a&gt;, Spark&apos;s SerializerManager has its own instance of a KryoSerializer which does not have the defaultClassLoader set on it. For normal task execution, that doesn&apos;t cause problems, because the serializer falls back to the current thread&apos;s task loader, which is set anyway.&lt;/p&gt;

&lt;p&gt;however, netty maintains its own thread pool, and those threads don&apos;t change their classloader to include the extra use jars needed for the custom kryo registrator. That only matters when blocks are sent across the network which force serde in the netty thread. That won&apos;t happen often, because (a) spark tries to execute tasks where the RDDs are already cached and (b) broadcast blocks generally don&apos;t require any serde in the netty threads (that occurs in the task thread that is reading the broadcast value).  However it can come up with remote cache reads, or if fetching a broadcast block forces another block to disk, which requires serialization.&lt;/p&gt;

&lt;p&gt;This doesn&apos;t effect the shuffle path, because the serde is never done in the threads created by netty.&lt;/p&gt;

&lt;p&gt;I think a fix for this should be fairly straight-forward, we just need to set the classloader on that extra kryo instance.&lt;/p&gt;

&lt;p&gt; (original problem description below)&lt;/p&gt;

&lt;p&gt;I unfortunately can&apos;t reliably reproduce this bug; it happens only occasionally, when training a logistic regression model with very large datasets. The training will often proceed through several &lt;tt&gt;treeAggregate&lt;/tt&gt; calls without any problems, and then suddenly workers will start running into this &lt;tt&gt;java.lang.ClassNotFoundException&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;After doing some debugging, it seems that whenever this error happens, Spark is trying to use the &lt;tt&gt;sun.misc.Launcher$AppClassLoader&lt;/tt&gt; &lt;tt&gt;ClassLoader&lt;/tt&gt; instance instead of the usual &lt;tt&gt;org.apache.spark.util.MutableURLClassLoader&lt;/tt&gt;. &lt;tt&gt;MutableURLClassLoader&lt;/tt&gt; can see my custom Kryo registrator, but the &lt;tt&gt;AppClassLoader&lt;/tt&gt; instance can&apos;t.&lt;/p&gt;

&lt;p&gt;When this error does pop up, it&apos;s usually accompanied by the task seeming to hang, and I need to kill Spark manually.&lt;/p&gt;

&lt;p&gt;I&apos;m running a Spark application in cluster mode via spark-submit, and I have a custom Kryo registrator. The JAR is built with &lt;tt&gt;sbt assembly&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Exception message:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;17/08/29 22:39:04 ERROR TransportRequestHandler: Error opening block StreamChunkId{streamId=542074019336, chunkIndex=0} for request from /10.0.29.65:34332
org.apache.spark.SparkException: Failed to register classes with Kryo
    at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:139)
    at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:292)
    at org.apache.spark.serializer.KryoSerializerInstance.&amp;lt;init&amp;gt;(KryoSerializer.scala:277)
    at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:186)
    at org.apache.spark.serializer.SerializerManager.dataSerializeStream(SerializerManager.scala:169)
    at org.apache.spark.storage.BlockManager$$anonfun$dropFromMemory$3.apply(BlockManager.scala:1382)
    at org.apache.spark.storage.BlockManager$$anonfun$dropFromMemory$3.apply(BlockManager.scala:1377)
    at org.apache.spark.storage.DiskStore.put(DiskStore.scala:69)
    at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1377)
    at org.apache.spark.storage.memory.MemoryStore.org$apache$spark$storage$memory$MemoryStore$$dropBlock$1(MemoryStore.scala:524)
    at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:545)
    at org.apache.spark.storage.memory.MemoryStore$$anonfun$evictBlocksToFreeSpace$2.apply(MemoryStore.scala:539)
    at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
    at org.apache.spark.storage.memory.MemoryStore.evictBlocksToFreeSpace(MemoryStore.scala:539)
    at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:92)
    at org.apache.spark.memory.StorageMemoryPool.acquireMemory(StorageMemoryPool.scala:73)
    at org.apache.spark.memory.StaticMemoryManager.acquireStorageMemory(StaticMemoryManager.scala:72)
    at org.apache.spark.storage.memory.MemoryStore.putBytes(MemoryStore.scala:147)
    at org.apache.spark.storage.BlockManager.maybeCacheDiskBytesInMemory(BlockManager.scala:1143)
    at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doGetLocalBytes(BlockManager.scala:594)
    at org.apache.spark.storage.BlockManager$$anonfun$getLocalBytes$2.apply(BlockManager.scala:559)
    at org.apache.spark.storage.BlockManager$$anonfun$getLocalBytes$2.apply(BlockManager.scala:559)
    at scala.Option.map(Option.scala:146)
    at org.apache.spark.storage.BlockManager.getLocalBytes(BlockManager.scala:559)
    at org.apache.spark.storage.BlockManager.getBlockData(BlockManager.scala:353)
    at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:61)
    at org.apache.spark.network.netty.NettyBlockRpcServer$$anonfun$1.apply(NettyBlockRpcServer.scala:60)
    at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
    at scala.collection.convert.Wrappers$IteratorWrapper.next(Wrappers.scala:31)
    at org.apache.spark.network.server.OneForOneStreamManager.getChunk(OneForOneStreamManager.java:89)
    at org.apache.spark.network.server.TransportRequestHandler.processFetchRequest(TransportRequestHandler.java:125)
    at org.apache.spark.network.server.TransportRequestHandler.handle(TransportRequestHandler.java:103)
    at org.apache.spark.network.server.TransportChannelHandler.channelRead(TransportChannelHandler.java:118)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at io.netty.handler.timeout.IdleStateHandler.channelRead(IdleStateHandler.java:287)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:102)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:85)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:336)
    at io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1294)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:357)
    at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:343)
    at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:911)
    at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
    at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:643)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:566)
    at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:480)
    at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:442)
    at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:131)
    at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
    at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: com.foo.bar.MyKryoRegistrator
    at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
    at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:331)
    at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
    at java.lang.Class.forName0(Native Method)
    at java.lang.Class.forName(Class.java:348)
    at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$5.apply(KryoSerializer.scala:134)
    at org.apache.spark.serializer.KryoSerializer$$anonfun$newKryo$5.apply(KryoSerializer.scala:134)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
    at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)
    at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
    at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:134)
    ... 60 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;My Spark session is created like so:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val spark = SparkSession.builder()
                .appName(&lt;span class=&quot;code-quote&quot;&gt;&quot;FooBar&quot;&lt;/span&gt;)
                .config(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.serializer&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.serializer.KryoSerializer&quot;&lt;/span&gt;)
                .config(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.kryoserializer.buffer.max&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;2047m&quot;&lt;/span&gt;)                                        
                .config(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.kryo.registrator&quot;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&quot;com.foo.bar.MyKryoRegistrator&quot;&lt;/span&gt;)
                .config(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.kryo.registrationRequired&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&quot;&lt;/span&gt;)
                .config(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.network.timeout&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;3600s&quot;&lt;/span&gt;)
                .config(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.driver.maxResultSize&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;0&quot;&lt;/span&gt;)
                .config(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.rdd.compress&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&quot;&lt;/span&gt;)
                .config(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.shuffle.spill&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&quot;&lt;/span&gt;)
                .getOrCreate()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;Here are the config options I&apos;m passing to spark-submit:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;--conf &quot;spark.executor.heartbeatInterval=400s&quot;
--conf &quot;spark.speculation=true&quot;
--conf &quot;spark.speculation.multiplier=30&quot;
--conf &quot;spark.speculation.quantile=0.95&quot;
--conf &quot;spark.memory.useLegacyMode=true&quot;
--conf &quot;spark.shuffle.memoryFraction=0.8&quot;
--conf &quot;spark.storage.memoryFraction=0.2&quot;
--driver-java-options &quot;-XX:+UseG1GC&quot;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I was able to find a workaround: copy your application JAR to each of the machines in your cluster, and pass the JAR&apos;s path to &lt;tt&gt;spark-submit&lt;/tt&gt; with:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;--conf &quot;spark.driver.extraClassPath=/path/to/sparklogisticregre&#8204;&#8203;ssion.jar&quot;
--conf &quot;spark.executor.extraClassPath=/path/to/sparklogisticreg&#8204;&#8203;ression.jar&quot;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13099975">SPARK-21928</key>
            <summary>ClassNotFoundException for custom Kryo registrator class during serde in netty threads</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="irashid">Imran Rashid</assignee>
                                    <reporter username="jbrock">John Brock</reporter>
                        <labels>
                    </labels>
                <created>Tue, 5 Sep 2017 22:37:20 +0000</created>
                <updated>Mon, 20 Nov 2023 17:37:31 +0000</updated>
                            <resolved>Thu, 21 Sep 2017 17:21:47 +0000</resolved>
                                    <version>2.1.1</version>
                    <version>2.2.0</version>
                                    <fixVersion>2.1.2</fixVersion>
                    <fixVersion>2.2.1</fixVersion>
                    <fixVersion>2.3.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="16171715" author="irashid" created="Tue, 19 Sep 2017 13:40:51 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbrock&quot; class=&quot;user-hover&quot; rel=&quot;jbrock&quot;&gt;jbrock&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;thanks for reporting this.  I have another bug report which I think is similar, I don&apos;t think this actually has anything to do with ML, it can happen anytime cached RDDs get sent over the network (perhaps that is a bit more likely to happen with ML workloads).&lt;/p&gt;

&lt;p&gt;I do have one question for you, though &amp;#8211; in the other bug report I have, the user says that when they hit this error, the executor gets stuck, which eventually leads to the application failing.  However, I haven&apos;t been able to reproduce that so far &amp;#8211; there are some errors, but from what I see, the executor just gives up fetching the remote data, and regenerates it locally.  What have you observed when this happens?&lt;/p&gt;

&lt;p&gt;more details:&lt;/p&gt;

&lt;p&gt;From &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13990&quot; title=&quot;Automatically pick serializer when caching RDDs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-13990&quot;&gt;&lt;del&gt;SPARK-13990&lt;/del&gt;&lt;/a&gt; &amp;amp; &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-13926&quot; title=&quot;Automatically use Kryo serializer when shuffling RDDs with simple types&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-13926&quot;&gt;&lt;del&gt;SPARK-13926&lt;/del&gt;&lt;/a&gt;, Spark&apos;s SerializerManager has &lt;a href=&quot;https://github.com/apache/spark/blob/581200af717bcefd11c9930ac063fe53c6fd2fde/core/src/main/scala/org/apache/spark/serializer/SerializerManager.scala#L42&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;its own instance of a KryoSerializer&lt;/a&gt; which does not have the defaultClassLoader set on it. For normal task execution, that doesn&apos;t cause problems, because the serializer falls back to the current thread&apos;s task loader, which is set anyway.&lt;/p&gt;

&lt;p&gt;however, netty maintains its own thread pool, and those threads don&apos;t change their classloader to include the extra use jars needed for the custom kryo registrator. That only matters when cached RDDs are sent across the network. That won&apos;t happen often, because spark tries to execute tasks where the RDDs are already cached.  (You&apos;ll notice your stack trace includes netty stuff underneath.)&lt;/p&gt;

&lt;p&gt;This doesn&apos;t effect the shuffle path, because the serde is never done in the threads created by netty.&lt;/p&gt;

&lt;p&gt;I think a fix for this should be fairly straight-forward, we just need to set the classloader on that extra kryo instance.&lt;/p&gt;</comment>
                            <comment id="16172115" author="apachespark" created="Tue, 19 Sep 2017 18:16:05 +0000"  >&lt;p&gt;User &apos;squito&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19280&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19280&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16172123" author="jbrock" created="Tue, 19 Sep 2017 18:24:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=irashid&quot; class=&quot;user-hover&quot; rel=&quot;irashid&quot;&gt;irashid&lt;/a&gt;, thanks for taking a look. I see the same thing as that user &amp;#8211; the executor gets stuck, causing the application to fail. Before I found the workaround I mentioned above with spark.driver.extraClassPath and spark.executor.extraClassPath, I was using speculation to kill off the hanging tasks, although this wasn&apos;t always enough (e.g., if a stage got stuck before reaching the speculation threshold), and sometimes caused long-running (but non-stuck) tasks to be killed.&lt;/p&gt;</comment>
                            <comment id="16174247" author="irashid" created="Thu, 21 Sep 2017 04:11:16 +0000"  >&lt;p&gt;I believe I figured out why things get stuck sometimes, filed &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-22083&quot; title=&quot;When dropping multiple blocks to disk, Spark should release all locks on a failure&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-22083&quot;&gt;&lt;del&gt;SPARK-22083&lt;/del&gt;&lt;/a&gt;.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbrock&quot; class=&quot;user-hover&quot; rel=&quot;jbrock&quot;&gt;jbrock&lt;/a&gt;, if you happen to still have logs from a case where you see this, can you check if the stuck executor shows something like &quot;INFO MemoryStore: 2 blocks selected for dropping&quot; (or maybe more than 2)?&lt;/p&gt;</comment>
                            <comment id="16175066" author="jbrock" created="Thu, 21 Sep 2017 16:37:20 +0000"  >&lt;p&gt;It does! I see this in the log right before an executor got stuck:&lt;br/&gt;
&lt;tt&gt;17/08/31 19:56:25 INFO MemoryStore: 3 blocks selected for dropping (284.2 MB bytes)&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="16175385" author="apachespark" created="Thu, 21 Sep 2017 20:12:04 +0000"  >&lt;p&gt;User &apos;squito&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19313&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19313&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16175410" author="irashid" created="Thu, 21 Sep 2017 20:29:32 +0000"  >&lt;p&gt;thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbrock&quot; class=&quot;user-hover&quot; rel=&quot;jbrock&quot;&gt;jbrock&lt;/a&gt;, thats great.  I think this is fully explained now.  I updated the title and description so folks know it is not related to ML, hope that is OK.&lt;/p&gt;</comment>
                            <comment id="16175452" author="jbrock" created="Thu, 21 Sep 2017 20:59:47 +0000"  >&lt;p&gt;Excellent, thanks for looking into this.&lt;/p&gt;</comment>
                            <comment id="16663313" author="soravgulati@gmail.com" created="Thu, 25 Oct 2018 06:22:22 +0000"  >&lt;p&gt;I am using Spark 2.30 version and I am still&#160;getting this Exception. Is it not fixed in Spark 2.3.0?&lt;/p&gt;</comment>
                            <comment id="16663798" author="irashid" created="Thu, 25 Oct 2018 14:14:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=soravgulati%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;soravgulati@gmail.com&quot;&gt;soravgulati@gmail.com&lt;/a&gt; this is believed to be fixed in 2.3.0. Can you share more details about what you see &amp;#8211; the full stack trace and what you were trying to do?  Its possible there is another cause of a similar exception.&lt;/p&gt;</comment>
                            <comment id="17788104" author="shivamsharma" created="Mon, 20 Nov 2023 17:37:31 +0000"  >&lt;p&gt;I am getting this intermittent failure on spark 2.4.3 version. Here is the full stack trace:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; java.lang.reflect.InvocationTargetException    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)    at java.lang.reflect.Method.invoke(Method.java:498)    at org.apache.spark.deploy.worker.DriverWrapper$.main(DriverWrapper.scala:65)    at org.apache.spark.deploy.worker.DriverWrapper.main(DriverWrapper.scala)Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 75 in stage 1.0 failed 4 times, most recent failure: Lost task 75.3 in stage 1.0 (TID 171, phx6-kwq.prod.xyz.internal, executor 71): java.io.IOException: org.apache.spark.SparkException: Failed to register classes with Kryo    at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)    at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:208)    at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)    at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)    at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)    at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:89)    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)    at org.apache.spark.scheduler.Task.run(Task.scala:121)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)Caused by: org.apache.spark.SparkException: Failed to register classes with Kryo    at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:140)    at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:324)    at org.apache.spark.serializer.KryoSerializerInstance.&amp;lt;init&amp;gt;(KryoSerializer.scala:309)    at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:218)    at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:305)    at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$3(TorrentBroadcast.scala:235)    at scala.Option.getOrElse(Option.scala:138)    at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:211)    at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)    ... 14 moreCaused by: java.lang.ClassNotFoundException: com.xyz.datashack.SparkKryoRegistrar    at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.findClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:530)    at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35)    at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:424)    at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)    at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(ChildFirstURLClassLoader.java:48)    at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:357)    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName0(Native Method)    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.java:348)    at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$6(KryoSerializer.scala:135)    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)    at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)    at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)    at scala.collection.TraversableLike.map(TraversableLike.scala:237)    at scala.collection.TraversableLike.map$(TraversableLike.scala:230)    at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)    at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:135)    ... 22 more
Driver stacktrace:    at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1889)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1877)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1876)    at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)    at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)    at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)    at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1876)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:926)    at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:926)    at scala.Option.foreach(Option.scala:274)    at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2110)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2059)    at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2048)    at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)    at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)    at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)    at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$1(RDD.scala:935)    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)    at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)    at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)    at org.apache.spark.rdd.RDD.foreachPartition(RDD.scala:933)    at org.apache.spark.api.java.JavaRDDLike.foreachPartition(JavaRDDLike.scala:219)    at org.apache.spark.api.java.JavaRDDLike.foreachPartition$(JavaRDDLike.scala:218)    at org.apache.spark.api.java.AbstractJavaRDDLike.foreachPartition(JavaRDDLike.scala:45)    at com.xyz.datashack.tcdn.Backfill.buildIndex(Backfill.java:211)    at com.xyz.datashack.tcdn.Backfill.runWithContext(Backfill.java:122)    at com.xyz.datashack.tcdn.Backfill.lambda$main$0(Backfill.java:94)    at com.xyz.datashack.tcdn.TCDClient.runWithReporting(TCDClient.java:54)    at com.xyz.datashack.tcdn.Backfill.main(Backfill.java:94)    ... 6 moreCaused by: java.io.IOException: org.apache.spark.SparkException: Failed to register classes with Kryo    at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1333)    at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:208)    at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute(TorrentBroadcast.scala:66)    at org.apache.spark.broadcast.TorrentBroadcast._value(TorrentBroadcast.scala:66)    at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:96)    at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:89)    at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)    at org.apache.spark.scheduler.Task.run(Task.scala:121)    at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)    at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)Caused by: org.apache.spark.SparkException: Failed to register classes with Kryo    at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:140)    at org.apache.spark.serializer.KryoSerializerInstance.borrowKryo(KryoSerializer.scala:324)    at org.apache.spark.serializer.KryoSerializerInstance.&amp;lt;init&amp;gt;(KryoSerializer.scala:309)    at org.apache.spark.serializer.KryoSerializer.newInstance(KryoSerializer.scala:218)    at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:305)    at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$3(TorrentBroadcast.scala:235)    at scala.Option.getOrElse(Option.scala:138)    at org.apache.spark.broadcast.TorrentBroadcast.$anonfun$readBroadcastBlock$1(TorrentBroadcast.scala:211)    at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1326)    ... 14 moreCaused by: java.lang.ClassNotFoundException: com.xyz.datashack.SparkKryoRegistrar    at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.findClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:530)    at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.java:35)    at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:424)    at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.java:40)    at org.apache.spark.util.ChildFirstURLClassLoader.loadClass(ChildFirstURLClassLoader.java:48)    at java.lang.&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.loadClass(&lt;span class=&quot;code-object&quot;&gt;ClassLoader&lt;/span&gt;.java:357)    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName0(Native Method)    at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.forName(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.java:348)    at org.apache.spark.serializer.KryoSerializer.$anonfun$newKryo$6(KryoSerializer.scala:135)    at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:237)    at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)    at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)    at scala.collection.TraversableLike.map(TraversableLike.scala:237)    at scala.collection.TraversableLike.map$(TraversableLike.scala:230)    at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)    at org.apache.spark.serializer.KryoSerializer.newKryo(KryoSerializer.scala:135)    ... 22 more &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13103808">SPARK-22083</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 51 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3johb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>