<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:32:36 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-41497] Accumulator undercounting in the case of retry task with rdd cache</title>
                <link>https://issues.apache.org/jira/browse/SPARK-41497</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Accumulator could be undercounted when the retried task has rdd cache.&#160; See the example below and you could also find the completed and reproducible example at &lt;a href=&quot;https://github.com/apache/spark/compare/master...Ngone51:spark:fix-acc&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/compare/master...Ngone51:spark:fix-acc&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-scala&quot;&gt;
test(&lt;span class=&quot;code-quote&quot;&gt;&quot;SPARK-XXX&quot;&lt;/span&gt;) {
  &lt;span class=&quot;code-comment&quot;&gt;// Set up a cluster &lt;span class=&quot;code-keyword&quot;&gt;with&lt;/span&gt; 2 executors
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;val&lt;/span&gt; conf = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf()
    .setMaster(&lt;span class=&quot;code-quote&quot;&gt;&quot;local-cluster[2, 1, 1024]&quot;&lt;/span&gt;).setAppName(&lt;span class=&quot;code-quote&quot;&gt;&quot;TaskSchedulerImplSuite&quot;&lt;/span&gt;)
  sc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkContext(conf)
  &lt;span class=&quot;code-comment&quot;&gt;// Set up a custom task scheduler. The scheduler will fail the first task attempt of the job
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;// submitted below. In particular, the failed first attempt task would success on computation
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;// (accumulator accounting, result caching) but only fail to report its success status due
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;// to the concurrent executor lost. The second task attempt would success.
&lt;/span&gt;  taskScheduler = setupSchedulerWithCustomStatusUpdate(sc)
  &lt;span class=&quot;code-keyword&quot;&gt;val&lt;/span&gt; myAcc = sc.longAccumulator(&lt;span class=&quot;code-quote&quot;&gt;&quot;myAcc&quot;&lt;/span&gt;)
  &lt;span class=&quot;code-comment&quot;&gt;// Initiate a rdd &lt;span class=&quot;code-keyword&quot;&gt;with&lt;/span&gt; only one partition so there&apos;s only one task and specify the storage level
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;with&lt;/span&gt; MEMORY_ONLY_2 so that the rdd result will be cached on both two executors.
&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;val&lt;/span&gt; rdd = sc.parallelize(0 until 10, 1).mapPartitions { iter =&amp;gt;
    myAcc.add(100)
    iter.map(x =&amp;gt; x + 1)
  }.persist(StorageLevel.MEMORY_ONLY_2)
  &lt;span class=&quot;code-comment&quot;&gt;// This will pass since the second task attempt will succeed
&lt;/span&gt;  assert(rdd.count() === 10)
  &lt;span class=&quot;code-comment&quot;&gt;// This will fail due to `myAcc.add(100)` won&apos;t be executed during the second task attempt&apos;s
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;// execution. Because the second task attempt will load the rdd cache directly instead of
&lt;/span&gt;  &lt;span class=&quot;code-comment&quot;&gt;// executing the task function so `myAcc.add(100)` is skipped.
&lt;/span&gt;  assert(myAcc.value === 100)
} &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;We could also hit this issue with decommission even if the rdd only has one copy. For example, decommission could migrate the rdd cache block to another executor (the result is actually the same with 2 copies) and the decommissioned executor lost before the task reports its success status to the driver.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;And the issue is a bit more complicated than expected to fix. I have tried to give some fixes but all of them are not ideal:&lt;/p&gt;

&lt;p&gt;Option 1: Clean up any rdd cache related to the failed task: in practice, this option can already fix the issue in most cases. However, theoretically, rdd cache could be reported to the driver right after the driver cleans up the failed task&apos;s caches due to asynchronous communication. So this option can&#8217;t resolve the issue thoroughly;&lt;/p&gt;

&lt;p&gt;Option 2: Disallow rdd cache reuse across the task attempts for the same task: this option can 100% fix the issue. The problem is this way can also affect the case where rdd cache can be reused across the attempts (e.g., when there is no accumulator operation in the task), which can have perf regression;&lt;/p&gt;

&lt;p&gt;Option 3:&#160;Introduce accumulator cache: first, this requires a new framework for supporting accumulator cache; second, the driver should improve its logic to distinguish whether the accumulator cache value should be reported to the user to avoid overcounting. For example, in the case of task retry, the value should be reported. However, in the case of rdd cache reuse, the value shouldn&#8217;t be reported (should it?);&lt;/p&gt;

&lt;p&gt;Option 4: Do task success validation when a task trying to load the rdd cache: this way defines a rdd cache is only valid/accessible if the task has succeeded. This way could be either overkill or a bit complex (because currently Spark would clean up the task state once it&#8217;s finished. So we need to maintain a structure to know if task once succeeded or not. )&lt;/p&gt;</description>
                <environment></environment>
        <key id="13513178">SPARK-41497</key>
            <summary>Accumulator undercounting in the case of retry task with rdd cache</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ivoson">Tengfei Huang</assignee>
                                    <reporter username="Ngone51">wuyi</reporter>
                        <labels>
                    </labels>
                <created>Mon, 12 Dec 2022 12:22:21 +0000</created>
                <updated>Sun, 5 Mar 2023 06:11:40 +0000</updated>
                            <resolved>Fri, 3 Mar 2023 00:11:09 +0000</resolved>
                                    <version>2.4.8</version>
                    <version>3.0.3</version>
                    <version>3.1.3</version>
                    <version>3.2.2</version>
                    <version>3.3.1</version>
                                    <fixVersion>3.5.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="17646097" author="ngone51" created="Mon, 12 Dec 2022 13:24:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mridulm80&quot; class=&quot;user-hover&quot; rel=&quot;mridulm80&quot;&gt;mridulm80&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tgraves&quot; class=&quot;user-hover&quot; rel=&quot;tgraves&quot;&gt;tgraves&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=attilapiros&quot; class=&quot;user-hover&quot; rel=&quot;attilapiros&quot;&gt;attilapiros&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ivoson&quot; class=&quot;user-hover&quot; rel=&quot;ivoson&quot;&gt;ivoson&lt;/a&gt; any good ideas?&lt;/p&gt;</comment>
                            <comment id="17646477" author="mridulm80" created="Tue, 13 Dec 2022 07:40:05 +0000"  >&lt;p&gt;Agree - there appears to be a bunch of scenarios where this can be triggered.&lt;br/&gt;
Essentially, whenever block save succeeds and task itself fails, we can end up with this scenario.&lt;br/&gt;
As detailed, this could be storage level with replication &amp;gt; 1, block decomissioning, etc - where executor or driver has replicated the data.&lt;br/&gt;
I think this can also happen when task itself fails - but after persisting the data (even without replication or decomm) - for example, due to some local/transient shuffle write issues (for example) or commitdenied, etc.&lt;/p&gt;

&lt;p&gt;For the options listed:&lt;br/&gt;
Agree, option 1 does not solve the issue.&lt;br/&gt;
I am also not inclined towards option 2 due to the potential perf impact - though I would expect this to be a rare scenario.&lt;br/&gt;
Option 3 looks like a very involved approach, and I am not sure if we can cover all the corner cases.&lt;/p&gt;

&lt;p&gt;I am wondering if we can modify option 4 such that it helps.&lt;br/&gt;
There are multiple approaches perhaps - one strawman proposal:&lt;br/&gt;
a) Add a bit to BlockStatus indicating whether block can be used or not. And currently this bit gets flipped when the task which computed it successfully completes.&lt;br/&gt;
b) Maintain a taskId -&amp;gt; BlockStatus* mapping - which is cleaned up whenever a task completes (if successful, then flip bit - else remove its blocks - and replicas (if any)).&lt;br/&gt;
c) Propagate taskId in reportBlockStatus from doPutIterator, etc - where new block is getting created in the system.&lt;/p&gt;

&lt;p&gt;Thoughts ?&lt;/p&gt;</comment>
                            <comment id="17646639" author="ngone51" created="Tue, 13 Dec 2022 13:36:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mridulm80&quot; class=&quot;user-hover&quot; rel=&quot;mridulm80&quot;&gt;mridulm80&lt;/a&gt; Sounds like a better idea than option 4. But I think it still doesn&apos;t work well for the case like:&lt;/p&gt;

&lt;p&gt;For example, a task is constructed by `rdd1.cache().rdd2`. So if the task fails due to rdd2&apos;s computation, I think rdd1&apos;s cache should still be able to reuse.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17646658" author="ivoson" created="Tue, 13 Dec 2022 14:25:25 +0000"  >&lt;p&gt;I also think that option3/4(include the improved proposal &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mridulm80&quot; class=&quot;user-hover&quot; rel=&quot;mridulm80&quot;&gt;mridulm80&lt;/a&gt; suggested) would be promising, the issue could be resolved either from the accumulator side or rdd cache side.&lt;br/&gt;
And option4 seems more straightforward since it&apos;s a complement to existing cache mechanism. And making decision based on task status could be a feasible solution. As mentioned above, the downside is that it may be overkill. If such cases are small probability events, maybe it is also acceptable.&lt;/p&gt;</comment>
                            <comment id="17646825" author="mridulm80" created="Tue, 13 Dec 2022 21:19:20 +0000"  >&lt;p&gt;&amp;gt; For example, a task is constructed by `rdd1.cache().rdd2`. So if the task fails due to rdd2&apos;s computation, I think rdd1&apos;s cache should still be able to reuse.&lt;/p&gt;

&lt;p&gt;We have three cases here.&lt;br/&gt;
For some initial task T1:&lt;/p&gt;

&lt;p&gt;a) Computation of rdd2 within T1 should have no issues, since initial computation would result in caching it locally - and rdd2 computation is using the result of that local read iterator &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;.&lt;br/&gt;
b) Failure of T1 and now T2 executes for the same partition - as proposed, T2 will not use result of T1&apos;s cache, since it not marked as usable (even though the block might be still marked cached &lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;).&lt;br/&gt;
c) Replication and/or decom and T2 runs - same case as (b).&lt;/p&gt;

&lt;p&gt;Probably &apos;usable&apos; is incorrect term - &apos;visible&apos; might be better ? That is, is this block visible to others (outside of the generating task).&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; We should special case and allow reads from the same task which cached the block - even if it has not yet been marked usable.&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; When T1 failed, we would drop the blocks which got cached due to it. But even in the case of a race, the flag prevents the use of the cached block.&lt;/p&gt;</comment>
                            <comment id="17646919" author="ngone51" created="Wed, 14 Dec 2022 05:09:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mridulm80&quot; class=&quot;user-hover&quot; rel=&quot;mridulm80&quot;&gt;mridulm80&lt;/a&gt;&#160; For b) and c), shouldn&apos;t we allow T2 to use the result of T1&apos;s cache if rdd1&apos;s computation doesn&apos;t include any accumulators?&lt;/p&gt;</comment>
                            <comment id="17646962" author="mridulm80" created="Wed, 14 Dec 2022 06:49:36 +0000"  >&lt;p&gt;Agree, if we can determine that - do we have a way to do that ?&lt;/p&gt;</comment>
                            <comment id="17646969" author="ngone51" created="Wed, 14 Dec 2022 07:19:02 +0000"  >&lt;p&gt;&amp;gt; do we have a way to do that ?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mridulm80&quot; class=&quot;user-hover&quot; rel=&quot;mridulm80&quot;&gt;mridulm80&lt;/a&gt;&#160; Currently, we only have the mapping between the task and accumulators. Accumulators are registered to the task via TaskContext.get() when they deserialize at the executor.&lt;/p&gt;

&lt;p&gt;If we could have a way to know which RDD scope the accumulator within when deserializing, we could set up the mapping between the RDD and accumulators then. This probably&lt;/p&gt;

&lt;p&gt;be the most difficult part.&lt;/p&gt;</comment>
                            <comment id="17646971" author="ngone51" created="Wed, 14 Dec 2022 07:28:56 +0000"  >&lt;p&gt;I&apos;m thinking if we could improve the improved Option 4 by changing the rdd cache reuse condition a bit:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;if there are no accumulators (external only probably) values changed after the rdd computation, then the rdd&apos;s cache should be marked as usable/visible no matter whether the task succeeds or fail;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;If there are accumulators values changed after the rdd computation, then the rdd&apos;s cache should only be marked as usable/visible only when the task succeeds.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;(let me think further and see if it&apos;s doable..)&lt;/p&gt;</comment>
                            <comment id="17646972" author="mridulm80" created="Wed, 14 Dec 2022 07:30:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ngone51&quot; class=&quot;user-hover&quot; rel=&quot;Ngone51&quot;&gt;Ngone51&lt;/a&gt; Agree, that is what I was not sure of (whether we can detect this scenario about use of accumulators which might be updated subsequently). Note that updates to the same accumulator can happen before and after a cache in user code - so we might be able to only catch scenario when there are no accumulators.&lt;br/&gt;
If I am not wrong, SQL makes very heavy use of accumulators, and so most stages will end up having them anyway - right ?&lt;/p&gt;

&lt;p&gt;I would expect this scenario (even without accumulator) to be fairly low frequency enough that the cost of extra recomputation might be fine.&lt;/p&gt;</comment>
                            <comment id="17653648" author="juliuszsompolski" created="Mon, 2 Jan 2023 16:09:09 +0000"  >&lt;p&gt;Note that this issue leads to a correctness issue in Delta Merge, because it depends on a SetAccumulator as a side output channel for gathering files that need to be rewritten by the Merge: &lt;a href=&quot;https://github.com/delta-io/delta/blob/master/core/src/main/scala/org/apache/spark/sql/delta/commands/MergeIntoCommand.scala#L445-L449&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/delta-io/delta/blob/master/core/src/main/scala/org/apache/spark/sql/delta/commands/MergeIntoCommand.scala#L445-L449&lt;/a&gt;&lt;br/&gt;
Delta assumes that Spark accumulators can overcount (in some cases where task retries update them in duplicate), but it was assumed that it should never undercount and lose output like that...&lt;/p&gt;

&lt;p&gt;Missing some files there can result in duplicate records being inserted instead of existing records being updated.&lt;/p&gt;</comment>
                            <comment id="17653781" author="ngone51" created="Tue, 3 Jan 2023 05:01:21 +0000"  >&lt;p&gt;&amp;gt; If I am not wrong, SQL makes very heavy use of accumulators, and so most stages will end up having them anyway - right ?&lt;/p&gt;

&lt;p&gt;Right.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&amp;gt; I would expect this scenario (even without accumulator) to be fairly low frequency enough that the cost of extra recomputation might be fine.&lt;br/&gt;
&#160;&lt;br/&gt;
Agree. So shall we proceed with the improved Option 4 that was proposed by you &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mridulm80&quot; class=&quot;user-hover&quot; rel=&quot;mridulm80&quot;&gt;mridulm80&lt;/a&gt; ? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ivoson&quot; class=&quot;user-hover&quot; rel=&quot;ivoson&quot;&gt;ivoson&lt;/a&gt; can help with the fix.&lt;br/&gt;
&#160;&lt;/p&gt;</comment>
                            <comment id="17654590" author="mridulm80" created="Wed, 4 Jan 2023 18:32:34 +0000"  >&lt;p&gt;Sounds good &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Ngone51&quot; class=&quot;user-hover&quot; rel=&quot;Ngone51&quot;&gt;Ngone51&lt;/a&gt;, thanks !&lt;/p&gt;</comment>
                            <comment id="17655806" author="apachespark" created="Sun, 8 Jan 2023 16:31:12 +0000"  >&lt;p&gt;User &apos;ivoson&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/39459&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/39459&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17655807" author="apachespark" created="Sun, 8 Jan 2023 16:31:38 +0000"  >&lt;p&gt;User &apos;ivoson&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/39459&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/39459&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17695936" author="mridulm80" created="Fri, 3 Mar 2023 00:11:09 +0000"  >&lt;p&gt;Issue resolved by pull request 39459&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/39459&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/39459&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17696526" author="apachespark" created="Sun, 5 Mar 2023 06:11:40 +0000"  >&lt;p&gt;User &apos;ivoson&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/40281&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/40281&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 36 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1dyx4:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>