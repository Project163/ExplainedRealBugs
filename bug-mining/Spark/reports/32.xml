<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:11:55 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-815] PySpark&apos;s parallelize() should batch objects after partitioning (instead of before)</title>
                <link>https://issues.apache.org/jira/browse/SPARK-815</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;PySpark uses batching when serializing and deserializing Python objects.  By default, it serializes objects in groups of 1024.&lt;/p&gt;

&lt;p&gt;The current batching code causes SparkContext.parallelize() to behave counterintuitively when parallelizing small datasets.  The current code batches the objects, then parallelizes the batches, so calls to parallelize() with small inputs will be unaffected by the number of partitions:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([1, 2, 3, 4], 2)
&amp;gt;&amp;gt;&amp;gt; rdd.glom().collect()
[[], [1, 2, 3, 4]]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Instead, parallelize() should first partition the elements and then batch them:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt;&amp;gt;&amp;gt; rdd = sc.parallelize([1, 2, 3, 4], 2)
&amp;gt;&amp;gt;&amp;gt; rdd.glom().collect()
[[1, 2], [3, 4]]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Maybe parallelize() should accept an option to control the batch size (right now, it can only be set when creating the SparkContext).&lt;/p&gt;</description>
                <environment></environment>
        <key id="12704541">SPARK-815</key>
            <summary>PySpark&apos;s parallelize() should batch objects after partitioning (instead of before)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="matei">Matei Alexandru Zaharia</assignee>
                                    <reporter username="joshrosen">Josh Rosen</reporter>
                        <labels>
                    </labels>
                <created>Mon, 15 Jul 2013 19:09:30 +0000</created>
                <updated>Sun, 30 Mar 2014 23:33:02 +0000</updated>
                            <resolved>Sun, 28 Jul 2013 23:56:37 +0000</resolved>
                                    <version>0.7.0</version>
                    <version>0.7.1</version>
                    <version>0.7.2</version>
                    <version>0.7.3</version>
                                    <fixVersion>0.7.3</fixVersion>
                    <fixVersion>0.8.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="13953130" author="matei" created="Sun, 28 Jul 2013 23:56:17 +0000"  >&lt;p&gt;I&apos;ve fixed this here: &lt;a href=&quot;https://github.com/mesos/spark/commit/feba7ee540fca28872957120e5e39b9e36466953&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/mesos/spark/commit/feba7ee540fca28872957120e5e39b9e36466953&lt;/a&gt;, though the solution is not completely perfect because it requires materializing the list if you pass a generator. On the other hand, that&apos;s what the Scala/Java parallelize does as well.&lt;/p&gt;

&lt;p&gt;Maybe a slightly better solution would be to materialize the first context.batchSize * numSplits elements from the generator, write those out with even splitting if you&apos;ve reached the end of the generator, or write them out as full batches otherwise. But I&apos;m not sure we want to go to this level of complexity now.&lt;/p&gt;

&lt;p&gt;A better thing to add by the way would be smart splitting support for xrange, similar to that of Range in Scala.&lt;/p&gt;</comment>
                            <comment id="13953132" author="joshrosen" created="Mon, 29 Jul 2013 00:03:08 +0000"  >&lt;p&gt;Funny timing: I wrote my own fix for this tonight, but I like your fix better: &lt;a href=&quot;https://github.com/JoshRosen/spark/compare/spark-815&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/JoshRosen/spark/compare/spark-815&lt;/a&gt;.  &lt;/p&gt;

&lt;p&gt;In my branch, I cleaned up some old test code that was setting the batch size to 2 to work around this issue.  We should probably clean that up in master now, too.&lt;/p&gt;</comment>
                            <comment id="13953133" author="matei" created="Mon, 29 Jul 2013 00:07:21 +0000"  >&lt;p&gt;I tried the tests and they all work with this, but it could be nice to update them if you&apos;d like.&lt;/p&gt;</comment>
                            <comment id="13953134" author="matei" created="Mon, 29 Jul 2013 00:10:23 +0000"  >&lt;p&gt;Sorry for not giving a heads-up BTW, I just fixed a bunch of small issues since I was coming back from this conference: &lt;a href=&quot;http://www.pydata.org/bos2013/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.pydata.org/bos2013/&lt;/a&gt; and had some time on the plane. One of the other annoying things I&apos;ve fixed is that first() and take() now stop after returning the first few elements, instead of computing the rest of the partition anyway, which makes them quite a bit faster.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>383431</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 17 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1u23b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>383699</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>