<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:37:50 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-13210] NPE in Sort</title>
                <link>https://issues.apache.org/jira/browse/SPARK-13210</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When run TPCDS query Q78 with scale 10:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;16/02/04 22:39:09 ERROR Executor: Managed memory leak detected; size = 268435456 bytes, TID = 143
16/02/04 22:39:09 ERROR Executor: Exception in task 0.0 in stage 47.0 (TID 143)
java.lang.NullPointerException
	at org.apache.spark.memory.TaskMemoryManager.getPage(TaskMemoryManager.java:333)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortComparator.compare(UnsafeInMemorySorter.java:60)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortComparator.compare(UnsafeInMemorySorter.java:39)
	at org.apache.spark.util.collection.TimSort.countRunAndMakeAscending(TimSort.java:270)
	at org.apache.spark.util.collection.TimSort.sort(TimSort.java:142)
	at org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.getSortedIterator(UnsafeInMemorySorter.java:239)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.getSortedIterator(UnsafeExternalSorter.java:415)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:116)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:168)
	at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:87)
	at org.apache.spark.sql.execution.Sort$$anonfun$1.apply(Sort.scala:60)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:735)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$23.apply(RDD.scala:735)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:313)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:277)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:77)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:45)
	at org.apache.spark.scheduler.Task.run(Task.scala:81)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12936962">SPARK-13210</key>
            <summary>NPE in Sort</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="davies">Davies Liu</assignee>
                                    <reporter username="davies">Davies Liu</reporter>
                        <labels>
                    </labels>
                <created>Fri, 5 Feb 2016 06:47:09 +0000</created>
                <updated>Wed, 28 Feb 2018 01:23:05 +0000</updated>
                            <resolved>Mon, 8 Feb 2016 20:10:49 +0000</resolved>
                                    <version>1.6.0</version>
                    <version>2.0.0</version>
                                    <fixVersion>1.6.1</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="15134828" author="apachespark" created="Fri, 5 Feb 2016 19:43:06 +0000"  >&lt;p&gt;User &apos;davies&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/11095&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/11095&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15137595" author="joshrosen" created="Mon, 8 Feb 2016 20:10:49 +0000"  >&lt;p&gt;Issue resolved by pull request 11095&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/11095&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/11095&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15137598" author="joshrosen" created="Mon, 8 Feb 2016 20:11:08 +0000"  >&lt;p&gt;I&apos;m also going to cherry-pick this for 1.6.1.&lt;/p&gt;</comment>
                            <comment id="15495921" author="tzachz" created="Fri, 16 Sep 2016 10:10:47 +0000"  >&lt;p&gt;I&apos;ve just seen this happening on Spark 1.6.2 - very similar stack trace (line number changed slightly, but same stack).&lt;/p&gt;

&lt;p&gt;I&apos;m guessing fix wasn&apos;t ported to 1.6.1 eventually? &lt;br/&gt;
If so - maybe 1.6.* versions should be added to &quot;affected versions&quot;?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-none&quot;&gt;java.lang.NullPointerException
	at org.apache.spark.memory.TaskMemoryManager.getPage(TaskMemoryManager.java:351)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortComparator.compare(UnsafeInMemorySorter.java:56)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortComparator.compare(UnsafeInMemorySorter.java:37)
	at org.apache.spark.util.collection.TimSort.countRunAndMakeAscending(TimSort.java:270)
	at org.apache.spark.util.collection.TimSort.sort(TimSort.java:142)
	at org.apache.spark.util.collection.Sorter.sort(Sorter.scala:37)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.getSortedIterator(UnsafeInMemorySorter.java:235)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:186)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:175)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:249)
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:83)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.reset(UnsafeInMemorySorter.java:122)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:201)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:175)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:249)
	at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:112)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPageIfNecessary(UnsafeExternalSorter.java:332)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.insertKVRecord(UnsafeExternalSorter.java:373)
	at org.apache.spark.sql.execution.UnsafeKVExternalSorter.insertKV(UnsafeKVExternalSorter.java:139)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply$mcV$sp(WriterContainer.scala:377)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply(WriterContainer.scala:343)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer$$anonfun$writeRows$4.apply(WriterContainer.scala:343)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1277)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:409)
	... 8 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="15543975" author="joshrosen" created="Tue, 4 Oct 2016 01:12:22 +0000"  >&lt;p&gt;Hmm, weird. It looks like this was cherry-picked in &lt;a href=&quot;https://github.com/apache/spark/commit/9b30096227263f77fc67ed8f12fb2911c3256774&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/9b30096227263f77fc67ed8f12fb2911c3256774&lt;/a&gt; and should therefore be in 1.6.1 (at least according to &lt;a href=&quot;https://github.com/apache/spark/compare/9b30096227263f77fc67ed8f12fb2911c3256774...branch-1.6&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/compare/9b30096227263f77fc67ed8f12fb2911c3256774...branch-1.6&lt;/a&gt;), so I wonder whether this is a different issue.&lt;/p&gt;</comment>
                            <comment id="15754237" author="emlyn" created="Fri, 16 Dec 2016 11:56:08 +0000"  >&lt;p&gt;I&apos;ve just had this suspiciously similar stack trace in Spark 2.0.2, not sure if it&apos;s the same thing:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.NullPointerException
	at org.apache.spark.memory.TaskMemoryManager.getPage(TaskMemoryManager.java:347)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.writeSortedFile(ShuffleExternalSorter.java:197)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.spill(ShuffleExternalSorter.java:259)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:171)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:245)
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:92)
	at org.apache.spark.shuffle.sort.ShuffleInMemorySorter.reset(ShuffleInMemorySorter.java:100)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.spill(ShuffleExternalSorter.java:261)
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:171)
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:245)
	at org.apache.spark.memory.MemoryConsumer.allocatePage(MemoryConsumer.java:121)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.acquireNewPageIfNecessary(ShuffleExternalSorter.java:364)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.insertRecord(ShuffleExternalSorter.java:387)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.insertRecordIntoSorter(UnsafeShuffleWriter.java:241)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:162)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15960147" author="gowthamvenkatrao" created="Fri, 7 Apr 2017 01:47:12 +0000"  >&lt;p&gt;we are facing same exception in spark 2.1.0&lt;/p&gt;

&lt;p&gt;java.lang.NullPointerException&lt;br/&gt;
	at org.apache.spark.memory.TaskMemoryManager.getPage(TaskMemoryManager.java:347)&lt;br/&gt;
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.writeSortedFile(ShuffleExternalSorter.java:193)&lt;br/&gt;
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.spill(ShuffleExternalSorter.java:254)&lt;br/&gt;
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:171)&lt;br/&gt;
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:245)&lt;br/&gt;
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:92)&lt;br/&gt;
	at org.apache.spark.shuffle.sort.ShuffleInMemorySorter.reset(ShuffleInMemorySorter.java:100)&lt;br/&gt;
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.spill(ShuffleExternalSorter.java:256)&lt;br/&gt;
	at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:171)&lt;br/&gt;
	at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:245)&lt;br/&gt;
	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:92)&lt;br/&gt;
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.growPointerArrayIfNecessary(ShuffleExternalSorter.java:328)&lt;br/&gt;
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.insertRecord(ShuffleExternalSorter.java:379)&lt;br/&gt;
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.insertRecordIntoSorter(UnsafeShuffleWriter.java:246)&lt;br/&gt;
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:167)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)&lt;br/&gt;
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:99)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;</comment>
                            <comment id="15960414" author="srowen" created="Fri, 7 Apr 2017 07:32:50 +0000"  >&lt;p&gt;For anyone seeing this exception, can you run with assertions on (&lt;tt&gt;-ea&lt;/tt&gt;)? I can&apos;t see a way this happens unless the page in the page table is null, and the code is asserting it&apos;s impossible, at least in master. If that&apos;s not true that much has to be investigated.&lt;/p&gt;</comment>
                            <comment id="16005588" author="dmcwhorter" created="Wed, 10 May 2017 22:44:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=srowen&quot; class=&quot;user-hover&quot; rel=&quot;srowen&quot;&gt;srowen&lt;/a&gt; Here&apos;s the error from Spark 2.1.1 with assertions enabled:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Stage 28:&amp;gt;  (0 + 7) / 60&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Stage 36:&amp;gt;  (7 + 1) / 93&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Stage 37:&amp;gt;  (0 + 0) / 11&amp;#93;&lt;/span&gt;2017-05-10 18:43:05 ERROR TaskContextImpl:91 - Error in TaskCompletionListener&lt;br/&gt;
java.lang.AssertionError&lt;br/&gt;
        at org.apache.spark.memory.TaskMemoryManager.freePage(TaskMemoryManager.java:288)&lt;br/&gt;
        at org.apache.spark.memory.MemoryConsumer.freePage(MemoryConsumer.java:140)&lt;br/&gt;
        at org.apache.spark.memory.MemoryConsumer.freeArray(MemoryConsumer.java:110)&lt;br/&gt;
        at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.free(UnsafeInMemorySorter.java:166)&lt;br/&gt;
        at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.cleanupResources(UnsafeExternalSorter.java:329)&lt;br/&gt;
        at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter$1.onTaskCompletion(UnsafeExternalSorter.java:169)&lt;br/&gt;
        at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:97)&lt;br/&gt;
        at org.apache.spark.TaskContextImpl$$anonfun$markTaskCompleted$1.apply(TaskContextImpl.scala:95)&lt;br/&gt;
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)&lt;br/&gt;
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)&lt;br/&gt;
        at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:95)&lt;br/&gt;
        at org.apache.spark.scheduler.Task.run(Task.scala:112)&lt;br/&gt;
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
2017-05-10 18:43:06 ERROR Executor:91 - Exception in task 1.0 in stage 28.0 (TID 465)&lt;br/&gt;
org.apache.spark.util.TaskCompletionListenerException&lt;br/&gt;
        at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:105)&lt;br/&gt;
        at org.apache.spark.scheduler.Task.run(Task.scala:112)&lt;br/&gt;
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Stage 28:&amp;gt;  (0 + 8) / 60&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Stage 36:&amp;gt;  (7 + 1) / 93&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Stage 37:&amp;gt;  (0 + 0) / 11&amp;#93;&lt;/span&gt;2017-05-10 18:43:06 WARN  TaskSetManager:66 - Lost task 1.0 in stage 28.0 (TID 465, localhost, executor driver): org.apache.spark.util.TaskCompletionListenerException&lt;br/&gt;
        at org.apache.spark.TaskContextImpl.markTaskCompleted(TaskContextImpl.scala:105)&lt;br/&gt;
        at org.apache.spark.scheduler.Task.run(Task.scala:112)&lt;br/&gt;
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:322)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;2017-05-10 18:43:06 ERROR TaskSetManager:70 - Task 1 in stage 28.0 failed 1 times; aborting job&lt;/p&gt;</comment>
                            <comment id="16005591" author="dmcwhorter" created="Wed, 10 May 2017 22:50:27 +0000"  >&lt;p&gt;I think its the same error, here&apos;s the start of the stack trace with assertions disabled:&lt;/p&gt;

&lt;p&gt;Caused by: java.lang.NullPointerException&lt;br/&gt;
        at org.apache.spark.memory.TaskMemoryManager.getPage(TaskMemoryManager.java:347)&lt;br/&gt;
        at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:301)&lt;br/&gt;
        at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.spill(UnsafeExternalSorter.java:216)&lt;br/&gt;
        at org.apache.spark.memory.TaskMemoryManager.acquireExecutionMemory(TaskMemoryManager.java:171)&lt;br/&gt;
        at org.apache.spark.memory.TaskMemoryManager.allocatePage(TaskMemoryManager.java:245)&lt;br/&gt;
        at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:92)&lt;/p&gt;</comment>
                            <comment id="16007303" author="dmcwhorter" created="Thu, 11 May 2017 22:23:31 +0000"  >&lt;p&gt;is it worth reopening this issue, or creating a new one to track?&lt;/p&gt;</comment>
                            <comment id="16050807" author="michael@hurts.ca" created="Thu, 15 Jun 2017 17:19:36 +0000"  >&lt;p&gt;I also saw this in a run on Spark 2.1.1. Same stack trace as in David Whorter&apos;s comment from May 10. It&apos;s not consistent though - I got an OutOfMemoryError in another attempt:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.OutOfMemoryError: Unable to acquire 16384 bytes of memory, got 0
 	at org.apache.spark.memory.MemoryConsumer.allocateArray(MemoryConsumer.java:100)
 	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter.&amp;lt;init&amp;gt;(UnsafeInMemorySorter.java:126)
 	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.&amp;lt;init&amp;gt;(UnsafeExternalSorter.java:154)
 	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:121)
 	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.fetchNextPartition(WindowExec.scala:340)
 	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.next(WindowExec.scala:391)
 	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.next(WindowExec.scala:290)
 	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)
 	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
 	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377)
 	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.fetchNextRow(WindowExec.scala:301)
 	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14$$anon$1.&amp;lt;init&amp;gt;(WindowExec.scala:310)
 	at org.apache.spark.sql.execution.window.WindowExec$$anonfun$14.apply(WindowExec.scala:290)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16379614" author="l1990790120" created="Wed, 28 Feb 2018 01:22:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Aspecting&quot; class=&quot;user-hover&quot; rel=&quot;Aspecting&quot;&gt;Aspecting&lt;/a&gt; Were you able to get around that?&#160;Have the same stack trace in 2.1.1&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13099575">SPARK-21907</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 37 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2sgp3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>