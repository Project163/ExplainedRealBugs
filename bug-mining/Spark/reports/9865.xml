<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:43:43 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-49261] Correlation between lit and round during grouping</title>
                <link>https://issues.apache.org/jira/browse/SPARK-49261</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Running following code:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; pyspark.sql.functions as F
from decimal &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; Decimal

data = [
&#160; (1, 100, Decimal(&lt;span class=&quot;code-quote&quot;&gt;&quot;1.1&quot;&lt;/span&gt;), &#160;&lt;span class=&quot;code-quote&quot;&gt;&quot;L&quot;&lt;/span&gt;, True),
&#160; (2, 200, Decimal(&lt;span class=&quot;code-quote&quot;&gt;&quot;1.2&quot;&lt;/span&gt;), &#160;&lt;span class=&quot;code-quote&quot;&gt;&quot;H&quot;&lt;/span&gt;, False),
&#160; (2, 300, Decimal(&lt;span class=&quot;code-quote&quot;&gt;&quot;2.345&quot;&lt;/span&gt;), &lt;span class=&quot;code-quote&quot;&gt;&quot;E&quot;&lt;/span&gt;, False),
]

columns = [&lt;span class=&quot;code-quote&quot;&gt;&quot;group_a&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;amount&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;selector_a&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;selector_b&quot;&lt;/span&gt;]

df = spark.createDataFrame(data, schema=columns)

df_final = (
&#160; df.select(
&#160; &#160; F.lit(6).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;run_number&quot;&lt;/span&gt;),
&#160; &#160; F.lit(&lt;span class=&quot;code-quote&quot;&gt;&quot;AA&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;run_type&quot;&lt;/span&gt;),
&#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;group_a&quot;&lt;/span&gt;),
&#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;),
&#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount&quot;&lt;/span&gt;),
&#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_a&quot;&lt;/span&gt;),
&#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_b&quot;&lt;/span&gt;),
&#160; )
&#160; .withColumn(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;amount_c&quot;&lt;/span&gt;,
&#160; &#160; F.when(
&#160; &#160; &#160; (F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_b&quot;&lt;/span&gt;) == False)
&#160; &#160; &#160; &amp;amp; (F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_a&quot;&lt;/span&gt;).isin([&lt;span class=&quot;code-quote&quot;&gt;&quot;L&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;H&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;E&quot;&lt;/span&gt;])),
&#160; &#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount&quot;&lt;/span&gt;),
&#160; &#160; ).otherwise(F.lit(None))
&#160; )
&#160; .withColumn(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;,
&#160; &#160; F.when(
&#160; &#160; &#160; (F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_b&quot;&lt;/span&gt;) == False)
&#160; &#160; &#160; &amp;amp; (F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_a&quot;&lt;/span&gt;).isin([&lt;span class=&quot;code-quote&quot;&gt;&quot;L&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;H&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;E&quot;&lt;/span&gt;])),
&#160; &#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;)
&#160; &#160; ).otherwise(F.lit(None))
&#160; )
)

group_by_cols = [
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;run_number&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;group_a&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;run_type&quot;&lt;/span&gt;
]

df_final = df_final.groupBy(group_by_cols).agg(
&#160; F.countDistinct(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount&quot;&lt;/span&gt;),
&#160; F.round(F.sum(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount&quot;&lt;/span&gt;)/ 1000, 1).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;total_amount&quot;&lt;/span&gt;),
&#160; F.sum(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount_c&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount_c&quot;&lt;/span&gt;),
&#160; F.countDistinct(&lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;).alias(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;
&#160; ),
)

df_final = (
&#160; df_final
&#160; .withColumn(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;total_amount&quot;&lt;/span&gt;,
&#160; &#160; F.round(F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;total_amount&quot;&lt;/span&gt;) / 1000, 6),
&#160; )
&#160; .withColumn(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount&quot;&lt;/span&gt;, F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount&quot;&lt;/span&gt;).&lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&quot;&lt;/span&gt;)
&#160; )
&#160; .withColumn(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;,
&#160; &#160; F.when(
&#160; &#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount_c&quot;&lt;/span&gt;).isNull(), F.lit(None).&lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&quot;&lt;/span&gt;)
&#160; &#160; ).otherwise(F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;).&lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&quot;&lt;/span&gt;)),
&#160; )
)

df_final = df_final.select(
&#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;total_amount&quot;&lt;/span&gt;),
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;run_number&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;group_a&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;run_type&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;amount_c&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;,
)

df_final.show() &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Produces error:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[[INTERNAL_ERROR](https:&lt;span class=&quot;code-comment&quot;&gt;//docs.microsoft.com/azure/databricks/error-messages/error-classes#internal_error)] Couldn&apos;t find total_amount#1046 in [group_a#984L,count_of_amount#1054,amount_c#1033,count_of_amount_c#1034L] SQLSTATE: XX000 &lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;With stack trace:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.spark.SparkException: [INTERNAL_ERROR] Couldn&apos;t find total_amount#1046 in [group_a#984L,count_of_amount#1054,amount_c#1033,count_of_amount_c#1034L] SQLSTATE: XX000 at org.apache.spark.SparkException$.internalError(SparkException.scala:97) at org.apache.spark.SparkException$.internalError(SparkException.scala:101) at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:81) at org.apache.spark.sql.catalyst.expressions.BindReferences$$anonfun$bindReference$1.applyOrElse(BoundAttribute.scala:74) at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:505) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:83) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:505) at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:481) at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:449) at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReference(BoundAttribute.scala:74) at org.apache.spark.sql.catalyst.expressions.BindReferences$.$anonfun$bindReferences$1(BoundAttribute.scala:97) at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286) at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62) at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49) at scala.collection.TraversableLike.map(TraversableLike.scala:286) at scala.collection.TraversableLike.map$(TraversableLike.scala:279) at scala.collection.AbstractTraversable.map(Traversable.scala:108) at org.apache.spark.sql.catalyst.expressions.BindReferences$.bindReferences(BoundAttribute.scala:97) at org.apache.spark.sql.execution.ProjectExec.doConsume(basicPhysicalOperators.scala:74) at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:202) at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:155) at org.apache.spark.sql.execution.aggregate.HashAggregateExec.consume(HashAggregateExec.scala:51) at org.apache.spark.sql.execution.aggregate.HashAggregateExec.generateResultFunction(HashAggregateExec.scala:411) at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doConsumeWithKeys(HashAggregateExec.scala:995) at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.doConsume(AggregateCodegenSupport.scala:81) at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.doConsume$(AggregateCodegenSupport.scala:77) at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doConsume(HashAggregateExec.scala:51) at org.apache.spark.sql.execution.CodegenSupport.constructDoConsumeFunction(WholeStageCodegenExec.scala:229) at org.apache.spark.sql.execution.CodegenSupport.consume(WholeStageCodegenExec.scala:200) at org.apache.spark.sql.execution.CodegenSupport.consume$(WholeStageCodegenExec.scala:155) at org.apache.spark.sql.execution.InputAdapter.consume(WholeStageCodegenExec.scala:506) at org.apache.spark.sql.execution.InputRDDCodegen.doProduce(WholeStageCodegenExec.scala:493) at org.apache.spark.sql.execution.InputRDDCodegen.doProduce$(WholeStageCodegenExec.scala:466) at org.apache.spark.sql.execution.InputAdapter.doProduce(WholeStageCodegenExec.scala:506) at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:100) at org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:130) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:385) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:381) at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:95) at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:94) at org.apache.spark.sql.execution.InputAdapter.produce(WholeStageCodegenExec.scala:506) at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduceWithKeys(HashAggregateExec.scala:629) at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.doProduce(AggregateCodegenSupport.scala:73) at org.apache.spark.sql.execution.aggregate.AggregateCodegenSupport.doProduce$(AggregateCodegenSupport.scala:69) at org.apache.spark.sql.execution.aggregate.HashAggregateExec.doProduce(HashAggregateExec.scala:51) at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:100) at org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:130) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:385) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:381) at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:95) at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:94) at org.apache.spark.sql.execution.aggregate.HashAggregateExec.produce(HashAggregateExec.scala:51) at org.apache.spark.sql.execution.ProjectExec.doProduce(basicPhysicalOperators.scala:59) at org.apache.spark.sql.execution.CodegenSupport.$anonfun$produce$1(WholeStageCodegenExec.scala:100) at org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:130) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:385) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:381) at org.apache.spark.sql.execution.CodegenSupport.produce(WholeStageCodegenExec.scala:95) at org.apache.spark.sql.execution.CodegenSupport.produce$(WholeStageCodegenExec.scala:94) at org.apache.spark.sql.execution.ProjectExec.produce(basicPhysicalOperators.scala:46) at org.apache.spark.sql.execution.WholeStageCodegenExec.doCodeGen(WholeStageCodegenExec.scala:666) at org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:729) at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$2(SparkPlan.scala:327) at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94) at org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:327) at org.apache.spark.sql.execution.SparkPlan$.org$apache$spark$sql$execution$SparkPlan$$withExecuteQueryLogging(SparkPlan.scala:130) at org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:385) at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:165) at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:381) at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:322) at org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:117) at org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:131) at org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:94) at org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:90) at org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:78) at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:549) at com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:94) at org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:540) at org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$2(ResultCacheManager.scala:555) at org.apache.spark.sql.execution.adaptive.ResultQueryStageExec.$anonfun$doMaterialize$1(QueryStageExec.scala:663) at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1175) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$6(SQLExecution.scala:778) at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$5(SQLExecution.scala:778) at com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$4(SQLExecution.scala:778) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$3(SQLExecution.scala:777) at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:776) at org.apache.spark.sql.execution.SQLExecution$.withOptimisticTransaction(SQLExecution.scala:798) at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:775) at java.util.concurrent.CompletableFuture$AsyncSupply.run(CompletableFuture.java:1604) at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.$anonfun$run$1(SparkThreadLocalForwardingThreadPoolExecutor.scala:134) at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23) at com.databricks.spark.util.IdentityClaim$.withClaim(IdentityClaim.scala:48) at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.$anonfun$runWithCaptured$4(SparkThreadLocalForwardingThreadPoolExecutor.scala:91) at com.databricks.unity.UCSEphemeralState$Handle.runWith(UCSEphemeralState.scala:45) at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:90) at org.apache.spark.util.threads.SparkThreadLocalCapturingHelper.runWithCaptured$(SparkThreadLocalForwardingThreadPoolExecutor.scala:67) at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.runWithCaptured(SparkThreadLocalForwardingThreadPoolExecutor.scala:131) at org.apache.spark.util.threads.SparkThreadLocalCapturingRunnable.run(SparkThreadLocalForwardingThreadPoolExecutor.scala:134) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:750)
 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;It seems to be a correlation between &lt;b&gt;F.lit(6).alias(&quot;run_number&quot;)&lt;/b&gt; and &lt;b&gt;F.round(F.col(&quot;total_amount&quot;) / 1000, 6)&lt;/b&gt;. If both &lt;b&gt;lit&lt;/b&gt; and &lt;b&gt;scale&lt;/b&gt; in &lt;b&gt;round&lt;/b&gt; are set to the same number i.e. &lt;b&gt;6&lt;/b&gt; code fails.&lt;/p&gt;

&lt;p&gt;If numbers are different all works.&lt;/p&gt;

&lt;p&gt;Moving &lt;b&gt;F.lit(6).alias(&quot;run_number&quot;)&lt;/b&gt; to the final &lt;b&gt;select&lt;/b&gt; also solves the problem when both numbers in &lt;b&gt;lit&lt;/b&gt; and &lt;b&gt;scale&lt;/b&gt; in &lt;b&gt;round&lt;/b&gt;&#160;are the same.&lt;/p&gt;

&lt;p&gt;Example of the working code:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; pyspark.sql.functions as F
from decimal &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; Decimal

data = [&#160; (1, 100, Decimal(&lt;span class=&quot;code-quote&quot;&gt;&quot;1.1&quot;&lt;/span&gt;), &#160;&lt;span class=&quot;code-quote&quot;&gt;&quot;L&quot;&lt;/span&gt;, True),
&#160; (2, 200, Decimal(&lt;span class=&quot;code-quote&quot;&gt;&quot;1.2&quot;&lt;/span&gt;), &#160;&lt;span class=&quot;code-quote&quot;&gt;&quot;H&quot;&lt;/span&gt;, False),
&#160; (2, 300, Decimal(&lt;span class=&quot;code-quote&quot;&gt;&quot;2.345&quot;&lt;/span&gt;), &lt;span class=&quot;code-quote&quot;&gt;&quot;E&quot;&lt;/span&gt;, False),
]

columns = [&lt;span class=&quot;code-quote&quot;&gt;&quot;group_a&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;amount&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;selector_a&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;selector_b&quot;&lt;/span&gt;]

df = spark.createDataFrame(data, schema=columns)

df_final = (
&#160; df.select(
&#160; &#160; F.lit(7).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;run_number&quot;&lt;/span&gt;),
&#160; &#160; F.lit(&lt;span class=&quot;code-quote&quot;&gt;&quot;AA&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;run_type&quot;&lt;/span&gt;),
&#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;group_a&quot;&lt;/span&gt;),
&#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;),
&#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount&quot;&lt;/span&gt;),
&#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_a&quot;&lt;/span&gt;),
&#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_b&quot;&lt;/span&gt;),
&#160; )
&#160; .withColumn(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;amount_c&quot;&lt;/span&gt;,
&#160; &#160; F.when(
&#160; &#160; &#160; (F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_b&quot;&lt;/span&gt;) == False)
&#160; &#160; &#160; &amp;amp; (F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_a&quot;&lt;/span&gt;).isin([&lt;span class=&quot;code-quote&quot;&gt;&quot;L&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;H&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;E&quot;&lt;/span&gt;])),
&#160; &#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount&quot;&lt;/span&gt;),
&#160; &#160; ).otherwise(F.lit(None))
&#160; )
&#160; .withColumn(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;,
&#160; &#160; F.when(
&#160; &#160; &#160; (F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_b&quot;&lt;/span&gt;) == False)
&#160; &#160; &#160; &amp;amp; (F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;selector_a&quot;&lt;/span&gt;).isin([&lt;span class=&quot;code-quote&quot;&gt;&quot;L&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;H&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;E&quot;&lt;/span&gt;])),
&#160; &#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;)
&#160; &#160; ).otherwise(F.lit(None))
&#160; )
)

group_by_cols = [
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;run_number&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;group_a&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;run_type&quot;&lt;/span&gt;
]

df_final = df_final.groupBy(group_by_cols).agg(
&#160; F.countDistinct(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount&quot;&lt;/span&gt;),
&#160; F.round(F.sum(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount&quot;&lt;/span&gt;)/ 1000, 1).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;total_amount&quot;&lt;/span&gt;),
&#160; F.sum(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount_c&quot;&lt;/span&gt;).alias(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount_c&quot;&lt;/span&gt;),
&#160; F.countDistinct(&lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;).alias(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;
&#160; ),
)

df_final = (
&#160; df_final
&#160; .withColumn(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;total_amount&quot;&lt;/span&gt;,
&#160; &#160; F.round(F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;total_amount&quot;&lt;/span&gt;) / 1000, 6),
&#160; )
&#160; .withColumn(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount&quot;&lt;/span&gt;, F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount&quot;&lt;/span&gt;).&lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&quot;&lt;/span&gt;)
&#160; )
&#160; .withColumn(
&#160; &#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;,
&#160; &#160; F.when(
&#160; &#160; &#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;amount_c&quot;&lt;/span&gt;).isNull(), F.lit(None).&lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&quot;&lt;/span&gt;)
&#160; &#160; ).otherwise(F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;).&lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;&quot;&lt;/span&gt;)),
&#160; )
)

df_final = df_final.select(
&#160; F.col(&lt;span class=&quot;code-quote&quot;&gt;&quot;total_amount&quot;&lt;/span&gt;),
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;run_number&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;group_a&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;run_type&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;amount_c&quot;&lt;/span&gt;,
&#160; &lt;span class=&quot;code-quote&quot;&gt;&quot;count_of_amount_c&quot;&lt;/span&gt;,
)

df_final.show() &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Output:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
+------------+----------+-------+--------+---------------+--------------------+-----------------+
|total_amount|run_number|group_a|run_type|count_of_amount|            amount_c|count_of_amount_c|
+------------+----------+-------+--------+---------------+--------------------+-----------------+
|    0.000000|         7|      2|      AA|              2|3.545000000000000000|                2|
|    0.000000|         7|      1|      AA|              1|                NULL|             NULL|
+------------+----------+-------+--------+---------------+--------------------+-----------------+&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Expected behavior:&lt;/p&gt;

&lt;p&gt;Values used in the &lt;b&gt;lit&lt;/b&gt; function shouldn&apos;t interfere with the &lt;b&gt;scale&lt;/b&gt; parameter in the &lt;b&gt;round&lt;/b&gt;&#160;function&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment>&lt;p&gt;Databricks DBR 14.3&lt;br/&gt;
Spark 3.5.0&lt;br/&gt;
Scala 2.12&lt;/p&gt;</environment>
        <key id="13589097">SPARK-49261</key>
            <summary>Correlation between lit and round during grouping</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bersprockets">Bruce Robbins</assignee>
                                    <reporter username="krislig">Krystian Kulig</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Fri, 16 Aug 2024 12:41:45 +0000</created>
                <updated>Fri, 13 Sep 2024 01:58:56 +0000</updated>
                            <resolved>Thu, 12 Sep 2024 15:20:56 +0000</resolved>
                                    <version>3.2.4</version>
                    <version>3.5.0</version>
                    <version>3.3.4</version>
                    <version>3.4.3</version>
                                    <fixVersion>3.4.4</fixVersion>
                    <fixVersion>3.5.4</fixVersion>
                    <fixVersion>4.0.0</fixVersion>
                                    <component>PySpark</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="17875657" author="bersprockets" created="Wed, 21 Aug 2024 20:32:10 +0000"  >&lt;blockquote&gt;&lt;p&gt;It seems to be a correlation between F.lit(6).alias(&quot;run_number&quot;) and F.round(F.col(&quot;total_amount&quot;) / 1000, 6). If both lit and scale in round are set to the same number i.e. 6 code fails.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That&apos;s a good summary of the issue. The bug seems to be &lt;a href=&quot;https://github.com/apache/spark/blob/a885365897acefcf353206aaabd0048e088cc9a7/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/optimizer/RewriteDistinctAggregates.scala#L409&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;. That code will replace foldable and non-foldable expressions with expressions from the group by attributes, but I think it should only replace non-foldable expressions.&lt;/p&gt;

&lt;p&gt;In the case of the round function, that code is patching the second parameter, which requires a foldable expression, with a non-foldable expression. As a result, &lt;tt&gt;RoundBase#checkInputDataTypes&lt;/tt&gt; fails.&lt;/p&gt;</comment>
                            <comment id="17881346" author="dongjoon" created="Thu, 12 Sep 2024 15:20:56 +0000"  >&lt;p&gt;Issue resolved by pull request 47876&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/47876&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/47876&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 8 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1qwlc:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>