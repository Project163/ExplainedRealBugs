<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:48:37 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-18167] Flaky test when hive partition pruning is enabled</title>
                <link>https://issues.apache.org/jira/browse/SPARK-18167</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;org.apache.spark.sql.hive.execution.SQLQuerySuite is flaking when hive partition pruning is enabled.&lt;/p&gt;

&lt;p&gt;Based on the stack traces, it seems to be an old issue where Hive fails to cast a numeric partition column (&quot;Invalid character string format for type DECIMAL&quot;). There are two possibilities here: either we are somehow corrupting the partition table to have non-decimal values in that column, or there is a transient issue with Derby.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Error Message  java.lang.reflect.InvocationTargetException: &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; Stacktrace  sbt.ForkMain$ForkError: java.lang.reflect.InvocationTargetException: &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; 	at sun.reflect.GeneratedMethodAccessor263.invoke(Unknown Source) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:497) 	at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:588) 	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:544) 	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:542) 	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:282) 	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:229) 	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:228) 	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:271) 	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitionsByFilter(HiveClientImpl.scala:542) 	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:702) 	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:686) 	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:91) 	at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitionsByFilter(HiveExternalCatalog.scala:686) 	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitionsByFilter(SessionCatalog.scala:769) 	at org.apache.spark.sql.execution.datasources.TableFileCatalog.filterPartitions(TableFileCatalog.scala:67) 	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:59) 	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:26) 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292) 	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292) 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74) 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291) 	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$.apply(PruneFileSourcePartitions.scala:26) 	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$.apply(PruneFileSourcePartitions.scala:25) 	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85) 	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82) 	at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foldl(IndexedSeqOptimized.scala:57) 	at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foldLeft(IndexedSeqOptimized.scala:66) 	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35) 	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82) 	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74) 	at scala.collection.immutable.List.foreach(List.scala:381) 	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74) 	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:73) 	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:73) 	at org.apache.spark.sql.QueryTest.assertEmptyMissingInput(QueryTest.scala:234) 	at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:170) 	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76$$anonfun$apply$mcV$sp$24.apply$mcV$sp(SQLQuerySuite.scala:1559) 	at org.apache.spark.sql.test.SQLTestUtils$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;withTable(SQLTestUtils.scala:168) 	at org.apache.spark.sql.hive.execution.SQLQuerySuite.withTable(SQLQuerySuite.scala:67) 	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply$mcV$sp(SQLQuerySuite.scala:1553) 	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply(SQLQuerySuite.scala:1553) 	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply(SQLQuerySuite.scala:1553) 	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22) 	at org.scalatest.OutcomeOf$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;outcomeOf(OutcomeOf.scala:85) 	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104) 	at org.scalatest.Transformer.apply(Transformer.scala:22) 	at org.scalatest.Transformer.apply(Transformer.scala:20) 	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166) 	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68) 	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;invokeWithFixture$1(FunSuiteLike.scala:163) 	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) 	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175) 	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306) 	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTest(FunSuiteLike.scala:175) 	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555) 	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) 	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208) 	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413) 	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401) 	at scala.collection.immutable.List.foreach(List.scala:381) 	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401) 	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396) 	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483) 	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTests(FunSuiteLike.scala:208) 	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555) 	at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(Suite.scala:1424) 	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(FunSuite.scala:1555) 	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) 	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212) 	at org.scalatest.SuperEngine.runImpl(Engine.scala:545) 	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(FunSuiteLike.scala:212) 	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(SparkFunSuite.scala:31) 	at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;liftedTree1$1(BeforeAndAfterAll.scala:257) 	at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(BeforeAndAfterAll.scala:256) 	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:31) 	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357) 	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502) 	at sbt.ForkMain$Run$2.call(ForkMain.java:296) 	at sbt.ForkMain$Run$2.call(ForkMain.java:286) 	at java.util.concurrent.FutureTask.run(FutureTask.java:266) 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745) Caused by: sbt.ForkMain$ForkError: org.apache.hadoop.hive.metastore.api.MetaException: Filtering is supported only on partition keys of type string 	at org.apache.hadoop.hive.metastore.parser.ExpressionTree$FilterBuilder.setError(ExpressionTree.java:185) 	at org.apache.hadoop.hive.metastore.parser.ExpressionTree$LeafNode.getJdoFilterPushdownParam(ExpressionTree.java:440) 	at org.apache.hadoop.hive.metastore.parser.ExpressionTree$LeafNode.generateJDOFilterOverPartitions(ExpressionTree.java:357) 	at org.apache.hadoop.hive.metastore.parser.ExpressionTree$LeafNode.generateJDOFilter(ExpressionTree.java:279) 	at org.apache.hadoop.hive.metastore.parser.ExpressionTree.generateJDOFilterFragment(ExpressionTree.java:578) 	at org.apache.hadoop.hive.metastore.ObjectStore.makeQueryFilterString(ObjectStore.java:2615) 	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsViaOrmFilter(ObjectStore.java:2199) 	at org.apache.hadoop.hive.metastore.ObjectStore.access$500(ObjectStore.java:160) 	at org.apache.hadoop.hive.metastore.ObjectStore$5.getJdoResult(ObjectStore.java:2530) 	at org.apache.hadoop.hive.metastore.ObjectStore$5.getJdoResult(ObjectStore.java:2515) 	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2391) 	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:2515) 	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:2335) 	at sun.reflect.GeneratedMethodAccessor266.invoke(Unknown Source) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:497) 	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114) 	at com.sun.proxy.$Proxy18.getPartitionsByFilter(Unknown Source) 	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:4442) 	at sun.reflect.GeneratedMethodAccessor265.invoke(Unknown Source) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:497) 	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107) 	at com.sun.proxy.$Proxy20.get_partitions_by_filter(Unknown Source) 	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByFilter(HiveMetaStoreClient.java:1103) 	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source) 	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 	at java.lang.reflect.Method.invoke(Method.java:497) 	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156) 	at com.sun.proxy.$Proxy21.listPartitionsByFilter(Unknown Source) 	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByFilter(Hive.java:2254) 	... 85 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;17:47:05.829 pool-1-thread-1-ScalaTest-running-SQLQuerySuite DEBUG IsolatedClientLoader: hive class: org.datanucleus.exceptions.TransactionNotWritableException - jar:file:/home/sparkivy/per-executor-caches/9/.ivy2/cache/org.datanucleus/datanucleus-core/jars/datanucleus-core-3.2.10.jar!/org/datanucleus/exceptions/TransactionNotWritableException.&lt;span class=&quot;code-keyword&quot;&gt;class
&lt;/span&gt;17:47:05.830 pool-1-thread-1-ScalaTest-running-SQLQuerySuite WARN MetaStoreDirectSql: Failed to execute [select &lt;span class=&quot;code-quote&quot;&gt;&quot;PARTITIONS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;PART_ID&quot;&lt;/span&gt; from &lt;span class=&quot;code-quote&quot;&gt;&quot;PARTITIONS&quot;&lt;/span&gt;  &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; join &lt;span class=&quot;code-quote&quot;&gt;&quot;TBLS&quot;&lt;/span&gt; on &lt;span class=&quot;code-quote&quot;&gt;&quot;PARTITIONS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;TBL_ID&quot;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&quot;TBLS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;TBL_ID&quot;&lt;/span&gt;     and &lt;span class=&quot;code-quote&quot;&gt;&quot;TBLS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;TBL_NAME&quot;&lt;/span&gt; = ?   &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; join &lt;span class=&quot;code-quote&quot;&gt;&quot;DBS&quot;&lt;/span&gt; on &lt;span class=&quot;code-quote&quot;&gt;&quot;TBLS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;DB_ID&quot;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&quot;DBS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;DB_ID&quot;&lt;/span&gt;      and &lt;span class=&quot;code-quote&quot;&gt;&quot;DBS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;NAME&quot;&lt;/span&gt; = ? &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; join &lt;span class=&quot;code-quote&quot;&gt;&quot;PARTITION_KEY_VALS&quot;&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;FILTER0&quot;&lt;/span&gt; on &lt;span class=&quot;code-quote&quot;&gt;&quot;FILTER0&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;PART_ID&quot;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&quot;PARTITIONS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;PART_ID&quot;&lt;/span&gt; and &lt;span class=&quot;code-quote&quot;&gt;&quot;FILTER0&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;INTEGER_IDX&quot;&lt;/span&gt; = 0 where (((&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; when &lt;span class=&quot;code-quote&quot;&gt;&quot;TBLS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;TBL_NAME&quot;&lt;/span&gt; = ? and &lt;span class=&quot;code-quote&quot;&gt;&quot;DBS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;NAME&quot;&lt;/span&gt; = ? and &lt;span class=&quot;code-quote&quot;&gt;&quot;FILTER0&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;PART_ID&quot;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&quot;PARTITIONS&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;PART_ID&quot;&lt;/span&gt; and &lt;span class=&quot;code-quote&quot;&gt;&quot;FILTER0&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;INTEGER_IDX&quot;&lt;/span&gt; = 0 then &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(&lt;span class=&quot;code-quote&quot;&gt;&quot;FILTER0&quot;&lt;/span&gt;.&lt;span class=&quot;code-quote&quot;&gt;&quot;PART_KEY_VAL&quot;&lt;/span&gt; as decimal(21,0)) &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; end) &amp;gt; ?))] with parameters [tbl10562, &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, tbl10562, &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, 2015]
javax.jdo.JDODataStoreException: Error executing SQL query &lt;span class=&quot;code-quote&quot;&gt;&quot;select &quot;&lt;/span&gt;PARTITIONS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; from &quot;&lt;/span&gt;PARTITIONS&lt;span class=&quot;code-quote&quot;&gt;&quot;  &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; join &quot;&lt;/span&gt;TBLS&lt;span class=&quot;code-quote&quot;&gt;&quot; on &quot;&lt;/span&gt;PARTITIONS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;TBL_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; = &quot;&lt;/span&gt;TBLS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;TBL_ID&lt;span class=&quot;code-quote&quot;&gt;&quot;     and &quot;&lt;/span&gt;TBLS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;TBL_NAME&lt;span class=&quot;code-quote&quot;&gt;&quot; = ?   &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; join &quot;&lt;/span&gt;DBS&lt;span class=&quot;code-quote&quot;&gt;&quot; on &quot;&lt;/span&gt;TBLS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;DB_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; = &quot;&lt;/span&gt;DBS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;DB_ID&lt;span class=&quot;code-quote&quot;&gt;&quot;      and &quot;&lt;/span&gt;DBS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;NAME&lt;span class=&quot;code-quote&quot;&gt;&quot; = ? &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; join &quot;&lt;/span&gt;PARTITION_KEY_VALS&lt;span class=&quot;code-quote&quot;&gt;&quot; &quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot; on &quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; = &quot;&lt;/span&gt;PARTITIONS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; and &quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;INTEGER_IDX&lt;span class=&quot;code-quote&quot;&gt;&quot; = 0 where (((&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; when &quot;&lt;/span&gt;TBLS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;TBL_NAME&lt;span class=&quot;code-quote&quot;&gt;&quot; = ? and &quot;&lt;/span&gt;DBS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;NAME&lt;span class=&quot;code-quote&quot;&gt;&quot; = ? and &quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; = &quot;&lt;/span&gt;PARTITIONS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; and &quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;INTEGER_IDX&lt;span class=&quot;code-quote&quot;&gt;&quot; = 0 then &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(&quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_KEY_VAL&lt;span class=&quot;code-quote&quot;&gt;&quot; as decimal(21,0)) &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; end) &amp;gt; ?))&quot;&lt;/span&gt;.
	at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:451)
	at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:321)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:1596)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:459)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:373)
	at org.apache.hadoop.hive.metastore.ObjectStore$5.getSqlResult(ObjectStore.java:2518)
	at org.apache.hadoop.hive.metastore.ObjectStore$5.getSqlResult(ObjectStore.java:2515)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2385)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:2515)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:2335)
	at sun.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
	at com.sun.proxy.$Proxy18.getPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:4442)
	at sun.reflect.GeneratedMethodAccessor265.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy20.get_partitions_by_filter(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByFilter(HiveMetaStoreClient.java:1103)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy21.listPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByFilter(Hive.java:2254)
	at sun.reflect.GeneratedMethodAccessor263.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:588)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:544)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:282)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:229)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:228)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:271)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitionsByFilter(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:702)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:686)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:91)
	at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitionsByFilter(HiveExternalCatalog.scala:686)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitionsByFilter(SessionCatalog.scala:769)
	at org.apache.spark.sql.execution.datasources.TableFileCatalog.filterPartitions(TableFileCatalog.scala:67)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:59)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:26)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$.apply(PruneFileSourcePartitions.scala:26)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$.apply(PruneFileSourcePartitions.scala:25)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:73)
	at org.apache.spark.sql.QueryTest.assertEmptyMissingInput(QueryTest.scala:234)
	at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:170)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76$$anonfun$apply$mcV$sp$24.apply$mcV$sp(SQLQuerySuite.scala:1559)
	at org.apache.spark.sql.test.SQLTestUtils$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;withTable(SQLTestUtils.scala:168)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite.withTable(SQLQuerySuite.scala:67)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply$mcV$sp(SQLQuerySuite.scala:1553)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply(SQLQuerySuite.scala:1553)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply(SQLQuerySuite.scala:1553)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(FunSuiteLike.scala:212)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(SparkFunSuite.scala:31)
	at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:31)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
NestedThrowablesStackTrace:
java.sql.SQLDataException: Invalid character string format &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; type DECIMAL.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedResultSet.closeOnTransactionError(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedResultSet.movePosition(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedResultSet.next(Unknown Source)
	at org.datanucleus.store.rdbms.query.ForwardQueryResult.initialise(ForwardQueryResult.java:99)
	at org.datanucleus.store.rdbms.query.SQLQuery.performExecute(SQLQuery.java:312)
	at org.datanucleus.store.query.Query.executeQuery(Query.java:1786)
	at org.datanucleus.store.query.AbstractSQLQuery.executeWithArray(AbstractSQLQuery.java:339)
	at org.datanucleus.api.jdo.JDOQuery.executeWithArray(JDOQuery.java:312)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:1596)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:459)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:373)
	at org.apache.hadoop.hive.metastore.ObjectStore$5.getSqlResult(ObjectStore.java:2518)
	at org.apache.hadoop.hive.metastore.ObjectStore$5.getSqlResult(ObjectStore.java:2515)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2385)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:2515)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:2335)
	at sun.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
	at com.sun.proxy.$Proxy18.getPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:4442)
	at sun.reflect.GeneratedMethodAccessor265.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy20.get_partitions_by_filter(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByFilter(HiveMetaStoreClient.java:1103)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy21.listPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByFilter(Hive.java:2254)
	at sun.reflect.GeneratedMethodAccessor263.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:588)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:544)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:282)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:229)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:228)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:271)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitionsByFilter(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:702)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:686)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:91)
	at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitionsByFilter(HiveExternalCatalog.scala:686)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitionsByFilter(SessionCatalog.scala:769)
	at org.apache.spark.sql.execution.datasources.TableFileCatalog.filterPartitions(TableFileCatalog.scala:67)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:59)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:26)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$.apply(PruneFileSourcePartitions.scala:26)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$.apply(PruneFileSourcePartitions.scala:25)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:73)
	at org.apache.spark.sql.QueryTest.assertEmptyMissingInput(QueryTest.scala:234)
	at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:170)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76$$anonfun$apply$mcV$sp$24.apply$mcV$sp(SQLQuerySuite.scala:1559)
	at org.apache.spark.sql.test.SQLTestUtils$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;withTable(SQLTestUtils.scala:168)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite.withTable(SQLQuerySuite.scala:67)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply$mcV$sp(SQLQuerySuite.scala:1553)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply(SQLQuerySuite.scala:1553)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply(SQLQuerySuite.scala:1553)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(FunSuiteLike.scala:212)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(SparkFunSuite.scala:31)
	at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:31)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: ERROR 22018: Invalid character string format &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; type DECIMAL.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.types.DataType.invalidFormat(Unknown Source)
	at org.apache.derby.iapi.types.DataType.setValue(Unknown Source)
	at org.apache.derby.exe.ac7815c0b6x0158x08a6x8f60x000029fece88c48.e4(Unknown Source)
	at org.apache.derby.impl.services.reflect.DirectCall.invoke(Unknown Source)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.NestedLoopJoinResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.ProjectRestrictResultSet.getNextRowCore(Unknown Source)
	at org.apache.derby.impl.sql.execute.BasicNoPutResultSetImpl.getNextRow(Unknown Source)
	... 118 more
17:47:05.832 pool-1-thread-1-ScalaTest-running-SQLQuerySuite WARN ObjectStore: Direct SQL failed, falling back to ORM
MetaException(message:See previous errors; Error executing SQL query &lt;span class=&quot;code-quote&quot;&gt;&quot;select &quot;&lt;/span&gt;PARTITIONS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; from &quot;&lt;/span&gt;PARTITIONS&lt;span class=&quot;code-quote&quot;&gt;&quot;  &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; join &quot;&lt;/span&gt;TBLS&lt;span class=&quot;code-quote&quot;&gt;&quot; on &quot;&lt;/span&gt;PARTITIONS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;TBL_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; = &quot;&lt;/span&gt;TBLS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;TBL_ID&lt;span class=&quot;code-quote&quot;&gt;&quot;     and &quot;&lt;/span&gt;TBLS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;TBL_NAME&lt;span class=&quot;code-quote&quot;&gt;&quot; = ?   &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; join &quot;&lt;/span&gt;DBS&lt;span class=&quot;code-quote&quot;&gt;&quot; on &quot;&lt;/span&gt;TBLS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;DB_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; = &quot;&lt;/span&gt;DBS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;DB_ID&lt;span class=&quot;code-quote&quot;&gt;&quot;      and &quot;&lt;/span&gt;DBS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;NAME&lt;span class=&quot;code-quote&quot;&gt;&quot; = ? &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; join &quot;&lt;/span&gt;PARTITION_KEY_VALS&lt;span class=&quot;code-quote&quot;&gt;&quot; &quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot; on &quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; = &quot;&lt;/span&gt;PARTITIONS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; and &quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;INTEGER_IDX&lt;span class=&quot;code-quote&quot;&gt;&quot; = 0 where (((&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; when &quot;&lt;/span&gt;TBLS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;TBL_NAME&lt;span class=&quot;code-quote&quot;&gt;&quot; = ? and &quot;&lt;/span&gt;DBS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;NAME&lt;span class=&quot;code-quote&quot;&gt;&quot; = ? and &quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; = &quot;&lt;/span&gt;PARTITIONS&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_ID&lt;span class=&quot;code-quote&quot;&gt;&quot; and &quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;INTEGER_IDX&lt;span class=&quot;code-quote&quot;&gt;&quot; = 0 then &lt;span class=&quot;code-keyword&quot;&gt;cast&lt;/span&gt;(&quot;&lt;/span&gt;FILTER0&lt;span class=&quot;code-quote&quot;&gt;&quot;.&quot;&lt;/span&gt;PART_KEY_VAL&lt;span class=&quot;code-quote&quot;&gt;&quot; as decimal(21,0)) &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; end) &amp;gt; ?))&quot;&lt;/span&gt;.)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.executeWithArray(MetaStoreDirectSql.java:1608)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilterInternal(MetaStoreDirectSql.java:459)
	at org.apache.hadoop.hive.metastore.MetaStoreDirectSql.getPartitionsViaSqlFilter(MetaStoreDirectSql.java:373)
	at org.apache.hadoop.hive.metastore.ObjectStore$5.getSqlResult(ObjectStore.java:2518)
	at org.apache.hadoop.hive.metastore.ObjectStore$5.getSqlResult(ObjectStore.java:2515)
	at org.apache.hadoop.hive.metastore.ObjectStore$GetHelper.run(ObjectStore.java:2385)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilterInternal(ObjectStore.java:2515)
	at org.apache.hadoop.hive.metastore.ObjectStore.getPartitionsByFilter(ObjectStore.java:2335)
	at sun.reflect.GeneratedMethodAccessor266.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
	at com.sun.proxy.$Proxy18.getPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.get_partitions_by_filter(HiveMetaStore.java:4442)
	at sun.reflect.GeneratedMethodAccessor265.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy20.get_partitions_by_filter(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.listPartitionsByFilter(HiveMetaStoreClient.java:1103)
	at sun.reflect.GeneratedMethodAccessor264.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy21.listPartitionsByFilter(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.getPartitionsByFilter(Hive.java:2254)
	at sun.reflect.GeneratedMethodAccessor263.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.sql.hive.client.Shim_v0_13.getPartitionsByFilter(HiveShim.scala:588)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:544)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$getPartitionsByFilter$1.apply(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:282)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:229)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:228)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:271)
	at org.apache.spark.sql.hive.client.HiveClientImpl.getPartitionsByFilter(HiveClientImpl.scala:542)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:702)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$listPartitionsByFilter$1.apply(HiveExternalCatalog.scala:686)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:91)
	at org.apache.spark.sql.hive.HiveExternalCatalog.listPartitionsByFilter(HiveExternalCatalog.scala:686)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.listPartitionsByFilter(SessionCatalog.scala:769)
	at org.apache.spark.sql.execution.datasources.TableFileCatalog.filterPartitions(TableFileCatalog.scala:67)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:59)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$$anonfun$apply$1.applyOrElse(PruneFileSourcePartitions.scala:26)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:292)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:74)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:291)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$.apply(PruneFileSourcePartitions.scala:26)
	at org.apache.spark.sql.execution.datasources.PruneFileSourcePartitions$.apply(PruneFileSourcePartitions.scala:25)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:85)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:82)
	at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foldl(IndexedSeqOptimized.scala:57)
	at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foldLeft(IndexedSeqOptimized.scala:66)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:35)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:82)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:74)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:74)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:73)
	at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:73)
	at org.apache.spark.sql.QueryTest.assertEmptyMissingInput(QueryTest.scala:234)
	at org.apache.spark.sql.QueryTest.checkAnswer(QueryTest.scala:170)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76$$anonfun$apply$mcV$sp$24.apply$mcV$sp(SQLQuerySuite.scala:1559)
	at org.apache.spark.sql.test.SQLTestUtils$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;withTable(SQLTestUtils.scala:168)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite.withTable(SQLQuerySuite.scala:67)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply$mcV$sp(SQLQuerySuite.scala:1553)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply(SQLQuerySuite.scala:1553)
	at org.apache.spark.sql.hive.execution.SQLQuerySuite$$anonfun$76.apply(SQLQuerySuite.scala:1553)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:68)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(FunSuiteLike.scala:212)
	at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(SparkFunSuite.scala:31)
	at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:31)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)
	at sbt.ForkMain$Run$2.call(ForkMain.java:296)
	at sbt.ForkMain$Run$2.call(ForkMain.java:286)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13016296">SPARK-18167</key>
            <summary>Flaky test when hive partition pruning is enabled</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="4">Incomplete</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="ekhliang">Eric Liang</reporter>
                        <labels>
                            <label>bulk-closed</label>
                    </labels>
                <created>Fri, 28 Oct 2016 22:42:44 +0000</created>
                <updated>Tue, 21 May 2019 04:37:11 +0000</updated>
                            <resolved>Tue, 21 May 2019 04:37:11 +0000</resolved>
                                                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="15616845" author="apachespark" created="Fri, 28 Oct 2016 22:46:05 +0000"  >&lt;p&gt;User &apos;ericl&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15676&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15676&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15623499" author="apachespark" created="Mon, 31 Oct 2016 21:36:06 +0000"  >&lt;p&gt;User &apos;ericl&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15701&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15701&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15623770" author="yhuai" created="Mon, 31 Oct 2016 23:27:08 +0000"  >&lt;p&gt;Issue resolved by pull request 15701&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15701&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15701&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15623959" author="apachespark" created="Tue, 1 Nov 2016 01:07:05 +0000"  >&lt;p&gt;User &apos;ericl&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15708&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15708&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15626439" author="apachespark" created="Tue, 1 Nov 2016 19:32:08 +0000"  >&lt;p&gt;User &apos;ericl&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15720&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15720&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15627343" author="apachespark" created="Wed, 2 Nov 2016 01:50:03 +0000"  >&lt;p&gt;User &apos;ericl&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15725&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15725&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15643296" author="rxin" created="Mon, 7 Nov 2016 06:42:41 +0000"  >&lt;p&gt;This is unfortunately still flaky. Reopening it.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13016309">SPARK-18168</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13016309">SPARK-18168</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 2 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i35kin:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>