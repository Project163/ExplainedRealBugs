<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:50:14 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-18528] limit + groupBy leads to java.lang.NullPointerException</title>
                <link>https://issues.apache.org/jira/browse/SPARK-18528</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Using limit on a DataFrame prior to groupBy will lead to a crash. Repartitioning will avoid the crash.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;will crash:&lt;/b&gt; &lt;tt&gt;df.limit(3).groupBy(&quot;user_id&quot;).count().show()&lt;/tt&gt;&lt;br/&gt;
&lt;b&gt;will work:&lt;/b&gt; &lt;tt&gt;df.limit(3).coalesce(1).groupBy(&apos;user_id&apos;).count().show()&lt;/tt&gt;&lt;br/&gt;
&lt;b&gt;will work:&lt;/b&gt; &lt;tt&gt;df.limit(3).repartition(&apos;user_id&apos;).groupBy(&apos;user_id&apos;).count().show()&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Here is a reproducible example along with the error message:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; df = spark.createDataFrame([ (1, 1), (1, 3), (2, 1), (3, 2), (3, 3) ], &lt;span class=&quot;error&quot;&gt;&amp;#91;&amp;quot;user_id&amp;quot;, &amp;quot;genre_id&amp;quot;&amp;#93;&lt;/span&gt;)&lt;br/&gt;
&amp;gt;&amp;gt;&amp;gt;&lt;br/&gt;
&amp;gt;&amp;gt;&amp;gt; df.show()&lt;br/&gt;
&lt;ins&gt;------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;-------+&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;user_id&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;genre_id&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;-------+&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;       1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;       3&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;       1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;       2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;       3&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;-------+&lt;br/&gt;
&amp;gt;&amp;gt;&amp;gt; df.groupBy(&quot;user_id&quot;).count().show()&lt;br/&gt;
&lt;ins&gt;------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;----+&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;user_id&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;count&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;----+&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;----+&lt;br/&gt;
&amp;gt;&amp;gt;&amp;gt; df.limit(3).groupBy(&quot;user_id&quot;).count().show()&lt;br/&gt;
&lt;span class=&quot;error&quot;&gt;&amp;#91;Stage 8:===================================================&amp;gt;(1964 + 24) / 2000&amp;#93;&lt;/span&gt;16/11/21 01:59:27 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 8204, lvsp20hdn012.stubprod.com): java.lang.NullPointerException&lt;br/&gt;
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.agg_doAggregateWithKeys$(Unknown Source)&lt;br/&gt;
    at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source)&lt;br/&gt;
    at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)&lt;br/&gt;
    at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:370)&lt;br/&gt;
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:246)&lt;br/&gt;
    at org.apache.spark.sql.execution.SparkPlan$$anonfun$4.apply(SparkPlan.scala:240)&lt;br/&gt;
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)&lt;br/&gt;
    at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:803)&lt;br/&gt;
    at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
    at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&lt;br/&gt;
    at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&lt;br/&gt;
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:70)&lt;br/&gt;
    at org.apache.spark.scheduler.Task.run(Task.scala:86)&lt;br/&gt;
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)&lt;br/&gt;
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)&lt;br/&gt;
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)&lt;br/&gt;
    at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;&lt;/blockquote&gt;
</description>
                <environment>&lt;p&gt;CentOS release 6.6, Linux 2.6.32-504.el6.x86_64&lt;/p&gt;</environment>
        <key id="13022251">SPARK-18528</key>
            <summary>limit + groupBy leads to java.lang.NullPointerException</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="maropu">Takeshi Yamamuro</assignee>
                                    <reporter username="correedsh">Corey</reporter>
                        <labels>
                    </labels>
                <created>Mon, 21 Nov 2016 17:31:20 +0000</created>
                <updated>Wed, 10 May 2017 01:19:57 +0000</updated>
                            <resolved>Thu, 22 Dec 2016 00:54:37 +0000</resolved>
                                    <version>2.0.1</version>
                                    <fixVersion>2.0.3</fixVersion>
                    <fixVersion>2.1.1</fixVersion>
                    <fixVersion>2.2.0</fixVersion>
                                    <component>PySpark</component>
                    <component>SQL</component>
                        <due></due>
                            <votes>3</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="15685451" author="djvulee" created="Tue, 22 Nov 2016 02:26:44 +0000"  >&lt;p&gt;I just test your example, but it works.&lt;/p&gt;

&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; df.limit(3).groupBy(&quot;user_id&quot;).count().show()&lt;br/&gt;
&lt;ins&gt;------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;----+&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;user_id&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;count&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;----+&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    2&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;      2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;    1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;ins&gt;------&lt;del&gt;&lt;/ins&gt;&lt;/del&gt;----+&lt;/p&gt;


&lt;p&gt;I test on a the spark2.0.1-hadoop2.6 version downloaded from the spark website.&lt;/p&gt;

&lt;p&gt;can you reproduce your error?&lt;/p&gt;
</comment>
                            <comment id="15685497" author="maropu" created="Tue, 22 Nov 2016 02:57:26 +0000"  >&lt;p&gt;I reproduced this in master;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; val df = Seq((&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;, 1), (&lt;span class=&quot;code-quote&quot;&gt;&quot;b&quot;&lt;/span&gt;, 2), (&lt;span class=&quot;code-quote&quot;&gt;&quot;a&quot;&lt;/span&gt;, 1), (&lt;span class=&quot;code-quote&quot;&gt;&quot;c&quot;&lt;/span&gt;, 5), (&lt;span class=&quot;code-quote&quot;&gt;&quot;b&quot;&lt;/span&gt;, 2)).toDF(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;value&quot;&lt;/span&gt;)
scala&amp;gt; df.limit(3).groupBy(&lt;span class=&quot;code-quote&quot;&gt;&quot;id&quot;&lt;/span&gt;).count().show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I checked code and I found this exception happened when execution was early stopped by the limit and hashmap (&lt;a href=&quot;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala#L603&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/aggregate/HashAggregateExec.scala#L603&lt;/a&gt;) was not initialized.&lt;br/&gt;
I&apos;m making pr to fix this. Thanks!&lt;/p&gt;</comment>
                            <comment id="15687690" author="apachespark" created="Tue, 22 Nov 2016 19:44:04 +0000"  >&lt;p&gt;User &apos;maropu&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/15980&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/15980&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16003265" author="ekundin" created="Tue, 9 May 2017 18:38:56 +0000"  >&lt;p&gt;I have just found the same bug with Spark 2.10, even though it&apos;s marked as fixed.  I believe you need to add 2.1.0 to the &quot;Affected Versions&quot; list.&lt;/p&gt;

&lt;p&gt;I have a Dataframe df with a few columns that I read in from json, and running:&lt;br/&gt;
df.select(&apos;foo&apos;).distinct().count()&lt;br/&gt;
works fine.&lt;br/&gt;
df.limit(100).select(&apos;foo&apos;).distinct().count()&lt;br/&gt;
throws a NPE.&lt;br/&gt;
df.limit(100).repartition(&apos;foo&apos;).select(&apos;foo&apos;).distinct().count()&lt;br/&gt;
works fine too.  Seems like the same bug, still broken.&lt;/p&gt;</comment>
                            <comment id="16003882" author="maropu" created="Wed, 10 May 2017 01:19:57 +0000"  >&lt;p&gt;You tried spark-v2.1.1? This issue is fixed in 2.0.3, 2.1.1, 2.2.0 (See Fix Versions).&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13027929">SPARK-18851</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13031276">SPARK-19037</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 27 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i36l93:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>