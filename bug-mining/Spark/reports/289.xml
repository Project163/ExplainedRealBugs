<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:13:49 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-1911] Warn users if their assembly jars are not built with Java 6</title>
                <link>https://issues.apache.org/jira/browse/SPARK-1911</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;The root cause of the problem is detailed in: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1520&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-1520&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In short, an assembly jar built with Java 7+ is not always accessible by Python or other versions of Java (especially Java 6). If the assembly jar is not built on the cluster itself, this problem may manifest itself in strange exceptions that are not trivial to debug. This is an issue especially for PySpark on YARN, which relies on the python files included within the assembly jar.&lt;/p&gt;

&lt;p&gt;Currently we warn users only in make-distribution.sh, but most users build the jars directly. At the very least we need to emphasize this in the docs (currently missing entirely). The next step is to add a warning prompt in the mvn scripts whenever Java 7+ is detected.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12716236">SPARK-1911</key>
            <summary>Warn users if their assembly jars are not built with Java 6</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srowen">Sean R. Owen</assignee>
                                    <reporter username="andrewor14">Andrew Or</reporter>
                        <labels>
                    </labels>
                <created>Thu, 22 May 2014 23:57:19 +0000</created>
                <updated>Wed, 11 Oct 2017 01:20:27 +0000</updated>
                            <resolved>Wed, 4 Mar 2015 11:44:04 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>1.2.2</fixVersion>
                    <fixVersion>1.3.0</fixVersion>
                                    <component>Documentation</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="14008260" author="tdas" created="Sun, 25 May 2014 01:36:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/859/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/859/&lt;/a&gt; adds a warning in make-distribution.sh if they try to compile spark with java 6.&lt;/p&gt;

&lt;p&gt;Note that the actual problem of making Java 7 compiled JARs work with Python still needs to be solved.&lt;/p&gt;</comment>
                            <comment id="14013098" author="airhorns" created="Thu, 29 May 2014 23:26:58 +0000"  >&lt;p&gt;Do we know why Python is unable to read Java 7 jars, and only sometimes? &lt;/p&gt;</comment>
                            <comment id="14013142" author="tdas" created="Fri, 30 May 2014 00:29:02 +0000"  >&lt;p&gt;As far as I think, it is because Java 7 uses Zip64 encoding when making JARs with more 2^16 files and python (at least 2.x) is not able to read Zip64. So it fails in those times when the Spark assembly JAR has more than 65k files, which in turn depends on whether it has been generated with YARN and/or Hive enabled.&lt;/p&gt;

&lt;p&gt;Java 6 uses the traditional Zip format to create JARs, even if it has more than 65k files. So python always seems to work with Java 6 Jars&lt;/p&gt;

&lt;p&gt;Caveat: I cant claim 100% certainty on this interpretation because there is so little documentation on this on the net.&lt;/p&gt;</comment>
                            <comment id="14042505" author="andrewor" created="Tue, 24 Jun 2014 18:45:00 +0000"  >&lt;p&gt;Looks like we still haven&apos;t fixed this. At the very least we should explicitly add this warning in the documentation. The next step that is more involved is to add a warning prompt in the mvn build scripts whenever Java 7+ is detected.&lt;/p&gt;</comment>
                            <comment id="14262552" author="naven084k" created="Thu, 1 Jan 2015 13:27:48 +0000"  >&lt;p&gt;Hi,  I have built spark assembly jar with java 6 using make-distribution.sh and started spark cluster on 2 nodes(which are on unix boxes)  I am able to execute java programs on the cluster.  Now I am able to connect to the cluster from my windows machine using pyspark interactive shell  Bin&amp;gt; pyspark &#8211;master spark://master:7078  And then I am trying to execute following commands at interactive shell  lines = sc.textFile(&quot;hdfs://master/data/spark/SINGLE.TXT&quot;) lineLengths = lines.map(lambda s: len(s)) totalLength = lineLengths.reduce(lambda a, b: a + b)  It is throwing the following error  Traceback (most recent call last): File &quot;&quot;, line 1, in  File &quot;C:\Users\npokala\Downloads\spark-java\spark-master\python\pyspark\rdd.py&quot;, line 715, in reduce vals = self.mapPartitions(func).collect() File &quot;C:\Users\npokala\Downloads\spark-java\spark-master\python\pyspark\rdd.py&quot;, line 676, in collect bytesInJava = self.jrdd.collect().iterator() File &quot;C:\Users\npokala\Downloads\spark-java\spark-master\python\lib\py4j-0.8.2.1-src.zip\py4j\java_gateway.py&quot;, line 538, in _&lt;em&gt;call&lt;/em&gt; 15/01/01 18:29:07 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on master:34586 (size: 3.9 KB, free: 1060.0 MB) File &quot;C:\Users\npokala\Downloads\spark-java\spark-master\python\lib\py4j-0.8.2.1-src.zip\py4j\protocol.py&quot;, line 300, in get_return_value py4j.protocol.Py4JJavaError: An error occurred while calling o24.collect. : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 4 times, most recent failure: Lost task 0.3 in stage 0.0 (TID 7, master): org.apache.spark.SparkException: Error from python worker: python: module pyspark.daemon not found PYTHONPATH was: /home/npokala/data/spark-install/spark-java-1.6/spark-master/python:/home/npokala/data/spark-install/spark-java-1.6/spark-master/python/lib/py4j-0.8.2.1-src.zip:/home/npokala/data/spark-install/spark-java-1.6/spark-master/assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop2.4.0.jar:/home/npokala/data/spark-install/spark-java-1.6/spark-master/sbin/../python/lib/py4j-0.8.2.1-src.zip:/home/npokala/data/spark-install/spark-java-1.6/spark-master/sbin/../python: java.io.EOFException at java.io.DataInputStream.readInt(DataInputStream.java:392) at org.apache.spark.api.python.PythonWorkerFactory.startDaemon(PythonWorkerFactory.scala:163) at org.apache.spark.api.python.PythonWorkerFactory.createThroughDaemon(PythonWorkerFactory.scala:86) at org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:62) at org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:102) at org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:70) at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:265) at org.apache.spark.rdd.RDD.iterator(RDD.scala:232) at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61) at org.apache.spark.scheduler.Task.run(Task.scala:56) at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) at java.lang.Thread.run(Thread.java:745)  Driver stacktrace: at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1214) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1203) at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1202) at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1202) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696) at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:696) at scala.Option.foreach(Option.scala:236) at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:696) at org.apache.spark.scheduler.DAGSchedulerEventProcessActor$$anonfun$receive$2.applyOrElse(DAGScheduler.scala:1420) at akka.actor.Actor$class.aroundReceive(Actor.scala:465) at org.apache.spark.scheduler.DAGSchedulerEventProcessActor.aroundReceive(DAGScheduler.scala:1375) at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516) at akka.actor.ActorCell.invoke(ActorCell.scala:487) at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238) at akka.dispatch.Mailbox.run(Mailbox.scala:220) at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393) at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)  Is there any pyspark daemons do I need to start on cluster or am I missing something else&lt;/p&gt;</comment>
                            <comment id="14345149" author="srowen" created="Tue, 3 Mar 2015 14:40:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor14&quot; class=&quot;user-hover&quot; rel=&quot;andrewor14&quot;&gt;andrewor14&lt;/a&gt; &lt;tt&gt;compute-classpath.sh&lt;/tt&gt; will now show a warning in this situation (cf. &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1703&quot; title=&quot;Warn users if Spark is run on JRE6 but compiled with JDK7&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-1703&quot;&gt;&lt;del&gt;SPARK-1703&lt;/del&gt;&lt;/a&gt;). I will send a PR for the doc change. Is this the same issue as &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1753&quot; title=&quot;PySpark on YARN does not work on assembly jar built on Red Hat based OS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-1753&quot;&gt;&lt;del&gt;SPARK-1753&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="14345151" author="apachespark" created="Tue, 3 Mar 2015 14:41:41 +0000"  >&lt;p&gt;User &apos;srowen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4874&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4874&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14346749" author="apachespark" created="Wed, 4 Mar 2015 11:09:48 +0000"  >&lt;p&gt;User &apos;srowen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/4888&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/4888&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14502589" author="stevel@apache.org" created="Mon, 20 Apr 2015 09:51:15 +0000"  >&lt;p&gt;This doesn&apos;t fix the problem, merely documents it.&lt;/p&gt;

&lt;p&gt;It should be doable by using Ant&apos;s &amp;lt;zip&amp;gt; task, which doesn&apos;t use the JDK zip routines. The assembly would be unzipped first, then zipped with zip63 option set to never&lt;/p&gt;

&lt;p&gt;see &lt;a href=&quot;https://ant.apache.org/manual/Tasks/zip.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://ant.apache.org/manual/Tasks/zip.html&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="14502590" author="srowen" created="Mon, 20 Apr 2015 09:55:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt; Yeah this is mostly duplicating &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1703&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-1703&lt;/a&gt; which has an actual check and warning. I think this JIRA/PR ended up just being about the follow-on doc change.&lt;/p&gt;</comment>
                            <comment id="16199666" author="swaapnika" created="Wed, 11 Oct 2017 01:20:27 +0000"  >&lt;p&gt;Does this issue still exist with Spark-2.2.? &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12712100">SPARK-1703</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12712986">SPARK-1753</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12708995">SPARK-1520</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>394445</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 5 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1vwz3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>394586</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>