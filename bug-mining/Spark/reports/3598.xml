<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:44:00 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-15269] Creating external table leaves empty directory under warehouse directory</title>
                <link>https://issues.apache.org/jira/browse/SPARK-15269</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Adding the following test case in &lt;tt&gt;HiveDDLSuite&lt;/tt&gt; may reproduce this issue:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  test(&lt;span class=&quot;code-quote&quot;&gt;&quot;foo&quot;&lt;/span&gt;) {
    withTempPath { dir =&amp;gt;
      val path = dir.getCanonicalPath
      spark.range(1).write.json(path)

      withTable(&lt;span class=&quot;code-quote&quot;&gt;&quot;ddl_test1&quot;&lt;/span&gt;) {
        sql(s&lt;span class=&quot;code-quote&quot;&gt;&quot;CREATE TABLE ddl_test1 USING json OPTIONS (PATH &lt;span class=&quot;code-quote&quot;&gt;&apos;$path&apos;&lt;/span&gt;)&quot;&lt;/span&gt;)
        sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;DROP TABLE ddl_test1&quot;&lt;/span&gt;)
        sql(s&lt;span class=&quot;code-quote&quot;&gt;&quot;CREATE TABLE ddl_test1 USING json AS SELECT 1 AS a&quot;&lt;/span&gt;)
      }
    }
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the first &lt;tt&gt;CREATE TABLE&lt;/tt&gt; command creates an external table since data source tables are always external when &lt;tt&gt;PATH&lt;/tt&gt; option is specified.&lt;/p&gt;

&lt;p&gt;When executing the second &lt;tt&gt;CREATE TABLE&lt;/tt&gt; command, which creates a managed table with the same name, it fails because there&apos;s already an unexpected directory with the same name as the table name in the warehouse directory:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[info] - foo *** FAILED *** (7 seconds, 649 milliseconds)
[info]   org.apache.spark.sql.AnalysisException: path file:/Users/lian/local/src/spark/workspace-b/target/tmp/warehouse-205e25e7-8918-4615-acf1-10e06af7c35c/ddl_test1 already exists.;
[info]   at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:88)
[info]   at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:57)
[info]   at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:55)
[info]   at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:69)
[info]   at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
[info]   at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
[info]   at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[info]   at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
[info]   at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
[info]   at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:85)
[info]   at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:85)
[info]   at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:417)
[info]   at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:231)
[info]   at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:57)
[info]   at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:55)
[info]   at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:69)
[info]   at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
[info]   at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
[info]   at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
[info]   at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
[info]   at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
[info]   at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
[info]   at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:85)
[info]   at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:85)
[info]   at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:186)
[info]   at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:167)
[info]   at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)
[info]   at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:541)
[info]   at org.apache.spark.sql.test.SQLTestUtils$$anonfun$sql$1.apply(SQLTestUtils.scala:59)
[info]   at org.apache.spark.sql.test.SQLTestUtils$$anonfun$sql$1.apply(SQLTestUtils.scala:59)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite$$anonfun$23$$anonfun$apply$mcV$sp$34$$anonfun$apply$6.apply$mcV$sp(HiveDDLSuite.scala:597)
[info]   at org.apache.spark.sql.test.SQLTestUtils$class.withTable(SQLTestUtils.scala:166)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite.withTable(HiveDDLSuite.scala:32)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite$$anonfun$23$$anonfun$apply$mcV$sp$34.apply(HiveDDLSuite.scala:594)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite$$anonfun$23$$anonfun$apply$mcV$sp$34.apply(HiveDDLSuite.scala:590)
[info]   at org.apache.spark.sql.test.SQLTestUtils$class.withTempPath(SQLTestUtils.scala:114)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite.withTempPath(HiveDDLSuite.scala:32)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite$$anonfun$23.apply$mcV$sp(HiveDDLSuite.scala:590)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite$$anonfun$23.apply(HiveDDLSuite.scala:590)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite$$anonfun$23.apply(HiveDDLSuite.scala:590)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:57)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite.org$scalatest$BeforeAndAfterEach$$super$runTest(HiveDDLSuite.scala:32)
[info]   at org.scalatest.BeforeAndAfterEach$class.runTest(BeforeAndAfterEach.scala:255)
[info]   at org.apache.spark.sql.hive.execution.HiveDDLSuite.runTest(HiveDDLSuite.scala:32)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:381)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.apache.spark.SparkFunSuite.org$scalatest$BeforeAndAfterAll$$super$run(SparkFunSuite.scala:29)
[info]   at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
[info]   at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
[info]   at org.apache.spark.SparkFunSuite.run(SparkFunSuite.scala:29)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:357)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:502)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:296)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:286)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[info]   at java.lang.Thread.run(Thread.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12967376">SPARK-15269</key>
            <summary>Creating external table leaves empty directory under warehouse directory</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="xwu0226">Xin Wu</assignee>
                                    <reporter username="lian cheng">Cheng Lian</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 May 2016 11:37:37 +0000</created>
                <updated>Wed, 1 Jun 2016 23:02:59 +0000</updated>
                            <resolved>Wed, 1 Jun 2016 23:02:59 +0000</resolved>
                                    <version>2.0.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>SQL</component>
                    <component>Tests</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="15279992" author="lian cheng" created="Wed, 11 May 2016 11:42:41 +0000"  >&lt;p&gt;Investigated this issue for a while, and observed the following facts:&lt;/p&gt;

&lt;p&gt;When executing the first DDL command, we may have the following call chain:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;CreateDataSourceTableUtils.createDataSourceTable
 ...
  HiveClientImpl.createTable
   withHiveState                                    (1)
    shim.setCurrentSessionState(state)              (2)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The unexpected empty table directory doesn&apos;t exist at (1), but shows up almost immediately after invoking (2) although (2) itself doesn&apos;t create any directories.&lt;/p&gt;

&lt;p&gt;It seems like some background thread is scanning something...&lt;/p&gt;</comment>
                            <comment id="15280877" author="xwu0226" created="Wed, 11 May 2016 21:44:37 +0000"  >&lt;p&gt;The root cause maybe the following?&lt;/p&gt;

&lt;p&gt;When the first table is created as external table with the data source path, but as json,  createDataSourceTables considers it as non-hive compatible table because json is not a Hive SerDe. Then, newSparkSQLSpecificMetastoreTable is invoked to create the CatalogTable before asking HiveClient to create the metastore table. In this call,  locationURI is not set. So when we convert CatalogTable to HiveTable before passing to Hive Metastore, hive table&apos;s data location is not set. Then, Hive metastore implicitly creates a data location as &amp;lt;hive warehouse&amp;gt;/tableName, which is &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;/Users/lian/local/src/spark/workspace-b/target/tmp/warehouse-205e25e7-8918-4615-acf1-10e06af7c35c/ddl_test1&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; in this JIRA. I also verified that creating an external directly in Hive shell without a path will result in a default table directory created by hive. &lt;/p&gt;

&lt;p&gt;Then, even after dropping table, hive will not delete this stealth directory because the table is external. &lt;/p&gt;

&lt;p&gt;when we create the 2nd table with select and without a path, the table is created as managed table, provided a default path in the options:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val optionsWithPath =
      &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CaseInsensitiveMap(options).contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;path&quot;&lt;/span&gt;)) {
        isExternal = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
        options + (&lt;span class=&quot;code-quote&quot;&gt;&quot;path&quot;&lt;/span&gt; -&amp;gt; sessionState.catalog.defaultTablePath(tableIdent))
      } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
        options
      }&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This default path happens to be the hive&apos;s warehouse directory + the table name, which is the same as the one hive metastore implicitly created earlier for the 1st table.  So when trying to write the provided data to this data source table by &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; val plan =
          InsertIntoHadoopFsRelation(
            outputPath,
            partitionColumns.map(UnresolvedAttribute.quoted),
            bucketSpec,
            format,
            () =&amp;gt; Unit, &lt;span class=&quot;code-comment&quot;&gt;// No existing table needs to be refreshed.
&lt;/span&gt;            options,
            data.logicalPlan,
            mode)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;, InsertIntoHadoopFsRelation complains about the path existence since the SaveMode is SaveMode.ErrorIfExists.&lt;/p&gt;</comment>
                            <comment id="15280888" author="xwu0226" created="Wed, 11 May 2016 21:52:08 +0000"  >&lt;p&gt;In spark-shell, I can recreate it as following:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; spark.range(1).write.json(&lt;span class=&quot;code-quote&quot;&gt;&quot;/home/xwu0226/spark-test/data/spark-15269&quot;&lt;/span&gt;)
Datasource.write -&amp;gt; Path: file:/home/xwu0226/spark-test/data/spark-15269
                                                                                
scala&amp;gt; spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table spark_15269 using json options(PATH &lt;span class=&quot;code-quote&quot;&gt;&apos;/home/xwu0226/spark-test/data/spark-15269&apos;&lt;/span&gt;)&quot;&lt;/span&gt;)
16/05/11 14:51:00 WARN CreateDataSourceTableUtils: Couldn&apos;t find corresponding Hive SerDe &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; data source provider json. Persisting data source relation `spark_15269` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.
going through newSparkSQLSpecificMetastoreTable()
res1: org.apache.spark.sql.DataFrame = []

scala&amp;gt; spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;drop table spark_15269&quot;&lt;/span&gt;)
res2: org.apache.spark.sql.DataFrame = []

scala&amp;gt; spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table spark_15269 using json as select 1 as a&quot;&lt;/span&gt;)
org.apache.spark.sql.AnalysisException: path file:/user/hive/warehouse/spark_15269 already exists.;
  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:88)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:62)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:60)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:85)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:85)
  at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:418)
  at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:229)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:62)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:60)
  at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)
  at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
  at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)
  at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)
  at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:85)
  at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:85)
  at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:186)
  at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:167)
  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)
  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:541)
  ... 48 elided
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15281621" author="xwu0226" created="Thu, 12 May 2016 15:35:54 +0000"  >&lt;p&gt;For the case where we can not recreate this issue, it is because the default database path we got at &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (!&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; CaseInsensitiveMap(options).contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;path&quot;&lt;/span&gt;)) {
        isExternal = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
        options + (&lt;span class=&quot;code-quote&quot;&gt;&quot;path&quot;&lt;/span&gt; -&amp;gt; sessionState.catalog.defaultTablePath(tableIdent))
      } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
        options
      }&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  is different from hive metastore&apos;s default warehouse dir. They are &quot;/user/hive/warehouse&quot; and &quot;&amp;lt;spark_home&amp;gt;/spark-warehouse&quot;, respectively.  &lt;/p&gt;

&lt;p&gt;When creating the first table, hive metastore&apos;s default warehouse dir is &quot;&amp;lt;spark_home&amp;gt;/spark-warehouse&quot;, while when creating the second table without PATH option, the sessionState.catalog.defaultTablePath returns  &quot;/user/hive/warehouse&quot;. Therefore, the 2nd table creation will not hit the issue. But the first table still leave the empty table directory behind after being dropped. &lt;/p&gt;

&lt;p&gt;Two questions:&lt;br/&gt;
1. Should we keep these 2 default database path consistent?&lt;br/&gt;
2. If they are consistent, we will hit the issue reported in this JIRA.. Then, can we also assign the provided path to the CatalogTable.storage.locationURI, even though newSparkSQLSpecificMetastoreTable is called in createDataSourceTables for a non-hive compatible metastore table? This will avoid leaving hive metastore to pick the default path for the table. &lt;/p&gt;</comment>
                            <comment id="15281657" author="lian cheng" created="Thu, 12 May 2016 16:07:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xwu0226&quot; class=&quot;user-hover&quot; rel=&quot;xwu0226&quot;&gt;xwu0226&lt;/a&gt; Thanks a lot for the detailed investigation! Would you like to fix this issue?&lt;/p&gt;</comment>
                            <comment id="15281748" author="xwu0226" created="Thu, 12 May 2016 16:56:16 +0000"  >&lt;p&gt;Yes, I can . Thanks!&lt;/p&gt;</comment>
                            <comment id="15282215" author="xwu0226" created="Thu, 12 May 2016 22:59:22 +0000"  >&lt;p&gt;FYI.. &lt;br/&gt;
The reason why the default database paths obtained by different ways are different as mentioned above, is that I have an older metastore_db in my SPARK_HOME, where the metastore database keeps the old hive.metastore.warehouse.dir value (/user/hive/warehouse). After I removed this metastore_db, I get the database path consistent now. &lt;/p&gt;

&lt;p&gt;Testing the fix for #2 now. Will submit PR soon. &lt;/p&gt;</comment>
                            <comment id="15283685" author="apachespark" created="Sat, 14 May 2016 21:53:03 +0000"  >&lt;p&gt;User &apos;xwu0226&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13120&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13120&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15297408" author="lian cheng" created="Tue, 24 May 2016 00:22:25 +0000"  >&lt;p&gt;Two facts make this issue pretty hard to be fixed cleanly:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;When persisting an external Spark SQL data source table to Hive metastore, we can&apos;t store data location URI of the external table in the standard Hive &lt;tt&gt;o.a.h.hive.ql.metadata.Table.dataLocation&lt;/tt&gt; field, because Hive only accepts directory paths as location URI while Spark SQL also allows reading from a single file. Due to this reason, we have to store the actual data location as a SerDe property and ignore the standard &lt;tt&gt;dataLocation&lt;/tt&gt; field.&lt;/li&gt;
	&lt;li&gt;When creating a table, &lt;tt&gt;Hive.createTable&lt;/tt&gt; always tries to create an empty table directory under default warehouse directory when &lt;tt&gt;o.a.h.hive.ql.metadata.Table.dataLocation&lt;/tt&gt; is null. However, for external tables, this directory won&apos;t be deleted while dropping the table.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This leads to the following contradiction:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We can&apos;t set &lt;tt&gt;Table.dataLocation&lt;/tt&gt; because it have to be a directory path, while we must also allow file paths as data locations.&lt;/li&gt;
	&lt;li&gt;We have to set &lt;tt&gt;Table.dataLocation&lt;/tt&gt; because otherwise Hive creates an unexpected empty directory but doesn&apos;t remove it while dropping the external table, and thus causes the bug described in this ticket.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Here are two options:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Workaround this contradiction by setting &lt;tt&gt;Table.dataLocation&lt;/tt&gt; to a random location and then delete it manually after creating the external table
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;Pros: Fix the bug, and keeps backwards compatibility&lt;/li&gt;
		&lt;li&gt;Cons: Sounds like a pretty ad-hoc dirty fix&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Same as Hive, only allow using directory paths as data locations when creating Spark SQL external data source tables in Spark 2.0
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;Pros: Cleaner fix&lt;/li&gt;
		&lt;li&gt;Cons: Breaks backwards compatibility.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I&apos;m working a fix using the first approach.&lt;/p&gt;</comment>
                            <comment id="15297446" author="apachespark" created="Tue, 24 May 2016 00:42:04 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13270&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13270&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15311320" author="lian cheng" created="Wed, 1 Jun 2016 23:02:59 +0000"  >&lt;p&gt;Issue resolved by pull request 13270&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13270&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13270&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 24 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2xkfj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12329449">2.0.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>