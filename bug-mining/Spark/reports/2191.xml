<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:30:05 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-9206] ClassCastException using HiveContext with GoogleHadoopFileSystem as fs.defaultFS</title>
                <link>https://issues.apache.org/jira/browse/SPARK-9206</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Originally reported on StackOverflow: &lt;a href=&quot;http://stackoverflow.com/questions/31478955/googlehadoopfilesystem-cannot-be-cast-to-hadoop-filesystem&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://stackoverflow.com/questions/31478955/googlehadoopfilesystem-cannot-be-cast-to-hadoop-filesystem&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Google&apos;s &quot;bdutil&quot; command-line tool (&lt;a href=&quot;https://github.com/GoogleCloudPlatform/bdutil&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/GoogleCloudPlatform/bdutil&lt;/a&gt;) is one of the main supported ways of deploying Hadoop and Spark cluster on Google Cloud Platform, and has default settings which configure fs.defaultFS to use the Google Cloud Storage connector for Hadoop (and performs installation of the connector jarfile on top of tarball-based Hadoop and Spark distributions).&lt;/p&gt;

&lt;p&gt;Starting in Spark 1.4.1, taking a default bdutil-based Spark deployment, running &quot;spark-shell&quot;, and then trying to read a file with sqlContext like:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;sqlContext.parquetFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;gs:&lt;span class=&quot;code-comment&quot;&gt;//my-bucket/my-file.parquet&quot;&lt;/span&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;results in the following:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;15/07/20 20:59:14 DEBUG IsolatedClientLoader: shared class: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
java.lang.RuntimeException: java.lang.ClassCastException: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem cannot be cast to org.apache.hadoop.fs.FileSystem
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.client.ClientWrapper.&amp;lt;init&amp;gt;(ClientWrapper.scala:116)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.&amp;lt;init&amp;gt;(IsolatedClientLoader.scala:168)
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.&amp;lt;init&amp;gt;(HiveContext.scala:371)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)
	at org.apache.spark.sql.hive.HiveContext$$anon$1.&amp;lt;init&amp;gt;(HiveContext.scala:383)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)
	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)
	at org.apache.spark.sql.DataFrame.&amp;lt;init&amp;gt;(DataFrame.scala:131)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:264)
	at org.apache.spark.sql.SQLContext.parquetFile(SQLContext.scala:1099)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:19)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:24)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:26)
	at $iwC$$iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:28)
	at $iwC$$iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:30)
	at $iwC$$iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:32)
	at $iwC$$iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:34)
	at $iwC.&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:36)
	at &amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:38)
	at .&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:42)
	at .&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)
	at .&amp;lt;init&amp;gt;(&amp;lt;console&amp;gt;:7)
	at .&amp;lt;clinit&amp;gt;(&amp;lt;console&amp;gt;)
	at $print(&amp;lt;console&amp;gt;)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassCastException: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem cannot be cast to org.apache.hadoop.fs.FileSystem
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2595)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:169)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:342)
	... 67 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This appears to be a combination of &lt;a href=&quot;https://github.com/apache/spark/commit/9ac8393663d759860c67799e000ec072ced76493&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/9ac8393663d759860c67799e000ec072ced76493&lt;/a&gt; and its related &quot;isolated classloader&quot; changes with the IsolatedClientLoader.isSharedClass method including &quot;com.google.&lt;b&gt;&quot; alongside java.lang.&lt;/b&gt;, java.net.*, etc., as shared classes, presumably for inclusion of Guava and possibly protobuf and gson libraries.&lt;/p&gt;

&lt;p&gt;Unfortunately, this also includes the Hadoop extended libraries like com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem (&lt;a href=&quot;https://github.com/GoogleCloudPlatform/bigdata-interop&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/GoogleCloudPlatform/bigdata-interop&lt;/a&gt;) and com.google.cloud.bigtable.* (&lt;a href=&quot;https://github.com/GoogleCloudPlatform/cloud-bigtable-client&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/GoogleCloudPlatform/cloud-bigtable-client&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This can be reproduced by downloading bdutil from &lt;a href=&quot;https://github.com/GoogleCloudPlatform/bdutil&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/GoogleCloudPlatform/bdutil&lt;/a&gt;, modifying bdutil/extensions/spark/spark_env.sh to set SPARK_HADOOP2_TARBALL_URI to some Spark 1.4.1 tarball URI (http URIs should work as well) and then deploying a cluster with:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;./bdutil -p &amp;lt;your project&amp;gt; -b &amp;lt;your GCS bucket&amp;gt; -z us-central1-f -e hadoop2 -e spark deploy
./bdutil -p &amp;lt;your project&amp;gt; -b &amp;lt;your GCS bucket&amp;gt; -z us-central1-f -e hadoop2 -e spark shell
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The last command opens an SSH session; then type:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;spark-shell
&amp;gt; sqlContext.parquetFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;gs:&lt;span class=&quot;code-comment&quot;&gt;//your-bucket/some/path/to/parquet/file.parquet&quot;&lt;/span&gt;)&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The ClassCastException should then immediately get thrown.&lt;/p&gt;

&lt;p&gt;The simple fix of simply excluding com.google.cloud.* from being &quot;shared classes&quot; appears to work just fine in an end-to-end bdutil-based deployment. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12846405">SPARK-9206</key>
            <summary>ClassCastException using HiveContext with GoogleHadoopFileSystem as fs.defaultFS</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dennishuo">Dennis Huo</assignee>
                                    <reporter username="dennishuo">Dennis Huo</reporter>
                        <labels>
                    </labels>
                <created>Mon, 20 Jul 2015 23:57:42 +0000</created>
                <updated>Tue, 10 Nov 2020 19:50:55 +0000</updated>
                            <resolved>Tue, 21 Jul 2015 20:13:06 +0000</resolved>
                                    <version>1.4.1</version>
                                    <fixVersion>1.5.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="14634310" author="apachespark" created="Tue, 21 Jul 2015 00:03:07 +0000"  >&lt;p&gt;User &apos;dennishuo&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/7549&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/7549&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14635739" author="marmbrus" created="Tue, 21 Jul 2015 20:13:06 +0000"  >&lt;p&gt;Issue resolved by pull request 7549&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/7549&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/7549&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13339776">HADOOP-17372</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 17 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2hh1j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>