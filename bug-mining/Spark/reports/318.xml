<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:14:03 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-1518] Spark master doesn&apos;t compile against hadoop-common trunk</title>
                <link>https://issues.apache.org/jira/browse/SPARK-1518</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;FSDataOutputStream::sync() has disappeared from trunk in Hadoop; FileLogger.scala is calling it.&lt;/p&gt;

&lt;p&gt;I&apos;ve changed it locally to hsync() so I can compile the code, but haven&apos;t checked yet whether those are equivalent. hsync() seems to have been there forever, so it hopefully works with all versions Spark cares about.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12708902">SPARK-1518</key>
            <summary>Spark master doesn&apos;t compile against hadoop-common trunk</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cmccabe">Colin McCabe</assignee>
                                    <reporter username="vanzin">Marcelo Masiero Vanzin</reporter>
                        <labels>
                    </labels>
                <created>Wed, 16 Apr 2014 20:23:54 +0000</created>
                <updated>Wed, 4 Jun 2014 22:56:55 +0000</updated>
                            <resolved>Wed, 4 Jun 2014 22:56:55 +0000</resolved>
                                                    <fixVersion>1.0.1</fixVersion>
                    <fixVersion>1.1.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="13973084" author="witgo" created="Thu, 17 Apr 2014 15:46:11 +0000"  >&lt;p&gt;As the hadoop API changes, some methods have been removed.&lt;br/&gt;
The hadoop related in spark core Independence to new modules. As in the case of yarn.&lt;/p&gt;</comment>
                            <comment id="14000612" author="cmccabe" created="Sat, 17 May 2014 02:06:59 +0000"  >&lt;p&gt;&lt;tt&gt;FSDataOutputStream::sync()&lt;/tt&gt; has been deprecated for a while, and finally got removed in &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-8124&quot; title=&quot;Remove the deprecated Syncable.sync() method&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-8124&quot;&gt;&lt;del&gt;HADOOP-8124&lt;/del&gt;&lt;/a&gt;.  It was a synonym for &lt;tt&gt;hflush&lt;/tt&gt;, so replacing it with that function would probably be more appropriate.  You usually don&apos;t want &lt;tt&gt;hsync&lt;/tt&gt;, since it forces an &lt;tt&gt;fsync&lt;/tt&gt; on each datanode that contains the file.&lt;/p&gt;

&lt;p&gt;It looks like there&apos;s some other YARN-related stuff that still fails to build once I make this fix, though...&lt;/p&gt;</comment>
                            <comment id="14002483" author="cmccabe" created="Mon, 19 May 2014 22:11:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pwendell&quot; class=&quot;user-hover&quot; rel=&quot;pwendell&quot;&gt;pwendell&lt;/a&gt;, I can take a look at this.  Can you assign it to me?&lt;/p&gt;</comment>
                            <comment id="14009135" author="pwendell" created="Mon, 26 May 2014 23:26:39 +0000"  >&lt;p&gt;Hey &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cmccabe&quot; class=&quot;user-hover&quot; rel=&quot;cmccabe&quot;&gt;cmccabe&lt;/a&gt;, what is the oldest version of hadooop that contains hflush? /cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor&quot; class=&quot;user-hover&quot; rel=&quot;andrewor&quot;&gt;andrewor&lt;/a&gt; who IIRC looked into this a bunch when writing the logger.&lt;/p&gt;</comment>
                            <comment id="14010068" author="cmccabe" created="Tue, 27 May 2014 18:34:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;Hey Colin Patrick McCabe, what is the oldest version of hadooop that contains hflush? /cc Andrew Or who IIRC looked into this a bunch when writing the logger.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The oldest Apache release I know about with hflush is Hadoop 0.21.&lt;/p&gt;</comment>
                            <comment id="14010117" author="pwendell" created="Tue, 27 May 2014 18:51:43 +0000"  >&lt;p&gt;Ah okay. I&apos;m not sure what the oldest version Hadoop that spark Spark compiles against pre 0.21, but it&apos;s worth knowing whether this change would cause us to drop support for some of the older versions.&lt;/p&gt;</comment>
                            <comment id="14010284" author="cmccabe" created="Tue, 27 May 2014 21:17:26 +0000"  >&lt;p&gt;I think it&apos;s very, very, very unlikely that anyone will want to run Hadoop 0.20 against Spark.  Why don&apos;t we:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;fix the compile against Hadoop trunk&lt;/li&gt;
	&lt;li&gt;wait for someone to show up who wants compatibility with hadoop 0.20 before we work on it?&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It seems like if there is interest in Spark on Hadoop 0.20, there will quickly be a patch submitted to get it compiling there.  If there is no such interest, then we&apos;ll be done here without doing a lot of work up front.&lt;/p&gt;

&lt;p&gt;If you agree then I&apos;ll create a pull req.  I have verified that fixing the flush thing un-breaks the compile on master.&lt;/p&gt;</comment>
                            <comment id="14010294" author="srowen" created="Tue, 27 May 2014 21:23:10 +0000"  >&lt;p&gt;0.20.x stopped in early 2010. It is ancient.&lt;/p&gt;</comment>
                            <comment id="14010296" author="cmccabe" created="Tue, 27 May 2014 21:27:26 +0000"  >&lt;p&gt;I wonder if we should have a discussion on the mailing list about the oldest version of Hadoop we should support.  I would argue that it should be 0.23.  Yahoo! is still using that version.  Perhaps other people have more information than I do, though.&lt;/p&gt;

&lt;p&gt;If we decide to support 0.20, I will create a patch that does this using reflection.  But I&apos;d rather get your guys&apos; opinion on whether that make sense first.&lt;/p&gt;</comment>
                            <comment id="14010305" author="vanzin" created="Tue, 27 May 2014 21:33:17 +0000"  >&lt;p&gt;Hmm, may I suggest a different approach?&lt;/p&gt;

&lt;p&gt;Andrew, who wrote the code, might have more info. But from my understanding, the flushes were needed because the history server might read logs from applications that were not yet finished. So the flush was a best-effort to avoid having the HS read files that contained partial JSON objects (and fail to parse them).&lt;/p&gt;

&lt;p&gt;But since then the HS was changed to only read logs from finished applications. I think it&apos;s safe to assume that finished applications are not writing to the event log anymore, so the above scenario doesn&apos;t exist.&lt;/p&gt;

&lt;p&gt;So could we just get rid of the explicit flush instead?&lt;/p&gt;</comment>
                            <comment id="14010322" author="srowen" created="Tue, 27 May 2014 21:39:25 +0000"  >&lt;p&gt;RE: Hadoop versions, in my reckoning of the twisted world of Hadoop versions, the 0.23.x branch is still active and so is kind of later than 1.0.x. It may be easier to retain 0.23 compatibility than 1.0.x for example.&lt;/p&gt;</comment>
                            <comment id="14010339" author="pwendell" created="Tue, 27 May 2014 21:47:01 +0000"  >&lt;p&gt;Re: versioning - I was just asking whether this changes the oldest version we are compatible with. That&apos;s a question we should ask of all Hadoop patches. I just tested Spark and it doesn&apos;t compile against 0.20.X, so this is a no-op in terms of compatibility anyways.&lt;/p&gt;

&lt;p&gt;It would be good to list the oldest upstream Hadoop version we support. Many people still run Spark against Hadoop 1.X/CDH3 variants. We get high download rates for those pre-built packages. I think this is a little different than e.g. CDH where people upgrade all components at the same time... people download newer versions of Spark and run it with old filesystems very often.&lt;/p&gt;</comment>
                            <comment id="14010364" author="cmccabe" created="Tue, 27 May 2014 21:56:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;It would be good to list the oldest upstream Hadoop version we support. Many people still run Spark against Hadoop 1.X/CDH3 variants. We get high download rates for those pre-built packages. I think this is a little different than e.g. CDH where people upgrade all components at the same time... people download newer versions of Spark and run it with old filesystems very often.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks, Patrick.  This is useful info... I didn&apos;t realize there was still interest in running Spark against CDH3.  Certainly we&apos;ll never support it directly, since CDH3 was end-of-lifed last year.  So we don&apos;t really support doing anything on CDH3... except upgrading it to CDH4 (or hopefully, 5 which has Spark. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Re: versioning - I was just asking whether this changes the oldest version we are compatible with. That&apos;s a question we should ask of all Hadoop patches. I just tested Spark and it doesn&apos;t compile against 0.20.X, so this is a no-op in terms of compatibility anyways.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It sounds like we&apos;re good to go on replacing the &lt;tt&gt;flush&lt;/tt&gt; with &lt;tt&gt;hsync&lt;/tt&gt; then?  I notice you marked this as &quot;critical&quot; recently; do you think it&apos;s important to 1.0?&lt;/p&gt;</comment>
                            <comment id="14010395" author="pwendell" created="Tue, 27 May 2014 22:13:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;Thanks, Patrick. This is useful info... I didn&apos;t realize there was still interest in running Spark against CDH3. Certainly we&apos;ll never support it directly, since CDH3 was end-of-lifed last year. So we don&apos;t really support doing anything on CDH3... except upgrading it to CDH4 (or hopefully, 5 which has Spark. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, the upstream project tries pretty hard to maintain compatibility with as many Hadoop versions as possible since we see many people running even fairly old ones in the wild. Of course, I&apos;m sure Cloudera will commercially support only the most recent ones.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Re: versioning - I was just asking whether this changes the oldest version we are compatible with. That&apos;s a question we should ask of all Hadoop patches. I just tested Spark and it doesn&apos;t compile against 0.20.X, so this is a no-op in terms of compatibility anyways.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It won&apos;t make the 1.0.0 window but we should get it into a 1.0.X release. My concern is that once newer Hadoop versions come out, I want people to be able to compile Spark against them. Again, since Spark is distributed independently from HDFS, this is something that happens a lot, people try to compile older Spark releases against newer Hadoop releases.&lt;/p&gt;</comment>
                            <comment id="14010578" author="cmccabe" created="Wed, 28 May 2014 00:29:55 +0000"  >&lt;p&gt;Sounds good.  &lt;a href=&quot;https://github.com/apache/spark/pull/898&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/898&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14010937" author="srowen" created="Wed, 28 May 2014 08:55:21 +0000"  >&lt;p&gt;Re: versioning one more time, really supporting a bunch of versions may get costly. It&apos;s already tricky to manage two builds times YARN-or-not, Hive-or-not, times 4 flavors of Hadoop. I doubt the assemblies are yet problem-free in all cases. &lt;/p&gt;

&lt;p&gt;In practice it look like one generic Hadoop 1, Hadoop 2, and CDH 4 release is produced, and 1 set of Maven artifact. (PS again I am not sure Spark should contain a CDH-specific distribution? realizing it&apos;s really a proxy for a particular Hadoop combo. Same goes for a MapR profile, which is really for vendors to maintain) That means right now you can&apos;t build a Spark app for anything but Hadoop 1.x with Maven, without installing it yourself, and there&apos;s not an official distro for anything but two major Hadoop versions. Support for niche versions isn&apos;t really there or promised anyway, and fleshing out &quot;support&quot; may make doing so pretty burdensome. &lt;/p&gt;

&lt;p&gt;There is no suggested action here; if anything I suggest that the right thing is to add Maven artifacts with classifiers, add a few binary artifacts, subtract a few vendor artifacts, but this is a different action.&lt;/p&gt;</comment>
                            <comment id="14011350" author="cmccabe" created="Wed, 28 May 2014 17:32:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;Re: versioning one more time, really supporting a bunch of versions may get costly. It&apos;s already tricky to manage two builds times YARN-or-not, Hive-or-not, times 4 flavors of Hadoop. I doubt the assemblies are yet problem-free in all cases.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think in this particular case, we can use reflection to support both Hadoop 1.X and newer stuff.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I am not sure Spark should contain a CDH-specific distribution? realizing it&apos;s really a proxy for a particular Hadoop combo. Same goes for a MapR profile, which is really for vendors to maintain)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree 100%.  We should keep vendor stuff out of the Apache release.  Vendors can create their own build setups (that&apos;s what they get paid to do, after all.)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;There is no suggested action here; if anything I suggest that the right thing is to add Maven artifacts with classifiers, add a few binary artifacts, subtract a few vendor artifacts, but this is a different action.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you have some ideas for how to improve the Maven build, it could be worth creating a JIRA.  I think you&apos;re right that we need to make it more flexible so that people can build against more versions without editing the pom.  It might be helpful to look at how HBase handles this in its &lt;tt&gt;pom.xml&lt;/tt&gt; files.&lt;/p&gt;</comment>
                            <comment id="14011851" author="pwendell" created="Wed, 28 May 2014 23:59:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;In practice it look like one generic Hadoop 1, Hadoop 2, and CDH 4 release is produced, and 1 set of Maven artifact. (PS again I am not sure Spark should contain a CDH-specific distribution? realizing it&apos;s really a proxy for a particular Hadoop combo. Same goes for a MapR profile, which is really for vendors to maintain) That means right now you can&apos;t build a Spark app for anything but Hadoop 1.x with Maven, without installing it yourself, and there&apos;s not an official distro for anything but two major Hadoop versions. Support for niche versions isn&apos;t really there or promised anyway, and fleshing out &quot;support&quot; may make doing so pretty burdensome.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We need to update the list of binary builds for Spark... some are getting outdated. The workflow for people building Spark apps is that they write their app against the Spark API&apos;s in Maven central (they can do this no matter which cluster they want to run on). To run the app, If they just want to run it locally they can spark-submit from any compiled package of Spark, or they can use their build tool to just run it. If they want to submit it to a cluster, users need to have a Spark package compiled for the Hadoop version on the cluster. Because of this we distribute pre-compiled builds to allow people to avoid ever having to compile Spark.&lt;/p&gt;

&lt;p&gt;In terms of vendor-specific builds, we&apos;ve done this because users asked for it. It&apos;s useful if, e.g. a user wants to submit a Spark job to a CDH or MapR cluster. Or run spark-shell locally and read data from a CDH HDFS cluster. That&apos;s the main use case we want to support.&lt;/p&gt;

&lt;p&gt;I don&apos;t know what it means that you &quot;can&apos;t build a Spark app&quot; for Hadoop 2.X. Building a Spark app is intentionally decoupled from the process of submitting an app to a cluster. We want users to be able to build Spark apps that they can run on e.g. different versions of Hadoop.&lt;/p&gt;</comment>
                            <comment id="14011897" author="srowen" created="Thu, 29 May 2014 00:42:37 +0000"  >&lt;p&gt;&quot;they write their app against the Spark API&apos;s in Maven central (they can do this no matter which cluster they want to run on)&quot; &lt;/p&gt;

&lt;p&gt;Yeah this is the issue. OK, if I compile against Spark artifacts as a runtime dependency and submit an app to the cluster, it should be OK no matter what build of Spark is running. The binding from Spark to Hadoop is hidden from the app.&lt;/p&gt;

&lt;p&gt;I am thinking of the case where I want to build an app that is a client of Spark &amp;#8211; embedding it. Then I am including the client of Hadoop for example. I have to match my cluster than and there is no Hadoop 2 Spark artifact.&lt;/p&gt;

&lt;p&gt;Am I missing something big here? that&apos;s my premise about why there would ever be a need for different artifacts. It&apos;s the same use case as in Sandy&apos;s blog: &lt;a href=&quot;http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://blog.cloudera.com/blog/2014/04/how-to-run-a-simple-apache-spark-app-in-cdh-5/&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14011942" author="matei" created="Thu, 29 May 2014 01:31:25 +0000"  >&lt;p&gt;Sean, the model for linking to Hadoop has been that users also add a dependency on hadoop-client if they want to access HDFS for the past few releases. See &lt;a href=&quot;http://spark.apache.org/docs/latest/scala-programming-guide.html#linking-with-spark&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/docs/latest/scala-programming-guide.html#linking-with-spark&lt;/a&gt; for example. This model is there because Hadoop itself has decided to create the hadoop-client Maven artifact as a way to get apps to link to it. It works for all the recent versions of Hadoop as far as I know &amp;#8211; users don&apos;t have to link against a custom-built Spark for their distro.&lt;/p&gt;

&lt;p&gt;Regarding binary builds on apache.org, we want users to be able to start using Spark as conveniently as possible on any distribution. It is the goal of the Apache project to have people use Apache Spark as easily as possible.&lt;/p&gt;</comment>
                            <comment id="14012301" author="srowen" created="Thu, 29 May 2014 11:55:09 +0000"  >&lt;p&gt;Yes Matei that&apos;s what I&apos;m getting at. Spark is a client of Hadoop, so if I use Spark, and Spark uses Hadoop, then I have to match the Hadoop that Spark uses to the cluster. It&apos;s not just if my app uses HDFS directly. I can manually override hadoop-client, although, I&apos;d have to reproduce a lot of the dependency-graph manipulation in Spark&apos;s build to make it work.&lt;/p&gt;

&lt;p&gt;In Sandy&apos;s blog post example he&apos;s just running the code on the cluster and pointing at the matched Spark/Hadoop jars already there. That&apos;s also a solution that will work for a lot of use cases. I accept that the use case I have in mind, which is adding Spark to a larger stand-alone app, is not everyone&apos;s use case, although it&apos;s not crazy. It doesn&apos;t work out if instead the Spark/Hadoop jars are packaged together into an assembly and run that way.&lt;/p&gt;

&lt;p&gt;I agree overriding the Hadoop dependency is a solution, and accept that Spark shouldn&apos;t necessarily bend over backwards for these Hadoop issues, but this does go back to your point about accessibility. Right now I think anyone that wants to do what I&apos;m doing for any Hadoop 2 app, and doesn&apos;t want to make a custom build or manually override dependencies, will just point at Cloudera&apos;s &quot;0.9.0-cdh5.0.1&quot; even if not using CDH. That felt funny.&lt;/p&gt;

&lt;p&gt;Apologies if I have somehow totally missed something. I&apos;ve talked too much, thanks for hearing out the use case. Maybe best to see if this is actually an issue anyone shares.&lt;/p&gt;</comment>
                            <comment id="14012520" author="matei" created="Thu, 29 May 2014 16:50:56 +0000"  >&lt;p&gt;Sorry, I&apos;m still not sure I understand what you&apos;re asking for &amp;#8211; maybe I missed it above. Are you worried that the Spark assembly on the cluster has to be pre-built against Hadoop? We could perhaps make it find stuff out of HADOOP_HOME, but then it wouldn&apos;t work for users that don&apos;t have a Hadoop installation, which is a lot of users. For client apps, it&apos;s really enough to add that hadoop-client dependency. No other manipulation is needed.&lt;/p&gt;

&lt;p&gt;If you want to build a client app that automatically works with multiple versions of Hadoop, you can also package it with Spark and hadoop-client marked as &quot;provided&quot; and use spark-submit to put the Spark assembly on your cluster in the classpath. Then it will work with whatever version that was built against. But you need to specify hadoop-client when you run without spark-submit if you want to talk to the version of HDFS in your cluster (e.g. you&apos;re testing the app on your laptop and trying to make it read from HDFS).&lt;/p&gt;</comment>
                            <comment id="14012552" author="srowen" created="Thu, 29 May 2014 17:11:17 +0000"  >&lt;p&gt;Heh, I think the essence is: at least one more separate Maven artifact, under a different classifier, for Hadoop 2.x builds. If you package that, you get Spark and everything it needs to work against a Hadoop 2 cluster. Yeah I see that you&apos;re suggesting various ways to push the app to the cluster, where it can bind to the right version of things, and that may be the right-est way to think about this. I had envisioned running a stand-alone app on a machine that is not part of the cluster, that is a client of it, and this means packaging in the right Hadoop client dependencies, and Spark already declares how it wants to include these various Hadoop client versions &amp;#8211; it&apos;s more than just including hadoop-client &amp;#8211; so wanted to leverage that. Let&apos;s see if this actually turns out to be a broader request though.&lt;/p&gt;</comment>
                            <comment id="14012651" author="matei" created="Thu, 29 May 2014 18:15:59 +0000"  >&lt;p&gt;Okay, got it. But this only applies to you running the job on your laptop, right? Because otherwise you&apos;ll get the right Hadoop via the installation on the cluster.&lt;/p&gt;

&lt;p&gt;For this use case I still think it&apos;s fine to require use of hadoop-client. It&apos;s been like that for the past 2 releases and nobody has asked questions about it. It&apos;s just one more entry to add to your pom.xml.&lt;/p&gt;

&lt;p&gt;The concrete problem is that Hadoop has been extremely fickle with compatibility even within a major release series (1.x or 2.x). HDFS protocol versions change and you can&apos;t access the cluster, YARN versions change, etc. I don&apos;t think there&apos;s a single release I&apos;d call &quot;Hadoop 2&quot;, and it would be confusing to users to link to the &quot;Hadoop 2&quot; artifact and not have it run on their cluster.&lt;/p&gt;</comment>
                            <comment id="14012655" author="matei" created="Thu, 29 May 2014 18:17:39 +0000"  >&lt;p&gt;BTW one other thing is that in 1.0, you can also use spark-submit in local mode to get your locally installed Spark. So people will be able to yum install spark-from-their-vendor, build their app with just spark-core, and then run it with the spark-submit on their PATH.&lt;/p&gt;</comment>
                            <comment id="14012895" author="cmccabe" created="Thu, 29 May 2014 21:00:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;The concrete problem is that Hadoop has been extremely fickle with compatibility even within a major release series (1.x or 2.x). HDFS protocol versions change and you can&apos;t access the cluster, YARN versions change, etc. I don&apos;t think there&apos;s a single release I&apos;d call &quot;Hadoop 2&quot;, and it would be confusing to users to link to the &quot;Hadoop 2&quot; artifact and not have it run on their cluster.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I know that there was an RPC compatibility break between &lt;tt&gt;2.1.1-beta&lt;/tt&gt; and &lt;tt&gt;2.1.0-beta&lt;/tt&gt;.  Around the 2.3 time-frame, Hadoop decided to freeze the RPC format at version 9, and try to maintain compatibility going forward.  You are right that bundling the appropriate version of the Hadoop client is the usual approach that projects which depend on Hadoop take, exactly to avoid these kinds of worries.&lt;/p&gt;</comment>
                            <comment id="14013378" author="sandyr" created="Fri, 30 May 2014 07:26:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t think there&apos;s a single release I&apos;d call &quot;Hadoop 2&quot;, and it would be confusing to users to link to the &quot;Hadoop 2&quot; artifact and not have it run on their cluster.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;While Hadoop releases have not historically been amazing at maintaining compatibility, I think this a bit of an overstatement.  There is a definitive Hadoop 2, which became GA starting at 2.2.  It has a set of public/stable APIs that have not been broken since then, a promise not to break them for the remainder of 2.x, and a comprehensive compatibility guide that describes exactly what &quot;break&quot; means - &lt;a href=&quot;http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/Compatibility.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hadoop.apache.org/docs/r2.3.0/hadoop-project-dist/hadoop-common/Compatibility.html&lt;/a&gt;.  All the major distributions (CDH, Pivotal, HDP, I think MapR?) support these APIs.  The Hadoop 2 releases that preceded 2.2 were labeled alpha and beta, and did not come with these same guarantees.&lt;/p&gt;

&lt;p&gt;While, with my Cloudera hat on, I&apos;d love for the CDH5 Spark artifacts to become the canonical Spark Hadoop 2 artifacts, with my Apache hat on, I do see some value in publishing Spark Hadoop 2 artifacts.  Though, as Matei and Patrick pointed out, these only matter when bundling Spark inside your own application.  In most cases, it&apos;s better to point to Spark jars installed on one&apos;s laptop or cluster.&lt;/p&gt;</comment>
                            <comment id="14013591" author="srowen" created="Fri, 30 May 2014 12:17:05 +0000"  >&lt;p&gt;Sorry for one more message here to reply to Matei &amp;#8211; yes it&apos;s the &quot;laptop&quot; use case except I&apos;d describe that as a not-uncommon production deployment! it&apos;s the embedded-client scenario. It is more than adding one hadoop-client dependency, because you need to emulate the excludes, etc that Spark has to. (But yeah then it works.) I agree supporting a bunch of Hadoop versions gets painful, as a result. This was why I was suggesting way up top that supporting old versions may become more trouble than its worht.&lt;/p&gt;</comment>
                            <comment id="14014536" author="matei" created="Sat, 31 May 2014 06:08:11 +0000"  >&lt;p&gt;Got it, the excludes have indeed gotten more painful, and I can see that being a problem. Maybe the solution would be to publish some kind of &quot;spark-core-hadoopX&quot; for each version of Hadoop, which depends on hadoop-client and spark-core. But then we&apos;ll need a list of supported versions to publish for. By the way AFAIK Maven classifiers do not solve this issue, as versions of an artifact with different classifiers must have the same dependency tree (they can differ in other things, e.g. maybe they&apos;re compiled on different Java versions).&lt;/p&gt;

&lt;p&gt;BTW, in terms of the Hadoop 2 thing, I just meant that there was a lot of variability before the community decided to go GA, and unfortunately a lot of users are on older versions. (Ironically sometimes because of this volatility). I definitely appreciate the move to GA and the compatibility policies. When I said Hadoop 2, I meant that, for example, do you consider 0.23 to be Hadoop 2? It&apos;s YARN-based and it was (and still is AFAIK) widely used at Yahoo. What about 2.0.x? Some users are on that too. What about CDH4? Its version number is 2.0.0-something. In any case we will be sensible about old versions, but my philosophy is always to support the broadest range possible, and from everything I&apos;ve seen it&apos;s paid off &amp;#8211; users appreciate when you do not force their hand to upgrade. This is why Yahoo for example continues to be our biggest contributor on YARN support, even though their YARN is pretty different.&lt;/p&gt;</comment>
                            <comment id="14014910" author="cmccabe" created="Sun, 1 Jun 2014 05:31:28 +0000"  >&lt;p&gt;It seems reasonable to have a list of supported versions in the Maven build.  That wouldn&apos;t exclude people from building against other versions, of course, but they might have to supply a maven definition via &lt;tt&gt;&amp;#45;D&lt;/tt&gt; or something.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For example, do you consider 0.23 to be Hadoop 2? It&apos;s YARN-based and it was (and still is AFAIK) widely used at Yahoo&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;0.23 is not Hadoop 2.  It&apos;s a branch that Yahoo! uses internally.  Everyone else has moved on to branch-2 (Hortonworks, Cloudera, WANDisco, Intel, etc. etc.)  Yahoo! also has some clusters running on branch-2, and that is their future too.  More info here: &lt;a href=&quot;http://osdir.com/ml/general-hadoop-apache/2012-04/msg00000.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://osdir.com/ml/general-hadoop-apache/2012-04/msg00000.html&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What about CDH4? Its version number is 2.0.0-something&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Technically CDH4 is &quot;Cloudera&apos;s distribution of Hadoop including Apache Hadoop 2.0.0.&quot;  Its evolution didn&apos;t stop with 2.0.0, though.  We still are going to make another release in the cdh4 line where we backport some things.  CDH5 is where the focus is now, though.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In any case we will be sensible about old versions, but my philosophy is always to support the broadest range possible, and from everything I&apos;ve seen it&apos;s paid off &#8211; users appreciate when you do not force their hand to upgrade. This is why Yahoo for example continues to be our biggest contributor on YARN support, even though their YARN is pretty different.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agree.&lt;/p&gt;</comment>
                            <comment id="14018303" author="pwendell" created="Wed, 4 Jun 2014 22:56:55 +0000"  >&lt;p&gt;Issue resolved by pull request 898&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/898&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/898&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>387225</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 24 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1upfb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>387488</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12326744">1.0.1</customfieldvalue>
    <customfieldvalue id="12326686">1.1.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>