<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:49:34 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-18220] ClassCastException occurs when using select query on ORC file</title>
                <link>https://issues.apache.org/jira/browse/SPARK-18220</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Error message is below.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;==========================================================
16/11/02 16:38:09 INFO ReaderImpl: Reading ORC rows from hdfs://xxx/part-00022 with {include: [true], offset: 0, length: 9223372036854775807}
16/11/02 16:38:09 INFO Executor: Finished task 17.0 in stage 22.0 (TID 42). 1220 bytes result sent to driver
16/11/02 16:38:09 INFO TaskSetManager: Finished task 17.0 in stage 22.0 (TID 42) in 116 ms on localhost (executor driver) (19/20)
16/11/02 16:38:09 ERROR Executor: Exception in task 10.0 in stage 22.0 (TID 35)
java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.io.Text
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$23.apply(HiveInspectors.scala:526)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:435)
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:232)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:804)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$24.apply(RDD.scala:804)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:99)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

ORC dump info.
==========================================================
File Version: 0.12 with HIVE_8732
16/11/02 16:39:21 INFO orc.ReaderImpl: Reading ORC rows from hdfs://XXX/part-00000 with {include: null, offset: 0, length: 9223372036854775807}
16/11/02 16:39:21 INFO orc.RecordReaderFactory: Schema is not specified on read. Using file schema.
Rows: 7
Compression: ZLIB
Compression size: 262144
Type: struct&amp;lt;a:varchar(2),b(50),c:varchar(6),d:varchar(50),e:varchar(4),f:varchar(50),g:timestamp&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13017115">SPARK-18220</key>
            <summary>ClassCastException occurs when using select query on ORC file</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cloud_fan">Wenchen Fan</assignee>
                                    <reporter username="jerryjung">Yousun Jeong</reporter>
                        <labels>
                            <label>orcfile</label>
                            <label>sql</label>
                    </labels>
                <created>Wed, 2 Nov 2016 07:56:28 +0000</created>
                <updated>Wed, 30 Nov 2016 17:47:25 +0000</updated>
                            <resolved>Wed, 30 Nov 2016 17:47:24 +0000</resolved>
                                    <version>2.1.0</version>
                                    <fixVersion>2.1.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="15653246" author="cloud_fan" created="Thu, 10 Nov 2016 06:59:25 +0000"  >&lt;p&gt;Is it an external orc file or written by Spark SQL? &lt;/p&gt;</comment>
                            <comment id="15656623" author="jerryjung" created="Fri, 11 Nov 2016 09:32:00 +0000"  >&lt;p&gt;It is written by Spark SQL. &lt;/p&gt;</comment>
                            <comment id="15665580" author="cloud_fan" created="Tue, 15 Nov 2016 00:51:42 +0000"  >&lt;p&gt;do you have the code snippet to reproduce this bug? i.e. how you write and read the orc file&lt;/p&gt;</comment>
                            <comment id="15696541" author="hvanhovell" created="Fri, 25 Nov 2016 19:32:54 +0000"  >&lt;p&gt;I tried reproducing this but to no avail. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jerryjung&quot; class=&quot;user-hover&quot; rel=&quot;jerryjung&quot;&gt;jerryjung&lt;/a&gt; could you give us a reproducible example?&lt;/p&gt;</comment>
                            <comment id="15698776" author="cloud_fan" created="Sun, 27 Nov 2016 00:46:20 +0000"  >&lt;p&gt;I&apos;m closing this ticket, please reopen it if there is a reproducible example&lt;/p&gt;</comment>
                            <comment id="15701117" author="jerryjung" created="Mon, 28 Nov 2016 06:42:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt;&lt;br/&gt;
For further information, tables were created in spark 1.X version.&lt;br/&gt;
The original data&apos;s orc dump is below.&lt;br/&gt;
===========================================================================================================&lt;br/&gt;
File Version: 0.12 with HIVE_8732&lt;br/&gt;
16/11/28 15:11:51 INFO orc.ReaderImpl: Reading ORC rows from hdfs://xxx:8020/xxx/part-00000 with &lt;/p&gt;
{include: null, offset: 0, length: 9223372036854775807}
&lt;p&gt;16/11/28 15:11:51 INFO orc.RecordReaderFactory: Schema is not specified on read. Using file schema.&lt;br/&gt;
Rows: 7&lt;br/&gt;
Compression: ZLIB&lt;br/&gt;
Compression size: 262144&lt;br/&gt;
Type: struct&amp;lt;aaa:varchar(2),bbb:varchar(50),ccc:varchar(6),ddd:varchar(50),eee:timestamp&amp;gt;&lt;/p&gt;

&lt;p&gt;Log message is below.&lt;br/&gt;
===========================================================================================================&lt;br/&gt;
16/11/28 15:35:31 INFO CatalystSqlParser: Parsing command: varchar(6)&lt;br/&gt;
16/11/28 15:35:31 INFO CatalystSqlParser: Parsing command: varchar(50)&lt;br/&gt;
16/11/28 15:35:31 INFO CatalystSqlParser: Parsing command: varchar(4)&lt;br/&gt;
16/11/28 15:35:31 INFO CatalystSqlParser: Parsing command: varchar(50)&lt;br/&gt;
16/11/28 15:35:31 INFO CatalystSqlParser: Parsing command: timestamp&lt;br/&gt;
16/11/28 15:35:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 258.9 KB, free 398.7 MB)&lt;br/&gt;
16/11/28 15:35:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.1 KB, free 398.7 MB)&lt;br/&gt;
16/11/28 15:35:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 80.80.11.97:47902 (size: 23.1 KB, free: 399.0 MB)&lt;br/&gt;
16/11/28 15:35:31 INFO SparkContext: Created broadcast 2 from processCmd at CliDriver.java:376&lt;br/&gt;
16/11/28 15:35:31 INFO PerfLogger: &amp;lt;PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl&amp;gt;&lt;br/&gt;
16/11/28 15:35:31 INFO OrcInputFormat: FooterCacheHitRatio: 0/0&lt;br/&gt;
16/11/28 15:35:31 INFO PerfLogger: &amp;lt;/PERFLOG method=OrcGetSplits start=1480314931501 end=1480314931511 duration=10 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl&amp;gt;&lt;br/&gt;
16/11/28 15:35:31 INFO SparkContext: Starting job: processCmd at CliDriver.java:376&lt;br/&gt;
16/11/28 15:35:31 INFO DAGScheduler: Got job 1 (processCmd at CliDriver.java:376) with 2 output partitions&lt;br/&gt;
16/11/28 15:35:31 INFO DAGScheduler: Final stage: ResultStage 1 (processCmd at CliDriver.java:376)&lt;br/&gt;
16/11/28 15:35:31 INFO DAGScheduler: Parents of final stage: List()&lt;br/&gt;
16/11/28 15:35:31 INFO DAGScheduler: Missing parents: List()&lt;br/&gt;
16/11/28 15:35:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;9&amp;#93;&lt;/span&gt; at processCmd at CliDriver.java:376), which has no missing parents&lt;br/&gt;
16/11/28 15:35:31 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 8.5 KB, free 398.7 MB)&lt;br/&gt;
16/11/28 15:35:31 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 4.5 KB, free 398.7 MB)&lt;br/&gt;
16/11/28 15:35:31 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 80.80.11.97:47902 (size: 4.5 KB, free: 399.0 MB)&lt;br/&gt;
16/11/28 15:35:31 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:996&lt;br/&gt;
16/11/28 15:35:31 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 1 (MapPartitionsRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;9&amp;#93;&lt;/span&gt; at processCmd at CliDriver.java:376)&lt;br/&gt;
16/11/28 15:35:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 2 tasks&lt;br/&gt;
16/11/28 15:35:31 INFO FairSchedulableBuilder: Added task set TaskSet_1.0 tasks to pool default&lt;br/&gt;
16/11/28 15:35:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 2, localhost, executor driver, partition 0, ANY, 5984 bytes)&lt;br/&gt;
16/11/28 15:35:31 INFO TaskSetManager: Starting task 1.0 in stage 1.0 (TID 3, localhost, executor driver, partition 1, ANY, 5984 bytes)&lt;br/&gt;
16/11/28 15:35:31 INFO Executor: Running task 0.0 in stage 1.0 (TID 2)&lt;br/&gt;
16/11/28 15:35:31 INFO Executor: Running task 1.0 in stage 1.0 (TID 3)&lt;br/&gt;
....&lt;br/&gt;
16/11/28 15:35:31 INFO OrcRawRecordMerger: min key = null, max key = null&lt;br/&gt;
16/11/28 15:35:31 INFO OrcRawRecordMerger: min key = null, max key = null&lt;br/&gt;
16/11/28 15:35:31 INFO ReaderImpl: Reading ORC rows from &lt;br/&gt;
...&lt;br/&gt;
16/11/28 15:35:31 ERROR Executor: Exception in task 0.0 in stage 1.0 (TID 2)&lt;br/&gt;
java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.io.Text&lt;br/&gt;
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$23.apply(HiveInspectors.scala:529)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:435)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:232)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$2.apply(SparkPlan.scala:225)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:819)&lt;br/&gt;
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitionsInternal$1$$anonfun$apply$25.apply(RDD.scala:819)&lt;br/&gt;
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)&lt;br/&gt;
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:108)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;

&lt;p&gt;I created a separate table by querying an existing table in hive for testing purposes.&lt;br/&gt;
The table is normally queried by spark-sql.&lt;br/&gt;
The orc dump information is as follows:&lt;br/&gt;
===========================================================================================================&lt;br/&gt;
Type: struct&amp;lt;_col0:string,_col1:string,_col2:string,_col3:string,_col4:timestamp&amp;gt;&lt;/p&gt;

&lt;p&gt;Log message is below.&lt;br/&gt;
===========================================================================================================&lt;br/&gt;
16/11/28 15:37:51 INFO CatalystSqlParser: Parsing command: string&lt;br/&gt;
16/11/28 15:37:51 INFO CatalystSqlParser: Parsing command: string&lt;br/&gt;
16/11/28 15:37:51 INFO CatalystSqlParser: Parsing command: string&lt;br/&gt;
16/11/28 15:37:51 INFO CatalystSqlParser: Parsing command: string&lt;br/&gt;
16/11/28 15:37:51 INFO CatalystSqlParser: Parsing command: timestamp&lt;br/&gt;
16/11/28 15:37:51 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 258.9 KB, free 398.5 MB)&lt;br/&gt;
16/11/28 15:37:51 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 23.1 KB, free 398.4 MB)&lt;br/&gt;
16/11/28 15:37:51 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 80.80.11.97:47902 (size: 23.1 KB, free: 399.0 MB)&lt;br/&gt;
16/11/28 15:37:51 INFO SparkContext: Created broadcast 4 from processCmd at CliDriver.java:376&lt;br/&gt;
16/11/28 15:37:51 INFO PerfLogger: &amp;lt;PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl&amp;gt;&lt;br/&gt;
16/11/28 15:37:51 INFO OrcInputFormat: FooterCacheHitRatio: 0/0&lt;br/&gt;
16/11/28 15:37:51 INFO PerfLogger: &amp;lt;/PERFLOG method=OrcGetSplits start=1480315071479 end=1480315071489 duration=10 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl&amp;gt;&lt;br/&gt;
16/11/28 15:37:51 INFO SparkContext: Starting job: processCmd at CliDriver.java:376&lt;br/&gt;
16/11/28 15:37:51 INFO DAGScheduler: Got job 2 (processCmd at CliDriver.java:376) with 2 output partitions&lt;br/&gt;
16/11/28 15:37:51 INFO DAGScheduler: Final stage: ResultStage 2 (processCmd at CliDriver.java:376)&lt;br/&gt;
16/11/28 15:37:51 INFO DAGScheduler: Parents of final stage: List()&lt;br/&gt;
16/11/28 15:37:51 INFO DAGScheduler: Missing parents: List()&lt;br/&gt;
16/11/28 15:37:51 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;14&amp;#93;&lt;/span&gt; at processCmd at CliDriver.java:376), which has no missing parents&lt;br/&gt;
16/11/28 15:37:51 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 8.4 KB, free 398.4 MB)&lt;br/&gt;
16/11/28 15:37:51 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 4.5 KB, free 398.4 MB)&lt;br/&gt;
16/11/28 15:37:51 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 80.80.11.97:47902 (size: 4.5 KB, free: 398.9 MB)&lt;br/&gt;
16/11/28 15:37:51 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:996&lt;br/&gt;
16/11/28 15:37:51 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 2 (MapPartitionsRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;14&amp;#93;&lt;/span&gt; at processCmd at CliDriver.java:376)&lt;br/&gt;
16/11/28 15:37:51 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks&lt;br/&gt;
16/11/28 15:37:51 INFO FairSchedulableBuilder: Added task set TaskSet_2.0 tasks to pool default&lt;br/&gt;
16/11/28 15:37:51 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 4, localhost, executor driver, partition 0, ANY, 5941 bytes)&lt;br/&gt;
16/11/28 15:37:51 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 5, localhost, executor driver, partition 1, ANY, 5941 bytes)&lt;br/&gt;
16/11/28 15:37:51 INFO Executor: Running task 0.0 in stage 2.0 (TID 4)&lt;br/&gt;
16/11/28 15:37:51 INFO Executor: Running task 1.0 in stage 2.0 (TID 5)&lt;br/&gt;
...&lt;br/&gt;
16/11/28 15:37:51 INFO OrcRawRecordMerger: min key = null, max key = null&lt;br/&gt;
16/11/28 15:37:51 INFO ReaderImpl: Reading ORC rows from &lt;br/&gt;
...&lt;br/&gt;
16/11/28 15:37:51 INFO Executor: Finished task 0.0 in stage 2.0 (TID 4). 1563 bytes result sent to driver&lt;br/&gt;
16/11/28 15:37:51 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 4) in 39 ms on localhost (executor driver) (1/2)&lt;br/&gt;
16/11/28 15:37:51 INFO Executor: Finished task 1.0 in stage 2.0 (TID 5). 1464 bytes result sent to driver&lt;br/&gt;
16/11/28 15:37:51 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 5) in 39 ms on localhost (executor driver) (2/2)&lt;br/&gt;
16/11/28 15:37:51 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool default&lt;br/&gt;
16/11/28 15:37:51 INFO DAGScheduler: ResultStage 2 (processCmd at CliDriver.java:376) finished in 0.044 s&lt;br/&gt;
16/11/28 15:37:51 INFO DAGScheduler: Job 2 finished: processCmd at CliDriver.java:376, took 0.057510 s&lt;/p&gt;

&lt;p&gt;The only difference between the two tables in my opinion is a struct.&lt;br/&gt;
Is there anything else I should check?&lt;/p&gt;</comment>
                            <comment id="15702009" author="cloud_fan" created="Mon, 28 Nov 2016 13:58:04 +0000"  >&lt;p&gt;&amp;gt; I created a separate table by querying an existing table in hive for testing purposes.&lt;/p&gt;

&lt;p&gt;can you provide the SQL statement that created this table? and it would be good if you can run `DESC TABLE` for the hive table and post the results. Thanks!&lt;/p&gt;</comment>
                            <comment id="15702104" author="jerryjung" created="Mon, 28 Nov 2016 14:38:19 +0000"  >&lt;p&gt;CREATE EXTERNAL TABLE `d_c`.`dcoc_ircs_op_brch`(`ircs_op_brch_cd` string, `ircs_op_brch_nm` string, `cms_brch_cd` string, `cms_brch_nm` string, `etl_job_dtm` timestamp)&lt;br/&gt;
ROW FORMAT SERDE &apos;org.apache.hadoop.hive.ql.io.orc.OrcSerde&apos;&lt;br/&gt;
WITH SERDEPROPERTIES (&lt;br/&gt;
  &apos;serialization.format&apos; = &apos;1&apos;&lt;br/&gt;
)&lt;br/&gt;
STORED AS&lt;br/&gt;
  INPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&apos;&lt;br/&gt;
  OUTPUTFORMAT &apos;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&apos;&lt;br/&gt;
LOCATION &apos;hdfs://xxx:8020/xxx&apos;&lt;br/&gt;
TBLPROPERTIES (&lt;br/&gt;
  &apos;rawDataSize&apos; = &apos;2426&apos;,&lt;br/&gt;
  &apos;numFiles&apos; = &apos;0&apos;,&lt;br/&gt;
  &apos;transient_lastDdlTime&apos; = &apos;1480313167&apos;,&lt;br/&gt;
  &apos;totalSize&apos; = &apos;0&apos;,&lt;br/&gt;
  &apos;COLUMN_STATS_ACCURATE&apos; = &apos;true&apos;,&lt;br/&gt;
  &apos;numRows&apos; = &apos;6&apos;&lt;br/&gt;
)&lt;/p&gt;
</comment>
                            <comment id="15703848" author="cloud_fan" created="Tue, 29 Nov 2016 01:49:40 +0000"  >&lt;p&gt;The orc file is written by Hive, not by Spark SQL, can you use `CREATE TABLE ... AS SELECT ... FROM hive_table` to make Spark SQL to write out the orc file and try again?&lt;/p&gt;</comment>
                            <comment id="15703853" author="cloud_fan" created="Tue, 29 Nov 2016 01:51:59 +0000"  >&lt;p&gt;BTW, the table created by Spark 1.X, are you able to read it with Spark 1.X?&lt;/p&gt;</comment>
                            <comment id="15703897" author="jerryjung" created="Tue, 29 Nov 2016 02:12:05 +0000"  >&lt;p&gt;Same error occurred!&lt;br/&gt;
spark-sql&amp;gt; CREATE  TABLE zz  as select * from d_c.dcoc_ircs_op_brch;&lt;br/&gt;
16/11/29 11:09:28 INFO SparkSqlParser: Parsing command: CREATE  TABLE zz  as select * from d_c.dcoc_ircs_op_brch&lt;br/&gt;
16/11/29 11:09:28 INFO HiveMetaStore: 0: get_database: d_c&lt;br/&gt;
16/11/29 11:09:28 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_database: d_c&lt;br/&gt;
16/11/29 11:09:28 INFO HiveMetaStore: 0: get_table : db=d_c tbl=dcoc_ircs_op_brch&lt;br/&gt;
16/11/29 11:09:28 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=d_c tbl=dcoc_ircs_op_brch&lt;br/&gt;
16/11/29 11:09:28 INFO HiveMetaStore: 0: get_table : db=d_c tbl=dcoc_ircs_op_brch&lt;br/&gt;
16/11/29 11:09:28 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=d_c tbl=dcoc_ircs_op_brch&lt;br/&gt;
16/11/29 11:09:28 INFO CatalystSqlParser: Parsing command: varchar(6)&lt;br/&gt;
16/11/29 11:09:28 INFO CatalystSqlParser: Parsing command: varchar(50)&lt;br/&gt;
16/11/29 11:09:28 INFO CatalystSqlParser: Parsing command: varchar(4)&lt;br/&gt;
16/11/29 11:09:28 INFO CatalystSqlParser: Parsing command: varchar(50)&lt;br/&gt;
16/11/29 11:09:28 INFO CatalystSqlParser: Parsing command: timestamp&lt;br/&gt;
16/11/29 11:09:30 INFO HiveMetaStore: 0: get_table : db=default tbl=zz&lt;br/&gt;
16/11/29 11:09:30 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=default tbl=zz&lt;br/&gt;
16/11/29 11:09:30 INFO HiveMetaStore: 0: get_database: default&lt;br/&gt;
16/11/29 11:09:30 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_database: default&lt;br/&gt;
16/11/29 11:09:30 INFO HiveMetaStore: 0: get_database: default&lt;br/&gt;
16/11/29 11:09:30 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_database: default&lt;br/&gt;
16/11/29 11:09:30 INFO HiveMetaStore: 0: get_table : db=default tbl=zz&lt;br/&gt;
16/11/29 11:09:30 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=default tbl=zz&lt;br/&gt;
16/11/29 11:09:30 INFO HiveMetaStore: 0: get_database: default&lt;br/&gt;
16/11/29 11:09:30 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_database: default&lt;br/&gt;
16/11/29 11:09:30 INFO HiveMetaStore: 0: create_table: Table(tableName:zz, dbName:default, owner:hadoop, createTime:1480385368, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:&lt;span class=&quot;error&quot;&gt;&amp;#91;FieldSchema(name:ircs_op_brch_cd, type:string, comment:null), FieldSchema(name:ircs_op_brch_nm, type:string, comment:null), FieldSchema(name:cms_brch_cd, type:string, comment:null), FieldSchema(name:cms_brch_nm, type:string, comment:null), FieldSchema(name:etl_job_dtm, type:timestamp, comment:null)&amp;#93;&lt;/span&gt;, location:hdfs://xxx/user/hive/warehouse/zz, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:&lt;/p&gt;
{serialization.format=1}
&lt;p&gt;), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{spark.sql.sources.schema.part.0={&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:&lt;span class=&quot;error&quot;&gt;&amp;#91;{&amp;quot;name&amp;quot;:&amp;quot;ircs_op_brch_cd&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;nullable&amp;quot;:true,&amp;quot;metadata&amp;quot;:{}},{&amp;quot;name&amp;quot;:&amp;quot;ircs_op_brch_nm&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;nullable&amp;quot;:true,&amp;quot;metadata&amp;quot;:{}},{&amp;quot;name&amp;quot;:&amp;quot;cms_brch_cd&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;nullable&amp;quot;:true,&amp;quot;metadata&amp;quot;:{}},{&amp;quot;name&amp;quot;:&amp;quot;cms_brch_nm&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;nullable&amp;quot;:true,&amp;quot;metadata&amp;quot;:{}},{&amp;quot;name&amp;quot;:&amp;quot;etl_job_dtm&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;timestamp&amp;quot;,&amp;quot;nullable&amp;quot;:true,&amp;quot;metadata&amp;quot;:{}}&amp;#93;&lt;/span&gt;}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=hive}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))&lt;br/&gt;
... parameters:{spark.sql.sources.schema.part.0={&quot;type&quot;:&quot;struct&quot;,&quot;fields&quot;:&lt;span class=&quot;error&quot;&gt;&amp;#91;{&amp;quot;name&amp;quot;:&amp;quot;ircs_op_brch_cd&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;nullable&amp;quot;:true,&amp;quot;metadata&amp;quot;:{}},{&amp;quot;name&amp;quot;:&amp;quot;ircs_op_brch_nm&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;nullable&amp;quot;:true,&amp;quot;metadata&amp;quot;:{}},{&amp;quot;name&amp;quot;:&amp;quot;cms_brch_cd&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;nullable&amp;quot;:true,&amp;quot;metadata&amp;quot;:{}},{&amp;quot;name&amp;quot;:&amp;quot;cms_brch_nm&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;string&amp;quot;,&amp;quot;nullable&amp;quot;:true,&amp;quot;metadata&amp;quot;:{}},{&amp;quot;name&amp;quot;:&amp;quot;etl_job_dtm&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;timestamp&amp;quot;,&amp;quot;nullable&amp;quot;:true,&amp;quot;metadata&amp;quot;:{}}&amp;#93;&lt;/span&gt;}, spark.sql.sources.schema.numParts=1, spark.sql.sources.provider=hive}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED_TABLE, privileges:PrincipalPrivilegeSet(userPrivileges:{}, groupPrivileges:null, rolePrivileges:null))&lt;br/&gt;
...&lt;br/&gt;
16/11/29 11:09:30 INFO FileUtils: Creating directory if it doesn&apos;t exist: hdfs://xxx/user/hive/warehouse/zz&lt;br/&gt;
16/11/29 11:09:31 INFO HiveMetaStore: 0: get_table : db=default tbl=zz&lt;br/&gt;
16/11/29 11:09:31 INFO audit: ugi=hadoop	ip=unknown-ip-addr	cmd=get_table : db=default tbl=zz&lt;br/&gt;
16/11/29 11:09:31 INFO CatalystSqlParser: Parsing command: string&lt;br/&gt;
16/11/29 11:09:31 INFO CatalystSqlParser: Parsing command: string&lt;br/&gt;
16/11/29 11:09:31 INFO CatalystSqlParser: Parsing command: string&lt;br/&gt;
16/11/29 11:09:31 INFO CatalystSqlParser: Parsing command: string&lt;br/&gt;
16/11/29 11:09:31 INFO CatalystSqlParser: Parsing command: timestamp&lt;br/&gt;
16/11/29 11:09:31 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 258.9 KB, free 398.7 MB)&lt;br/&gt;
16/11/29 11:09:31 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.1 KB, free 398.7 MB)&lt;br/&gt;
16/11/29 11:09:31 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 80.80.11.97:39889 (size: 23.1 KB, free: 399.0 MB)&lt;br/&gt;
16/11/29 11:09:31 INFO SparkContext: Created broadcast 0 from processCmd at CliDriver.java:376&lt;br/&gt;
16/11/29 11:09:31 INFO FileUtils: Creating directory if it doesn&apos;t exist: hdfs://xxx/user/hive/warehouse/zz/.hive-staging_hive_2016-11-29_11-09-31_532_3872774957419759303-1&lt;br/&gt;
16/11/29 11:09:31 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id&lt;br/&gt;
16/11/29 11:09:31 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id&lt;br/&gt;
16/11/29 11:09:31 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id&lt;br/&gt;
16/11/29 11:09:31 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap&lt;br/&gt;
16/11/29 11:09:31 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition&lt;br/&gt;
16/11/29 11:09:31 INFO PerfLogger: &amp;lt;PERFLOG method=OrcGetSplits from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl&amp;gt;&lt;br/&gt;
16/11/29 11:09:31 INFO deprecation: mapred.input.dir is deprecated. Instead, use mapreduce.input.fileinputformat.inputdir&lt;br/&gt;
16/11/29 11:09:31 INFO OrcInputFormat: FooterCacheHitRatio: 0/0&lt;br/&gt;
16/11/29 11:09:31 INFO PerfLogger: &amp;lt;/PERFLOG method=OrcGetSplits start=1480385371818 end=1480385371865 duration=47 from=org.apache.hadoop.hive.ql.io.orc.ReaderImpl&amp;gt;&lt;br/&gt;
16/11/29 11:09:31 INFO SparkContext: Starting job: processCmd at CliDriver.java:376&lt;br/&gt;
16/11/29 11:09:31 INFO DAGScheduler: Got job 0 (processCmd at CliDriver.java:376) with 2 output partitions&lt;br/&gt;
16/11/29 11:09:31 INFO DAGScheduler: Final stage: ResultStage 0 (processCmd at CliDriver.java:376)&lt;br/&gt;
16/11/29 11:09:31 INFO DAGScheduler: Parents of final stage: List()&lt;br/&gt;
16/11/29 11:09:31 INFO DAGScheduler: Missing parents: List()&lt;br/&gt;
16/11/29 11:09:31 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; at processCmd at CliDriver.java:376), which has no missing parents&lt;br/&gt;
16/11/29 11:09:32 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 75.8 KB, free 398.7 MB)&lt;br/&gt;
16/11/29 11:09:32 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 29.7 KB, free 398.6 MB)&lt;br/&gt;
16/11/29 11:09:32 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 80.80.11.97:39889 (size: 29.7 KB, free: 398.9 MB)&lt;br/&gt;
16/11/29 11:09:32 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:996&lt;br/&gt;
16/11/29 11:09:32 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;3&amp;#93;&lt;/span&gt; at processCmd at CliDriver.java:376)&lt;br/&gt;
16/11/29 11:09:32 INFO TaskSchedulerImpl: Adding task set 0.0 with 2 tasks&lt;br/&gt;
16/11/29 11:09:32 INFO FairSchedulableBuilder: Added task set TaskSet_0.0 tasks to pool default&lt;br/&gt;
16/11/29 11:09:32 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, executor driver, partition 0, ANY, 5987 bytes)&lt;br/&gt;
16/11/29 11:09:32 INFO TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, executor driver, partition 1, ANY, 5987 bytes)&lt;br/&gt;
16/11/29 11:09:32 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)&lt;br/&gt;
16/11/29 11:09:32 INFO Executor: Running task 1.0 in stage 0.0 (TID 1)&lt;br/&gt;
16/11/29 11:09:32 INFO HadoopRDD: Input split: hdfs://xxx/edw/warehouse/dw/db=d_c/tb=dcoc_ircs_op_brch/part-00000:0+815&lt;br/&gt;
16/11/29 11:09:32 INFO HadoopRDD: Input split: hdfs://xxx/edw/warehouse/dw/db=d_c/tb=dcoc_ircs_op_brch/part-00001:0+749&lt;br/&gt;
16/11/29 11:09:32 INFO OrcRawRecordMerger: min key = null, max key = null&lt;br/&gt;
16/11/29 11:09:32 INFO OrcRawRecordMerger: min key = null, max key = null&lt;br/&gt;
16/11/29 11:09:32 INFO ReaderImpl: Reading ORC rows from hdfs://xxx/edw/warehouse/dw/db=d_c/tb=dcoc_ircs_op_brch/part-00001 with &lt;/p&gt;
{include: [true, true, true, true, true, true], offset: 0, length: 9223372036854775807}
&lt;p&gt;16/11/29 11:09:32 INFO ReaderImpl: Reading ORC rows from hdfs://xxx/edw/warehouse/dw/db=d_c/tb=dcoc_ircs_op_brch/part-00000 with &lt;/p&gt;
{include: [true, true, true, true, true, true], offset: 0, length: 9223372036854775807}
&lt;p&gt;16/11/29 11:09:32 INFO CodeGenerator: Code generated in 337.683679 ms&lt;br/&gt;
16/11/29 11:09:32 ERROR Executor: Exception in task 1.0 in stage 0.0 (TID 1)&lt;br/&gt;
java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.io.Text&lt;br/&gt;
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$23.apply(HiveInspectors.scala:529)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:435)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)&lt;br/&gt;
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&lt;br/&gt;
	at org.apache.spark.sql.hive.SparkHiveWriterContainer.writeToFile(hiveWriterContainers.scala:185)&lt;br/&gt;
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:108)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
16/11/29 11:09:32 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)&lt;br/&gt;
java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.io.Text&lt;br/&gt;
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$23.apply(HiveInspectors.scala:529)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:435)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)&lt;br/&gt;
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&lt;br/&gt;
	at org.apache.spark.sql.hive.SparkHiveWriterContainer.writeToFile(hiveWriterContainers.scala:185)&lt;br/&gt;
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:108)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;br/&gt;
16/11/29 11:09:32 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost, executor driver): java.lang.ClassCastException: org.apache.hadoop.hive.serde2.io.HiveVarcharWritable cannot be cast to org.apache.hadoop.io.Text&lt;br/&gt;
	at org.apache.hadoop.hive.serde2.objectinspector.primitive.WritableStringObjectInspector.getPrimitiveWritableObject(WritableStringObjectInspector.java:41)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$unwrapperFor$23.apply(HiveInspectors.scala:529)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$14$$anonfun$apply$15.apply(TableReader.scala:419)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:435)&lt;br/&gt;
	at org.apache.spark.sql.hive.HadoopTableReader$$anonfun$fillObject$2.apply(TableReader.scala:426)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
	at scala.collection.Iterator$class.foreach(Iterator.scala:893)&lt;br/&gt;
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)&lt;br/&gt;
	at org.apache.spark.sql.hive.SparkHiveWriterContainer.writeToFile(hiveWriterContainers.scala:185)&lt;br/&gt;
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:151)&lt;br/&gt;
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)&lt;br/&gt;
	at org.apache.spark.scheduler.Task.run(Task.scala:108)&lt;br/&gt;
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:745)&lt;/p&gt;</comment>
                            <comment id="15703909" author="jerryjung" created="Tue, 29 Nov 2016 02:18:06 +0000"  >&lt;p&gt;Sure, It was working in Spark 1.X.&lt;br/&gt;
Even the 2.0.3 version works fine.&lt;br/&gt;
As mentioned above, errors only occur in version 2.1.0.&lt;/p&gt;</comment>
                            <comment id="15705327" author="apachespark" created="Tue, 29 Nov 2016 13:42:06 +0000"  >&lt;p&gt;User &apos;cloud-fan&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16060&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16060&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 51 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i35pjz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12335644">2.1.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>