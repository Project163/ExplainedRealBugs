<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:32:17 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-10554] Potential NPE with ShutdownHook</title>
                <link>https://issues.apache.org/jira/browse/SPARK-10554</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Originally posted in user mailing list &lt;a href=&quot;http://apache-spark-user-list.1001560.n3.nabble.com/Potential-NPE-while-exiting-spark-shell-tt24523.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I&apos;m currently using Spark 1.3.0 on yarn cluster deployed through CDH5.4. My cluster does not have a &apos;default&apos; queue, and launching &apos;spark-shell&apos; submits an yarn application that gets killed immediately because queue does not exist. However, the spark-shell session is still in progress after throwing a bunch of errors while creating sql context. Upon submitting an &apos;exit&apos; command, there appears to be a NPE from DiskBlockManager with the following stack trace &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;ERROR Utils: Uncaught exception in thread delete Spark local dirs 
java.lang.NullPointerException 
        at org.apache.spark.storage.DiskBlockManager.org$apache$spark$storage$DiskBlockManager$$doStop(DiskBlockManager.scala:161) 
        at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply$mcV$sp(DiskBlockManager.scala:141) 
        at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply(DiskBlockManager.scala:139) 
        at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply(DiskBlockManager.scala:139) 
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617) 
        at org.apache.spark.storage.DiskBlockManager$$anon$1.run(DiskBlockManager.scala:139) 
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;delete Spark local dirs&quot;&lt;/span&gt; java.lang.NullPointerException 
        at org.apache.spark.storage.DiskBlockManager.org$apache$spark$storage$DiskBlockManager$$doStop(DiskBlockManager.scala:161) 
        at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply$mcV$sp(DiskBlockManager.scala:141) 
        at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply(DiskBlockManager.scala:139) 
        at org.apache.spark.storage.DiskBlockManager$$anon$1$$anonfun$run$1.apply(DiskBlockManager.scala:139) 
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1617) 
        at org.apache.spark.storage.DiskBlockManager$$anon$1.run(DiskBlockManager.scala:139) 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I believe the problem appears to be surfacing from a shutdown hook that&apos;s tries to cleanup local directories. In this specific case because the yarn application was not submitted successfully, the block manager was not registered; as a result it does not have a valid blockManagerId as seen here &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/v1.3.0/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala#L161&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/v1.3.0/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala#L161&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Has anyone faced this issue before? Could this be a problem with the way shutdown hook behaves currently? &lt;/p&gt;

&lt;p&gt;Note: I referenced source from apache spark repo than cloudera.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12863220">SPARK-10554</key>
            <summary>Potential NPE with ShutdownHook</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nithinasokan">Nithin Asokan</assignee>
                                    <reporter username="nithinasokan">Nithin Asokan</reporter>
                        <labels>
                    </labels>
                <created>Thu, 10 Sep 2015 22:10:00 +0000</created>
                <updated>Sun, 17 May 2020 18:21:06 +0000</updated>
                            <resolved>Sat, 12 Sep 2015 08:51:15 +0000</resolved>
                                    <version>1.5.0</version>
                                    <fixVersion>1.5.1</fixVersion>
                    <fixVersion>1.6.0</fixVersion>
                                    <component>Block Manager</component>
                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="14739714" author="nithinasokan" created="Thu, 10 Sep 2015 22:12:59 +0000"  >&lt;p&gt;The fix could be as easy as checking &lt;tt&gt;blockManagerId&lt;/tt&gt; is null &lt;a href=&quot;https://github.com/apache/spark/blob/v1.3.0/core/src/main/scala/org/apache/spark/storage/DiskBlockManager.scala#L161&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt; and moving forward with cleaning up the folders. But I&apos;m not sure if there could be other areas that I&apos;m missing. &lt;/p&gt;</comment>
                            <comment id="14740470" author="srowen" created="Fri, 11 Sep 2015 09:30:07 +0000"  >&lt;p&gt;Yes looks like the same in &lt;tt&gt;master&lt;/tt&gt; as well, and that&apos;s likely the cause. You&apos;re welcome to open a PR, or I can later today.&lt;/p&gt;

&lt;p&gt;(See &lt;a href=&quot;https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark&lt;/a&gt; for how to fill out the JIRA fields too)&lt;/p&gt;</comment>
                            <comment id="14740883" author="nithinasokan" created="Fri, 11 Sep 2015 14:18:03 +0000"  >&lt;p&gt;I took a closer look at the logs, and I think my initial suggestion may not work well. With a null check we can get rid of the NPE, if &lt;tt&gt;blockManagerId&lt;/tt&gt; is null I think we will not get to the block of code that delete folders, as a result we may leave some orphan folders.&lt;/p&gt;

&lt;p&gt;Here are some logs that I noticed when spark-shell starts&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;15/09/11 09:04:01 INFO DiskBlockManager: Created local directory at /tmp/spark-886a9094-a496-409c-9d20-4667e768a05c/blockmgr-9e87c7d5-8614-470a-8800-9b335f305cef
15/09/11 09:04:01 INFO MemoryStore: MemoryStore started with capacity 265.1 MB
15/09/11 09:04:01 INFO HttpFileServer: HTTP File server directory is /tmp/spark-ee20d914-ba59-4d7c-a93f-31786f349f82/httpd-a3831baf-5a71-4693-b94f-38de2c1c3b61
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think we probably need to cleanup these orphan folders; I&apos;m fairly new to spark and scala, so please suggest a possible approach for this? Is &lt;tt&gt;blockManager.blockManagerId.isDriver&lt;/tt&gt; really needed? Can we assume that we need to delete folders anytime the shutdown hook is invoked? &lt;/p&gt;</comment>
                            <comment id="14740904" author="srowen" created="Fri, 11 Sep 2015 14:29:51 +0000"  >&lt;p&gt;Well the situation here is that the block manager fails to initialize, and is stopped very early. Many bets are off. Since the dir may be shared I think the conservative thing is to not delete files.&lt;/p&gt;

&lt;p&gt;Yes the condition is needed since the driver will always clean up its local files as it&apos;s not participating in things like the external shuffle service. But if we don&apos;t know we&apos;re the driver for some reason, I think it&apos;s best to not proceed. Anyway, that&apos;s the current behavior!&lt;/p&gt;</comment>
                            <comment id="14741229" author="apachespark" created="Fri, 11 Sep 2015 17:50:42 +0000"  >&lt;p&gt;User &apos;nasokan&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/8720&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/8720&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14741979" author="srowen" created="Sat, 12 Sep 2015 08:51:15 +0000"  >&lt;p&gt;Issue resolved by pull request 8720&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/8720&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/8720&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 10 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2k0wf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>