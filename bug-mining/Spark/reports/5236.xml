<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:56:32 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-22249] UnsupportedOperationException: empty.reduceLeft when caching a dataframe</title>
                <link>https://issues.apache.org/jira/browse/SPARK-22249</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;It seems that the &lt;tt&gt;isin()&lt;/tt&gt; method with an empty list as argument only works, if the dataframe is not cached. If it is cached, it results in an exception. To reproduce&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$ pyspark
&amp;gt;&amp;gt;&amp;gt; df = spark.createDataFrame([pyspark.Row(KEY=&lt;span class=&quot;code-quote&quot;&gt;&quot;value&quot;&lt;/span&gt;)])
&amp;gt;&amp;gt;&amp;gt; df.where(df[&lt;span class=&quot;code-quote&quot;&gt;&quot;KEY&quot;&lt;/span&gt;].isin([])).show()
+---+
|KEY|
+---+
+---+

&amp;gt;&amp;gt;&amp;gt; df.cache()
DataFrame[KEY: string]
&amp;gt;&amp;gt;&amp;gt; df.where(df[&lt;span class=&quot;code-quote&quot;&gt;&quot;KEY&quot;&lt;/span&gt;].isin([])).show()
Traceback (most recent call last):
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;&amp;lt;stdin&amp;gt;&quot;&lt;/span&gt;, line 1, in &amp;lt;module&amp;gt;
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/anaconda3/envs/&amp;lt;myenv&amp;gt;/lib/python3.6/site-packages/pyspark/sql/dataframe.py&quot;&lt;/span&gt;, line 336, in show
    print(self._jdf.showString(n, 20))
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/anaconda3/envs/&amp;lt;myenv&amp;gt;/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/java_gateway.py&quot;&lt;/span&gt;, line 1133, in __call__
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/anaconda3/envs/&amp;lt;myenv&amp;gt;/lib/python3.6/site-packages/pyspark/sql/utils.py&quot;&lt;/span&gt;, line 63, in deco
    &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; f(*a, **kw)
  File &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/anaconda3/envs/&amp;lt;myenv&amp;gt;/lib/python3.6/site-packages/pyspark/python/lib/py4j-0.10.4-src.zip/py4j/protocol.py&quot;&lt;/span&gt;, line 319, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; calling o302.showString.
: java.lang.UnsupportedOperationException: empty.reduceLeft
	at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;reduceLeft(TraversableOnce.scala:180)
	at scala.collection.mutable.ArrayBuffer.scala$collection$IndexedSeqOptimized$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$reduceLeft(ArrayBuffer.scala:48)
	at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;reduceLeft(IndexedSeqOptimized.scala:74)
	at scala.collection.mutable.ArrayBuffer.reduceLeft(ArrayBuffer.scala:48)
	at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;reduce(TraversableOnce.scala:208)
	at scala.collection.AbstractTraversable.reduce(Traversable.scala:104)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$1.applyOrElse(InMemoryTableScanExec.scala:107)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$1.applyOrElse(InMemoryTableScanExec.scala:71)
	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:223)
	at scala.PartialFunction$Lifted.apply(PartialFunction.scala:219)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$2.apply(InMemoryTableScanExec.scala:112)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec$$anonfun$2.apply(InMemoryTableScanExec.scala:111)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)
	at scala.collection.immutable.List.foreach(List.scala:381)
	at scala.collection.TraversableLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;flatMap(TraversableLike.scala:241)
	at scala.collection.immutable.List.flatMap(List.scala:344)
	at org.apache.spark.sql.execution.columnar.InMemoryTableScanExec.&amp;lt;init&amp;gt;(InMemoryTableScanExec.scala:111)
	at org.apache.spark.sql.execution.SparkStrategies$InMemoryScans$$anonfun$3.apply(SparkStrategies.scala:307)
	at org.apache.spark.sql.execution.SparkStrategies$InMemoryScans$$anonfun$3.apply(SparkStrategies.scala:307)
	at org.apache.spark.sql.execution.SparkPlanner.pruneFilterProject(SparkPlanner.scala:99)
	at org.apache.spark.sql.execution.SparkStrategies$InMemoryScans$.apply(SparkStrategies.scala:303)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$1.apply(QueryPlanner.scala:62)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:439)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:77)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2$$anonfun$apply$2.apply(QueryPlanner.scala:74)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(Iterator.scala:893)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1336)
	at scala.collection.TraversableOnce$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractIterator.foldLeft(Iterator.scala:1336)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:74)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner$$anonfun$2.apply(QueryPlanner.scala:66)
	at scala.collection.Iterator$$anon$12.nextCur(Iterator.scala:434)
	at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:440)
	at org.apache.spark.sql.catalyst.planning.QueryPlanner.plan(QueryPlanner.scala:92)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan$lzycompute(QueryExecution.scala:84)
	at org.apache.spark.sql.execution.QueryExecution.sparkPlan(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:89)
	at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:89)
	at org.apache.spark.sql.Dataset.withAction(Dataset.scala:2832)
	at org.apache.spark.sql.Dataset.head(Dataset.scala:2153)
	at org.apache.spark.sql.Dataset.take(Dataset.scala:2366)
	at org.apache.spark.sql.Dataset.showString(Dataset.scala:245)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:280)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:214)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment>&lt;p&gt;$ uname -a&lt;br/&gt;
Darwin MAC-UM-024.local 16.7.0 Darwin Kernel Version 16.7.0: Thu Jun 15 17:36:27 PDT 2017; root:xnu-3789.70.16~2/RELEASE_X86_64 x86_64&lt;br/&gt;
$ pyspark --version&lt;br/&gt;
Welcome to&lt;br/&gt;
      ____              __&lt;br/&gt;
     / _&lt;em&gt;/&lt;/em&gt;_  ___ ____&lt;em&gt;/ /&lt;/em&gt;_&lt;br/&gt;
    &lt;em&gt;\ \/ _ \/ _ `/ __/  &apos;&lt;/em&gt;/&lt;br/&gt;
   /__&lt;em&gt;/ .&lt;/em&gt;&lt;em&gt;/&amp;#95;,&lt;/em&gt;/&lt;em&gt;/ /&lt;/em&gt;/&amp;#95;\   version 2.2.0&lt;br/&gt;
      /_/&lt;/p&gt;

&lt;p&gt;Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_92&lt;br/&gt;
Branch &lt;br/&gt;
Compiled by user jenkins on 2017-06-30T22:58:04Z&lt;br/&gt;
Revision &lt;br/&gt;
Url &lt;/p&gt;</environment>
        <key id="13108572">SPARK-22249</key>
            <summary>UnsupportedOperationException: empty.reduceLeft when caching a dataframe</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mgaido">Marco Gaido</assignee>
                                    <reporter username="asmaier">Andreas Maier</reporter>
                        <labels>
                    </labels>
                <created>Wed, 11 Oct 2017 11:59:22 +0000</created>
                <updated>Mon, 12 Dec 2022 18:10:34 +0000</updated>
                            <resolved>Tue, 17 Oct 2017 07:44:09 +0000</resolved>
                                    <version>2.1.1</version>
                    <version>2.2.0</version>
                                    <fixVersion>2.2.1</fixVersion>
                    <fixVersion>2.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="16203751" author="apachespark" created="Fri, 13 Oct 2017 16:01:09 +0000"  >&lt;p&gt;User &apos;mgaido91&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19494&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19494&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16205434" author="gurwls223" created="Mon, 16 Oct 2017 05:04:28 +0000"  >&lt;p&gt;Looks a duplicate of &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-19317&quot; title=&quot;UnsupportedOperationException: empty.reduceLeft in LinearSeqOptimized&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-19317&quot;&gt;&lt;del&gt;SPARK-19317&lt;/del&gt;&lt;/a&gt; BTW. Both look failed by the same reason:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/v2.1.0/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala#L90&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/v2.1.0/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala#L90&lt;/a&gt; (2.1.0)&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/blob/v2.2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala#L107&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/v2.2.0/sql/core/src/main/scala/org/apache/spark/sql/execution/columnar/InMemoryTableScanExec.scala#L107&lt;/a&gt; (2.2.0)&lt;/p&gt;


</comment>
                            <comment id="16207131" author="srowen" created="Tue, 17 Oct 2017 07:44:09 +0000"  >&lt;p&gt;Resolved by &lt;a href=&quot;https://github.com/apache/spark/pull/19494&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19494&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16208462" author="apachespark" created="Tue, 17 Oct 2017 21:48:04 +0000"  >&lt;p&gt;User &apos;mgaido91&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19522&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19522&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16216634" author="asmaier" created="Tue, 24 Oct 2017 10:00:00 +0000"  >&lt;p&gt;Thank you for solving this issue so quickly. Not every open source project is reacting so fast. &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13036737">SPARK-19317</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 4 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3l4pb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>