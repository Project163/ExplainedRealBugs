<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:31:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-9960] run-example SparkPi fails on Mac</title>
                <link>https://issues.apache.org/jira/browse/SPARK-9960</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description></description>
                <environment>&lt;p&gt;java version &quot;1.7.0_71&quot;, Mac OS X 10.9.5&lt;/p&gt;</environment>
        <key id="12856038">SPARK-9960</key>
            <summary>run-example SparkPi fails on Mac</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="8">Not A Problem</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="nvspark">Naga</reporter>
                        <labels>
                            <label>run-example</label>
                    </labels>
                <created>Fri, 14 Aug 2015 03:34:26 +0000</created>
                <updated>Fri, 14 Aug 2015 06:18:02 +0000</updated>
                            <resolved>Fri, 14 Aug 2015 04:01:29 +0000</resolved>
                                    <version>1.4.1</version>
                                                    <component>Examples</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="14696419" author="nvspark" created="Fri, 14 Aug 2015 03:45:50 +0000"  >&lt;p&gt;Here&apos;s runtime output (includes stack trace) ...&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;~/spark-1.4.1-bin-without-hadoop $ bin/run-example SparkPi --driver-library-path &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt; 
15/08/13 20:42:05 INFO spark.SparkContext: Running Spark version 1.4.1
15/08/13 20:42:06 WARN util.NativeCodeLoader: Unable to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your platform... using builtin-java classes where applicable
15/08/13 20:42:06 INFO spark.&lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: Changing view acls to: nv
15/08/13 20:42:06 INFO spark.&lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: Changing modify acls to: nv
15/08/13 20:42:06 INFO spark.&lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: &lt;span class=&quot;code-object&quot;&gt;SecurityManager&lt;/span&gt;: authentication disabled; ui acls disabled; users with view permissions: Set(nv); users with modify permissions: Set(nv)
15/08/13 20:42:06 INFO slf4j.Slf4jLogger: Slf4jLogger started
15/08/13 20:42:06 INFO Remoting: Starting remoting
15/08/13 20:42:06 INFO Remoting: Remoting started; listening on addresses :[akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//sparkDriver@10.0.0.6:62576]
&lt;/span&gt;15/08/13 20:42:06 INFO util.Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;sparkDriver&apos;&lt;/span&gt; on port 62576.
15/08/13 20:42:06 INFO spark.SparkEnv: Registering MapOutputTracker
15/08/13 20:42:06 INFO spark.SparkEnv: Registering BlockManagerMaster
15/08/13 20:42:07 INFO storage.DiskBlockManager: Created local directory at /&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/0j/bkhg_dw17w96qxddkmryz63r0000gn/T/spark-c2b2b947-327a-4e54-aa3b-ae11526aebde/blockmgr-37bc3de6-86da-491a-9a39-a9f44c3a0919
15/08/13 20:42:07 INFO storage.MemoryStore: MemoryStore started with capacity 265.4 MB
15/08/13 20:42:07 INFO spark.HttpFileServer: HTTP File server directory is /&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/0j/bkhg_dw17w96qxddkmryz63r0000gn/T/spark-c2b2b947-327a-4e54-aa3b-ae11526aebde/httpd-775eeb2e-20ac-4dd4-92db-e94bd053ebf3
15/08/13 20:42:07 INFO spark.HttpServer: Starting HTTP Server
15/08/13 20:42:07 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/08/13 20:42:07 INFO server.AbstractConnector: Started SocketConnector@0.0.0.0:62577
15/08/13 20:42:07 INFO util.Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;HTTP file server&apos;&lt;/span&gt; on port 62577.
15/08/13 20:42:07 INFO spark.SparkEnv: Registering OutputCommitCoordinator
15/08/13 20:42:07 INFO server.Server: jetty-8.y.z-SNAPSHOT
15/08/13 20:42:07 INFO server.AbstractConnector: Started SelectChannelConnector@0.0.0.0:4040
15/08/13 20:42:07 INFO util.Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;SparkUI&apos;&lt;/span&gt; on port 4040.
15/08/13 20:42:07 INFO ui.SparkUI: Started SparkUI at http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.0.6:4040
&lt;/span&gt;15/08/13 20:42:07 INFO spark.SparkContext: Added JAR file:/Users/nv/spark-1.4.1-bin-without-hadoop/lib/spark-examples-1.4.1-hadoop2.2.0.jar at http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.0.6:62577/jars/spark-examples-1.4.1-hadoop2.2.0.jar with timestamp 1439523727458
&lt;/span&gt;15/08/13 20:42:07 INFO executor.Executor: Starting executor ID driver on host localhost
15/08/13 20:42:07 INFO util.Utils: Successfully started service &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.spark.network.netty.NettyBlockTransferService&apos;&lt;/span&gt; on port 62578.
15/08/13 20:42:07 INFO netty.NettyBlockTransferService: Server created on 62578
15/08/13 20:42:07 INFO storage.BlockManagerMaster: Trying to register BlockManager
15/08/13 20:42:07 INFO storage.BlockManagerMasterEndpoint: Registering block manager localhost:62578 with 265.4 MB RAM, BlockManagerId(driver, localhost, 62578)
15/08/13 20:42:07 INFO storage.BlockManagerMaster: Registered BlockManager
15/08/13 20:42:07 INFO spark.SparkContext: Starting job: reduce at SparkPi.scala:35
15/08/13 20:42:08 INFO scheduler.DAGScheduler: Got job 0 (reduce at SparkPi.scala:35) with 2 output partitions (allowLocal=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
15/08/13 20:42:08 INFO scheduler.DAGScheduler: Final stage: ResultStage 0(reduce at SparkPi.scala:35)
15/08/13 20:42:08 INFO scheduler.DAGScheduler: Parents of &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; stage: List()
15/08/13 20:42:08 INFO scheduler.DAGScheduler: Missing parents: List()
15/08/13 20:42:08 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:31), which has no missing parents
15/08/13 20:42:08 INFO storage.MemoryStore: ensureFreeSpace(1888) called with curMem=0, maxMem=278302556
15/08/13 20:42:08 INFO storage.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 1888.0 B, free 265.4 MB)
15/08/13 20:42:08 INFO storage.MemoryStore: ensureFreeSpace(1202) called with curMem=1888, maxMem=278302556
15/08/13 20:42:08 INFO storage.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 1202.0 B, free 265.4 MB)
15/08/13 20:42:08 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:62578 (size: 1202.0 B, free: 265.4 MB)
15/08/13 20:42:08 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:874
15/08/13 20:42:08 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at map at SparkPi.scala:31)
15/08/13 20:42:08 INFO scheduler.TaskSchedulerImpl: Adding task set 0.0 with 2 tasks
15/08/13 20:42:08 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, PROCESS_LOCAL, 1442 bytes)
15/08/13 20:42:08 INFO scheduler.TaskSetManager: Starting task 1.0 in stage 0.0 (TID 1, localhost, PROCESS_LOCAL, 1442 bytes)
15/08/13 20:42:08 INFO executor.Executor: Running task 1.0 in stage 0.0 (TID 1)
15/08/13 20:42:08 INFO executor.Executor: Running task 0.0 in stage 0.0 (TID 0)
15/08/13 20:42:08 INFO executor.Executor: Fetching http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.0.6:62577/jars/spark-examples-1.4.1-hadoop2.2.0.jar with timestamp 1439523727458
&lt;/span&gt;15/08/13 20:43:08 INFO executor.Executor: Fetching http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.0.6:62577/jars/spark-examples-1.4.1-hadoop2.2.0.jar with timestamp 1439523727458
&lt;/span&gt;15/08/13 20:43:08 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.net.SocketTimeoutException: connect timed out
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
	at sun.net.www.http.HttpClient.&amp;lt;init&amp;gt;(HttpClient.java:211)
	at sun.net.www.http.HttpClient.New(HttpClient.java:308)
	at sun.net.www.http.HttpClient.New(HttpClient.java:326)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:639)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:453)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:398)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:390)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:390)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:193)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
15/08/13 20:43:08 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.net.SocketTimeoutException: connect timed out
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
	at sun.net.www.http.HttpClient.&amp;lt;init&amp;gt;(HttpClient.java:211)
	at sun.net.www.http.HttpClient.New(HttpClient.java:308)
	at sun.net.www.http.HttpClient.New(HttpClient.java:326)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:639)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:453)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:398)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:390)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:390)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:193)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

15/08/13 20:43:08 ERROR scheduler.TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
15/08/13 20:43:08 INFO scheduler.TaskSchedulerImpl: Cancelling stage 0
15/08/13 20:43:08 INFO executor.Executor: Executor is trying to kill task 1.0 in stage 0.0 (TID 1)
15/08/13 20:43:08 INFO scheduler.TaskSchedulerImpl: Stage 0 was cancelled
15/08/13 20:43:08 INFO scheduler.DAGScheduler: ResultStage 0 (reduce at SparkPi.scala:35) failed in 60.088 s
15/08/13 20:43:08 INFO scheduler.DAGScheduler: Job 0 failed: reduce at SparkPi.scala:35, took 60.229834 s
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.net.SocketTimeoutException: connect timed out
	at java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:339)
	at java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:200)
	at java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:182)
	at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.net.Socket.connect(Socket.java:579)
	at sun.net.NetworkClient.doConnect(NetworkClient.java:175)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:432)
	at sun.net.www.http.HttpClient.openServer(HttpClient.java:527)
	at sun.net.www.http.HttpClient.&amp;lt;init&amp;gt;(HttpClient.java:211)
	at sun.net.www.http.HttpClient.New(HttpClient.java:308)
	at sun.net.www.http.HttpClient.New(HttpClient.java:326)
	at sun.net.www.protocol.http.HttpURLConnection.getNewHttpClient(HttpURLConnection.java:996)
	at sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:932)
	at sun.net.www.protocol.http.HttpURLConnection.connect(HttpURLConnection.java:850)
	at org.apache.spark.util.Utils$.doFetchFile(Utils.scala:639)
	at org.apache.spark.util.Utils$.fetchFile(Utils.scala:453)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:398)
	at org.apache.spark.executor.Executor$$anonfun$org$apache$spark$executor$Executor$$updateDependencies$5.apply(Executor.scala:390)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:98)
	at scala.collection.mutable.HashTable$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreachEntry(HashTable.scala:226)
	at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:39)
	at scala.collection.mutable.HashMap.foreach(HashMap.scala:98)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.executor.Executor.org$apache$spark$executor$Executor$$updateDependencies(Executor.scala:390)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:193)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
	at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
15/08/13 20:43:08 INFO spark.SparkContext: Invoking stop() from shutdown hook
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/metrics/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/kill,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/api,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/&lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt;,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/threadDump,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/executors,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/environment,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/rdd,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/storage,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/pool,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/stage,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/stages,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/job,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs/json,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO handler.ContextHandler: stopped o.s.j.s.ServletContextHandler{/jobs,&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;}
15/08/13 20:43:08 INFO ui.SparkUI: Stopped Spark web UI at http:&lt;span class=&quot;code-comment&quot;&gt;//10.0.0.6:4040
&lt;/span&gt;15/08/13 20:43:08 INFO scheduler.DAGScheduler: Stopping DAGScheduler
15/08/13 20:43:08 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
15/08/13 20:43:08 INFO util.Utils: path = /&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/0j/bkhg_dw17w96qxddkmryz63r0000gn/T/spark-c2b2b947-327a-4e54-aa3b-ae11526aebde/blockmgr-37bc3de6-86da-491a-9a39-a9f44c3a0919, already present as root &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; deletion.
15/08/13 20:43:08 INFO storage.MemoryStore: MemoryStore cleared
15/08/13 20:43:08 INFO storage.BlockManager: BlockManager stopped
15/08/13 20:43:08 INFO storage.BlockManagerMaster: BlockManagerMaster stopped
15/08/13 20:43:08 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
15/08/13 20:43:08 INFO spark.SparkContext: Successfully stopped SparkContext
15/08/13 20:43:08 INFO util.Utils: Shutdown hook called
15/08/13 20:43:08 INFO util.Utils: Deleting directory /&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt;/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/folders/0j/bkhg_dw17w96qxddkmryz63r0000gn/T/spark-c2b2b947-327a-4e54-aa3b-ae11526aebde
~/spark-1.4.1-bin-without-hadoop $ 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14696432" author="nvspark" created="Fri, 14 Aug 2015 04:01:30 +0000"  >&lt;p&gt;Works after turning off McAfee Endpoint Protection&lt;/p&gt;</comment>
                            <comment id="14696546" author="apachespark" created="Fri, 14 Aug 2015 06:18:02 +0000"  >&lt;p&gt;User &apos;farseer90718&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/8188&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/8188&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 14 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2iwo7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>