<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:45:56 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-16344] Array of struct with a single field name &quot;element&quot; can&apos;t be decoded from Parquet files written by Spark 1.6+</title>
                <link>https://issues.apache.org/jira/browse/SPARK-16344</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;This is a weird corner case. Users may hit this issue if they have a schema that&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;has an array field whose element type is a struct, and&lt;/li&gt;
	&lt;li&gt;the struct has one and only one field, and&lt;/li&gt;
	&lt;li&gt;that field is named as &quot;element&quot;.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The following Spark shell snippet for Spark 1.6 reproduces this bug:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;A(element: &lt;span class=&quot;code-object&quot;&gt;Long&lt;/span&gt;)
&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;B(f: Array[A])

val path = &lt;span class=&quot;code-quote&quot;&gt;&quot;/tmp/silly.parquet&quot;&lt;/span&gt;
Seq(B(Array(A(42)))).toDF(&lt;span class=&quot;code-quote&quot;&gt;&quot;f0&quot;&lt;/span&gt;).write.mode(&lt;span class=&quot;code-quote&quot;&gt;&quot;overwrite&quot;&lt;/span&gt;).parquet(path)

val df = sqlContext.read.parquet(path)
df.printSchema()
&lt;span class=&quot;code-comment&quot;&gt;// root
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;//  |-- f0: array (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;//  |    |-- element: struct (containsNull = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;//  |    |    |-- element: &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; (nullable = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;)
&lt;/span&gt;
df.show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Exception thrown:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/silly.parquet/part-r-00007-e06db7b0-5181-4a14-9fee-5bb452e883a0.gz.parquet
        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:228)
        at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:201)
        at org.apache.spark.rdd.SqlNewHadoopRDD$$anon$1.hasNext(SqlNewHadoopRDD.scala:194)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:308)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:212)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:212)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1858)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:89)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: Expected instance of group converter but got &quot;org.apache.spark.sql.execution.datasources.parquet.CatalystPrimitiveConverter&quot;
        at org.apache.parquet.io.api.Converter.asGroupConverter(Converter.java:37)
        at org.apache.parquet.io.RecordReaderImplementation.&amp;lt;init&amp;gt;(RecordReaderImplementation.java:266)
        at org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:134)
        at org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:99)
        at org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:154)
        at org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:99)
        at org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:137)
        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:208)
        ... 26 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Spark 2.0.0-SNAPSHOT and Spark master also suffer this issue. To reproduce it using these versions, just replace &lt;tt&gt;sqlContext&lt;/tt&gt; in the above snippet with &lt;tt&gt;spark&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;The reason behind is related to Parquet backwards-compatibility rules for LIST types defined in &lt;a href=&quot;https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#lists&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;parquet-format spec&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The Spark SQL schema shown above&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;root
 |-- f0: array (nullable = true)
 |    |-- element: struct (containsNull = true)
 |    |    |-- element: long (nullable = true)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;is equivalent to the following SQL type:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;STRUCT&amp;lt;
  f: ARRAY&amp;lt;
    STRUCT&amp;lt;element: BIGINT&amp;gt;
  &amp;gt;
&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;According to the parquet-format spec, the standard layout of a LIST-like structure is a 3-level layout:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;lt;list-repetition&amp;gt; group &amp;lt;name&amp;gt; (LIST) {
  repeated group list {
    &amp;lt;element-repetition&amp;gt; &amp;lt;element-type&amp;gt; element;
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Thus, the standard representation of the aforementioned SQL type should be:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;message root {
  optional group f (LIST) {
    repeated group list {
      optional group element {    (1)
        optional int64 element;   (2)
      }
    }
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the two &quot;element&quot; fields are different:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The &lt;tt&gt;group&lt;/tt&gt; field &quot;element&quot; at (1) is a &quot;container&quot; of list element type. This is defined as part of the parquet-format spec.&lt;/li&gt;
	&lt;li&gt;The &lt;tt&gt;int64&lt;/tt&gt; field &quot;element&quot; at (2) corresponds to the &lt;tt&gt;element&lt;/tt&gt; field of case class &lt;tt&gt;A&lt;/tt&gt; we defined above.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;However, due to historical reasons, various existing systems do not conform to the parquet-format spec and may write LIST structures in a non-standard layout. For example, parquet-avro and parquet-thrift use a 2-level layout like&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;// parquet-avro style
&amp;lt;list-repetition&amp;gt; group &amp;lt;name&amp;gt; (LIST) {
  repeated &amp;lt;element-type&amp;gt; array;
}

// parquet-thrift style
&amp;lt;list-repetition&amp;gt; group &amp;lt;name&amp;gt; (LIST) {
  repeated &amp;lt;element-type&amp;gt; &amp;lt;name&amp;gt;_tuple;
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To keep backwards-compatibility, the parquet-format spec defined a set of &lt;a href=&quot;https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#backward-compatibility-rules&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;backwards-compatibility rules&lt;/a&gt; to also recognize these patterns.&lt;/p&gt;

&lt;p&gt;Unfortunately, these backwards-compatibility rules makes the Parquet schema we mentioned above ambiguous:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;message root {
  optional group f (LIST) {
    repeated group list {
      optional group element {
        optional int64 element;
      }
    }
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When interpreted using the standard 3-level layout, it is the expected type:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;STRUCT&amp;lt;
  f: ARRAY&amp;lt;
    STRUCT&amp;lt;element: BIGINT&amp;gt;
  &amp;gt;
&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When interpreted using the legacy 2-level layout, it is the unexpected type&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;// When interpreted as legacy 2-level layout
STRUCT&amp;lt;
  f: ARRAY&amp;lt;
    STRUCT&amp;lt;element: STRUCT&amp;lt;element: BIGINT&amp;gt;&amp;gt;
  &amp;gt;
&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is because the nested struct field name happens to be &quot;element&quot;, which is used as a dedicated name of the element type &quot;container&quot; group in the standard 3-level layout, and lead to the ambiguity.&lt;/p&gt;

&lt;p&gt;Currently, Spark 1.6.x, 2.0.0-SNAPSHOT, and master chose the 2nd one. We can fix this issue by giving the standard 3-level layout a higher priority when trying to match schema patterns.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12986130">SPARK-16344</key>
            <summary>Array of struct with a single field name &quot;element&quot; can&apos;t be decoded from Parquet files written by Spark 1.6+</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lian cheng">Cheng Lian</assignee>
                                    <reporter username="lian cheng">Cheng Lian</reporter>
                        <labels>
                    </labels>
                <created>Fri, 1 Jul 2016 10:37:46 +0000</created>
                <updated>Thu, 4 Feb 2021 06:57:29 +0000</updated>
                            <resolved>Wed, 20 Jul 2016 23:50:00 +0000</resolved>
                                    <version>1.6.0</version>
                    <version>1.6.1</version>
                    <version>1.6.2</version>
                    <version>2.0.0</version>
                                    <fixVersion>2.1.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="15358799" author="apachespark" created="Fri, 1 Jul 2016 11:03:04 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14013&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14013&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15358837" author="apachespark" created="Fri, 1 Jul 2016 11:42:05 +0000"  >&lt;p&gt;User &apos;liancheng&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14014&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14014&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15364706" author="rdblue" created="Wed, 6 Jul 2016 17:35:11 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lian+cheng&quot; class=&quot;user-hover&quot; rel=&quot;lian cheng&quot;&gt;lian cheng&lt;/a&gt;, I&apos;m looking at this today.&lt;/p&gt;</comment>
                            <comment id="15365062" author="rdblue" created="Wed, 6 Jul 2016 20:53:36 +0000"  >&lt;p&gt;It looks like the main change is to specifically catch the 3-level name structure, &lt;tt&gt;list-name (LIST) -&amp;gt; &quot;list&quot; -&amp;gt; &quot;element&quot;&lt;/tt&gt;. The problem with this approach is that it doesn&apos;t solve the problem entirely either.&lt;/p&gt;

&lt;p&gt;Let me try to give a bit more background. In parquet-avro, there are two &lt;tt&gt;isElementType&lt;/tt&gt; methods; one in the schema converter and one in the record converter. The one in the schema converter will guess whether the Parquet type uses a 3-level list or a 2-level list when it can&apos;t be determined according to the spec&apos;s backward-compatibility rules. That guess assumes a 2-level structure by default and at the next major release will guess a 3-level structure. (This can be controlled by a property.) But this is only used when the reader doesn&apos;t supply a read schema / expected schema and the code has to convert from Parquet&apos;s type to get one. Ideally, we always have a read schema from the file, from the reader&apos;s expected class (if using Java objects), or from the reader passing in the expected schema. That&apos;s why the other &lt;tt&gt;isElementType&lt;/tt&gt; method exists: it looks at the expected schema and the file schema to determine whether the caller has passed in a schema with the extra single-field list/element struct.&lt;/p&gt;

&lt;p&gt;That code has to distinguish between two cases for a 3-level list:&lt;br/&gt;
1. When the caller expects &lt;tt&gt;List&amp;lt;OneTuple&amp;lt;ElementType&amp;gt;&amp;gt;&lt;/tt&gt;, with the extra record layer that was originally returned when Avro only knew about 2-level lists.&lt;br/&gt;
2. When the caller expects &lt;tt&gt;List&amp;lt;ElementType&amp;gt;&lt;/tt&gt;, without an extra layer.&lt;/p&gt;

&lt;p&gt;The code currently assumes that if the element schema appears to match the repeated type that the caller has passed a schema indicating case 1. This issue points out that the matching isn&apos;t perfect and an element with a single field named &quot;element&quot; will incorrectly match case 1 when it was really case 2. The problem with the solution in PR #14013, if it were applied to Avro, is that it breaks if the caller is actually passing a schema for case 1.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure whether Spark works like Avro and has two &lt;tt&gt;isElementType&lt;/tt&gt; methods. If Spark can guarantee that the table schema is never case 1, then it is correct to use the logic in the PR. I don&apos;t think that&apos;s always the case because the table schema may come from user objects in a Dataset or from the Hive MetaStore. But, this may be a reasonable heuristic if you think case 2 is far more common than case 1. For parquet-avro, I think the user supplying a single-field record with the inner field named &quot;element&quot; is rare enough that it doesn&apos;t really matter, but it&apos;s up to you guys in the Spark community on this issue.&lt;/p&gt;

&lt;p&gt;One last thing: based on the rest of the schema structure, there should be only one way to match the expected schema to the file schema. You could always try both and fall back to the other case, or have a more complicated &lt;tt&gt;isElementType&lt;/tt&gt; method that recurses down the sub-trees to find a match. I didn&apos;t implement this in parquet-avro because I think it&apos;s a rare problem and not worth the time.&lt;/p&gt;</comment>
                            <comment id="15366069" author="lian cheng" created="Thu, 7 Jul 2016 13:13:13 +0000"  >&lt;p&gt;Thanks for the detailed response!&lt;/p&gt;

&lt;p&gt;Spark SQL also has two &lt;tt&gt;isElementType&lt;/tt&gt; methods. Actually, I was following parquet-avro when writing that part of code.&lt;/p&gt;

&lt;p&gt;For the two cases you listed, from the perspective of application-level data model, it&apos;s true that a type like &lt;tt&gt;List&amp;lt;OneTuple&amp;lt;ElementType&amp;gt;&amp;gt;&lt;/tt&gt; can be rare. However, we did hit this kind of schema in production, which is the reason why I noticed this issue.&lt;/p&gt;

&lt;p&gt;On the other hand, to the best of my knowledge, no well-known Parquet data model writes a two-level LIST structure with a repeated field named &quot;list&quot;. That&apos;s why I thought this heuristic can be relatively safe.&lt;/p&gt;

&lt;p&gt;I agree that recursing down the data type tree is the proper way to fix this issue, but it&apos;s probably not worth the extra complexity introduced.&lt;/p&gt;</comment>
                            <comment id="15366296" author="rdblue" created="Thu, 7 Jul 2016 15:44:50 +0000"  >&lt;p&gt;I agree, using list/element is reasonable. I just want to note that this doesn&apos;t solve the problem, it changes the heuristic to do the right thing for one case and the wrong thing for another.&lt;/p&gt;

&lt;p&gt;I won&apos;t make this change in parquet-avro before a major release because it isn&apos;t backward-compatible. I know of cases where applications were written expecting the additional layer and pass it in as the expected schema. Making this change would break those applications to make a rare struct work. For Spark, it&apos;s up to you guys because this is a major release (assuming it makes 2.0) and the need for backward-compatibility is different.&lt;/p&gt;</comment>
                            <comment id="15369032" author="lian cheng" created="Sat, 9 Jul 2016 09:25:20 +0000"  >&lt;p&gt;I was re-thinking about &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rdblue&quot; class=&quot;user-hover&quot; rel=&quot;rdblue&quot;&gt;rdblue&lt;/a&gt;&apos;s comment above, and tried to build some more corner cases that &lt;a href=&quot;https://github.com/apache/spark/pull/14014&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;PR #14014&lt;/a&gt; can&apos;t handle. Here is a similar case constructed using Hive 1.2.1:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-sql&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;CREATE&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;TABLE&lt;/span&gt; s
STORED &lt;span class=&quot;code-keyword&quot;&gt;AS&lt;/span&gt; PARQUET
&lt;span class=&quot;code-keyword&quot;&gt;AS&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;SELECT&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;ARRAY&lt;/span&gt;(NAMED_STRUCT(&lt;span class=&quot;code-quote&quot;&gt;&apos;array_element&apos;&lt;/span&gt;, 1)) &lt;span class=&quot;code-keyword&quot;&gt;AS&lt;/span&gt; f;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When writing to Parquet, Hive encodes array fields into the following non-standard 3-level layout:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;optional group &amp;lt;name&amp;gt; (LIST) {
  repeated group bag {
    optional &amp;lt;element-type&amp;gt; array_element;
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;According to this template layout, the above SQL DDL write a Parquet file with the following schema:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$ parquet-schema $WAREHOUSE_DIR/s/000000_0
message hive_schema {
  optional group f (LIST) {
    repeated group bag {
      optional group array_element {
        optional int32 array_element;
      }
    }
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Reading this file using Spark patched with PR #14014 results in the same exception described in the ticket description. This is not surprising since the case above is exactly the same with the tracked one except that the actual field names are different.&lt;/p&gt;</comment>
                            <comment id="15369481" author="lian cheng" created="Sun, 10 Jul 2016 08:06:45 +0000"  >&lt;p&gt;Thanks to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rdblue&quot; class=&quot;user-hover&quot; rel=&quot;rdblue&quot;&gt;rdblue&lt;/a&gt;&apos;s comment about why there&apos;re two different &lt;tt&gt;isElementType&lt;/tt&gt; methods parquet-avro, I finally realized that Spark SQL doesn&apos;t really need two &lt;tt&gt;isElementType&lt;/tt&gt; methods as what parquet-avro does, and came up with a proper fix of this issue for Spark SQL (already pushed this to &lt;a href=&quot;https://github.com/apache/spark/pull/14014&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;PR #14014&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Here I&apos;m trying to write down my understanding for future reference.&lt;/p&gt;

&lt;p&gt;One important difference between parquet-avro and Spark SQL 1.6+ is how the requested Parquet schema is set in &lt;tt&gt;ReadSupport.init()&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;In parquet-avro, the requested Parquet schema can be set via two methods:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;If no requested Avro schema is specified, the full Parquet schema of the file to be read is used as requested schema.&lt;/li&gt;
	&lt;li&gt;If a requested Avro schema is specified, parquet-avro converts the requested Avro schema into a Parquet schema using &lt;tt&gt;AvroSchemaConverter&lt;/tt&gt;, and use the converted Parquet schema as requested Parquet schema.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The 2nd case is risky because the converted Parquet schema may not conform to the &quot;flavor&quot; of the actual schema of the physical Parquet file. For example, the file might be created using parquet-protobuf, and use &lt;tt&gt;repeated int32 f;&lt;/tt&gt; to represent an integer array, while parquet-avro uses either a 2-level or a 3-level layout. This inconsistency limits interoperability of parquet-avro. Although the 2nd &lt;tt&gt;isElementType&lt;/tt&gt; in &lt;tt&gt;AvroRecordConverter&lt;/tt&gt; helps to reconcile part of the corner cases by comparing the requested schema and the expected Avro schema, this issue is still not completely resolved.&lt;/p&gt;

&lt;p&gt;In Spark SQL, to provide better interoperability, the requested Parquet schema generated for each physical file is always tailored from the actual schema of the file to be read. The way we do the tailoring is like the following:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;The query execution engine always provides a requested schema &lt;tt&gt;cs&lt;/tt&gt; in the form of a Catalyst &lt;tt&gt;StructType&lt;/tt&gt;.&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;ParquetReadSupport.init()&lt;/tt&gt; calls &lt;tt&gt;ParquetReadSupport.clipParquetSchema()&lt;/tt&gt; to tailor the Parquet schema &lt;tt&gt;ps&lt;/tt&gt; of the physical file using &lt;tt&gt;cs&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;All column paths that exist in &lt;tt&gt;cs&lt;/tt&gt; but missing in &lt;tt&gt;ps&lt;/tt&gt; are added to &lt;tt&gt;ps&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;All column paths that exist in &lt;tt&gt;ps&lt;/tt&gt; but missing in &lt;tt&gt;ps&lt;/tt&gt; are removed from &lt;tt&gt;ps&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;All backward-compatibility rules are properly considered while adding/removing column paths to/from &lt;tt&gt;ps&lt;/tt&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;The above work was done in &lt;a href=&quot;https://github.com/apache/spark/pull/8509&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;PR #8509&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;In this way, it&apos;s guaranteed that the tailored requested schema always fits the file to be read. Thus we don&apos;t really need the 2nd &lt;tt&gt;isElementType&lt;/tt&gt; in &lt;tt&gt;ParquetRowConverter&lt;/tt&gt; to do any further reconcilation. The real cause of this JIRA ticket is that the two &lt;tt&gt;ParquetRowConverter.isElementType&lt;/tt&gt; makes a different decision from &lt;tt&gt;ParquetSchemaConverter.isElementType&lt;/tt&gt;. For the schema in question&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;optional group f (LIST) {
  repeated group list {
    optional group element {
      optional int32 element;
    }
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;tt&gt;ParquetSchemaConverter.isElementType&lt;/tt&gt; thinks it&apos;s a standard 3-level layout, which is correct, while &lt;tt&gt;ParquetRowConverter.isElementType&lt;/tt&gt; thinks it&apos;s a 2-level layout. Thus the generated row converter doesn&apos;t conform with the requested schema, and caused the problem.&lt;/p&gt;

&lt;p&gt;By removing &lt;tt&gt;ParquetRowConverter.isElementType&lt;/tt&gt;, we can avoid the inconsistent decisions. When trying to test whether a repeated field within a LIST-annotated field corresponds to the element type or not in &lt;tt&gt;ParquetRowConverter&lt;/tt&gt;, we only need to try to convert the repeated type into a Catalyst type to see whether the converted type matches the actual Catalyst array element type.&lt;/p&gt;

&lt;p&gt;The above fix is also equivalent to the proper fix &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rdblue&quot; class=&quot;user-hover&quot; rel=&quot;rdblue&quot;&gt;rdblue&lt;/a&gt; mentioned since the schema conversion process properly recurses the data type sub-tree. The Hive case mentioned above is also fixed by this approach.&lt;/p&gt;

&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yhuai&quot; class=&quot;user-hover&quot; rel=&quot;yhuai&quot;&gt;yhuai&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15371013" author="rdblue" created="Mon, 11 Jul 2016 15:44:22 +0000"  >&lt;p&gt;Sounds good to me! I like the idea of converting back to the expected type and then checking that it is correct. That&apos;s an easy way to solve the problem entirely. I think we could do the same thing in parquet-avro to solve more cases.&lt;/p&gt;

&lt;p&gt;The problem you raise with parquet-avro is a separate issue because the schema in case 2 is only used to filter columns. Setting up the converters is done independently. Also, this is why there are two methods for setting Avro schemas: the projection schema and the read schema. That gives some additional flexibility to make the projection schema match and solves cases like renamed fields. But you&apos;re right that it&apos;s not complete right now and needs to be fixed.&lt;/p&gt;</comment>
                            <comment id="15386831" author="yhuai" created="Wed, 20 Jul 2016 23:50:01 +0000"  >&lt;p&gt;Issue resolved by pull request 14014&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/14014&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/14014&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17278524" author="sekiforever" created="Thu, 4 Feb 2021 04:45:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lian+cheng&quot; class=&quot;user-hover&quot; rel=&quot;lian cheng&quot;&gt;lian cheng&lt;/a&gt; I know this jira has been resolved years ago. I am currently using AvroParquetReader to read spark created parquet data which contains array values, and I ran into a problem that resolved schema is an array of a record type instead of an array of primitive type (which was used as the schema in spark code to generate parquet files). I think this has to do with the 2-level/3-level layout you mentioned in this ticket. I already asked this question in #spark slack channel of ASF workspace. Do you mind sharing your contact info so that we can discuss it quickly? Thanks so much for your help!&lt;/p&gt;</comment>
                            <comment id="17278596" author="sekiforever" created="Thu, 4 Feb 2021 06:57:29 +0000"  >&lt;p&gt;Actually this ticket is very similar to the problem I am facing: &lt;a href=&quot;https://issues.apache.org/jira/browse/PARQUET-651&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/PARQUET-651&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12986173">PARQUET-651</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 40 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i30f5j:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12335644">2.1.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>