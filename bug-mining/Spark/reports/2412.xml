<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:31:58 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-5945] Spark should not retry a stage infinitely on a FetchFailedException</title>
                <link>https://issues.apache.org/jira/browse/SPARK-5945</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;While investigating &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-5928&quot; title=&quot;Remote Shuffle Blocks cannot be more than 2 GB&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-5928&quot;&gt;&lt;del&gt;SPARK-5928&lt;/del&gt;&lt;/a&gt;, I noticed some very strange behavior in the way spark retries stages after a FetchFailedException.  It seems that on a FetchFailedException, instead of simply killing the task and retrying, Spark aborts the stage and retries.  If it just retried the task, the task might fail 4 times and then trigger the usual job killing mechanism.  But by killing the stage instead, the max retry logic is skipped (it looks to me like there is no limit for retries on a stage).&lt;/p&gt;

&lt;p&gt;After a bit of discussion with Kay Ousterhout, it seems the idea is that if a fetch fails, we assume that the block manager we are fetching from has failed, and that it will succeed if we retry the stage w/out that block manager.  In that case, it wouldn&apos;t make any sense to retry the task, since its doomed to fail every time, so we might as well kill the whole stage.  But this raises two questions:&lt;/p&gt;


&lt;p&gt;1) Is it really safe to assume that a FetchFailedException means that the BlockManager has failed, and ti will work if we just try another one?  &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-5928&quot; title=&quot;Remote Shuffle Blocks cannot be more than 2 GB&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-5928&quot;&gt;&lt;del&gt;SPARK-5928&lt;/del&gt;&lt;/a&gt; shows that there are at least some cases where that assumption is wrong.  Even if we fix that case, this logic seems brittle to the next case we find.  I guess the idea is that this behavior is what gives us the &quot;R&quot; in RDD ... but it seems like its not really that robust and maybe should be reconsidered.&lt;/p&gt;

&lt;p&gt;2) Should stages only be retried a limited number of times?  It would be pretty easy to put in a limited number of retries per stage.  Though again, we encounter issues with keeping things resilient.  Theoretically one stage could have many retries, but due to failures in different stages further downstream, so we might need to track the cause of each retry as well to still have the desired behavior.&lt;/p&gt;

&lt;p&gt;In general it just seems there is some flakiness in the retry logic.  This is the only reproducible example I have at the moment, but I vaguely recall hitting other cases of strange behavior w/ retries when trying to run long pipelines.  Eg., if one executor is stuck in a GC during a fetch, the fetch fails, but the executor eventually comes back and the stage gets retried again, but the same GC issues happen the second time around, etc.&lt;/p&gt;

&lt;p&gt;Copied from &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-5928&quot; title=&quot;Remote Shuffle Blocks cannot be more than 2 GB&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-5928&quot;&gt;&lt;del&gt;SPARK-5928&lt;/del&gt;&lt;/a&gt;, here&apos;s the example program that can regularly produce a loop of stage failures.  Note that it will only fail from a remote fetch, so it can&apos;t be run locally &amp;#8211; I ran with &lt;tt&gt;MASTER=yarn-client spark-shell --num-executors 2 --executor-memory 4000m&lt;/tt&gt;&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    val rdd = sc.parallelize(1 to 1e6.toInt, 1).map{ ignore =&amp;gt;
      val n = 3e3.toInt
      val arr = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Array[&lt;span class=&quot;code-object&quot;&gt;Byte&lt;/span&gt;](n)
      &lt;span class=&quot;code-comment&quot;&gt;//need to make sure the array doesn&apos;t compress to something small
&lt;/span&gt;      scala.util.Random.nextBytes(arr)
      arr
    }
    rdd.map { x =&amp;gt; (1, x)}.groupByKey().count()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12776752">SPARK-5945</key>
            <summary>Spark should not retry a stage infinitely on a FetchFailedException</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ilganeli">Ilya Ganelin</assignee>
                                    <reporter username="irashid">Imran Rashid</reporter>
                        <labels>
                    </labels>
                <created>Mon, 23 Feb 2015 01:48:58 +0000</created>
                <updated>Thu, 3 Sep 2015 05:08:47 +0000</updated>
                            <resolved>Thu, 3 Sep 2015 05:08:47 +0000</resolved>
                                                    <fixVersion>1.6.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>4</votes>
                                    <watches>20</watches>
                                                                                                                <comments>
                            <comment id="14341285" author="suyan" created="Sat, 28 Feb 2015 03:25:59 +0000"  >&lt;p&gt;I encounter stage retry infinitely when a executor lost because a Spark bug which I already fix in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-5259&quot; title=&quot;Do not submit stage until its dependencies map outputs are registered&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-5259&quot;&gt;&lt;del&gt;SPARK-5259&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;For solve stage retry, I add a retry limit.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; FetchFailed{
    ....
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (disallowStageRetryForTest) {
          abortStage(failedStage, &lt;span class=&quot;code-quote&quot;&gt;&quot;Fetch failure will not retry stage due to testing config&quot;&lt;/span&gt;)
        } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (failedStage.attemptId &amp;gt;= maxStageFailures) {
          abortStage(failedStage, s&lt;span class=&quot;code-quote&quot;&gt;&quot;Fetch failure will not retry stage&quot;&lt;/span&gt; +
            &lt;span class=&quot;code-quote&quot;&gt;&quot; due to reach to max Failure times: &quot;&lt;/span&gt; + maxStageFailures)
        } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (failedStages.isEmpty &amp;amp;&amp;amp; eventProcessActor != &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) {
          &lt;span class=&quot;code-comment&quot;&gt;// Don&lt;span class=&quot;code-quote&quot;&gt;&apos;t schedule an event to resubmit failed stages &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; failed isn&apos;&lt;/span&gt;t empty, because
&lt;/span&gt;   .....
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="14367891" author="ilganeli" created="Wed, 18 Mar 2015 20:59:43 +0000"  >&lt;p&gt;Hi Imran - I&apos;d be happy to tackle this. Could you please assign it to me? Thank you. &lt;/p&gt;</comment>
                            <comment id="14376454" author="irashid" created="Mon, 23 Mar 2015 19:28:32 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ilganeli&quot; class=&quot;user-hover&quot; rel=&quot;ilganeli&quot;&gt;ilganeli&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;sorry for taking a while to respond.  I think the main issue here is not so much just implementing the code (as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=SuYan&quot; class=&quot;user-hover&quot; rel=&quot;SuYan&quot;&gt;SuYan&lt;/a&gt; already has shown the small required patch).  The big issue is figuring out what the desired semantics are (see the questions I listed above), which means just getting feedback from all the required people on this one.  But if you want to drive that process, that sounds great, it would really be appreciated!&lt;/p&gt;</comment>
                            <comment id="14507583" author="apachespark" created="Wed, 22 Apr 2015 18:15:08 +0000"  >&lt;p&gt;User &apos;ilganeli&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/5636&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/5636&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14507957" author="kayousterhout" created="Wed, 22 Apr 2015 21:35:43 +0000"  >&lt;p&gt;Commenting here rather than on the github for archiving purposes!&lt;/p&gt;

&lt;p&gt;I took at look at the proposed pull request, and I&apos;d be in favor of a much simpler approach, where for each stage, we track the number of failures (from any cause), and then fail the job once a stage fails 4 times (4, for consistency with the max task failures).  If the stage succeeds, we can reset the count to 0, to avoid the potential problem &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=imranr&quot; class=&quot;user-hover&quot; rel=&quot;imranr&quot;&gt;imranr&lt;/a&gt; mentioned for stages that are re-used by many jobs (so the counter would be numConsecutiveFailures or something like that).  This can just be added to the Stage class, I think.  This is consistent with the approach we use for tasks, where if a task has failed 4 times (for any reason), we abort the stage.&lt;/p&gt;

&lt;p&gt;I also would advocate against adding a configuration parameter for this.  I can&apos;t imagine a case where someone would want to keep trying after 4 failures, and I think sometimes configuration parameters for things like this lead people to believe they can fix a problem by changing the configuration variable (just up the max number of failures!!) when really there is some bigger underlying issue they should fix.  4 seems to have worked well for tasks, so I&apos;d just use the same default here (and it&apos;s always easy to add a configuration variable later on if lots of people say they need it).&lt;/p&gt;</comment>
                            <comment id="14508019" author="ilganeli" created="Wed, 22 Apr 2015 22:08:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kayousterhout&quot; class=&quot;user-hover&quot; rel=&quot;kayousterhout&quot;&gt;kayousterhout&lt;/a&gt; - thanks for the review. If I understand correctly, your suggestion would still address &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=imranr&quot; class=&quot;user-hover&quot; rel=&quot;imranr&quot;&gt;imranr&lt;/a&gt;&apos;s second comment since the first stage would always (or mostly succeed), e.g. it wouldn&apos;t have N consecutive failures so even if subsequent stages fail, those wouldn&apos;t count towards the failure count for this particular stage since it would have been reset when it succeeded. &lt;/p&gt;

&lt;p&gt;Do you have any thoughts on the first comment? Specifically, is retrying a stage likely to succeed at all or is it a waste of effort in the first place?&lt;/p&gt;</comment>
                            <comment id="14508022" author="kayousterhout" created="Wed, 22 Apr 2015 22:11:14 +0000"  >&lt;p&gt;When there&apos;s a fetch failed exception, what happens is that we mark the corresponding map tasks as failed, and then re-run all of the failed tasks from the previous stage.  When that&apos;s done, we re-run the stage with the original fetch failed exception &amp;#8211; so in the normal case, the stage with the fetch failed exception should succeed the second time.&lt;/p&gt;</comment>
                            <comment id="14508041" author="irashid" created="Wed, 22 Apr 2015 22:35:34 +0000"  >&lt;p&gt;good point about moving the design discussion to jira, thanks Kay.&lt;/p&gt;

&lt;p&gt;Yes, totally agree that we want to retry at least once, its definitely &quot;normal&quot; that in a big cluster, a node will go bad from time-to-time, but the good thing is Spark knows how to recover.  I also agree that we shouldn&apos;t care about the cause of the failure, just the failure count.&lt;/p&gt;

&lt;p&gt;I totally see your points about the different configuration parameters, but let me just play devil&apos;s advocate.  yes, configuration parameters are confusing to users, but thats a reason we should have sensible defaults and most users should never need to touch them.  That doesn&apos;t mean nobody will want them.  Tasks and stages are in some ways very different things &amp;#8211; tasks are meant to be very small and lightweight, so failing a few extra times is no big deal.  But stages can be really big &amp;#8211; I would imagine in most cases, you actually might want to fail completely if the stage fails even twice, just because you can waste so much time in stage failure.  Then again, there might be the other extreme, with really big clusters and unstable hardware, maybe two failures won&apos;t be that big a deal, so some users will want it higher.&lt;/p&gt;

&lt;p&gt;I disagree that its easy to add the config later.  Yes, its easy to make the code change.  But its hard to deploy the change in a production environment.  And I can see this as a parameter that devops team needs to play with for their exact system / workload / SLAs etc.  &amp;#8211; not sure at all, but I think we just don&apos;t know, and so we should leave the door open.&lt;/p&gt;

&lt;p&gt;I also can&apos;t see any reason why anyone would want infinite retries &amp;#8211; but I&apos;m hesitant (and asked for the change) just b/c of changing from the old behavior.  I guess int.maxvalue is close enough if somebody needs it?&lt;/p&gt;</comment>
                            <comment id="14508091" author="kayousterhout" created="Wed, 22 Apr 2015 22:55:15 +0000"  >&lt;p&gt;I realized there might be a cleaner solution here: I wonder if we should just break the FetchFailedException into two subtypes, one of which is UnrecoverableFetchFailedException.  If a shuffle block is too large, there&apos;s no point in retrying the stage; we should just fail it.  I realized maybe this is what you were alluding to in your earlier comment, when you asked whether it&apos;s worth it to retry.  It seems like, at the point when we throw the exception, we do know whether it&apos;s worth it to retry, and it would be cleaner to just throw an appropriate exception. Thoughts?&lt;/p&gt;</comment>
                            <comment id="14508455" author="irashid" created="Thu, 23 Apr 2015 04:49:03 +0000"  >&lt;p&gt;I think we want to do something more than just handling the case of blocks that are too large.  Putting in a special case to avoid retrying at all in that case would be fine, but to me that is a separate issue, the more important thing to do is to put in a general retry limit.  There could be all sorts of other reasons for fetch failures (I just looked into a case where an OOM would kill an executor, which lead to 10 hours of stage retry attempts before somebody manually killed the job).&lt;/p&gt;</comment>
                            <comment id="14508473" author="kayousterhout" created="Thu, 23 Apr 2015 05:09:19 +0000"  >&lt;p&gt;Good point &amp;#8211; that makes sense.&lt;/p&gt;

&lt;p&gt;I&apos;m still in favor of not adding a config parameter: I think we&apos;re in agreement that it&apos;s hard to imagine a case where someone wants this to be &lt;b&gt;more&lt;/b&gt; than 4, so making it 4 seems strictly better than the current approach where it is infinite.  If people complain or want to configure it to be less, we can always change this in a minor release.&lt;/p&gt;</comment>
                            <comment id="14510154" author="ilganeli" created="Fri, 24 Apr 2015 00:40:35 +0000"  >&lt;p&gt;So to recap:&lt;br/&gt;
a) Move failure count tracking into Stage&lt;br/&gt;
b) Reset failure count on Stage success, so even if that stage is re-submitted due to failures downstream, we never hit the cap&lt;br/&gt;
c) Remove config parameter. &lt;/p&gt;</comment>
                            <comment id="14518807" author="irashid" created="Wed, 29 Apr 2015 06:21:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kayousterhout&quot; class=&quot;user-hover&quot; rel=&quot;kayousterhout&quot;&gt;kayousterhout&lt;/a&gt; can you please clarify &amp;#8211; did you want to just hardcode to 4, or did you want to reuse &lt;tt&gt;spark.task.maxFailures&lt;/tt&gt; for stage failures as well?&lt;/p&gt;</comment>
                            <comment id="14519650" author="kayousterhout" created="Wed, 29 Apr 2015 16:20:14 +0000"  >&lt;p&gt;I wanted to hardcode to 4 (totally agree with the sentiment you expressed earlier in this thread, that it doesn&apos;t make sense / is very confusing to re-use a config parameter for two different things).&lt;/p&gt;</comment>
                            <comment id="14560007" author="irashid" created="Tue, 26 May 2015 22:32:44 +0000"  >&lt;p&gt;blocked by &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-7308&quot; title=&quot;Should there be multiple concurrent attempts for one stage?&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-7308&quot;&gt;&lt;del&gt;SPARK-7308&lt;/del&gt;&lt;/a&gt; b/c you need to know which attempt is being failed&lt;/p&gt;</comment>
                            <comment id="14611960" author="darabos" created="Thu, 2 Jul 2015 13:38:59 +0000"  >&lt;p&gt;At the moment we have a ton of these infinite retries. A stage is retried a few dozen times, then its parent goes missing and Spark starts retrying the parent until it also goes missing... We are still debugging the cause of our fetch failures, but I just wanted to mention that if there were a &lt;tt&gt;spark.stage.maxFailures&lt;/tt&gt; option, we would be setting it to 1 at this point.&lt;/p&gt;

&lt;p&gt;Thanks for all the work on this bug. Even if it&apos;s not fixed yet, it&apos;s very informative.&lt;/p&gt;</comment>
                            <comment id="14703568" author="rxin" created="Wed, 19 Aug 2015 18:56:40 +0000"  >&lt;p&gt;I have retargeted this and downgraded it from Blocker to Critical since it&apos;s been there for a while and not a regression.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                                                <inwardlinks description="is blocked by">
                                        <issuelink>
            <issuekey id="12818973">SPARK-6746</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12826627">SPARK-7308</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 13 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i25wr3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12333083">1.6.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>