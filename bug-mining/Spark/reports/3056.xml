<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:37:23 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-6847] Stack overflow on updateStateByKey which followed by a dstream with checkpoint set</title>
                <link>https://issues.apache.org/jira/browse/SPARK-6847</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;The issue happens with the following sample code: uses &lt;tt&gt;updateStateByKey&lt;/tt&gt; followed by a &lt;tt&gt;map&lt;/tt&gt; with checkpoint interval 10 seconds&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    val sparkConf = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkConf().setAppName(&lt;span class=&quot;code-quote&quot;&gt;&quot;test&quot;&lt;/span&gt;)
    val streamingContext = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StreamingContext(sparkConf, Seconds(10))
    streamingContext.checkpoint(&lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;&quot;checkpoint&quot;&lt;/span&gt;&quot;&quot;)
    val source = streamingContext.socketTextStream(&lt;span class=&quot;code-quote&quot;&gt;&quot;localhost&quot;&lt;/span&gt;, 9999)
    val updatedResult = source.map(
        (1,_)).updateStateByKey(
            (newlist : Seq[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;], oldstate : Option[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]) =&amp;gt;     newlist.headOption.orElse(oldstate))
    updatedResult.map(_._2)
    .checkpoint(Seconds(10))
    .foreachRDD((rdd, t) =&amp;gt; {
      println(&lt;span class=&quot;code-quote&quot;&gt;&quot;Deep: &quot;&lt;/span&gt; + rdd.toDebugString.split(&lt;span class=&quot;code-quote&quot;&gt;&quot;\n&quot;&lt;/span&gt;).length)
      println(t.toString() + &lt;span class=&quot;code-quote&quot;&gt;&quot;: &quot;&lt;/span&gt; + rdd.collect.length)
    })
    streamingContext.start()
    streamingContext.awaitTermination()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From the output, we can see that the dependency will be increasing time over time, the &lt;tt&gt;updateStateByKey&lt;/tt&gt; never get check-pointed,  and finally, the stack overflow will happen. &lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The rdd in &lt;tt&gt;updatedResult.map(_._2)&lt;/tt&gt; get check-pointed in this case, but not the &lt;tt&gt;updateStateByKey&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;If remove the &lt;tt&gt;checkpoint(Seconds(10))&lt;/tt&gt; from the map result ( &lt;tt&gt;updatedResult.map(_._2)&lt;/tt&gt; ), the stack overflow will not happen&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="12820006">SPARK-6847</key>
            <summary>Stack overflow on updateStateByKey which followed by a dstream with checkpoint set</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zsxwing">Shixiong Zhu</assignee>
                                    <reporter username="jhu">Jack Hu</reporter>
                        <labels>
                            <label>StackOverflowError</label>
                            <label>Streaming</label>
                    </labels>
                <created>Fri, 10 Apr 2015 09:50:45 +0000</created>
                <updated>Mon, 1 Feb 2016 20:05:40 +0000</updated>
                            <resolved>Mon, 1 Feb 2016 19:02:36 +0000</resolved>
                                    <version>1.3.0</version>
                    <version>1.4.1</version>
                    <version>1.5.2</version>
                    <version>1.6.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                                    <component>DStreams</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="14489413" author="srowen" created="Fri, 10 Apr 2015 11:31:14 +0000"  >&lt;p&gt;I suspect that&apos;s exactly it: &lt;tt&gt;newlist.headOption.orElse(oldstate)&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Your state has an object graph that grows without bound. Eventually it will cause a problem when, for example, you serialize the state for a checkpoint and the object graph creates an equally deeply nested method call.&lt;/p&gt;

&lt;p&gt;I think you simply should not write your state this way, as a lazy evaluation that has to hold on to old state forever. Return either new or old state immediately in the method. I don&apos;t think this is a Spark problem.&lt;/p&gt;</comment>
                            <comment id="14491448" author="srowen" created="Sun, 12 Apr 2015 12:31:10 +0000"  >&lt;p&gt;I&apos;m fairly sure this is NotAProblem, for reasons above. If there&apos;s a good and different argument otherwise then we could reopen later.&lt;/p&gt;</comment>
                            <comment id="14491873" author="jhu" created="Mon, 13 Apr 2015 03:33:53 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sowen&quot; class=&quot;user-hover&quot; rel=&quot;sowen&quot;&gt;sowen&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I tested more cases:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;only change the &lt;tt&gt;newlist.headOption.orElse(oldstate)&lt;/tt&gt; to &lt;tt&gt;Some(&quot;a&quot;)&lt;/tt&gt;, the issue still exists&lt;/li&gt;
	&lt;li&gt;only change the streaming batch interval to &lt;tt&gt;2 seconds&lt;/tt&gt;, keep the  &lt;tt&gt;newlist.headOption.orElse(oldstate)&lt;/tt&gt; and checkpoint interval 10 seconds, the issue does not exist.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;So this issue may be related to the checkpoint interval and batch interval. &lt;/p&gt;</comment>
                            <comment id="14492126" author="srowen" created="Mon, 13 Apr 2015 09:34:03 +0000"  >&lt;p&gt;Can you provide (the top part of) the stack overflow stack? so we can see where it&apos;s occurring. I think it&apos;s something building a very long object graph but that is the first step to confirm.&lt;/p&gt;</comment>
                            <comment id="14493517" author="jhu" created="Tue, 14 Apr 2015 03:44:11 +0000"  >&lt;p&gt;Here is the part of the stack (Full stack at: &lt;a href=&quot;https://gist.github.com/jhu-chang/38a6c052aff1d666b785&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/jhu-chang/38a6c052aff1d666b785&lt;/a&gt;)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;15/04/14 11:28:20 &lt;span class=&quot;error&quot;&gt;&amp;#91;Executor task launch worker-1&amp;#93;&lt;/span&gt; ERROR org.apache.spark.executor.Executor: Exception in task 1.0 in stage 27554.0 (TID 3801)&lt;br/&gt;
java.lang.StackOverflowError&lt;br/&gt;
	at java.io.ObjectStreamClass.setPrimFieldValues(ObjectStreamClass.java:1243)&lt;br/&gt;
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1984)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)&lt;br/&gt;
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)&lt;br/&gt;
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)&lt;br/&gt;
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:483)&lt;br/&gt;
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)&lt;br/&gt;
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)&lt;br/&gt;
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)&lt;br/&gt;
	at scala.collection.immutable.$colon$colon.readObject(List.scala:366)&lt;br/&gt;
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:483)&lt;br/&gt;
	at java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1017)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1896)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)&lt;br/&gt;
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)&lt;br/&gt;
	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1993)&lt;br/&gt;
	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1918)&lt;br/&gt;
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1801)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1351)&lt;br/&gt;
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:371)&lt;br/&gt;
	at scala.collection.immutable.$colon$colon.readObject(List.scala:362)&lt;br/&gt;
	at sun.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:483)&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="14493749" author="srowen" created="Tue, 14 Apr 2015 07:53:40 +0000"  >&lt;p&gt;Yeah, doesn&apos;t quite help since it is not clear where it starts, but that top may be lost.&lt;/p&gt;

&lt;p&gt;Given the observations, the problem may be putting all of the input data into one key, effectively making all RDDs one record, then checkpointing that infrequently, which means it goes to serialize a large object. &quot;Large&quot; isn&apos;t the problem but whatever it is seems to have a long object dependency graph, maybe a linked list of blocks for example. This would explain why no checkpointing or smaller intervals, could be the difference. How about also turning down the checkpoint interval?&lt;/p&gt;

&lt;p&gt;It shouldn&apos;t occur ideally but this might be pushing the intended usage a bit far by having as skewed a data distribution as possible. Does this come up in real usage? You&apos;d generally expect the data per key per interval to be smallish.&lt;/p&gt;</comment>
                            <comment id="14493804" author="jhu" created="Tue, 14 Apr 2015 08:46:50 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sowen&quot; class=&quot;user-hover&quot; rel=&quot;sowen&quot;&gt;sowen&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The checkpoint interval can not be turn down (smaller than 10 seconds) since it must be bigger or equal than the batch interval. I will try to more checkpoint interval like 20 seconds, 30 seconds...&lt;/p&gt;

&lt;p&gt;We have a real case that has the same problem, it only updates small set of values per key per interval (one event per key per interval)&lt;/p&gt;

&lt;p&gt;One observation is that: the &lt;tt&gt;updateStateByKey&lt;/tt&gt; is automatically checkpointed&lt;/p&gt;</comment>
                            <comment id="14493854" author="srowen" created="Tue, 14 Apr 2015 09:50:54 +0000"  >&lt;p&gt;I think it&apos;s worth reopening although I still don&apos;t know what the issue is, but it&apos;s not obviously a usage problem. Yes I meant turn down the batch interval too but I suppose you&apos;ve covered that case. I don&apos;t recall hearing other issues like this, and what you&apos;re describing sounds like it would affect most any use of checkpointing, so that&apos;s surprising. Still could be some issue in here but you may have to lead debugging if you&apos;re seeking a resolution. I don&apos;t have bright ideas from here.&lt;/p&gt;</comment>
                            <comment id="14495914" author="jhu" created="Wed, 15 Apr 2015 09:23:56 +0000"  >&lt;p&gt;I did a little more investigation about this issue, that appears to be a problem with some operations(&lt;tt&gt;updateStateByKey&lt;/tt&gt;, &lt;tt&gt;reduceByKeyAndWindow&lt;/tt&gt; with in-reduce function) which must be check-pointed and followed by a operation with checkpoint (either manual added like the code of this JIRA description or an operation which must be check-pointed) and the checkpoint interval of these two operation is the same (or the followed operation has a checkpoint interval the same with batch interval).&lt;br/&gt;
The following code will have this issue: assume default batch interval is 2 seconds, the default checkpoint interval is 10 seconds&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;tt&gt;source.updateStateByKey(func).map(f).checkpoint(10 seconds)&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;source.updateStateByKey(func).map(f).updateStateByKey(func2)&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;source.updateStateByKey(func).map(f).checkpoint(2 seconds)&lt;/tt&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;These DO NOT have this issue&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;tt&gt;source.updateStateByKey(func).map(f).checkpoint(4 seconds)&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;source.updateStateByKey(func).map(f).updateStateByKey(func2).checkpoint(4 seconds)&lt;/tt&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;A rdd graph which contains two rdds needs to be check-pointed would be generated from these sample codes. &lt;/p&gt;

&lt;p&gt;If the child(ren) rdd(s) also need to do the checkpoint at the same time the parent needs to do, then the parent will not do checkpoint according the &lt;tt&gt;rdd.doCheckpoint&lt;/tt&gt;. In this case, the rdd comes from &lt;tt&gt;updateStateByKey&lt;/tt&gt; will never be check-pointed at the issued sample code, that leads the stack overflow. (&lt;tt&gt;updateStateByKey&lt;/tt&gt; needs checkpoint to break the dependency in this operation) &lt;/p&gt;

&lt;p&gt;If the child(ren) rdd(s) is not always check-pointed at the same time of the parent needs to do, there is a chance that the parent rdd (comes from &lt;tt&gt;updateStateByKey&lt;/tt&gt;) can do some successful checkpoint to break the dependency, although the checkpoint may have some delay. So no stack overflow will happen.&lt;/p&gt;

&lt;p&gt;So, currently, we got a workaround of this issue by setting the checkpoint interval to different values if we use operations that must be check-pointed in streaming project. Maybe this is not a easy fix here, hope we can add some validation at least&lt;/p&gt;</comment>
                            <comment id="14950156" author="glyton.camilleri" created="Fri, 9 Oct 2015 10:00:00 +0000"  >&lt;p&gt;Hi, &lt;/p&gt;

&lt;p&gt;I&apos;ve also bumped into this very same issue but couldn&apos;t find a good value for &lt;tt&gt;checkpoint&lt;/tt&gt;; our setup consists of a kafka-stream with 10s time-window, trying various values for the checkpoint interval (default, 10s, and 15s). &lt;/p&gt;

&lt;p&gt;It always takes a long time for the exception to appear, often in the range of 10 hours or so, making the problem relatively painful to debug. We&apos;ll be trying to investigate further, but it would be great if someone could shed some more light on the issue.&lt;/p&gt;</comment>
                            <comment id="14951593" author="jhu" created="Sat, 10 Oct 2015 04:30:22 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=glyton.camilleri&quot; class=&quot;user-hover&quot; rel=&quot;glyton.camilleri&quot;&gt;glyton.camilleri&lt;/a&gt;&lt;br/&gt;
You can check whether there are two dstreams in the DAG need to be checkpointed (updateStateByKey, reduceByKeyAndWindow), it yes, you can workaround this to use some output for the previous DStream which needs to checkpointed. &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val d1 = input.updateStateByKey(func)
val d2 = d1.map(...).updateStateByKey(func)
d2.foreachRDD(rdd =&amp;gt; print(rdd.count))
&lt;span class=&quot;code-comment&quot;&gt;/// workaround the stack over flow listed in &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; JIRA
&lt;/span&gt;d1.foreachRDD(rdd =&amp;gt; rdd.foreach(_ =&amp;gt; Unit))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14971004" author="glyton.camilleri" created="Fri, 23 Oct 2015 13:47:47 +0000"  >&lt;p&gt;Hi,&lt;br/&gt;
we managed to actually get rid of the overflow issues by settings checkpoints on more streams than we thought we needed to, in addition to implementing a small change following your suggestion; before the fix, the setup was similar to what you describe:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val dStream1 = &lt;span class=&quot;code-comment&quot;&gt;// create kafka stream and &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; some preprocessing
&lt;/span&gt;val dStream2 = dStream1.updateStateByKey { func }.checkpoint(timeWindow * 2)
val dStream3 = dStream2.map { ... }

&lt;span class=&quot;code-comment&quot;&gt;// (1) perform some side-effect on the state
&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (certainConditionsAreMet) dStream2.foreachRDD { 
  _.foreachPartition { ... }
}

&lt;span class=&quot;code-comment&quot;&gt;// (2) publish &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; results to a set of Kafka topics
&lt;/span&gt;dStream3.transform { ... }.foreachRDD {
  _.foreachPartition { ... }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There were two things we did:&lt;br/&gt;
a) set different checkpoints for &lt;tt&gt;dStream2&lt;/tt&gt; and &lt;tt&gt;dStream3&lt;/tt&gt;, whereas before we were only setting the checkpoint for &lt;tt&gt;dStream2&lt;/tt&gt;&lt;br/&gt;
b) changed (1) above such then when &lt;tt&gt;!certainConditionsAreMet&lt;/tt&gt;, we just consume the stream like you describe in your suggestion&lt;/p&gt;

&lt;p&gt;I honestly think that b) was more likely to be influential in removing the StackOverflowError really, but we decided to leave the checkpoint settings in a) there anyway.&lt;br/&gt;
Apologies for the late follow-up, but we needed to make sure the issue had actually been resolved.&lt;/p&gt;</comment>
                            <comment id="15008908" author="yunjie" created="Tue, 17 Nov 2015 16:10:05 +0000"  >&lt;p&gt;Hi Glyton Camilleri &amp;amp; Jack Hu,&lt;br/&gt;
  We also ran into the same StackOverflow issue in our application, where we wrote like&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val dStream1 = context.union(kafkaStreams).updateStateByKey(updateFunc).checkpoint(Seconds(50))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  I&apos;ve read about your comments, does it mean that I can get rid of this issue by simply add an extra meaningless map() step to dStream1? Or I should do something like&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val workaroundStream = dStream1.map(...).checkpoint(Seconds(some_value_other_than_50))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;  I was confused by what certainConditionsAreMet refers to, or what kind of the content should be filled in &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;_.foreachPartition { ... }&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; so that I had to ask here for detail.&lt;/p&gt;

&lt;p&gt;Best Regards.&lt;/p&gt;</comment>
                            <comment id="15008993" author="glyton.camilleri" created="Tue, 17 Nov 2015 16:47:22 +0000"  >&lt;p&gt;Hi Yunjie,&lt;/p&gt;

&lt;p&gt;Whether 50 seconds is good or not as a checkpoint interval depends largely on the time-window the stream is acting on; so if the stream is set to execute jobs every 10 seconds, then 50 seconds could be fine.&lt;/p&gt;

&lt;p&gt;In my example code, &lt;tt&gt;certainConditionsAreMet&lt;/tt&gt; was just a place-holder: the conditions met were application-specific in that case; so in other words, there were conditions under which we would perform the side-effect on the stream, which in our case ((1) above) was saving the contents of the stream to HDFS. So the fix looked something like this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  def isTimeToSave: &lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt; = ... &lt;span class=&quot;code-comment&quot;&gt;// &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; function decides whether it&apos;s time to store the contents of the stream to HDFS
&lt;/span&gt;
  def saveData[A](stream: DStream[A]) = &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (isTimeToSave) stream.foreachRDD { 
    ... &lt;span class=&quot;code-comment&quot;&gt;// write data in HDFS
&lt;/span&gt;  } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; stream.foreachRDD { 
    _.foreachPartition { _ =&amp;gt; () } &lt;span class=&quot;code-comment&quot;&gt;// just &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; nothing 
&lt;/span&gt;  }   
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The &lt;tt&gt;else&lt;/tt&gt; part is what i&apos;m referring to above.&lt;/p&gt;</comment>
                            <comment id="15109955" author="zsxwing" created="Thu, 21 Jan 2016 03:02:44 +0000"  >&lt;p&gt;I think this one has been fixed by &lt;a href=&quot;https://github.com/apache/spark/pull/10623&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10623&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Could you try the latest codes of master or 1.6?&lt;/p&gt;</comment>
                            <comment id="15111849" author="jhu" created="Fri, 22 Jan 2016 03:22:57 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I just test a simple case with 1.6, it still exists:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;batch interval = 2 seconds
source.updateStateByKey(func).map(f).checkpoint(2 seconds)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15113490" author="zsxwing" created="Sat, 23 Jan 2016 02:32:18 +0000"  >&lt;p&gt;It has not yet been released. You need to use the master branch or 1.6 branch to test it.&lt;/p&gt;</comment>
                            <comment id="15114722" author="jhu" created="Mon, 25 Jan 2016 04:13:44 +0000"  >&lt;p&gt;Test on latest 1.6 branch (f913f7e &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-12120&quot; title=&quot;Improve exception message when failing to initialize HiveContext in PySpark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-12120&quot;&gt;&lt;del&gt;SPARK-12120&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;PYSPARK&amp;#93;&lt;/span&gt; Improve exception message when failing to init), it still exists.&lt;/p&gt;</comment>
                            <comment id="15116044" author="zsxwing" created="Mon, 25 Jan 2016 21:04:29 +0000"  >&lt;p&gt;I found the issue. The problem is in the following line:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    updatedResult.map(_._2)
      .checkpoint(Seconds(10))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Because `checkpoint` is called after `map`, only MapPartitionsRDD will do checkpoint and the state RDD in updateStateByKey will be skipped. When an RDD executes checkpointing, its dependencies (parent RDDs) will be skipped. You can just switch `map` and `checkpoint` like this:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    updatedResult.checkpoint(Seconds(10)).map(_._2)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15116046" author="zsxwing" created="Mon, 25 Jan 2016 21:06:41 +0000"  >&lt;p&gt;The user should not checkpoint DStream/RDDs after `updateStateByKey`. Otherwise, the state RDD of `updateStateByKey` cannot be checkpointed.&lt;/p&gt;</comment>
                            <comment id="15116545" author="jhu" created="Tue, 26 Jan 2016 02:43:32 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zsxwing&quot; class=&quot;user-hover&quot; rel=&quot;zsxwing&quot;&gt;zsxwing&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Even user does not implicit do checkpoint after the &lt;tt&gt;upstateByKey&lt;/tt&gt;, this issue still will happen in following cases&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;tt&gt;updateStateByKey().filter().updateStateByKey()&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;updateStateByKey().filter().reduceByKeyAndWindow(reduce, inreduce, ...)&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;reduceByKeyAndWindow(reduce,inreduce,...).filter().udateStateByKey()&lt;/tt&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;If do not plan to fix this issue, may be an implicit workaround/warning should give to user to such usage. &lt;br/&gt;
It will be very hard to find the real cause if the application is complicate. &lt;/p&gt;</comment>
                            <comment id="15117873" author="zsxwing" created="Tue, 26 Jan 2016 20:00:37 +0000"  >&lt;p&gt;Looks several operators have this issue, such as, updateStateByKey, mapWithState, reduceByKeyAndWindow. Let&apos;s try to find out a solution.&lt;/p&gt;</comment>
                            <comment id="15118335" author="apachespark" created="Wed, 27 Jan 2016 00:25:06 +0000"  >&lt;p&gt;User &apos;zsxwing&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/10934&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10934&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15126928" author="zsxwing" created="Mon, 1 Feb 2016 20:05:40 +0000"  >&lt;p&gt;As my PR changed internal semantics, it&apos;s only merged to master branch (2.0.0). &lt;/p&gt;

&lt;p&gt;For pre 2.0.0, you may need to trigger the checkpoint by yourself. E.g., for &lt;tt&gt;updateStateByKey().filter().updateStateByKey()&lt;/tt&gt;, you can update to &lt;tt&gt;dstream.updateStateByKey().count(); dstream.updateStateByKey().filter().updateStateByKey()&lt;/tt&gt; to trigger the checkpoint for the first &quot;updateStateByKey&quot;.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 42 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2d3an:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12329449">2.0.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>