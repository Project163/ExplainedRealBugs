<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:03:14 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-25271] Creating parquet table with all the column null throws exception</title>
                <link>https://issues.apache.org/jira/browse/SPARK-25271</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
 1)cat /data/parquet.dat

1$abc2$pqr:3$xyz
&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2)spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table vp_reader_temp (projects map&amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;, string&amp;gt;) ROW FORMAT DELIMITED FIELDS TERMINATED BY &lt;span class=&quot;code-quote&quot;&gt;&apos;,&apos;&lt;/span&gt; COLLECTION ITEMS TERMINATED BY &lt;span class=&quot;code-quote&quot;&gt;&apos;:&apos;&lt;/span&gt; MAP KEYS TERMINATED BY &lt;span class=&quot;code-quote&quot;&gt;&apos;$&apos;&lt;/span&gt;&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
3)spark.sql(&quot;
LOAD DATA LOCAL INPATH&#160;&lt;span class=&quot;code-quote&quot;&gt;&apos;/data/parquet.dat&apos;&lt;/span&gt; INTO TABLE vp_reader_temp&quot;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
4)spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table vp_reader STORED AS PARQUET as select * from vp_reader_temp&quot;&lt;/span&gt;)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;&lt;b&gt;Result :&lt;/b&gt; Throwing exception (Working fine with spark 2.2.1)&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
java.lang.RuntimeException: Parquet record is malformed: empty fields are illegal, the field should be ommited completely instead
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:64)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:59)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:31)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:123)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:180)
	at org.apache.parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:46)
	at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:112)
	at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:125)
	at org.apache.spark.sql.hive.execution.HiveOutputWriter.write(HiveFileFormat.scala:149)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:406)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:283)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:281)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1438)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:286)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:211)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:210)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:109)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:349)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: org.apache.parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead
	at org.apache.parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endField(MessageColumnIO.java:320)
	at org.apache.parquet.io.RecordConsumerLoggingWrapper.endField(RecordConsumerLoggingWrapper.java:165)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeMap(DataWritableWriter.java:241)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeValue(DataWritableWriter.java:116)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeGroupFields(DataWritableWriter.java:89)
	at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:60)
	... 21 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13181915">SPARK-25271</key>
            <summary>Creating parquet table with all the column null throws exception</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="viirya">L. C. Hsieh</assignee>
                                    <reporter username="shivusondur@gmail.com">Shivu Sondur</reporter>
                        <labels>
                    </labels>
                <created>Wed, 29 Aug 2018 13:08:50 +0000</created>
                <updated>Mon, 12 Dec 2022 18:10:52 +0000</updated>
                            <resolved>Thu, 20 Dec 2018 02:50:47 +0000</resolved>
                                    <version>2.3.1</version>
                                    <fixVersion>2.4.8</fixVersion>
                    <fixVersion>3.0.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>12</watches>
                                                                                                                <comments>
                            <comment id="16596300" author="shivusondur@gmail.com" created="Wed, 29 Aug 2018 13:10:33 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16596949" author="viirya" created="Thu, 30 Aug 2018 00:07:54 +0000"  >&lt;p&gt;I think this is a known issue on Hive and Parquet, some context can be found at &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-11625&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-11625&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;It can be reproduced by:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table vp_reader STORED AS PARQUET as select map() as a&quot;&lt;/span&gt;)
18/08/30 00:07:15 ERROR DataWritableWriter: Parquet record is malformed: empty fields are illegal, the field should be ommited completely instead
parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead                                          
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If you don&apos;t store it as Parquet format, it can work:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table vp_reader STORED AS ORC as select map() as a&quot;&lt;/span&gt;)
sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from vp_reader&quot;&lt;/span&gt;).show
+---+
|  a|
+---+
| []|
+---+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16597134" author="shivusondur@gmail.com" created="Thu, 30 Aug 2018 07:17:42 +0000"  >&lt;p&gt;&#160;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=viirya&quot; class=&quot;user-hover&quot; rel=&quot;viirya&quot;&gt;viirya&lt;/a&gt;, It works fine in Spark 2.2.1 version. Below is the details&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
c:\spark-2.2.1-bin-hadoop2.7&amp;gt;bin\spark-shell
 Using Spark&apos;s &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; log4j profile: org/apache/spark/log4j-defaults.properties
 Setting &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; log level to &lt;span class=&quot;code-quote&quot;&gt;&quot;WARN&quot;&lt;/span&gt;.
 To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
 Spark context Web UI available at http:&lt;span class=&quot;code-comment&quot;&gt;//localhost:4040
&lt;/span&gt; Spark context available as &lt;span class=&quot;code-quote&quot;&gt;&apos;sc&apos;&lt;/span&gt; (master = local[*], app id = local-1535611823064).
 Spark session available as &lt;span class=&quot;code-quote&quot;&gt;&apos;spark&apos;&lt;/span&gt;.
 Welcome to
 ____ __
 / _/_ ___ ____/ /_
 \ \/ _ \/ _ `/ __/ &apos;/
 /__/ ./_,&lt;span class=&quot;code-comment&quot;&gt;// //_\ version 2.2.1
&lt;/span&gt; /_/
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_60)
 Type in expressions to have them evaluated.
 Type :help &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; more information.
scala&amp;gt; spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table vp_reader_temp (projects map&amp;lt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;, string&amp;gt;) ROW FORMAT DELIMITED FIELDS TERMINATED BY &lt;span class=&quot;code-quote&quot;&gt;&apos;,&apos;&lt;/span&gt; COLLECTION ITEMS TERMINATED BY &lt;span class=&quot;code-quote&quot;&gt;&apos;:&apos;&lt;/span&gt; MAP KEYS TERMINATED BY &lt;span class=&quot;code-quote&quot;&gt;&apos;$&apos;&lt;/span&gt;&quot;&lt;/span&gt;)
 res0: org.apache.spark.sql.DataFrame = []
scala&amp;gt; spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;LOAD DATA LOCAL INPATH &lt;span class=&quot;code-quote&quot;&gt;&apos;parquetReader&apos;&lt;/span&gt; INTO TABLE vp_reader_temp&quot;&lt;/span&gt;)
 res1: org.apache.spark.sql.DataFrame = []
scala&amp;gt; spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table vp_reader STORED AS PARQUET as select * from vp_reader_temp&quot;&lt;/span&gt;)
 res2: org.apache.spark.sql.DataFrame = []
scala&amp;gt; spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from vp_reader&quot;&lt;/span&gt;).collect
 res3: Array[org.apache.spark.sql.Row] = Array([Map(1 -&amp;gt; abc, 2 -&amp;gt; pqr, 3 -&amp;gt; xyz)], [Map()])
scala&amp;gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16603106" author="shivusondur@gmail.com" created="Tue, 4 Sep 2018 14:25:30 +0000"  >&lt;p&gt;After further analyzing the issue i got following&#160;details&lt;/p&gt;

&lt;p&gt;In &#160;SingleDirectoryWriteTask private class(org.apache.spark.sql.execution.datasources.FileFormatWriter File) , currentWriter is&#160; initialized with different outputWriter in spark-2.2.1 and spar-2.3.1, as shown below.&#160;&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Spark-2.3.1= currentWriter is initilized with &lt;span class=&quot;code-quote&quot;&gt;&quot;HiveOutputWriter&quot;&lt;/span&gt;
Spark-2.2.1= currentWriter is initilized with &lt;span class=&quot;code-quote&quot;&gt;&quot;ParquetOutputWriter&quot;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;So&#160;ParquetOutputWriter may be handling the null/empty values.&lt;/p&gt;</comment>
                            <comment id="16603126" author="s71955" created="Tue, 4 Sep 2018 14:41:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sowen&quot; class=&quot;user-hover&quot; rel=&quot;sowen&quot;&gt;sowen&lt;/a&gt;&#160; Will this cause a compatibility problem compare to older version, If user has&#160; null record ,then he is getting an exception with the current version where as the older version of spark(2.2.1)&#160; wont throw any exception.&lt;/p&gt;

&lt;p&gt;I think the Output writers has been updated in the below PR&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/20521&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20521&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16603128" author="s71955" created="Tue, 4 Sep 2018 14:41:48 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16605453" author="gurwls223" created="Thu, 6 Sep 2018 08:21:18 +0000"  >&lt;p&gt;will take a look later but mind if I ask to elaborate&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think the Output writers has been updated in the below PR&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/spark/pull/20521&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20521&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;? Sounds rather a corner case but still a regression.&lt;/p&gt;</comment>
                            <comment id="16606693" author="shivusondur@gmail.com" created="Fri, 7 Sep 2018 04:13:40 +0000"  >&lt;p&gt;As &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=S71955&quot; class=&quot;user-hover&quot; rel=&quot;S71955&quot;&gt;S71955&lt;/a&gt; told,&#160;The Behaviour changed from the &lt;a href=&quot;https://github.com/apache/spark/pull/20521&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20521&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;While debugging &quot;spark.sql(&quot;create table vp_reader STORED AS PARQUET as select * from vp_reader_temp&quot;)&quot;&#160; &#160; I&#160;found following&#160; details.&lt;br/&gt;
&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
In Spark-2.2.1 It is using the &lt;span class=&quot;code-quote&quot;&gt;&quot;InsertIntoTable&quot;&lt;/span&gt;(org.apache.spark.sql.hive.execution.createHiveTableAsSelectCommand.run()) which will use ParquetFileFormat as shown below snaps&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;1 Figure: It uses  &quot;InsertIntoTable&quot; for plan generation&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12938757/12938757_image-2018-09-07-09-29-33-370.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;       &lt;/p&gt;

&lt;p&gt;2 Figure: It is using the &quot;ParquetFileFormat&quot;  as fileformat&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12938758/12938758_image-2018-09-07-09-29-52-899.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
But in Spark-2.3.1,&#160;It is using the &lt;span class=&quot;code-quote&quot;&gt;&quot;InsertIntoHiveTable&quot;&lt;/span&gt;,&#160;(org.apache.spark.sql.hive.execution.createHiveTableAsSelectCommand.run()) which will use&#160;HiveFileFormat&#160;as shown below snap&apos;s&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&#160;&lt;br/&gt;
3 Figure: It uses &quot;InsertIntoHiveTable&quot; for plan generation&lt;br/&gt;
&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12938760/12938760_image-2018-09-07-09-32-43-892.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;


&lt;p&gt;4 Figure: It is using the &quot;HiveFileFormat&quot; as fileformat&lt;br/&gt;
&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12938761/12938761_image-2018-09-07-09-33-03-095.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt; Let me know any further clarification&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16611535" author="viirya" created="Wed, 12 Sep 2018 03:37:50 +0000"  >&lt;p&gt;Yeah, looks like after some changes, this kind of queries now uses Hive&apos;s record writer. So it inherits the issue in Hive.&lt;/p&gt;</comment>
                            <comment id="16623133" author="apachespark" created="Fri, 21 Sep 2018 06:27:12 +0000"  >&lt;p&gt;User &apos;viirya&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/22514&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16716496" author="githubbot" created="Tue, 11 Dec 2018 08:11:48 +0000"  >&lt;p&gt;viirya commented on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446109671&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446109671&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Synced with master.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716498" author="githubbot" created="Tue, 11 Dec 2018 08:12:12 +0000"  >&lt;p&gt;SparkQA commented on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446109785&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446109785&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   **&lt;a href=&quot;#99958 has started&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Test build #99958 has started&lt;/a&gt;(&lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99958/testReport)**&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99958/testReport)**&lt;/a&gt; for PR 22514 at commit &lt;span class=&quot;error&quot;&gt;&amp;#91;`ef52536`&amp;#93;&lt;/span&gt;(&lt;a href=&quot;https://github.com/apache/spark/commit/ef5253662442504718c74522764f279387b1b217&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/ef5253662442504718c74522764f279387b1b217&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716506" author="githubbot" created="Tue, 11 Dec 2018 08:17:49 +0000"  >&lt;p&gt;AmplabJenkins commented on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446111230&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446111230&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Merged build finished. Test PASSed.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716507" author="githubbot" created="Tue, 11 Dec 2018 08:17:50 +0000"  >&lt;p&gt;AmplabJenkins commented on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446111236&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446111236&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Test PASSed.&lt;br/&gt;
   Refer to this link for build results (access rights to CI server needed): &lt;br/&gt;
   &lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5961/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5961/&lt;/a&gt;&lt;br/&gt;
   Test PASSed.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716508" author="githubbot" created="Tue, 11 Dec 2018 08:18:10 +0000"  >&lt;p&gt;AmplabJenkins removed a comment on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446111230&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446111230&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Merged build finished. Test PASSed.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716509" author="githubbot" created="Tue, 11 Dec 2018 08:18:10 +0000"  >&lt;p&gt;AmplabJenkins removed a comment on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446111236&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446111236&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Test PASSed.&lt;br/&gt;
   Refer to this link for build results (access rights to CI server needed): &lt;br/&gt;
   &lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5961/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://amplab.cs.berkeley.edu/jenkins//job/testing-k8s-prb-make-spark-distribution-unified/5961/&lt;/a&gt;&lt;br/&gt;
   Test PASSed.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716540" author="githubbot" created="Tue, 11 Dec 2018 08:27:49 +0000"  >&lt;p&gt;cloud-fan commented on a change in pull request #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#discussion_r240510001&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#discussion_r240510001&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -97,9 +77,118 @@ case class CreateHiveTableAsSelectCommand(&lt;br/&gt;
     Seq.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  // Returns `DataWritingCommand` used to write data when the table exists.&lt;br/&gt;
+  def writingCommandForExistingTable(&lt;br/&gt;
+    catalog: SessionCatalog,&lt;br/&gt;
+    tableDesc: CatalogTable): DataWritingCommand&lt;br/&gt;
+&lt;br/&gt;
+  // Returns `DataWritingCommand` used to write data when the table doesn&apos;t exist.&lt;br/&gt;
+  def writingCommandForNewTable(&lt;br/&gt;
+    catalog: SessionCatalog,&lt;br/&gt;
+    tableDesc: CatalogTable): DataWritingCommand&lt;br/&gt;
+&lt;br/&gt;
   override def argString: String = {&lt;br/&gt;
     s&quot;[Database:${tableDesc.database}, &quot; +&lt;br/&gt;
     s&quot;TableName: ${tableDesc.identifier.table}, &quot; +&lt;br/&gt;
     s&quot;InsertIntoHiveTable]&quot;&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Create table and insert the query result into it.&lt;br/&gt;
+ *&lt;br/&gt;
+ * @param tableDesc the table description, which may contain serde, storage handler etc.&lt;br/&gt;
+ * @param query the query whose result will be insert into the new relation&lt;br/&gt;
+ * @param mode SaveMode&lt;br/&gt;
+ */&lt;br/&gt;
+case class CreateHiveTableAsSelectCommand(&lt;br/&gt;
+    tableDesc: CatalogTable,&lt;br/&gt;
+    query: LogicalPlan,&lt;br/&gt;
+    outputColumnNames: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    mode: SaveMode)&lt;br/&gt;
+  extends CreateHiveTableAsSelectBase {&lt;br/&gt;
+&lt;br/&gt;
+  override def writingCommandForExistingTable(&lt;br/&gt;
+      catalog: SessionCatalog,&lt;br/&gt;
+      tableDesc: CatalogTable): DataWritingCommand = {&lt;br/&gt;
+    // For CTAS, there is no static partition values to insert.&lt;br/&gt;
+    val partition = tableDesc.partitionColumnNames.map(_ -&amp;gt; None).toMap&lt;br/&gt;
+    InsertIntoHiveTable(&lt;br/&gt;
+      tableDesc,&lt;br/&gt;
+      partition,&lt;br/&gt;
+      query,&lt;br/&gt;
+      overwrite = false,&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   so the only difference is this parameter?&lt;/p&gt;

&lt;p&gt;   Now I feel maybe it&apos;s better to define only one `getWritingCommand` in `CreateHiveTableAsSelectBase`.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716544" author="githubbot" created="Tue, 11 Dec 2018 08:31:26 +0000"  >&lt;p&gt;cloud-fan commented on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446114955&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446114955&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   To be safe, let&apos;s add a `HiveUtils.CONVERT_METASTORE_CTAS` with default value true in this PR. It&apos;s also a good practice to have fine-grained optimization flags. I think migration guide is not needed here.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716557" author="githubbot" created="Tue, 11 Dec 2018 08:34:30 +0000"  >&lt;p&gt;viirya commented on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446115800&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446115800&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   I see, we have discussed before. Is it good to add it here or a follow-up?&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716560" author="githubbot" created="Tue, 11 Dec 2018 08:35:22 +0000"  >&lt;p&gt;viirya commented on a change in pull request #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#discussion_r240512341&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#discussion_r240512341&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -97,9 +77,118 @@ case class CreateHiveTableAsSelectCommand(&lt;br/&gt;
     Seq.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt;&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  // Returns `DataWritingCommand` used to write data when the table exists.&lt;br/&gt;
+  def writingCommandForExistingTable(&lt;br/&gt;
+    catalog: SessionCatalog,&lt;br/&gt;
+    tableDesc: CatalogTable): DataWritingCommand&lt;br/&gt;
+&lt;br/&gt;
+  // Returns `DataWritingCommand` used to write data when the table doesn&apos;t exist.&lt;br/&gt;
+  def writingCommandForNewTable(&lt;br/&gt;
+    catalog: SessionCatalog,&lt;br/&gt;
+    tableDesc: CatalogTable): DataWritingCommand&lt;br/&gt;
+&lt;br/&gt;
   override def argString: String = {&lt;br/&gt;
     s&quot;[Database:${tableDesc.database}, &quot; +&lt;br/&gt;
     s&quot;TableName: ${tableDesc.identifier.table}, &quot; +&lt;br/&gt;
     s&quot;InsertIntoHiveTable]&quot;&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Create table and insert the query result into it.&lt;br/&gt;
+ *&lt;br/&gt;
+ * @param tableDesc the table description, which may contain serde, storage handler etc.&lt;br/&gt;
+ * @param query the query whose result will be insert into the new relation&lt;br/&gt;
+ * @param mode SaveMode&lt;br/&gt;
+ */&lt;br/&gt;
+case class CreateHiveTableAsSelectCommand(&lt;br/&gt;
+    tableDesc: CatalogTable,&lt;br/&gt;
+    query: LogicalPlan,&lt;br/&gt;
+    outputColumnNames: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    mode: SaveMode)&lt;br/&gt;
+  extends CreateHiveTableAsSelectBase {&lt;br/&gt;
+&lt;br/&gt;
+  override def writingCommandForExistingTable(&lt;br/&gt;
+      catalog: SessionCatalog,&lt;br/&gt;
+      tableDesc: CatalogTable): DataWritingCommand = {&lt;br/&gt;
+    // For CTAS, there is no static partition values to insert.&lt;br/&gt;
+    val partition = tableDesc.partitionColumnNames.map(_ -&amp;gt; None).toMap&lt;br/&gt;
+    InsertIntoHiveTable(&lt;br/&gt;
+      tableDesc,&lt;br/&gt;
+      partition,&lt;br/&gt;
+      query,&lt;br/&gt;
+      overwrite = false,&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   ha, ok.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716566" author="githubbot" created="Tue, 11 Dec 2018 08:40:19 +0000"  >&lt;p&gt;cloud-fan commented on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446117432&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446117432&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Seems like a trivial change, let&apos;s do it in this PR.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716980" author="githubbot" created="Tue, 11 Dec 2018 12:12:42 +0000"  >&lt;p&gt;SparkQA commented on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446181366&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446181366&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   **&lt;a href=&quot;#99958 has finished&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Test build #99958 has finished&lt;/a&gt;(&lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99958/testReport)**&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99958/testReport)**&lt;/a&gt; for PR 22514 at commit &lt;span class=&quot;error&quot;&gt;&amp;#91;`ef52536`&amp;#93;&lt;/span&gt;(&lt;a href=&quot;https://github.com/apache/spark/commit/ef5253662442504718c74522764f279387b1b217&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/ef5253662442504718c74522764f279387b1b217&lt;/a&gt;).&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;This patch passes all tests.&lt;/li&gt;
	&lt;li&gt;This patch merges cleanly.&lt;/li&gt;
	&lt;li&gt;This patch adds the following public classes &lt;em&gt;(experimental)&lt;/em&gt;:&lt;/li&gt;
	&lt;li&gt;`sealed trait SingleValueExecutorMetricType extends ExecutorMetricType `&lt;/li&gt;
	&lt;li&gt;`class GBTClassifierParams(GBTParams, HasVarianceImpurity):`&lt;/li&gt;
	&lt;li&gt;`class GBTClassifier(JavaEstimator, HasFeaturesCol, HasLabelCol, HasPredictionCol,`&lt;/li&gt;
	&lt;li&gt;`class HasDistanceMeasure(Params):`&lt;/li&gt;
	&lt;li&gt;`class HasValidationIndicatorCol(Params):`&lt;/li&gt;
	&lt;li&gt;`class HasVarianceImpurity(Params):`&lt;/li&gt;
	&lt;li&gt;`class TreeRegressorParams(HasVarianceImpurity):`&lt;/li&gt;
	&lt;li&gt;`class GBTParams(TreeEnsembleParams, HasMaxIter, HasStepSize, HasValidationIndicatorCol):`&lt;/li&gt;
	&lt;li&gt;`class GBTRegressorParams(GBTParams, TreeRegressorParams):`&lt;/li&gt;
	&lt;li&gt;`class GBTRegressor(JavaEstimator, HasFeaturesCol, HasLabelCol, HasPredictionCol,`&lt;/li&gt;
	&lt;li&gt;`class ArrowCollectSerializer(Serializer):`&lt;/li&gt;
	&lt;li&gt;`class CSVInferSchema(val options: CSVOptions) extends Serializable `&lt;/li&gt;
	&lt;li&gt;`class InterpretedSafeProjection(expressions: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Expression&amp;#93;&lt;/span&gt;) extends Projection `&lt;/li&gt;
	&lt;li&gt;`sealed trait DateTimeFormatter `&lt;/li&gt;
	&lt;li&gt;`class Iso8601DateTimeFormatter(`&lt;/li&gt;
	&lt;li&gt;`class LegacyDateTimeFormatter(`&lt;/li&gt;
	&lt;li&gt;`class LegacyFallbackDateTimeFormatter(`&lt;/li&gt;
	&lt;li&gt;`sealed trait DateFormatter `&lt;/li&gt;
	&lt;li&gt;`class Iso8601DateFormatter(`&lt;/li&gt;
	&lt;li&gt;`class LegacyDateFormatter(`&lt;/li&gt;
	&lt;li&gt;`class LegacyFallbackDateFormatter(`&lt;/li&gt;
	&lt;li&gt;`case class ArrowEvalPython(`&lt;/li&gt;
	&lt;li&gt;`case class BatchEvalPython(`&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716981" author="githubbot" created="Tue, 11 Dec 2018 12:13:11 +0000"  >&lt;p&gt;SparkQA removed a comment on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446109785&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446109785&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   **&lt;a href=&quot;#99958 has started&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Test build #99958 has started&lt;/a&gt;(&lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99958/testReport)**&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/99958/testReport)**&lt;/a&gt; for PR 22514 at commit &lt;span class=&quot;error&quot;&gt;&amp;#91;`ef52536`&amp;#93;&lt;/span&gt;(&lt;a href=&quot;https://github.com/apache/spark/commit/ef5253662442504718c74522764f279387b1b217&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/commit/ef5253662442504718c74522764f279387b1b217&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716982" author="githubbot" created="Tue, 11 Dec 2018 12:14:34 +0000"  >&lt;p&gt;AmplabJenkins commented on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446181832&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446181832&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Merged build finished. Test PASSed.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716983" author="githubbot" created="Tue, 11 Dec 2018 12:14:35 +0000"  >&lt;p&gt;AmplabJenkins commented on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446181838&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446181838&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Test PASSed.&lt;br/&gt;
   Refer to this link for build results (access rights to CI server needed): &lt;br/&gt;
   &lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99958/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99958/&lt;/a&gt;&lt;br/&gt;
   Test PASSed.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716989" author="githubbot" created="Tue, 11 Dec 2018 12:15:09 +0000"  >&lt;p&gt;AmplabJenkins removed a comment on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446181832&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446181832&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Merged build finished. Test PASSed.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16716990" author="githubbot" created="Tue, 11 Dec 2018 12:15:11 +0000"  >&lt;p&gt;AmplabJenkins removed a comment on issue #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514#issuecomment-446181838&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514#issuecomment-446181838&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Test PASSed.&lt;br/&gt;
   Refer to this link for build results (access rights to CI server needed): &lt;br/&gt;
   &lt;a href=&quot;https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99958/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://amplab.cs.berkeley.edu/jenkins//job/SparkPullRequestBuilder/99958/&lt;/a&gt;&lt;br/&gt;
   Test PASSed.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16725531" author="cloud_fan" created="Thu, 20 Dec 2018 02:50:47 +0000"  >&lt;p&gt;Issue resolved by pull request 22514&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/22514&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16725532" author="githubbot" created="Thu, 20 Dec 2018 02:50:57 +0000"  >&lt;p&gt;asfgit closed pull request #22514: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;SQL&amp;#93;&lt;/span&gt; Hive ctas commands should use data source if it is convertible&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/spark/pull/22514&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/22514&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala&lt;br/&gt;
index e1faecedd20ed..096481f68275d 100644&lt;br/&gt;
&amp;#8212; a/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala&lt;br/&gt;
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/command/ddl.scala&lt;br/&gt;
@@ -820,6 +820,14 @@ object DDLUtils &lt;/p&gt;
{
     table.provider.isDefined &amp;amp;&amp;amp; table.provider.get.toLowerCase(Locale.ROOT) != HIVE_PROVIDER
   }

&lt;p&gt;+  def readHiveTable(table: CatalogTable): HiveTableRelation = &lt;/p&gt;
{
+    HiveTableRelation(
+      table,
+      // Hive table columns are always nullable.
+      table.dataSchema.asNullable.toAttributes,
+      table.partitionSchema.asNullable.toAttributes)
+  }
&lt;p&gt;+&lt;br/&gt;
   /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Throws a standard error for actions that require partitionProvider = hive.&lt;br/&gt;
    */&lt;br/&gt;
diff --git a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala&lt;br/&gt;
index b304e2da6e1cf..b5cf8c9515bfb 100644
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;a/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala&lt;br/&gt;
+++ b/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala&lt;br/&gt;
@@ -244,27 +244,19 @@ class FindDataSourceTable(sparkSession: SparkSession) extends Rule&lt;span class=&quot;error&quot;&gt;&amp;#91;LogicalPlan&amp;#93;&lt;/span&gt;&lt;br/&gt;
     })&lt;br/&gt;
   }&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;private def readHiveTable(table: CatalogTable): LogicalPlan = 
{
-    HiveTableRelation(
-      table,
-      // Hive table columns are always nullable.
-      table.dataSchema.asNullable.toAttributes,
-      table.partitionSchema.asNullable.toAttributes)
-  }
&lt;p&gt;-&lt;br/&gt;
   override def apply(plan: LogicalPlan): LogicalPlan = plan resolveOperators &lt;/p&gt;
{
     case i @ InsertIntoTable(UnresolvedCatalogRelation(tableMeta), _, _, _, _)
         if DDLUtils.isDatasourceTable(tableMeta) =&amp;gt;
       i.copy(table = readDataSourceTable(tableMeta))
 
     case i @ InsertIntoTable(UnresolvedCatalogRelation(tableMeta), _, _, _, _) =&amp;gt;
-      i.copy(table = readHiveTable(tableMeta))
+      i.copy(table = DDLUtils.readHiveTable(tableMeta))
 
     case UnresolvedCatalogRelation(tableMeta) if DDLUtils.isDatasourceTable(tableMeta) =&amp;gt;
       readDataSourceTable(tableMeta)
 
     case UnresolvedCatalogRelation(tableMeta) =&amp;gt;
-      readHiveTable(tableMeta)
+      DDLUtils.readHiveTable(tableMeta)
   }
&lt;p&gt; }&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala&lt;br/&gt;
index 5823548a8063c..03f4b8d83e353 100644&lt;br/&gt;
&amp;#8212; a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala&lt;br/&gt;
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveMetastoreCatalog.scala&lt;br/&gt;
@@ -17,6 +17,8 @@&lt;/p&gt;

&lt;p&gt; package org.apache.spark.sql.hive&lt;/p&gt;

&lt;p&gt;+import java.util.Locale&lt;br/&gt;
+&lt;br/&gt;
 import scala.util.control.NonFatal&lt;/p&gt;

&lt;p&gt; import com.google.common.util.concurrent.Striped&lt;br/&gt;
@@ -29,6 +31,8 @@ import org.apache.spark.sql.catalyst.&lt;/p&gt;
{QualifiedTableName, TableIdentifier}
&lt;p&gt; import org.apache.spark.sql.catalyst.catalog._&lt;br/&gt;
 import org.apache.spark.sql.catalyst.plans.logical._&lt;br/&gt;
 import org.apache.spark.sql.execution.datasources._&lt;br/&gt;
+import org.apache.spark.sql.execution.datasources.parquet.&lt;/p&gt;
{ParquetFileFormat, ParquetOptions}&lt;br/&gt;
+import org.apache.spark.sql.internal.SQLConf&lt;br/&gt;
 import org.apache.spark.sql.internal.SQLConf.HiveCaseSensitiveInferenceMode._&lt;br/&gt;
 import org.apache.spark.sql.types._&lt;br/&gt;
 &lt;br/&gt;
@@ -113,7 +117,44 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;hive&amp;#93;&lt;/span&gt; class HiveMetastoreCatalog(sparkSession: SparkSession) extends Log&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
 &lt;br/&gt;
-  def convertToLogicalRelation(&lt;br/&gt;
+  // Return true for Apache ORC and Hive ORC-related configuration names.&lt;br/&gt;
+  // Note that Spark doesn&apos;t support configurations like `hive.merge.orcfile.stripe.level`.&lt;br/&gt;
+  private def isOrcProperty(key: String) =&lt;br/&gt;
+    key.startsWith(&quot;orc.&quot;) || key.contains(&quot;.orc.&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  private def isParquetProperty(key: String) =&lt;br/&gt;
+    key.startsWith(&quot;parquet.&quot;) || key.contains(&quot;.parquet.&quot;)&lt;br/&gt;
+&lt;br/&gt;
+  def convert(relation: HiveTableRelation): LogicalRelation = {&lt;br/&gt;
+    val serde = relation.tableMeta.storage.serde.getOrElse(&quot;&quot;).toLowerCase(Locale.ROOT)&lt;br/&gt;
+&lt;br/&gt;
+    // Consider table and storage properties. For properties existing in both sides, storage&lt;br/&gt;
+    // properties will supersede table properties.&lt;br/&gt;
+    if (serde.contains(&quot;parquet&quot;)) {
+      val options = relation.tableMeta.properties.filterKeys(isParquetProperty) ++
+        relation.tableMeta.storage.properties + (ParquetOptions.MERGE_SCHEMA -&amp;gt;
+        SQLConf.get.getConf(HiveUtils.CONVERT_METASTORE_PARQUET_WITH_SCHEMA_MERGING).toString)
+        convertToLogicalRelation(relation, options, classOf[ParquetFileFormat], &quot;parquet&quot;)
+    } else {&lt;br/&gt;
+      val options = relation.tableMeta.properties.filterKeys(isOrcProperty) ++&lt;br/&gt;
+        relation.tableMeta.storage.properties&lt;br/&gt;
+      if (SQLConf.get.getConf(SQLConf.ORC_IMPLEMENTATION) == &quot;native&quot;) {
+        convertToLogicalRelation(
+          relation,
+          options,
+          classOf[org.apache.spark.sql.execution.datasources.orc.OrcFileFormat],
+          &quot;orc&quot;)
+      } else {
+        convertToLogicalRelation(
+          relation,
+          options,
+          classOf[org.apache.spark.sql.hive.orc.OrcFileFormat],
+          &quot;orc&quot;)
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;br/&gt;
+&lt;br/&gt;
+  private def convertToLogicalRelation(&lt;br/&gt;
       relation: HiveTableRelation,&lt;br/&gt;
       options: Map&lt;span class=&quot;error&quot;&gt;&amp;#91;String, String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
       fileFormatClass: Class&lt;span class=&quot;error&quot;&gt;&amp;#91;_ &amp;lt;: FileFormat&amp;#93;&lt;/span&gt;,&lt;br/&gt;
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala&lt;br/&gt;
index 07ee105404311..8a5ab188a949f 100644&lt;br/&gt;
&amp;#8212; a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala&lt;br/&gt;
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveStrategies.scala&lt;br/&gt;
@@ -31,8 +31,7 @@ import org.apache.spark.sql.catalyst.plans.logical.{InsertIntoDir, InsertIntoTab&lt;br/&gt;
 import org.apache.spark.sql.catalyst.rules.Rule&lt;br/&gt;
 import org.apache.spark.sql.execution._&lt;br/&gt;
 import org.apache.spark.sql.execution.command.{CreateTableCommand, DDLUtils}&lt;br/&gt;
-import org.apache.spark.sql.execution.datasources.{CreateTable, LogicalRelation}&lt;br/&gt;
-import org.apache.spark.sql.execution.datasources.parquet.{ParquetFileFormat, ParquetOptions}
&lt;p&gt;+import org.apache.spark.sql.execution.datasources.CreateTable&lt;br/&gt;
 import org.apache.spark.sql.hive.execution._&lt;br/&gt;
 import org.apache.spark.sql.internal.&lt;/p&gt;
{HiveSerDe, SQLConf}

&lt;p&gt;@@ -181,49 +180,17 @@ case class RelationConversions(&lt;br/&gt;
     conf: SQLConf,&lt;br/&gt;
     sessionCatalog: HiveSessionCatalog) extends Rule&lt;span class=&quot;error&quot;&gt;&amp;#91;LogicalPlan&amp;#93;&lt;/span&gt; {&lt;br/&gt;
   private def isConvertible(relation: HiveTableRelation): Boolean = &lt;/p&gt;
{
-    val serde = relation.tableMeta.storage.serde.getOrElse(&quot;&quot;).toLowerCase(Locale.ROOT)
-    serde.contains(&quot;parquet&quot;) &amp;amp;&amp;amp; conf.getConf(HiveUtils.CONVERT_METASTORE_PARQUET) ||
-      serde.contains(&quot;orc&quot;) &amp;amp;&amp;amp; conf.getConf(HiveUtils.CONVERT_METASTORE_ORC)
+    isConvertible(relation.tableMeta)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// Return true for Apache ORC and Hive ORC-related configuration names.&lt;/li&gt;
	&lt;li&gt;// Note that Spark doesn&apos;t support configurations like `hive.merge.orcfile.stripe.level`.&lt;/li&gt;
	&lt;li&gt;private def isOrcProperty(key: String) =&lt;/li&gt;
	&lt;li&gt;key.startsWith(&quot;orc.&quot;) || key.contains(&quot;.orc.&quot;)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private def isParquetProperty(key: String) =&lt;/li&gt;
	&lt;li&gt;key.startsWith(&quot;parquet.&quot;) || key.contains(&quot;.parquet.&quot;)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private def convert(relation: HiveTableRelation): LogicalRelation = {&lt;/li&gt;
	&lt;li&gt;val serde = relation.tableMeta.storage.serde.getOrElse(&quot;&quot;).toLowerCase(Locale.ROOT)&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;// Consider table and storage properties. For properties existing in both sides, storage&lt;/li&gt;
	&lt;li&gt;// properties will supersede table properties.&lt;/li&gt;
	&lt;li&gt;if (serde.contains(&quot;parquet&quot;)) 
{
-      val options = relation.tableMeta.properties.filterKeys(isParquetProperty) ++
-        relation.tableMeta.storage.properties + (ParquetOptions.MERGE_SCHEMA -&amp;gt;
-        conf.getConf(HiveUtils.CONVERT_METASTORE_PARQUET_WITH_SCHEMA_MERGING).toString)
-      sessionCatalog.metastoreCatalog
-        .convertToLogicalRelation(relation, options, classOf[ParquetFileFormat], &quot;parquet&quot;)
-    }
&lt;p&gt; else {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;val options = relation.tableMeta.properties.filterKeys(isOrcProperty) ++&lt;/li&gt;
	&lt;li&gt;relation.tableMeta.storage.properties&lt;/li&gt;
	&lt;li&gt;if (conf.getConf(SQLConf.ORC_IMPLEMENTATION) == &quot;native&quot;) 
{
-        sessionCatalog.metastoreCatalog.convertToLogicalRelation(
-          relation,
-          options,
-          classOf[org.apache.spark.sql.execution.datasources.orc.OrcFileFormat],
-          &quot;orc&quot;)
-      }
&lt;p&gt; else &lt;/p&gt;
{
-        sessionCatalog.metastoreCatalog.convertToLogicalRelation(
-          relation,
-          options,
-          classOf[org.apache.spark.sql.hive.orc.OrcFileFormat],
-          &quot;orc&quot;)
-      }&lt;/li&gt;
	&lt;li&gt;}&lt;br/&gt;
+  private def isConvertible(tableMeta: CatalogTable): Boolean = 
{
+    val serde = tableMeta.storage.serde.getOrElse(&quot;&quot;).toLowerCase(Locale.ROOT)
+    serde.contains(&quot;parquet&quot;) &amp;amp;&amp;amp; SQLConf.get.getConf(HiveUtils.CONVERT_METASTORE_PARQUET) ||
+      serde.contains(&quot;orc&quot;) &amp;amp;&amp;amp; SQLConf.get.getConf(HiveUtils.CONVERT_METASTORE_ORC)
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  private val metastoreCatalog = sessionCatalog.metastoreCatalog&lt;br/&gt;
+&lt;br/&gt;
   override def apply(plan: LogicalPlan): LogicalPlan = {&lt;br/&gt;
     plan resolveOperators &lt;/p&gt;
{
       // Write path
@@ -231,12 +198,21 @@ case class RelationConversions(
         // Inserting into partitioned table is not supported in Parquet/Orc data source (yet).
           if query.resolved &amp;amp;&amp;amp; DDLUtils.isHiveTable(r.tableMeta) &amp;amp;&amp;amp;
             !r.isPartitioned &amp;amp;&amp;amp; isConvertible(r) =&amp;gt;
-        InsertIntoTable(convert(r), partition, query, overwrite, ifPartitionNotExists)
+        InsertIntoTable(metastoreCatalog.convert(r), partition,
+          query, overwrite, ifPartitionNotExists)
 
       // Read path
       case relation: HiveTableRelation
           if DDLUtils.isHiveTable(relation.tableMeta) &amp;amp;&amp;amp; isConvertible(relation) =&amp;gt;
-        convert(relation)
+        metastoreCatalog.convert(relation)
+
+      // CTAS
+      case CreateTable(tableDesc, mode, Some(query))
+          if DDLUtils.isHiveTable(tableDesc) &amp;amp;&amp;amp; tableDesc.partitionColumnNames.isEmpty &amp;amp;&amp;amp;
+            isConvertible(tableDesc) &amp;amp;&amp;amp; SQLConf.get.getConf(HiveUtils.CONVERT_METASTORE_CTAS) =&amp;gt;
+        DDLUtils.checkDataColNames(tableDesc)
+        OptimizedCreateHiveTableAsSelectCommand(
+          tableDesc, query, query.output.map(_.name), mode)
     }
&lt;p&gt;   }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala&lt;br/&gt;
index 66067704195dd..b60d4c71f5941 100644&lt;br/&gt;
&amp;#8212; a/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala&lt;br/&gt;
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/HiveUtils.scala&lt;br/&gt;
@@ -110,6 +110,14 @@ private&lt;span class=&quot;error&quot;&gt;&amp;#91;spark&amp;#93;&lt;/span&gt; object HiveUtils extends Logging {&lt;br/&gt;
     .booleanConf&lt;br/&gt;
     .createWithDefault(true)&lt;/p&gt;

&lt;p&gt;+  val CONVERT_METASTORE_CTAS = buildConf(&quot;spark.sql.hive.convertMetastoreCtas&quot;)&lt;br/&gt;
+    .doc(&quot;When set to true,  Spark will try to use built-in data source writer &quot; +&lt;br/&gt;
+      &quot;instead of Hive serde in CTAS. This flag is effective only if &quot; +&lt;br/&gt;
+      &quot;`spark.sql.hive.convertMetastoreParquet` or `spark.sql.hive.convertMetastoreOrc` is &quot; +&lt;br/&gt;
+      &quot;enabled respectively for Parquet and ORC formats&quot;)&lt;br/&gt;
+    .booleanConf&lt;br/&gt;
+    .createWithDefault(true)&lt;br/&gt;
+&lt;br/&gt;
   val HIVE_METASTORE_SHARED_PREFIXES = buildConf(&quot;spark.sql.hive.metastore.sharedPrefixes&quot;)&lt;br/&gt;
     .doc(&quot;A comma separated list of class prefixes that should be loaded using the classloader &quot; +&lt;br/&gt;
       &quot;that is shared between Spark SQL and a specific version of Hive. An example of classes &quot; +&lt;br/&gt;
diff --git a/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala b/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala&lt;br/&gt;
index fd1e931ee0c7a..608f21e726259 100644&lt;br/&gt;
&amp;#8212; a/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala&lt;br/&gt;
+++ b/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/CreateHiveTableAsSelectCommand.scala&lt;br/&gt;
@@ -20,32 +20,26 @@ package org.apache.spark.sql.hive.execution&lt;br/&gt;
 import scala.util.control.NonFatal&lt;/p&gt;

&lt;p&gt; import org.apache.spark.sql.&lt;/p&gt;
{AnalysisException, Row, SaveMode, SparkSession}
&lt;p&gt;-import org.apache.spark.sql.catalyst.catalog.CatalogTable&lt;br/&gt;
-import org.apache.spark.sql.catalyst.expressions.Attribute&lt;br/&gt;
+import org.apache.spark.sql.catalyst.catalog.&lt;/p&gt;
{CatalogTable, SessionCatalog}
&lt;p&gt; import org.apache.spark.sql.catalyst.plans.logical.LogicalPlan&lt;br/&gt;
 import org.apache.spark.sql.execution.SparkPlan&lt;br/&gt;
-import org.apache.spark.sql.execution.command.DataWritingCommand&lt;br/&gt;
+import org.apache.spark.sql.execution.command.&lt;/p&gt;
{DataWritingCommand, DDLUtils}
&lt;p&gt;+import org.apache.spark.sql.execution.datasources.&lt;/p&gt;
{HadoopFsRelation, InsertIntoHadoopFsRelationCommand, LogicalRelation}
&lt;p&gt;+import org.apache.spark.sql.hive.HiveSessionCatalog&lt;/p&gt;

&lt;p&gt;+trait CreateHiveTableAsSelectBase extends DataWritingCommand {&lt;br/&gt;
+  val tableDesc: CatalogTable&lt;br/&gt;
+  val query: LogicalPlan&lt;br/&gt;
+  val outputColumnNames: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;&lt;br/&gt;
+  val mode: SaveMode&lt;/p&gt;

&lt;p&gt;-/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Create table and insert the query result into it.&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* @param tableDesc the Table Describe, which may contain serde, storage handler etc.&lt;/li&gt;
	&lt;li&gt;* @param query the query whose result will be insert into the new relation&lt;/li&gt;
	&lt;li&gt;* @param mode SaveMode&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
-case class CreateHiveTableAsSelectCommand(&lt;/li&gt;
	&lt;li&gt;tableDesc: CatalogTable,&lt;/li&gt;
	&lt;li&gt;query: LogicalPlan,&lt;/li&gt;
	&lt;li&gt;outputColumnNames: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;/li&gt;
	&lt;li&gt;mode: SaveMode)&lt;/li&gt;
	&lt;li&gt;extends DataWritingCommand {&lt;br/&gt;
-&lt;/li&gt;
	&lt;li&gt;private val tableIdentifier = tableDesc.identifier&lt;br/&gt;
+  protected val tableIdentifier = tableDesc.identifier&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;   override def run(sparkSession: SparkSession, child: SparkPlan): Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;Row&amp;#93;&lt;/span&gt; = {&lt;br/&gt;
     val catalog = sparkSession.sessionState.catalog&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (catalog.tableExists(tableIdentifier)) {&lt;br/&gt;
+    val tableExists = catalog.tableExists(tableIdentifier)&lt;br/&gt;
+&lt;br/&gt;
+    if (tableExists) 
{
       assert(mode != SaveMode.Overwrite,
         s&quot;Expect the table $tableIdentifier has been dropped when the save mode is Overwrite&quot;)
 
@@ -57,15 +51,8 @@ case class CreateHiveTableAsSelectCommand(
         return Seq.empty
       }&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// For CTAS, there is no static partition values to insert.&lt;/li&gt;
	&lt;li&gt;val partition = tableDesc.partitionColumnNames.map(_ -&amp;gt; None).toMap&lt;/li&gt;
	&lt;li&gt;InsertIntoHiveTable(&lt;/li&gt;
	&lt;li&gt;tableDesc,&lt;/li&gt;
	&lt;li&gt;partition,&lt;/li&gt;
	&lt;li&gt;query,&lt;/li&gt;
	&lt;li&gt;overwrite = false,&lt;/li&gt;
	&lt;li&gt;ifPartitionNotExists = false,&lt;/li&gt;
	&lt;li&gt;outputColumnNames = outputColumnNames).run(sparkSession, child)&lt;br/&gt;
+      val command = getWritingCommand(catalog, tableDesc, tableExists = true)&lt;br/&gt;
+      command.run(sparkSession, child)&lt;br/&gt;
     } else {&lt;br/&gt;
       // TODO ideally, we should get the output data ready first and then&lt;br/&gt;
       // add the relation into catalog, just in case of failure occurs while data&lt;br/&gt;
@@ -77,15 +64,8 @@ case class CreateHiveTableAsSelectCommand(&lt;br/&gt;
       try 
{
         // Read back the metadata of the table which was created just now.
         val createdTableMeta = catalog.getTableMetadata(tableDesc.identifier)
-        // For CTAS, there is no static partition values to insert.
-        val partition = createdTableMeta.partitionColumnNames.map(_ -&amp;gt; None).toMap
-        InsertIntoHiveTable(
-          createdTableMeta,
-          partition,
-          query,
-          overwrite = true,
-          ifPartitionNotExists = false,
-          outputColumnNames = outputColumnNames).run(sparkSession, child)
+        val command = getWritingCommand(catalog, createdTableMeta, tableExists = false)
+        command.run(sparkSession, child)
       }
&lt;p&gt; catch &lt;/p&gt;
{
         case NonFatal(e) =&amp;gt;
           // drop the created table.
@@ -97,9 +77,89 @@ case class CreateHiveTableAsSelectCommand(
     Seq.empty[Row]
   }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;+  // Returns `DataWritingCommand` which actually writes data into the table.&lt;br/&gt;
+  def getWritingCommand(&lt;br/&gt;
+    catalog: SessionCatalog,&lt;br/&gt;
+    tableDesc: CatalogTable,&lt;br/&gt;
+    tableExists: Boolean): DataWritingCommand&lt;br/&gt;
+&lt;br/&gt;
   override def argString: String = {&lt;br/&gt;
     s&quot;[Database:${tableDesc.database}, &quot; +&lt;br/&gt;
     s&quot;TableName: ${tableDesc.identifier.table}, &quot; +&lt;br/&gt;
     s&quot;InsertIntoHiveTable]&quot;&lt;br/&gt;
   }&lt;br/&gt;
 }&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Create table and insert the query result into it.&lt;br/&gt;
+ *&lt;br/&gt;
+ * @param tableDesc the table description, which may contain serde, storage handler etc.&lt;br/&gt;
+ * @param query the query whose result will be insert into the new relation&lt;br/&gt;
+ * @param mode SaveMode&lt;br/&gt;
+ */&lt;br/&gt;
+case class CreateHiveTableAsSelectCommand(&lt;br/&gt;
+    tableDesc: CatalogTable,&lt;br/&gt;
+    query: LogicalPlan,&lt;br/&gt;
+    outputColumnNames: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    mode: SaveMode)&lt;br/&gt;
+  extends CreateHiveTableAsSelectBase {&lt;br/&gt;
+&lt;br/&gt;
+  override def getWritingCommand(&lt;br/&gt;
+      catalog: SessionCatalog,&lt;br/&gt;
+      tableDesc: CatalogTable,&lt;br/&gt;
+      tableExists: Boolean): DataWritingCommand = &lt;/p&gt;
{
+    // For CTAS, there is no static partition values to insert.
+    val partition = tableDesc.partitionColumnNames.map(_ -&amp;gt; None).toMap
+    InsertIntoHiveTable(
+      tableDesc,
+      partition,
+      query,
+      overwrite = if (tableExists) false else true,
+      ifPartitionNotExists = false,
+      outputColumnNames = outputColumnNames)
+  }
&lt;p&gt;+}&lt;br/&gt;
+&lt;br/&gt;
+/**&lt;br/&gt;
+ * Create table and insert the query result into it. This creates Hive table but inserts&lt;br/&gt;
+ * the query result into it by using data source.&lt;br/&gt;
+ *&lt;br/&gt;
+ * @param tableDesc the table description, which may contain serde, storage handler etc.&lt;br/&gt;
+ * @param query the query whose result will be insert into the new relation&lt;br/&gt;
+ * @param mode SaveMode&lt;br/&gt;
+ */&lt;br/&gt;
+case class OptimizedCreateHiveTableAsSelectCommand(&lt;br/&gt;
+    tableDesc: CatalogTable,&lt;br/&gt;
+    query: LogicalPlan,&lt;br/&gt;
+    outputColumnNames: Seq&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;,&lt;br/&gt;
+    mode: SaveMode)&lt;br/&gt;
+  extends CreateHiveTableAsSelectBase {&lt;br/&gt;
+&lt;br/&gt;
+  override def getWritingCommand(&lt;br/&gt;
+      catalog: SessionCatalog,&lt;br/&gt;
+      tableDesc: CatalogTable,&lt;br/&gt;
+      tableExists: Boolean): DataWritingCommand = {&lt;br/&gt;
+    val metastoreCatalog = catalog.asInstanceOf&lt;span class=&quot;error&quot;&gt;&amp;#91;HiveSessionCatalog&amp;#93;&lt;/span&gt;.metastoreCatalog&lt;br/&gt;
+    val hiveTable = DDLUtils.readHiveTable(tableDesc)&lt;br/&gt;
+&lt;br/&gt;
+    val hadoopRelation = metastoreCatalog.convert(hiveTable) match &lt;/p&gt;
{
+      case LogicalRelation(t: HadoopFsRelation, _, _, _) =&amp;gt; t
+      case _ =&amp;gt; throw new AnalysisException(s&quot;$tableIdentifier should be converted to &quot; +
+        &quot;HadoopFsRelation.&quot;)
+    }
&lt;p&gt;+&lt;br/&gt;
+    InsertIntoHadoopFsRelationCommand(&lt;br/&gt;
+      hadoopRelation.location.rootPaths.head,&lt;br/&gt;
+      Map.empty, // We don&apos;t support to convert partitioned table.&lt;br/&gt;
+      false,&lt;br/&gt;
+      Seq.empty, // We don&apos;t support to convert partitioned table.&lt;br/&gt;
+      hadoopRelation.bucketSpec,&lt;br/&gt;
+      hadoopRelation.fileFormat,&lt;br/&gt;
+      hadoopRelation.options,&lt;br/&gt;
+      query,&lt;br/&gt;
+      if (tableExists) mode else SaveMode.Overwrite,&lt;br/&gt;
+      Some(tableDesc),&lt;br/&gt;
+      Some(hadoopRelation.location),&lt;br/&gt;
+      query.output.map(_.name))&lt;br/&gt;
+  }&lt;br/&gt;
+}&lt;br/&gt;
diff --git a/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala b/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala&lt;br/&gt;
index e5c9df05d5674..470c6a342b4dd 100644&lt;br/&gt;
&amp;#8212; a/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala&lt;br/&gt;
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/hive/HiveParquetSuite.scala&lt;br/&gt;
@@ -92,4 +92,18 @@ class HiveParquetSuite extends QueryTest with ParquetTest with TestHiveSingleton&lt;br/&gt;
       }&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;br/&gt;
+&lt;br/&gt;
+  test(&quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;: write empty map into hive parquet table&quot;) {&lt;br/&gt;
+    import testImplicits._&lt;br/&gt;
+&lt;br/&gt;
+    Seq(Map(1 -&amp;gt; &quot;a&quot;), Map.empty&lt;span class=&quot;error&quot;&gt;&amp;#91;Int, String&amp;#93;&lt;/span&gt;).toDF(&quot;m&quot;).createOrReplaceTempView(&quot;p&quot;)&lt;br/&gt;
+    withTempView(&quot;p&quot;) {&lt;br/&gt;
+      val targetTable = &quot;targetTable&quot;&lt;br/&gt;
+      withTable(targetTable) &lt;/p&gt;
{
+        sql(s&quot;CREATE TABLE $targetTable STORED AS PARQUET AS SELECT m FROM p&quot;)
+        checkAnswer(sql(s&quot;SELECT m FROM $targetTable&quot;),
+          Row(Map(1 -&amp;gt; &quot;a&quot;)) :: Row(Map.empty[Int, String]) :: Nil)
+      }
&lt;p&gt;+    }&lt;br/&gt;
+  }&lt;br/&gt;
 }&lt;br/&gt;
diff --git a/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala b/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala&lt;br/&gt;
index fab2a27cdef17..6acf44606cbbe 100644&lt;br/&gt;
&amp;#8212; a/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala&lt;br/&gt;
+++ b/sql/hive/src/test/scala/org/apache/spark/sql/hive/execution/SQLQuerySuite.scala&lt;br/&gt;
@@ -2276,6 +2276,46 @@ class SQLQuerySuite extends QueryTest with SQLTestUtils with TestHiveSingleton {&lt;br/&gt;
     }&lt;br/&gt;
   }&lt;/p&gt;

&lt;p&gt;+  test(&quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25271&quot; title=&quot;Creating parquet table with all the column null throws exception&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-25271&quot;&gt;&lt;del&gt;SPARK-25271&lt;/del&gt;&lt;/a&gt;: Hive ctas commands should use data source if it is convertible&quot;) {&lt;br/&gt;
+    withTempView(&quot;p&quot;) {&lt;br/&gt;
+      Seq(1, 2, 3).toDF(&quot;id&quot;).createOrReplaceTempView(&quot;p&quot;)&lt;br/&gt;
+&lt;br/&gt;
+      Seq(&quot;orc&quot;, &quot;parquet&quot;).foreach { format =&amp;gt;&lt;br/&gt;
+        Seq(true, false).foreach { isConverted =&amp;gt;&lt;br/&gt;
+          withSQLConf(&lt;br/&gt;
+            HiveUtils.CONVERT_METASTORE_ORC.key -&amp;gt; s&quot;$isConverted&quot;,&lt;br/&gt;
+            HiveUtils.CONVERT_METASTORE_PARQUET.key -&amp;gt; s&quot;$isConverted&quot;) {&lt;br/&gt;
+            Seq(true, false).foreach { isConvertedCtas =&amp;gt;&lt;br/&gt;
+              withSQLConf(HiveUtils.CONVERT_METASTORE_CTAS.key -&amp;gt; s&quot;$isConvertedCtas&quot;) {&lt;br/&gt;
+&lt;br/&gt;
+                val targetTable = &quot;targetTable&quot;&lt;br/&gt;
+                withTable(targetTable) {&lt;br/&gt;
+                  val df = sql(s&quot;CREATE TABLE $targetTable STORED AS $format AS SELECT id FROM p&quot;)&lt;br/&gt;
+                  checkAnswer(sql(s&quot;SELECT id FROM $targetTable&quot;),&lt;br/&gt;
+                    Row(1) :: Row(2) :: Row(3) :: Nil)&lt;br/&gt;
+&lt;br/&gt;
+                  val ctasDSCommand = df.queryExecution.analyzed.collect &lt;/p&gt;
{
+                    case _: OptimizedCreateHiveTableAsSelectCommand =&amp;gt; true
+                  }
&lt;p&gt;.headOption&lt;br/&gt;
+                  val ctasCommand = df.queryExecution.analyzed.collect &lt;/p&gt;
{
+                    case _: CreateHiveTableAsSelectCommand =&amp;gt; true
+                  }
&lt;p&gt;.headOption&lt;br/&gt;
+&lt;br/&gt;
+                  if (isConverted &amp;amp;&amp;amp; isConvertedCtas) &lt;/p&gt;
{
+                    assert(ctasDSCommand.nonEmpty)
+                    assert(ctasCommand.isEmpty)
+                  }
&lt;p&gt; else &lt;/p&gt;
{
+                    assert(ctasDSCommand.isEmpty)
+                    assert(ctasCommand.nonEmpty)
+                  }
&lt;p&gt;+                }&lt;br/&gt;
+              }&lt;br/&gt;
+            }&lt;br/&gt;
+          }&lt;br/&gt;
+        }&lt;br/&gt;
+      }&lt;br/&gt;
+    }&lt;br/&gt;
+  }&lt;/p&gt;

&lt;p&gt;   test(&quot;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26181&quot; title=&quot;the `hasMinMaxStats` method of `ColumnStatsMap` is not correct&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26181&quot;&gt;&lt;del&gt;SPARK-26181&lt;/del&gt;&lt;/a&gt; hasMinMaxStats method of ColumnStatsMap is not correct&quot;) {&lt;br/&gt;
     withSQLConf(SQLConf.CBO_ENABLED.key -&amp;gt; &quot;true&quot;) {&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="17212487" author="apachespark" created="Mon, 12 Oct 2020 16:29:05 +0000"  >&lt;p&gt;User &apos;viirya&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/30017&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/30017&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17494333" author="JIRAUSER281773" created="Fri, 18 Feb 2022 02:58:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=viirya&quot; class=&quot;user-hover&quot; rel=&quot;viirya&quot;&gt;viirya&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=apachespark&quot; class=&quot;user-hover&quot; rel=&quot;apachespark&quot;&gt;apachespark&lt;/a&gt; Hello, I did not have this problem in Spark2.4.0-CDH6.3.2 version, but this problem was repeated in Spark2.4.3 version, I do not understand why the lower version succeeded and the higher version failed, I would like to ask whether the fix of this bug does not support the 2.4.3 version? The following is my version information and error message:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
spark version: 2.4.0-cdh6.3.2
hive version: 2.1.1-cdh.6.3.2
scala&amp;gt; spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table test STORED AS PARQUET as select map() as a&quot;&lt;/span&gt;)
scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from test&quot;&lt;/span&gt;).show
+---+ &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;&#160;
| &#160;a|
+---+
| []|
+---+

-----------------------------------------------------------------------------------------------------------------
spark version: 2.4.3
hive version: 3.1.2
scala&amp;gt; spark.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table test STORED AS PARQUET as select map() as a&quot;&lt;/span&gt;)

Caused by: org.apache.spark.SparkException: Task failed &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; writing rows.
&#160; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:257)
&#160; at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)
&#160; at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)
&#160; at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
&#160; at org.apache.spark.scheduler.Task.run(Task.scala:121)
&#160; at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
&#160; at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
&#160; at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
&#160; at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
&#160; at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
&#160; at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
Caused by: java.lang.RuntimeException: Parquet record is malformed: empty fields are illegal, the field should be ommited completely instead
&#160; at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:64)
&#160; at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:59)
&#160; at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriteSupport.write(DataWritableWriteSupport.java:31)
&#160; at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:121)
&#160; at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:123)
&#160; at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:42)
&#160; at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:111)
&#160; at org.apache.hadoop.hive.ql.io.parquet.write.ParquetRecordWriterWrapper.write(ParquetRecordWriterWrapper.java:124)
&#160; at org.apache.spark.sql.hive.execution.HiveOutputWriter.write(HiveFileFormat.scala:149)
&#160; at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:137)
&#160; at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:245)
&#160; at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:242)
&#160; at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1394)
&#160; at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:248)
&#160; ... 10 more
Caused by: parquet.io.ParquetEncodingException: empty fields are illegal, the field should be ommited completely instead
&#160; at parquet.io.MessageColumnIO$MessageColumnIORecordConsumer.endField(MessageColumnIO.java:244)
&#160; at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeMap(DataWritableWriter.java:241)
&#160; at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeValue(DataWritableWriter.java:116)
&#160; at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.writeGroupFields(DataWritableWriter.java:89)
&#160; at org.apache.hadoop.hive.ql.io.parquet.write.DataWritableWriter.write(DataWritableWriter.java:60)
&#160; ... 23 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17494342" author="viirya" created="Fri, 18 Feb 2022 03:11:52 +0000"  >&lt;p&gt;Based on this JIRA, we only have this fix since 2.4.8.&lt;/p&gt;

&lt;p&gt;I guess 2.4.0-cdh6.3.2 may backport the fix as this is a distribution maintained by the vendor, I don&apos;t know about the detail.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13259560">SPARK-29295</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310051">
                    <name>Supercedes</name>
                                            <outwardlinks description="supercedes">
                                        <issuelink>
            <issuekey id="13206338">SPARK-26437</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12938756" name="image-2018-09-07-09-12-34-944.png" size="229513" author="shivusondur@gmail.com" created="Fri, 7 Sep 2018 03:42:37 +0000"/>
                            <attachment id="12938757" name="image-2018-09-07-09-29-33-370.png" size="229513" author="shivusondur@gmail.com" created="Fri, 7 Sep 2018 03:59:38 +0000"/>
                            <attachment id="12938758" name="image-2018-09-07-09-29-52-899.png" size="194099" author="shivusondur@gmail.com" created="Fri, 7 Sep 2018 03:59:57 +0000"/>
                            <attachment id="12938760" name="image-2018-09-07-09-32-43-892.png" size="230564" author="shivusondur@gmail.com" created="Fri, 7 Sep 2018 04:02:48 +0000"/>
                            <attachment id="12938761" name="image-2018-09-07-09-33-03-095.png" size="266063" author="shivusondur@gmail.com" created="Fri, 7 Sep 2018 04:03:08 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 38 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3xjyf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>