<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:14:34 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-1112] When spark.akka.frameSize &gt; 10, task results bigger than 10MiB block execution</title>
                <link>https://issues.apache.org/jira/browse/SPARK-1112</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When I set the spark.akka.frameSize to something over 10, the messages sent from the executors to the driver completely block the execution if the message is bigger than 10MiB and smaller than the frameSize (if it&apos;s above the frameSize, it&apos;s ok)&lt;/p&gt;

&lt;p&gt;Workaround is to set the spark.akka.frameSize to 10. In this case, since 0.8.1, the blockManager deal with  the data to be sent. It seems slower than akka direct message though.&lt;/p&gt;

&lt;p&gt;The configuration seems to be correctly read (see actorSystemConfig.txt), so I don&apos;t see where the 10MiB could come from &lt;/p&gt;</description>
                <environment></environment>
        <key id="12704733">SPARK-1112</key>
            <summary>When spark.akka.frameSize &gt; 10, task results bigger than 10MiB block execution</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mengxr">Xiangrui Meng</assignee>
                                    <reporter username="guillaumepitel">Guillaume Pitel</reporter>
                        <labels>
                    </labels>
                <created>Thu, 20 Feb 2014 08:34:26 +0000</created>
                <updated>Mon, 1 Dec 2014 08:29:53 +0000</updated>
                            <resolved>Thu, 17 Jul 2014 04:31:18 +0000</resolved>
                                    <version>0.9.0</version>
                    <version>1.0.0</version>
                                    <fixVersion>0.9.2</fixVersion>
                    <fixVersion>1.0.1</fixVersion>
                    <fixVersion>1.1.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>18</watches>
                                                                                                                <comments>
                            <comment id="13953780" author="kayousterhout" created="Thu, 20 Feb 2014 15:47:07 +0000"  >&lt;p&gt;Thanks for reporting this!  Does Spark hang, or does the worker throw an exception?  If the former, would you mind uploading the Spark worker log, and if the latter, can you add the stack trace?&lt;/p&gt;</comment>
                            <comment id="13953786" author="guillaumepitel" created="Thu, 20 Feb 2014 23:03:43 +0000"  >&lt;p&gt;No Exception, and not &quot;hanging&quot; in the bad way the executors can sometime hang : if I kill the driver, the workers receive the shutdown signal and exit cleanly.&lt;/p&gt;

&lt;p&gt;Here are the logs :&lt;/p&gt;

&lt;p&gt;DRIVER :&lt;/p&gt;

&lt;p&gt;14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:13 as 2083 bytes in 0 ms&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:14 as TID 2294 on executor 1: t4.exensa.loc (PROCESS_LOCAL)&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:14 as 2083 bytes in 0 ms&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:15 as TID 2295 on executor 4: t3.exensa.loc (PROCESS_LOCAL)&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:15 as 2083 bytes in 1 ms&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:16 as TID 2296 on executor 0: t0.exensa.loc (PROCESS_LOCAL)&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:16 as 2083 bytes in 2 ms&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:17 as TID 2297 on executor 3: t1.exensa.loc (PROCESS_LOCAL)&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:17 as 2083 bytes in 1 ms&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:18 as TID 2298 on executor 2: t5.exensa.loc (PROCESS_LOCAL)&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:18 as 2083 bytes in 1 ms&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Starting task 12.0:19 as TID 2299 on executor 5: t6.exensa.loc (PROCESS_LOCAL)&lt;br/&gt;
14/02/19 16:21:49 INFO TaskSetManager: Serialized task 12.0:19 as 2083 bytes in 1 ms&lt;/p&gt;

&lt;p&gt;EXECUTOR :&lt;br/&gt;
14/02/19 15:21:53 INFO Executor: Serialized size of result for 2287 is 17229427&lt;br/&gt;
14/02/19 15:21:53 INFO Executor: Sending result for 2287 directly to driver&lt;br/&gt;
14/02/19 15:21:53 INFO Executor: Serialized size of result for 2299 is 17229262&lt;br/&gt;
14/02/19 15:21:53 INFO Executor: Sending result for 2299 directly to driver&lt;br/&gt;
14/02/19 15:21:53 INFO Executor: Finished task ID 2299&lt;br/&gt;
14/02/19 15:21:53 INFO Executor: Finished task ID 2287&lt;br/&gt;
14/02/19 15:21:53 INFO Executor: Serialized size of result for 2281 is 17229426&lt;br/&gt;
14/02/19 15:21:53 INFO Executor: Sending result for 2281 directly to driver&lt;br/&gt;
14/02/19 15:21:53 INFO Executor: Finished task ID 2281&lt;/p&gt;

&lt;p&gt;There is a timezone difference between driver &amp;amp; executor&lt;/p&gt;
</comment>
                            <comment id="13953787" author="roshan" created="Thu, 20 Feb 2014 23:14:31 +0000"  >&lt;p&gt;I have a similar issue, I&apos;m on spark-0.9.0 compiled with cdh-4.2.1&lt;/p&gt;

&lt;p&gt;For me, serialized tasks over 10 MB do not reach executors. I&apos;ve tried this with spark.akka.frameSize set to 160 and 10. The workaround suggested (setting spark.akka.frameSize to 10) does not work for me.&lt;/p&gt;

&lt;p&gt;I&apos;ve confirmed that even if serialized tasks are just under 10MB, the executors do get them and the task is completed.&lt;/p&gt;

&lt;p&gt;Spark hangs. There are no exceptions or unusual ERROR/WARN/DEBUG logs in the driver, master, executor or worker daemon logs. The executors just don&apos;t seem to have received the tasks. The application UI shows the task status as running, but never progresses.&lt;/p&gt;

&lt;p&gt;Here are the last few lines from my driver and one executor:&lt;/p&gt;

&lt;p&gt;DRIVER:&lt;br/&gt;
14/02/21 06:37:00 INFO TaskSetManager: Finished TID 797 in 53897 ms on spark-slave01 (progress: 78/80)&lt;br/&gt;
14/02/21 06:37:00 INFO DAGScheduler: Completed ResultTask(9, 58)&lt;br/&gt;
14/02/21 06:37:08 INFO TaskSetManager: Finished TID 768 in 75767 ms on spark-slave02 (progress: 79/80)&lt;br/&gt;
14/02/21 06:37:08 INFO TaskSchedulerImpl: Remove TaskSet 9.0 from pool &lt;br/&gt;
14/02/21 06:37:08 INFO DAGScheduler: Completed ResultTask(9, 69)&lt;br/&gt;
14/02/21 06:37:08 INFO DAGScheduler: Stage 9 (reduceByKeyLocally at SKMeans.scala:174) finished in 99.048 s&lt;br/&gt;
14/02/21 06:37:08 INFO SparkContext: Job finished: reduceByKeyLocally at SKMeans.scala:174, took 99.359019444 s&lt;br/&gt;
14/02/21 06:37:09 INFO SparkContext: Starting job: reduceByKeyLocally at SKMeans.scala:174&lt;br/&gt;
14/02/21 06:37:09 INFO DAGScheduler: Got job 7 (reduceByKeyLocally at SKMeans.scala:174) with 80 output partitions (allowLocal=false)&lt;br/&gt;
14/02/21 06:37:09 INFO DAGScheduler: Final stage: Stage 10 (reduceByKeyLocally at SKMeans.scala:174)&lt;br/&gt;
14/02/21 06:37:09 INFO DAGScheduler: Parents of final stage: List()&lt;br/&gt;
14/02/21 06:37:09 INFO DAGScheduler: Missing parents: List()&lt;br/&gt;
14/02/21 06:37:09 INFO DAGScheduler: Submitting Stage 10 (MapPartitionsRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;28&amp;#93;&lt;/span&gt; at reduceByKeyLocally at SKMeans.scala:174), which has no missing parents&lt;br/&gt;
14/02/21 06:37:10 INFO DAGScheduler: Submitting 80 missing tasks from Stage 10 (MapPartitionsRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;28&amp;#93;&lt;/span&gt; at reduceByKeyLocally at SKMeans.scala:174)&lt;br/&gt;
14/02/21 06:37:10 INFO TaskSchedulerImpl: Adding task set 10.0 with 80 tasks&lt;br/&gt;
14/02/21 06:37:10 INFO TaskSetManager: Starting task 10.0:0 as TID 800 on executor 2: spark-slave01 (PROCESS_LOCAL)&lt;br/&gt;
14/02/21 06:37:10 INFO TaskSetManager: Serialized task 10.0:0 as 10700743 bytes in 19 ms&lt;br/&gt;
...&amp;lt;Starting task and Serialized task lines repeat for each of 40 tasks.&amp;gt;&lt;/p&gt;

&lt;p&gt;EXECUTOR: spark-slave01&lt;br/&gt;
14/02/21 06:36:08 DEBUG Executor: Task 798&apos;s epoch is 3&lt;br/&gt;
14/02/21 06:36:08 DEBUG CacheManager: Looking for partition rdd_4_60&lt;br/&gt;
14/02/21 06:36:08 DEBUG BlockManager: Getting local block rdd_4_60&lt;br/&gt;
14/02/21 06:36:08 DEBUG BlockManager: Level for block rdd_4_60 is StorageLevel(false, true, true, 1)&lt;br/&gt;
14/02/21 06:36:08 DEBUG BlockManager: Getting block rdd_4_60 from memory&lt;br/&gt;
14/02/21 06:36:08 INFO BlockManager: Found block rdd_4_60 locally&lt;br/&gt;
14/02/21 06:36:21 INFO Executor: Serialized size of result for 765 is 1224222&lt;br/&gt;
14/02/21 06:36:21 INFO Executor: Sending result for 765 directly to driver&lt;br/&gt;
14/02/21 06:36:21 INFO Executor: Finished task ID 765&lt;br/&gt;
14/02/21 06:36:34 INFO Executor: Serialized size of result for 790 is 1262463&lt;br/&gt;
14/02/21 06:36:34 INFO Executor: Sending result for 790 directly to driver&lt;br/&gt;
14/02/21 06:36:34 INFO Executor: Finished task ID 790&lt;br/&gt;
14/02/21 06:36:37 INFO Executor: Serialized size of result for 784 is 1394816&lt;br/&gt;
14/02/21 06:36:37 INFO Executor: Sending result for 784 directly to driver&lt;br/&gt;
14/02/21 06:36:37 INFO Executor: Finished task ID 784&lt;br/&gt;
14/02/21 06:36:38 INFO Executor: Serialized size of result for 787 is 1409571&lt;br/&gt;
14/02/21 06:36:38 INFO Executor: Sending result for 787 directly to driver&lt;br/&gt;
14/02/21 06:36:38 INFO Executor: Finished task ID 787&lt;br/&gt;
14/02/21 06:36:41 INFO Executor: Serialized size of result for 798 is 1270321&lt;br/&gt;
14/02/21 06:36:41 INFO Executor: Sending result for 798 directly to driver&lt;br/&gt;
14/02/21 06:36:41 INFO Executor: Finished task ID 798&lt;br/&gt;
14/02/21 06:36:50 INFO Executor: Serialized size of result for 792 is 1175064&lt;br/&gt;
14/02/21 06:36:50 INFO Executor: Sending result for 792 directly to driver&lt;br/&gt;
14/02/21 06:36:50 INFO Executor: Finished task ID 792&lt;br/&gt;
14/02/21 06:36:52 INFO Executor: Serialized size of result for 794 is 1485354&lt;br/&gt;
14/02/21 06:36:52 INFO Executor: Sending result for 794 directly to driver&lt;br/&gt;
14/02/21 06:36:52 INFO Executor: Finished task ID 794&lt;br/&gt;
14/02/21 06:37:00 INFO Executor: Serialized size of result for 797 is 1615486&lt;br/&gt;
14/02/21 06:37:00 INFO Executor: Sending result for 797 directly to driver&lt;br/&gt;
14/02/21 06:37:00 INFO Executor: Finished task ID 797&lt;/p&gt;

&lt;p&gt;Roshan&lt;/p&gt;</comment>
                            <comment id="13953788" author="kayousterhout" created="Thu, 20 Feb 2014 23:54:06 +0000"  >&lt;p&gt;Roshan, the issue you&apos;re seeing is different &amp;#8211; Guillaume&apos;s issue is when task results are too large to be sent using Akka (in which case Spark should use a different code path to send task results to the executor); your issue is when the task itself is too large, in which case Spark (in theory!) gives up and throw an error.  We should fix both problems, but would you mind opening a separate issue?&lt;/p&gt;</comment>
                            <comment id="13953790" author="kayousterhout" created="Thu, 20 Feb 2014 23:56:32 +0000"  >&lt;p&gt;Guillaume, just to clarify, the logs you pasted above are for when you set the maximum frame size to 16MiB?  I&apos;m asking because the task results seem to be just slightly larger than 16MiB, which isn&apos;t the failure case you mentioned in your description.&lt;/p&gt;</comment>
                            <comment id="13953791" author="guillaumepitel" created="Fri, 21 Feb 2014 00:04:27 +0000"  >&lt;p&gt;Sorry, I should have specified it. The frameSize was set to 512 for those logs. I&apos;ve also tried with 16, and it works when results are over 16MB&lt;/p&gt;</comment>
                            <comment id="13953789" author="kayousterhout" created="Fri, 21 Feb 2014 00:06:49 +0000"  >&lt;p&gt;Cool thanks for clarifying!  Looking into this...&lt;/p&gt;</comment>
                            <comment id="13953792" author="guillaumepitel" created="Fri, 21 Feb 2014 01:04:08 +0000"  >&lt;p&gt;When I set BOTH the property on driver with &lt;/p&gt;

&lt;p&gt;System.setProperty(&quot;spark.akka.frameSize&quot;, 128) AND I pass the env parameter to SparkContext with SPARK_JAVA_OPTS = &quot;-Dspark.akka.frameSize=128&quot; &lt;/p&gt;

&lt;p&gt;Then it seems to works.&lt;/p&gt;

&lt;p&gt;So maybe the problem comes from properties not being passed correctly to workers when executors are instanciated ?&lt;/p&gt;

&lt;p&gt;Also, I&apos;m using packaged binary distribution for CDH4 on a standalone cluster&lt;/p&gt;</comment>
                            <comment id="13953796" author="roshan" created="Fri, 21 Feb 2014 01:17:49 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;I just realized my driver wasn&apos;t picking up spark.akka.frameSize value, because of a problem in the way I was passing it in. However, my executor&apos;s were picking this value correctly from their conf/spark-env.sh files.&lt;/p&gt;

&lt;p&gt;Now, both sides, the driver and executors print the correct value for frameSize with spark.akka.logAkkaConfig=true.&lt;/p&gt;

&lt;p&gt;I also noticed that simply starting the driver with java -Dspark.akka.frameSize=200 does not propagate this automatically to the executors. Not that this is an issue. I guess I was just confused about the configuration.&lt;/p&gt;

&lt;p&gt;Guillaume, seems like you have the reverse situation as mine, ie. your drivers are correctly configured with the right frameSize, but the executors are still using the 10MB default?&lt;/p&gt;

&lt;p&gt;To conclude, after ensuring that the driver is correctly configured with the right frameSize, so far, serialized tasks larger than 10MB are being received by the executors and run successfully.&lt;/p&gt;

&lt;p&gt;Roshan&lt;/p&gt;</comment>
                            <comment id="13953793" author="guillaumepitel" created="Fri, 21 Feb 2014 05:35:36 +0000"  >&lt;p&gt;You&apos;re right Roshan.I was expecting the akka properties set before SparkContext creation to be propagated to the Executors (and based on what the Spark code does, it should be the case). &lt;/p&gt;

&lt;p&gt;I think it should be enforced for the whole akka stuff (timeouts and so on), as well as for the rest of spark properties&lt;/p&gt;</comment>
                            <comment id="13953806" author="patrick" created="Fri, 21 Feb 2014 23:33:12 +0000"  >&lt;p&gt;Hey There,&lt;/p&gt;

&lt;p&gt;I spent some time playing with this and couldn&apos;t reproduce the issue. The driver should capture the options and pass them to executors. I just tested this with a local cluster and was able to verify the akka frame size is passed to executors even if it&apos;s not set in spark-env.sh where the executor launches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=roshan&quot; class=&quot;user-hover&quot; rel=&quot;roshan&quot;&gt;roshan&lt;/a&gt; - what happens if you remove the setting from spark-env.sh on the executors and only set it at the driver. Does that work correctly?&lt;/p&gt;</comment>
                            <comment id="13953807" author="guillaumepitel" created="Sat, 22 Feb 2014 00:16:23 +0000"  >&lt;p&gt;In my code I already had some options passed in SPARK_JAVA_OPTS in the env of the SparkContext, but nothing about the akka frameSize. Could it be related ?&lt;/p&gt;

&lt;p&gt;Since it was working correctly in 0.8.1, maybe it&apos;s related to the new SparkConf ?&lt;/p&gt;</comment>
                            <comment id="13953808" author="patrick" created="Sat, 22 Feb 2014 00:23:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=guillaumepitel&quot; class=&quot;user-hover&quot; rel=&quot;guillaumepitel&quot;&gt;guillaumepitel&lt;/a&gt; If you aren&apos;t setting akka.frameSize in SPARK_JAVA_OPTS then where are you setting it?&lt;/p&gt;

&lt;p&gt;What I was saying is that if you do System.setProperty(spark.akka.frameSize, XX) before you create the SparkContext it should collect this and send it to the executors correctly. One thing is if you set it after you create the SparkContext it won&apos;t work... are you doing this by any chance?&lt;/p&gt;</comment>
                            <comment id="13953809" author="guillaumepitel" created="Sat, 22 Feb 2014 00:39:31 +0000"  >&lt;p&gt;No, I create the SparkContext after setting the properties (and I&apos;ve nothing on my nodes for configuring the frameSize, it&apos;s a per-process configuration). &lt;/p&gt;

&lt;p&gt;So before my workaround, I was just setting the property (and the environment in the UI was showing the right value) and passing a SPARK_JAVA_OPTS to the env of the SparkContext with &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.setProperty(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.serializer&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.serializer.KryoSerializer&quot;&lt;/span&gt;)
&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.setProperty(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.kryo.registrator&quot;&lt;/span&gt;, registrator)
&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.setProperty(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.kryo.referenceTracking&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&quot;&lt;/span&gt;)
&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.setProperty(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.kryoserializer.buffer.mb&quot;&lt;/span&gt;, bufferSize.toString)
&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.setProperty(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.locality.wait&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;10000&quot;&lt;/span&gt;)
&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.setProperty(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.hadoop.mapreduce.output.fileoutputformat.compress&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&quot;&lt;/span&gt;)
&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.setProperty(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.hadoop.mapreduce.output.fileoutputformat.compress.codec&quot;&lt;/span&gt;, codec)
&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.setProperty(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.hadoop.mapreduce.output.fileoutputformat.compress.type&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;BLOCK&quot;&lt;/span&gt;)
&lt;span class=&quot;code-object&quot;&gt;System&lt;/span&gt;.setProperty(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.akka.frameSize&quot;&lt;/span&gt;, akkaFrameSize.toString)

val sb = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StringBuilder()
sb.append(&lt;span class=&quot;code-quote&quot;&gt;&quot;-Dspark.storage.memoryFraction=&quot;&lt;/span&gt; + sparkMemoryFraction())
sb.append(&lt;span class=&quot;code-quote&quot;&gt;&quot; -Dspark.worker.timeout=&quot;&lt;/span&gt; + sparkWorkerTimeout())
sb.append(&lt;span class=&quot;code-quote&quot;&gt;&quot; -Dspark.akka.askTimeout=&quot;&lt;/span&gt; + sparkAkkaAskTimeout())
sb.append(&lt;span class=&quot;code-quote&quot;&gt;&quot; -Dspark.akka.timeout=&quot;&lt;/span&gt; + sparkAkkaTimeout())
sb.append(&lt;span class=&quot;code-quote&quot;&gt;&quot; -Dspark.shuffle.consolidateFiles=&quot;&lt;/span&gt; + sparkShuffleConsolidateFiles())

&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; env = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; HashMap[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;]()
env += &lt;span class=&quot;code-quote&quot;&gt;&quot;SPARK_JAVA_OPTS&quot;&lt;/span&gt; -&amp;gt; sb.toString()

val sc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkContext(sparkMaster(), appName, sparkHome(), jars(), env)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That caused a problem (the akka frame size seemed to be passed to the executor, but only after the creation of the actorSystem, because it was taking the right code path, but the akka system didn&apos;t seem to be properly configured).&lt;/p&gt;

&lt;p&gt;Now if I add this to my code :&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;sb.append(&lt;span class=&quot;code-quote&quot;&gt;&quot; -Dspark.akka.frameSize=&quot;&lt;/span&gt; + akkaFrameSize)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It works&lt;/p&gt;</comment>
                            <comment id="13953815" author="roshan" created="Sat, 22 Feb 2014 03:58:52 +0000"  >&lt;p&gt;@Patrick Wendell -&lt;/p&gt;

&lt;p&gt;Case 1.&lt;br/&gt;
DRIVER:&lt;br/&gt;
I start the driver with java -Dspark.akka.frameSize=200 -D... -cp .....&lt;/p&gt;

&lt;p&gt;EXECUTOR:&lt;br/&gt;
spark-env.sh on workers with frameSize specified: &lt;br/&gt;
export SPARK_JAVA_OPTS=&apos;-Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.frameSize=160 -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200&apos;&lt;/p&gt;

&lt;p&gt;Executor akka config log reports frameSize=160&lt;/p&gt;

&lt;p&gt;Case 2. &lt;br/&gt;
DRIVER:&lt;br/&gt;
I start the driver with java -Dspark.akka.frameSize=200 -D... -cp ....&lt;/p&gt;

&lt;p&gt;EXECUTOR:&lt;br/&gt;
spark-env.sh on workers with frameSize NOT specified: &lt;br/&gt;
export SPARK_JAVA_OPTS=&apos;-Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200&apos;&lt;/p&gt;

&lt;p&gt;Executor akka config log on workers has no frameSize and executor process(by ps waux) has no frameSize. Here, I suppose it picks up the default 10MB.&lt;/p&gt;

&lt;p&gt;Case 3. &lt;br/&gt;
DRIVER:&lt;br/&gt;
This time I export SPARK_JAVA_OPTS=&quot;-Dspark.akka.frameSize=220&quot; on the dirver host, before running the driver with java -Dspark.akka.frameSize=200 -D... -cp ....&lt;br/&gt;
I&apos;m not reading SPARK_JAVA_OPTS in the driver code.&lt;br/&gt;
Driver&apos;s Akka config log, says frameSize=200.&lt;/p&gt;

&lt;p&gt;EXECUTOR1 on HOST1:&lt;br/&gt;
No frameSize specified in spark-env&lt;br/&gt;
Executor akka config log says frameSize=220.&lt;/p&gt;

&lt;p&gt;EXECUTOR2 on HOST2:&lt;br/&gt;
frameSize=160 in spark-env&lt;br/&gt;
Executor akka config log says frameSize=220, &lt;/p&gt;

&lt;p&gt;ps waux shows the process was started with this cmd:&lt;br/&gt;
/usr/java/default/bin/java -cp sparkJar.jar -Dspark.local.dir=/data/spark/tmp -Dspark.akka.logAkkaConfig=true -Dspark.akka.frameSize=160 -Dspark.akka.timeout=100 -Dspark.akka.askTimeout=30 -Dspark.akka.logLifecycleEvents=true -Dspark.worker.timeout=200 -Dspark.akka.frameSize=220 -Xms3072M -Xmx3072M org.apache.spark.executor.CoarseGrainedExecutorBackend .....&lt;/p&gt;

&lt;p&gt;So the SPARK_JAVA_OPTS from both the executor and the driver are appended, but since java overrides the first frameSize with the second, the executor runs with frameSize=220&lt;/p&gt;

&lt;p&gt;Case 4.&lt;br/&gt;
DRIVER:&lt;br/&gt;
This time I export SPARK_JAVA_OPTS=&quot;-Dspark.akka.frameSize=220&quot; on the driver host, but don&apos;t specify frameSize in the launch command.&lt;br/&gt;
Again, I don&apos;t SPARK_JAVA_OPTS in the driver code.&lt;br/&gt;
No frameSize in driver&apos;s akka config log. I expect driver picks up the default 10MB frameSize.&lt;/p&gt;

&lt;p&gt;Executors are the same as in case3.&lt;/p&gt;

&lt;p&gt;What I&apos;m seeing is that, you can specify the frameSize for a driver in the launch command as java -Dspark.akka.frameSize property, and for the executors in their respective spark-env.sh. If you specify SPARK_JAVA_OPTS on the driver side, then this will override the value for the executors, but not for the driver. I don&apos;t use spark-class.sh to launch my driver, because somewhere I read that its meant for spark internal classes and examples. I also currently build my SparkContext directly, without using SparkConf.&lt;/p&gt;

&lt;p&gt;This not an issue for me any longer. That being said, it would have saved me loads of time, if the logs had provided some indication that sending serialized tasks to the executors had failed because they were larger than 10MB. Also, the documentation could be a bit clearer about what is set where and which property overrides or is overridden.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="14011968" author="swkimme" created="Thu, 29 May 2014 01:59:32 +0000"  >&lt;p&gt;Hi all, &lt;/p&gt;

&lt;p&gt;I&apos;m very new to Spark and doing some tests, I&apos;ve experienced similar issue.&lt;br/&gt;
(tested with Spark Shell, 0.9.1, r3.8xlarge instance on EC2 - 32 core / 244GiB MEM)&lt;/p&gt;

&lt;p&gt;I was trying to broadcast 700MB of data and Spark hangs when I run collect() method for the data. &lt;/p&gt;

&lt;p&gt;Here&apos;s the strange things :&lt;br/&gt;
1) when I tried &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val userInfo = sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;file:&lt;span class=&quot;code-comment&quot;&gt;///spark/logs/user_sign_up2.csv&quot;&lt;/span&gt;).map{line =&amp;gt; val split = line.split(&lt;span class=&quot;code-quote&quot;&gt;&quot;,&quot;&lt;/span&gt;); (split(1), split)}
&lt;/span&gt;val userInfoMap = userInfo.collectAsMap
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;it runs well.&lt;br/&gt;
2) when I tried &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val userInfo = sc.textFile(&lt;span class=&quot;code-quote&quot;&gt;&quot;file:&lt;span class=&quot;code-comment&quot;&gt;///spark/logs/user_sign_up2.csv&quot;&lt;/span&gt;).map{line =&amp;gt; val split = line.split(&lt;span class=&quot;code-quote&quot;&gt;&quot;,&quot;&lt;/span&gt;); (split(1), split(5))} 
&lt;/span&gt;val userInfoMap = userInfo.collectAsMap
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Spark hangs.&lt;br/&gt;
3) when I slightly control the data size using sample() method or cutting the data file, it runs well. &lt;/p&gt;

&lt;p&gt;Our team investigated logs from master and worker then we found worker finished all tasks but master couldn&apos;t retrieve the result from a task the result size larger than 10MB&lt;/p&gt;

&lt;p&gt;We tried to apply the workaround setting spark.akka.frameSize to 9, it works like a charm.&lt;/p&gt;

&lt;p&gt;I guess it might hard to reproduce the issue, please contact me if there&apos;s need of testing or getting logs. &lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                            <comment id="14011978" author="matei" created="Thu, 29 May 2014 02:17:56 +0000"  >&lt;p&gt;I&apos;m curious, why did you want to make the frameSize this big &amp;#8211; are the tasks themselves also big or just the results? There might be other buffers in Akka that can&apos;t be made bigger than this. It&apos;s possible that this changed in a newer Akka version (because larger frame sizes used to work before).&lt;/p&gt;</comment>
                            <comment id="14011993" author="swkimme" created="Thu, 29 May 2014 02:49:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=matei&quot; class=&quot;user-hover&quot; rel=&quot;matei&quot;&gt;matei&lt;/a&gt;&lt;br/&gt;
I&apos;ve found the default of spark.akka.frameSize is 10 from the config document, &lt;a href=&quot;http://spark.apache.org/docs/0.9.1/configuration.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://spark.apache.org/docs/0.9.1/configuration.html&lt;/a&gt;&lt;br/&gt;
just tried to slightly larger and smaller (11 and 9) values.&lt;/p&gt;

&lt;p&gt;I did collect() method on the userInfo and it might contains large data. (edited the first comment.)&lt;/p&gt;
</comment>
                            <comment id="14031307" author="xiaocai" created="Fri, 13 Jun 2014 23:00:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=matei&quot; class=&quot;user-hover&quot; rel=&quot;matei&quot;&gt;matei&lt;/a&gt; Do you know which akka version we should use to be able to use big frame size. &lt;/p&gt;</comment>
                            <comment id="14032867" author="xiaocai" created="Mon, 16 Jun 2014 20:09:57 +0000"  >&lt;p&gt;To follow up this thread, I have done some experiments when the frameSize is around 10MB .&lt;/p&gt;

&lt;p&gt;1) spark.akka.frameSize = 10&lt;br/&gt;
If one of the partition size is very close to 10MB, say 9.97MB, the execution blocks without any exception or warning. Worker finished the task to send the serialized result, and then throw exception saying hadoop IPC client connection stops (changing the logging to debug level). However, the master never receives the results and the program just hangs.&lt;br/&gt;
But if sizes for all the partitions less than some number btw 9.96MB amd 9.97MB, the program works fine.&lt;br/&gt;
2) spark.akka.frameSize = 9&lt;br/&gt;
when the partition size is just a little bit smaller than 9MB, it fails as well.&lt;/p&gt;

&lt;p&gt;This bug behavior is not exactly what spark-1112 is about, could you please guide me how to open a separate bug when the serialization size is very close to 10MB. &lt;/p&gt;

&lt;p&gt;Thanks a lot&lt;/p&gt;</comment>
                            <comment id="14036704" author="xiaocai" created="Thu, 19 Jun 2014 00:10:33 +0000"  >&lt;p&gt;I have filed a bug &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-2156&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-2156&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14036801" author="pwendell" created="Thu, 19 Jun 2014 01:38:18 +0000"  >&lt;p&gt;We were able to reproduce this - thanks for reporting it.&lt;/p&gt;</comment>
                            <comment id="14036843" author="xiaocai" created="Thu, 19 Jun 2014 02:06:31 +0000"  >&lt;p&gt;Awesome, looking forward to the fix. At least better error or exception message would be helpful. &lt;/p&gt;</comment>
                            <comment id="14036869" author="mengxr" created="Thu, 19 Jun 2014 02:35:30 +0000"  >&lt;p&gt;PR: &lt;a href=&quot;https://github.com/apache/spark/pull/1124&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1124&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14040359" author="pwendell" created="Mon, 23 Jun 2014 02:49:36 +0000"  >&lt;p&gt;This is fixed in the 1.0 branch via:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/1172&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1172&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14042960" author="pwendell" created="Wed, 25 Jun 2014 02:06:52 +0000"  >&lt;p&gt;Fixed in 1.1.0 via:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/1132&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1132&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14043730" author="reachbach" created="Wed, 25 Jun 2014 16:48:06 +0000"  >&lt;p&gt;Can a clear workaround be specified for this bug please? For those unable to upgrade to run on 1.0.1  or 1.1.0 in production, general instructions on the workaround are required. This is a huge blocker for current production deployments (even on 1.0.0) otherwise. For instance, running a saveAsTextFile() on an RDD (~400MB) causes execution to freeze with the last log statements seen on the driver being:&lt;/p&gt;

&lt;p&gt;14/06/25 16:38:55 INFO spark.SparkContext: Starting job: saveAsTextFile at Test.java:99&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Got job 6 (saveAsTextFile at Test.java:99) with 2 output partitions (allowLocal=false)&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Final stage: Stage 6(saveAsTextFile at Test.java:99)&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Parents of final stage: List()&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Missing parents: List()&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Submitting Stage 6 (MappedRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;558&amp;#93;&lt;/span&gt; at saveAsTextFile at Test.java:99), which has no missing parents&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.DAGScheduler: Submitting 2 missing tasks from Stage 6 (MappedRDD&lt;span class=&quot;error&quot;&gt;&amp;#91;558&amp;#93;&lt;/span&gt; at saveAsTextFile at Test.java:99)&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.TaskSchedulerImpl: Adding task set 6.0 with 2 tasks&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.TaskSetManager: Starting task 6.0:0 as TID 5 on executor 1: somehost.corp (PROCESS_LOCAL)&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.TaskSetManager: Serialized task 6.0:0 as 351777 bytes in 36 ms&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.TaskSetManager: Starting task 6.0:1 as TID 6 on executor 0: someotherhost.corp (PROCESS_LOCAL)&lt;br/&gt;
14/06/25 16:38:55 INFO scheduler.TaskSetManager: Serialized task 6.0:1 as 186453 bytes in 16 ms&lt;/p&gt;

&lt;p&gt;The test setup for reproducing this issue has two slaves (each with 24G) running spark standalone. The driver runs with Xmx 4G.&lt;/p&gt;

&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="14043738" author="pwendell" created="Wed, 25 Jun 2014 16:57:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=reachbach&quot; class=&quot;user-hover&quot; rel=&quot;reachbach&quot;&gt;reachbach&lt;/a&gt; If you are running on standalone mode, it might work if you go on every node in your cluster and add the following to spark-env.sh:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;export SPARK_JAVA_OPTS=&lt;span class=&quot;code-quote&quot;&gt;&quot;-Dspark.akka.frameSize=XXX&quot;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, this work around will only work if every job in your cluster is using the same frame size (XXX).&lt;/p&gt;

&lt;p&gt;The main recommendation is to upgrade to 1.0.1. We are very conservative about what we merge into maintenance branches, so we recommend users upgrade immediately once we release them.&lt;/p&gt;</comment>
                            <comment id="14043744" author="pwendell" created="Wed, 25 Jun 2014 16:59:17 +0000"  >&lt;p&gt;This is not resolved yet because it needs to be back ported into 0.9&lt;/p&gt;</comment>
                            <comment id="14064552" author="mengxr" created="Thu, 17 Jul 2014 04:01:52 +0000"  >&lt;p&gt;PR for branch-0.9: &lt;a href=&quot;https://github.com/apache/spark/pull/1455&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1455&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14064562" author="mengxr" created="Thu, 17 Jul 2014 04:31:18 +0000"  >&lt;p&gt;Issue resolved by pull request 1455&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/1455&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/1455&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14073244" author="djvulee" created="Thu, 24 Jul 2014 14:42:11 +0000"  >&lt;p&gt;Does anyone test in version0.9.2&#65292;I found it also failed , while  v1.0.1 &amp;amp; v1.1.0 is ok. &lt;/p&gt;</comment>
                            <comment id="14229531" author="joshrosen" created="Mon, 1 Dec 2014 08:29:53 +0000"  >&lt;p&gt;Looks like the &quot;Fix Versions&quot; accidentally got overwritten during a backport / cherry-pick, so I&apos;ve restored them based on the issue history.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12721097">SPARK-2138</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>382997</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 51 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1tzev:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>383265</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12326687">0.9.2</customfieldvalue>
    <customfieldvalue id="12326744">1.0.1</customfieldvalue>
    <customfieldvalue id="12326686">1.1.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>