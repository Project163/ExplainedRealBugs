<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:32:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-39399] proxy-user not working for Spark on k8s in cluster deploy mode</title>
                <link>https://issues.apache.org/jira/browse/SPARK-39399</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;As part of &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25355&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-25355&lt;/a&gt; Proxy user support was added for Spark on K8s. But the PR only added proxy user argument on the spark-submit command. The actual functionality of authentication using the proxy user is not working in case of cluster deploy mode.&lt;/p&gt;

&lt;p&gt;We get AccessControlException when trying to access the kerberized HDFS through a proxy user.&#160;&lt;/p&gt;

&lt;p&gt;Spark-Submit:&lt;br/&gt;
$SPARK_HOME/bin/spark-submit \&lt;br/&gt;
--master &amp;lt;K8S_APISERVER&amp;gt; \&lt;br/&gt;
--deploy-mode cluster \&lt;br/&gt;
--name with_proxy_user_di \&lt;br/&gt;
--proxy-user &amp;lt;username&amp;gt; \&lt;br/&gt;
--class org.apache.spark.examples.SparkPi \&lt;br/&gt;
--conf spark.kubernetes.container.image=&amp;lt;SPARK3.2_with_hadoop3.1_image&amp;gt; \&lt;br/&gt;
--conf spark.kubernetes.driver.limit.cores=1 \&lt;br/&gt;
--conf spark.executor.instances=1 \&lt;br/&gt;
--conf spark.kubernetes.authenticate.driver.serviceAccountName=spark \&lt;br/&gt;
--conf spark.kubernetes.namespace=&amp;lt;namespace_name&amp;gt; \&lt;br/&gt;
--conf spark.kubernetes.kerberos.krb5.path=/etc/krb5.conf \&lt;br/&gt;
--conf spark.eventLog.enabled=true \&lt;br/&gt;
--conf spark.eventLog.dir=hdfs://&amp;lt;hdfs_cluster&amp;gt;/scaas/shs_logs \&lt;/p&gt;

&lt;p&gt;--conf spark.kubernetes.file.upload.path=hdfs://&amp;lt;hdfs_cluster&amp;gt;/tmp \&lt;/p&gt;

&lt;p&gt;--conf spark.kubernetes.container.image.pullPolicy=Always \&lt;br/&gt;
$SPARK_HOME/examples/jars/spark-examples_2.12-3.2.0-1.jar &lt;br/&gt;
Driver Logs:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
++ id -u
+ myuid=185
++ id -g
+ mygid=0
+ set +e
++ getent passwd 185
+ uidentry=
+ set -e
+ &lt;span class=&quot;code-quote&quot;&gt;&apos;[&apos;&lt;/span&gt; -z &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos; &apos;&lt;/span&gt;]&apos;
+ &lt;span class=&quot;code-quote&quot;&gt;&apos;[&apos;&lt;/span&gt; -w /etc/passwd &lt;span class=&quot;code-quote&quot;&gt;&apos;]&apos;&lt;/span&gt;
+ echo &lt;span class=&quot;code-quote&quot;&gt;&apos;185:x:185:0:anonymous uid:/opt/spark:/bin/&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;&lt;/span&gt;
+ SPARK_CLASSPATH=&lt;span class=&quot;code-quote&quot;&gt;&apos;:/opt/spark/jars/*&apos;&lt;/span&gt;
+ env
+ grep SPARK_JAVA_OPT_
+ sort -t_ -k4 -n
+ sed &lt;span class=&quot;code-quote&quot;&gt;&apos;s/[^=]*=\(.*\)/\1/g&apos;&lt;/span&gt;
+ readarray -t SPARK_EXECUTOR_JAVA_OPTS
+ &lt;span class=&quot;code-quote&quot;&gt;&apos;[&apos;&lt;/span&gt; -n &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos; &apos;&lt;/span&gt;]&apos;
+ &lt;span class=&quot;code-quote&quot;&gt;&apos;[&apos;&lt;/span&gt; -z &lt;span class=&quot;code-quote&quot;&gt;&apos;]&apos;&lt;/span&gt;
+ &lt;span class=&quot;code-quote&quot;&gt;&apos;[&apos;&lt;/span&gt; -z &lt;span class=&quot;code-quote&quot;&gt;&apos;]&apos;&lt;/span&gt;
+ &lt;span class=&quot;code-quote&quot;&gt;&apos;[&apos;&lt;/span&gt; -n &lt;span class=&quot;code-quote&quot;&gt;&apos;&apos; &apos;&lt;/span&gt;]&apos;
+ &lt;span class=&quot;code-quote&quot;&gt;&apos;[&apos;&lt;/span&gt; -z x &lt;span class=&quot;code-quote&quot;&gt;&apos;]&apos;&lt;/span&gt;
+ SPARK_CLASSPATH=&lt;span class=&quot;code-quote&quot;&gt;&apos;/opt/hadoop/conf::/opt/spark/jars/*&apos;&lt;/span&gt;
+ &lt;span class=&quot;code-quote&quot;&gt;&apos;[&apos;&lt;/span&gt; -z x &lt;span class=&quot;code-quote&quot;&gt;&apos;]&apos;&lt;/span&gt;
+ SPARK_CLASSPATH=&lt;span class=&quot;code-quote&quot;&gt;&apos;/opt/spark/conf:/opt/hadoop/conf::/opt/spark/jars/*&apos;&lt;/span&gt;
+ &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&quot;$1&quot;&lt;/span&gt; in
+ shift 1
+ CMD=(&lt;span class=&quot;code-quote&quot;&gt;&quot;$SPARK_HOME/bin/spark-submit&quot;&lt;/span&gt; --conf &lt;span class=&quot;code-quote&quot;&gt;&quot;spark.driver.bindAddress=$SPARK_DRIVER_BIND_ADDRESS&quot;&lt;/span&gt; --deploy-mode client &lt;span class=&quot;code-quote&quot;&gt;&quot;$@&quot;&lt;/span&gt;)
+ exec /usr/bin/tini -s -- /opt/spark/bin/spark-submit --conf spark.driver.bindAddress=&amp;lt;addr&amp;gt; --deploy-mode client --proxy-user proxy_user --properties-file /opt/spark/conf/spark.properties --&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.examples.SparkPi spark-internal
WARNING: An illegal reflective access operation has occurred
WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.2.0-1.jar) to constructor java.nio.DirectByteBuffer(&lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt;,&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;)
WARNING: Please consider reporting &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; to the maintainers of org.apache.spark.unsafe.Platform
WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations
WARNING: All illegal access operations will be denied in a &lt;span class=&quot;code-keyword&quot;&gt;future&lt;/span&gt; release
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=&lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;, sampleName=&quot;&lt;/span&gt;Ops&lt;span class=&quot;code-quote&quot;&gt;&quot;, always=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, type=DEFAULT, value={&quot;&lt;/span&gt;Rate of successful kerberos logins and latency (milliseconds)&lt;span class=&quot;code-quote&quot;&gt;&quot;}, valueName=&quot;&lt;/span&gt;Time&quot;)
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=&lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;, sampleName=&quot;&lt;/span&gt;Ops&lt;span class=&quot;code-quote&quot;&gt;&quot;, always=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, type=DEFAULT, value={&quot;&lt;/span&gt;Rate of failed kerberos logins and latency (milliseconds)&lt;span class=&quot;code-quote&quot;&gt;&quot;}, valueName=&quot;&lt;/span&gt;Time&quot;)
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=&lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;, sampleName=&quot;&lt;/span&gt;Ops&lt;span class=&quot;code-quote&quot;&gt;&quot;, always=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, type=DEFAULT, value={&quot;&lt;/span&gt;GetGroups&lt;span class=&quot;code-quote&quot;&gt;&quot;}, valueName=&quot;&lt;/span&gt;Time&quot;)
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=&lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;, sampleName=&quot;&lt;/span&gt;Ops&lt;span class=&quot;code-quote&quot;&gt;&quot;, always=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, type=DEFAULT, value={&quot;&lt;/span&gt;Renewal failures since startup&lt;span class=&quot;code-quote&quot;&gt;&quot;}, valueName=&quot;&lt;/span&gt;Time&quot;)
22/04/26 08:54:38 DEBUG MutableMetricsFactory: field &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(about=&lt;span class=&quot;code-quote&quot;&gt;&quot;&quot;, sampleName=&quot;&lt;/span&gt;Ops&lt;span class=&quot;code-quote&quot;&gt;&quot;, always=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, type=DEFAULT, value={&quot;&lt;/span&gt;Renewal failures since last successful login&lt;span class=&quot;code-quote&quot;&gt;&quot;}, valueName=&quot;&lt;/span&gt;Time&quot;)
22/04/26 08:54:38 DEBUG MetricsSystemImpl: UgiMetrics, User and group related metrics
22/04/26 08:54:38 DEBUG SecurityUtil: Setting hadoop.security.token.service.use_ip to &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
22/04/26 08:54:38 DEBUG Shell: Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
&#160; &#160; at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:469)
&#160; &#160; at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:440)
&#160; &#160; at org.apache.hadoop.util.Shell.&amp;lt;clinit&amp;gt;(Shell.java:517)
&#160; &#160; at org.apache.hadoop.util.StringUtils.&amp;lt;clinit&amp;gt;(StringUtils.java:78)
&#160; &#160; at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
&#160; &#160; at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:102)
&#160; &#160; at org.apache.hadoop.security.SecurityUtil.&amp;lt;clinit&amp;gt;(SecurityUtil.java:86)
&#160; &#160; at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:315)
&#160; &#160; at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:303)
&#160; &#160; at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1827)
&#160; &#160; at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:709)
&#160; &#160; at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:659)
&#160; &#160; at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:570)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:161)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
22/04/26 08:54:38 DEBUG Shell: setsid exited with exit code 0
22/04/26 08:54:38 DEBUG Groups: &#160;Creating &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Groups object
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: backing jks path initialized to file:/etc/security/bind.jceks
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: initialized local file as &lt;span class=&quot;code-quote&quot;&gt;&apos;/etc/security/bind.jceks&apos;&lt;/span&gt;.
22/04/26 08:54:38 DEBUG AbstractJavaKeyStoreProvider: the local file does not exist.
22/04/26 08:54:38 DEBUG LdapGroupsMapping: Usersearch baseDN: dc=&amp;lt;dc&amp;gt;
22/04/26 08:54:38 DEBUG LdapGroupsMapping: Groupsearch baseDN: dc=&amp;lt;dc&amp;gt;
22/04/26 08:54:38 DEBUG Groups: Group mapping impl=org.apache.hadoop.security.LdapGroupsMapping; cacheTimeout=300000; warningDeltaMs=5000
22/04/26 08:54:38 DEBUG UserGroupInformation: hadoop login
22/04/26 08:54:38 DEBUG UserGroupInformation: hadoop login commit
22/04/26 08:54:38 DEBUG UserGroupInformation: using local user:UnixPrincipal: 185
22/04/26 08:54:38 DEBUG UserGroupInformation: Using user: &lt;span class=&quot;code-quote&quot;&gt;&quot;UnixPrincipal: 185&quot;&lt;/span&gt; with name 185
22/04/26 08:54:38 DEBUG UserGroupInformation: User entry: &lt;span class=&quot;code-quote&quot;&gt;&quot;185&quot;&lt;/span&gt;
22/04/26 08:54:38 DEBUG UserGroupInformation: Reading credentials from location set in HADOOP_TOKEN_FILE_LOCATION: /mnt/secrets/hadoop-credentials/..2022_04_26_08_54_34.1262645511/hadoop-tokens
22/04/26 08:54:39 DEBUG UserGroupInformation: Loaded 3 tokens
22/04/26 08:54:39 DEBUG UserGroupInformation: UGI loginUser:185 (auth:SIMPLE)
22/04/26 08:54:39 DEBUG UserGroupInformation: PrivilegedAction as:proxy_user (auth:PROXY) via 185 (auth:SIMPLE) from:org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163)
22/04/26 08:54:39 DEBUG FileSystem: Loading filesystems
22/04/26 08:54:39 DEBUG FileSystem: file:&lt;span class=&quot;code-comment&quot;&gt;// = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.fs.LocalFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG FileSystem: viewfs:&lt;span class=&quot;code-comment&quot;&gt;// = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.fs.viewfs.ViewFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG FileSystem: har:&lt;span class=&quot;code-comment&quot;&gt;// = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.fs.HarFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG FileSystem: http:&lt;span class=&quot;code-comment&quot;&gt;// = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.fs.http.HttpFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG FileSystem: https:&lt;span class=&quot;code-comment&quot;&gt;// = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.fs.http.HttpsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG FileSystem: hdfs:&lt;span class=&quot;code-comment&quot;&gt;// = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.hdfs.DistributedFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG FileSystem: webhdfs:&lt;span class=&quot;code-comment&quot;&gt;// = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.hdfs.web.WebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG FileSystem: swebhdfs:&lt;span class=&quot;code-comment&quot;&gt;// = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.hdfs.web.SWebHdfsFileSystem from /opt/spark/jars/hadoop-client-api-3.1.1.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG FileSystem: nullscan:&lt;span class=&quot;code-comment&quot;&gt;// = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.hive.ql.io.NullScanFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG FileSystem: file:&lt;span class=&quot;code-comment&quot;&gt;// = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.hive.ql.io.ProxyLocalFileSystem from /opt/spark/jars/hive-exec-2.3.9-core.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG FileSystem: Looking &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; FS supporting hdfs
22/04/26 08:54:39 DEBUG FileSystem: looking &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; configuration option fs.hdfs.impl
22/04/26 08:54:39 DEBUG FileSystem: Looking in service filesystems &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; implementation class
22/04/26 08:54:39 DEBUG FileSystem: FS &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; hdfs is &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.hdfs.DistributedFileSystem
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.read.shortcircuit = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.domain.socket.path = /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/hadoop-hdfs/dn_socket
22/04/26 08:54:39 DEBUG DFSClient: Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
22/04/26 08:54:39 DEBUG HAUtilClient: No HA service delegation token found &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; logical URI hdfs:&lt;span class=&quot;code-comment&quot;&gt;//&amp;lt;hdfs&amp;gt;/tmp/spark-upload-bf713a0c-166b-43fc-a5e6-24957e75b224/spark-examples_2.12-3.0.1.jar
&lt;/span&gt;22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.use.legacy.blockreader.local = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.read.shortcircuit = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.client.domain.socket.data.traffic = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;
22/04/26 08:54:39 DEBUG DfsClientConf: dfs.domain.socket.path = /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/hadoop-hdfs/dn_socket
22/04/26 08:54:39 DEBUG RetryUtils: multipleLinearRandomRetry = &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
22/04/26 08:54:39 DEBUG Server: rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@4a325eb9
22/04/26 08:54:39 DEBUG Client: getting client out of cache: org.apache.hadoop.ipc.Client@2577d6c8
22/04/26 08:54:40 DEBUG NativeCodeLoader: Trying to load the custom-built &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library...
22/04/26 08:54:40 DEBUG NativeCodeLoader: Failed to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path: [/usr/java/packages/lib, /usr/lib64, /lib64, /lib, /usr/lib]
22/04/26 08:54:40 DEBUG NativeCodeLoader: java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
22/04/26 08:54:40 WARN NativeCodeLoader: Unable to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your platform... using builtin-java classes where applicable
22/04/26 08:54:40 WARN DomainSocketFactory: The &lt;span class=&quot;code-object&quot;&gt;short&lt;/span&gt;-circuit local reads feature cannot be used because libhadoop cannot be loaded.
22/04/26 08:54:40 DEBUG DataTransferSaslUtil: DataTransferProtocol using SaslPropertiesResolver, configured QOP dfs.data.transfer.protection = authentication,privacy, configured &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;dfs.data.transfer.saslproperties.resolver.class = &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.security.SaslPropertiesResolver
22/04/26 08:54:40 DEBUG Client: The ping interval is 60000 ms.
22/04/26 08:54:40 DEBUG Client: Connecting to &amp;lt;server&amp;gt;/&amp;lt;ip&amp;gt;:8020
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedAction as:185 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:796)
22/04/26 08:54:40 DEBUG SaslRpcClient: Sending sasl message state: NEGOTIATE22/04/26 08:54:40 DEBUG SaslRpcClient: Get token info proto:&lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolPB info:@org.apache.hadoop.security.token.TokenInfo(value=org.apache.hadoop.hdfs.security.token.delegation.DelegationTokenSelector.class)
22/04/26 08:54:40 DEBUG SaslRpcClient: tokens aren&lt;span class=&quot;code-quote&quot;&gt;&apos;t supported &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; protocol or user doesn&apos;&lt;/span&gt;t have one
22/04/26 08:54:40 DEBUG SaslRpcClient: client isn&apos;t using kerberos
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedActionException as:185 (auth:SIMPLE) cause:org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedAction as:185 (auth:SIMPLE) from:org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720)
22/04/26 08:54:40 WARN Client: Exception encountered &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; connecting to the server : org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
22/04/26 08:54:40 DEBUG UserGroupInformation: PrivilegedActionException as:185 (auth:SIMPLE) cause:java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
22/04/26 08:54:40 DEBUG Client: closing ipc connection to &amp;lt;server&amp;gt;/&amp;lt;ip&amp;gt;:8020: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
&#160; &#160; at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:757)
&#160; &#160; at java.base/java.security.AccessController.doPrivileged(Native Method)
&#160; &#160; at java.base/javax.security.auth.Subject.doAs(Unknown Source)
&#160; &#160; at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
&#160; &#160; at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:720)
&#160; &#160; at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:813)
&#160; &#160; at org.apache.hadoop.ipc.Client$Connection.access$3600(Client.java:410)
&#160; &#160; at org.apache.hadoop.ipc.Client.getConnection(Client.java:1558)
&#160; &#160; at org.apache.hadoop.ipc.Client.call(Client.java:1389)
&#160; &#160; at org.apache.hadoop.ipc.Client.call(Client.java:1353)
&#160; &#160; at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
&#160; &#160; at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
&#160; &#160; at com.sun.proxy.$Proxy14.getFileInfo(Unknown Source)
&#160; &#160; at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getFileInfo(ClientNamenodeProtocolTranslatorPB.java:900)
&#160; &#160; at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
&#160; &#160; at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
&#160; &#160; at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
&#160; &#160; at java.base/java.lang.reflect.Method.invoke(Unknown Source)
&#160; &#160; at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
&#160; &#160; at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
&#160; &#160; at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
&#160; &#160; at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
&#160; &#160; at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
&#160; &#160; at com.sun.proxy.$Proxy15.getFileInfo(Unknown Source)
&#160; &#160; at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:1654)
&#160; &#160; at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1579)
&#160; &#160; at org.apache.hadoop.hdfs.DistributedFileSystem$29.doCall(DistributedFileSystem.java:1576)
&#160; &#160; at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
&#160; &#160; at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:1591)
&#160; &#160; at org.apache.hadoop.fs.Globber.getFileStatus(Globber.java:65)
&#160; &#160; at org.apache.hadoop.fs.Globber.doGlob(Globber.java:270)
&#160; &#160; at org.apache.hadoop.fs.Globber.glob(Globber.java:149)
&#160; &#160; at org.apache.hadoop.fs.FileSystem.globStatus(FileSystem.java:2067)
&#160; &#160; at org.apache.spark.util.DependencyUtils$.resolveGlobPath(DependencyUtils.scala:318)
&#160; &#160; at org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2(DependencyUtils.scala:273)
&#160; &#160; at org.apache.spark.util.DependencyUtils$.$anonfun$resolveGlobPaths$2$adapted(DependencyUtils.scala:271)
&#160; &#160; at scala.collection.TraversableLike.$anonfun$flatMap$1(TraversableLike.scala:293)
&#160; &#160; at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)
&#160; &#160; at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)
&#160; &#160; at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:38)
&#160; &#160; at scala.collection.TraversableLike.flatMap(TraversableLike.scala:293)
&#160; &#160; at scala.collection.TraversableLike.flatMap$(TraversableLike.scala:290)
&#160; &#160; at scala.collection.AbstractTraversable.flatMap(Traversable.scala:108)
&#160; &#160; at org.apache.spark.util.DependencyUtils$.resolveGlobPaths(DependencyUtils.scala:271)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.$anonfun$prepareSubmitEnvironment$4(SparkSubmit.scala:364)
&#160; &#160; at scala.Option.map(Option.scala:230)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:364)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:898)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:165)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit$$anon$1.run(SparkSubmit.scala:163)
&#160; &#160; at java.base/java.security.AccessController.doPrivileged(Native Method)
&#160; &#160; at java.base/javax.security.auth.Subject.doAs(Unknown Source)
&#160; &#160; at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:163)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1043)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1052)
&#160; &#160; at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
&#160; &#160; at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:173)
&#160; &#160; at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:390)
&#160; &#160; at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:614)
&#160; &#160; at org.apache.hadoop.ipc.Client$Connection.access$2300(Client.java:410)
&#160; &#160; at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:800)
&#160; &#160; at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:796)
&#160; &#160; at java.base/java.security.AccessController.doPrivileged(Native Method)
&#160; &#160; at java.base/javax.security.auth.Subject.doAs(Unknown Source)
&#160; &#160; at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
&#160; &#160; at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:796)
&#160; &#160; ... 53 more  &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The reason for no delegation token found is that the proxy user UGI doesn&apos;t have any credentials/tokens ( tokenSize:: 0 )&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: HDFS_DELEGATION_TOKEN, Service: ha-hdfs:&amp;lt;hdfs&amp;gt;, Ident: (token &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; proxyUser: HDFS_DELEGATION_TOKEN owner=proxyUser, renewer=proxyUser, realUser=superuser/test@test.com, issueDate=1651165129518, maxDate=1651769929518, sequenceNumber=180516, masterKeyId=601)
22/04/28 16:59:37 DEBUG Token: Cannot find &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; token kind HIVE_DELEGATION_TOKEN
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: HIVE_DELEGATION_TOKEN, Service: , Ident: 00 08 73 68 72 70 72 61 73 61 04 68 69 76 65 1e 6c 69 76 79 2f 6c 69 76 79 2d 69 6e 74 40 43 4f 52 50 44 45 56 2e 56 49 53 41 2e 43 4f 4d 8a 01 80 71 1c 71 b5 8a 01 80 b9 35 79 b5 8e 15 cd 8e 03 6e
22/04/28 16:59:37 DEBUG UserGroupInformation: loginUser-token::Kind: kms-dt, Service: &amp;lt;ip&amp;gt;:9292, Ident: (kms-dt owner=proxyUser, renewer=proxyUser, realUser=superuser, issueDate=1651165129566, maxDate=1651769929566, sequenceNumber=181197, masterKeyId=1152)
22/04/28 16:59:37 DEBUG UserGroupInformation: UGI loginUser:185 (auth:SIMPLE)
22/04/28 16:59:37 DEBUG UserGroupInformation: createProxyUser: from:org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
22/04/28 16:59:37 DEBUG UserGroupInformation: proxy user created, ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE) &#160;subject::Subject:
&#160; &#160; Principal: proxyUser
&#160; &#160; Principal: 185 (auth:SIMPLE)
&#160;tokenSize:: 0 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
22/04/28 16:59:38 DEBUG AbstractNNFailoverProxyProvider: ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE) &#160;tokensize:: 0
22/04/28 16:59:38 DEBUG HAUtilClient: ugi::proxyUser (auth:PROXY) via 185 (auth:SIMPLE) &#160;tokenSize::0
22/04/28 16:59:38 DEBUG AbstractDelegationTokenSelector: kindName:: HDFS_DELEGATION_TOKEN &#160;service:: ha-hdfs:&amp;lt;hdfs&amp;gt; tokens size:: 0
22/04/28 16:59:38 DEBUG HAUtilClient: No HA service delegation token found &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; logical URI hdfs:&lt;span class=&quot;code-comment&quot;&gt;//&amp;lt;hdfs&amp;gt;:8020/tmp/spark-upload-10582dde-f07c-4bf7-a611-5afbdd12ff6c/spark-examples_2.12-3.0.1.jar &lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Please refer to the last 4 comments on &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-25355&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-25355&lt;/a&gt;.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13448766">SPARK-39399</key>
            <summary>proxy-user not working for Spark on k8s in cluster deploy mode</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="unamesk15">Shrikant Prasad</assignee>
                                    <reporter username="unamesk15">Shrikant Prasad</reporter>
                        <labels>
                    </labels>
                <created>Tue, 7 Jun 2022 10:15:39 +0000</created>
                <updated>Tue, 14 Mar 2023 15:51:20 +0000</updated>
                            <resolved>Wed, 8 Mar 2023 03:35:52 +0000</resolved>
                                    <version>3.2.0</version>
                                    <fixVersion>3.2.4</fixVersion>
                    <fixVersion>3.3.3</fixVersion>
                    <fixVersion>3.4.0</fixVersion>
                                    <component>Kubernetes</component>
                    <component>Spark Core</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="17550919" author="JIRAUSER280449" created="Tue, 7 Jun 2022 10:26:42 +0000"  >&lt;p&gt;I would like to work on this issue. Have a working solution which is in-line with proxy-user implementation for Spark on Yarn and Mesos.&lt;/p&gt;</comment>
                            <comment id="17554904" author="pralabhkumar" created="Thu, 16 Jun 2022 04:56:13 +0000"  >&lt;p&gt;ping &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt;&#160; , please help us on the same or please provide some reference who can take this forward.&#160;&#160;&lt;/p&gt;</comment>
                            <comment id="17556302" author="pralabhkumar" created="Mon, 20 Jun 2022 09:55:28 +0000"  >&lt;p&gt;Gentle ping &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt; &#160; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dongjoon&quot; class=&quot;user-hover&quot; rel=&quot;dongjoon&quot;&gt;dongjoon&lt;/a&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17581495" author="JIRAUSER280449" created="Thu, 18 Aug 2022 19:12:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dongjoon&quot; class=&quot;user-hover&quot; rel=&quot;dongjoon&quot;&gt;dongjoon&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt; Can you please have a look at this issue and let me know if I need to add any more details in order to take this forward.&lt;/p&gt;</comment>
                            <comment id="17604810" author="apachespark" created="Wed, 14 Sep 2022 15:42:39 +0000"  >&lt;p&gt;User &apos;shrprasa&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/37880&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/37880&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17627528" author="jianghuazhu" created="Wed, 2 Nov 2022 08:32:18 +0000"  >&lt;p&gt;It looks like HIVE_DELEGATION_TOKEN is not loaded and populated to Token#tokenKindMap.&lt;br/&gt;
Here are some sources of reference:&lt;br/&gt;
 &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13051704/13051704_screenshot-1.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;We should first check the dependencies related to hive. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=unamesk15&quot; class=&quot;user-hover&quot; rel=&quot;unamesk15&quot;&gt;unamesk15&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17697716" author="qin yao" created="Wed, 8 Mar 2023 03:35:52 +0000"  >&lt;p&gt;Issue resolved by pull request 37880&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/37880&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/37880&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                            <outwardlinks description="causes">
                                        <issuelink>
            <issuekey id="13528391">SPARK-42785</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="13051704" name="screenshot-1.png" size="191788" author="jianghuazhu" created="Wed, 2 Nov 2022 08:31:49 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            2 years, 35 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z130i0:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>