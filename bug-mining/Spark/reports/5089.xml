<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:55:05 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-19471] A confusing NullPointerException when creating table</title>
                <link>https://issues.apache.org/jira/browse/SPARK-19471</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;After upgrading our Spark from 1.6.2 to 2.1.0, I encounter a confusing NullPointerException when creating table under Spark 2.1.0, but the problem does not exists in Spark 1.6.1. &lt;/p&gt;

&lt;p&gt;Environment: Hive 1.2.1, Hadoop 2.6.4 &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;==================== Code ==================== 
// spark is an instance of HiveContext 
// merge is a Hive UDF 
val df = spark.sql(&quot;SELECT merge(field_a, null) AS new_a, field_b AS new_b FROM tb_1 group by field_a, field_b&quot;) 
df.createTempView(&quot;tb_temp&quot;) 
spark.sql(&quot;create table tb_result stored as parquet as &quot; + 
  &quot;SELECT new_a&quot; + 
  &quot;FROM tb_temp&quot; + 
  &quot;LEFT JOIN `tb_2` ON &quot; + 
  &quot;if(((`tb_temp`.`new_b`) = &apos;&apos; OR (`tb_temp`.`new_b`) IS NULL), concat(&apos;GrLSRwZE_&apos;, cast((rand() * 200) AS int)), (`tb_temp`.`new_b`)) = `tb_2`.`fka6862f17`&quot;) 

==================== Physical Plan ==================== 
*Project [new_a] 
+- *BroadcastHashJoin [if (((new_b = ) || isnull(new_b))) concat(GrLSRwZE_, cast(cast((_nondeterministic * 200.0) as int) as string)) else new_b], [fka6862f17], LeftOuter, BuildRight 
   :- HashAggregate(keys=[field_a, field_b], functions=[], output=[new_a, new_b, _nondeterministic]) 
   :  +- Exchange(coordinator ) hashpartitioning(field_a, field_b, 180), coordinator[target post-shuffle partition size: 1024880] 
   :     +- *HashAggregate(keys=[field_a, field_b], functions=[], output=[field_a, field_b]) 
   :        +- *FileScan parquet bdp.tb_1[field_a,field_b] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_1, PartitionFilters: [], PushedFilters: [], ReadSchema: struct 
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) 
      +- *Project [fka6862f17] 
         +- *FileScan parquet bdp.tb_2[fka6862f17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct 

What does &apos;*&apos; mean before HashAggregate? 

==================== Exception ==================== 
org.apache.spark.SparkException: Task failed while writing rows 
... 
java.lang.NullPointerException 
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply_2$(Unknown Source) 
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source) 
        at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$3.apply(AggregationIterator.scala:260) 
        at org.apache.spark.sql.execution.aggregate.AggregationIterator$$anonfun$generateResultProjection$3.apply(AggregationIterator.scala:259) 
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.next(TungstenAggregationIterator.scala:392) 
        at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.next(TungstenAggregationIterator.scala:79) 
        at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIterator.processNext(Unknown Source) 
        at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43) 
        at org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$8$$anon$1.hasNext(WholeStageCodegenExec.scala:377) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:252) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:199) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:197) 
        at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1341) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:202) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$4.apply(FileFormatWriter.scala:138) 
        at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$4.apply(FileFormatWriter.scala:137) 
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87) 
        at org.apache.spark.scheduler.Task.run(Task.scala:99) 
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:282) 
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) 
        at java.lang.Thread.run(Thread.java:745) 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I also found that when I changed my code as follow: &lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;spark.sql(&quot;create table tb_result stored as parquet as &quot; + 
  &quot;SELECT new_b&quot; + 
  &quot;FROM tb_temp&quot; + 
  &quot;LEFT JOIN `tb_2` ON &quot; + 
  &quot;if(((`tb_temp`.`new_b`) = &apos;&apos; OR (`tb_temp`.`new_b`) IS NULL), concat(&apos;GrLSRwZE_&apos;, cast((rand() * 200) AS int)), (`tb_temp`.`new_b`)) = `tb_2`.`fka6862f17`&quot;) 

or 

spark.sql(&quot;create table tb_result stored as parquet as &quot; + 
  &quot;SELECT new_a&quot; + 
  &quot;FROM tb_temp&quot; + 
  &quot;LEFT JOIN `tb_2` ON &quot; + 
  &quot;if(((`tb_temp`.`new_b`) = &apos;&apos; OR (`tb_temp`.`new_b`) IS NULL), concat(&apos;GrLSRwZE_&apos;, cast((200) AS int)), (`tb_temp`.`new_b`)) = `tb_2`.`fka6862f17`&quot;) 

will not have this problem. 

== Physical Plan of select new_b ... == 
*Project [new_b] 
+- *BroadcastHashJoin [if (((new_b = ) || isnull(new_b))) concat(GrLSRwZE_, cast(cast((_nondeterministic * 200.0) as int) as string)) else new_b], [fka6862f17], LeftOuter, BuildRight 
   :- *HashAggregate(keys=[field_a, field_b], functions=[], output=[new_b, _nondeterministic]) 
   :  +- Exchange(coordinator ) hashpartitioning(field_a, field_b, 180), coordinator[target post-shuffle partition size: 1024880] 
   :     +- *HashAggregate(keys=[field_a, field_b], functions=[], output=[field_a, field_b]) 
   :        +- *FileScan parquet bdp.tb_1[field_a,field_b] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_1, PartitionFilters: [], PushedFilters: [], ReadSchema: struct 
   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, string, true])) 
      +- *Project [fka6862f17] 
         +- *FileScan parquet bdp.tb_2[fka6862f17] Batched: true, Format: Parquet, Location: InMemoryFileIndex[hdfs://hdcluster/data/tb_2, PartitionFilters: [], PushedFilters: [], ReadSchema: struct 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Difference is `HashAggregate(keys=&lt;span class=&quot;error&quot;&gt;&amp;#91;field_a, field_b&amp;#93;&lt;/span&gt;, functions=[], output=&lt;span class=&quot;error&quot;&gt;&amp;#91;new_b, _nondeterministic&amp;#93;&lt;/span&gt;)` has a &apos;*&apos; char before it. &lt;/p&gt;

&lt;p&gt;It looks like something wrong with WholeStageCodegen when combine HiveUDF + rand() + group by + join. &lt;/p&gt;</description>
                <environment></environment>
        <key id="13040527">SPARK-19471</key>
            <summary>A confusing NullPointerException when creating table</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="donnyzone">Feng Zhu</assignee>
                                    <reporter username="stanzhai">StanZhai</reporter>
                        <labels>
                    </labels>
                <created>Mon, 6 Feb 2017 04:26:51 +0000</created>
                <updated>Wed, 31 Jan 2018 17:51:22 +0000</updated>
                            <resolved>Mon, 14 Aug 2017 16:46:14 +0000</resolved>
                                    <version>2.1.0</version>
                                    <fixVersion>2.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="15853906" author="apachespark" created="Mon, 6 Feb 2017 12:22:04 +0000"  >&lt;p&gt;User &apos;yangw1234&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16820&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16820&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16097221" author="cenyuhai" created="Sat, 22 Jul 2017 10:03:33 +0000"  >&lt;p&gt;I try this pr, it works well.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 17 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i39n8v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>