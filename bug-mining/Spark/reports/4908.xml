<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:53:55 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-19185] ConcurrentModificationExceptions with CachedKafkaConsumers when Windowing</title>
                <link>https://issues.apache.org/jira/browse/SPARK-19185</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;We&apos;ve been running into ConcurrentModificationExcpetions &quot;KafkaConsumer is not safe for multi-threaded access&quot; with the CachedKafkaConsumer. I&apos;ve been working through debugging this issue and after looking through some of the spark source code I think this is a bug.&lt;/p&gt;

&lt;p&gt;Our set up is:&lt;br/&gt;
Spark 2.0.2, running in Mesos 0.28.0-2 in client mode, using Spark-Streaming-Kafka-010&lt;br/&gt;
spark.executor.cores 1&lt;br/&gt;
spark.mesos.extra.cores 1&lt;/p&gt;

&lt;p&gt;Batch interval: 10s, window interval: 180s, and slide interval: 30s&lt;/p&gt;

&lt;p&gt;We would see the exception when in one executor there are two task worker threads assigned the same Topic+Partition, but a different set of offsets.&lt;/p&gt;

&lt;p&gt;They would both get the same CachedKafkaConsumer, and whichever task thread went first would seek and poll for all the records, and at the same time the second thread would try to seek to its offset but fail because it is unable to acquire the lock.&lt;/p&gt;

&lt;p&gt;Time0 E0 Task0 - TopicPartition(&quot;abc&quot;, 0) X to Y&lt;br/&gt;
Time0 E0 Task1 - TopicPartition(&quot;abc&quot;, 0) Y to Z&lt;/p&gt;

&lt;p&gt;Time1 E0 Task0 - Seeks and starts to poll&lt;br/&gt;
Time1 E0 Task1 - Attempts to seek, but fails&lt;/p&gt;

&lt;p&gt;Here are some relevant logs:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;17/01/06 03:10:01 Executor task launch worker-1 INFO KafkaRDD: Computing topic test-topic, partition 2 offsets 4394204414 -&amp;gt; 4394238058
17/01/06 03:10:01 Executor task launch worker-0 INFO KafkaRDD: Computing topic test-topic, partition 2 offsets 4394238058 -&amp;gt; 4394257712
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer: Get spark-executor-consumer test-topic 2 nextOffset 4394204414 requested 4394204414
17/01/06 03:10:01 Executor task launch worker-0 DEBUG CachedKafkaConsumer: Get spark-executor-consumer test-topic 2 nextOffset 4394204414 requested 4394238058
17/01/06 03:10:01 Executor task launch worker-0 INFO CachedKafkaConsumer: Initial fetch &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; spark-executor-consumer test-topic 2 4394238058
17/01/06 03:10:01 Executor task launch worker-0 DEBUG CachedKafkaConsumer: Seeking to test-topic-2 4394238058
17/01/06 03:10:01 Executor task launch worker-0 WARN BlockManager: Putting block rdd_199_2 failed due to an exception
17/01/06 03:10:01 Executor task launch worker-0 WARN BlockManager: Block rdd_199_2 could not be removed as it was not found on disk or in memory
17/01/06 03:10:01 Executor task launch worker-0 ERROR Executor: Exception in task 49.0 in stage 45.0 (TID 3201)
java.util.ConcurrentModificationException: KafkaConsumer is not safe &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; multi-threaded access
	at org.apache.kafka.clients.consumer.KafkaConsumer.acquire(KafkaConsumer.java:1431)
	at org.apache.kafka.clients.consumer.KafkaConsumer.seek(KafkaConsumer.java:1132)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.seek(CachedKafkaConsumer.scala:95)
	at org.apache.spark.streaming.kafka010.CachedKafkaConsumer.get(CachedKafkaConsumer.scala:69)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:227)
	at org.apache.spark.streaming.kafka010.KafkaRDD$KafkaRDDIterator.next(KafkaRDD.scala:193)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:462)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:408)
	at org.apache.spark.storage.memory.MemoryStore.putIteratorAsBytes(MemoryStore.scala:360)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:951)
	at org.apache.spark.storage.BlockManager$$anonfun$doPutIterator$1.apply(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:866)
	at org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:926)
	at org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:670)
	at org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:330)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:281)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:105)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:319)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:283)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:79)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:47)
	at org.apache.spark.scheduler.Task.run(Task.scala:86)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:274)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer: Polled [test-topic-2]  8237
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer: Get spark-executor-consumer test-topic 2 nextOffset 4394204415 requested 4394204415
17/01/06 03:10:01 Executor task launch worker-1 DEBUG CachedKafkaConsumer: Get spark-executor-consumer test-topic 2 nextOffset 4394204416 requested 4394204416
... 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It looks like when WindowedDStream does the getOrCompute call its computing all the sets of of offsets it needs and tries to farm out the work in parallel. So each available worker task gets each set of offsets that need to be read.&lt;/p&gt;

&lt;p&gt;After realizing what was going on I tested four states:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;spark.executor.cores 1 and spark.mesos.extra.cores 0
	&lt;ul&gt;
		&lt;li&gt;No Exceptions&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;spark.executor.cores 1 and spark.mesos.extra.cores 1
	&lt;ul&gt;
		&lt;li&gt;ConcurrentModificationException&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;spark.executor.cores 2 and spark.mesos.extra.cores 0
	&lt;ul&gt;
		&lt;li&gt;ConcurrentModificationException&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;spark.executor.cores 2 and spark.mesos.extra.cores 1
	&lt;ul&gt;
		&lt;li&gt;ConcurrentModificationException&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Minimal set of code I was able to reproduce with:&lt;br/&gt;
Streaming batch interval was set to 2 seconds. This increased the rate of exceptions I saw.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;val kafkaParams = Map[&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;](
  &lt;span class=&quot;code-quote&quot;&gt;&quot;bootstrap.servers&quot;&lt;/span&gt; -&amp;gt; brokers,
  &lt;span class=&quot;code-quote&quot;&gt;&quot;key.deserializer&quot;&lt;/span&gt; -&amp;gt; classOf[KafkaAvroDeserializer],
  &lt;span class=&quot;code-quote&quot;&gt;&quot;value.deserializer&quot;&lt;/span&gt; -&amp;gt; classOf[KafkaAvroDeserializer],
  &lt;span class=&quot;code-quote&quot;&gt;&quot;enable.auto.commit&quot;&lt;/span&gt; -&amp;gt; (&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;: java.lang.&lt;span class=&quot;code-object&quot;&gt;Boolean&lt;/span&gt;),
  &lt;span class=&quot;code-quote&quot;&gt;&quot;group.id&quot;&lt;/span&gt; -&amp;gt; groupId,
  &lt;span class=&quot;code-quote&quot;&gt;&quot;schema.registry.url&quot;&lt;/span&gt; -&amp;gt; schemaRegistryUrl,
  &lt;span class=&quot;code-quote&quot;&gt;&quot;auto.offset.reset&quot;&lt;/span&gt; -&amp;gt; offset
)
val inputStream = KafkaUtils.createDirectStream[&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;](
  ssc,
  PreferConsistent,
  Subscribe[&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;, &lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;]
    (kafkaTopic, kafkaParams)
)
val windowStream = inputStream.map(_.toString).window(Seconds(180), Seconds(30))

windowStream.foreachRDD{
  rdd =&amp;gt; {
    val filtered = rdd.filter(_.contains(&lt;span class=&quot;code-quote&quot;&gt;&quot;idb&quot;&lt;/span&gt;))

    filtered.foreach(
      message =&amp;gt; {
        &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; i = 0
        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (i == 0) {
          logger.info(message)
          i = i + 1
        }
      }
    )
  }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment>&lt;p&gt;Spark 2.0.2&lt;br/&gt;
Spark Streaming Kafka 010&lt;br/&gt;
Mesos 0.28.0 - client mode&lt;/p&gt;

&lt;p&gt;spark.executor.cores 1&lt;br/&gt;
spark.mesos.extra.cores 1&lt;/p&gt;</environment>
        <key id="13034056">SPARK-19185</key>
            <summary>ConcurrentModificationExceptions with CachedKafkaConsumers when Windowing</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gsomogyi">Gabor Somogyi</assignee>
                                    <reporter username="kalvinnchau">Kalvin Chau</reporter>
                        <labels>
                            <label>streaming</label>
                            <label>windowing</label>
                    </labels>
                <created>Thu, 12 Jan 2017 02:16:56 +0000</created>
                <updated>Wed, 15 May 2019 08:17:40 +0000</updated>
                            <resolved>Tue, 22 May 2018 20:44:08 +0000</resolved>
                                    <version>2.0.2</version>
                                    <fixVersion>2.4.0</fixVersion>
                                    <component>DStreams</component>
                        <due></due>
                            <votes>9</votes>
                                    <watches>27</watches>
                                                                                                                <comments>
                            <comment id="15823164" author="cody@koeninger.org" created="Sun, 15 Jan 2017 15:17:42 +0000"  >&lt;p&gt;This is a good error report, sorry it&apos;s taken me a while to get back to you on this.&lt;/p&gt;

&lt;p&gt;My immediate suggestions to you as a workaround would be&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Try persist before windowing, so that batches of offsets from Kafka are only fetched once, rather than repeatedly and possibly simultaneously for a given kafka partition.  I&apos;m assuming that&apos;s the underlying issue, but could be wrong.&lt;/li&gt;
	&lt;li&gt;Failing that, KafkaRDD&apos;s constructor takes a boolean parameter indicating whether to use the consumer cache.  You can straightforwardly modify DirectKafkaInputDStream.compute to pass false.  This will require rebuilding only the kafka consumer jar, not redeploying all of spark.  This will be a performance hit, especially if you&apos;re using SSL, but is better than nothing.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Fixing this in the Spark master branch (either by allowing configuration of whether to use the consumer cache, or replacing the consumer cache with a pool of consumers with different group ids for the same topicpartition) is going to require getting the attention of a committer.  I don&apos;t really have the time to mess with that right now (happy to do the work, but zero interest in tracking down committers and arguing design decisions).&lt;/p&gt;

&lt;p&gt;That being said, if one of the workarounds suggested above doesn&apos;t help you, let me know.&lt;/p&gt;</comment>
                            <comment id="15825698" author="unclegen" created="Tue, 17 Jan 2017 09:22:41 +0000"  >&lt;p&gt;Any updates about this issue? One of the workarounds suggested above can help you?  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kalvinnchau&quot; class=&quot;user-hover&quot; rel=&quot;kalvinnchau&quot;&gt;kalvinnchau&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15826474" author="kalvinnchau" created="Tue, 17 Jan 2017 17:46:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cody%40koeninger.org&quot; class=&quot;user-hover&quot; rel=&quot;cody@koeninger.org&quot;&gt;cody@koeninger.org&lt;/a&gt;  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=uncleGen&quot; class=&quot;user-hover&quot; rel=&quot;uncleGen&quot;&gt;uncleGen&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;Thanks, I haven&apos;t had the opportunity to test either of those options yet. &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;But unless the persisting somehow prevents two Task threads within the executor from trying to read from the same partition, then I don&apos;t see how that would prevent the underlying issue. But I&apos;m not 100% sure how that affects the scheduling of tasks.&lt;/li&gt;
	&lt;li&gt;I&apos;ll keep this in mind as well, seems straight forward.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Our current workaround is just making sure we have 1 task thread per executor, which is a performance hit especially for some of our higher velocity streams. To get around that we&apos;ve increased the number of partitions on the topic, and upped the number of executors to match.&lt;/p&gt;

&lt;p&gt;I think having consumer cache be configurable is a good idea, can&apos;t we technically already do that by setting the size of the cache to 0? (using &quot;spark.sql.kafkaConsumerCache.capacity&quot;) Or will that fail? I haven&apos;t had a chance to test that either.&lt;br/&gt;
I think having a pool of consumers with N group ids for a topicPartition (N being the number task threads within an executor) seems like good idea.&lt;/p&gt;

&lt;p&gt;If there were a solid direction going forward wouldn&apos;t mind tackling this issue either to contribute back!&lt;/p&gt;</comment>
                            <comment id="15826527" author="cody@koeninger.org" created="Tue, 17 Jan 2017 18:09:28 +0000"  >&lt;p&gt;I&apos;d expect setting cache capacity to zero to cause failures, but it&apos;s probably (slightly) faster to try than just patching the lines of code I pointed out.&lt;/p&gt;

&lt;p&gt;In general, a single application using kafka consumers should not be reading from different places in the same topicpartition in different threads, because it breaks ordering guarantees.  That&apos;s an implication of Kafka semantics, not Spark semantics.  That&apos;s why the consumer cache exists the way it does.&lt;/p&gt;

&lt;p&gt;Changing that behavior on a widespread basis is going to break ordering guarantees, which will break some people&apos;s existing jobs.  Hence my comments about arguing design decisions with committers.&lt;/p&gt;</comment>
                            <comment id="15827215" author="helena_e" created="Wed, 18 Jan 2017 01:34:59 +0000"  >&lt;p&gt;I&apos;ve seen this as well, the exceptions, as expected, are never raised if not using the cache. &lt;br/&gt;
Spark 2.1.&lt;/p&gt;

&lt;p&gt;The exception is raised in the seek function. I added an opt-in config for that temporarily but will work on a better solution. Perf hits aren&apos;t something I can do &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15827330" author="unclegen" created="Wed, 18 Jan 2017 02:25:02 +0000"  >&lt;p&gt;IMHO, persisting can not cover all scene, just like executor failed or cached data loss. Maybe, we can expose much clearer hint, like &quot;KafkaConsumer is not safe for multi-threaded access, you may set &apos;useConsumerCache&apos; as false, or do not run multi-tasks in single executor&quot; or something else. Besides, we may expose useConsumerCache with a configuration (true as default). This will not chang the behavior on a widespread basis.&lt;/p&gt;</comment>
                            <comment id="15827368" author="apachespark" created="Wed, 18 Jan 2017 03:20:03 +0000"  >&lt;p&gt;User &apos;uncleGen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/16629&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/16629&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15827371" author="unclegen" created="Wed, 18 Jan 2017 03:24:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cody%40koeninger.org&quot; class=&quot;user-hover&quot; rel=&quot;cody@koeninger.org&quot;&gt;cody@koeninger.org&lt;/a&gt; I provide a PR to give more clear hint for users when encounter this problem.&lt;/p&gt;</comment>
                            <comment id="16041271" author="apachespark" created="Wed, 7 Jun 2017 17:34:03 +0000"  >&lt;p&gt;User &apos;markgrover&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/18234&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/18234&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16043030" author="vanzin" created="Thu, 8 Jun 2017 16:56:57 +0000"  >&lt;p&gt;I merged Mark&apos;s patch above to master and branch-2.2, but it&apos;s just a work-around, not a fix, so I&apos;ll leave the bug open (and with no &quot;fix version&quot;).&lt;/p&gt;</comment>
                            <comment id="16382034" author="gsomogyi" created="Thu, 1 Mar 2018 14:08:15 +0000"  >&lt;p&gt;Same problem exists in structured streaming. Creating a new PR with similar work-around.&lt;/p&gt;</comment>
                            <comment id="16382119" author="apachespark" created="Thu, 1 Mar 2018 15:04:04 +0000"  >&lt;p&gt;User &apos;gaborgsomogyi&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20703&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20703&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16405887" author="kaushik_srinivas" created="Tue, 20 Mar 2018 06:54:31 +0000"  >&lt;p&gt;same issue found with kafka spark streaming 010.&lt;/p&gt;

&lt;p&gt;With increased batch window, this issue seems to be more frequently appearing.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-23663&quot; title=&quot;Spark Streaming Kafka 010 , fails with &amp;quot;java.util.ConcurrentModificationException: KafkaConsumer is not safe for multi-threaded access&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-23663&quot;&gt;&lt;del&gt;SPARK-23663&lt;/del&gt;&lt;/a&gt; created for reference. Is disabling window the workaround as of now ?&lt;/p&gt;

&lt;p&gt;We see in spark 2.2.0&lt;/p&gt;</comment>
                            <comment id="16428321" author="apachespark" created="Fri, 6 Apr 2018 13:27:05 +0000"  >&lt;p&gt;User &apos;gaborgsomogyi&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20997&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20997&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16484535" author="vanzin" created="Tue, 22 May 2018 20:44:08 +0000"  >&lt;p&gt;Issue resolved by pull request 20997&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/20997&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/20997&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16718510" author="ayushchauhan" created="Wed, 12 Dec 2018 06:32:00 +0000"  >&lt;p&gt;Is this issue resolved because &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-22606&quot; title=&quot;There may be two or more tasks in one executor will use the same kafka consumer at the same time, then it will throw an exception: &amp;quot;KafkaConsumer is not safe for multi-threaded access&amp;quot; &quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-22606&quot;&gt;&lt;del&gt;SPARK-22606&lt;/del&gt;&lt;/a&gt; is still in progress? I am getting this error in spark 2.3.2 and&#160;spark-streaming-kafka-0-10.&#160;&lt;/p&gt;

&lt;p&gt;So this workaround is the solution to this problem? Is there any downside of setting this property?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
spark.streaming.kafka.consumer.cache.enabled=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&#160;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16718595" author="gsomogyi" created="Wed, 12 Dec 2018 08:26:55 +0000"  >&lt;p&gt;There are many related issues open but this should solve the issue. As you see it&apos;s resolved on 2.4.0. The workaround is to turn cache off (what you&apos;ve written).&lt;/p&gt;

&lt;p&gt;I&apos;ll take a look at all the jiras and comment on which looks like relevant and ask for re-test.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;GENERAL WARNING FOR THIS FEATURE: Kafka keeps order within one partition, but reading the same partition from multiple threads may change the order!&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16783363" author="apachespark" created="Mon, 4 Mar 2019 13:20:48 +0000"  >&lt;p&gt;User &apos;gaborgsomogyi&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/23959&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/23959&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13075514">SPARK-20911</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13144634">SPARK-23663</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13120708">SPARK-22606</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13049780">SPARK-19888</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13233403">SPARK-27720</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 37 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i38lbj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>