<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:30:39 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-40114] Arrow 9.0.0 support with SparkR</title>
                <link>https://issues.apache.org/jira/browse/SPARK-40114</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
== Failed ======================================================================
-- 1. Error (test_sparkSQL_arrow.R:103:3): dapply() Arrow optimization ---------
Error in `readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)`: invalid &lt;span class=&quot;code-quote&quot;&gt;&apos;n&apos;&lt;/span&gt; argument
Backtrace:
 1. SparkR::collect(ret)
      at test_sparkSQL_arrow.R:103:2
 2. SparkR::collect(ret)
 3. SparkR (local) .local(x, ...)
 7. SparkR:::readRaw(conn)
 8. base::readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)
-- 2. Error (test_sparkSQL_arrow.R:133:3): dapply() Arrow optimization - type sp
Error in `readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)`: invalid &lt;span class=&quot;code-quote&quot;&gt;&apos;n&apos;&lt;/span&gt; argument
Backtrace:
 1. SparkR::collect(ret)
      at test_sparkSQL_arrow.R:133:2
 2. SparkR::collect(ret)
 3. SparkR (local) .local(x, ...)
 7. SparkR:::readRaw(conn)
 8. base::readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)
-- 3. Error (test_sparkSQL_arrow.R:143:3): dapply() Arrow optimization - type sp
Error in `readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)`: invalid &lt;span class=&quot;code-quote&quot;&gt;&apos;n&apos;&lt;/span&gt; argument
Backtrace:
  1. testthat::expect_true(all(collect(ret) == rdf))
       at test_sparkSQL_arrow.R:143:2
  5. SparkR::collect(ret)
  6. SparkR (local) .local(x, ...)
 10. SparkR:::readRaw(conn)
 11. base::readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)
-- 4. Error (test_sparkSQL_arrow.R:184:3): gapply() Arrow optimization ---------
Error in `readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)`: invalid &lt;span class=&quot;code-quote&quot;&gt;&apos;n&apos;&lt;/span&gt; argument
Backtrace:
 1. SparkR::collect(ret)
      at test_sparkSQL_arrow.R:184:2
 2. SparkR::collect(ret)
 3. SparkR (local) .local(x, ...)
 7. SparkR:::readRaw(conn)
 8. base::readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)
-- 5. Error (test_sparkSQL_arrow.R:217:3): gapply() Arrow optimization - type sp
Error in `readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)`: invalid &lt;span class=&quot;code-quote&quot;&gt;&apos;n&apos;&lt;/span&gt; argument
Backtrace:
 1. SparkR::collect(ret)
      at test_sparkSQL_arrow.R:217:2
 2. SparkR::collect(ret)
 3. SparkR (local) .local(x, ...)
 7. SparkR:::readRaw(conn)
 8. base::readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)
-- 6. Error (test_sparkSQL_arrow.R:229:3): gapply() Arrow optimization - type sp
Error in `readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)`: invalid &lt;span class=&quot;code-quote&quot;&gt;&apos;n&apos;&lt;/span&gt; argument
Backtrace:
  1. testthat::expect_true(all(collect(ret) == rdf))
       at test_sparkSQL_arrow.R:229:2
  5. SparkR::collect(ret)
  6. SparkR (local) .local(x, ...)
 10. SparkR:::readRaw(conn)
 11. base::readBin(con, raw(), as.integer(dataLen), endian = &lt;span class=&quot;code-quote&quot;&gt;&quot;big&quot;&lt;/span&gt;)
-- 7. Failure (test_sparkSQL_arrow.R:247:3): SPARK-32478: gapply() Arrow optimiz
`count(...)` threw an error with unexpected message.
Expected match: &lt;span class=&quot;code-quote&quot;&gt;&quot;expected IntegerType, IntegerType, got IntegerType, StringType&quot;&lt;/span&gt;
Actual message: &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 29.0 failed 1 times, most recent failure: Lost task 0.0 in stage 29.0 (TID 54) (APPVYR-WIN executor driver): org.apache.spark.SparkException: R unexpectedly exited.\nR worker produced errors: The tzdb &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt; is not installed. Timezones will not be available to Arrow compute functions.\nError in arrow::write_arrow(df, raw()) : write_arrow has been removed\nCalls: &amp;lt;Anonymous&amp;gt; -&amp;gt; writeRaw -&amp;gt; writeInt -&amp;gt; writeBin -&amp;gt; &amp;lt;Anonymous&amp;gt;\nExecution halted\n\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator$$anonfun$1.applyOrElse(BaseRRunner.scala:144)\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator$$anonfun$1.applyOrElse(BaseRRunner.scala:137)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:194)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:123)\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator.hasNext(BaseRRunner.scala:113)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1490)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:154)\r\n\t... 20 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2706)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2642)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2641)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2641)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1189)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1189)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1189)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2897)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2836)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2825)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\nCaused by: org.apache.spark.SparkException: R unexpectedly exited.\nR worker produced errors: The tzdb &lt;span class=&quot;code-keyword&quot;&gt;package&lt;/span&gt; is not installed. Timezones will not be available to Arrow compute functions.\nError in arrow::write_arrow(df, raw()) : write_arrow has been removed\nCalls: &amp;lt;Anonymous&amp;gt; -&amp;gt; writeRaw -&amp;gt; writeInt -&amp;gt; writeBin -&amp;gt; &amp;lt;Anonymous&amp;gt;\nExecution halted\n\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator$$anonfun$1.applyOrElse(BaseRRunner.scala:144)\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator$$anonfun$1.applyOrElse(BaseRRunner.scala:137)\r\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:194)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:123)\r\n\tat org.apache.spark.api.r.BaseRRunner$ReaderIterator.hasNext(BaseRRunner.scala:113)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage3.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:101)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1490)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:210)\r\n\tat java.net.SocketInputStream.read(SocketInputStream.java:141)\r\n\tat java.io.BufferedInputStream.fill(BufferedInputStream.java:246)\r\n\tat java.io.BufferedInputStream.read(BufferedInputStream.java:265)\r\n\tat java.io.DataInputStream.readInt(DataInputStream.java:387)\r\n\tat org.apache.spark.sql.execution.r.ArrowRRunner$$anon$2.read(ArrowRRunner.scala:154)\r\n\t... 20 more\r\n&quot;&lt;/span&gt;
Backtrace:
  1. testthat::expect_error(...)
       at test_sparkSQL_arrow.R:247:2
  7. SparkR::count(...)
  8. SparkR:::callJMethod(x@sdf, &lt;span class=&quot;code-quote&quot;&gt;&quot;count&quot;&lt;/span&gt;)
  9. SparkR:::invokeJava(isStatic = FALSE, objId$id, methodName, ...)
 10. SparkR:::handleErrors(returnStatus, conn)
== DONE ========================================================================

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://ci.appveyor.com/project/HyukjinKwon/spark/builds/44490387&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://ci.appveyor.com/project/HyukjinKwon/spark/builds/44490387&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13477165">SPARK-40114</key>
            <summary>Arrow 9.0.0 support with SparkR</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gurwls223">Hyukjin Kwon</assignee>
                                    <reporter username="gurwls223">Hyukjin Kwon</reporter>
                        <labels>
                    </labels>
                <created>Wed, 17 Aug 2022 05:18:12 +0000</created>
                <updated>Mon, 12 Dec 2022 17:51:00 +0000</updated>
                            <resolved>Wed, 17 Aug 2022 14:47:34 +0000</resolved>
                                    <version>3.4.0</version>
                                    <fixVersion>3.4.0</fixVersion>
                                    <component>SparkR</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="17580731" author="apachespark" created="Wed, 17 Aug 2022 11:05:43 +0000"  >&lt;p&gt;User &apos;HyukjinKwon&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/37553&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/37553&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17580824" author="dongjoon" created="Wed, 17 Aug 2022 14:47:34 +0000"  >&lt;p&gt;Issue resolved by pull request 37553&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/37553&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/37553&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 12 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z17tew:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>