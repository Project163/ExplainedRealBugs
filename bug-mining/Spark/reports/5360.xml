<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:57:24 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-19809] NullPointerException on zero-size ORC file</title>
                <link>https://issues.apache.org/jira/browse/SPARK-19809</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When reading from hive ORC table if there are some 0 byte files we get NullPointerException:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$BISplitStrategy.getSplits(OrcInputFormat.java:560)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1010)
	at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)
	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:199)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:240)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:240)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:240)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at org.apache.spark.rdd.UnionRDD$$anonfun$1.apply(UnionRDD.scala:66)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.rdd.UnionRDD.getPartitions(UnionRDD.scala:66)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:240)
	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:242)
	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:240)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.rdd.RDD.partitions(RDD.scala:240)
	at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:190)
	at org.apache.spark.sql.execution.Limit.executeCollect(basicOperators.scala:165)
	at org.apache.spark.sql.execution.SparkPlan.executeCollectPublic(SparkPlan.scala:174)
	at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)
	at org.apache.spark.sql.DataFrame$$anonfun$org$apache$spark$sql$DataFrame$$execute$1$1.apply(DataFrame.scala:1499)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:2086)
	at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$execute$1(DataFrame.scala:1498)
	at org.apache.spark.sql.DataFrame.org$apache$spark$sql$DataFrame$$collect(DataFrame.scala:1505)
	at org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1375)
	at org.apache.spark.sql.DataFrame$$anonfun$head$1.apply(DataFrame.scala:1374)
	at org.apache.spark.sql.DataFrame.withCallback(DataFrame.scala:2099)
	at org.apache.spark.sql.DataFrame.head(DataFrame.scala:1374)
	at org.apache.spark.sql.DataFrame.take(DataFrame.scala:1456)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.zeppelin.spark.ZeppelinContext.showDF(ZeppelinContext.java:209)
	at org.apache.zeppelin.spark.SparkSqlInterpreter.interpret(SparkSqlInterpreter.java:129)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:94)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:341)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:176)
	at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:139)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13047966">SPARK-19809</key>
            <summary>NullPointerException on zero-size ORC file</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dongjoon">Dongjoon Hyun</assignee>
                                    <reporter username="mdawid92">Micha&#322; Dawid</reporter>
                        <labels>
                    </labels>
                <created>Fri, 3 Mar 2017 09:58:48 +0000</created>
                <updated>Mon, 12 Dec 2022 18:11:07 +0000</updated>
                            <resolved>Wed, 13 Dec 2017 06:42:42 +0000</resolved>
                                    <version>1.6.3</version>
                    <version>2.0.2</version>
                    <version>2.1.1</version>
                    <version>2.2.1</version>
                                    <fixVersion>2.3.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="15953209" author="gurwls223" created="Mon, 3 Apr 2017 09:57:18 +0000"  >&lt;p&gt;I don&apos;t think there is 0 byte ORC file. It should have the footer. Moreover, currently, Spark&apos;s ORC datasource does not write out empty files (see &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-15474&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-15474&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Please reopen this if I misunderstood. It would be great if there is some steps to reproduce maybe to verify this issue.&lt;/p&gt;

&lt;p&gt;I am resolving this.&lt;/p&gt;</comment>
                            <comment id="15953341" author="mdawid92" created="Mon, 3 Apr 2017 11:57:25 +0000"  >&lt;p&gt;Those empty files have been created while processing with Pig scripts.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;-rw-rw-rw-   3 etl hdfs      14103 2017-04-03 01:26 part-v001-o000-r-00000_a_2
-rw-rw-rw-   3 etl hdfs          0 2017-04-03 01:26 part-v001-o000-r-00000_a_3
-rw-rw-rw-   3 etl hdfs      10125 2017-04-03 01:27 part-v001-o000-r-00000_a_4 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15953386" author="gurwls223" created="Mon, 3 Apr 2017 12:26:33 +0000"  >&lt;p&gt;Shoudn&apos;t it contain footer and schema information or a magic number at least? I am not sure if we can say 0 byte file is an ORC file. &lt;/p&gt;</comment>
                            <comment id="16026515" author="dongjoon" created="Fri, 26 May 2017 17:09:16 +0000"  >&lt;p&gt;IMO, we had better be more robust on this. The 3rd party tools (reported pig or sqoop) sometimes introduce this issues. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table empty_orc(a &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) stored as orc location &lt;span class=&quot;code-quote&quot;&gt;&apos;/tmp/empty_orc&apos;&lt;/span&gt;&quot;&lt;/span&gt;).show
++
||
++
++

$ touch /tmp/empty_orc/zero.orc

scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from empty_orc&quot;&lt;/span&gt;).show
java.lang.RuntimeException: serious problem
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:202)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16027361" author="gurwls223" created="Sat, 27 May 2017 08:52:01 +0000"  >&lt;p&gt;I think this is then rather about handling malformed files (e.g., &lt;tt&gt;spark.sql.files.ignoreCorruptFiles&lt;/tt&gt;).&lt;/p&gt;</comment>
                            <comment id="16027476" author="dongjoon" created="Sat, 27 May 2017 16:16:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hyukjin.kwon&quot; class=&quot;user-hover&quot; rel=&quot;hyukjin.kwon&quot;&gt;hyukjin.kwon&lt;/a&gt;. I don&apos;t think so. Parquet file does not need `spark.sql.files.ignoreCorruptFiles` option.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table empty_parquet(a &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) stored as parquet location &lt;span class=&quot;code-quote&quot;&gt;&apos;/tmp/empty_parquet&apos;&lt;/span&gt;&quot;&lt;/span&gt;).show
++
||
++
++

$ touch /tmp/empty_parquet/zero.parquet

scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from empty_parquet&quot;&lt;/span&gt;).show
+---+
|  a|
+---+
+---+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can test this in Spark with &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-20728&quot; title=&quot;Make ORCFileFormat configurable between sql/hive and sql/core&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-20728&quot;&gt;&lt;del&gt;SPARK-20728&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table empty_orc2(a &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) using orc location &lt;span class=&quot;code-quote&quot;&gt;&apos;/tmp/empty_orc&apos;&lt;/span&gt;&quot;&lt;/span&gt;).show
++
||
++
++

scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from empty_orc2&quot;&lt;/span&gt;).show
+---+
|  a|
+---+
+---+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think this is a part of &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-20901&quot; title=&quot;Feature parity for ORC with Parquet&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-20901&quot;&gt;SPARK-20901&lt;/a&gt;. And ORC community will handle this. What we need is just to use latest ORC. One thing I&apos;m wondering is this is tracked in &lt;a href=&quot;https://issues.apache.org/jira/browse/ORC-162&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/ORC-162&lt;/a&gt; (Open).&lt;/p&gt;</comment>
                            <comment id="16027987" author="gurwls223" created="Mon, 29 May 2017 01:26:23 +0000"  >&lt;p&gt;Yea, I agree that it should be dependent on the format specification/implementation, whether it is malformed or not. I think Parquet itself treats 0 bytes files as malformed file because it should read footer but it throws an exception up to my knowledge. &lt;/p&gt;

&lt;p&gt;The former case looks filtering out the whole partitions in &lt;tt&gt;FileSourceScanExec&lt;/tt&gt;. Parquet requires to read the footers and it throws an exception, for example, I manually updated the code path to not skip the partitions so that the parquet reader is actually being called as below:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.RuntimeException: file:/.../tmp.abc is not a Parquet file (too small)
	at org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:466)
	at org.apache.parquet.hadoop.ParquetFileReader.&amp;lt;init&amp;gt;(ParquetFileReader.java:568)
	at org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:492)
	at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:166)
	at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:147)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If we don&apos;t specify the schema, it also throws an exception as below:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;spark.read.parquet(&lt;span class=&quot;code-quote&quot;&gt;&quot;.../tmp.abc&quot;&lt;/span&gt;).show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.io.IOException: Could not read footer &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; file: FileStatus{path=file:/.../tmp.abc; isDirectory=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;; length=0; replication=0; blocksize=0; modification_time=0; access_time=0; owner=; group=; permission=rw-rw-rw-; isSymlink=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;}
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:498)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$$anonfun$readParquetFootersInParallel$1.apply(ParquetFileFormat.scala:485)
	at scala.collection.parallel.AugmentedIterableIterator$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;flatmap2combiner(RemainsIterator.scala:132)
	at scala.collection.parallel.immutable.ParVector$ParVectorIterator.flatmap2combiner(ParVector.scala:62)
	at scala.collection.parallel.ParIterableLike$FlatMap.leaf(ParIterableLike.scala:1072)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Assuming it is treated as a malformed file (per the ORC JIRA you pointed out above) for the current status, it looks a malformed file and it sounds we should be able to skip this in client side whether it should be dealt with &lt;tt&gt;spark.sql.files.ignoreCorruptFiles&lt;/tt&gt; or not.&lt;/p&gt;

&lt;p&gt;For example, I found a related JIRA - &lt;a href=&quot;https://issues.apache.org/jira/browse/AVRO-1530&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/AVRO-1530&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/HIVE-11977&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HIVE-11977&lt;/a&gt;. &lt;em&gt;If I read this correctly&lt;/em&gt;, Avro looks decided not to change the behaviour but Hive deals with it.&lt;/p&gt;

&lt;p&gt;Only for this issue, I also agree that this could be a subset of the issues you pointed out.&lt;/p&gt;</comment>
                            <comment id="16028008" author="dongjoon" created="Mon, 29 May 2017 03:01:28 +0000"  >&lt;p&gt;Great investigation! Thank you.&lt;/p&gt;</comment>
                            <comment id="16054031" author="renu_yadav" created="Mon, 19 Jun 2017 13:52:25 +0000"  >&lt;p&gt;What is the resolution of this issue. spark.sql.files.ignoreCorruptFiles does not work for orc file.&lt;br/&gt;
Please help.&lt;/p&gt;</comment>
                            <comment id="16054076" author="gurwls223" created="Mon, 19 Jun 2017 14:18:44 +0000"  >&lt;p&gt;What you see is what you get. This is &quot;Reopened&quot; per the discussion above and &quot;Unresolved&quot; yet.&lt;/p&gt;</comment>
                            <comment id="16054108" author="dongjoon" created="Mon, 19 Jun 2017 14:33:54 +0000"  >&lt;p&gt;Yep. I&apos;m trying to fix this with new ORC data source. It will be 2.3.0.&lt;/p&gt;</comment>
                            <comment id="16286965" author="apachespark" created="Tue, 12 Dec 2017 01:58:04 +0000"  >&lt;p&gt;User &apos;dongjoon-hyun&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19948&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16288402" author="gurwls223" created="Tue, 12 Dec 2017 22:42:51 +0000"  >&lt;p&gt;Issue resolved by pull request 19948&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19948&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16288622" author="apachespark" created="Wed, 13 Dec 2017 02:30:07 +0000"  >&lt;p&gt;User &apos;dongjoon-hyun&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/19960&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19960&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16288623" author="dongjoon" created="Wed, 13 Dec 2017 02:32:01 +0000"  >&lt;p&gt;Since Hive 1.2.1 library code path still has this problem, users may hit this when spark.sql.hive.convertMetastoreOrc=false. However, after &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-22279&quot; title=&quot;Turn on spark.sql.hive.convertMetastoreOrc by default&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-22279&quot;&gt;&lt;del&gt;SPARK-22279&lt;/del&gt;&lt;/a&gt;, Apache Spark with the default configuration doesn&apos;t hit this bug. The PR adds a test coverage for `convertMetastoreOrc=true (default)` on both `native` and `hive` ORC implementation in order to prevent regression.&lt;/p&gt;</comment>
                            <comment id="16378018" author="tafranky@gmail.com" created="Tue, 27 Feb 2018 04:33:03 +0000"  >&lt;p&gt;Need a pointer on the following.&#160;&#160;&lt;/p&gt;

&lt;p&gt;Env : Spark 2.2.1&lt;/p&gt;

&lt;p&gt;1- I set the property&#160;&#160;spark.sql.hive.convertMetastoreOrc to true&lt;/p&gt;

&lt;p&gt;2-&#160;My hive table has the following&#160; schema&lt;/p&gt;

&lt;p&gt;CREATE TABLE `ft_orc`(&lt;br/&gt;
 `int` int,&lt;br/&gt;
 `double` double,&lt;br/&gt;
 `big+int` bigint,&lt;br/&gt;
 `$tring` string,&lt;br/&gt;
 `(decimal)` decimal(15,8),&lt;br/&gt;
 `flo@t` float,&lt;br/&gt;
 `datetime` date,&lt;br/&gt;
 `timestamp` timestamp,&lt;br/&gt;
 `01` int)&lt;br/&gt;
 CLUSTERED BY (&lt;br/&gt;
 `int`)&lt;br/&gt;
 INTO 20 BUCKETS&lt;br/&gt;
 ROW FORMAT SERDE&lt;br/&gt;
 &apos;org.apache.hadoop.hive.ql.io.orc.OrcSerde&apos;&lt;br/&gt;
 WITH SERDEPROPERTIES (&lt;br/&gt;
 &apos;field.delim&apos;=&apos;,&apos;,&lt;br/&gt;
 &apos;serialization.format&apos;=&apos;,&apos;)&lt;br/&gt;
 STORED AS INPUTFORMAT&lt;br/&gt;
 &apos;org.apache.hadoop.hive.ql.io.orc.OrcInputFormat&apos;&lt;br/&gt;
 OUTPUTFORMAT&lt;br/&gt;
 &apos;org.apache.hadoop.hive.ql.io.orc.OrcOutputFormat&apos; ;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I loaded the table&#160; with 1 row of&#160; data&#160;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12912198/12912198_image-2018-02-26-20-29-49-410.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I tried to&#160; run the following simple statement&#160;&#160;&lt;/p&gt;

&lt;p&gt;scala&amp;gt; var res =spark.sql(&quot; SELECT alias.`int` as a0, alias.`double` as a1, alias.`big+int` as a2, alias.`$tring` as a3, CAST(alias.`(decimal)` AS DOUBLE) as a4, CAST(alias.`flo@t` AS DOUBLE) as a5, CAST(alias.`datetime` AS TIMESTAMP) as a6, alias.`timestamp` as a7, alias.`01` as a8 FROM default.ft_orc alias&quot; )&lt;br/&gt;
18/02/27 04:30:57 WARN HiveConf: HiveConf of name hive.conf.hidden.list does not exist&lt;br/&gt;
18/02/27 04:30:57 WARN HiveConf: HiveConf of name hive.conf.hidden.list does not exist&lt;br/&gt;
java.lang.IndexOutOfBoundsException&lt;br/&gt;
 at java.nio.Buffer.checkIndex(Buffer.java:540)&lt;br/&gt;
 at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139)&lt;br/&gt;
 at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:374)&lt;br/&gt;
 at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.&amp;lt;init&amp;gt;(ReaderImpl.java:316)&lt;br/&gt;
 at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:187)&lt;br/&gt;
 at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:68)&lt;br/&gt;
 at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:67)&lt;br/&gt;
 at scala.collection.Iterator$$anon$11.next(Iterator.scala:409)&lt;br/&gt;
 at scala.collection.TraversableOnce$class.collectFirst(TraversableOnce.scala:145)&lt;br/&gt;
 at scala.collection.AbstractIterator.collectFirst(Iterator.scala:1336)&lt;br/&gt;
 at org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:69)&lt;br/&gt;
 at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)&lt;br/&gt;
 at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:77)&lt;br/&gt;
 at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)&lt;br/&gt;
 at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)&lt;br/&gt;
 at scala.collection.immutable.List.foreach(List.scala:381)&lt;br/&gt;
 at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)&lt;br/&gt;
 at scala.collection.immutable.List.flatMap(List.scala:344)&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Any pointer ?&#160;&lt;/p&gt;

&lt;p&gt;Should I file a separate Jira ?&#160;&lt;/p&gt;</comment>
                            <comment id="16378022" author="dongjoon" created="Tue, 27 Feb 2018 04:36:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tafranky%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;tafranky@gmail.com&quot;&gt;tafranky@gmail.com&lt;/a&gt;. Please see the fixed version of this JIRA issue. It&apos;s 2.3.0, not 2.2.1.&lt;/p&gt;</comment>
                            <comment id="16378028" author="tafranky@gmail.com" created="Tue, 27 Feb 2018 04:43:39 +0000"  >&lt;p&gt;1- I am kind of constrained to&#160; spark 2.2.1&#160; at the moment .&#160;&lt;/p&gt;

&lt;p&gt;2- My understanding is that the only thing different with spark 2.3.0 is that&#160;&#160;spark.sql.hive.convertMetastoreOrc&#160; is defaulted to true.&#160;&lt;/p&gt;

&lt;p&gt;I looked at&#160; &lt;a href=&quot;https://github.com/apache/spark/pull/19948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19948&lt;/a&gt;&#160; and&#160; &lt;a href=&quot;https://github.com/apache/spark/pull/19960&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/19960&lt;/a&gt;&#160;. Am I missing&#160; anything ?&#160;&lt;/p&gt;</comment>
                            <comment id="16378034" author="dongjoon" created="Tue, 27 Feb 2018 04:52:29 +0000"  >&lt;p&gt;2.3.0 RC5 voting will end tonight. You had better try 2.3.0 RC5 and report a JIRA issue against 2.3.0.&lt;/p&gt;

&lt;p&gt;1. For 2.1.1, that&apos;s too bad. There is no plan to resolve this in 2.1.1. It&apos;s already too old. This will works only Spark 2.3.0. If there are some issues, they will be fixed on Spark 2.3.1.&lt;/p&gt;

&lt;p&gt;2. As you see, those PRs are adding test cases. ORC also added zero-size file supports, but Spark handles them in an upper layer, too. That&apos;s the difference you are wondering.&lt;/p&gt;</comment>
                            <comment id="16378046" author="tafranky@gmail.com" created="Tue, 27 Feb 2018 05:01:16 +0000"  >&lt;p&gt;Just to confirm , your&#160; earlier comment referred to&#160; spark 2.1.1. You meant&#160; spark 2.2.1&#160; right ?&#160;&#160;&lt;/p&gt;</comment>
                            <comment id="16378963" author="dongjoon" created="Tue, 27 Feb 2018 17:14:29 +0000"  >&lt;p&gt;I meant 2.1.1 literally. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="16598227" author="shirisht" created="Fri, 31 Aug 2018 04:13:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dongjoon&quot; class=&quot;user-hover&quot; rel=&quot;dongjoon&quot;&gt;dongjoon&lt;/a&gt; I am encountering the same problem even with Spark version 2.3.1.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[local:~] spark-shell
2018-08-30 21:07:25 WARN  NativeCodeLoader:62 - Unable to load &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;-hadoop library &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; your platform... using builtin-java classes where applicable
Setting &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; log level to &lt;span class=&quot;code-quote&quot;&gt;&quot;WARN&quot;&lt;/span&gt;.
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
Spark context Web UI available at http:&lt;span class=&quot;code-comment&quot;&gt;//localhost:4040
&lt;/span&gt;Spark context available as &lt;span class=&quot;code-quote&quot;&gt;&apos;sc&apos;&lt;/span&gt; (master = local[*], app id = local-1535688452266).
Spark session available as &lt;span class=&quot;code-quote&quot;&gt;&apos;spark&apos;&lt;/span&gt;.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  &apos;_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.3.1
      /_/
         
Using Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_101)
Type in expressions to have them evaluated.
Type :help &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; more information.

scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table empty_orc(a &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) stored as orc location &lt;span class=&quot;code-quote&quot;&gt;&apos;/tmp/empty_orc&apos;&lt;/span&gt;&quot;&lt;/span&gt;).show
2018-08-30 21:07:44 WARN  ObjectStore:6666 - Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
2018-08-30 21:07:44 WARN  ObjectStore:568 - Failed to get database &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, returning NoSuchObjectException
2018-08-30 21:07:45 WARN  ObjectStore:568 - Failed to get database global_temp, returning NoSuchObjectException
++
||
++
++

&lt;span class=&quot;code-comment&quot;&gt;// in a different terminal, I did &lt;span class=&quot;code-quote&quot;&gt;&quot;touch /tmp/empty_orc/zero.orc&quot;&lt;/span&gt;
&lt;/span&gt;
scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from empty_orc&quot;&lt;/span&gt;).show
java.lang.RuntimeException: serious problem
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021)
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048)
  at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)
  at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)
  at scala.Option.getOrElse(Option.scala:121)
  at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)
  at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:340)
  at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38)
  at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)
  at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484)
  at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2484)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2698)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:254)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:723)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:682)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:691)
  ... 49 elided
Caused by: java.lang.NullPointerException
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$BISplitStrategy.getSplits(OrcInputFormat.java:560)
  at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1010)
  ... 99 more

scala&amp;gt; 

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16598465" author="dongjoon" created="Fri, 31 Aug 2018 09:08:42 +0000"  >&lt;p&gt;Hi, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=shirisht&quot; class=&quot;user-hover&quot; rel=&quot;shirisht&quot;&gt;shirisht&lt;/a&gt;. You need to turn on `convertMetastoreOrc`&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;set spark.sql.hive.convertMetastoreOrc=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&quot;&lt;/span&gt;)
res4: org.apache.spark.sql.DataFrame = [key: string, value: string]

scala&amp;gt; sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;select * from empty_orc&quot;&lt;/span&gt;).show
+---+
|  a|
+---+
+---+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16604926" author="shirisht" created="Wed, 5 Sep 2018 20:51:31 +0000"  >&lt;p&gt;Thank you &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dongjoon&quot; class=&quot;user-hover&quot; rel=&quot;dongjoon&quot;&gt;dongjoon&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16734510" author="prashanthsandela" created="Fri, 4 Jan 2019 19:27:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dongjoon&quot; class=&quot;user-hover&quot; rel=&quot;dongjoon&quot;&gt;dongjoon&lt;/a&gt; I&apos;m encountering same&#160;similar issue with spark version 2.3.1&lt;/p&gt;

&lt;p&gt;I&apos;m trying to read from a table which was ingested by sqoop. There are few 0 byte files for this table. The file&#160;sizes looks like below:&#160;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;-rw-rw-r-- 3 cloud-user root 17.3 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00000
-rw-rw-r-- 3 cloud-user root 10.3 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00001
-rw-rw-r-- 3 cloud-user root 19.9 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00002
-rw-rw-r-- 3 cloud-user root 13.0 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00003
-rw-rw-r-- 3 cloud-user root 0 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00004
-rw-rw-r-- 3 cloud-user root 3.4 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00005
-rw-rw-r-- 3 cloud-user root 13.8 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00006
-rw-rw-r-- 3 cloud-user root 0 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00007
-rw-rw-r-- 3 cloud-user root 0 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00008
-rw-rw-r-- 3 cloud-user root 6.9 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00009
-rw-rw-r-- 3 cloud-user root 9.0 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00010
-rw-rw-r-- 3 cloud-user root 11.4 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00011
-rw-rw-r-- 3 cloud-user root 14.7 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00012
-rw-rw-r-- 3 cloud-user root 17.4 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00013
-rw-rw-r-- 3 cloud-user root 17.1 M 2019-01-03 22:20 /apps/hive/warehouse/default.db/table_with_few_zero_byte_files/part-m-00014&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Spark throws exception while reading this table.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;scala&amp;gt; spark.read.table(&quot;table_with_few_zero_byte_files&quot;).show() 
java.lang.RuntimeException: serious problem at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1021) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.getSplits(OrcInputFormat.java:1048) at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253) at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.rdd.RDD.partitions(RDD.scala:251) at org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:340) at org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:38) at org.apache.spark.sql.Dataset.org$apache$spark$sql$Dataset$$collectFromPlan(Dataset.scala:3273) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484) at org.apache.spark.sql.Dataset$$anonfun$head$1.apply(Dataset.scala:2484) at org.apache.spark.sql.Dataset$$anonfun$52.apply(Dataset.scala:3254) at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77) at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3253) at org.apache.spark.sql.Dataset.head(Dataset.scala:2484) at org.apache.spark.sql.Dataset.take(Dataset.scala:2698) at org.apache.spark.sql.Dataset.showString(Dataset.scala:254) at org.apache.spark.sql.Dataset.show(Dataset.scala:723) at org.apache.spark.sql.Dataset.show(Dataset.scala:682) at org.apache.spark.sql.Dataset.show(Dataset.scala:691) ... 49 elided Caused by: java.lang.NullPointerException at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat$BISplitStrategy.getSplits(OrcInputFormat.java:560) at org.apache.hadoop.hive.ql.io.orc.OrcInputFormat.generateSplitsInfo(OrcInputFormat.java:1010) ... 99 more 
&#160;
scala&amp;gt; sql(&quot;set spark.sql.hive.convertMetastoreOrc=true&quot;) 
res22: org.apache.spark.sql.DataFrame = [key: string, value: string] 
&#160;
scala&amp;gt; spark.read.table(&quot;table_with_few_zero_byte_files&quot;).show() 
java.lang.IndexOutOfBoundsException at java.nio.Buffer.checkIndex(Buffer.java:540) at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:139) at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.extractMetaInfoFromFooter(ReaderImpl.java:377) at org.apache.hadoop.hive.ql.io.orc.ReaderImpl.&amp;lt;init&amp;gt;(ReaderImpl.java:319) at org.apache.hadoop.hive.ql.io.orc.OrcFile.createReader(OrcFile.java:187) at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:75) at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$getFileReader$2.apply(OrcFileOperator.scala:73) at scala.collection.Iterator$$anon$11.next(Iterator.scala:409) at scala.collection.TraversableOnce$class.collectFirst(TraversableOnce.scala:145) at scala.collection.AbstractIterator.collectFirst(Iterator.scala:1336) at org.apache.spark.sql.hive.orc.OrcFileOperator$.getFileReader(OrcFileOperator.scala:86) at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:95) at org.apache.spark.sql.hive.orc.OrcFileOperator$$anonfun$readSchema$1.apply(OrcFileOperator.scala:95) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241) at scala.collection.immutable.List.foreach(List.scala:381) at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241) at scala.collection.immutable.List.flatMap(List.scala:344) at org.apache.spark.sql.hive.orc.OrcFileOperator$.readSchema(OrcFileOperator.scala:95) at org.apache.spark.sql.hive.orc.OrcFileFormat.inferSchema(OrcFileFormat.scala:63) at org.apache.spark.sql.hive.HiveMetastoreCatalog.org$apache$spark$sql$hive$HiveMetastoreCatalog$$inferIfNeeded(HiveMetastoreCatalog.scala:239) at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6$$anonfun$7.apply(HiveMetastoreCatalog.scala:193) at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6$$anonfun$7.apply(HiveMetastoreCatalog.scala:192) at scala.Option.getOrElse(Option.scala:121) at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6.apply(HiveMetastoreCatalog.scala:192) at org.apache.spark.sql.hive.HiveMetastoreCatalog$$anonfun$6.apply(HiveMetastoreCatalog.scala:185) at org.apache.spark.sql.hive.HiveMetastoreCatalog.withTableCreationLock(HiveMetastoreCatalog.scala:54) at org.apache.spark.sql.hive.HiveMetastoreCatalog.convertToLogicalRelation(HiveMetastoreCatalog.scala:185) at org.apache.spark.sql.hive.RelationConversions.org$apache$spark$sql$hive$RelationConversions$$convert(HiveStrategies.scala:205) at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:226) at org.apache.spark.sql.hive.RelationConversions$$anonfun$apply$4.applyOrElse(HiveStrategies.scala:215) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:289) at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:70) at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:288) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:286) at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:306) at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:187) at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:304) at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:286) at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:215) at org.apache.spark.sql.hive.RelationConversions.apply(HiveStrategies.scala:180) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:87) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:84) at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:57) at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:66) at scala.collection.mutable.ArrayBuffer.foldLeft(ArrayBuffer.scala:48) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:84) at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:76) at scala.collection.immutable.List.foreach(List.scala:381) at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:76) at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:124) at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:118) at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:103) at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:57) at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:55) at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:47) at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:74) at org.apache.spark.sql.SparkSession.table(SparkSession.scala:627) at org.apache.spark.sql.SparkSession.table(SparkSession.scala:623) at org.apache.spark.sql.DataFrameReader.table(DataFrameReader.scala:654) ... 49 elided

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Unfortunately, we don&apos;t control the creation of table. What would be a config that could help me read this table?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16734521" author="dongjoon" created="Fri, 4 Jan 2019 19:37:14 +0000"  >&lt;p&gt;Hi, did you use `spark.sql.orc.impl=native`, too? New ORC is not default in Spark 2.3.x.&lt;/p&gt;</comment>
                            <comment id="16734523" author="prashanthsandela" created="Fri, 4 Jan 2019 19:42:20 +0000"  >&lt;p&gt;Awesome! This config of&#160;`spark.sql.orc.impl=native` works. Thanks for quick response.&#160;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="13075230">SPARK-20901</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="13046674">SPARK-19752</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12912198" name="image-2018-02-26-20-29-49-410.png" size="9309" author="tafranky@gmail.com" created="Tue, 27 Feb 2018 04:29:49 +0000"/>
                            <attachment id="12912199" name="spark.sql.hive.convertMetastoreOrc.txt" size="6764" author="tafranky@gmail.com" created="Tue, 27 Feb 2018 04:33:15 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 45 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3awhr:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>