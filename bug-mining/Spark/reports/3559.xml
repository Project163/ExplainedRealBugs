<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:43:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-15345] SparkSession&apos;s conf doesn&apos;t take effect when there&apos;s already an existing SparkContext</title>
                <link>https://issues.apache.org/jira/browse/SPARK-15345</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I am working with branch-2.0, spark is compiled with hive support (-Phive and -Phvie-thriftserver).&lt;br/&gt;
I am trying to access databases using this snippet:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;from pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; HiveContext
hc = HiveContext(sc)
hc.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;show databases&quot;&lt;/span&gt;).collect()
[Row(result=&lt;span class=&quot;code-quote&quot;&gt;&apos;&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;&apos;&lt;/span&gt;)]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This means that spark doesn&apos;t find any databases specified in configuration.&lt;br/&gt;
Using the same configuration (i.e. hive-site.xml and core-site.xml) in spark 1.6, and launching above snippet, I can print out existing databases.&lt;/p&gt;

&lt;p&gt;When run in DEBUG mode this is what spark (2.0) prints out:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;16/05/16 12:17:47 INFO SparkSqlParser: Parsing command: show databases
16/05/16 12:17:47 DEBUG SimpleAnalyzer: 
=== Result of Batch Resolution ===
!&apos;Project [unresolveddeserializer(createexternalrow(&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (isnull(input[0, string])) &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; input[0, string].toString, StructField(result,StringType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)), result#2) AS #3]   Project [createexternalrow(&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (isnull(result#2)) &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; result#2.toString, StructField(result,StringType,&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)) AS #3]
 +- LocalRelation [result#2]                                                                                                                                                     +- LocalRelation [result#2]
        
16/05/16 12:17:47 DEBUG ClosureCleaner: +++ Cleaning closure &amp;lt;function1&amp;gt; (org.apache.spark.sql.Dataset$$anonfun$53) +++
16/05/16 12:17:47 DEBUG ClosureCleaner:  + declared fields: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; org.apache.spark.sql.Dataset$$anonfun$53.serialVersionUID
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; org.apache.spark.sql.types.StructType org.apache.spark.sql.Dataset$$anonfun$53.structType$1
16/05/16 12:17:47 DEBUG ClosureCleaner:  + declared methods: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; org.apache.spark.sql.Dataset$$anonfun$53.apply(java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; org.apache.spark.sql.Dataset$$anonfun$53.apply(org.apache.spark.sql.catalyst.InternalRow)
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; classes: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt; classes: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt; objects: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + populating accessed fields because &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the starting closure
16/05/16 12:17:47 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/05/16 12:17:47 DEBUG ClosureCleaner:  +++ closure &amp;lt;function1&amp;gt; (org.apache.spark.sql.Dataset$$anonfun$53) is now cleaned +++
16/05/16 12:17:47 DEBUG ClosureCleaner: +++ Cleaning closure &amp;lt;function1&amp;gt; (org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$javaToPython$1) +++
16/05/16 12:17:47 DEBUG ClosureCleaner:  + declared fields: 1
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$javaToPython$1.serialVersionUID
16/05/16 12:17:47 DEBUG ClosureCleaner:  + declared methods: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$javaToPython$1.apply(java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$javaToPython$1.apply(scala.collection.Iterator)
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; classes: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt; classes: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt; objects: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + populating accessed fields because &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the starting closure
16/05/16 12:17:47 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/05/16 12:17:47 DEBUG ClosureCleaner:  +++ closure &amp;lt;function1&amp;gt; (org.apache.spark.sql.execution.python.EvaluatePython$$anonfun$javaToPython$1) is now cleaned +++
16/05/16 12:17:47 DEBUG ClosureCleaner: +++ Cleaning closure &amp;lt;function1&amp;gt; (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13) +++
16/05/16 12:17:47 DEBUG ClosureCleaner:  + declared fields: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.serialVersionUID
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; org.apache.spark.rdd.RDD$$anonfun$collect$1 org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.$&lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt;
16/05/16 12:17:47 DEBUG ClosureCleaner:  + declared methods: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13.apply(scala.collection.Iterator)
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; classes: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt; classes: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD$$anonfun$collect$1
16/05/16 12:17:47 DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt; objects: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      &amp;lt;function0&amp;gt;
16/05/16 12:17:47 DEBUG ClosureCleaner:      MapPartitionsRDD[5] at collect at &amp;lt;stdin&amp;gt;:1
16/05/16 12:17:47 DEBUG ClosureCleaner:  + populating accessed fields because &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the starting closure
16/05/16 12:17:47 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      (&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($&lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt;))
16/05/16 12:17:47 DEBUG ClosureCleaner:      (&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
16/05/16 12:17:47 DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; not clone it: (&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.rdd.RDD,MapPartitionsRDD[5] at collect at &amp;lt;stdin&amp;gt;:1)
16/05/16 12:17:47 DEBUG ClosureCleaner:  + cloning the object &amp;lt;function0&amp;gt; of &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.rdd.RDD$$anonfun$collect$1
16/05/16 12:17:47 DEBUG ClosureCleaner:  + cleaning cloned closure &amp;lt;function0&amp;gt; recursively (org.apache.spark.rdd.RDD$$anonfun$collect$1)
16/05/16 12:17:47 DEBUG ClosureCleaner: +++ Cleaning closure &amp;lt;function0&amp;gt; (org.apache.spark.rdd.RDD$$anonfun$collect$1) +++
16/05/16 12:17:47 DEBUG ClosureCleaner:  + declared fields: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; org.apache.spark.rdd.RDD$$anonfun$collect$1.serialVersionUID
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.$&lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt;
16/05/16 12:17:47 DEBUG ClosureCleaner:  + declared methods: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; org.apache.spark.rdd.RDD org.apache.spark.rdd.RDD$$anonfun$collect$1.org$apache$spark$rdd$RDD$$anonfun$$$&lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt;()
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; org.apache.spark.rdd.RDD$$anonfun$collect$1.apply()
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; classes: 1
16/05/16 12:17:47 DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt; classes: 1
16/05/16 12:17:47 DEBUG ClosureCleaner:      org.apache.spark.rdd.RDD
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt; objects: 1
16/05/16 12:17:47 DEBUG ClosureCleaner:      MapPartitionsRDD[5] at collect at &amp;lt;stdin&amp;gt;:1
16/05/16 12:17:47 DEBUG ClosureCleaner:  + fields accessed by starting closure: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      (&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.rdd.RDD$$anonfun$collect$1,Set($&lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt;))
16/05/16 12:17:47 DEBUG ClosureCleaner:      (&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.rdd.RDD,Set(org$apache$spark$rdd$RDD$$evidence$1))
16/05/16 12:17:47 DEBUG ClosureCleaner:  + outermost object is not a closure or REPL line object, so &lt;span class=&quot;code-keyword&quot;&gt;do&lt;/span&gt; not clone it: (&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.spark.rdd.RDD,MapPartitionsRDD[5] at collect at &amp;lt;stdin&amp;gt;:1)
16/05/16 12:17:47 DEBUG ClosureCleaner:  +++ closure &amp;lt;function0&amp;gt; (org.apache.spark.rdd.RDD$$anonfun$collect$1) is now cleaned +++
16/05/16 12:17:47 DEBUG ClosureCleaner:  +++ closure &amp;lt;function1&amp;gt; (org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$13) is now cleaned +++
16/05/16 12:17:47 DEBUG ClosureCleaner: +++ Cleaning closure &amp;lt;function2&amp;gt; (org.apache.spark.SparkContext$$anonfun$runJob$5) +++
16/05/16 12:17:47 DEBUG ClosureCleaner:  + declared fields: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; org.apache.spark.SparkContext$$anonfun$runJob$5.serialVersionUID
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; scala.Function1 org.apache.spark.SparkContext$$anonfun$runJob$5.cleanedFunc$1
16/05/16 12:17:47 DEBUG ClosureCleaner:  + declared methods: 2
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; org.apache.spark.SparkContext$$anonfun$runJob$5.apply(java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;,java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
16/05/16 12:17:47 DEBUG ClosureCleaner:      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; org.apache.spark.SparkContext$$anonfun$runJob$5.apply(org.apache.spark.TaskContext,scala.collection.Iterator)
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt; classes: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt; classes: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + &lt;span class=&quot;code-keyword&quot;&gt;outer&lt;/span&gt; objects: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + populating accessed fields because &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; is the starting closure
16/05/16 12:17:47 DEBUG ClosureCleaner:  + fields accessed by starting closure: 0
16/05/16 12:17:47 DEBUG ClosureCleaner:  + there are no enclosing objects!
16/05/16 12:17:47 DEBUG ClosureCleaner:  +++ closure &amp;lt;function2&amp;gt; (org.apache.spark.SparkContext$$anonfun$runJob$5) is now cleaned +++
16/05/16 12:17:47 INFO SparkContext: Starting job: collect at &amp;lt;stdin&amp;gt;:1
16/05/16 12:17:47 INFO DAGScheduler: Got job 1 (collect at &amp;lt;stdin&amp;gt;:1) with 1 output partitions
16/05/16 12:17:47 INFO DAGScheduler: Final stage: ResultStage 1 (collect at &amp;lt;stdin&amp;gt;:1)
16/05/16 12:17:47 INFO DAGScheduler: Parents of &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; stage: List()
16/05/16 12:17:47 INFO DAGScheduler: Missing parents: List()
16/05/16 12:17:47 DEBUG DAGScheduler: submitStage(ResultStage 1)
16/05/16 12:17:47 DEBUG DAGScheduler: missing: List()
16/05/16 12:17:47 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at collect at &amp;lt;stdin&amp;gt;:1), which has no missing parents
16/05/16 12:17:47 DEBUG DAGScheduler: submitMissingTasks(ResultStage 1)
16/05/16 12:17:47 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.1 KB, free 5.8 GB)
16/05/16 12:17:47 DEBUG BlockManager: Put block broadcast_1 locally took  1 ms
16/05/16 12:17:47 DEBUG BlockManager: Putting block broadcast_1 without replication took  1 ms
16/05/16 12:17:47 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1856.0 B, free 5.8 GB)
16/05/16 12:17:47 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 188.165.13.157:35738 (size: 1856.0 B, free: 5.8 GB)
16/05/16 12:17:47 DEBUG BlockManagerMaster: Updated info of block broadcast_1_piece0
16/05/16 12:17:47 DEBUG BlockManager: Told master about block broadcast_1_piece0
16/05/16 12:17:47 DEBUG BlockManager: Put block broadcast_1_piece0 locally took  1 ms
16/05/16 12:17:47 DEBUG BlockManager: Putting block broadcast_1_piece0 without replication took  2 ms
16/05/16 12:17:47 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012
16/05/16 12:17:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at collect at &amp;lt;stdin&amp;gt;:1)
16/05/16 12:17:47 DEBUG DAGScheduler: New pending partitions: Set(0)
16/05/16 12:17:47 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
16/05/16 12:17:47 DEBUG TaskSetManager: Epoch &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; TaskSet 1.0: 0
16/05/16 12:17:47 DEBUG TaskSetManager: Valid locality levels &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; TaskSet 1.0: NO_PREF, ANY
16/05/16 12:17:47 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1, runningTasks: 0
16/05/16 12:17:47 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, xxx3, partition 0, PROCESS_LOCAL, 5542 bytes)
16/05/16 12:17:47 DEBUG TaskSetManager: No tasks &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; locality level NO_PREF, so moving to locality level ANY
16/05/16 12:17:47 INFO SparkDeploySchedulerBackend: Launching task 1 on executor id: 0 hostname: xxx3.
16/05/16 12:17:48 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1, runningTasks: 1
16/05/16 12:17:48 DEBUG BlockManager: Getting local block broadcast_1_piece0 as bytes
16/05/16 12:17:48 DEBUG BlockManager: Level &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; block broadcast_1_piece0 is StorageLevel(disk=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, memory=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, offheap=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, deserialized=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, replication=1)
16/05/16 12:17:48 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 188.165.13.158:53616 (size: 1856.0 B, free: 14.8 GB)
16/05/16 12:17:49 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1, runningTasks: 1
16/05/16 12:17:50 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1, runningTasks: 1
16/05/16 12:17:50 DEBUG TaskSchedulerImpl: parentName: , name: TaskSet_1, runningTasks: 0
16/05/16 12:17:50 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2156 ms on xxx3 (1/1)
16/05/16 12:17:50 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
16/05/16 12:17:50 INFO DAGScheduler: ResultStage 1 (collect at &amp;lt;stdin&amp;gt;:1) finished in 2.158 s
16/05/16 12:17:50 DEBUG DAGScheduler: After removal of stage 1, remaining stages = 0
16/05/16 12:17:50 INFO DAGScheduler: Job 1 finished: collect at &amp;lt;stdin&amp;gt;:1, took 2.174808 s
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I can&apos;t see any information on Hive connection in this trace.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12969989">SPARK-15345</key>
            <summary>SparkSession&apos;s conf doesn&apos;t take effect when there&apos;s already an existing SparkContext</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rxin">Reynold Xin</assignee>
                                    <reporter username="m1lan">Piotr Milanowski</reporter>
                        <labels>
                    </labels>
                <created>Mon, 16 May 2016 12:39:24 +0000</created>
                <updated>Fri, 28 Jul 2017 04:04:39 +0000</updated>
                            <resolved>Thu, 23 Jun 2016 03:25:35 +0000</resolved>
                                                    <fixVersion>2.0.0</fixVersion>
                                    <component>PySpark</component>
                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>18</watches>
                                                                                                                <comments>
                            <comment id="15286373" author="zjffdu" created="Tue, 17 May 2016 10:25:49 +0000"  >&lt;p&gt;I can reproduce it even use SparkSession, should be a bug, try to work on it. &lt;/p&gt;</comment>
                            <comment id="15287907" author="zjffdu" created="Tue, 17 May 2016 23:48:51 +0000"  >&lt;p&gt;Try to work on it.&lt;/p&gt;</comment>
                            <comment id="15288089" author="zjffdu" created="Wed, 18 May 2016 02:11:51 +0000"  >&lt;p&gt;The root cause is that in pyspark SparkContext is created first and it is used for creating SparkSession so that the SparkContext&apos;s conf is used instead of that of SparkSession.  Actually this issue also happens in scala. Here&apos;s one sample in spark-shell&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.SparkContext
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; org.apache.spark.sql.SparkSession
sc.stop()
&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; sc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; SparkContext()
val session = SparkSession.builder().master(&lt;span class=&quot;code-quote&quot;&gt;&quot;local&quot;&lt;/span&gt;).appName(&lt;span class=&quot;code-quote&quot;&gt;&quot;test&quot;&lt;/span&gt;).enableHiveSupport().getOrCreate()
session.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;show tables&quot;&lt;/span&gt;).show()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For this sample code, the session created doesn&apos;t have hive support, because it use the new created SparkContext&apos;s conf. I think this is a very critical issue in the perspective of user experience. &lt;/p&gt;</comment>
                            <comment id="15288101" author="apachespark" created="Wed, 18 May 2016 02:21:04 +0000"  >&lt;p&gt;User &apos;zjffdu&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13160&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13160&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15292339" author="apachespark" created="Thu, 19 May 2016 23:34:05 +0000"  >&lt;p&gt;User &apos;rxin&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/13200&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13200&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15292711" author="rxin" created="Fri, 20 May 2016 04:54:13 +0000"  >&lt;p&gt;I think I have fixed it in &lt;a href=&quot;https://github.com/apache/spark/pull/13200&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13200&lt;/a&gt;. If there is still a problem, please reopen. Thanks.&lt;/p&gt;</comment>
                            <comment id="15292715" author="maver1ck" created="Fri, 20 May 2016 04:58:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rxin&quot; class=&quot;user-hover&quot; rel=&quot;rxin&quot;&gt;rxin&lt;/a&gt; Are you planning another PR for Python as in comment ?&lt;br/&gt;
&quot;I updated Python docs. The Python change seems slightly larger and since it is not user facing, I&apos;m going to defer it to another pr.&quot;&lt;/p&gt;

&lt;p&gt;Or should I assume that Python part is working ?&lt;/p&gt;</comment>
                            <comment id="15292717" author="rxin" created="Fri, 20 May 2016 05:00:13 +0000"  >&lt;p&gt;Haven&apos;t tested it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I am a little bit busy later today. Do you want to take a look at the Python part?&lt;/p&gt;

&lt;p&gt;Also cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andrewor14&quot; class=&quot;user-hover&quot; rel=&quot;andrewor14&quot;&gt;andrewor14&lt;/a&gt; who might be able to work on the Python part.&lt;/p&gt;</comment>
                            <comment id="15292719" author="maver1ck" created="Fri, 20 May 2016 05:02:44 +0000"  >&lt;p&gt;Will try to test it with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=m1lan&quot; class=&quot;user-hover&quot; rel=&quot;m1lan&quot;&gt;m1lan&lt;/a&gt; today.&lt;/p&gt;
</comment>
                            <comment id="15293899" author="andrewor14" created="Fri, 20 May 2016 18:16:39 +0000"  >&lt;p&gt;The python part should be resolved by &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-15417&quot; title=&quot;Failed to enable hive support in PySpark shell&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-15417&quot;&gt;&lt;del&gt;SPARK-15417&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://github.com/apache/spark/pull/13203&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13203&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15298107" author="m1lan" created="Tue, 24 May 2016 12:42:39 +0000"  >&lt;p&gt;Databases can be accessed and created after proposed changes.&lt;/p&gt;</comment>
                            <comment id="15298131" author="zjffdu" created="Tue, 24 May 2016 12:56:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=m1lan&quot; class=&quot;user-hover&quot; rel=&quot;m1lan&quot;&gt;m1lan&lt;/a&gt; I reopened it, because it hasn&apos;t resolved the issue of my last comment. &lt;/p&gt;</comment>
                            <comment id="15299568" author="jameszhouyi" created="Wed, 25 May 2016 06:52:42 +0000"  >&lt;p&gt;1) Spark SQL can&apos;t find existing hive metastore database in spark-sql shell by issuing &apos;show databases;&apos;&lt;br/&gt;
2) Always told me that there is already existing database..(i saw a local derby metastore_db folder in current directory). it seemed that spark sql can&apos;t read the hive conf(eg, hive-site.xml )..&lt;br/&gt;
3) Key configurations in spark-defaults.conf:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;spark.sql.hive.metastore.version=1.1.0
spark.sql.hive.metastore.jars=/usr/lib/hive/lib/*:/usr/lib/hadoop/client/*
spark.executor.extraClassPath=/etc/hive/conf
spark.driver.extraClassPath=/etc/hive/conf
spark.yarn.jars=local:/usr/lib/spark/jars/*
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;16/05/23 09:48:24 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Database test_sparksql already exists)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:898)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:133)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)&lt;br/&gt;
	at com.sun.proxy.$Proxy34.create_database(Unknown Source)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:645)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:91)&lt;br/&gt;
	at com.sun.proxy.$Proxy35.createDatabase(Unknown Source)&lt;br/&gt;
	at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:341)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:289)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:289)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:289)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:260)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:207)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:206)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:249)&lt;br/&gt;
	at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:288)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:94)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:94)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:94)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:68)&lt;br/&gt;
	at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:93)&lt;br/&gt;
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:142)&lt;br/&gt;
	at org.apache.spark.sql.execution.command.CreateDatabaseCommand.run(ddl.scala:58)&lt;br/&gt;
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:57)&lt;br/&gt;
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:55)&lt;br/&gt;
	at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:69)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:115)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:136)&lt;br/&gt;
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:133)&lt;br/&gt;
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:114)&lt;br/&gt;
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:85)&lt;br/&gt;
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:85)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:187)&lt;br/&gt;
	at org.apache.spark.sql.Dataset.&amp;lt;init&amp;gt;(Dataset.scala:168)&lt;br/&gt;
	at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:63)&lt;br/&gt;
	at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:529)&lt;br/&gt;
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:649)&lt;br/&gt;
	at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:62)&lt;br/&gt;
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:325)&lt;br/&gt;
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)&lt;br/&gt;
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:311)&lt;br/&gt;
	at org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:409)&lt;br/&gt;
	at org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:425)&lt;br/&gt;
	at org.apache.hadoop.hive.cli.CliDriver.processInitFiles(CliDriver.java:436)&lt;br/&gt;
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:158)&lt;br/&gt;
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:724)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)&lt;br/&gt;
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)&lt;/p&gt;</comment>
                            <comment id="15299575" author="zjffdu" created="Wed, 25 May 2016 07:00:25 +0000"  >&lt;p&gt;Have you rebase to the latest commit ? It works well for me in spark-sql shell. &lt;/p&gt;</comment>
                            <comment id="15299587" author="jameszhouyi" created="Wed, 25 May 2016 07:06:08 +0000"  >&lt;p&gt;OK Thanks !&lt;/p&gt;</comment>
                            <comment id="15300088" author="jameszhouyi" created="Wed, 25 May 2016 14:01:24 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeffzhang&quot; class=&quot;user-hover&quot; rel=&quot;jeffzhang&quot;&gt;jeffzhang&lt;/a&gt;&lt;br/&gt;
I rebuild Spark 2.0 with &lt;a href=&quot;https://github.com/apache/spark/pull/13160&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13160&lt;/a&gt;, but Spark SQL still can&apos;t find existing hive metastore database. i still saw a metastore_db created..&lt;/p&gt;</comment>
                            <comment id="15300159" author="srowen" created="Wed, 25 May 2016 14:46:04 +0000"  >&lt;p&gt;Jeff are you saying you now think this is resolved?&lt;/p&gt;</comment>
                            <comment id="15301427" author="jameszhouyi" created="Thu, 26 May 2016 03:06:06 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jeffzhang&quot; class=&quot;user-hover&quot; rel=&quot;jeffzhang&quot;&gt;jeffzhang&lt;/a&gt;&lt;br/&gt;
In my test environment, i installed CDH 5.7.0 Hive( hive metastore service) and Apache Spark 2.0 snapshot..i can work well with Apache Spark 1.6 but it can&apos;t work after deploying the Apache Spark 2.0 and it can&apos;t connect my existing hive metastore database. What&apos;s the key configuration in spark-defaults.conf to work except for below configurations? Please kindly correct me.. Thanks !&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;spark.sql.hive.metastore.version=1.1.0
spark.sql.hive.metastore.jars=/usr/lib/hive/lib/*:/usr/lib/hadoop/client/*
spark.executor.extraClassPath=/etc/hive/conf
spark.driver.extraClassPath=/etc/hive/conf
spark.yarn.jars=/usr/lib/spark/jars/*
spark.sql.warehouse.dir=/user/hive/warehouse
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15301437" author="zjffdu" created="Thu, 26 May 2016 03:09:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;but it can&apos;t work after deploying the Apache Spark 2.0 and it can&apos;t connect my existing hive metastore database.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;What&apos;s the log you see ?&lt;/p&gt;</comment>
                            <comment id="15301456" author="jameszhouyi" created="Thu, 26 May 2016 03:21:14 +0000"  >&lt;p&gt;I issued &apos;show databases;&apos; , &apos; use XXX&apos; and &apos;show tables;&apos;  and found the result is empty and there is no any tables to show. BTW, i can see tables by &apos;show tables&apos; in Hive CLI.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;spark-sql&amp;gt; show databases;
16/05/26 11:11:47 INFO execution.SparkSqlParser: Parsing command: show databases
16/05/26 11:11:47 INFO log.PerfLogger: &amp;lt;PERFLOG method=create_database from=org.apache.hadoop.hive.metastore.RetryingHMSHandler&amp;gt;
16/05/26 11:11:47 INFO metastore.HiveMetaStore: 0: create_database: Database(name:&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, description:&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; database, locationUri:hdfs:&lt;span class=&quot;code-comment&quot;&gt;//hw-node2:8020/user/hive/warehouse, parameters:{})
&lt;/span&gt;16/05/26 11:11:47 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr      cmd=create_database: Database(name:&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;, description:&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; database, locationUri:hdfs:&lt;span class=&quot;code-comment&quot;&gt;//hw-node2:8020/user/hive/warehouse, parameters:{})
&lt;/span&gt;16/05/26 11:11:47 ERROR metastore.RetryingHMSHandler: AlreadyExistsException(message:Database &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt; already exists)
        at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_database(HiveMetaStore.java:944)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invokeInternal(RetryingHMSHandler.java:138)
        at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:99)
        at com.sun.proxy.$Proxy34.create_database(Unknown Source)
        at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createDatabase(HiveMetaStoreClient.java:646)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:105)
        at com.sun.proxy.$Proxy35.createDatabase(Unknown Source)
        at org.apache.hadoop.hive.ql.metadata.Hive.createDatabase(Hive.java:345)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply$mcV$sp(HiveClientImpl.scala:289)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:289)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createDatabase$1.apply(HiveClientImpl.scala:289)
        at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:260)
        at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:207)
        at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:206)
        at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:249)
        at org.apache.spark.sql.hive.client.HiveClientImpl.createDatabase(HiveClientImpl.scala:288)
        at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply$mcV$sp(HiveExternalCatalog.scala:94)
        at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:94)
        at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createDatabase$1.apply(HiveExternalCatalog.scala:94)
        at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:68)
        at org.apache.spark.sql.hive.HiveExternalCatalog.createDatabase(HiveExternalCatalog.scala:93)
        at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createDatabase(SessionCatalog.scala:142)
        at org.apache.spark.sql.catalyst.catalog.SessionCatalog.&amp;lt;init&amp;gt;(SessionCatalog.scala:84)
        at org.apache.spark.sql.hive.HiveSessionCatalog.&amp;lt;init&amp;gt;(HiveSessionCatalog.scala:50)
        at org.apache.spark.sql.hive.HiveSessionState.catalog$lzycompute(HiveSessionState.scala:49)
        at org.apache.spark.sql.hive.HiveSessionState.catalog(HiveSessionState.scala:48)
        at org.apache.spark.sql.hive.HiveSessionState$$anon$1.&amp;lt;init&amp;gt;(HiveSessionState.scala:63)
        at org.apache.spark.sql.hive.HiveSessionState.analyzer$lzycompute(HiveSessionState.scala:63)
        at org.apache.spark.sql.hive.HiveSessionState.analyzer(HiveSessionState.scala:62)
        at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:48)
        at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:62)
        at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:532)
        at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:652)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLDriver.run(SparkSQLDriver.scala:62)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.processCmd(SparkSQLCLIDriver.scala:323)
        at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:376)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:239)
        at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:724)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:119)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
16/05/26 11:11:47 INFO log.PerfLogger: &amp;lt;/PERFLOG method=create_database start=1464232307903 end=1464232307908 duration=5 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=0 retryCount=-1 error=&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&amp;gt;
16/05/26 11:11:48 INFO log.PerfLogger: &amp;lt;PERFLOG method=get_databases from=org.apache.hadoop.hive.metastore.RetryingHMSHandler&amp;gt;
16/05/26 11:11:48 INFO metastore.HiveMetaStore: 0: get_databases: *
16/05/26 11:11:48 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr      cmd=get_databases: *
16/05/26 11:11:48 INFO log.PerfLogger: &amp;lt;/PERFLOG method=get_databases start=1464232308202 end=1464232308208 duration=6 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=0 retryCount=0 error=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;gt;
16/05/26 11:11:48 INFO spark.SparkContext: Starting job: processCmd at CliDriver.java:376
16/05/26 11:11:48 INFO scheduler.DAGScheduler: Got job 0 (processCmd at CliDriver.java:376) with 1 output partitions
16/05/26 11:11:48 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (processCmd at CliDriver.java:376)
16/05/26 11:11:48 INFO scheduler.DAGScheduler: Parents of &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; stage: List()
16/05/26 11:11:48 INFO scheduler.DAGScheduler: Missing parents: List()
16/05/26 11:11:48 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at processCmd at CliDriver.java:376), which has no missing parents
16/05/26 11:11:48 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 3.9 KB, free 511.1 MB)
16/05/26 11:11:48 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 2.3 KB, free 511.1 MB)
16/05/26 11:11:48 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 192.168.3.11:39454 (size: 2.3 KB, free: 511.1 MB)
16/05/26 11:11:48 INFO spark.SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1012
16/05/26 11:11:48 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at processCmd at CliDriver.java:376)
16/05/26 11:11:48 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks
16/05/26 11:11:49 INFO spark.ExecutorAllocationManager: Requesting 1 &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; executor because tasks are backlogged (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; desired total will be 1)
16/05/26 11:11:53 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) (192.168.3.15:44052) with ID 1
16/05/26 11:11:53 INFO spark.ExecutorAllocationManager: New executor 1 has registered (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; total is 1)
16/05/26 11:11:53 INFO storage.BlockManagerMasterEndpoint: Registering block manager hw-node5:54623 with 511.1 MB RAM, BlockManagerId(1, hw-node5, 54623)
16/05/26 11:11:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, 192.168.3.15, partition 0, PROCESS_LOCAL, 5549 bytes)
16/05/26 11:11:53 INFO cluster.YarnClientSchedulerBackend: Launching task 0 on executor id: 1 hostname: 192.168.3.15.
16/05/26 11:11:54 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on hw-node5:54623 (size: 2.3 KB, free: 511.1 MB)
16/05/26 11:11:56 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 2734 ms on 192.168.3.15 (1/1)
16/05/26 11:11:56 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool
16/05/26 11:11:56 INFO scheduler.DAGScheduler: ResultStage 0 (processCmd at CliDriver.java:376) finished in 7.670 s
16/05/26 11:11:56 INFO scheduler.DAGScheduler: Job 0 finished: processCmd at CliDriver.java:376, took 7.882660 s
bigbench_bb101_3tb_240_sparksql
&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;use bigbench_bb101_3tb_240_sparksql;
16/05/26 11:15:49 INFO execution.SparkSqlParser: Parsing command: use bigbench_bb101_3tb_240_sparksql
16/05/26 11:15:49 INFO log.PerfLogger: &amp;lt;PERFLOG method=get_database from=org.apache.hadoop.hive.metastore.RetryingHMSHandler&amp;gt;
16/05/26 11:15:49 INFO metastore.HiveMetaStore: 0: get_database: bigbench_bb101_3tb_240_sparksql
16/05/26 11:15:49 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr      cmd=get_database: bigbench_bb101_3tb_240_sparksql
16/05/26 11:15:49 INFO log.PerfLogger: &amp;lt;/PERFLOG method=get_database start=1464232549404 end=1464232549408 duration=4 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=0 retryCount=0 error=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;gt;
16/05/26 11:15:49 INFO log.PerfLogger: &amp;lt;PERFLOG method=get_database from=org.apache.hadoop.hive.metastore.RetryingHMSHandler&amp;gt;
16/05/26 11:15:49 INFO metastore.HiveMetaStore: 0: get_database: bigbench_bb101_3tb_240_sparksql
16/05/26 11:15:49 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr      cmd=get_database: bigbench_bb101_3tb_240_sparksql
16/05/26 11:15:49 INFO log.PerfLogger: &amp;lt;/PERFLOG method=get_database start=1464232549410 end=1464232549412 duration=2 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=0 retryCount=0 error=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;gt;
16/05/26 11:15:49 INFO spark.SparkContext: Starting job: processCmd at CliDriver.java:376
16/05/26 11:15:49 INFO scheduler.DAGScheduler: Got job 1 (processCmd at CliDriver.java:376) with 1 output partitions
16/05/26 11:15:49 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (processCmd at CliDriver.java:376)
16/05/26 11:15:49 INFO scheduler.DAGScheduler: Parents of &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; stage: List()
16/05/26 11:15:49 INFO scheduler.DAGScheduler: Missing parents: List()
16/05/26 11:15:49 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[5] at processCmd at CliDriver.java:376), which has no missing parents
16/05/26 11:15:49 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 3.2 KB, free 511.1 MB)
16/05/26 11:15:49 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 1964.0 B, free 511.1 MB)
16/05/26 11:15:49 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 192.168.3.11:39454 (size: 1964.0 B, free: 511.1 MB)
16/05/26 11:15:49 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1012
16/05/26 11:15:49 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[5] at processCmd at CliDriver.java:376)
16/05/26 11:15:49 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks
16/05/26 11:15:50 INFO spark.ExecutorAllocationManager: Requesting 1 &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; executor because tasks are backlogged (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; desired total will be 1)
16/05/26 11:15:52 INFO spark.ContextCleaner: Cleaned accumulator 0
16/05/26 11:15:52 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 192.168.3.11:39454 in memory (size: 2.3 KB, free: 511.1 MB)
16/05/26 11:15:53 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) (192.168.3.15:44072) with ID 2
16/05/26 11:15:53 INFO spark.ExecutorAllocationManager: New executor 2 has registered (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; total is 1)
16/05/26 11:15:53 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, 192.168.3.15, partition 0, PROCESS_LOCAL, 5389 bytes)
16/05/26 11:15:53 INFO cluster.YarnClientSchedulerBackend: Launching task 1 on executor id: 2 hostname: 192.168.3.15.
16/05/26 11:15:53 INFO storage.BlockManagerMasterEndpoint: Registering block manager hw-node5:56967 with 511.1 MB RAM, BlockManagerId(2, hw-node5, 56967)
16/05/26 11:15:53 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on hw-node5:56967 (size: 1964.0 B, free: 511.1 MB)
16/05/26 11:15:55 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2497 ms on 192.168.3.15 (1/1)
16/05/26 11:15:55 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool
16/05/26 11:15:55 INFO scheduler.DAGScheduler: ResultStage 1 (processCmd at CliDriver.java:376) finished in 6.284 s
16/05/26 11:15:55 INFO scheduler.DAGScheduler: Job 1 finished: processCmd at CliDriver.java:376, took 6.308676 s
Time taken: 6.371 seconds
16/05/26 11:15:55 INFO CliDriver: Time taken: 6.371 seconds
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;show tables;
16/05/26 11:18:01 INFO execution.SparkSqlParser: Parsing command: show tables
16/05/26 11:18:01 INFO log.PerfLogger: &amp;lt;PERFLOG method=get_database from=org.apache.hadoop.hive.metastore.RetryingHMSHandler&amp;gt;
16/05/26 11:18:01 INFO metastore.HiveMetaStore: 0: get_database: bigbench_bb101_3tb_240_sparksql
16/05/26 11:18:01 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr      cmd=get_database: bigbench_bb101_3tb_240_sparksql
16/05/26 11:18:01 INFO log.PerfLogger: &amp;lt;/PERFLOG method=get_database start=1464232681190 end=1464232681193 duration=3 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=0 retryCount=0 error=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;gt;
16/05/26 11:18:01 INFO log.PerfLogger: &amp;lt;PERFLOG method=get_database from=org.apache.hadoop.hive.metastore.RetryingHMSHandler&amp;gt;
16/05/26 11:18:01 INFO metastore.HiveMetaStore: 0: get_database: bigbench_bb101_3tb_240_sparksql
16/05/26 11:18:01 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr      cmd=get_database: bigbench_bb101_3tb_240_sparksql
16/05/26 11:18:01 INFO log.PerfLogger: &amp;lt;/PERFLOG method=get_database start=1464232681194 end=1464232681196 duration=2 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=0 retryCount=0 error=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;gt;
16/05/26 11:18:01 INFO log.PerfLogger: &amp;lt;PERFLOG method=get_tables from=org.apache.hadoop.hive.metastore.RetryingHMSHandler&amp;gt;
16/05/26 11:18:01 INFO metastore.HiveMetaStore: 0: get_tables: db=bigbench_bb101_3tb_240_sparksql pat=*
16/05/26 11:18:01 INFO HiveMetaStore.audit: ugi=root    ip=unknown-ip-addr      cmd=get_tables: db=bigbench_bb101_3tb_240_sparksql pat=*
16/05/26 11:18:01 INFO log.PerfLogger: &amp;lt;/PERFLOG method=get_tables start=1464232681197 end=1464232681238 duration=41 from=org.apache.hadoop.hive.metastore.RetryingHMSHandler threadId=0 retryCount=0 error=&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&amp;gt;
16/05/26 11:18:01 INFO spark.SparkContext: Starting job: processCmd at CliDriver.java:376
16/05/26 11:18:01 INFO scheduler.DAGScheduler: Got job 2 (processCmd at CliDriver.java:376) with 1 output partitions
16/05/26 11:18:01 INFO scheduler.DAGScheduler: Final stage: ResultStage 2 (processCmd at CliDriver.java:376)
16/05/26 11:18:01 INFO scheduler.DAGScheduler: Parents of &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; stage: List()
16/05/26 11:18:01 INFO scheduler.DAGScheduler: Missing parents: List()
16/05/26 11:18:01 INFO scheduler.DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:376), which has no missing parents
16/05/26 11:18:01 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 4.0 KB, free 511.1 MB)
16/05/26 11:18:01 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 2.4 KB, free 511.1 MB)
16/05/26 11:18:01 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 192.168.3.11:39454 (size: 2.4 KB, free: 511.1 MB)
16/05/26 11:18:01 INFO spark.SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1012
16/05/26 11:18:01 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[8] at processCmd at CliDriver.java:376)
16/05/26 11:18:01 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks
16/05/26 11:18:02 INFO spark.ExecutorAllocationManager: Requesting 1 &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; executor because tasks are backlogged (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; desired total will be 1)
16/05/26 11:18:04 INFO cluster.YarnClientSchedulerBackend: Registered executor NettyRpcEndpointRef(&lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;) (192.168.3.15:44086) with ID 3
16/05/26 11:18:04 INFO spark.ExecutorAllocationManager: New executor 3 has registered (&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; total is 1)
16/05/26 11:18:04 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, 192.168.3.15, partition 0, PROCESS_LOCAL, 5365 bytes)
16/05/26 11:18:04 INFO cluster.YarnClientSchedulerBackend: Launching task 2 on executor id: 3 hostname: 192.168.3.15.
16/05/26 11:18:04 INFO storage.BlockManagerMasterEndpoint: Registering block manager hw-node5:57277 with 511.1 MB RAM, BlockManagerId(3, hw-node5, 57277)
16/05/26 11:18:05 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on hw-node5:57277 (size: 2.4 KB, free: 511.1 MB)
16/05/26 11:18:06 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1952 ms on 192.168.3.15 (1/1)
16/05/26 11:18:06 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool
16/05/26 11:18:06 INFO scheduler.DAGScheduler: ResultStage 2 (processCmd at CliDriver.java:376) finished in 5.532 s
16/05/26 11:18:06 INFO scheduler.DAGScheduler: Job 2 finished: processCmd at CliDriver.java:376, took 5.555326 s
Time taken: 5.677 seconds
16/05/26 11:18:06 INFO CliDriver: Time taken: 5.677 seconds
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15301462" author="zjffdu" created="Thu, 26 May 2016 03:25:06 +0000"  >&lt;p&gt;This seems hadoop distributor specific, you may need to ask in CDH forum. &lt;/p&gt;</comment>
                            <comment id="15301468" author="jameszhouyi" created="Thu, 26 May 2016 03:35:06 +0000"  >&lt;p&gt;I&apos;m confused that it worked well with Apache Spark 1.6 and can&apos;t work after only replacing it with 2.0. &lt;/p&gt;</comment>
                            <comment id="15327152" author="m1lan" created="Mon, 13 Jun 2016 10:52:55 +0000"  >&lt;p&gt;Does not work as expected when using spark-submit; for example, this works fine and prints all databases in Hive storage&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;# file test_db.py
from pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkSession
from pyspark &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkConf

&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; __name__ == &lt;span class=&quot;code-quote&quot;&gt;&quot;__main__&quot;&lt;/span&gt;:
    conf = SparkConf()
    hive_context = (SparkSession.builder.config(conf=conf).enableHiveSupport().getOrCreate())
    print(hive_context.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;show databases&quot;&lt;/span&gt;).collect())
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;However, using HiveContext yields only &apos;default&apos; database:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;#file test.py
from pyspark.sql &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; HiveContext
from pyspark improt SparkContext, SparkConf

&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; __name__ == &lt;span class=&quot;code-quote&quot;&gt;&quot;__main__&quot;&lt;/span&gt;:
    conf = SparkConf()
    sc = SparkContext(conf=conf)
    hive_context = HiveContext(sc)
    print(hive_context.sql(&lt;span class=&quot;code-quote&quot;&gt;&quot;show databases&quot;&lt;/span&gt;).collect())

    # The result is
    #[Row(result=&lt;span class=&quot;code-quote&quot;&gt;&apos;&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;&apos;&lt;/span&gt;)]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is there something I am still missing? I am using the newest branch-2.0&lt;/p&gt;</comment>
                            <comment id="15327803" author="hvanhovell" created="Mon, 13 Jun 2016 17:35:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=m1lan&quot; class=&quot;user-hover&quot; rel=&quot;m1lan&quot;&gt;m1lan&lt;/a&gt; Just to be sure, is this the actual code you have copy &amp;amp; pasted here? There is a typo in &lt;tt&gt;conf = SparkConrf()&lt;/tt&gt;, should be  &lt;tt&gt;conf = SparkConf()&lt;/tt&gt;.&lt;/p&gt;</comment>
                            <comment id="15329023" author="m1lan" created="Tue, 14 Jun 2016 07:39:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=hvanhovell&quot; class=&quot;user-hover&quot; rel=&quot;hvanhovell&quot;&gt;hvanhovell&lt;/a&gt; Thanks, I corrected it. But this is not the problem, script I was using did not have this typo. The result of last script was &lt;tt&gt;[Row(result=&apos;default&apos;)]&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="15345639" author="cloud_fan" created="Thu, 23 Jun 2016 03:17:33 +0000"  >&lt;p&gt;is this still a problem?&lt;/p&gt;</comment>
                            <comment id="15345652" author="zjffdu" created="Thu, 23 Jun 2016 03:25:24 +0000"  >&lt;p&gt;It has been resolved in &lt;a href=&quot;https://github.com/apache/spark/pull/13160&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/13160&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15345655" author="cloud_fan" created="Thu, 23 Jun 2016 03:27:20 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rxin&quot; class=&quot;user-hover&quot; rel=&quot;rxin&quot;&gt;rxin&lt;/a&gt; should we resolve this ticket?&lt;/p&gt;</comment>
                            <comment id="15345999" author="m1lan" created="Thu, 23 Jun 2016 07:51:33 +0000"  >&lt;p&gt;Please note, that the issue I raised in my last comment still seems to be unsolved&lt;/p&gt;</comment>
                            <comment id="15347497" author="cloud_fan" created="Fri, 24 Jun 2016 00:50:23 +0000"  >&lt;p&gt;hi, can you open a new JIRA for the remaining problem? thanks!&lt;/p&gt;</comment>
                            <comment id="15347576" author="yhuai" created="Fri, 24 Jun 2016 02:31:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=m1lan&quot; class=&quot;user-hover&quot; rel=&quot;m1lan&quot;&gt;m1lan&lt;/a&gt; Do you use hive-site.xml to store the url to your metastore?&lt;/p&gt;</comment>
                            <comment id="15350524" author="m1lan" created="Mon, 27 Jun 2016 07:36:36 +0000"  >&lt;p&gt;Yes, hive-site.xml exists and contains information on the location of Hive metastore. This issue is appears only when those snippets are run with &lt;tt&gt;spark-submit&lt;/tt&gt; command. In pyspark it works fine.&lt;/p&gt;

&lt;p&gt;I created a separate issue for this problem: &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-16224&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;SPARK-16224&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15350526" author="m1lan" created="Mon, 27 Jun 2016 07:38:17 +0000"  >&lt;p&gt;I created a separate issue for this:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-16224&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;SPARK-16224&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15414544" author="tanejagagan" created="Wed, 10 Aug 2016 01:05:27 +0000"  >&lt;p&gt;i was annoyed by this bug but fortunately found the workaround &lt;br/&gt;
it will work fine if you start the server with &lt;br/&gt;
export HADOOP_CONF_DIR=/etc/hadoop/conf:/etc/hive/conf&lt;br/&gt;
bin/spark-shell&lt;/p&gt;

&lt;p&gt;It would give trouble if HIVE_CONF_DIR is as below &lt;br/&gt;
export HADOOP_CONF_DIR=/etc/hadoop/conf&lt;br/&gt;
export HIVE_CONF_DIR=/etc/hive/conf&lt;br/&gt;
bin/spark-shell&lt;/p&gt;</comment>
                            <comment id="16104418" author="subu_technocrat" created="Fri, 28 Jul 2017 04:04:39 +0000"  >&lt;p&gt;Spark Hive reporting pyspark.sql.utils.AnalysisException: u&apos;Table not found: XXX&apos; when run on yarn cluster. We are still facing this issue with spark 2.1.1 any update on this is this resolved?&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12971293">SPARK-15417</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 16 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2y0i7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12329449">2.0.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>