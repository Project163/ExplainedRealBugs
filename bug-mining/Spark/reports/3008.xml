<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:37:02 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-9844] File appender race condition during SparkWorker shutdown</title>
                <link>https://issues.apache.org/jira/browse/SPARK-9844</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;We find this issue still exists in 1.3.1&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,653 SparkWorker-0 ExternalLogger.java:96 - Error writing stream to file /&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/spark/worker/worker-0/app-20150728224954-0003/0/stderr
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,653 SparkWorker-0 ExternalLogger.java:96 - java.io.IOException: Stream closed
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170) ~[na:1.8.0_40]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283) ~[na:1.8.0_40]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.BufferedInputStream.read(BufferedInputStream.java:345) ~[na:1.8.0_40]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,654 SparkWorker-0 ExternalLogger.java:96 - 	at java.io.FilterInputStream.read(FilterInputStream.java:107) ~[na:1.8.0_40]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70) ~[spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,655 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1618) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
ERROR [&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;-6] 2015-07-28 22:49:57,656 SparkWorker-0 ExternalLogger.java:96 - 	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38) [spark-core_2.10-1.3.1.1.jar:1.3.1.1]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;at  &lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala#L159&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/ExecutorRunner.scala#L159&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The process auto shuts down, but the log appenders are still running, which causes the error log messages.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12854269">SPARK-9844</key>
            <summary>File appender race condition during SparkWorker shutdown</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="bryanc">Bryan Cutler</assignee>
                                    <reporter username="alexliu68">Alex Liu</reporter>
                        <labels>
                    </labels>
                <created>Tue, 11 Aug 2015 20:45:38 +0000</created>
                <updated>Wed, 17 Feb 2016 18:15:15 +0000</updated>
                            <resolved>Thu, 14 Jan 2016 11:06:02 +0000</resolved>
                                    <version>1.3.0</version>
                    <version>1.4.0</version>
                                    <fixVersion>1.6.1</fixVersion>
                    <fixVersion>2.0.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>14</watches>
                                                                                                                <comments>
                            <comment id="14692928" author="ykrips" created="Wed, 12 Aug 2015 05:03:46 +0000"  >&lt;p&gt;Can I take this issue?&lt;/p&gt;</comment>
                            <comment id="14693177" author="ykrips" created="Wed, 12 Aug 2015 09:04:30 +0000"  >&lt;p&gt;I quickly assessed this issue, and I found same issue on 1.4.1 cluster, but I did not on 1.5.0 cluster.&lt;/p&gt;</comment>
                            <comment id="14693666" author="alexliu68" created="Wed, 12 Aug 2015 15:22:03 +0000"  >&lt;p&gt;Please provide a pull for it if you could&lt;/p&gt;</comment>
                            <comment id="14744590" author="garman" created="Tue, 15 Sep 2015 00:45:02 +0000"  >&lt;p&gt;I believe this still exists in 1.5.0, I am getting this on my workers almost every run thru:&lt;/p&gt;

&lt;p&gt;{{java.io.IOException: Stream closed&lt;br/&gt;
	at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)&lt;br/&gt;
	at java.io.BufferedInputStream.read1(BufferedInputStream.java:283)&lt;br/&gt;
	at java.io.BufferedInputStream.read(BufferedInputStream.java:345)&lt;br/&gt;
	at java.io.FilterInputStream.read(FilterInputStream.java:107)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)&lt;br/&gt;
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)&lt;br/&gt;
	at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)&lt;br/&gt;
2015-09-15 00:40:03,555 WARN ReliableDeliverySupervisor: Association with remote system &lt;span class=&quot;error&quot;&gt;&amp;#91;akka.tcp://sparkExecutor@10.192.5.12:49855&amp;#93;&lt;/span&gt; has failed, address is now gated for &lt;span class=&quot;error&quot;&gt;&amp;#91;5000&amp;#93;&lt;/span&gt; ms. Reason: &lt;span class=&quot;error&quot;&gt;&amp;#91;Disassociated&amp;#93;&lt;/span&gt;}}&lt;/p&gt;</comment>
                            <comment id="14805000" author="jixianwu" created="Fri, 18 Sep 2015 05:26:38 +0000"  >&lt;p&gt;yes, this issue still exists in 1.5.0.&lt;/p&gt;

&lt;p&gt;I run spark job by submitting spark-itemsimilarity by mahout dsl, same log output:&lt;/p&gt;

&lt;p&gt;15/09/18 12:41:41 ERROR FileAppender: Error writing stream to file /data/cloud/spark/work/app-20150918123941-0000/0/stderr&lt;br/&gt;
java.io.IOException: Stream closed&lt;br/&gt;
        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:170)&lt;br/&gt;
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:283)&lt;br/&gt;
        at java.io.BufferedInputStream.read(BufferedInputStream.java:345)&lt;br/&gt;
        at java.io.FilterInputStream.read(FilterInputStream.java:107)&lt;br/&gt;
        at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)&lt;br/&gt;
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)&lt;br/&gt;
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)&lt;br/&gt;
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)&lt;br/&gt;
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)&lt;br/&gt;
        at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)&lt;br/&gt;
15/09/18 12:41:41 INFO Worker: Executor app-20150918123941-0000/0 finished with state KILLED exitStatus 143&lt;br/&gt;
then job failed.&lt;/p&gt;

&lt;p&gt;I run this job again, but this log no show.&lt;/p&gt;</comment>
                            <comment id="14952761" author="krash" created="Mon, 12 Oct 2015 07:44:35 +0000"  >&lt;p&gt;Is this issue really minor? In my case, this bug causes workers to get OOM error in 1.5.0&lt;/p&gt;</comment>
                            <comment id="15005504" author="jasson15" created="Sat, 14 Nov 2015 17:33:12 +0000"  >&lt;p&gt;Got the same error log in workers and my workers keep being disassociated.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;15/11/15 01:25:26 INFO worker.Worker: Asked to kill executor app-20151115012248-0081/2
15/11/15 01:25:26 INFO worker.ExecutorRunner: Runner thread &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; executor app-20151115012248-0081/2 interrupted
15/11/15 01:25:26 INFO worker.ExecutorRunner: Killing process!
15/11/15 01:25:26 ERROR logging.FileAppender: Error writing stream to file /usr/local/spark-1.5.1-bin-hadoop2.6/work/app-20151115012248-0081/2/stderr
java.io.IOException: Stream closed
        at java.io.BufferedInputStream.getBufIfOpen(BufferedInputStream.java:162)
        at java.io.BufferedInputStream.read1(BufferedInputStream.java:272)
        at java.io.BufferedInputStream.read(BufferedInputStream.java:334)
        at java.io.FilterInputStream.read(FilterInputStream.java:107)
        at org.apache.spark.util.logging.FileAppender.appendStreamToFile(FileAppender.scala:70)
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply$mcV$sp(FileAppender.scala:39)
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
        at org.apache.spark.util.logging.FileAppender$$anon$1$$anonfun$run$1.apply(FileAppender.scala:39)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1699)
        at org.apache.spark.util.logging.FileAppender$$anon$1.run(FileAppender.scala:38)
15/11/15 01:25:26 INFO worker.Worker: Executor app-20151115012248-0081/2 finished with state KILLED exitStatus 143
15/11/15 01:25:26 INFO worker.Worker: Cleaning up local directories &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; application app-20151115012248-0081
15/11/15 01:25:26 WARN remote.ReliableDeliverySupervisor: Association with remote system [akka.tcp:&lt;span class=&quot;code-comment&quot;&gt;//sparkExecutor@10.1.2.1:46780] has failed, address is
&lt;/span&gt; now gated &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; [5000] ms. Reason: [Disassociated]
15/11/15 01:25:26 INFO shuffle.ExternalShuffleBlockResolver: Application app-20151115012248-0081 removed, cleanupLocalDirs = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We use python3 to run our Spark jobs&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;#!/usr/bin/python3

&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; os
&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; sys

SPARK_HOME = &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/local/spark&quot;&lt;/span&gt;

os.environ[&lt;span class=&quot;code-quote&quot;&gt;&quot;SPARK_HOME&quot;&lt;/span&gt;] = SPARK_HOME
os.environ[&lt;span class=&quot;code-quote&quot;&gt;&quot;JAVA_HOME&quot;&lt;/span&gt;] = &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/lib/jvm/java-7-oracle&quot;&lt;/span&gt;
os.environ[&lt;span class=&quot;code-quote&quot;&gt;&quot;PYSPARK_PYTHON&quot;&lt;/span&gt;] = &lt;span class=&quot;code-quote&quot;&gt;&quot;/usr/bin/python3&quot;&lt;/span&gt;
sys.path.append(os.path.join(SPARK_HOME, &lt;span class=&quot;code-quote&quot;&gt;&apos;python&apos;&lt;/span&gt;))
sys.path.append(os.path.join(SPARK_HOME, &lt;span class=&quot;code-quote&quot;&gt;&apos;python/lib/py4j-0.8.2.1-src.zip&apos;&lt;/span&gt;))

from pyspark &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkContext, SparkConf

conf = (SparkConf().setMaster(&lt;span class=&quot;code-quote&quot;&gt;&quot;spark:&lt;span class=&quot;code-comment&quot;&gt;//10.1.2.1:7077&quot;&lt;/span&gt;)
&lt;/span&gt;        .setAppName(&lt;span class=&quot;code-quote&quot;&gt;&quot;Generate&quot;&lt;/span&gt;)
        .setAll((
            (&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.cores.max&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1&quot;&lt;/span&gt;),
            (&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.driver.memory&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1g&quot;&lt;/span&gt;),
            (&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.executor.memory&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1g&quot;&lt;/span&gt;),
            (&lt;span class=&quot;code-quote&quot;&gt;&quot;spark.python.worker.memory&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;1g&quot;&lt;/span&gt;))))
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15086202" author="bryanc" created="Wed, 6 Jan 2016 20:10:35 +0000"  >&lt;p&gt;I came across this error recently too as of 1.6.0 and have been looking into the cause.  I can work on a PR unless someone else has already started.&lt;/p&gt;</comment>
                            <comment id="15086241" author="yhuai" created="Wed, 6 Jan 2016 20:37:56 +0000"  >&lt;p&gt;I think no one is working on it. Feel free to send out a PR with the fix and regression test &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15093055" author="apachespark" created="Tue, 12 Jan 2016 01:11:05 +0000"  >&lt;p&gt;User &apos;BryanCutler&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/10714&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10714&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15097996" author="srowen" created="Thu, 14 Jan 2016 11:06:02 +0000"  >&lt;p&gt;Resolved by &lt;a href=&quot;https://github.com/apache/spark/pull/10714&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/10714&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15150862" author="mballoni" created="Wed, 17 Feb 2016 17:48:02 +0000"  >&lt;p&gt; Is there any way of avoiding this error in 1.4.1? All my workers are being shut down a few times a day.&lt;br/&gt;
 I was thinking about turning off the log appenders, however I&apos;m not sure that it would be of any help.&lt;/p&gt;</comment>
                            <comment id="15150913" author="bryanc" created="Wed, 17 Feb 2016 18:15:15 +0000"  >&lt;p&gt;This error is benign for the most part, once it gets here, the worker is already being shut down.  So it is probably something else that is causing your worker to shut down.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12932120">SPARK-12876</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12753741">SPARK-4300</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12753741">SPARK-4300</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 39 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2ilu7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>