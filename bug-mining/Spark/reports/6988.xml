<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:11:33 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-30528] Potential performance regression with DPP subquery duplication</title>
                <link>https://issues.apache.org/jira/browse/SPARK-30528</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;In DPP, heuristics to decide if DPP is going to benefit relies on the sizes of the tables in the right subtree of the join. This might not be a correct estimate especially when the detailed column level stats are not available.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
    &lt;span class=&quot;code-comment&quot;&gt;// the pruning overhead is the total size in bytes of all scan relations
&lt;/span&gt;    val overhead = otherPlan.collectLeaves().map(_.stats.sizeInBytes).sum.toFloat
    filterRatio * partPlan.stats.sizeInBytes.toFloat &amp;gt; overhead.toFloat
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Also, DPP executes the entire right side of the join as a subquery because of which multiple scans happen for the tables in the right subtree of the join. This can cause issues when join is non-Broadcast Hash Join (BHJ) and reuse of the subquery result does not happen. Also, I couldn&#8217;t figure out, why do the results from the subquery get re-used only for BHJ?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Consider a query,&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
SELECT * 
FROM   store_sales_partitioned 
       JOIN (SELECT * 
             FROM   store_returns_partitioned, 
                    date_dim 
             WHERE  sr_returned_date_sk = d_date_sk) ret_date 
         ON ss_sold_date_sk = d_date_sk 
WHERE  d_fy_quarter_seq &amp;gt; 0 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;DPP will kick-in for both the join. (Please check the image plan.png attached below for the plan)&lt;/p&gt;

&lt;p&gt;Some of the observations -&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Based on heuristics, DPP would go ahead with pruning if the cost of scanning the tables in the right sub-tree of the join is less than the benefit due to pruning. This is due to the reason that multiple scans will be needed for an SMJ. But heuristics simply checks if the benefits offset the cost of multiple scans and do not take into consideration other operations like Join, etc in the right subtree which can be quite expensive. This issue will be particularly prominent when detailed column level stats are not available. In the example above, a decision that pruningHasBenefit was made on the basis of sizes of the tables&#160;store_returns_partitioned and date_dim but did not take into consideration the join between them before the join happens with the store_sales_partitioned table.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Multiple scans are needed when the join is SMJ as the reuse of the exchanges does not happen. This is because Aggregate gets added on top of the right subtree to be executed as a subquery in order to prune only required columns. Here, scanning all the columns as the right subtree of the join would, and reusing the same exchange might be more helpful as it avoids duplicate scans.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This was just a representative example, but in-general for cases such as in the image cases.png below, DPP can cause performance issues.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Also, for the cases when there are multiple DPP compatible join conditions in the same join, the entire right subtree of the join would be executed as a subquery that many times. Consider an example,&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
SELECT&#160;*&#160;
FROM&#160;&#160;&#160;partitionedtable&#160;
&#160;&#160;&#160;&#160;&#160;&#160;&#160;JOIN&#160;nonpartitionedtable&#160;
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ON&#160;partcol1&#160;=&#160;col1&#160;
&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;AND&#160;partcol2&#160;=&#160;col2&#160;
WHERE&#160;&#160;nonpartitionedtable.id&#160;&amp;gt;&#160;0&#160;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Here the right subtree of the join (scan of table&#160;nonpartitionedtable) would be executed twice as a subquery, once each for the every join condition. These two subqueries should be aggregated and executed only once as they are almost the same apart from the columns that they prune. Check the image dup_subquery.png attached below for the details.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13279811">SPARK-30528</key>
            <summary>Potential performance regression with DPP subquery duplication</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="maryannxue">Wei Xue</assignee>
                                    <reporter username="mayurb31">Mayur Bhosale</reporter>
                        <labels>
                            <label>performance</label>
                    </labels>
                <created>Thu, 16 Jan 2020 12:13:24 +0000</created>
                <updated>Sun, 17 May 2020 17:58:37 +0000</updated>
                            <resolved>Thu, 13 Feb 2020 11:44:15 +0000</resolved>
                                    <version>3.0.0</version>
                                    <fixVersion>3.0.0</fixVersion>
                                    <component>Optimizer</component>
                    <component>SQL</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="17019268" author="mayurb31" created="Mon, 20 Jan 2020 07:16:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=maryannxue&quot; class=&quot;user-hover&quot; rel=&quot;maryannxue&quot;&gt;maryannxue&lt;/a&gt;&#160;can you please comment on this?&lt;/p&gt;</comment>
                            <comment id="17020385" author="maryannxue" created="Tue, 21 Jan 2020 17:03:56 +0000"  >&lt;p&gt;Good point, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mayurb31&quot; class=&quot;user-hover&quot; rel=&quot;mayurb31&quot;&gt;mayurb31&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;1. Heuristics: yes, we should improve the cost estimate for the filter plan. As a quick workaround, though, you could set&#160;&lt;/p&gt;

&lt;p&gt;&quot;spark.sql.optimizer.dynamicPartitionPruning.fallbackFilterRatio&quot; to &quot;0.0&quot;, which would disable this kind of DPP if columns stats are not available.&lt;/p&gt;

&lt;p&gt;2. Reuse: yes, that&apos;s a dilemma here: first of all we wanna reduce the result set returned to the driver for pruning values, and that&apos;s why the Aggregate is added. It might kill some potential opportunities for exchange reuse if the filter plan contains another join, but that kind of potential opportunity is not fully guaranteed even if we didn&apos;t push down the Aggregate, for the&#160; join in the filter plan can turn out to be a BHJ. However, `pruningHasBenefit` had intended to cost the entire DPP subquery as overhead without considering reuse, so this takes us back to point 1: we should improve the costing of heuristics so that this kind of DPP should not be triggered at all if the scan + join would be just too much work.&lt;/p&gt;

&lt;p&gt;3. Can you attach the query plan instead of UI for your last example? I think the reuse did not happen because the first subquery selects `col1` while the second `col2`?&#160;&lt;/p&gt;</comment>
                            <comment id="17020803" author="mayurb31" created="Wed, 22 Jan 2020 06:08:54 +0000"  >&lt;p&gt;Thanks for the explanation &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=maryannxue&quot; class=&quot;user-hover&quot; rel=&quot;maryannxue&quot;&gt;maryannxue&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Should DPP be turned off by default till the heuristics are improved or keep having it turned on by default but don&apos;t do DPP when the column level stats are not available? Because for some cases this can be really disastrous.&lt;/li&gt;
	&lt;li&gt;Can we use the bloom filter to store the pruning values (for non-Broadcast Hash Join)? This will have multiple advantages -
	&lt;ol&gt;
		&lt;li&gt;The size of the result returned to the driver would be way smaller&lt;/li&gt;
		&lt;li&gt;Faster lookups compared to hashSet&lt;/li&gt;
		&lt;li&gt;Reuse of the exchange will happen (because we won&apos;t be adding Aggregate on top)&lt;/li&gt;
		&lt;li&gt;Duplicate subqueries because of multiple join conditions on partitioned columns will get removed (cases like example 3 in the description above)&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&#160; &#160; &#160; &#160; &#160;This will require more thoughts though. Let me know if this sounds feasible and useful, then I can get back with more details and can pick it up as well.&#160;&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160;3. Yes, one of the subqueries selects `col1` and the other selects `col2`.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
== Physical Plan ==                                                             
*(5) SortMergeJoin [partcol1#2L, partcol2#3], [col1#5L, col2#6], Inner
:- *(2) Sort [partcol1#2L ASC NULLS FIRST, partcol2#3 ASC NULLS FIRST], &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, 0
:  +- Exchange hashpartitioning(partcol1#2L, partcol2#3, 200), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, [id=#103]
:     +- *(1) ColumnarToRow
:        +- FileScan parquet &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.partitionedtable[id#0L,name#1,partCol1#2L,partCol2#3] Batched: &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, DataFilters: [], Format: Parquet, Location: PrunedInMemoryFileIndex[file:/~/src/spark/bin/sp..., PartitionFilters: [isnotnull(partCol2#3), isnotnull(partCol1#2L), dynamicpruningexpression(partCol1#2L IN subquery#..., PushedFilters: [], ReadSchema: struct&amp;lt;id:bigint,name:string&amp;gt;
:              :- Subquery subquery#19, [id=#49]
:              :  +- *(2) HashAggregate(keys=[col1#5L], functions=[])
:              :     +- Exchange hashpartitioning(col1#5L, 200), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, [id=#45]
:              :        +- *(1) HashAggregate(keys=[col1#5L], functions=[])
:              :           +- *(1) Project [col1#5L]
:              :              +- *(1) Filter (((isnotnull(id#4L) AND (id#4L &amp;gt; 0)) AND isnotnull(col2#6)) AND isnotnull(col1#5L))
:              :                 +- *(1) ColumnarToRow
:              :                    +- FileScan parquet &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.nonpartitionedtable[id#4L,col1#5L,col2#6] Batched: &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, DataFilters: [isnotnull(id#4L), (id#4L &amp;gt; 0), isnotnull(col2#6), isnotnull(col1#5L)], Format: Parquet, Location: InMemoryFileIndex[file:/~/src/spark/bin/spark-wa..., PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThan(id,0), IsNotNull(col2), IsNotNull(col1)], ReadSchema: struct&amp;lt;id:bigint,col1:bigint,col2:string&amp;gt;
:              +- Subquery subquery#21, [id=#82]
:                 +- *(2) HashAggregate(keys=[col2#6], functions=[])
:                    +- Exchange hashpartitioning(col2#6, 200), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, [id=#78]
:                       +- *(1) HashAggregate(keys=[col2#6], functions=[])
:                          +- *(1) Project [col2#6]
:                             +- *(1) Filter (((isnotnull(id#4L) AND (id#4L &amp;gt; 0)) AND isnotnull(col2#6)) AND isnotnull(col1#5L))
:                                +- *(1) ColumnarToRow
:                                   +- FileScan parquet &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.nonpartitionedtable[id#4L,col1#5L,col2#6] Batched: &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, DataFilters: [isnotnull(id#4L), (id#4L &amp;gt; 0), isnotnull(col2#6), isnotnull(col1#5L)], Format: Parquet, Location: InMemoryFileIndex[file:/~/src/spark/bin/spark-wa..., PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThan(id,0), IsNotNull(col2), IsNotNull(col1)], ReadSchema: struct&amp;lt;id:bigint,col1:bigint,col2:string&amp;gt;
+- *(4) Sort [col1#5L ASC NULLS FIRST, col2#6 ASC NULLS FIRST], &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;, 0
   +- Exchange hashpartitioning(col1#5L, col2#6, 200), &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, [id=#113]
      +- *(3) Project [id#4L, col1#5L, col2#6, name#7]
         +- *(3) Filter (((isnotnull(id#4L) AND (id#4L &amp;gt; 0)) AND isnotnull(col2#6)) AND isnotnull(col1#5L))
            +- *(3) ColumnarToRow
               +- FileScan parquet &lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;.nonpartitionedtable[id#4L,col1#5L,col2#6,name#7] Batched: &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;, DataFilters: [isnotnull(id#4L), (id#4L &amp;gt; 0), isnotnull(col2#6), isnotnull(col1#5L)], Format: Parquet, Location: InMemoryFileIndex[file:/~/src/spark/bin/spark-wa..., PartitionFilters: [], PushedFilters: [IsNotNull(id), GreaterThan(id,0), IsNotNull(col2), IsNotNull(col1)], ReadSchema: struct&amp;lt;id:bigint,col1:bigint,col2:string,name:string&amp;gt; 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160; &#160; &#160; &#160; &#160; &#160;If we don&apos;t decide to go with removing Aggregate (not using BloomFilter), should we combine such DPP subqueries into a&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;single sub-query? We can avoid duplicate computation this way.&lt;/p&gt;</comment>
                            <comment id="17021257" author="maryannxue" created="Wed, 22 Jan 2020 16:39:43 +0000"  >&lt;ol&gt;
	&lt;li&gt;Turning off the non-broadcast-reuse DPP by default is an option.&lt;/li&gt;
	&lt;li&gt;So it really depends: bloom filter would reduce the partition filtering rate, thus getting you less benefit, and the penalties are (without extra aggregate):
	&lt;ol&gt;
		&lt;li&gt;Duplicate scans or other ops like joins, if the filter plan is partially reused;&lt;/li&gt;
		&lt;li&gt;Otherwise, you could implement sth. just like broadcast reuse to fully reuse the shuffle exchange of the filter plan, but you need to be aware that you are now serializing the two child operators&apos; computation of an SMJ.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Yes, you could.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="17036153" author="cloud_fan" created="Thu, 13 Feb 2020 11:44:15 +0000"  >&lt;p&gt;Issue resolved by pull request 27551&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/27551&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/27551&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12991121" name="cases.png" size="163885" author="mayurb31" created="Thu, 16 Jan 2020 12:14:45 +0000"/>
                            <attachment id="12991358" name="dup_subquery.png" size="1718724" author="mayurb31" created="Mon, 20 Jan 2020 07:14:01 +0000"/>
                            <attachment id="12991122" name="plan.png" size="468503" author="mayurb31" created="Thu, 16 Jan 2020 12:14:45 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 39 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0aknk:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>