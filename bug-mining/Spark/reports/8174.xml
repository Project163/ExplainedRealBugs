<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 19:27:43 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-36673] Incorrect Unions of struct with mismatched field name case</title>
                <link>https://issues.apache.org/jira/browse/SPARK-36673</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;If a nested field has different casing on two sides of the union, the resultant schema of the union will both fields in its schemaa&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
scala&amp;gt; val df1 = spark.range(2).withColumn(&lt;span class=&quot;code-quote&quot;&gt;&quot;nested&quot;&lt;/span&gt;, struct(expr(&lt;span class=&quot;code-quote&quot;&gt;&quot;id * 5 AS INNER&quot;&lt;/span&gt;)))
df1: org.apache.spark.sql.DataFrame = [id: bigint, nested: struct&amp;lt;INNER: bigint&amp;gt;]

val df2 = spark.range(2).withColumn(&lt;span class=&quot;code-quote&quot;&gt;&quot;nested&quot;&lt;/span&gt;, struct(expr(&lt;span class=&quot;code-quote&quot;&gt;&quot;id * 5 AS &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt;&quot;&lt;/span&gt;)))
df2: org.apache.spark.sql.DataFrame = [id: bigint, nested: struct&amp;lt;&lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt;: bigint&amp;gt;]

scala&amp;gt; df1.union(df2).printSchema
root
 |-- id: &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; (nullable = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
 |-- nested: struct (nullable = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
 |    |-- INNER: &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; (nullable = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
 |    |-- &lt;span class=&quot;code-keyword&quot;&gt;inner&lt;/span&gt;: &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; (nullable = &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;)
 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This seems like a bug. I would expect that Spark SQL would either just union by index or if the user has requested &lt;tt&gt;unionByName&lt;/tt&gt;, then it should matched fields case insensitively if &lt;tt&gt;spark.sql.caseSensitive&lt;/tt&gt; is &lt;tt&gt;false&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;However the output data only has one nested column&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
scala&amp;gt; df1.union(df2).show()
+---+------+
| id|nested|
+---+------+
|  0|   {0}|
|  1|   {5}|
|  0|   {0}|
|  1|   {5}|
+---+------+
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Trying to project fields of &lt;tt&gt;nested&lt;/tt&gt; throws an error:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
scala&amp;gt; df1.union(df2).select(&lt;span class=&quot;code-quote&quot;&gt;&quot;nested.*&quot;&lt;/span&gt;).show()
java.lang.ArrayIndexOutOfBoundsException: 1
  at org.apache.spark.sql.types.StructType.apply(StructType.scala:414)
  at org.apache.spark.sql.catalyst.expressions.GetStructField.dataType(complexTypeExtractors.scala:108)
  at org.apache.spark.sql.catalyst.expressions.Alias.toAttribute(namedExpressions.scala:192)
  at org.apache.spark.sql.catalyst.plans.logical.Project.$anonfun$output$1(basicLogicalOperators.scala:63)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.AbstractTraversable.map(Traversable.scala:108)
  at org.apache.spark.sql.catalyst.plans.logical.Project.output(basicLogicalOperators.scala:63)
  at org.apache.spark.sql.catalyst.plans.logical.Union.$anonfun$output$3(basicLogicalOperators.scala:260)
  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:238)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at scala.collection.TraversableLike.map(TraversableLike.scala:238)
  at scala.collection.TraversableLike.map$(TraversableLike.scala:231)
  at scala.collection.immutable.List.map(List.scala:298)
  at org.apache.spark.sql.catalyst.plans.logical.Union.output(basicLogicalOperators.scala:260)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.outputSet$lzycompute(QueryPlan.scala:49)
  at org.apache.spark.sql.catalyst.plans.QueryPlan.outputSet(QueryPlan.scala:49)
  at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$$anonfun$apply$8.applyOrElse(Optimizer.scala:747)
  at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$$anonfun$apply$8.applyOrElse(Optimizer.scala:695)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$1(TreeNode.scala:316)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:72)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:316)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:406)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:242)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:404)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:357)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDown$3(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$mapChildren$1(TreeNode.scala:406)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:242)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:404)
  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:357)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:321)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown(AnalysisHelper.scala:171)
  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDown$(AnalysisHelper.scala:169)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDown(LogicalPlan.scala:29)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:305)
  at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$.apply(Optimizer.scala:695)
  at org.apache.spark.sql.catalyst.optimizer.ColumnPruning$.apply(Optimizer.scala:693)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:215)
  at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)
  at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)
  at scala.collection.immutable.List.foldLeft(List.scala:89)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:212)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:204)
  at scala.collection.immutable.List.foreach(List.scala:392)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:204)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:182)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:182)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$optimizedPlan$1(QueryExecution.scala:88)
  at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:144)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:771)
  at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:144)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan$lzycompute(QueryExecution.scala:85)
  at org.apache.spark.sql.execution.QueryExecution.optimizedPlan(QueryExecution.scala:85)
  at org.apache.spark.sql.execution.QueryExecution.assertOptimized(QueryExecution.scala:96)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan$lzycompute(QueryExecution.scala:114)
  at org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:111)
  at org.apache.spark.sql.execution.QueryExecution.$anonfun$simpleString$2(QueryExecution.scala:162)
  at org.apache.spark.sql.execution.ExplainUtils$.processPlan(ExplainUtils.scala:115)
  at org.apache.spark.sql.execution.QueryExecution.simpleString(QueryExecution.scala:162)
  at org.apache.spark.sql.execution.QueryExecution.org$apache$spark$sql$execution$QueryExecution$$explainString(QueryExecution.scala:207)
  at org.apache.spark.sql.execution.QueryExecution.explainString(QueryExecution.scala:176)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:98)
  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)
  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)
  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:771)
  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)
  at org.apache.spark.sql.Dataset.withAction(Dataset.scala:3703)
  at org.apache.spark.sql.Dataset.head(Dataset.scala:2740)
  at org.apache.spark.sql.Dataset.take(Dataset.scala:2947)
  at org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)
  at org.apache.spark.sql.Dataset.showString(Dataset.scala:340)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:827)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:786)
  at org.apache.spark.sql.Dataset.show(Dataset.scala:795)
  ... 47 elided
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This behaviour was introduced in &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26812&quot; title=&quot;PushProjectionThroughUnion nullability issue&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26812&quot;&gt;&lt;del&gt;SPARK-26812&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13399539">SPARK-36673</key>
            <summary>Incorrect Unions of struct with mismatched field name case</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="viirya">L. C. Hsieh</assignee>
                                    <reporter username="shardulm">Shardul Mahadik</reporter>
                        <labels>
                    </labels>
                <created>Mon, 6 Sep 2021 08:47:30 +0000</created>
                <updated>Fri, 17 Sep 2021 16:08:59 +0000</updated>
                            <resolved>Fri, 17 Sep 2021 13:38:09 +0000</resolved>
                                    <version>3.1.1</version>
                    <version>3.2.0</version>
                                    <fixVersion>3.2.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="17410477" author="shardulm" created="Mon, 6 Sep 2021 08:53:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mgaido&quot; class=&quot;user-hover&quot; rel=&quot;mgaido&quot;&gt;mgaido&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cloud_fan&quot; class=&quot;user-hover&quot; rel=&quot;cloud_fan&quot;&gt;cloud_fan&lt;/a&gt; Since you guys were involved in the original PR for &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-26812&quot; title=&quot;PushProjectionThroughUnion nullability issue&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SPARK-26812&quot;&gt;&lt;del&gt;SPARK-26812&lt;/del&gt;&lt;/a&gt;, do you have thoughts on what the right behavior is here?&lt;/p&gt;</comment>
                            <comment id="17410488" author="mgaido" created="Mon, 6 Sep 2021 09:14:34 +0000"  >&lt;p&gt;AFAIK, in SQL the names in the struct are case sensitive, while the name of the normal fields are not. I am not sure about the right behavior here, but maybe I would expect an error at analysis time. Definitely, the current behavior is not correct.&lt;/p&gt;</comment>
                            <comment id="17412841" author="xkrogen" created="Thu, 9 Sep 2021 20:55:49 +0000"  >&lt;p&gt;From the Scaladoc for &lt;tt&gt;union&lt;/tt&gt;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
   * Also as standard in SQL, &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; function resolves columns by position (not by name):
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;So it seems to me we should be ignoring the names altogether and just doing positional matching.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;AFAIK, in SQL the names in the struct are case sensitive, while the name of the normal fields are not.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I was under the impression that nested fields are also case-insensitive. Do we have any documentation around this?&lt;/p&gt;

&lt;p&gt;In any case, when we fix this, we need to be careful about &lt;tt&gt;unionByName&lt;/tt&gt;, including the new-ish &lt;tt&gt;allowMissingColumns&lt;/tt&gt; option...&lt;/p&gt;</comment>
                            <comment id="17415864" author="cloud_fan" created="Thu, 16 Sep 2021 04:28:21 +0000"  >&lt;p&gt;This seems like a bug. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=viirya&quot; class=&quot;user-hover&quot; rel=&quot;viirya&quot;&gt;viirya&lt;/a&gt;&#160;what do you think?&lt;/p&gt;</comment>
                            <comment id="17415921" author="viirya" created="Thu, 16 Sep 2021 06:53:03 +0000"  >&lt;p&gt;The schema after union looks incorrect. By the definition of `union`, there should be only one column in the nested struct.&lt;/p&gt;</comment>
                            <comment id="17416311" author="apachespark" created="Thu, 16 Sep 2021 19:47:02 +0000"  >&lt;p&gt;User &apos;viirya&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/34025&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/34025&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17416693" author="cloud_fan" created="Fri, 17 Sep 2021 13:38:09 +0000"  >&lt;p&gt;Issue resolved by pull request 34025&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/34025&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/34025&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17416782" author="apachespark" created="Fri, 17 Sep 2021 16:08:59 +0000"  >&lt;p&gt;User &apos;viirya&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/34032&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/34032&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13213441">SPARK-26812</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 8 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0un5c:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>