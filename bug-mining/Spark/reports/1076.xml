<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:20:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4196] Streaming + checkpointing + saveAsNewAPIHadoopFiles = NotSerializableException for Hadoop Configuration</title>
                <link>https://issues.apache.org/jira/browse/SPARK-4196</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;I am reasonably sure there is some issue here in Streaming and that I&apos;m not missing something basic, but not 100%. I went ahead and posted it as a JIRA to track, since it&apos;s come up a few times before without resolution, and right now I can&apos;t get checkpointing to work at all.&lt;/p&gt;

&lt;p&gt;When Spark Streaming checkpointing is enabled, I see a NotSerializableException thrown for a Hadoop Configuration object, and it seems like it is not one from my user code.&lt;/p&gt;

&lt;p&gt;Before I post my particular instance see &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/spark-user/201408.mbox/%3C1408135046777-12202.post@n3.nabble.com%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mail-archives.apache.org/mod_mbox/spark-user/201408.mbox/%3C1408135046777-12202.post@n3.nabble.com%3E&lt;/a&gt; for another occurrence.&lt;/p&gt;

&lt;p&gt;I was also on customer site last week debugging an identical issue with checkpointing in a Scala-based program and they also could not enable checkpointing without hitting exactly this error.&lt;/p&gt;

&lt;p&gt;The essence of my code is:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; JavaSparkContext sparkContext = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; JavaSparkContext(sparkConf);

    JavaStreamingContextFactory streamingContextFactory = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt;
JavaStreamingContextFactory() {
      @Override
      &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; JavaStreamingContext create() {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; JavaStreamingContext(sparkContext, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt;
Duration(batchDurationMS));
      }
    };

      streamingContext = JavaStreamingContext.getOrCreate(
          checkpointDirString, sparkContext.hadoopConfiguration(),
streamingContextFactory, &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;);
      streamingContext.checkpoint(checkpointDirString);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It yields:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;2014-10-31 14:29:00,211 ERROR OneForOneStrategy:66
org.apache.hadoop.conf.Configuration
- field (class &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.streaming.dstream.PairDStreamFunctions$$anonfun$9&quot;&lt;/span&gt;,
name: &lt;span class=&quot;code-quote&quot;&gt;&quot;conf$2&quot;&lt;/span&gt;, type: &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.hadoop.conf.Configuration&quot;&lt;/span&gt;)
- object (class
&lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.streaming.dstream.PairDStreamFunctions$$anonfun$9&quot;&lt;/span&gt;,
&amp;lt;function2&amp;gt;)
- field (class &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.streaming.dstream.ForEachDStream&quot;&lt;/span&gt;,
name: &lt;span class=&quot;code-quote&quot;&gt;&quot;org$apache$spark$streaming$dstream$ForEachDStream$$foreachFunc&quot;&lt;/span&gt;,
type: &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;interface&lt;/span&gt; scala.Function2&quot;&lt;/span&gt;)
- object (class &lt;span class=&quot;code-quote&quot;&gt;&quot;org.apache.spark.streaming.dstream.ForEachDStream&quot;&lt;/span&gt;,
org.apache.spark.streaming.dstream.ForEachDStream@cb8016a)
...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;This looks like it&apos;s due to PairRDDFunctions, as this saveFunc seems&lt;br/&gt;
to be  org.apache.spark.streaming.dstream.PairDStreamFunctions$$anonfun$9&lt;br/&gt;
:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;def saveAsNewAPIHadoopFiles(
    prefix: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;,
    suffix: &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;,
    keyClass: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;[_],
    valueClass: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;[_],
    outputFormatClass: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;[_ &amp;lt;: NewOutputFormat[_, _]],
    conf: Configuration = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Configuration
  ) {
  val saveFunc = (rdd: RDD[(K, V)], time: Time) =&amp;gt; {
    val file = rddToFileName(prefix, suffix, time)
    rdd.saveAsNewAPIHadoopFile(file, keyClass, valueClass,
outputFormatClass, conf)
  }
  self.foreachRDD(saveFunc)
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is that not a problem? but then I don&apos;t know how it would ever work in Spark. But then again I don&apos;t see why this is an issue and only when checkpointing is enabled.&lt;/p&gt;

&lt;p&gt;Long-shot, but I wonder if it is related to closure issues like &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-1866&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-1866&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12752311">SPARK-4196</key>
            <summary>Streaming + checkpointing + saveAsNewAPIHadoopFiles = NotSerializableException for Hadoop Configuration</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tdas">Tathagata Das</assignee>
                                    <reporter username="srowen">Sean R. Owen</reporter>
                        <labels>
                    </labels>
                <created>Sun, 2 Nov 2014 17:50:42 +0000</created>
                <updated>Wed, 26 Nov 2014 13:52:52 +0000</updated>
                            <resolved>Tue, 25 Nov 2014 22:44:10 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>1.1.2</fixVersion>
                    <fixVersion>1.2.0</fixVersion>
                                    <component>DStreams</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="14196641" author="cody@koeninger.org" created="Tue, 4 Nov 2014 19:22:57 +0000"  >&lt;p&gt;Have you tried replacing&lt;/p&gt;

&lt;p&gt;JavaStreamingContext.getOrCreate(checkpointDirString, sparkContext.hadoopConfiguration(),&lt;/p&gt;

&lt;p&gt;with&lt;/p&gt;

&lt;p&gt;JavaStreamingContext.getOrCreate(checkpointDirString, org.apache.spark.deploy.SparkHadoopUtil.get.conf,&lt;/p&gt;
</comment>
                            <comment id="14196762" author="srowen" created="Tue, 4 Nov 2014 20:33:00 +0000"  >&lt;p&gt;Same problem I&apos;m afraid. The serialization error suggests it&apos;s the fact that the configuration &amp;#8211; whatever its source in the caller &amp;#8211; is serialized in a call to foreachRDD in saveAsNewAPIHadoopFiles.&lt;/p&gt;</comment>
                            <comment id="14206380" author="srowen" created="Tue, 11 Nov 2014 12:54:14 +0000"  >&lt;p&gt;More info. The problem is that &lt;tt&gt;CheckpointWriter&lt;/tt&gt; serializes the &lt;tt&gt;DStreamGraph&lt;/tt&gt; when checkpointing is enabled. In the case of, for example, &lt;tt&gt;saveAsNewAPIHadoopFiles&lt;/tt&gt;, this includes a &lt;tt&gt;ForEachDStream&lt;/tt&gt; with a reference to a Hadoop &lt;tt&gt;Configuration&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;This isn&apos;t a problem without checkpointing because Spark is not going to need to serialize this &lt;tt&gt;ForEachDStream&lt;/tt&gt; closure to execute it in general. But it does to checkpoint it.&lt;/p&gt;

&lt;p&gt;Does that make sense? I&apos;m not sure what to do but this is presenting a significant problem to me as I can&apos;t see a sly workaround to make streaming, with saving Hadoop files, with checkpointing, to work.&lt;/p&gt;


&lt;p&gt;Here&apos;s a cobbled-together test that shows the problem:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  test(&lt;span class=&quot;code-quote&quot;&gt;&quot;recovery with save to HDFS stream&quot;&lt;/span&gt;) {
    &lt;span class=&quot;code-comment&quot;&gt;// Set up the streaming context and input streams
&lt;/span&gt;    val testDir = Utils.createTempDir()
    val outDir = Utils.createTempDir()
    &lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt; ssc = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; StreamingContext(master, framework, Seconds(1))
    ssc.checkpoint(checkpointDir)
    val fileStream = ssc.textFileStream(testDir.toString)
    &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (i &amp;lt;- Seq(1, 2, 3)) {
      Files.write(i + &lt;span class=&quot;code-quote&quot;&gt;&quot;\n&quot;&lt;/span&gt;, &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; File(testDir, i.toString), Charset.forName(&lt;span class=&quot;code-quote&quot;&gt;&quot;UTF-8&quot;&lt;/span&gt;))
      &lt;span class=&quot;code-comment&quot;&gt;// wait to make sure that the file is written such that it gets shown in the file listings
&lt;/span&gt;    }

    val reducedStream = fileStream.map(x =&amp;gt; (x, x)).saveAsNewAPIHadoopFiles(
      outDir.toURI.toString,
      &lt;span class=&quot;code-quote&quot;&gt;&quot;saveAsNewAPIHadoopFilesTest&quot;&lt;/span&gt;,
      classOf[Text],
      classOf[Text],
      classOf[TextOutputFormat[Text,Text]],
      ssc.sparkContext.hadoopConfiguration)

    ssc.start()
    ssc.awaitTermination(5000)
    ssc.stop()

    val checkpointDirFile = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; File(checkpointDir)
    &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;(outDir.listFiles().length &amp;gt; 0)
    &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;(checkpointDirFile.listFiles().length == 1)
    &lt;span class=&quot;code-keyword&quot;&gt;assert&lt;/span&gt;(checkpointDirFile.listFiles()(0).listFiles().length &amp;gt; 0)

    Utils.deleteRecursively(testDir)
    Utils.deleteRecursively(outDir)
  }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You&apos;ll see the &lt;tt&gt;NotSerializableException&lt;/tt&gt; clearly if you hack &lt;tt&gt;Checkpoint.write()&lt;/tt&gt;:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;  def write(checkpoint: Checkpoint) {
    val bos = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ByteArrayOutputStream()
    val zos = compressionCodec.compressedOutputStream(bos)
    val oos = &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; ObjectOutputStream(zos)
    &lt;span class=&quot;code-keyword&quot;&gt;try&lt;/span&gt; {
      oos.writeObject(checkpoint)
    } &lt;span class=&quot;code-keyword&quot;&gt;catch&lt;/span&gt; {
      &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt; e: Exception =&amp;gt;
        e.printStackTrace()
        &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; e
    }
    ...
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14224582" author="tdas" created="Tue, 25 Nov 2014 14:17:41 +0000"  >&lt;p&gt;Let me try to take a pass on this. &lt;/p&gt;</comment>
                            <comment id="14224637" author="tdas" created="Tue, 25 Nov 2014 14:56:02 +0000"  >&lt;p&gt;As for now there is a workaround. You could directly use &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;dstream.foreachRDD { rdd =&amp;gt; 
    
    rdd.saveAsNewAPIHadoopFile(...) 
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And in that you will have more control over the configuration object. You can wrap the Writable configuration object using SerializableWritable (&lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SerializableWritable.scala&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/SerializableWritable.scala&lt;/a&gt;). &lt;/p&gt;</comment>
                            <comment id="14224649" author="srowen" created="Tue, 25 Nov 2014 15:07:42 +0000"  >&lt;p&gt;That didn&apos;t work for me, IIRC. The problem is that the checkpointing process still serializes the whole graph, and this results in the same graph. saveAsNewAPIHadoopFile creates an anonymous function that always has a reference to a HadoopConfiguration.&lt;/p&gt;

&lt;p&gt;I think I can dig deeper and copy-and-paste the saveAsNewAPIHadoopFile implementation and eventually make it serialize the Configuration with SerializableWritable, sure. If that&apos;s the answer, cool, but it should go back upstream into Spark too I suppose?&lt;/p&gt;</comment>
                            <comment id="14224651" author="apachespark" created="Tue, 25 Nov 2014 15:09:26 +0000"  >&lt;p&gt;User &apos;tdas&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/3457&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/3457&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14226192" author="srowen" created="Wed, 26 Nov 2014 13:52:52 +0000"  >&lt;p&gt;Looks good. At least, this commit got me past this particular issue. Thanks!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 51 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i21vin:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>