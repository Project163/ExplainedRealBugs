<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:26:19 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-4783] System.exit() calls in SparkContext disrupt applications embedding Spark</title>
                <link>https://issues.apache.org/jira/browse/SPARK-4783</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;A common architectural choice for integrating Spark within a larger application is to employ a gateway to handle Spark jobs. The gateway is a server which contains one or more long-running sparkcontexts.&lt;/p&gt;

&lt;p&gt;A typical server is created with the following pseudo code:&lt;/p&gt;

&lt;p&gt;var continue = true&lt;br/&gt;
while (continue){&lt;br/&gt;
 try &lt;/p&gt;
{
    server.run() 
  }
&lt;p&gt; catch (e) {&lt;br/&gt;
  continue = log_and_examine_error(e)&lt;br/&gt;
}&lt;/p&gt;

&lt;p&gt;The problem is that sparkcontext frequently calls System.exit when it encounters a problem which means the server can only be re-spawned at the process level, which is much more messy than the simple code above.&lt;/p&gt;

&lt;p&gt;Therefore, I believe it makes sense to replace all System.exit calls in sparkcontext with the throwing of a fatal error. &lt;/p&gt;</description>
                <environment></environment>
        <key id="12760021">SPARK-4783</key>
            <summary>System.exit() calls in SparkContext disrupt applications embedding Spark</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="srowen">Sean R. Owen</assignee>
                                    <reporter username="hymanroth">David Semeria</reporter>
                        <labels>
                    </labels>
                <created>Sun, 7 Dec 2014 12:25:33 +0000</created>
                <updated>Thu, 2 Jun 2016 11:27:59 +0000</updated>
                            <resolved>Thu, 16 Apr 2015 09:45:48 +0000</resolved>
                                                    <fixVersion>1.4.0</fixVersion>
                                    <component>Spark Core</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="14237341" author="pwendell" created="Mon, 8 Dec 2014 01:10:14 +0000"  >&lt;p&gt;For code cleanliness, we should go through and look everywhere we call System.exit() and see which ones can be converted safely to exceptions.  Most of our use of System.exit is on the executor side, but there may be a few on the driver/SparkContext side.&lt;/p&gt;

&lt;p&gt;That said, if there is a fatal exception in the SparkContext, I don&apos;t think your app can safely just catch the exception, log it, and create a new SparkContext. Is that what you are trying to do? In that case there could be static state around that is not properly cleaned up and will cause the new context to be buggy.&lt;/p&gt;
</comment>
                            <comment id="14237718" author="hymanroth" created="Mon, 8 Dec 2014 10:13:19 +0000"  >&lt;p&gt;The key idea with this proposal is that the server at least gets a chance to log the error (remotely) and decide what to do next. If the server also provides other services beyond Spark, these can continue to be available even if the Spark part is down and cannot be safely restarted.&lt;/p&gt;

&lt;p&gt;As regards a possibly buggy static state, yes that would be a problem.&lt;/p&gt;

&lt;p&gt;As an interim solution, I suggest SparkContext would throw an exception with extends either &quot;SparkRecoverable&quot; or &quot;SparkUnrecoverable&quot; (for example).&lt;/p&gt;

&lt;p&gt;In the longer term, I can&apos;t think of a reason why SparkContext could not clean and recreate the static state upon each invocation.&lt;/p&gt;</comment>
                            <comment id="14483625" author="michalklos" created="Tue, 7 Apr 2015 17:57:32 +0000"  >&lt;p&gt;We are running into this exact issue. We have a driver application that has other responsibilities beyond submitting spark work and we don&apos;t want it to die if there is an issue with the cluster. The cluster can be recovered or a new one can be spun up with the same DNS, and we can start fresh with a new context. But, in the meantime, we want the driver app to continue running and carry on its other business. &lt;/p&gt;

&lt;p&gt;Specifically for us, we are having issues with this exit:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L409-L411&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/TaskSchedulerImpl.scala#L409-L411&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We are considering patching it, but not sure whether or not this will cause other problems or if there was a good reason it hasn&apos;t been removed yet.&lt;/p&gt;</comment>
                            <comment id="14489036" author="ardlema" created="Fri, 10 Apr 2015 06:37:22 +0000"  >&lt;p&gt;We are having exactly the same issue. In fact yesterday I opened a new jira ticket to report this (I wasn&apos;t aware of this one): &lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-6804&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/SPARK-6804&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In our case we are building a web application that is using Spark under the hood and we are also having issues with the same exit invocation at the TaskSchedulerImpl class. As that call is killing the JVM our web app is also killed.&lt;/p&gt;

&lt;p&gt;I&apos;m totally agree with the throwing exception approach proposal.&lt;/p&gt;</comment>
                            <comment id="14489202" author="stevel@apache.org" created="Fri, 10 Apr 2015 08:58:30 +0000"  >&lt;p&gt;There&apos;s a class &lt;tt&gt;org.apache.hadoop.util.ExitUtil&lt;/tt&gt; that wraps all of Hadoop&apos;s inner exit codes; lets you optionall download the feature. It&apos;s tagged as hadoop private, but that is fixable; &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9626&quot; title=&quot;Add an interface for any exception to serve up an Exit code&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9626&quot;&gt;&lt;del&gt;HADOOP-9626&lt;/del&gt;&lt;/a&gt; would be the place to do it. It was primarily written for testing, but with that extra patch, would allow exceptions to serve up the exit code.&lt;/p&gt;</comment>
                            <comment id="14489495" author="srowen" created="Fri, 10 Apr 2015 12:01:32 +0000"  >&lt;p&gt;How about narrowly addressing what appears to be the pain point here, which is &lt;tt&gt;TaskSchedulerImpl&lt;/tt&gt;? I don&apos;t immediately see why that has to exit instantly. Scanning the other occurrences they look mostly &quot;OK&quot; in that they are in CLI programs, examples, tests, daemon processes, rather than something that would be embedded. Go for a PR.&lt;/p&gt;</comment>
                            <comment id="14491993" author="ardlema" created="Mon, 13 Apr 2015 07:03:45 +0000"  >&lt;p&gt;Does it mean that you guys are going to create a PR with a fix/change proposal for this? Or just asking someone to create that PR? If so I am willing to create it.&lt;/p&gt;</comment>
                            <comment id="14492346" author="srowen" created="Mon, 13 Apr 2015 12:38:05 +0000"  >&lt;p&gt;I have a PR ready, but am testing it. I am seeing test failures but am not sure if they&apos;re related. You are also welcome to go ahead with a PR if you think you have a handle on it and I can chime in with what I know.&lt;/p&gt;</comment>
                            <comment id="14492393" author="apachespark" created="Mon, 13 Apr 2015 13:31:53 +0000"  >&lt;p&gt;User &apos;srowen&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/5492&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/5492&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14497843" author="srowen" created="Thu, 16 Apr 2015 09:45:48 +0000"  >&lt;p&gt;Issue resolved by pull request 5492&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/5492&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/5492&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14659864" author="ardlema" created="Thu, 6 Aug 2015 11:37:02 +0000"  >&lt;p&gt;Still having this issue. We&apos;ve found out that the exception throw by TaskSchedulerImpl is being caught by SparkUncaughtException which is calling System.exit() again.&lt;/p&gt;

&lt;p&gt;Would it make sense just logging the error and not throwing the exception? See &lt;a href=&quot;https://github.com/apache/spark/pull/7993&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/7993&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12974411">SPARK-15685</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12819728">SPARK-6804</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12852548">SPARK-9687</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 15 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i235o7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>