<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:29:53 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-8646] PySpark does not run on YARN if master not provided in command line</title>
                <link>https://issues.apache.org/jira/browse/SPARK-8646</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;Running pyspark jobs result in a &quot;no module named pyspark&quot; when run in yarn-client mode in spark 1.4.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/SPARK-6869&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;I believe this JIRA represents the change that introduced this error.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This does not represent a binary compatible change to spark. Scripts that worked on previous spark versions (ie comands the use spark-submit) should continue to work without modification between minor versions.&lt;/p&gt;</description>
                <environment>&lt;p&gt;SPARK_HOME=local/path/to/spark1.4install/dir&lt;/p&gt;

&lt;p&gt;also with&lt;br/&gt;
SPARK_HOME=local/path/to/spark1.4install/dir&lt;br/&gt;
PYTHONPATH=$SPARK_HOME/python/lib&lt;/p&gt;

&lt;p&gt;Spark apps are submitted with the command:&lt;br/&gt;
$SPARK_HOME/bin/spark-submit outofstock/data_transform.py hdfs://foe-dev/DEMO_DATA/FACT_POS hdfs:/user/juliet/ex/ yarn-client&lt;/p&gt;

&lt;p&gt;data_transform contains a main method, and the rest of the args are parsed in my own code.&lt;/p&gt;
</environment>
        <key id="12840659">SPARK-8646</key>
            <summary>PySpark does not run on YARN if master not provided in command line</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lianhuiwang">Lianhui Wang</assignee>
                                    <reporter username="juliet">Juliet Hougland</reporter>
                        <labels>
                    </labels>
                <created>Thu, 25 Jun 2015 21:40:42 +0000</created>
                <updated>Fri, 31 Jul 2015 10:46:47 +0000</updated>
                            <resolved>Fri, 17 Jul 2015 02:33:00 +0000</resolved>
                                    <version>1.4.0</version>
                                    <fixVersion>1.5.0</fixVersion>
                                    <component>PySpark</component>
                    <component>YARN</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14602007" author="juliet" created="Thu, 25 Jun 2015 21:41:41 +0000"  >&lt;p&gt;The logs from failures when only SPARK_HOME is set and when PYTHONPATH is also set.&lt;/p&gt;</comment>
                            <comment id="14602585" author="srowen" created="Fri, 26 Jun 2015 08:55:31 +0000"  >&lt;p&gt;You&apos;re saying it doesn&apos;t work at all on YARN? I&apos;d hope there are some unit tests for this but I am not sure if it covers this case. Do we know more about the likely issue here &amp;#8211; something isn&apos;t packaging pyspark, or not unpacking it? CC &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lianhuiwang&quot; class=&quot;user-hover&quot; rel=&quot;lianhuiwang&quot;&gt;lianhuiwang&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14603228" author="vanzin" created="Fri, 26 Jun 2015 17:18:31 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=j_houg&quot; class=&quot;user-hover&quot; rel=&quot;j_houg&quot;&gt;j_houg&lt;/a&gt;,&lt;/p&gt;

&lt;p&gt;Seems there&apos;s something weird going on in your setup. I downloaded the 1.4 hadoop 2.6 archive you&apos;re using, and ran this command line, without setting any extra env variables:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;HADOOP_CONF_DIR=/etc/hadoop/conf ./bin/spark-submit --master yarn-client examples/src/main/python/pi.py
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And it works. Notably, I see these two lines that seem to be missing from your logs:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;15/06/26 10:14:28 INFO yarn.Client: Uploading resource file:/tmp/spark-1.4.0-bin-hadoop2.6/python/lib/pyspark.zip -&amp;gt; hdfs://vanzin-st1-1.vpc.cloudera.com:8020/user/systest/.sparkStaging/application_1435333340717_0002/pyspark.zip
15/06/26 10:14:28 INFO yarn.Client: Uploading resource file:/tmp/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip -&amp;gt; hdfs://vanzin-st1-1.vpc.cloudera.com:8020/user/systest/.sparkStaging/application_1435333340717_0002/py4j-0.8.2.1-src.zip
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That&apos;s the code added in the change you mention; it&apos;s actually what allows pyspark to run with that large assembly (which python cannot read).&lt;/p&gt;

&lt;p&gt;Can you double check the command line you&apos;re running (or try the simple example above)? Also, make sure your &lt;tt&gt;$SPARK_HOME/conf&lt;/tt&gt; directory is not pointing at some other Spark configuration, or that you don&apos;t have any other env variables that may be affecting Spark configuration.&lt;/p&gt;</comment>
                            <comment id="14603263" author="juliet" created="Fri, 26 Jun 2015 17:32:38 +0000"  >&lt;p&gt;Results from pi-test are uploaded in the attachment pi-test.log. Still a module missing error, this time it is pandas.algo.&lt;/p&gt;</comment>
                            <comment id="14603275" author="juliet" created="Fri, 26 Jun 2015 17:40:05 +0000"  >&lt;p&gt;I ran the same line that gave me errors last time with HADOOP_CONF_DIR=/etc/hadoop/conf prepended. The command I used was:&lt;/p&gt;

&lt;p&gt;HADOOP_CONF_DIR=/etc/hadoop/conf $SPARK_HOME/bin/spark-submit outofstock/data_transform.py hdfs://foe-dev/DEMO_DATA/FACT_POS hdfs:/user/juliet/ex1/ yarn-client 2&amp;gt; spark1.4-SPARK_HOME-set-inline-HADOOP_CONF_DIR.log&lt;/p&gt;

&lt;p&gt;I&apos;ve attached the output from that, it appears to be the same to me.&lt;/p&gt;</comment>
                            <comment id="14603523" author="vanzin" created="Fri, 26 Jun 2015 20:16:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;Still a module missing error, this time it is pandas.algo.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Seems like you may have pandas installed on your driver node but not on cluster nodes. Could you check that? The code that uses pandas (in sql/context.py) seems to be careful about only using it when available.&lt;/p&gt;</comment>
                            <comment id="14603973" author="lianhuiwang" created="Sat, 27 Jun 2015 04:40:58 +0000"  >&lt;p&gt;from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=juliet&quot; class=&quot;user-hover&quot; rel=&quot;juliet&quot;&gt;juliet&lt;/a&gt; &apos;s logs, i think you miss python &apos;pandas.algos&apos; module that pyspark does not provide. i think that you need to install it on nodes.&lt;/p&gt;</comment>
                            <comment id="14604014" author="juliet" created="Sat, 27 Jun 2015 06:28:09 +0000"  >&lt;p&gt;When I configure spark to use my virtualenv that is on every node of the cluster and includes pandas, the pi job works fine. This makes sense to me because in the job that I have that fails, a spark context can be created without a module import error. the part that doesn&apos;t make sense to me is why pandas.algo would be needed at all. Looking at the code for the pi job, it is not part an import that is declared in the file. This is orthogonal to the point of this ticket, but is very very strange to me.&lt;/p&gt;

&lt;p&gt;The module import error that is the core of this JIRA occurs when I need to write out results of a computation (ie calling sc.writeTextFile) which require the pyspark module to be available on the worker nodes.&lt;/p&gt;</comment>
                            <comment id="14604226" author="lianhuiwang" created="Sat, 27 Jun 2015 16:19:39 +0000"  >&lt;p&gt;now i use spark-1.5.0-SNAPSHOT to run pi.py without install pandas  and it is ok. now i find that sql/dataframe.py must need to import pandas and if you do not use sql/dataframe.py i think it do not need pandas. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=juliet&quot; class=&quot;user-hover&quot; rel=&quot;juliet&quot;&gt;juliet&lt;/a&gt; can you provide executor&apos;s logs that can be got more details?&lt;/p&gt;</comment>
                            <comment id="14614673" author="srowen" created="Mon, 6 Jul 2015 07:54:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=j_houg&quot; class=&quot;user-hover&quot; rel=&quot;j_houg&quot;&gt;j_houg&lt;/a&gt; is the resolution here just that pandas has to be installed if pandas is used?&lt;/p&gt;</comment>
                            <comment id="14614856" author="juliet" created="Mon, 6 Jul 2015 10:49:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sowen&quot; class=&quot;user-hover&quot; rel=&quot;sowen&quot;&gt;sowen&lt;/a&gt; The pandas error came when I tried to run the pi job-- which doesn&apos;t import pandas at all. The only imports in $SPARK_1.4_HOME/examples/src/main/python/pi.py are as follows:&lt;/p&gt;

&lt;p&gt;    import sys&lt;br/&gt;
    from random import random&lt;br/&gt;
    from operator import add&lt;br/&gt;
    from pyspark import SparkContext&lt;/p&gt;


&lt;p&gt; PySpark itself doesn&apos;t require pandas (if it does, that should be documented) so having the pi job (doesn&apos;t require pandas) fail with a pandas not found error is wrong, because at no point should the pi job or pyspark itself require pandas. The pandas error is very, very weird but not obviously directly related to this ticket. The problem I reported here has to do with pyspark itself not being shipped or perhaps available to the worker nodes when I run a pyspark app from spark 1.4 using YARN.&lt;/p&gt;</comment>
                            <comment id="14614885" author="srowen" created="Mon, 6 Jul 2015 11:11:53 +0000"  >&lt;p&gt;Right, none of this uses pandas directly. As &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt; says the code appears to be careful about only calling &quot;import pandas&quot; when needed &lt;tt&gt;toPandas()&lt;/tt&gt; or catching for the error when it&apos;s not available. My guess is that &lt;tt&gt;has_pandas&lt;/tt&gt; is true on the driver but then that causes it to do things that the executors can&apos;t honor since they don&apos;t have pandas.&lt;/p&gt;

&lt;p&gt;It does sound like a docs issue. Some Pyspark operations need pandas and you need a uniform Python installation across driver and executor &amp;#8211; either both have it or both don&apos;t. I suppose that&apos;s always good practice, but not obvious, that it could manifest like this.&lt;/p&gt;

&lt;p&gt;How about adding some docs?&lt;/p&gt;

&lt;p&gt;Or &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=davies&quot; class=&quot;user-hover&quot; rel=&quot;davies&quot;&gt;davies&lt;/a&gt; et al is there a better way to guard this? rather than check once whether pandas can be imported, check at &quot;runtime&quot; in the createDataFrame method? kind of like &lt;tt&gt;toPandas&lt;/tt&gt; does? &lt;/p&gt;</comment>
                            <comment id="14615293" author="davies" created="Mon, 6 Jul 2015 16:47:42 +0000"  >&lt;p&gt;To be clear, PySpark does NOT depends on pandas. In dataframe.py, it works with pandas dataframe only when you have it.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=juliet&quot; class=&quot;user-hover&quot; rel=&quot;juliet&quot;&gt;juliet&lt;/a&gt; example/pi.py should run fine in YARN (it does not need panda at all). Is it possible that `outofstock/data_transform.py` depends on `pandas.algos` (pandas.algos is used by a closure from driver), and you upload the wrong log file?&lt;/p&gt;</comment>
                            <comment id="14615809" author="juliet" created="Mon, 6 Jul 2015 22:50:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=davies&quot; class=&quot;user-hover&quot; rel=&quot;davies&quot;&gt;davies&lt;/a&gt; Please look at the logs I have attached. The pandas.algo import error only appears in the pi-test.log file. I ran pi-test as a method to help debug this problem at the request of &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt;. If you look at three other log files (with env diferences in the file names) those are from running my out-of-stock job. That job does have quite a few dependencies but I make sure those are available to the driver and workers. &lt;/p&gt;

&lt;p&gt;The real (first) issue that this ticket is related to is that pyspark isn&apos;t available on worker nodes. The same command I can use to run my job on spark 1.3 does not work with spark 1.4.&lt;/p&gt;</comment>
                            <comment id="14620466" author="lianhuiwang" created="Thu, 9 Jul 2015 13:04:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=j_houg&quot; class=&quot;user-hover&quot; rel=&quot;j_houg&quot;&gt;j_houg&lt;/a&gt; can you add --verbose to spark-submit command? and look at what is your spark.submit.pyArchives.&lt;br/&gt;
because from you logs, i find that it do not upload pyArchive files, like:pyspark.zip and py4j-0.8.2.1-src.zip. and you can check whether in SPARK_HOME/python/lib path it has these two zips.&lt;/p&gt;</comment>
                            <comment id="14623008" author="juliet" created="Fri, 10 Jul 2015 22:40:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lianhuiwang&quot; class=&quot;user-hover&quot; rel=&quot;lianhuiwang&quot;&gt;lianhuiwang&lt;/a&gt; I just uploaded the log files from using the verbose flag. I think I may have important clues as to where the problem lies. Instead of using &apos;--master yarn-client&apos; as part of my spark-submit command, I parse my own cli arg in my main class to get the spark master and initialize a configuration with it. If I add --master yarn-client in addition to my normal master specification, the job runs fine.&lt;/p&gt;

&lt;p&gt;The following command works in Spark 1.3 but not in 1.4:&lt;br/&gt;
    $SPARK_HOME/bin/spark-submit --verbose outofstock/data_transform.py \&lt;br/&gt;
    hdfs://foe-dev/DEMO_DATA/FACT_POS     hdfs:/user/juliet/ex4/ yarn-client&lt;/p&gt;

&lt;p&gt;If I add the --master yarn-client parameter to the command it works. Specifically:&lt;br/&gt;
    $SPARK_HOME/bin/spark-submit --verbose --master yarn-client outofstock/data_transform.py \&lt;br/&gt;
    hdfs://foe-dev/DEMO_DATA/FACT_POS     hdfs:/user/juliet/ex4/ yarn-client&lt;/p&gt;</comment>
                            <comment id="14624521" author="lianhuiwang" created="Mon, 13 Jul 2015 11:01:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=juliet&quot; class=&quot;user-hover&quot; rel=&quot;juliet&quot;&gt;juliet&lt;/a&gt; from your spark1.4-verbose.log, i find that master= local&lt;span class=&quot;error&quot;&gt;&amp;#91;*&amp;#93;&lt;/span&gt;. so maybe in spark-defaults.conf, you config spark.master=local? other situation is in your data_transform.py, maybe you use sparkConf.set(&quot;spark.master&quot;,&quot;local&quot;). Can you check whether these situations have been happened?&lt;/p&gt;</comment>
                            <comment id="14625087" author="vanzin" created="Mon, 13 Jul 2015 18:16:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=j_houg&quot; class=&quot;user-hover&quot; rel=&quot;j_houg&quot;&gt;j_houg&lt;/a&gt; could you also run the command with the SPARK_PRINT_LAUNCH_COMMAND=1 env variable set, and post the command logged to stderr?&lt;/p&gt;</comment>
                            <comment id="14625245" author="juliet" created="Mon, 13 Jul 2015 20:02:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lianhuiwang&quot; class=&quot;user-hover&quot; rel=&quot;lianhuiwang&quot;&gt;lianhuiwang&lt;/a&gt; in $SPARK_HOME/conf I only have the spark-defaults.conf.template file, not a non-template version. I also do not set the spark master to local programmatically.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vanzin&quot; class=&quot;user-hover&quot; rel=&quot;vanzin&quot;&gt;vanzin&lt;/a&gt; The command logged to stderr is:&lt;/p&gt;

&lt;p&gt;Spark Command: /usr/lib/jvm/java-1.7.0-openjdk-1.7.0.65.x86_64/jre/bin/java -cp /home/juliet/bin/spark-1.4.0-bin-hadoop2.6/conf/:/home/juliet/bin/spark-1.4.0-bin-hadoop2.6/lib/spark-assembly-1.4.0-hadoop2.6.0.jar:/home/juliet/bin/spark-1.4.0-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar:/home/juliet/bin/spark-1.4.0-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar:/home/juliet/bin/spark-1.4.0-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar:/etc/hadoop/conf/ -Xms512m -Xmx512m -XX:&lt;br/&gt;
MaxPermSize=128m org.apache.spark.deploy.SparkSubmit --verbose outofstock/data_transform.py&lt;br/&gt;
hdfs://foe-dev/DEMO_DATA/FACT_POS hdfs:/user/juliet/ex7/ yarn-client&lt;/p&gt;

&lt;p&gt;(sorry for the way the classpath gets chopped up between lines.) yarn-client is getting passed as a argument to my code, but because I am not specifying the master via the cli --master flag or via spark-defaults.conf it does not affect how the job initially starts up.&lt;/p&gt;</comment>
                            <comment id="14625725" author="lianhuiwang" created="Tue, 14 Jul 2015 02:45:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=juliet&quot; class=&quot;user-hover&quot; rel=&quot;juliet&quot;&gt;juliet&lt;/a&gt; can you provide your spark-submit command? &lt;br/&gt;
i think the correct command in spark 1.4 is $SPARK_HOME/bin/spark-submit --master yarn-client outofstock/data_transform.py hdfs://foe-dev/DEMO_DATA/FACT_POS hdfs:/user/juliet/ex4/&lt;br/&gt;
is it the same as your command?&lt;/p&gt;</comment>
                            <comment id="14628822" author="vanzin" created="Wed, 15 Jul 2015 21:55:54 +0000"  >&lt;p&gt;Hmmm, the command output looks fine, so it seems this was not a regression caused by the launcher library. But let me try it locally and see what I get.&lt;/p&gt;</comment>
                            <comment id="14628927" author="vanzin" created="Wed, 15 Jul 2015 23:25:18 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=j_houg&quot; class=&quot;user-hover&quot; rel=&quot;j_houg&quot;&gt;j_houg&lt;/a&gt;, could you share the exact code you&apos;re using to instantiate the context?&lt;/p&gt;

&lt;p&gt;Here&apos;s a script I wrote:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; sys
from pyspark &lt;span class=&quot;code-keyword&quot;&gt;import&lt;/span&gt; SparkContext
SparkContext(master=sys.argv[1]).stop()
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And invoking spark-submit yields the expected results.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;./bin/spark-submit /tmp/script.py local&lt;/tt&gt; works. &lt;/p&gt;

&lt;p&gt;&lt;tt&gt;./bin/spark-submit /tmp/script.py foo&lt;/tt&gt; fails because &quot;foo&quot; is not a valid master.&lt;/p&gt;

&lt;p&gt;So everything seems to be working as expected, which makes me suspicious of your code.&lt;/p&gt;</comment>
                            <comment id="14628947" author="juliet" created="Wed, 15 Jul 2015 23:42:25 +0000"  >&lt;p&gt;The failure happens at the point that I need to write out a file on the cluster and pyspark facilities need to be available to executors, not just the driver program. I can parse args and start a spark context fine, it fails at the point that I call sc.saveAsTextFile. Relevant lines:&lt;/p&gt;

&lt;div class=&quot;panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;panelContent&quot;&gt;
&lt;p&gt;def analyze(data_io):&lt;br/&gt;
    sc = data_io.sc()&lt;br/&gt;
    sc.addPyFile(&quot;file:/home/juliet/src/out-of-stock/outofstock/GeometricModel.py&quot;)&lt;br/&gt;
    keyed_ts_rdd = to_keyed_ts(sc.textFile(data_io.input_path)).cache()&lt;br/&gt;
    keyed_days_btwn_sales = keyed_ts_rdd.mapValues(days_between_sales).cache()&lt;br/&gt;
    keyed_outliers = keyed_ts_rdd.mapValues(flag_outliers)&lt;br/&gt;
    to_csv_lines(keyed_outliers).saveAsTextFile(data_io.sales_outliers_path) # Point of failure&lt;br/&gt;
    &amp;lt;Other Stuff&amp;gt;&lt;/p&gt;

&lt;p&gt;if _&lt;em&gt;name&lt;/em&gt;_ == &quot;_&lt;em&gt;main&lt;/em&gt;_&quot;:&lt;br/&gt;
    parser = argparse.ArgumentParser(description=&apos;Analyze store-item sales history for anomolies.&apos;)&lt;br/&gt;
    parser.add_argument(&apos;input_path&apos;)&lt;br/&gt;
    parser.add_argument(&apos;output_dir&apos;)&lt;br/&gt;
    parser.add_argument(&apos;mode&apos;)&lt;br/&gt;
    args = parser.parse_args()&lt;/p&gt;

&lt;p&gt;    dataIO = DataIO(args.input_path, args.output_dir, mode=args.mode)&lt;br/&gt;
    analyze(dataIO)&lt;/p&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This runs fine on Spark 1.3, and produces reasonable results that get written to files in hdfs. I&apos;m pretty confident that my use of argparse and other logic in my code work fine. (Note eddited because of strange jira formatting)&lt;/p&gt;</comment>
                            <comment id="14628959" author="vanzin" created="Wed, 15 Jul 2015 23:55:40 +0000"  >&lt;p&gt;I think I know what&apos;s going on. Since you&apos;re not passing the &quot;--master&quot; command line argument to spark-submit, SparkSubmit does not know you&apos;ll be running the app in yarn mode, so it does not collect information about the pyspark archives to upload. So pyspark modules are not available in the cluster when you run your app.&lt;/p&gt;

&lt;p&gt;If you just add &quot;--master yarn-client&quot; to your command line, it should work, even if it is redundant. Nevertheless, it would be nice to fix this in the Spark code too.&lt;/p&gt;</comment>
                            <comment id="14628971" author="juliet" created="Thu, 16 Jul 2015 00:02:38 +0000"  >&lt;p&gt;Yea, it works fine if I add that arg. There are two reasons I think this should be fixed in Spark, despite there being a work around. First, I think API compatibility should/does include scripts. Second, if Spark provides the ability to set the master via code, it should be respected and actually work. Otherwise, the option that doesn&apos;t work (setting master via code) should not be available at all.&lt;/p&gt;</comment>
                            <comment id="14628977" author="vanzin" created="Thu, 16 Jul 2015 00:06:35 +0000"  >&lt;p&gt;Changed the title to be more descriptive of the underlying issue.&lt;/p&gt;</comment>
                            <comment id="14629407" author="apachespark" created="Thu, 16 Jul 2015 08:13:06 +0000"  >&lt;p&gt;User &apos;lianhuiwang&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/7438&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/7438&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14629409" author="lianhuiwang" created="Thu, 16 Jul 2015 08:15:14 +0000"  >&lt;p&gt;yes,  when i use this command: ./bin/spark-submit ./pi.py yarn-client 10,  yarn&apos; client do not upload pyspark.zip, so that can not be worked. i submit a PR that resolve this problem based on master branch.  there is some problems on spark-1.4.0 branch because it finds pyspark libraries in sparkSubmit, not in Client.  if this must be needed in spark-1.4.0, latter i will take a look at it.&lt;/p&gt;</comment>
                            <comment id="14649062" author="wumin810711" created="Fri, 31 Jul 2015 10:46:47 +0000"  >&lt;p&gt;Hi, I got same issue when I running the pyspark program with yarn-client mode and spark 1.4.1 from Biginsight 4.1(Ambari). Because the assembly jar no longer contains the python scripts of pyspark and py4j, so I set the spark home via SparkContext.setSparkHome() to spark-client location(because this is one Ambari hadoop, so the spark-client contains the python folder, and it includes the py4j and pyspark scripts). The API document shows this will be applied to slave nodes, I assume this can be applied for &quot;spark on yarn&quot; also, but it does not work.  The worker nodes always get the PYTHONPATH from cached assembly jar. &lt;br/&gt;
After checked the SparkContext code, seems the sparkHome will be set into SparkConf as &quot;spark.home&quot;, so I think maybe it should be distributed to all executor and pyspark can use this parameter to locate the PYTHONPATH also.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12820375">SPARK-6869</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12744822" name="executor.log" size="48006" author="juliet" created="Fri, 10 Jul 2015 21:46:28 +0000"/>
                            <attachment id="12742163" name="pi-test.log" size="24954" author="juliet" created="Fri, 26 Jun 2015 17:32:38 +0000"/>
                            <attachment id="12741954" name="spark1.4-SPARK_HOME-set-PYTHONPATH-set.log" size="23964" author="juliet" created="Thu, 25 Jun 2015 21:41:41 +0000"/>
                            <attachment id="12742165" name="spark1.4-SPARK_HOME-set-inline-HADOOP_CONF_DIR.log" size="24971" author="juliet" created="Fri, 26 Jun 2015 17:40:05 +0000"/>
                            <attachment id="12741953" name="spark1.4-SPARK_HOME-set.log" size="24824" author="juliet" created="Thu, 25 Jun 2015 21:41:41 +0000"/>
                            <attachment id="12744834" name="spark1.4-verbose.log" size="22373" author="juliet" created="Fri, 10 Jul 2015 22:26:52 +0000"/>
                            <attachment id="12744833" name="verbose-executor.log" size="35862" author="juliet" created="Fri, 10 Jul 2015 22:26:52 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 16 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2gifb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12332078">1.5.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>