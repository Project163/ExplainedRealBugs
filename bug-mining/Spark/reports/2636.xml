<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 18:34:02 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[SPARK-11103] Parquet filters push-down may cause exception when schema merging is turned on</title>
                <link>https://issues.apache.org/jira/browse/SPARK-11103</link>
                <project id="12315420" key="SPARK">Spark</project>
                    <description>&lt;p&gt;When evolving a schema in parquet files, spark properly expose all columns found in the different parquet files but when trying to query the data, it is not possible to apply a filter on a column that is not present in all files.&lt;/p&gt;

&lt;p&gt;To reproduce:&lt;br/&gt;
&lt;b&gt;SQL:&lt;/b&gt;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;create table `table1` STORED AS PARQUET LOCATION &apos;hdfs://&amp;lt;SERVER&amp;gt;:&amp;lt;PORT&amp;gt;/path/to/table/id=1/&apos; as select 1 as `col1`;
create table `table2` STORED AS PARQUET LOCATION &apos;hdfs://&amp;lt;SERVER&amp;gt;:&amp;lt;PORT&amp;gt;/path/to/table/id=2/&apos; as select 1 as `col1`, 2 as `col2`;
create table `table3` USING org.apache.spark.sql.parquet OPTIONS (path &quot;hdfs://&amp;lt;SERVER&amp;gt;:&amp;lt;PORT&amp;gt;/path/to/table&quot;);
select col1 from `table3` where col2 = 2;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The last select will output the following Stack Trace:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;An error occurred when executing the SQL command:
select col1 from `table3` where col2 = 2

[Simba][HiveJDBCDriver](500051) ERROR processing query/statement. Error Code: 0, SQL state: TStatus(statusCode:ERROR_STATUS, infoMessages:[*org.apache.hive.service.cli.HiveSQLException:org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7212.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7212.0 (TID 138449, 208.92.52.88): java.lang.IllegalArgumentException: Column [col2] was not found in schema!
	at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.getColumnDescriptor(SchemaCompatibilityValidator.java:190)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:178)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumnFilterPredicate(SchemaCompatibilityValidator.java:160)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:94)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:59)
	at org.apache.parquet.filter2.predicate.Operators$Eq.accept(Operators.java:180)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validate(SchemaCompatibilityValidator.java:64)
	at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:59)
	at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:40)
	at org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:126)
	at org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:46)
	at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:160)
	at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140)
	at org.apache.spark.rdd.SqlNewHadoopRDD$$anon$1.&amp;lt;init&amp;gt;(SqlNewHadoopRDD.scala:155)
	at org.apache.spark.rdd.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:120)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace::26:25, org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:runInternal:SparkExecuteStatementOperation.scala:259, org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation:run:SparkExecuteStatementOperation.scala:144, org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:388, org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:369, sun.reflect.GeneratedMethodAccessor134:invoke::-1, sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43, java.lang.reflect.Method:invoke:Method.java:497, org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78, org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36, org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63, java.security.AccessController:doPrivileged:AccessController.java:-2, javax.security.auth.Subject:doAs:Subject.java:422, org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1628, org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59, com.sun.proxy.$Proxy25:executeStatement::-1, org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:261, org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:486, org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1313, org.apache.hive.service.cli.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1298, org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39, org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39, org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56, org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:285, java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1142, java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:617, java.lang.Thread:run:Thread.java:745], errorCode:0, errorMessage:org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 7212.0 failed 4 times, most recent failure: Lost task 0.3 in stage 7212.0 (TID 138449, 208.92.52.88): java.lang.IllegalArgumentException: Column [col2] was not found in schema!
	at org.apache.parquet.Preconditions.checkArgument(Preconditions.java:55)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.getColumnDescriptor(SchemaCompatibilityValidator.java:190)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumn(SchemaCompatibilityValidator.java:178)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validateColumnFilterPredicate(SchemaCompatibilityValidator.java:160)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:94)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.visit(SchemaCompatibilityValidator.java:59)
	at org.apache.parquet.filter2.predicate.Operators$Eq.accept(Operators.java:180)
	at org.apache.parquet.filter2.predicate.SchemaCompatibilityValidator.validate(SchemaCompatibilityValidator.java:64)
	at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:59)
	at org.apache.parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:40)
	at org.apache.parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:126)
	at org.apache.parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:46)
	at org.apache.parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:160)
	at org.apache.parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:140)
	at org.apache.spark.rdd.SqlNewHadoopRDD$$anon$1.&amp;lt;init&amp;gt;(SqlNewHadoopRDD.scala:155)
	at org.apache.spark.rdd.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:120)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.UnionRDD.compute(UnionRDD.scala:87)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:), Query: select col1 from `table3` where col2 = 2. [SQL State=HY000, DB Errorcode=500051]

Execution time: 0.44s

1 statement failed.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="12904858">SPARK-11103</key>
            <summary>Parquet filters push-down may cause exception when schema merging is turned on</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gurwls223">Hyukjin Kwon</assignee>
                                    <reporter username="dricard">Dominic Ricard</reporter>
                        <labels>
                    </labels>
                <created>Wed, 14 Oct 2015 13:11:41 +0000</created>
                <updated>Mon, 12 Dec 2022 18:11:11 +0000</updated>
                            <resolved>Fri, 30 Oct 2015 10:23:43 +0000</resolved>
                                    <version>1.5.1</version>
                                    <fixVersion>1.5.2</fixVersion>
                    <fixVersion>1.6.0</fixVersion>
                                    <component>SQL</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="14956988" author="gurwls223" created="Wed, 14 Oct 2015 14:08:39 +0000"  >&lt;p&gt;I tested this case. The problem was, Parquet filters are pushed down regardless of each schema of the splits (or rather files).&lt;/p&gt;

&lt;p&gt;Would the predicate pushdown need to be prevented when using mergeSchema option?&lt;/p&gt;</comment>
                            <comment id="14957317" author="dricard" created="Wed, 14 Oct 2015 17:23:54 +0000"  >&lt;p&gt;Strangely enough, this works perfectly:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;select 
  col1
from
  `table3`
where
  (CASE WHEN col2 = 2 THEN true ELSE false END) = true;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And returns only the row that contains col2 = 2&lt;/p&gt;</comment>
                            <comment id="14957965" author="gurwls223" created="Wed, 14 Oct 2015 22:58:43 +0000"  >&lt;p&gt;In this case, this should be fine because filter is not pushed down to Parquet and data is filtered by Spark filter.&lt;/p&gt;

&lt;p&gt;If you set off spark.sql.parquet.filterPushdown which is true by default, the original case also should work okay. &lt;/p&gt;</comment>
                            <comment id="14958841" author="dricard" created="Thu, 15 Oct 2015 12:52:56 +0000"  >&lt;p&gt;Setting the property &lt;tt&gt;spark.sql.parquet.filterPushdown&lt;/tt&gt; to &lt;tt&gt;false&lt;/tt&gt; fixed the issue.&lt;/p&gt;

&lt;p&gt;Knowing all this, does this indicate a bug in the filter2 implementation of the Parquet library? Maybe this issue should be moved to the Parquet project for someone to look at...&lt;/p&gt;
</comment>
                            <comment id="14958879" author="gurwls223" created="Thu, 15 Oct 2015 13:19:32 +0000"  >&lt;p&gt;For me, I think Spark should appropriately set filters for each file, which I think is pretty tricky, or simply prevent filtering for this case. Would anybody give us some feedback please?&lt;/p&gt;</comment>
                            <comment id="14964337" author="gurwls223" created="Tue, 20 Oct 2015 01:02:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lian+cheng&quot; class=&quot;user-hover&quot; rel=&quot;lian cheng&quot;&gt;lian cheng&lt;/a&gt; This looks clearly an issue and I made three version of patches. However, I want to be sure of which would be proper before making a PR.&lt;/p&gt;

&lt;p&gt;1. Set &lt;tt&gt;false&lt;/tt&gt; to &lt;tt&gt;spark.sql.parquet.filterPushdown&lt;/tt&gt; when using &lt;tt&gt;mergeSchema&lt;/tt&gt;&lt;br/&gt;
2. If &lt;tt&gt;spark.sql.parquet.filterPushdown&lt;/tt&gt; is &lt;tt&gt;true&lt;/tt&gt;, retrieve all the schema of every part-files (and also merged one) and check if each can accept the given schema and then, apply the filter only when they all can accept, which I think it&apos;s a bit over-implemented.&lt;br/&gt;
3. If &lt;tt&gt;spark.sql.parquet.filterPushdown&lt;/tt&gt; is &lt;tt&gt;true&lt;/tt&gt;, retrieve all the schema of every part-files (and also merged one) and apply the filter to each split (rather split) that can accept the filter which (I think it&apos;s hacky) ends up different configurations for each task in a job.&lt;/p&gt;

&lt;p&gt;Would you please give me some feedbacks?&lt;/p&gt;</comment>
                            <comment id="14977961" author="lian cheng" created="Wed, 28 Oct 2015 08:30:08 +0000"  >&lt;p&gt;Quoted from my reply on the user list:&lt;/p&gt;

&lt;p&gt;For 1: This one is pretty simple and safe, I&apos;d like to have this for 1.5.2, or 1.5.3 if we can&apos;t make it for 1.5.2.&lt;/p&gt;

&lt;p&gt;For 2: I&apos;d like to have this for Spark master. Actually we only need to calculate the intersection of all file schemata. We can make ParquetRelation.mergeSchemaInParallel return two StructTypes, the first one is the original merged schema, the other is the intersection of all file schemata, which only contains fields that exist in all file schemata. Then we decide which filter to pushed down according to the second StructType.&lt;/p&gt;

&lt;p&gt;For 3:  The idea with which I came up at first was similar to this one. Instead of pulling all file schemata to driver side, we can push filter push-down code to executor side. Namely, passing candidate filters to executor side, and compute the Parquet filter predicates according to individual file schema. I haven&apos;t looked into this direction in depth, but we can probably put this part into CatalystReadSupport, which is now initialized on executor side.   However, correctness of this approach can only be guaranteed by the defensive filtering we do in Spark SQL (i.e. apply all the filters no matter they are pushed down or not), but we are considering to remove it because it imposes unnecessary performance cost. This makes me hesitant to go along this way.&lt;/p&gt;

&lt;p&gt;From my side, I think this is a bug of Parquet. Parquet was designed to support schema evolution. When scanning a Parquet file, if a column exists in the requested schema but is missing in the file schema, that column is filled with null. This should also hold for pushed-down filter predicates. For example, if filter &quot;a = 1&quot; is pushed down but column &quot;a&quot; doesn&apos;t exist in the Parquet file being scanned, it&apos;s safe to assume &quot;a&quot; is null in all records and drop all of them. On the contrary, if &quot;a IS NULL&quot; is pushed down, all records should be preserved.&lt;/p&gt;

&lt;p&gt;Filed &lt;a href=&quot;https://issues.apache.org/jira/browse/PARQUET-389&quot; title=&quot;Filter predicates should work with missing columns&quot; class=&quot;issue-link&quot; data-issue-key=&quot;PARQUET-389&quot;&gt;&lt;del&gt;PARQUET-389&lt;/del&gt;&lt;/a&gt; to track this issue.&lt;/p&gt;</comment>
                            <comment id="14977976" author="apachespark" created="Wed, 28 Oct 2015 08:45:03 +0000"  >&lt;p&gt;User &apos;HyukjinKwon&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9327&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9327&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14979950" author="lian cheng" created="Thu, 29 Oct 2015 07:07:20 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rxin&quot; class=&quot;user-hover&quot; rel=&quot;rxin&quot;&gt;rxin&lt;/a&gt; I think this should be a blocker for 1.5.2 because we turned on Parquet filter push-down by default in 1.5, and makes this issue a regression compared to 1.4.&lt;/p&gt;</comment>
                            <comment id="14982298" author="lian cheng" created="Fri, 30 Oct 2015 10:23:43 +0000"  >&lt;p&gt;Issue resolved by pull request 9327&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9327&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9327&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14983694" author="apachespark" created="Sat, 31 Oct 2015 00:38:06 +0000"  >&lt;p&gt;User &apos;yhuai&apos; has created a pull request for this issue:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/spark/pull/9387&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/spark/pull/9387&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14990793" author="rxin" created="Thu, 5 Nov 2015 00:37:29 +0000"  >&lt;p&gt;I think this was included in 1.5.2&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12835828">SPARK-8128</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12909334">SPARK-11428</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12908483">PARQUET-389</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12909366">SPARK-11434</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 2 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2n053:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12333643">1.5.2</customfieldvalue>
    <customfieldvalue id="12333083">1.6.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>