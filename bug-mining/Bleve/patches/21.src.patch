diff --git a/analysis/token_filters/persian_normalize/persian_normalize.go b/analysis/token_filters/persian_normalize/persian_normalize.go
new file mode 100644
index 00000000..12401663
--- /dev/null
+++ b/analysis/token_filters/persian_normalize/persian_normalize.go
@@ -0,0 +1,64 @@
+//  Copyright (c) 2014 Couchbase, Inc.
+//  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file
+//  except in compliance with the License. You may obtain a copy of the License at
+//    http://www.apache.org/licenses/LICENSE-2.0
+//  Unless required by applicable law or agreed to in writing, software distributed under the
+//  License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
+//  either express or implied. See the License for the specific language governing permissions
+//  and limitations under the License.
+package persian_normalize
+
+import (
+	"bytes"
+
+	"github.com/couchbaselabs/bleve/analysis"
+)
+
+const (
+	YEH         = '\u064A'
+	FARSI_YEH   = '\u06CC'
+	YEH_BARREE  = '\u06D2'
+	KEHEH       = '\u06A9'
+	KAF         = '\u0643'
+	HAMZA_ABOVE = '\u0654'
+	HEH_YEH     = '\u06C0'
+	HEH_GOAL    = '\u06C1'
+	HEH         = '\u0647'
+)
+
+type PersianNormalizeFilter struct {
+}
+
+func NewPersianNormalizeFilter() *PersianNormalizeFilter {
+	return &PersianNormalizeFilter{}
+}
+
+func (s *PersianNormalizeFilter) Filter(input analysis.TokenStream) analysis.TokenStream {
+	rv := make(analysis.TokenStream, 0)
+
+	for _, token := range input {
+		term := normalize(token.Term)
+		token.Term = term
+		rv = append(rv, token)
+	}
+
+	return rv
+}
+
+func normalize(input []byte) []byte {
+	runes := bytes.Runes(input)
+	for i := 0; i < len(runes); i++ {
+		switch runes[i] {
+		case FARSI_YEH, YEH_BARREE:
+			runes[i] = YEH
+		case KEHEH:
+			runes[i] = KAF
+		case HEH_YEH, HEH_GOAL:
+			runes[i] = HEH
+		case HAMZA_ABOVE: // necessary for HEH + HAMZA
+			runes = analysis.DeleteRune(runes, i)
+			i--
+		}
+	}
+	return analysis.BuildTermFromRunes(runes)
+}
diff --git a/analysis/token_filters/persian_normalize/persian_normalize_test.go b/analysis/token_filters/persian_normalize/persian_normalize_test.go
new file mode 100644
index 00000000..58a9565c
--- /dev/null
+++ b/analysis/token_filters/persian_normalize/persian_normalize_test.go
@@ -0,0 +1,124 @@
+//  Copyright (c) 2014 Couchbase, Inc.
+//  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file
+//  except in compliance with the License. You may obtain a copy of the License at
+//    http://www.apache.org/licenses/LICENSE-2.0
+//  Unless required by applicable law or agreed to in writing, software distributed under the
+//  License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
+//  either express or implied. See the License for the specific language governing permissions
+//  and limitations under the License.
+package persian_normalize
+
+import (
+	"reflect"
+	"testing"
+
+	"github.com/couchbaselabs/bleve/analysis"
+)
+
+func TestPersianStemmerFilter(t *testing.T) {
+	tests := []struct {
+		input  analysis.TokenStream
+		output analysis.TokenStream
+	}{
+		// FarsiYeh
+		{
+			input: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("های"),
+				},
+			},
+			output: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("هاي"),
+				},
+			},
+		},
+		// YehBarree
+		{
+			input: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("هاے"),
+				},
+			},
+			output: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("هاي"),
+				},
+			},
+		},
+		// Keheh
+		{
+			input: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("کشاندن"),
+				},
+			},
+			output: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("كشاندن"),
+				},
+			},
+		},
+		// HehYeh
+		{
+			input: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("كتابۀ"),
+				},
+			},
+			output: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("كتابه"),
+				},
+			},
+		},
+		// HehHamzaAbove
+		{
+			input: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("كتابهٔ"),
+				},
+			},
+			output: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("كتابه"),
+				},
+			},
+		},
+		// HehGoal
+		{
+			input: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("زادہ"),
+				},
+			},
+			output: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte("زاده"),
+				},
+			},
+		},
+		// empty
+		{
+			input: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte(""),
+				},
+			},
+			output: analysis.TokenStream{
+				&analysis.Token{
+					Term: []byte(""),
+				},
+			},
+		},
+	}
+
+	persianNormalizeFilter := NewPersianNormalizeFilter()
+	for _, test := range tests {
+		actual := persianNormalizeFilter.Filter(test.input)
+		if !reflect.DeepEqual(actual, test.output) {
+			t.Errorf("expected %#v, got %#v", test.output, actual)
+			t.Errorf("expected % x, got % x", test.output[0].Term, actual[0].Term)
+		}
+	}
+}
diff --git a/analysis/token_filters/sorani_normalize/sorani_normalize.go b/analysis/token_filters/sorani_normalize/sorani_normalize.go
index e9288d56..e7d55481 100644
--- a/analysis/token_filters/sorani_normalize/sorani_normalize.go
+++ b/analysis/token_filters/sorani_normalize/sorani_normalize.go
@@ -11,7 +11,6 @@ package sorani_normalize
 import (
 	"bytes"
 	"unicode"
-	"unicode/utf8"
 
 	"github.com/couchbaselabs/bleve/analysis"
 )
@@ -76,7 +75,7 @@ func normalize(input []byte) []byte {
 			if i > 0 && runes[i-1] == HEH {
 				runes[i-1] = AE
 			}
-			runes = deleteRune(runes, i)
+			runes = analysis.DeleteRune(runes, i)
 			i--
 		case HEH:
 			if i == len(runes)-1 {
@@ -93,32 +92,14 @@ func normalize(input []byte) []byte {
 		case RREH_ABOVE:
 			runes[i] = RREH
 		case TATWEEL, KASRATAN, DAMMATAN, FATHATAN, FATHA, DAMMA, KASRA, SHADDA, SUKUN:
-			runes = deleteRune(runes, i)
+			runes = analysis.DeleteRune(runes, i)
 			i--
 		default:
 			if unicode.In(runes[i], unicode.Cf) {
-				runes = deleteRune(runes, i)
+				runes = analysis.DeleteRune(runes, i)
 				i--
 			}
 		}
 	}
-	return buildTermFromRunes(runes)
-}
-
-func deleteRune(in []rune, pos int) []rune {
-	if pos >= len(in) {
-		return in
-	}
-	copy(in[pos:], in[pos+1:])
-	return in[:len(in)-1]
-}
-
-func buildTermFromRunes(runes []rune) []byte {
-	rv := make([]byte, 0, len(runes)*4)
-	for _, r := range runes {
-		runeBytes := make([]byte, utf8.RuneLen(r))
-		utf8.EncodeRune(runeBytes, r)
-		rv = append(rv, runeBytes...)
-	}
-	return rv
+	return analysis.BuildTermFromRunes(runes)
 }
diff --git a/analysis/token_filters/sorani_normalize/sorani_normalize_test.go b/analysis/token_filters/sorani_normalize/sorani_normalize_test.go
index a6438fb4..e1d2ba8e 100644
--- a/analysis/token_filters/sorani_normalize/sorani_normalize_test.go
+++ b/analysis/token_filters/sorani_normalize/sorani_normalize_test.go
@@ -15,27 +15,6 @@ import (
 	"github.com/couchbaselabs/bleve/analysis"
 )
 
-func TestDeleteRune(t *testing.T) {
-	tests := []struct {
-		in     []rune
-		delPos int
-		out    []rune
-	}{
-		{
-			in:     []rune{'a', 'b', 'c'},
-			delPos: 1,
-			out:    []rune{'a', 'c'},
-		},
-	}
-
-	for _, test := range tests {
-		actual := deleteRune(test.in, test.delPos)
-		if !reflect.DeepEqual(actual, test.out) {
-			t.Errorf("expected %#v, got %#v", test.out, actual)
-		}
-	}
-}
-
 func TestSoraniStemmerFilter(t *testing.T) {
 	tests := []struct {
 		input  analysis.TokenStream
diff --git a/analysis/util.go b/analysis/util.go
new file mode 100644
index 00000000..62337ec1
--- /dev/null
+++ b/analysis/util.go
@@ -0,0 +1,31 @@
+//  Copyright (c) 2014 Couchbase, Inc.
+//  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file
+//  except in compliance with the License. You may obtain a copy of the License at
+//    http://www.apache.org/licenses/LICENSE-2.0
+//  Unless required by applicable law or agreed to in writing, software distributed under the
+//  License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
+//  either express or implied. See the License for the specific language governing permissions
+//  and limitations under the License.
+package analysis
+
+import (
+	"unicode/utf8"
+)
+
+func DeleteRune(in []rune, pos int) []rune {
+	if pos >= len(in) {
+		return in
+	}
+	copy(in[pos:], in[pos+1:])
+	return in[:len(in)-1]
+}
+
+func BuildTermFromRunes(runes []rune) []byte {
+	rv := make([]byte, 0, len(runes)*4)
+	for _, r := range runes {
+		runeBytes := make([]byte, utf8.RuneLen(r))
+		utf8.EncodeRune(runeBytes, r)
+		rv = append(rv, runeBytes...)
+	}
+	return rv
+}
diff --git a/analysis/util_test.go b/analysis/util_test.go
new file mode 100644
index 00000000..e0067278
--- /dev/null
+++ b/analysis/util_test.go
@@ -0,0 +1,35 @@
+//  Copyright (c) 2014 Couchbase, Inc.
+//  Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file
+//  except in compliance with the License. You may obtain a copy of the License at
+//    http://www.apache.org/licenses/LICENSE-2.0
+//  Unless required by applicable law or agreed to in writing, software distributed under the
+//  License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND,
+//  either express or implied. See the License for the specific language governing permissions
+//  and limitations under the License.
+package analysis
+
+import (
+	"reflect"
+	"testing"
+)
+
+func TestDeleteRune(t *testing.T) {
+	tests := []struct {
+		in     []rune
+		delPos int
+		out    []rune
+	}{
+		{
+			in:     []rune{'a', 'b', 'c'},
+			delPos: 1,
+			out:    []rune{'a', 'c'},
+		},
+	}
+
+	for _, test := range tests {
+		actual := DeleteRune(test.in, test.delPos)
+		if !reflect.DeepEqual(actual, test.out) {
+			t.Errorf("expected %#v, got %#v", test.out, actual)
+		}
+	}
+}
diff --git a/config.go b/config.go
index 4a79f10b..6ea2f9f6 100644
--- a/config.go
+++ b/config.go
@@ -28,6 +28,7 @@ import (
 	"github.com/couchbaselabs/bleve/analysis/token_filters/elision_filter"
 	"github.com/couchbaselabs/bleve/analysis/token_filters/length_filter"
 	"github.com/couchbaselabs/bleve/analysis/token_filters/lower_case_filter"
+	"github.com/couchbaselabs/bleve/analysis/token_filters/persian_normalize"
 	"github.com/couchbaselabs/bleve/analysis/token_filters/sorani_normalize"
 	"github.com/couchbaselabs/bleve/analysis/token_filters/sorani_stemmer_filter"
 	"github.com/couchbaselabs/bleve/analysis/token_filters/stemmer_filter"
@@ -289,6 +290,7 @@ func init() {
 	Config.Analysis.TokenFilters["normalize_nfkc"] = unicode_normalize.MustNewUnicodeNormalizeFilter(unicode_normalize.NFKC)
 	Config.Analysis.TokenFilters["normalize_nfkd"] = unicode_normalize.MustNewUnicodeNormalizeFilter(unicode_normalize.NFKD)
 	Config.Analysis.TokenFilters["normalize_ckb"] = sorani_normalize.NewSoraniNormalizeFilter()
+	Config.Analysis.TokenFilters["normalize_fa"] = persian_normalize.NewPersianNormalizeFilter()
 
 	// register analyzers
 	keywordAnalyzer := Config.MustBuildNewAnalyzer([]string{}, "single", []string{})
