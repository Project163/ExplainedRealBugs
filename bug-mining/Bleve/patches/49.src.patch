diff --git a/analysis/token_filters/unicode_normalize/unicode_normalize.go b/analysis/token_filters/unicode_normalize/unicode_normalize.go
index 05867711..dac6171c 100644
--- a/analysis/token_filters/unicode_normalize/unicode_normalize.go
+++ b/analysis/token_filters/unicode_normalize/unicode_normalize.go
@@ -28,7 +28,7 @@ var forms = map[string]norm.Form{
 	NFC:  norm.NFC,
 	NFD:  norm.NFD,
 	NFKC: norm.NFKC,
-	NFKD: norm.NFKC,
+	NFKD: norm.NFKD,
 }
 
 type UnicodeNormalizeFilter struct {
diff --git a/analysis/token_filters/unicode_normalize/unicode_normalize_test.go b/analysis/token_filters/unicode_normalize/unicode_normalize_test.go
index a02f6794..b415c652 100644
--- a/analysis/token_filters/unicode_normalize/unicode_normalize_test.go
+++ b/analysis/token_filters/unicode_normalize/unicode_normalize_test.go
@@ -67,7 +67,7 @@ func TestUnicodeNormalization(t *testing.T) {
 			},
 		},
 		{
-			formName: NFKD,
+			formName: NFKC,
 			input: analysis.TokenStream{
 				&analysis.Token{
 					Term: []byte("ｳﾞｨｯﾂ"),
@@ -80,7 +80,7 @@ func TestUnicodeNormalization(t *testing.T) {
 			},
 		},
 		{
-			formName: NFKD,
+			formName: NFKC,
 			input: analysis.TokenStream{
 				&analysis.Token{
 					Term: []byte("ﾊﾟﾅｿﾆｯｸ"),
