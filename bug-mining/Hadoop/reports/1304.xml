<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 21:45:49 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HADOOP-11064] UnsatisifedLinkError with hadoop 2.4 JARs on hadoop-2.6 due to NativeCRC32 method changes</title>
                <link>https://issues.apache.org/jira/browse/HADOOP-11064</link>
                <project id="12310240" key="HADOOP">Hadoop Common</project>
                    <description>&lt;p&gt;The private native method names and signatures in &lt;tt&gt;NativeCrc32&lt;/tt&gt; were changed in &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-10975&quot; title=&quot;org.apache.hadoop.util.DataChecksum should support native checksum calculation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-10975&quot;&gt;&lt;del&gt;HDFS-6561&lt;/del&gt;&lt;/a&gt; ... as a result hadoop-common-2.4 JARs get unsatisifed link errors when they try to perform checksums. &lt;/p&gt;

&lt;p&gt;This essentially stops Hadoop 2.4 applications running on Hadoop 2.6 unless rebuilt and repackaged with the hadoop- 2.6 JARs&lt;/p&gt;</description>
                <environment>&lt;p&gt;Hadoop 2.6 cluster, trying to run code containing hadoop 2.4 JARs&lt;/p&gt;</environment>
        <key id="12739441">HADOOP-11064</key>
            <summary>UnsatisifedLinkError with hadoop 2.4 JARs on hadoop-2.6 due to NativeCRC32 method changes</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="cnauroth">Chris Nauroth</assignee>
                                    <reporter username="stevel@apache.org">Steve Loughran</reporter>
                        <labels>
                    </labels>
                <created>Fri, 5 Sep 2014 09:32:25 +0000</created>
                <updated>Fri, 16 Oct 2015 17:30:55 +0000</updated>
                            <resolved>Wed, 24 Sep 2014 22:35:37 +0000</resolved>
                                    <version>2.6.0</version>
                                    <fixVersion>2.6.0</fixVersion>
                                    <component>native</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>18</watches>
                                                                                                                <comments>
                            <comment id="14122721" author="stevel@apache.org" created="Fri, 5 Sep 2014 09:36:11 +0000"  >&lt;p&gt;Stack trace from HBase&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;FATAL [master:c6401:45972] master.HMaster: Unhandled exception. Startingshutdown.java.lang.UnsatisfiedLinkError: org.apache.hadoop.util.NativeCrc32.nativeVerifyChunkedSums(IILjava/nio/ByteBuffer;ILjava/nio/ByteBuffer;IILjava/lang/&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;;J)V
at org.apache.hadoop.util.NativeCrc32.nativeVerifyChunkedSums(Native Method)
at org.apache.hadoop.util.NativeCrc32.verifyChunkedSums(NativeCrc32.java:57)
at org.apache.hadoop.util.DataChecksum.verifyChunkedSums(DataChecksum.java:291)
at org.apache.hadoop.hdfs.BlockReaderLocal.doByteBufferRead(BlockReaderLocal.java:338)
at org.apache.hadoop.hdfs.BlockReaderLocal.fillSlowReadBuffer(BlockReaderLocal.java:388)
at org.apache.hadoop.hdfs.BlockReaderLocal.read(BlockReaderLocal.java:408)
at org.apache.hadoop.hdfs.DFSInputStream$ByteArrayStrategy.doRead(DFSInputStream.java:642)
at org.apache.hadoop.hdfs.DFSInputStream.readBuffer(DFSInputStream.java:698)
at org.apache.hadoop.hdfs.DFSInputStream.readWithStrategy(DFSInputStream.java:752)
at org.apache.hadoop.hdfs.DFSInputStream.read(DFSInputStream.java:793)
at java.io.DataInputStream.read(DataInputStream.java:149)
at org.apache.hadoop.io.IOUtils.readFully(IOUtils.java:192)
at org.apache.hadoop.hbase.util.FSUtils.getVersion(FSUtils.java:495)
at org.apache.hadoop.hbase.util.FSUtils.checkVersion(FSUtils.java:582)
at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:460)
at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:151)
at org.apache.hadoop.hbase.master.MasterFileSystem.&amp;lt;init&amp;gt;(MasterFileSystem.java:128)
at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:790)
at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:603)
at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14122731" author="stevel@apache.org" created="Fri, 5 Sep 2014 09:41:58 +0000"  >&lt;p&gt;The culprit appears to be that the &lt;tt&gt;nativeVerify&lt;/tt&gt; methods have been renamed &lt;tt&gt;nativeCompute&lt;/tt&gt; and their signature changed. If they were private and in the same class this would not be an issue &#8212;but they are really in the external &lt;tt&gt;hadoop.so&lt;/tt&gt; lib, which is now out of sync with any hadoop applications using hadoop.jar &amp;lt; 2.6&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt; void nativeVerifyChunkedSums(
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bytesPerSum, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; checksumType,
      ByteBuffer sums, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; sumsOffset,
      ByteBuffer data, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; dataOffset, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; dataLength,
      &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; fileName, &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; basePos);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt; void nativeComputeChunkedSums(
      &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; bytesPerSum, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; checksumType,
      ByteBuffer sums, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; sumsOffset,
      ByteBuffer data, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; dataOffset, &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; dataLength,
      &lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt; fileName, &lt;span class=&quot;code-object&quot;&gt;long&lt;/span&gt; basePos, &lt;span class=&quot;code-object&quot;&gt;boolean&lt;/span&gt; verify);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The obvious fix would be to reinstate the existing methods/signatures and relay internally to the new methods.&lt;/p&gt;</comment>
                            <comment id="14125735" author="tlipcon" created="Mon, 8 Sep 2014 16:47:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt; &amp;#8211; hmm, I&apos;m not 100% following here. Are you suggesting that we are supposed to support mixed-version installations? I&apos;d always assumed that, if someone is running hadoop 2.4 client, then they would &lt;em&gt;only&lt;/em&gt; have hadoop 2.4 artifacts on their path (classpath and java.library.path alike). Here you&apos;re saying that the user has Hadoop 2.4 jars with Hadoop 2.6 SOs on the same classpath.&lt;/p&gt;

&lt;p&gt;In the same way that we don&apos;t expect HDFS 2.6 to work with a Common 2.4 JAR, I don&apos;t think this is a supported scenario. Is this scenario listed in any of our compatibility documentation?&lt;/p&gt;</comment>
                            <comment id="14125742" author="tlipcon" created="Mon, 8 Sep 2014 16:50:18 +0000"  >&lt;p&gt;sorry, hit send too early:&lt;/p&gt;

&lt;p&gt;Doing a fix to maintain the old private API is certainly an easy option like you suggested, but it would be nice to have this compatibility guarantee written down somewhere if we feel it is something we want to maintain. I&apos;m -0 on guaranteeing support for mixed version classpaths, since the testing matrix becomes quite large, and it means that &lt;b&gt;internal&lt;/b&gt; APIs (these methods are marked InterfaceAudience.Private) now need attention with regard to compatibility. What distinguishes the shared library case from the mixed hdfs/common case mentioned above?&lt;/p&gt;</comment>
                            <comment id="14125799" author="ndimiduk" created="Mon, 8 Sep 2014 17:39:54 +0000"  >&lt;p&gt;Backward compatibility within a major version is usually expected from a user perspective.&lt;/p&gt;</comment>
                            <comment id="14125965" author="stevel@apache.org" created="Mon, 8 Sep 2014 19:32:00 +0000"  >&lt;p&gt;I&apos;m saying &quot;if someone untars the hbase binaries and tries to use them in a hadoop 2.6 cluster, they get to see a stack trace&quot;. HBase includes all its JARs, which are 100% in sync, it&apos;s just that nobody bundles the native libs too.&lt;/p&gt;

&lt;p&gt;I dont care about cross-JAR compatibility, but the changes in the .lib/.so code stop me running any hadoop 2.4 code if the 2.6 libs are on their LIBPATH. That includes standalone applications which would otherwise run happily without any native JARs. &lt;/p&gt;


&lt;p&gt;IMO those native bits of code are something we need to keep stable, even though they are never intended for direct public use. We should also have the java clients pick up version problems (2.6 code loading 2.4 lib)&lt;/p&gt;</comment>
                            <comment id="14125968" author="aw" created="Mon, 8 Sep 2014 19:35:11 +0000"  >&lt;p&gt;If nothing else, we should really properly version them instead of making up wackadoodle numbers.  (libhadoop.so has been 1.0.0 for how many releases now?  Probably since it was introduced!)&lt;/p&gt;</comment>
                            <comment id="14125978" author="cmccabe" created="Mon, 8 Sep 2014 19:43:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;This essentially stops Hadoop 2.4 applications running on Hadoop 2.6 unless rebuilt and repackaged with the hadoop- 2.6 JARs&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t follow.  What is to rebuild?  If you put the Hadoop 2.6 jars on the classpath and the hadoop 2.6 libhadoop.so on the classpath, your application should work.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Backward compatibility within a major version is usually expected from a user perspective.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;As Todd mentioned, we don&apos;t support mixing Hadoop 2.4 and Hadoop 2.6 jars on the same CLASSPATH.  Similarly, we don&apos;t support mixing a libhadoop.so from one version with a libhadoop.so from another version.  Backwards compatibility refers to compatibility with the public APIs, not compatibility with internal, non-public APIs.&lt;/p&gt;

&lt;p&gt;There&apos;s been a few of these jiras recently where people seem to want to put multiple versions of libhadoop.so on their classpath and expect it to work (I guess it would be hadoop.dll on Windows).  I don&apos;t understand the motivation for doing this-- can&apos;t the launcher scripts for Windows simply set up the CLASSPATH and LD_LIBRARY_PATH appropriately for the version being launched?  What am I missing?&lt;/p&gt;

&lt;p&gt;If it&apos;s absolutely, positively necessary to support throwing multiple versions of the libhadoop library on the classpath at once, we could inject the version into the library name / version info.&lt;/p&gt;</comment>
                            <comment id="14126207" author="tlipcon" created="Mon, 8 Sep 2014 22:22:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;If nothing else, we should really properly version them instead of making up wackadoodle numbers. (libhadoop.so has been 1.0.0 for how many releases now? Probably since it was introduced!)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed, but unfortunately afaik Java doesn&apos;t provide a way to specify a particular so version dependency. I asked this on Quora a few years ago:&lt;br/&gt;
&lt;a href=&quot;http://www.quora.com/Is-there-a-way-to-force-Java-to-load-a-particular-soversion-of-a-JNI-dependency&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.quora.com/Is-there-a-way-to-force-Java-to-load-a-particular-soversion-of-a-JNI-dependency&lt;/a&gt;&lt;br/&gt;
and unfortunately got no real answers.&lt;/p&gt;

&lt;p&gt;So, does this mean we need to always keep binary compatibility of the internal-facing libhadoop.so? Could we change hbase to pick up the Hadoop libraries from HADOOP_HOME instead of bundling them? It seems like it should either (a) bundle everything, including the native code, or (b) bundle nothing, and load everything from HADOOP_HOME. What&apos;s causing a problem is that it&apos;s using bundled jars with system-located native code.&lt;/p&gt;</comment>
                            <comment id="14126239" author="cnauroth" created="Mon, 8 Sep 2014 22:47:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;...bundle everything, including the native code...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think there is a reliance on the Maven jar file dependency as a reliable unit of versioning.  Applications expect that if they bundle their relevant Hadoop jars all at the same version, then everything is going to work.  Unfortunately, it turns out that hadoop-common.jar is an &quot;incomplete&quot; artifact in terms of Maven versioning because of the tight coupling between the jar and the native code.&lt;/p&gt;

&lt;p&gt;If we bundled the native libs inside hadoop-common.jar (or some new jar), and extracted it automatically at runtime, then downstream projects could get a completely versioned artifact through the Maven dependency.  The snappy-java project is an example of something that does this.  It detects the OS and extracts the corresponding native library at runtime.  I believe hadoop-lzo has just started doing something similar too.&lt;/p&gt;

&lt;p&gt;Of course, this opens up a can of worms around release engineering.  The current Apache release process only builds the native components on a single platform.  I can&apos;t think of a way to make this work without a lot of infrastructure work as a pre-requisite.&lt;/p&gt;</comment>
                            <comment id="14126265" author="aw" created="Mon, 8 Sep 2014 23:06:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;unfortunately afaik Java doesn&apos;t provide a way to specify a particular so version dependency.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I keep thinking that System.load(&quot;/full/path/libname.so.1.2.3&quot;) worked.  (We should be able to figure out the full path by just processing java.library.path manually or maybe reading hadoop.native.dir or whatever.)&lt;/p&gt;</comment>
                            <comment id="14126326" author="cmccabe" created="Mon, 8 Sep 2014 23:48:46 +0000"  >&lt;p&gt;Here&apos;s a patch I whipped up to rename &lt;tt&gt;libhadoop.so&lt;/tt&gt; to &lt;tt&gt;libhadoop-3.0.0-SNAPSHOT.so&lt;/tt&gt; (or whatever your version number may happen to be)&lt;/p&gt;

&lt;p&gt;This is basically not using the &quot;normal&quot; versioning scheme which Java seems to ignore in &lt;tt&gt;System.load&lt;/tt&gt;, but changing the name of the library-- an important distinction.  This allows us to avoid things like parsing &lt;tt&gt;java.library.path&lt;/tt&gt; ourselves, etc.&lt;/p&gt;

&lt;p&gt;I think this will put the issue to rest and give people some relief from version conflicts and API issues.  I don&apos;t know if there&apos;s anything that needs to be done on the bigtop / packaging side...&lt;/p&gt;</comment>
                            <comment id="14126345" author="cnauroth" created="Tue, 9 Sep 2014 00:04:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt;, did you want the scope of this jira focused on reinstating the old native function signatures in &lt;tt&gt;NativeCrc32&lt;/tt&gt;?  That&apos;s the fastest path to resolving the blocker.&lt;/p&gt;

&lt;p&gt;The longer-term policy discussion is great too, but perhaps that&apos;s better handled separately.&lt;/p&gt;</comment>
                            <comment id="14126408" author="cmccabe" created="Tue, 9 Sep 2014 00:50:22 +0000"  >&lt;p&gt;I&apos;m concerned that this will set a bad precedent here that we are unable to change libhadoop.so between versions.  We certainly have done this a lot in the past, and it&apos;s important that we keep this flexibility.  Since we have some time before 2.6, shouldn&apos;t we consider the library renaming solution?&lt;/p&gt;</comment>
                            <comment id="14126415" author="hadoopqa" created="Tue, 9 Sep 2014 00:56:17 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12667284/HADOOP-11064.001.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12667284/HADOOP-11064.001.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision 7498dd7.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  There were no new javadoc warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 eclipse:eclipse&lt;/font&gt;.  The patch built with eclipse:eclipse.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in hadoop-common-project/hadoop-common.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 contrib tests&lt;/font&gt;.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4674//testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4674//testReport/&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4674//console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4674//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14126835" author="stevel@apache.org" created="Tue, 9 Sep 2014 10:30:39 +0000"  >&lt;p&gt;I&apos;d like to see this patch not reverting any of the changes, but offering the existing API calls. On windows machines, the native lib is near-essential for functionality; on linux it is the only way to get performance on IO. With things like unix-socket shortcuts and native encryption, this only gets worse. &lt;/p&gt;

&lt;p&gt;It shows up on YARN apps more than anywhere else because these apps are not things built by bigtop or anyone else and pre-installed on the cluster. They are apps submitted remotely &#8212;and to avoid problems with transitive dependencies &amp;amp;c, they are moving towards a &quot;submit all the JARs&quot; strategy. Twill does this, Tez does this, &lt;a href=&quot;https://issues.apache.org/jira/browse/SLIDER-330&quot; title=&quot;Slider to upload all JARs to AM via distributed cache&quot; class=&quot;issue-link&quot; data-issue-key=&quot;SLIDER-330&quot;&gt;&lt;del&gt;SLIDER-330&lt;/del&gt;&lt;/a&gt; shows where it is on my todo list. This stops my code being brittle against versions, but it only works if the right JAR is on the CP&lt;/p&gt;

&lt;p&gt;We can&apos;t (currently) mandate that YARN apps know which platform lib to use, to include it in their binaries and upload to it, so we do need a good story here. Long term, I think &lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;some versioning like the current patch can work&lt;/li&gt;
	&lt;li&gt;I think for windows, we should at least have a winutils.jar JAR that includes the windows one. As chris points out, packaging gets complex here. Unless, perhaps, it gets teased out and made its own self-contained hadop-tools project that gets pre-published, potentially on a different release schedule.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Anyway, right now I&apos;d be happier with a recovery of the old methods, while we think of a good policy and how to implement it.&lt;/p&gt;</comment>
                            <comment id="14127277" author="cnauroth" created="Tue, 9 Sep 2014 17:36:56 +0000"  >&lt;p&gt;I&apos;m +1 for short-term reinstatement of the old function signatures and ongoing work on the long-term policy.  Colin, I agree that it&apos;s not ideal to lock ourselves into maintaining every function signature that ever existed over time.  At least speaking for myself, I have no intention of pointing at this as a precedent to block implementation of a proper versioning scheme.  Hopefully that helps ease some of the very legitimate concerns that you raised.&lt;/p&gt;

&lt;p&gt;BTW, I believe the patch you posted would break Windows.  &lt;tt&gt;NativeCodeLoader&lt;/tt&gt; would attempt to load hadoop-&amp;lt;version&amp;gt;.dll, but the Windows native build hasn&apos;t been updated to produce a hadoop-&amp;lt;version&amp;gt;.dll instead of a hadoop.dll.&lt;/p&gt;</comment>
                            <comment id="14127468" author="cmccabe" created="Tue, 9 Sep 2014 19:57:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;BTW, I believe the patch you posted would break Windows. NativeCodeLoader would attempt to load hadoop-&amp;lt;version&amp;gt;.dll, but the Windows native build hasn&apos;t been updated to produce a hadoop-&amp;lt;version&amp;gt;.dll instead of a hadoop.dll.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Perhaps I&apos;m missing something obvious here, but shouldn&apos;t the following code be enough to produce &lt;tt&gt;hadoop-&amp;lt;version&amp;gt;.dll&lt;/tt&gt; under Windows?&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;SET(CMAKE_BUILD_WITH_INSTALL_RPATH TRUE)
SET(HADOOP_LIB_NAME hadoop-${HADOOP_VERSION})
add_dual_library(${HADOOP_LIB_NAME}
    main/&lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;/src/exception.c
    ....
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Would be nice if someone could give the patch a try on Windows and give me some feedback.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m +1 for short-term reinstatement of the old function signatures and ongoing work on the long-term policy. Colin, I agree that it&apos;s not ideal to lock ourselves into maintaining every function signature that ever existed over time. At least speaking for myself, I have no intention of pointing at this as a precedent to block implementation of a proper versioning scheme. Hopefully that helps ease some of the very legitimate concerns that you raised.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;There are a few months until 2.6 will be released-- do we really need to hack this?  I think the versioning solution is quite clean and easy to implement.  Trying to mix build products from different releases, even as a short-term solution, just puts us down the wrong path, I think-- the same as people trying to mix jars from different releases.&lt;/p&gt;</comment>
                            <comment id="14127565" author="cnauroth" created="Tue, 9 Sep 2014 21:13:40 +0000"  >&lt;p&gt;The hadoop.dll build is still driven from a checked-in project file instead of CMake.  Because of this, certain build changes require duplication of logic for both CMake and MSBuild.  I&apos;ve attached a v2 patch showing the change.  This just covers hadoop.dll.  We&apos;ll also need to consider winutils.exe, so the patch isn&apos;t complete yet.  (BTW for anyone observing, I used CMake on Windows for the libhdfs port in &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-573&quot; title=&quot;Porting libhdfs to Windows&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-573&quot;&gt;&lt;del&gt;HDFS-573&lt;/del&gt;&lt;/a&gt;, so I think that&apos;s a good sign that we could port hadoop.dll to use CMake and eliminate these kinds of pitfalls.  That&apos;s a topic for another time though.)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think the versioning solution is quite clean and easy to implement.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, but I also think there are still some unresolved issues driven by the use case Steve described.&lt;/p&gt;

&lt;p&gt;The basic use case is to submit a YARN application with complete control of the Hadoop dependencies.  The proposed patch would publish versioned builds of the form libhadoop-&amp;lt;version&amp;gt;.so.  The &lt;tt&gt;System.loadLibrary&lt;/tt&gt; call would match strictly on the exact jar version.&lt;/p&gt;

&lt;p&gt;However, the application owner might decide to upgrade the project&apos;s Hadoop dependency before the cluster administrator deploys the new libhadoop-&amp;lt;version&amp;gt;.so, which would result in the application unexpectedly getting link errors.  Note that this would be a problem even for minor upgrades, where we&apos;ve stated a guarantee of backwards-compatibility.  If the application changes its dependency from 2.4.1 to 2.4.2, even if we didn&apos;t change any native code in that rev, the application will get a link error due to missing libhadoop-2.4.2.so on the cluster.&lt;/p&gt;

&lt;p&gt;This then implies that cluster administrators have a responsibility to deploy and maintain copies of every new point release of every major release ever deployed, just in case one of the devs they support decides to submit an application with that version.  Things that used to be routine deployments now might require more explicit coordination between application developers and cluster administrators.  Most administrators would prefer to deploy only the software versions that they&apos;re really interested in running.&lt;/p&gt;

&lt;p&gt;FWIW, I&apos;d prefer some kind of solution that bundles the native build into a versioned jar.  As far as I can tell, that provides the cleanest isolation for this use case.  Unfortunately, the build infrastructure requirements for this look infeasible to me in the short-term.&lt;/p&gt;

&lt;p&gt;These issues make me think the versioning solution needs additional design work and buy-in from a large portion of the community before we settle on it.  Restoring the old function signatures would be helpful to unblock downstream projects that are trying to test against 2.6.0-SNAPSHOT clusters right now.  Granted, 2.6.0-SNAPSHOT isn&apos;t a real release, so we&apos;re technically under no obligation, but I see it as a matter of good citizenship.&lt;/p&gt;</comment>
                            <comment id="14127581" author="hadoopqa" created="Tue, 9 Sep 2014 21:26:15 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12667487/HADOOP-11064.002.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12667487/HADOOP-11064.002.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision 3e8f353.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 patch&lt;/font&gt;.  The patch command could not apply the patch.&lt;/p&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4684//console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4684//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14127627" author="cnauroth" created="Tue, 9 Sep 2014 21:55:44 +0000"  >&lt;p&gt;Uploading patch v3 to fix some mishandled line endings in the v2 patch.&lt;/p&gt;</comment>
                            <comment id="14127711" author="hadoopqa" created="Tue, 9 Sep 2014 22:46:48 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12667506/HADOOP-11064.003.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12667506/HADOOP-11064.003.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision 3e8f353.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  There were no new javadoc warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 eclipse:eclipse&lt;/font&gt;.  The patch built with eclipse:eclipse.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in hadoop-common-project/hadoop-common.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 contrib tests&lt;/font&gt;.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4686//testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4686//testReport/&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4686//console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4686//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14127882" author="cmccabe" created="Wed, 10 Sep 2014 01:11:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think that&apos;s a good sign that we could port hadoop.dll to use CMake and eliminate these kinds of pitfalls. That&apos;s a topic for another time though.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, it would be great to see the Windows build of &lt;tt&gt;hadoop.dll&lt;/tt&gt; use CMake.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;FWIW, I&apos;d prefer some kind of solution that bundles the native build into a versioned jar. As far as I can tell, that provides the cleanest isolation for this use case. Unfortunately, the build infrastructure requirements for this look infeasible to me in the short-term.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It seems like this is a general issue with native dependencies of jars submitted through YARN.  It would be better to solve it inside YARN, than to force a draconian internal API compatibility policy on to libhadoop.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;These issues make me think the versioning solution needs additional design work and buy-in from a large portion of the community before we settle on it. Restoring the old function signatures would be helpful to unblock downstream projects that are trying to test against 2.6.0-SNAPSHOT clusters right now. Granted, 2.6.0-SNAPSHOT isn&apos;t a real release, so we&apos;re technically under no obligation, but I see it as a matter of good citizenship.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think the versioning solution does show good citizenship.  We are going out of our way to work around this issue with YARN and native dependencies.  It&apos;s not a perfect workaround, but I don&apos;t see how it can be, unless YARN is changed to somehow distribute these dependencies (as you hinted at?)&lt;/p&gt;

&lt;p&gt;Also, what are the &quot;old&quot; function signatures?  The ones for Hadoop 2.4?  2.5?  2.5.1?  libhadoop changes all the time and its development isn&apos;t frozen.&lt;/p&gt;

&lt;p&gt;I am + 1 for v3 of the patch... and based on previous conversations, I think most people would be OK with adding versioning to libhadoop.  Perhaps bring it up on hadoop-dev if you think this needs more eyes?&lt;/p&gt;

&lt;p&gt;I apologize again for being difficult here... you guys have done a lot of good work on the native library and on Windows support.  But I want to make sure we&apos;re not constrained in the future.  It may be a while before we can get a perfect solution to this.&lt;/p&gt;</comment>
                            <comment id="14128018" author="tlipcon" created="Wed, 10 Sep 2014 04:04:05 +0000"  >&lt;p&gt;Colin &amp;#8211; are you aware of any other changes in the recent history of libhadoop where we broke APIs rather than just augmenting them? I agree the versioning thing is a good idea, but also seems reasonable enough to provide the &quot;relay&quot; methods if it&apos;s trivial to do so here.&lt;/p&gt;</comment>
                            <comment id="14128027" author="cnauroth" created="Wed, 10 Sep 2014 04:27:19 +0000"  >&lt;blockquote&gt;&lt;p&gt;I agree, it would be great to see the Windows build of hadoop.dll use CMake.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Earlier today, I filed &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11080&quot; title=&quot;Convert Windows native build in hadoop-common to use CMake.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11080&quot;&gt;HADOOP-11080&lt;/a&gt; to track this.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It would be better to solve it inside YARN, than to force a draconian internal API compatibility policy on to libhadoop.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I can&apos;t think of a feature request that can be made of YARN for this.  hadoop-common.jar essentially does not publish a versioned release of its corresponding native component.  Asking YARN to figure out the right matching native build at submission time would be brittle.  Asking the applications to bundle the matching native build version themselves pushes the build complexity that we haven&apos;t solved down to our downstream consumers.&lt;/p&gt;

&lt;p&gt;Hopefully I didn&apos;t imply that YARN could figure out how to distribute the dependency.  What I really wanted to convey is that hadoop-common-&amp;lt;version&amp;gt;.jar is tightly coupled to a specific version of libhadoop.so, and if libhadoop.so was inside the jar, then it would solve the problem for all applications that want to bundle a specific Hadoop jar version and submit that as a YARN application.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Also, what are the &quot;old&quot; function signatures? The ones for Hadoop 2.4? 2.5? 2.5.1?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s a fair point.  The specific bug report here involves a 2.4.0 YARN application submitted to a 2.6.0 YARN cluster, but I don&apos;t know what other incompatibilities we&apos;d expose with other version combinations.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I am + 1 for v3 of the patch... and based on previous conversations, I think most people would be OK with adding versioning to libhadoop.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I heard a general desire for .so versioning.  (The .so version has been at 1.0.0 forever as far as I can tell.)  I could be wrong, but I don&apos;t think I heard general consensus for the scheme implemented in the patch.  This is not standard .so versioning.  Instead, it&apos;s a whole new library name with every release, even minor releases.  Just so everyone is on the same page, here is what we get from a release with this patch:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;gt; ll hadoop-3.0.0-SNAPSHOT/lib/&lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;/libhadoop*.so*
lrwxrwxrwx 1 cnauroth   33 Sep  9 20:49 hadoop-3.0.0-SNAPSHOT/lib/&lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;/libhadoop-3.0.0-SNAPSHOT.so -&amp;gt; libhadoop-3.0.0-SNAPSHOT.so.1.0.0*
-rwxrwxr-x 1 cnauroth 763K Sep  9 20:49 hadoop-3.0.0-SNAPSHOT/lib/&lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt;/libhadoop-3.0.0-SNAPSHOT.so.1.0.0*
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This means that during upgrades, the cluster administrator or the distro packaging must maintain all prior versions.  Otherwise, upgrading a cluster from 2.4.0 to 2.4.1 risks link errors for applications that happen to bundle the 2.4.0 jar.  In this sense, I&apos;m concerned that the patch actually makes the situation worse.  It certainly changes the maintenance and deployment model.  With consideration of that, are you still +1?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I apologize again for being difficult here...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No apologies necessary.  This is all healthy discussion.  You&apos;ve presented fair counter-points and handled the discussion respectfully.  I hope I&apos;ve handled my end of the discussion as well as you have.  &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14128212" author="stevel@apache.org" created="Wed, 10 Sep 2014 08:08:15 +0000"  >&lt;ol&gt;
	&lt;li&gt;I have nothing against versioning, but IMO that can be a different JIRA, &quot;add versioning to the libhadoop&quot;. This JIRA is covering linkage errors which we can fix.&lt;/li&gt;
	&lt;li&gt;w.r.t patch 3, the log of exceptions should include the stack trace, perhaps.&lt;/li&gt;
&lt;/ol&gt;


&lt;blockquote&gt;&lt;p&gt;There are a few months until 2.6 will be released-- do we really need to hack this? &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes. Because those of us who have switched our code to only work against branch-2 are the one finding bugs sooner rather than later. If we weren&apos;t, this issue wouldn&apos;t have surfaced until hadoop-2.6 shipped &#8212;at which point the fixes become an even bigger piece of firefighting in a sprint to get 2.6.1 out the door the following week.&lt;/p&gt;

&lt;p&gt;If branch-2 isn&apos;t in a state usable by anyone downstream, it doesn&apos;t get used, regressions don&apos;t get picked up.&lt;/p&gt;

&lt;p&gt;Right now, for us, it isn&apos;t usable &#8212;because we&apos;re the only team that&apos;s tried to deploy HBase 0.98 on a Hadoop 2.6 codebase cluster. &lt;/p&gt;

&lt;p&gt;Furthermore, I don&apos;t think it is &quot;a hack&quot;, it is &quot;retain the entry points which hadoop 2.4 code expect of the native library&quot;&lt;/p&gt;</comment>
                            <comment id="14130426" author="cnauroth" created="Thu, 11 Sep 2014 18:07:41 +0000"  >&lt;p&gt;After further discussion, the following is a summary of potential solutions to this problem:&lt;br/&gt;
1. Freeze the libhadoop.so API forever.&lt;br/&gt;
2. Library versioning plus maintaining a library on the servers for each supported release.&lt;br/&gt;
3. Bundle the .dll or .so file inside a jar somehow so that YARN / Slider can distribute it.&lt;/p&gt;

&lt;p&gt;#1. Advantages:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;???&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;#1. Disadvantages:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;We&apos;re unable to improve libhadoop.so in the future.&lt;/li&gt;
	&lt;li&gt;There will be puzzling interactions when mixing and matching versions.  &quot;New&quot; bugs in libhadoop.so will show up with old hadoop releases, causing confusion in bug trackers.&lt;/li&gt;
	&lt;li&gt;We don&apos;t have any way of enforcing C API stability.  Jenkins doesn&apos;t check for it, most Java programmers don&apos;t know how to achieve it.&lt;/li&gt;
	&lt;li&gt;There is still no ability for applications using new Hadoop versions to make use of old libhadoop.so versions, unless we adopt an even worse compatibility policy that nothing new can be added to libhadoop.so.&lt;/li&gt;
	&lt;li&gt;Given all of the above, this option seems to be off the table.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;#2. Advantages:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Simple to implement.&lt;/li&gt;
	&lt;li&gt;There&apos;s already a patch that implements it.&lt;/li&gt;
	&lt;li&gt;We want libhadoop.so library versioning anyway, even if we later adopt another solution in addition to this&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;#2. Disadvantages:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Admins using Slider / YARN will need to ensure that the appropriate versions of libhadoop are present on the server.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;#3. Advantages:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&quot;Cleanest&quot; solution, since it allows us to reuse YARN&apos;s existing distribution mechanisms.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;#3. Disadvantages:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;There are technical challenges to bundling a library in a jar that we haven&apos;t yet tackled.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Short-term, there is now agreement that the scope of this jira is to restore the 2 function signatures that were removed to unblock downstream projects.&lt;/p&gt;

&lt;p&gt;Colin plans to arrange a conference call for further discussion next week.  The details will be posted here for anyone who wants to join, and we&apos;ll also call wider attention to this issue on the mailing lists.&lt;/p&gt;</comment>
                            <comment id="14134749" author="cmccabe" created="Tue, 16 Sep 2014 00:30:34 +0000"  >&lt;p&gt;This patch adds forwarding methods&lt;/p&gt;</comment>
                            <comment id="14134803" author="hadoopqa" created="Tue, 16 Sep 2014 01:19:04 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12668920/HADOOP-11064.004.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12668920/HADOOP-11064.004.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision 932ae03.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  There were no new javadoc warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 eclipse:eclipse&lt;/font&gt;.  The patch built with eclipse:eclipse.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 findbugs&lt;/font&gt;.  The patch does not introduce any new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests in hadoop-common-project/hadoop-common:&lt;/p&gt;

&lt;p&gt;                  org.apache.hadoop.metrics2.impl.TestMetricsSystemImpl&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 contrib tests&lt;/font&gt;.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4735//testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4735//testReport/&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4735//console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4735//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14135696" author="cnauroth" created="Tue, 16 Sep 2014 16:48:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cmccabe&quot; class=&quot;user-hover&quot; rel=&quot;cmccabe&quot;&gt;cmccabe&lt;/a&gt;, my understanding is that we need to restore &lt;tt&gt;nativeVerifyChunkedSums&lt;/tt&gt;.  (See method signature in Steve&apos;s comment from 05/Sep/14.)&lt;/p&gt;</comment>
                            <comment id="14146780" author="cnauroth" created="Wed, 24 Sep 2014 19:38:25 +0000"  >&lt;p&gt;I went ahead and prepared a patch that restores &lt;tt&gt;nativeVerifyChunkedSums&lt;/tt&gt;, marked as deprecated.  I&apos;m uploading this as v5.  I also added a new test suite, &lt;tt&gt;TestNativeCrc32&lt;/tt&gt;.  The main purpose of this test suite was to exercise the old &lt;tt&gt;nativeVerifyChunkedSums&lt;/tt&gt; code to help protect us from removing it again.  While I was in here, I decided to make it a comprehensive test suite that covers all of the &lt;tt&gt;NativeCrc32&lt;/tt&gt; methods.&lt;/p&gt;</comment>
                            <comment id="14146841" author="hadoopqa" created="Wed, 24 Sep 2014 20:29:49 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12671029/HADOOP-11064.005.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12671029/HADOOP-11064.005.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision 9fa5a89.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 1 new or modified test files.&lt;/p&gt;

&lt;p&gt;      &lt;font color=&quot;red&quot;&gt;-1 javac&lt;/font&gt;.  The applied patch generated 1265 javac compiler warnings (more than the trunk&apos;s current 1263 warnings).&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  There were no new javadoc warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 eclipse:eclipse&lt;/font&gt;.  The patch built with eclipse:eclipse.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in hadoop-common-project/hadoop-common.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 contrib tests&lt;/font&gt;.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4801//testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4801//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4801//artifact/PreCommit-HADOOP-Build-patchprocess/newPatchFindbugsWarningshadoop-common.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4801//artifact/PreCommit-HADOOP-Build-patchprocess/newPatchFindbugsWarningshadoop-common.html&lt;/a&gt;&lt;br/&gt;
Javac warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4801//artifact/PreCommit-HADOOP-Build-patchprocess/diffJavacWarnings.txt&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4801//artifact/PreCommit-HADOOP-Build-patchprocess/diffJavacWarnings.txt&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4801//console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4801//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14146899" author="cnauroth" created="Wed, 24 Sep 2014 21:16:09 +0000"  >&lt;p&gt;Here is patch v6 with 2 lines added to suppress deprecation warnings in the test.  The findbugs warning is unrelated.&lt;/p&gt;</comment>
                            <comment id="14146909" author="cmccabe" created="Wed, 24 Sep 2014 21:23:25 +0000"  >&lt;p&gt;Thank you for taking the lead on this, Chris.  I still find this an unpleasant solution, but it looks like what we&apos;re going to go with for 2.6.  Sorry that I have not had time to call a meeting due to schedule constraints.&lt;/p&gt;

&lt;p&gt;+1 for the patch pending jenkins&lt;/p&gt;</comment>
                            <comment id="14146913" author="cnauroth" created="Wed, 24 Sep 2014 21:24:43 +0000"  >&lt;p&gt;Thanks for the review, Colin.  After I resolve this, I&apos;ll file a separate jira for the follow-ups.  We can resume discussion there.&lt;/p&gt;</comment>
                            <comment id="14146970" author="hadoopqa" created="Wed, 24 Sep 2014 22:19:34 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12671048/HADOOP-11064.006.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12671048/HADOOP-11064.006.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision 9fa5a89.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 tests included&lt;/font&gt;.  The patch appears to include 1 new or modified test files.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  There were no new javadoc warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 eclipse:eclipse&lt;/font&gt;.  The patch built with eclipse:eclipse.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 1 new Findbugs (version 2.0.3) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 core tests&lt;/font&gt;.  The patch failed these unit tests in hadoop-common-project/hadoop-common:&lt;/p&gt;

&lt;p&gt;                  org.apache.hadoop.crypto.random.TestOsSecureRandom&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 contrib tests&lt;/font&gt;.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4803//testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4803//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4803//artifact/PreCommit-HADOOP-Build-patchprocess/newPatchFindbugsWarningshadoop-common.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4803//artifact/PreCommit-HADOOP-Build-patchprocess/newPatchFindbugsWarningshadoop-common.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/4803//console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/4803//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="14146993" author="cnauroth" created="Wed, 24 Sep 2014 22:35:37 +0000"  >&lt;p&gt;I committed this to trunk and branch-2.  Thanks for reviewing the patch, Colin.  Thank you to everyone for the discussion.  I&apos;ll comment back here after I file the follow-up jira, so that everyone is aware.&lt;/p&gt;</comment>
                            <comment id="14147071" author="cnauroth" created="Wed, 24 Sep 2014 23:20:23 +0000"  >&lt;p&gt;I filed &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11127&quot; title=&quot;Improve versioning and compatibility support in native library for downstream hadoop-common users.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11127&quot;&gt;HADOOP-11127&lt;/a&gt; for the follow-up.  I already added anyone who commented on the discussion here as a watcher on the new issue.  If you&apos;ve been silently watching, then you might want to add yourself over there.&lt;/p&gt;</comment>
                            <comment id="14147645" author="hudson" created="Thu, 25 Sep 2014 11:35:31 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-Yarn-trunk #691 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Yarn-trunk/691/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-Yarn-trunk/691/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11064&quot; title=&quot;UnsatisifedLinkError with hadoop 2.4 JARs on hadoop-2.6 due to NativeCRC32 method changes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11064&quot;&gt;&lt;del&gt;HADOOP-11064&lt;/del&gt;&lt;/a&gt;. UnsatisifedLinkError with hadoop 2.4 JARs on hadoop-2.6 due to NativeCRC32 method changes. Contributed by Chris Nauroth. (cnauroth: rev cbf0ae742ae3db964550df11c4044d3e16013959)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestNativeCrc32.java&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/NativeCrc32.c&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/NativeCrc32.java&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14147769" author="hudson" created="Thu, 25 Sep 2014 14:11:52 +0000"  >&lt;p&gt;SUCCESS: Integrated in Hadoop-Hdfs-trunk #1882 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Hdfs-trunk/1882/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-Hdfs-trunk/1882/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11064&quot; title=&quot;UnsatisifedLinkError with hadoop 2.4 JARs on hadoop-2.6 due to NativeCRC32 method changes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11064&quot;&gt;&lt;del&gt;HADOOP-11064&lt;/del&gt;&lt;/a&gt;. UnsatisifedLinkError with hadoop 2.4 JARs on hadoop-2.6 due to NativeCRC32 method changes. Contributed by Chris Nauroth. (cnauroth: rev cbf0ae742ae3db964550df11c4044d3e16013959)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/NativeCrc32.c&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestNativeCrc32.java&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/NativeCrc32.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14147974" author="hudson" created="Thu, 25 Sep 2014 17:03:34 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-Mapreduce-trunk #1907 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1907/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1907/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-11064&quot; title=&quot;UnsatisifedLinkError with hadoop 2.4 JARs on hadoop-2.6 due to NativeCRC32 method changes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-11064&quot;&gt;&lt;del&gt;HADOOP-11064&lt;/del&gt;&lt;/a&gt;. UnsatisifedLinkError with hadoop 2.4 JARs on hadoop-2.6 due to NativeCRC32 method changes. Contributed by Chris Nauroth. (cnauroth: rev cbf0ae742ae3db964550df11c4044d3e16013959)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/util/NativeCrc32.java&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/util/TestNativeCrc32.java&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;hadoop-common-project/hadoop-common/src/main/native/src/org/apache/hadoop/util/NativeCrc32.c&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12743886">HADOOP-11127</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12905590">HADOOP-12488</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310050">
                    <name>Regression</name>
                                                                <inwardlinks description="is broken by">
                                        <issuelink>
            <issuekey id="12722345">HADOOP-10975</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12560405">HDFS-3528</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12667284" name="HADOOP-11064.001.patch" size="4376" author="cmccabe" created="Mon, 8 Sep 2014 23:48:46 +0000"/>
                            <attachment id="12667487" name="HADOOP-11064.002.patch" size="5531" author="cnauroth" created="Tue, 9 Sep 2014 21:13:40 +0000"/>
                            <attachment id="12667506" name="HADOOP-11064.003.patch" size="5106" author="cnauroth" created="Tue, 9 Sep 2014 21:55:44 +0000"/>
                            <attachment id="12668920" name="HADOOP-11064.004.patch" size="5253" author="cmccabe" created="Tue, 16 Sep 2014 00:30:34 +0000"/>
                            <attachment id="12671029" name="HADOOP-11064.005.patch" size="12319" author="cnauroth" created="Wed, 24 Sep 2014 19:38:25 +0000"/>
                            <attachment id="12671048" name="HADOOP-11064.006.patch" size="12391" author="cnauroth" created="Wed, 24 Sep 2014 21:16:09 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>6.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 8 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1zpvj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>