<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 21:42:12 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HADOOP-9103] UTF8 class does not properly decode Unicode characters outside the basic multilingual plane</title>
                <link>https://issues.apache.org/jira/browse/HADOOP-9103</link>
                <project id="12310240" key="HADOOP">Hadoop Common</project>
                    <description>&lt;p&gt;this the log information  of the  exception  from the SecondaryNameNode: &lt;br/&gt;
2012-03-28 00:48:42,553 ERROR org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode: java.io.IOException: Found lease for&lt;br/&gt;
 non-existent file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/??&lt;cite&gt;@&lt;/cite&gt;?????????????&lt;br/&gt;
??????????tor.qzone.qq.com/keypart-00174&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFilesUnderConstruction(FSImage.java:1211)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:959)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.doMerge(SecondaryNameNode.java:589)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode$CheckpointStorage.access$000(SecondaryNameNode.java:473)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doMerge(SecondaryNameNode.java:350)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doCheckpoint(SecondaryNameNode.java:314)&lt;br/&gt;
        at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:225)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:619)&lt;/p&gt;

&lt;p&gt;this is the log information  about the file from namenode:&lt;br/&gt;
2012-03-28 00:32:26,528 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=boss,boss	ip=/10.131.16.34	cmd=create	src=/user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174	dst=null	perm=boss:boss:rw-r-&lt;del&gt;r&lt;/del&gt;-&lt;br/&gt;
2012-03-28 00:37:42,387 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* NameSystem.allocateBlock: /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174. blk_2751836614265659170_184668759&lt;br/&gt;
2012-03-28 00:37:42,696 INFO org.apache.hadoop.hdfs.StateChange: DIR* NameSystem.completeFile: file /user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174 is closed by DFSClient_attempt_201203271849_0016_r_000174_0&lt;br/&gt;
2012-03-28 00:37:50,315 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=boss,boss	ip=/10.131.16.34	cmd=rename	src=/user/boss/pgv/fission/task16/split/_temporary/_attempt_201203271849_0016_r_000174_0/  @?            tor.qzone.qq.com/keypart-00174	dst=/user/boss/pgv/fission/task16/split/  @?            tor.qzone.qq.com/keypart-00174	perm=boss:boss:rw-r-&lt;del&gt;r&lt;/del&gt;-&lt;/p&gt;


&lt;p&gt;after check the code that save FSImage,I found there are a problem that maybe a bug of HDFS Code,I past below:&lt;br/&gt;
------------&lt;del&gt;this is the saveFSImage method  in  FSImage.java, I make some mark at the problem code&lt;/del&gt;-----------&lt;/p&gt;

&lt;p&gt;/**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Save the contents of the FS image to the file.&lt;br/&gt;
   */&lt;br/&gt;
  void saveFSImage(File newFile) throws IOException {&lt;br/&gt;
    FSNamesystem fsNamesys = FSNamesystem.getFSNamesystem();&lt;br/&gt;
    FSDirectory fsDir = fsNamesys.dir;&lt;br/&gt;
    long startTime = FSNamesystem.now();&lt;br/&gt;
    //&lt;br/&gt;
    // Write out data&lt;br/&gt;
    //&lt;br/&gt;
    DataOutputStream out = new DataOutputStream(&lt;br/&gt;
                                                new BufferedOutputStream(&lt;br/&gt;
                                                                         new FileOutputStream(newFile)));&lt;br/&gt;
    try 
{
      .........
    
      // save the rest of the nodes
      saveImage(strbuf, 0, fsDir.rootDir, out);------------------problem
      fsNamesys.saveFilesUnderConstruction(out);------------------problem  detail is below
      strbuf = null;
    }
&lt;p&gt; finally &lt;/p&gt;
{
      out.close();
    }&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    LOG.info(&quot;Image file of size &quot; + newFile.length() + &quot; saved in &quot; &lt;br/&gt;
        + (FSNamesystem.now() - startTime)/1000 + &quot; seconds.&quot;);&lt;br/&gt;
  }&lt;/p&gt;

&lt;p&gt; /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Save file tree image starting from the given root.&lt;/li&gt;
	&lt;li&gt;This is a recursive procedure, which first saves all children of&lt;/li&gt;
	&lt;li&gt;a current directory and then moves inside the sub-directories.&lt;br/&gt;
   */&lt;br/&gt;
  private static void saveImage(ByteBuffer parentPrefix,&lt;br/&gt;
                                int prefixLength,&lt;br/&gt;
                                INodeDirectory current,&lt;br/&gt;
                                DataOutputStream out) throws IOException 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    int newPrefixLength = prefixLength;    if (current.getChildrenRaw() == null)      return;    for(INode child }&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt; // Helper function that writes an INodeUnderConstruction&lt;br/&gt;
  // into the input stream&lt;br/&gt;
  //&lt;br/&gt;
  static void writeINodeUnderConstruction(DataOutputStream out,&lt;br/&gt;
                                           INodeFileUnderConstruction cons,&lt;br/&gt;
                                           String path) &lt;br/&gt;
                                           throws IOException &lt;/p&gt;
{
    writeString(path, out);------------------problem
    ..........
  }

&lt;p&gt;  static private final UTF8 U_STR = new UTF8();&lt;br/&gt;
  static void writeString(String str, DataOutputStream out) throws IOException &lt;/p&gt;
{
    U_STR.set(str);
    U_STR.write(out);------------------problem 
  }

&lt;p&gt;  /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Converts a string to a byte array using UTF8 encoding.&lt;br/&gt;
   */&lt;br/&gt;
  static byte[] string2Bytes(String str) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    try {
      return str.getBytes(&quot;UTF8&quot;);------------------problem 
    } catch(UnsupportedEncodingException e) {
      assert false : &quot;UTF8 encoding is not supported &quot;;
    }    return null;  }&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;-----------------------------------------&lt;del&gt;below is the explain&lt;/del&gt;-----------------------&lt;br/&gt;
in  saveImage method:  child.getLocalNameBytes(),the  bytes use the method of str.getBytes(&quot;UTF8&quot;);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;but in writeINodeUnderConstruction, the bytes user the method of Class  UTF8 to get the bytes.&lt;/p&gt;

&lt;p&gt;I make a test use our messy code file name , found the the two bytes arrsy are not equal. so I both use the class UTF8,then the problem desappare.&lt;/p&gt;

&lt;p&gt;I think this is a bug of HDFS or UTF8.&lt;/p&gt;</description>
                <environment>&lt;p&gt;SUSE LINUX&lt;/p&gt;</environment>
        <key id="12551704">HADOOP-9103</key>
            <summary>UTF8 class does not properly decode Unicode characters outside the basic multilingual plane</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tlipcon">Todd Lipcon</assignee>
                                    <reporter username="yixiaohuamaxfly">yixiaohua</reporter>
                        <labels>
                    </labels>
                <created>Fri, 20 Apr 2012 01:07:25 +0000</created>
                <updated>Wed, 3 Sep 2014 23:11:05 +0000</updated>
                            <resolved>Wed, 5 Dec 2012 21:14:28 +0000</resolved>
                                    <version>0.20.1</version>
                                    <fixVersion>2.0.3-alpha</fixVersion>
                                    <component>io</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                    <workratio workratioPercent="0"/>
                                    <progress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="0">
                                    <originalProgress>
                                                    <row percentage="100" backgroundColor="#89afd7"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="0" backgroundColor="#51a825"/>
                                                    <row percentage="100" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                    <timeoriginalestimate seconds="43200">12h</timeoriginalestimate>
                            <timeestimate seconds="43200">12h</timeestimate>
                                        <comments>
                            <comment id="13258048" author="yixiaohuamaxfly" created="Fri, 20 Apr 2012 06:38:41 +0000"  >&lt;p&gt;I  only change the writeString method  to make it  the same with INode  stringToBytes method,and then the problem disappears.&lt;/p&gt;</comment>
                            <comment id="13258331" author="tlipcon" created="Fri, 20 Apr 2012 15:51:24 +0000"  >&lt;p&gt;Rather than change the code to not use UTF8, I think we should figure out why the UTF8 writeString function is writing the wrong data. Is &quot;&#20081;&#30721;&quot; the string that causes the problem? I tried to reproduce using this string, but it works fine here.&lt;/p&gt;

&lt;p&gt;(I did &quot;hadoop fs -put /etc/issue &apos;&#20081;&#30721;&apos;&quot;, then successfully restarted and catted the file)&lt;/p&gt;</comment>
                            <comment id="13259396" author="yixiaohuamaxfly" created="Mon, 23 Apr 2012 05:45:15 +0000"  >&lt;p&gt;dear  todd:&lt;br/&gt;
   &quot;&#20081;&#30721;&quot; is not the string that causes the problem,it is chinese I don&apos;t how do describe, I has place the string that causes the  problem and the test code  in attachments . wish for your reply,  best wishes!&lt;/p&gt;</comment>
                            <comment id="13259400" author="yixiaohuamaxfly" created="Mon, 23 Apr 2012 05:50:17 +0000"  >&lt;p&gt;I am try to figure out the problem of UTF8 &lt;sub&gt;_&lt;/sub&gt;&lt;/p&gt;</comment>
                            <comment id="13259527" author="hadoopqa" created="Mon, 23 Apr 2012 11:05:45 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12523758/TestUTF8AndStringGetBytes.java&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12523758/TestUTF8AndStringGetBytes.java&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    -1 patch.  The patch command could not apply the patch.&lt;/p&gt;

&lt;p&gt;Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HDFS-Build/2312//console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HDFS-Build/2312//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13506105" author="cmccabe" created="Thu, 29 Nov 2012 01:05:36 +0000"  >&lt;p&gt;I ran TestUTF8AndStringGetBytes.java with the provided ProblemString.txt and got this output:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;49
49
30
30
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This seems expected, however-- the &lt;tt&gt;String#getBytes&lt;/tt&gt; method returns UTF-8 with the non-BMP code points encoded using UTF-8.  &lt;tt&gt;hadoop.io.UTF8&lt;/tt&gt; returns UTF-8 with the non-BMP code points encoded as supplementary pairs.&lt;/p&gt;

&lt;p&gt;I don&apos;t see what any of this has to do with the FSImage or block leases-- since we always encode/decode using &lt;tt&gt;hadoop.io.UTF8&lt;/tt&gt;, and never anything else, there should be no problem.&lt;/p&gt;</comment>
                            <comment id="13506193" author="tlipcon" created="Thu, 29 Nov 2012 03:56:48 +0000"  >&lt;p&gt;We had a customer run into a similar issue today, and I figured out the problem &amp;#8211; UTF8.readChars() only handles UTF8 sequences up to 3 bytes. After some spelunking, Colin, Andy, and I came to the following conclusions with regard to characters outside the basic multi-lingual plane. Given the unicode character &#128049; (CAT-FACE U+1F431 &lt;a href=&quot;http://www.fileformat.info/info/unicode/char/1f431/index.htm):&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.fileformat.info/info/unicode/char/1f431/index.htm):&lt;/a&gt;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;DFSUtil.string2Bytes calls through to Java&apos;s encoding, which yields the following 4-byte sequence: f09f90b1&lt;/li&gt;
	&lt;li&gt;UTF8.writeString does our own encoding, which yields the following 6-byte sequence: eda0bdedb0b1&lt;/li&gt;
	&lt;li&gt;If you read back the 6-byte sequence using new String(bytes, &quot;UTF-8&quot;), it properly decodes back into the cat-face&lt;/li&gt;
	&lt;li&gt;If you read back the 6-byte sequence using UTF8.readChars, it also gets back the cat-face&lt;/li&gt;
	&lt;li&gt;If you read back the 4-byte (proper UTF8) sequence using UTF8.readChars, you get back some nonsense (&quot;&#2000;&#1136;&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It turns out that UTF8.java doesn&apos;t actually encode in UTF8 - but rather it encodes in &quot;CESU-8&quot; (thanks Colin for finding this!). Both Java and UTF8.java can properly decode that back into the correct String. However, if you encode a non-BMP character using Java (into proper UTF8 instead of CESU-8), then UTF8.java can&apos;t properly decode it.&lt;/p&gt;

&lt;p&gt;The issue is that the code in UTF8.readChars doesn&apos;t handle any UTF8 sequences longer than 3 bytes. By extending the code to handle longer byte sequences, we were able to fix the issue.&lt;/p&gt;

&lt;p&gt;I&apos;ll upload a patch with a unit test either later tonight or tomorrow.&lt;/p&gt;</comment>
                            <comment id="13506232" author="yixiaohuamaxfly" created="Thu, 29 Nov 2012 05:38:13 +0000"  >&lt;p&gt;Todd thank you for your reply,you are so kind.&lt;/p&gt;</comment>
                            <comment id="13506254" author="tlipcon" created="Thu, 29 Nov 2012 06:28:22 +0000"  >&lt;p&gt;Attached patch should fix this issue. I also amended the javadoc for UTF8 to indicate that it encodes CESU-8&lt;/p&gt;</comment>
                            <comment id="13506325" author="cmccabe" created="Thu, 29 Nov 2012 08:39:55 +0000"  >&lt;p&gt;I said:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;since we always encode/decode using hadoop.io.UTF8, and never anything else, there should be no problem...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I take this back; looks like we don&apos;t always encode/decode using &lt;tt&gt;hadoop.io.UTF8&lt;/tt&gt;.  D&apos;oh!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Attached patch should fix this issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Nice.  Should we test for rejecting 5-byte and 6-byte sequences, since I notice you added some code to do that?&lt;/p&gt;

&lt;p&gt;I&apos;m also a little scared by the idea that we have differently-encoded byte[] running around for the same file name strings.  We have to be very careful about this.  Unfortunately, we can&apos;t change the decoder to emit real UTF-8 (rather than CESU-8) without making a backwards-incompatible change, since as INode.java reminds us, &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;   *  The name in HdfsFileStatus should keep the same encoding as &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.
   *  &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; encoding is changed, implicitly getFileInfo and listStatus in
   *  clientProtocol are changed; The decoding at the client
   *  side should change accordingly.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I also wonder if this means that we need to hunt down all the places not using CESU-8.  Otherwise older clients are just not going to work with astral plane code points, even after this fix... However, we could do that in a separate JIRA, not here.&lt;/p&gt;</comment>
                            <comment id="13506727" author="tlipcon" created="Thu, 29 Nov 2012 19:52:19 +0000"  >&lt;blockquote&gt;&lt;p&gt;Nice. Should we test for rejecting 5-byte and 6-byte sequences, since I notice you added some code to do that?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I added a test for an invalid sequence. I didn&apos;t think it was necessary to add a separate test for a 5-byte sequence, since it would trigger the same &quot;invalid&quot; code path. Got an example hex sequence you think we should test against?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m also a little scared by the idea that we have differently-encoded byte[] running around for the same file name strings. We have to be very careful about this. &lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;...However, we could do that in a separate JIRA, not here&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Agreed. Let&apos;s open a separate HDFS JIRA and use this for the Common-side fix. This patch alone was enough to successfully restart a NN which had an open file with a 4-byte codepoint.&lt;/p&gt;</comment>
                            <comment id="13506855" author="cmccabe" created="Thu, 29 Nov 2012 21:52:19 +0000"  >&lt;blockquote&gt;&lt;p&gt;Got an example hex sequence you think we should test against?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Here is a 5-byte sequence that used to be valid UTF-8, before the 4-byte max rule was put into place:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;0xF8 0x88 0x80 0x80 0x80&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Source: &lt;a href=&quot;http://www.cl.cam.ac.uk/~mgk25/ucs/examples/UTF-8-test.txt&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.cl.cam.ac.uk/~mgk25/ucs/examples/UTF-8-test.txt&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13506865" author="tlipcon" created="Thu, 29 Nov 2012 22:08:44 +0000"  >&lt;p&gt;Attached patch includes the test sequence Colin provided above.&lt;/p&gt;</comment>
                            <comment id="13506894" author="adi2" created="Thu, 29 Nov 2012 22:35:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;+   * This is a regression est for &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9103&quot; title=&quot;UTF8 class does not properly decode Unicode characters outside the basic multilingual plane&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9103&quot;&gt;&lt;del&gt;HDFS-3307&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;test, not est.  Since this jira has moved to &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9103&quot; title=&quot;UTF8 class does not properly decode Unicode characters outside the basic multilingual plane&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9103&quot;&gt;&lt;del&gt;HADOOP-9103&lt;/del&gt;&lt;/a&gt;, update the reference.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;+ * Note that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; decodes UTF-8 but actually encodes CESU-8, a variant of
+ * UTF-8: see http:&lt;span class=&quot;code-comment&quot;&gt;//en.wikipedia.org/wiki/CESU-8&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Rather than adding a comment saying &quot;this code is buggy&quot;, how about we fix the bug?  Outputting proper 4-byte UTF8 sequences for a given UTF-16 surrogate pair is a much better solution than the current behavior.&lt;/p&gt;

&lt;p&gt;So as far as it goes the patch looks good.  I&apos;ll look into the surrogate pair stuff.&lt;/p&gt;</comment>
                            <comment id="13506912" author="cmccabe" created="Thu, 29 Nov 2012 22:58:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;Rather than adding a comment saying &quot;this code is buggy&quot;, how about we fix the bug? Outputting proper 4-byte UTF8 sequences for a given UTF-16 surrogate pair is a much better solution than the current behavior.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That would be an incompatible change.  Consider what happens when the server hands back 4-byte UTF-8 sequences to existing DFSClients.  Boom, they fall over.&lt;/p&gt;</comment>
                            <comment id="13506932" author="tlipcon" created="Thu, 29 Nov 2012 23:25:09 +0000"  >&lt;p&gt;Fixed typo in the test javadoc&lt;/p&gt;</comment>
                            <comment id="13506933" author="tlipcon" created="Thu, 29 Nov 2012 23:26:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;Rather than adding a comment saying &quot;this code is buggy&quot;, how about we fix the bug? Outputting proper 4-byte UTF8 sequences for a given UTF-16 surrogate pair is a much better solution than the current behavior.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s not &quot;buggy&quot; it&apos;s just &quot;different&quot; (reminds me of something my elementary school teachers used to say). But on a serious note, yea, what Colin said above &amp;#8211; it could break existing clients of the code who are using the old code to &lt;em&gt;decode&lt;/em&gt;, and were relying on the fact that we are able to round-trip non-BMP characters through UTF8.java.&lt;/p&gt;
</comment>
                            <comment id="13507039" author="adi2" created="Fri, 30 Nov 2012 02:07:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;It&apos;s not &quot;buggy&quot; it&apos;s just &quot;different&quot; &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s buggy if we ever end up writing a CESU-8 bytestream where someone else expects UTF-8.  For example, &lt;tt&gt;dfs -ls&lt;/tt&gt; writing CESU-8 to stdout wouldn&apos;t work properly, because other programs such as &lt;tt&gt;xterm&lt;/tt&gt; or &lt;tt&gt;putty&lt;/tt&gt; don&apos;t implement the CESU-8 decoding rules.  (This example doesn&apos;t happen currently, because the CESU-8 filename is deserialized into a String, where it&apos;s interpreted as a surrogate pair, which is then written, and the correct surrogate pair -&amp;gt; UTF-8 encoding happens on the output side.)  Hopefully we haven&apos;t overlooked any such existing bugs and nobody accidentally uses UTF8.java in the future.  (At least it&apos;s marked @Deprecated.)&lt;/p&gt;

&lt;p&gt;Agreed that as long as UTF8.java is the thing that reads the bytestream, we can continue to implement CESU-8 and it can remain partially backwards compatible with previous versions of UTF8.java.&lt;/p&gt;</comment>
                            <comment id="13509404" author="tlipcon" created="Tue, 4 Dec 2012 01:11:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;Hopefully we haven&apos;t overlooked any such existing bugs and nobody accidentally uses UTF8.java in the future. (At least it&apos;s marked @Deprecated.)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right. That&apos;s my thinking.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Agreed that as long as UTF8.java is the thing that reads the bytestream, we can continue to implement CESU-8 and it can remain partially backwards compatible with previous versions of UTF8.java.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Java String can also properly decode the CESU-8, so we can move forward by getting rid of usage of UTF8.readString ASAP, and then some day later kill all the paths that use the write path (once we&apos;re sure the read path is dead).&lt;/p&gt;

&lt;p&gt;Could a committer please review this? Thanks.&lt;/p&gt;</comment>
                            <comment id="13510119" author="atm" created="Tue, 4 Dec 2012 23:32:23 +0000"  >&lt;p&gt;+1, the patch looks good to me.&lt;/p&gt;

&lt;p&gt;Thanks a lot for looking into this tricky issue, Todd, Colin, and Andy.&lt;/p&gt;</comment>
                            <comment id="13510773" author="tlipcon" created="Wed, 5 Dec 2012 21:14:28 +0000"  >&lt;p&gt;Committed to trunk and branch-2. Thanks for reporting this, yixiaohua, and thanks everyone for the reviews.&lt;/p&gt;</comment>
                            <comment id="13510904" author="hudson" created="Wed, 5 Dec 2012 23:28:37 +0000"  >&lt;p&gt;Integrated in Hadoop-trunk-Commit #3088 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-trunk-Commit/3088/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-trunk-Commit/3088/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9103&quot; title=&quot;UTF8 class does not properly decode Unicode characters outside the basic multilingual plane&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9103&quot;&gt;&lt;del&gt;HADOOP-9103&lt;/del&gt;&lt;/a&gt;. UTF8 class does not properly decode Unicode characters outside the basic multilingual plane. Contributed by Todd Lipcon. (Revision 1417649)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
todd : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1417649&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1417649&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/UTF8.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestUTF8.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13511322" author="hudson" created="Thu, 6 Dec 2012 13:10:37 +0000"  >&lt;p&gt;Integrated in Hadoop-Hdfs-trunk #1246 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Hdfs-trunk/1246/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-Hdfs-trunk/1246/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9103&quot; title=&quot;UTF8 class does not properly decode Unicode characters outside the basic multilingual plane&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9103&quot;&gt;&lt;del&gt;HADOOP-9103&lt;/del&gt;&lt;/a&gt;. UTF8 class does not properly decode Unicode characters outside the basic multilingual plane. Contributed by Todd Lipcon. (Revision 1417649)&lt;/p&gt;

&lt;p&gt;     Result = FAILURE&lt;br/&gt;
todd : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1417649&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1417649&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/UTF8.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestUTF8.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13511391" author="hudson" created="Thu, 6 Dec 2012 14:05:56 +0000"  >&lt;p&gt;Integrated in Hadoop-Mapreduce-trunk #1277 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1277/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1277/&lt;/a&gt;)&lt;br/&gt;
    &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9103&quot; title=&quot;UTF8 class does not properly decode Unicode characters outside the basic multilingual plane&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9103&quot;&gt;&lt;del&gt;HADOOP-9103&lt;/del&gt;&lt;/a&gt;. UTF8 class does not properly decode Unicode characters outside the basic multilingual plane. Contributed by Todd Lipcon. (Revision 1417649)&lt;/p&gt;

&lt;p&gt;     Result = SUCCESS&lt;br/&gt;
todd : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1417649&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1417649&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/UTF8.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/TestUTF8.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12646011">HADOOP-9544</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310050">
                    <name>Regression</name>
                                            <outwardlinks description="breaks">
                                        <issuelink>
            <issuekey id="12618994">HDFS-4282</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12523457" name="FSImage.java" size="3029" author="yixiaohuamaxfly" created="Fri, 20 Apr 2012 06:38:40 +0000"/>
                            <attachment id="12523756" name="ProblemString.txt" size="81" author="yixiaohuamaxfly" created="Mon, 23 Apr 2012 05:44:57 +0000"/>
                            <attachment id="12523758" name="TestUTF8AndStringGetBytes.java" size="755" author="yixiaohuamaxfly" created="Mon, 23 Apr 2012 05:48:35 +0000"/>
                            <attachment id="12523755" name="TestUTF8AndStringGetBytes.java" size="743" author="yixiaohuamaxfly" created="Mon, 23 Apr 2012 05:44:56 +0000"/>
                            <attachment id="12555431" name="hadoop-9103.txt" size="6280" author="tlipcon" created="Thu, 29 Nov 2012 23:25:09 +0000"/>
                            <attachment id="12555414" name="hadoop-9103.txt" size="6277" author="tlipcon" created="Thu, 29 Nov 2012 22:08:44 +0000"/>
                            <attachment id="12555322" name="hadoop-9103.txt" size="5763" author="tlipcon" created="Thu, 29 Nov 2012 06:28:22 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>236511</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 50 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0jwbz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>114168</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12310230" key="com.atlassian.jira.plugin.system.customfieldtypes:textfield">
                        <customfieldname>Tags</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>messy code,SecondaryNameNode,FSImage </customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>