<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 21:57:31 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HADOOP-18159] Certificate doesn&apos;t match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]</title>
                <link>https://issues.apache.org/jira/browse/HADOOP-18159</link>
                <project id="12310240" key="HADOOP">Hadoop Common</project>
                    <description>&lt;h2&gt;&lt;a name=&quot;Ifyouseethiserrormessagewhentryingtouses3a%3A%2F%2Forgs%3A%2F%2FURLs%2Clookforcopiesofcosapibundle.jaronyourclasspathandremovethem.&quot;&gt;&lt;/a&gt;If you see this error message when trying to use s3a:// or gs:// URLs, look for copies of cos_api-bundle.jar on your classpath and remove them.&lt;/h2&gt;

&lt;p&gt;Libraries which include shaded apache httpclient libraries (hadoop-client-runtime.jar, aws-java-sdk-bundle.jar, gcs-connector-shaded.jar, cos_api-bundle.jar) all load and use the unshaded resource mozilla/public-suffix-list.txt. If an out of date version of this is found on the classpath first, attempts to negotiate TLS connections may fail with the error &quot;Certificate doesn&apos;t match any of the subject alternative names&quot;. &lt;/p&gt;

&lt;p&gt;In a hadoop installation, you can use the findclass tool to track down where the public-suffix-list.txt is coming from.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
hadoop org.apache.hadoop.util.FindClass locate mozilla/&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt;-suffix-list.txt
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So far, the cos_api-bundle-5.6.19.jar appears to be the source of this problem.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2&gt;&lt;a name=&quot;bugreport&quot;&gt;&lt;/a&gt;bug report&lt;/h2&gt;


&lt;p&gt;Trying to run any job after bumping our Spark version (which is now using Hadoop 3.3.1), lead us to the current exception while reading files on s3:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a:&lt;span class=&quot;code-comment&quot;&gt;//&amp;lt;bucket&amp;gt;/&amp;lt;path&amp;gt;.parquet: com.amazonaws.SdkClientException: Unable to execute HTTP request: Certificate &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &amp;lt;bucket.s3.amazonaws.com&amp;gt; doesn&lt;span class=&quot;code-quote&quot;&gt;&apos;t match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]: Unable to execute HTTP request: Certificate &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &amp;lt;bucket&amp;gt; doesn&apos;&lt;/span&gt;t match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com] at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:208) at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170) at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3351) at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185) at org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4277) at &lt;/span&gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Caused by: javax.net.ssl.SSLPeerUnverifiedException: Certificate &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &amp;lt;bucket.s3.amazonaws.com&amp;gt; doesn&apos;t match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]
		at com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.verifyHostname(SSLConnectionSocketFactory.java:507)
		at com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.createLayeredSocket(SSLConnectionSocketFactory.java:437)
		at com.amazonaws.thirdparty.apache.http.conn.ssl.SSLConnectionSocketFactory.connectSocket(SSLConnectionSocketFactory.java:384)
		at com.amazonaws.thirdparty.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:142)
		at com.amazonaws.thirdparty.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:376)
		at sun.reflect.GeneratedMethodAccessor36.invoke(Unknown Source)
		at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
		at java.lang.reflect.Method.invoke(Method.java:498)
		at com.amazonaws.http.conn.ClientConnectionManagerFactory$Handler.invoke(ClientConnectionManagerFactory.java:76)
		at com.amazonaws.http.conn.$Proxy16.connect(Unknown Source)
		at com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:393)
		at com.amazonaws.thirdparty.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)
		at com.amazonaws.thirdparty.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:186)
		at com.amazonaws.thirdparty.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:185)
		at com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:83)
		at com.amazonaws.thirdparty.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:56)
		at com.amazonaws.http.apache.client.impl.SdkHttpClient.execute(SdkHttpClient.java:72)
		at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1333)
		at com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1145)  &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;We found similar problems in the following tickets but:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-17017&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-17017&lt;/a&gt;&#160;(we don&apos;t use `.` in our bucket names)&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/aws/aws-sdk-java-v2/issues/1786&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/aws/aws-sdk-java-v2/issues/1786&lt;/a&gt; (we tried to override it by using `httpclient:4.5.10` or `httpclient:4.5.8`, with no effect).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We couldn&apos;t test it using the native `openssl` configuration due to our setup, so we would like to stick with the java ssl implementation, if possible.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment>&lt;p&gt;hadoop 3.3.1&lt;/p&gt;

&lt;p&gt;httpclient 4.5.13&lt;/p&gt;

&lt;p&gt;JDK8&lt;/p&gt;</environment>
        <key id="13434074">HADOOP-18159</key>
            <summary>Certificate doesn&apos;t match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="andre.amorimfonseca@gmail.com">Andr&#233; F.</assignee>
                                    <reporter username="andre.amorimfonseca@gmail.com">Andr&#233; F.</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Wed, 16 Mar 2022 10:37:46 +0000</created>
                <updated>Mon, 12 Feb 2024 07:07:05 +0000</updated>
                            <resolved>Tue, 21 Jun 2022 19:17:36 +0000</resolved>
                                    <version>3.3.1</version>
                    <version>3.3.2</version>
                    <version>3.3.3</version>
                                    <fixVersion>3.4.0</fixVersion>
                    <fixVersion>3.3.5</fixVersion>
                                    <component>fs/s3</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="1800">0.5h</timespent>
                                <comments>
                            <comment id="17517566" author="stevel@apache.org" created="Tue, 5 Apr 2022 16:54:33 +0000"  >&lt;p&gt;so we&apos;ve come across something really odd related to this. do you have the hadoop-cos module in your tools/lib dir?&lt;/p&gt;

&lt;p&gt;if so, what happens if you remove this?&lt;/p&gt;</comment>
                            <comment id="17522175" author="JIRAUSER285813" created="Thu, 14 Apr 2022 09:05:32 +0000"  >&lt;p&gt;I tried removing it from the classpath, but I&apos;m getting the same issue&lt;/p&gt;</comment>
                            <comment id="17522214" author="stevel@apache.org" created="Thu, 14 Apr 2022 10:12:56 +0000"  >&lt;p&gt;hmm.&lt;br/&gt;
can you &lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;grab the hadoop 3.3.2 distro, untar it, set up a core-site.xml to use openssl? this won&apos;t affect the rest of your deployment&lt;/li&gt;
	&lt;li&gt;download the cloudstore jar, and run its storediag command against the bucket, and see what it says &lt;a href=&quot;https://github.com/steveloughran/cloudstore&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/steveloughran/cloudstore&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;recent releases add some TLS diagnostics&lt;/p&gt;
</comment>
                            <comment id="17554528" author="JIRAUSER285813" created="Wed, 15 Jun 2022 11:30:11 +0000"  >&lt;p&gt;Hello again, after coming back to this problem, I think I found the root cause of the issue.&lt;/p&gt;

&lt;p&gt;Indeed `hadoop-cos` is somehow related to the problem. On Spark 3.2 It is bringing `cos_api-bundle-5.6.19.jar` to the classpath and this jar contains an outdated `public-suffix-list.txt` file, with the following contents:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// Amazon S3 : https://aws.amazon.com/s3/
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// Submitted by Luke Wells &amp;lt;psl-maintainers@amazon.com&amp;gt;
&lt;/span&gt;*.s3.amazonaws.com
s3-ap-northeast-1.amazonaws.com
s3-ap-northeast-2.amazonaws.com
s3-ap-south-1.amazonaws.com &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The file which the default hostname verifier should be using should come from `hadoop-client-runtime-3.3.1.jar`, which has the following contents:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// Amazon S3 : https://aws.amazon.com/s3/
&lt;/span&gt;&lt;span class=&quot;code-comment&quot;&gt;// Submitted by Luke Wells &amp;lt;psl-maintainers@amazon.com&amp;gt;
&lt;/span&gt;s3.amazonaws.com
s3-ap-northeast-1.amazonaws.com
s3-ap-northeast-2.amazonaws.com
s3-ap-south-1.amazonaws.com
s3-ap-southeast-1.amazonaws.com
s3-ap-southeast-2.amazonaws.com
s3-ca-central-1.amazonaws.com
s3-eu-central-1.amazonaws.com
... (all regional endpoints) &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;For some reason, the `public-suffix-list.txt` was picked from `cos_api-bundle-5.6.19.jar` on my classpath, not from `hadoop-client-runtime-3.3.1.jar`.&lt;/p&gt;

&lt;p&gt;Replacing `cos_api-bundle-5.6.19.jar` with a more up-to-date version (`cos_api-bundle-5.6.69.jar`) fixed this issue (since it has the same `public-suffix-list.txt` than in `hadoop-client-runtime-3.3.1.jar`). I created the following PR to bump this library on &lt;a href=&quot;https://github.com/apache/hadoop/pull/4444&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hadoop/pull/4444&lt;/a&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17554662" author="stevel@apache.org" created="Wed, 15 Jun 2022 15:29:51 +0000"  >&lt;p&gt;oh, this is bad. i didn&apos;t know about that file. now I will fear it. &lt;/p&gt;

&lt;p&gt;1. we should modify storediag to find the file and print it&lt;br/&gt;
2. wonder if we should block jars from adding this if we don&apos;t expect&lt;/p&gt;</comment>
                            <comment id="17554677" author="JIRAUSER285813" created="Wed, 15 Jun 2022 15:46:30 +0000"  >&lt;p&gt;I think another good strategy, if this is possible, would be enforcing that this file is always loaded from a specific dependency on the classpath. Maybe adding it to the aws-sdk or httpclient and shading&#160; with another name (since this is where the default hostname verifier comes from), could protect us from any issue.&lt;/p&gt;</comment>
                            <comment id="17554753" author="stevel@apache.org" created="Wed, 15 Jun 2022 18:50:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think another good strategy, if this is possible, would be enforcing that this file is always loaded from a specific dependency on the classpath. Maybe adding it to the aws-sdk or httpclient and shading  with another name (since this is where the default hostname verifier comes from), could protect us from any issu&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;d say yes, except for the &quot;if this is possible&quot; clause, which i now doubt&lt;/p&gt;

&lt;p&gt;in a hadoop installation, try using findclass (a little backdoor diagnostics command) to see where this is being picked up&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
hadoop org.apache.hadoop.util.FindClass locate mozilla/&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt;-suffix-list.tx
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
cdh6.3                 common/lib/httpclient-4.5.3.jar
hadoop-3.4.0-SNAPSHOT  common/lib/aws-java-sdk-bundle-1.12.132.jar!/mozilla/&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt;-suffix-list.txt
hadoop 3.3.3           common/lib/httpclient-4.5.13.jar!/mozilla/&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt;-suffix-list.txt
cdh 7.1.x              common/lib/gcs-connector-2.1.2.7.1.8.0-SNAPSHOT-shaded.jar!/mozilla/&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt;-suffix-list.txt
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;(why yes, i do have a lot of releases on my laptop...)&lt;/p&gt;

&lt;p&gt;anyway, the location jitters depending on classes in the cp. as even those nominally shaded binaries (aws, gcs) depend on the same location of the text file.&lt;/p&gt;

&lt;p&gt;This is &lt;b&gt;not good&lt;/b&gt;.&lt;/p&gt;


&lt;p&gt;I&apos;ve just updated cloudstore to include the location of this in its diagnostics. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/steveloughran/cloudstore/releases/tag/tag-2022-06-15-release-public-suffix-llist&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/steveloughran/cloudstore/releases/tag/tag-2022-06-15-release-public-suffix-llist&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;we will see this again. probably just luck so far.&lt;/p&gt;</comment>
                            <comment id="17556924" author="stevel@apache.org" created="Tue, 21 Jun 2022 14:10:41 +0000"  >&lt;p&gt;httpclient is also a dependency of hadoop-auth and hadop-azure; the hadoop-client-runtime appears to mention it too&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
[INFO] Including commons-cli:commons-cli:jar:1.2 in the shaded jar.
[INFO] Including org.apache.commons:commons-math3:jar:3.1.1 in the shaded jar.
[INFO] Including org.apache.httpcomponents:httpclient:jar:4.5.13 in the shaded jar.
[INFO] Including org.apache.httpcomponents:httpcore:jar:4.4.13 in the shaded jar.
[INFO] Including commons-codec:commons-codec:jar:1.15 in the shaded jar.

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;presumably that means: we need to lead by example and fix up the reference there before asking anyone else. &lt;/p&gt;

&lt;p&gt;for 3.3.4 I&apos;m cutting the hadoop-cos dependency; lowest risk change: &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-18307&quot; title=&quot;remove hadoop-cos as a dependency of hadoop-cloud-storage&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-18307&quot;&gt;&lt;del&gt;HADOOP-18307&lt;/del&gt;&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="17558491" author="stevel@apache.org" created="Fri, 24 Jun 2022 13:20:48 +0000"  >&lt;p&gt;edited the description to tell people coming here from a search engine what to do&lt;/p&gt;</comment>
                            <comment id="17597190" author="JIRAUSER295079" created="Mon, 29 Aug 2022 13:09:46 +0000"  >&lt;p&gt;can I confirm anyone successfully overcome the issue after changed to hadoop 3.3.4 ? I tested 3.3.4 and also recompiled 3.4 , still getting the same error. I connect to own s3. I tested no issue when run on Spark 3.1.2.&lt;/p&gt;

&lt;p&gt;I used below jars&lt;/p&gt;

&lt;p&gt;hadoop-aws-3.4.0-SNAPSHOT.jar&lt;/p&gt;

&lt;p&gt;hadoop-client-api-3.4.0-SNAPSHOT.jar&lt;/p&gt;

&lt;p&gt;hadoop-client-runtime-3.4.0-SNAPSHOT.jar&lt;/p&gt;

&lt;p&gt;aws-java-sdk-bundle-1.12.291-SNAPSHOT.jar&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I also get same error&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;22/08/29 12:47:22 INFO MetricsSystemImpl: s3a-file-system metrics system started&lt;br/&gt;
22/08/29 12:47:22 WARN SDKV2Upgrade: Directly referencing AWS SDK V1 credential provider com.amazonaws.auth.EnvironmentVariableCredentialsProvider. AWS SDK V1 credential providers will be removed once S3A is upgraded to SDK V2&lt;br/&gt;
22/08/29 13:05:46 INFO DelegatingLogStore: LogStore `LogStoreAdapter(io.delta.storage.S3SingleDriverLogStore)` is used for scheme `s3a`&lt;br/&gt;
22/08/29 13:07:33 WARN DeltaLog: Failed to parse s3a://bucket1/delta/text/_delta_log/_last_checkpoint. This may happen if there was an error during read operation, or a file appears to be partial. Sleeping and trying again.&lt;br/&gt;
org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a://bucket1/delta/text/_delta_log/_last_checkpoint: com.amazonaws.SdkClientException: Unable to execute HTTP request: Certificate for &amp;lt;bucket1.server_name.custom_s3_endpoint.com&amp;gt; doesn&apos;t match any of the subject alternative names: &lt;span class=&quot;error&quot;&gt;&amp;#91;server_name.custom_s3_endpoint.com, *.custom_s3_endpoint.com&amp;#93;&lt;/span&gt;: Unable to execute HTTP request: Certificate for &amp;lt;bucket1.server_name.custom_s3_endpoint.com&amp;gt; doesn&apos;t match any of the subject alternative names: &lt;span class=&quot;error&quot;&gt;&amp;#91;server_name.custom_s3_endpoint.com, *.custom_s3_endpoint.com&amp;#93;&lt;/span&gt;&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:212)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:173)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3679)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3582)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.s3a.S3AFileSystem.extractOrFetchSimpleFileStatus(S3AFileSystem.java:5217)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$executeOpen$6(S3AFileSystem.java:1540)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.s3a.S3AFileSystem.executeOpen(S3AFileSystem.java:1538)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.s3a.S3AFileSystem.open(S3AFileSystem.java:1512)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:996)&lt;br/&gt;
&#160; &#160; at io.delta.storage.HadoopFileSystemLogStore.read(HadoopFileSystemLogStore.java:46)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.storage.LogStoreAdaptor.read(LogStore.scala:382)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.storage.DelegatingLogStore.read(DelegatingLogStore.scala:96)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$2(Checkpoints.scala:373)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog.recordFrameProfile(DeltaLog.scala:63)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)&lt;br/&gt;
&#160; &#160; at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)&lt;br/&gt;
&#160; &#160; at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog.recordOperation(DeltaLog.scala:63)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog.recordDeltaOperation(DeltaLog.scala:63)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.Checkpoints.$anonfun$loadMetadataFromFile$1(Checkpoints.scala:372)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag(DeltaLogging.scala:143)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.withDmqTag$(DeltaLogging.scala:142)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog.withDmqTag(DeltaLog.scala:63)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.Checkpoints.loadMetadataFromFile(Checkpoints.scala:371)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.Checkpoints.lastCheckpoint(Checkpoints.scala:366)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.Checkpoints.lastCheckpoint$(Checkpoints.scala:365)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog.lastCheckpoint(DeltaLog.scala:63)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.SnapshotManagement.$init$(SnapshotManagement.scala:56)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog.&amp;lt;init&amp;gt;(DeltaLog.scala:68)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$3(DeltaLog.scala:593)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$2(DeltaLog.scala:589)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile(DeltaLogging.scala:139)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.recordFrameProfile$(DeltaLogging.scala:137)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog$.recordFrameProfile(DeltaLog.scala:456)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.$anonfun$recordDeltaOperationInternal$1(DeltaLogging.scala:132)&lt;br/&gt;
&#160; &#160; at com.databricks.spark.util.DatabricksLogging.recordOperation(DatabricksLogging.scala:77)&lt;br/&gt;
&#160; &#160; at com.databricks.spark.util.DatabricksLogging.recordOperation$(DatabricksLogging.scala:67)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog$.recordOperation(DeltaLog.scala:456)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperationInternal(DeltaLogging.scala:131)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation(DeltaLogging.scala:121)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.metering.DeltaLogging.recordDeltaOperation$(DeltaLogging.scala:109)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog$.recordDeltaOperation(DeltaLog.scala:456)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog$.createDeltaLog$1(DeltaLog.scala:588)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog$.$anonfun$apply$4(DeltaLog.scala:604)&lt;br/&gt;
&#160; &#160; at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4792)&lt;br/&gt;
&#160; &#160; at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)&lt;br/&gt;
&#160; &#160; at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)&lt;br/&gt;
&#160; &#160; at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)&lt;br/&gt;
&#160; &#160; at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)&lt;br/&gt;
&#160; &#160; at com.google.common.cache.LocalCache.get(LocalCache.java:4000)&lt;br/&gt;
&#160; &#160; at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4789)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog$.getDeltaLogFromCache$1(DeltaLog.scala:604)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog$.apply(DeltaLog.scala:611)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.DeltaLog$.forTable(DeltaLog.scala:507)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog$lzycompute(DeltaTableV2.scala:83)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.catalog.DeltaTableV2.deltaLog(DeltaTableV2.scala:82)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.catalog.DeltaTableV2.$anonfun$snapshot$3(DeltaTableV2.scala:113)&lt;br/&gt;
&#160; &#160; at scala.Option.getOrElse(Option.scala:189)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot$lzycompute(DeltaTableV2.scala:113)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.catalog.DeltaTableV2.snapshot(DeltaTableV2.scala:101)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema$lzycompute(DeltaTableV2.scala:119)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.catalog.DeltaTableV2.tableSchema(DeltaTableV2.scala:117)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.delta.catalog.DeltaTableV2.schema(DeltaTableV2.scala:121)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.execution.datasources.v2.DataSourceV2Relation$.create(DataSourceV2Relation.scala:197)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$createRelation$1(Analyzer.scala:1189)&lt;br/&gt;
&#160; &#160; at scala.Option.map(Option.scala:230)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.createRelation(Analyzer.scala:1161)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$3(Analyzer.scala:1212)&lt;br/&gt;
&#160; &#160; at scala.Option.orElse(Option.scala:447)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.$anonfun$lookupRelation$1(Analyzer.scala:1210)&lt;br/&gt;
&#160; &#160; at scala.Option.orElse(Option.scala:447)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupRelation(Analyzer.scala:1202)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1071)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$14.applyOrElse(Analyzer.scala:1035)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:138)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:97)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:138)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1117)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1116)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:208)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1117)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1116)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.OrderPreservingUnaryNode.mapChildren(LogicalPlan.scala:208)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$2(AnalysisHelper.scala:135)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren(TreeNode.scala:1117)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.trees.UnaryLike.mapChildren$(TreeNode.scala:1116)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.GlobalLimit.mapChildren(basicLogicalOperators.scala:1457)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:135)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:134)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:130)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:30)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:1035)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:994)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:211)&lt;br/&gt;
&#160; &#160; at scala.collection.LinearSeqOptimized.foldLeft(LinearSeqOptimized.scala:126)&lt;br/&gt;
&#160; &#160; at scala.collection.LinearSeqOptimized.foldLeft$(LinearSeqOptimized.scala:122)&lt;br/&gt;
&#160; &#160; at scala.collection.immutable.List.foldLeft(List.scala:91)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:208)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:200)&lt;br/&gt;
&#160; &#160; at scala.collection.immutable.List.foreach(List.scala:431)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:200)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:227)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:223)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:172)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:223)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:187)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:179)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:88)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:179)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:208)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:330)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:207)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.execution.QueryExecution.$anonfun$analyzed$1(QueryExecution.scala:76)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:111)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:185)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:512)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:185)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:184)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:98)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:96)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:622)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:617)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:651)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.org$apache$spark$sql$hive$thriftserver$SparkExecuteStatementOperation$$execute(SparkExecuteStatementOperation.scala:226)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.$anonfun$run$2(SparkExecuteStatementOperation.scala:165)&lt;br/&gt;
&#160; &#160; at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties(SparkOperation.scala:79)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.hive.thriftserver.SparkOperation.withLocalProperties$(SparkOperation.scala:63)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation.withLocalProperties(SparkExecuteStatementOperation.scala:40)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:165)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2$$anon$3.run(SparkExecuteStatementOperation.scala:160)&lt;br/&gt;
&#160; &#160; at java.base/java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
&#160; &#160; at java.base/javax.security.auth.Subject.doAs(Unknown Source)&lt;br/&gt;
&#160; &#160; at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1919)&lt;br/&gt;
&#160; &#160; at org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation$$anon$2.run(SparkExecuteStatementOperation.scala:174)&lt;br/&gt;
&#160; &#160; at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)&lt;br/&gt;
&#160; &#160; at java.base/java.util.concurrent.FutureTask.run(Unknown Source)&lt;br/&gt;
&#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)&lt;br/&gt;
&#160; &#160; at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)&lt;br/&gt;
&#160; &#160; at java.base/java.lang.Thread.run(Unknown Source)&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17597771" author="JIRAUSER295133" created="Tue, 30 Aug 2022 11:24:20 +0000"  >&lt;p&gt;Came here to say the same as comet. I just spend 3 days trying to fix this but to no avail. My setup on an AWS notebook instance:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;sagemaker-pyspark 1.4.5 (python package)&lt;/li&gt;
	&lt;li&gt;pyspark 3.3.0 (dependency of above)&lt;/li&gt;
	&lt;li&gt;jars:&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;aws-java-sdk-bundle-1.11.901.jar&lt;br/&gt;
aws-java-sdk-core-1.12.262.jar&lt;br/&gt;
aws-java-sdk-kms-1.12.262.jar&lt;br/&gt;
aws-java-sdk-s3-1.12.262.jar&lt;br/&gt;
aws-java-sdk-sagemaker-1.12.262.jar&lt;br/&gt;
aws-java-sdk-sagemakerruntime-1.12.262.jar&lt;br/&gt;
aws-java-sdk-sts-1.12.262.jar&lt;br/&gt;
hadoop-aws-3.3.1.jar&lt;br/&gt;
sagemaker-spark_2.12-spark_3.3.0-1.4.5.jar&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Problem:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Upon reading a file from S3 this error is thrown&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
22/08/30 11:00:22 WARN FileStreamSink: Assume no metadata directory. Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; looking &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; metadata directory in the path: s3a:&lt;span class=&quot;code-comment&quot;&gt;//comp.data.sci.data.tst/some/folder/export_date=20220822.
&lt;/span&gt;org.apache.hadoop.fs.s3a.AWSClientIOException: getFileStatus on s3a:&lt;span class=&quot;code-comment&quot;&gt;//comp.data.sci.data.tst/some/folder/export_date=20220822: com.amazonaws.SdkClientException: Unable to execute HTTP request: Certificate &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &amp;lt;comp.data.sci.data.tst.s3.amazonaws.com&amp;gt; doesn&lt;span class=&quot;code-quote&quot;&gt;&apos;t match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]: Unable to execute HTTP request: Certificate &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &amp;lt;comp.data.sci.data.tst.s3.amazonaws.com&amp;gt; doesn&apos;&lt;/span&gt;t match any of the subject alternative names: [*.s3.amazonaws.com, s3.amazonaws.com]
&lt;/span&gt;        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:208)
        at org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:170)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3351)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3185)
        at org.apache.hadoop.fs.s3a.S3AFileSystem.isDirectory(S3AFileSystem.java:4277)
        at org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:54)
        at org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:370)
        at org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:228)
        at org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:210)
        at scala.Option.getOrElse(Option.scala:189) &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;{{}}&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Based on suggested workarounds in the article above I tried a couple of things&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;1. downgrade httpclient to version 4.5.10 &#8594; didn&#8217;t work&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;2. tried to set the &lt;a href=&quot;https://github.com/aws/aws-sdk-java-v2/issues/1786&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/aws/aws-sdk-java-v2/issues/1786&lt;/a&gt; aws-java-sdk to disable SSL certificate checking &#8594; didn&#8217;t work&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;3. try to read from a bucket that doesn&#8217;t contain dots (.) &#8594; works but I don&apos;t have the freedom to change it to dashes.&lt;/li&gt;
	&lt;li&gt;4. look for any cos jars but there are non in my pyspark installation&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Would you advice me to wait until the bug is fixed or is there anything else I can try?&lt;/p&gt;</comment>
                            <comment id="17598093" author="JIRAUSER282415" created="Tue, 30 Aug 2022 22:19:02 +0000"  >&lt;p&gt;Do you have release date for 3.4 release ? I have tested everything in this issue without success.&lt;/p&gt;

&lt;p&gt;I&apos;ve tested under spark 3.0.3 docker image&lt;/p&gt;

&lt;p&gt;hadoop&#160;3.3.1 to 3.3.4&lt;/p&gt;

&lt;p&gt;Removing cos_api-bundle&lt;/p&gt;

&lt;p&gt;Upgrade and downgrade httpclient versions.&lt;/p&gt;

&lt;p&gt;Replace aws java sdk bundles with newer versions. etc.&lt;/p&gt;

&lt;p&gt;Nothing works. So sad &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="17598193" author="JIRAUSER285813" created="Wed, 31 Aug 2022 06:16:37 +0000"  >&lt;p&gt;I can confirm it is working. For the examples above, it seems that the bucket names have dots and in this case, be sure you are setting the `fs.s3a.path.style.access` property to `true`.&lt;/p&gt;</comment>
                            <comment id="17598388" author="JIRAUSER295079" created="Wed, 31 Aug 2022 13:39:18 +0000"  >&lt;p&gt;yes, it works, appreciated &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andre.amorimfonseca%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;andre.amorimfonseca@gmail.com&quot;&gt;andre.amorimfonseca@gmail.com&lt;/a&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17599108" author="JIRAUSER282415" created="Thu, 1 Sep 2022 18:40:12 +0000"  >&lt;p&gt;+1 &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andre.amorimfonseca%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;andre.amorimfonseca@gmail.com&quot;&gt;andre.amorimfonseca@gmail.com&lt;/a&gt; thanks for your time and dedication on this.&lt;/p&gt;</comment>
                            <comment id="17600421" author="JIRAUSER295133" created="Mon, 5 Sep 2022 13:40:39 +0000"  >&lt;p&gt;+3 &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=andre.amorimfonseca%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;andre.amorimfonseca@gmail.com&quot;&gt;andre.amorimfonseca@gmail.com&lt;/a&gt; for taking the time to outline. In the end the following steps were needed to get pyspark 3.3.0 working on an AWS notebook instance in a custom conda env:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Install full version of spark 3.3.0 + pyspark 3.3.0&lt;/li&gt;
	&lt;li&gt;Allow for dots in bucket: .config(&quot;spark.hadoop.fs.s3a.path.style.access&quot;, &quot;true&quot;)&lt;/li&gt;
	&lt;li&gt;Update to aws-java-sdk-bundle-1.12.262.jar (in sagemaker_pyspark/jars folder)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17612986" author="stevel@apache.org" created="Wed, 5 Oct 2022 12:53:22 +0000"  >&lt;p&gt;created &lt;a href=&quot;https://github.com/aws/aws-sdk-java/issues/2860&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/aws/aws-sdk-java/issues/2860&lt;/a&gt; for the AWS SDK team&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13451387">HADOOP-18307</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            3 years, 5 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z10j40:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>