<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 21:32:26 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HADOOP-3205] Read multiple chunks directly from FSInputChecker subclass into user buffers</title>
                <link>https://issues.apache.org/jira/browse/HADOOP-3205</link>
                <project id="12310240" key="HADOOP">Hadoop Common</project>
                    <description>&lt;p&gt;Implementations of FSInputChecker and FSOutputSummer like DFS do not have access to full user buffer. At any time DFS can access only up to 512 bytes even though user usually reads with a much larger buffer (often controlled by io.file.buffer.size). This requires implementations to double buffer data if an implementation wants to read or write larger chunks of data from underlying storage.&lt;/p&gt;

&lt;p&gt;We could separate changes for FSInputChecker and FSOutputSummer into two separate jiras.&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12393368">HADOOP-3205</key>
            <summary>Read multiple chunks directly from FSInputChecker subclass into user buffers</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tlipcon">Todd Lipcon</assignee>
                                    <reporter username="rangadi">Raghu Angadi</reporter>
                        <labels>
                    </labels>
                <created>Tue, 8 Apr 2008 06:14:43 +0000</created>
                <updated>Tue, 24 Aug 2010 20:34:03 +0000</updated>
                            <resolved>Tue, 5 Jan 2010 22:15:19 +0000</resolved>
                                    <version>0.22.0</version>
                                    <fixVersion>0.21.0</fixVersion>
                                    <component>fs</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="12772878" author="tlipcon" created="Tue, 3 Nov 2009 04:59:06 +0000"  >&lt;p&gt;Been looking at this ticket tonight. I&apos;m not sure exactly what you&apos;re getting it. As I am understanding it, the wrapping looks something like:&lt;/p&gt;

&lt;p&gt;User Reader -&amp;gt; FSInputChecker -&amp;gt; FSInputChecker subclass -&amp;gt; BufferedInputStream -&amp;gt; Underlying source&lt;/p&gt;

&lt;p&gt;e.g:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;        java.io.FileInputStream.readBytes(FileInputStream.java:Unknown line)
        java.io.FileInputStream.read(FileInputStream.java:199)
        org.apache.hadoop.fs.RawLocalFileSystem$TrackingFileInputStream.read(RawLocalFileSystem.jav
a:90)
        org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream.read(RawLocalFileSystem.java
:143)
        java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
        java.io.BufferedInputStream.read1(BufferedInputStream.java:258)
        java.io.BufferedInputStream.read(BufferedInputStream.java:317)
        java.io.DataInputStream.read(DataInputStream.java:132)
        org.apache.hadoop.fs.FSInputChecker.readFully(FSInputChecker.java:385)
        org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker.readChunk(ChecksumFileSystem.java:224)
        org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:238)
        org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:190)
        org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:158)
        java.io.DataInputStream.read(DataInputStream.java:83)
        org.apache.hadoop.io.IOUtils.copyBytes(IOUtils.java:72)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The user&apos;s buffer size passed in to fs.open(...) controls the size of the BufferedInputStream that wraps the underlying input stream (ie raw file or socket). The FSInputChecker does indeed call read() on that BufferedInputStream once for every 512 bytes (directly into the user buffer), but in my profiling this doesn&apos;t seem to be a CPU hog, since it only results in one syscall to the underlying stream for every io.file.buffer.size.&lt;/p&gt;

&lt;p&gt;As a test of the CPU overhead, I put an 800M file (checksummed) in /dev/shm and profiled hadoop fs -cat with io.file.buffer.size=64K. This obviously stresses the CPU hogs and syscall overhead without any actual disk involved. The top consumers are:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;   1 61.17% 61.17%    4363 300617 org.apache.hadoop.fs.FSInputChecker.readChecksumChunk
   2 13.11% 74.28%     935 300618 java.io.FileInputStream.readBytes
   3  7.71% 82.00%     550 300632 java.io.DataInputStream.read
   4  5.02% 87.02%     358 300600 java.io.FileOutputStream.writeBytes
   5  3.76% 90.77%     268 300657 java.io.DataInputStream.readFully
   6  1.67% 92.44%     119 300631 java.io.DataInputStream.readFully
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The particular line of readChecksumChunk that&apos;s consuming the time is line 241 (sum.update) - this indicates that the overhead here is just from checksumming and not from memory copies. The one possible gain I could see here would be to revert to a JNI implementation of CRC32 that can do multiple checksum chunks at once - we found that JNI was slow due to a constant overhead &quot;jumping the gap&quot; to C for small sizes, but we can probably get 50% checksum speedup for some buffers. This was originally rejected in &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-6148&quot; title=&quot;Implement a pure Java CRC32 calculator&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-6148&quot;&gt;&lt;del&gt;HADOOP-6148&lt;/del&gt;&lt;/a&gt; due to the complexity of maintaining two different CRC32 implementations.&lt;/p&gt;

&lt;p&gt;Are you suggesting here that we could do away with the internal buffer and assume that users are always going to do large reads? Doesn&apos;t that violate the contract of fs.open taking a buffer size?&lt;/p&gt;</comment>
                            <comment id="12772914" author="hong.tang" created="Tue, 3 Nov 2009 07:23:28 +0000"  >&lt;p&gt;I think it would be greate if native checksumming code operates on direct buffers and verifies many 512B chunks at a time.&lt;/p&gt;

&lt;p&gt;Something like what I suggested here: &lt;a href=&quot;http://developer.yahoo.net/blogs/hadoop/2009/08/the_anatomy_of_hadoop_io_pipel.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://developer.yahoo.net/blogs/hadoop/2009/08/the_anatomy_of_hadoop_io_pipel.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="12773109" author="tlipcon" created="Tue, 3 Nov 2009 18:42:55 +0000"  >&lt;p&gt;Hi Hong,&lt;/p&gt;

&lt;p&gt;Thanks for the input. How do others feel about using a separate CRC32 path for the &quot;bulk checksum checking&quot; in the read path, probably through JNI when available? I had suggested this in &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-6148&quot; title=&quot;Implement a pure Java CRC32 calculator&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-6148&quot;&gt;&lt;del&gt;HADOOP-6148&lt;/del&gt;&lt;/a&gt; and people said it would be unmaintainable. Given that checksum algorithms rarely change and are easy to verify, I disagree, but would like to have some +1s for this direction before I spend the time writing the code.&lt;/p&gt;

&lt;p&gt;Regarding the other points in your blog post, it seems to imply that we&apos;d have to change around a lot of the APIs to work with ByteBuffers rather than byte[], potentially all the way down to the user-facing layer. This would be a big API change. Where is a good place to start, and what kind of backwards compatibility layer will we need?&lt;/p&gt;</comment>
                            <comment id="12773132" author="rangadi" created="Tue, 3 Nov 2009 19:20:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;This was originally rejected in &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-6148&quot; title=&quot;Implement a pure Java CRC32 calculator&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-6148&quot;&gt;&lt;del&gt;HADOOP-6148&lt;/del&gt;&lt;/a&gt; due to the complexity of maintaining two different CRC32 &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This jira is not about CRC32 cost, but I don&apos;t see why we can&apos;t use pure java CRC32 from &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-6148&quot; title=&quot;Implement a pure Java CRC32 calculator&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-6148&quot;&gt;&lt;del&gt;HADOOP-6148&lt;/del&gt;&lt;/a&gt;. It is already used in DataNode. CRC32 implementation is transparent to FSInputChecker. If it is good for multiple other places in Hadoop, it is good for FileSystem as well.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Are you suggesting here that we could do away with the internal buffer and assume that users are always going to do large reads? Doesn&apos;t that violate the contract of fs.open taking a buffer size?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;essentially, yes. When the user gives large buffer, there is no need to copy to intermediate buffer. We would not require or assume the user gives a large buffer but the common case is that user does. DFSClient would read fixed length packet header from the underlying socket and then read the data directly to user buffer if the size is comparable or larger than the packet (64k).&lt;/p&gt;

&lt;p&gt;I don&apos;t see how any this would violate the contract. fs.open buffer size is only a hint.. underlying FS should know what is more optimal.&lt;/p&gt;</comment>
                            <comment id="12773168" author="tlipcon" created="Tue, 3 Nov 2009 20:20:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t see why we can&apos;t use pure java CRC32 from &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-6148&quot; title=&quot;Implement a pure Java CRC32 calculator&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-6148&quot;&gt;&lt;del&gt;HADOOP-6148&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We already do use this - the microbenchmark above (reading checksummed files from /dev/shm) shows that CRC is the majority of the CPU overhead in FSInputChecker and that array copying makes up very little of the time.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;When the user gives large buffer, there is no need to copy to intermediate buffer&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I see... so I guess what you&apos;re saying is that we should do away with the internal BufferedInputStream in DFSClient.BlockReader, and then occasionally insert a buffer only in the case when the user-provided buffer is small? This seems like a fair amount of confusing complexity due to the buffer management involved.&lt;/p&gt;

&lt;p&gt;Do we have some kind of benchmark that indicates that these copies make up any appreciable overhead compared to the fairly slow checksumming?&lt;/p&gt;
</comment>
                            <comment id="12773191" author="rangadi" created="Tue, 3 Nov 2009 21:03:10 +0000"  >
&lt;p&gt;obviously, one buffer copy is not expected to consume more CPU than a CRC32 checksum (much less for checksum of small chunks like 512). I roughly esitmated each buffer copy to take around 1/3rd of what CRC32 takes (ratio might be larger with improved CRC32). Does it mean it is not worth fixing second or third large CPU hogs on client? Of course when there is compression and other higher processing is involved, even CRC32 wouldn&apos;t be the largest CPU hog.&lt;/p&gt;

&lt;p&gt;We reduced CPU on DataNode while serving (&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-2758&quot; title=&quot;Reduce memory copies when data is read from DFS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-2758&quot;&gt;&lt;del&gt;HADOOP-2758&lt;/del&gt;&lt;/a&gt;, &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-3164&quot; title=&quot;Use FileChannel.transferTo() when data is read from DataNode.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-3164&quot;&gt;&lt;del&gt;HADOOP-3164&lt;/del&gt;&lt;/a&gt;) mainly by avoiding buffer copies (there is CRC involved). All the benchmarks there measure CPU consumed based on actual CPU reported by the OS (not by a profiler).. it is also essentially a &apos;dfs -cat&apos;.&lt;/p&gt;

&lt;p&gt;In your tests is it reading a dfs file? I used &apos;dfs -cat&apos; extensively in Datanode CPU benchmarks reported in the above Jiras.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This seems like a fair amount of confusing complexity due to the buffer management involved.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am not so sure. But just not buffering at all might be good enough (the smallest size would still be 512 bytes).&lt;/p&gt;</comment>
                            <comment id="12773193" author="rangadi" created="Tue, 3 Nov 2009 21:05:37 +0000"  >&lt;p&gt;Also, as Hong&apos;s blog post points out, there are copies at many places. Advantage of improving the interface is not just avoiding one copy, but could eventually support better handling of direct buffers. &lt;/p&gt;

&lt;p&gt;Plus, looking at how long this jira has been open, it is no blocker &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12773199" author="tlipcon" created="Tue, 3 Nov 2009 21:17:54 +0000"  >&lt;p&gt;Looking at the source of BufferedInputStream (at &lt;a href=&quot;http://www.docjar.com/html/api/java/io/BufferedInputStream.java.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.docjar.com/html/api/java/io/BufferedInputStream.java.html&lt;/a&gt;) it actually seems like BufferedInputStream is already handling pass-through to the underlying stream in the case that the read buffer is as large as its own buffer. That was the crucial bit I was missing that explains why performing the underlying reads in larger chunks would make a difference, even without removing the BIS.&lt;/p&gt;

&lt;p&gt;I&apos;ll give it a go and see if there is any discernible performance increase.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Plus, looking at how long this jira has been open, it is no blocker&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Of course &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12773713" author="tlipcon" created="Wed, 4 Nov 2009 23:55:01 +0000"  >&lt;p&gt;Here&apos;s a patch that implements the FSInputChecker side of this ticket.&lt;/p&gt;

&lt;p&gt;Benchmark results are promising. I put a 700MB file in /dev/shm with its associated checksum and then timed &quot;hadoop fs -cat /dev/shm/bigfile&quot; 100 times with the patch and without the patch. Here is R output from the analysis of these times:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&amp;gt; p.user &amp;lt;- read.table(file=&quot;/tmp/times.patch.user&quot;)
&amp;gt; p.sys &amp;lt;- read.table(file=&quot;/tmp/times.patch.sys&quot;)
&amp;gt; p.wall &amp;lt;- read.table(file=&quot;/tmp/times.patch.wall&quot;)
&amp;gt; t.user &amp;lt;- read.table(file=&quot;/tmp/times.trunk.user&quot;)
&amp;gt; t.sys &amp;lt;- read.table(file=&quot;/tmp/times.trunk.sys&quot;)
&amp;gt; t.wall &amp;lt;- read.table(file=&quot;/tmp/times.trunk.wall&quot;)
&amp;gt; t.test(t.user,p.user,alternative=&quot;greater&quot;)

        Welch Two Sample t-test

data:  t.user and p.user 
t = 21.0552, df = 134.54, p-value &amp;lt; 2.2e-16
alternative hypothesis: true difference in means is greater than 0 
95 percent confidence interval:
 0.4654936       Inf 
sample estimates:
mean of x mean of y 
 3.713000  3.207763 

&amp;gt; 3.2077/3.713
[1] 0.8639106
&amp;gt; t.test(t.sys,p.sys,alternative=&quot;greater&quot;)

        Welch Two Sample t-test

data:  t.sys and p.sys 
t = 1.3567, df = 137.286, p-value = 0.08856
alternative hypothesis: true difference in means is greater than 0 
95 percent confidence interval:
 -0.003768599          Inf 
sample estimates:
mean of x mean of y 
 0.980500  0.963421 

&amp;gt; t.test(t.wall,p.wall,alternative=&quot;greater&quot;)

        Welch Two Sample t-test

data:  t.wall and p.wall 
t = 6.5711, df = 118.318, p-value = 7.034e-10
alternative hypothesis: true difference in means is greater than 0 
95 percent confidence interval:
 0.3020628       Inf 
sample estimates:
mean of x mean of y 
 7.667800  7.263816
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To interpret the results for those who don&apos;t know R:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The user time is reduced with 100% confidence. With 95% confidence it&apos;s reduced by at least 0.465s = 12.5%&lt;/li&gt;
	&lt;li&gt;The sys time is not significantly reduced - p &amp;gt; 0.05. This is consistent with our expectation that we&apos;re doing the same number of syscalls, just avoiding buffer copies in user space.&lt;/li&gt;
	&lt;li&gt;Wall clock time is reduced with 100% confidence. With 95% confidence it&apos;s reduced by at least 0.302s = 3.9%.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I didn&apos;t include the R output, but analyis on the &quot;CPU%&quot; column of the &quot;time&quot; results gives 100% confidence of a reduction in CPU percent util, 95% confidence of at least 3.34%.&lt;/p&gt;

&lt;p&gt;The patch itself can probably be improved - just wanted to get early comments. I did briefly test that HDFS still functions, but have not run through all the unit tests. I also want to rerun the above benchmarks with io.file.buffer.size tuned up to 64K or 128K as most people do in production.&lt;/p&gt;</comment>
                            <comment id="12773718" author="hong.tang" created="Thu, 5 Nov 2009 00:01:13 +0000"  >&lt;p&gt;results looking good&lt;/p&gt;</comment>
                            <comment id="12773721" author="tlipcon" created="Thu, 5 Nov 2009 00:10:33 +0000"  >&lt;p&gt;Renaming ticket to focus on FSInputChecker. Let&apos;s do this one step at a time and open another JIRA for OutputSummer&lt;/p&gt;</comment>
                            <comment id="12773731" author="tlipcon" created="Thu, 5 Nov 2009 00:46:23 +0000"  >&lt;p&gt;Just reran benchmarks with 128K io.file.buffer.size. All of the improvements stayed practically identical, percentage-wise.&lt;/p&gt;</comment>
                            <comment id="12773735" author="hadoopqa" created="Thu, 5 Nov 2009 01:27:24 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12424076/hadoop-3205.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12424076/hadoop-3205.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision 832590.&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    +1 findbugs.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    -1 core tests.  The patch failed core unit tests.&lt;/p&gt;

&lt;p&gt;    +1 contrib tests.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/126/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12774127" author="tlipcon" created="Thu, 5 Nov 2009 23:28:25 +0000"  >&lt;p&gt;Here&apos;s a patch which fixes the bugs that caused the unit test failures.&lt;/p&gt;

&lt;p&gt;There&apos;s one TODO still in the code to figure out a good setting for MAX_CHUNKS (ie the max number of checksum chunks that should be read in one call to the underlying stream).&lt;/p&gt;

&lt;p&gt;This is still TODO since I made an odd discovery about this - the logic we were going on here was that the performance improvement was due to an eliminated buffer copy when the size of the read where &amp;gt;= the size of the buffer in the underlying BufferedInputStream. This would mean that the correct size for MAX_CHUNKS is ceil(io.file.buffer.size / 512) (ie 256 for a 128KB buffer I was testing with). If MAX_CHUNKS is less than that, then reads to the BIS would be less than its buffer size and thus you&apos;d incur a copy.&lt;/p&gt;

&lt;p&gt;However, my benchmarking shows that this &lt;b&gt;isn&apos;t&lt;/b&gt; the performance gain. Even with MAX_CHUNKS set to 4, there&apos;s a significant performance gain over MAX_CHUNKS set to 1. There is no significant difference between MAX_CHUNKS=127 and MAX_CHUNKS=128 for a 64K buffer, whereas the understanding above would indicate that 128 would eliminate a copy whereas 127 would not.&lt;/p&gt;

&lt;p&gt;So, I think this is actually improving performance because of some other effect like better cache locality by operating in larger chunks. Admittedly, cache locality is always the fallback excuse for a performance increase, but I don&apos;t have a better explanation yet. Anyone care to hazard a guess?&lt;/p&gt;</comment>
                            <comment id="12774137" author="hadoopqa" created="Thu, 5 Nov 2009 23:45:08 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12424165/hadoop-3205.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12424165/hadoop-3205.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision 832590.&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    +1 findbugs.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    -1 core tests.  The patch failed core unit tests.&lt;/p&gt;

&lt;p&gt;    +1 contrib tests.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/128/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12774140" author="tlipcon" created="Fri, 6 Nov 2009 00:12:43 +0000"  >&lt;p&gt;Previously I had been testing on a trunk from before &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-6223&quot; title=&quot;New improved FileSystem interface for those implementing new files systems.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-6223&quot;&gt;&lt;del&gt;HADOOP-6223&lt;/del&gt;&lt;/a&gt;. This new patch includes the same changes to ChecksumFs as were made to ChecksumFileSystem to fix TestLocalFSFileContextMainOperations.&lt;/p&gt;</comment>
                            <comment id="12774160" author="rangadi" created="Fri, 6 Nov 2009 00:47:43 +0000"  >
&lt;p&gt;I briefly went through the patch. looks fine. will review  soon. It updates readChunk()&apos;s contract to include multiple chunks. &lt;/p&gt;

&lt;p&gt;Are the DFSClient changes going to be part of a different jira? Let me know if I should do that.&lt;br/&gt;
I think hdfs client&apos;s read numbers would as good as slightly better.&lt;/p&gt;</comment>
                            <comment id="12774161" author="rangadi" created="Fri, 6 Nov 2009 00:51:33 +0000"  >&lt;p&gt;&amp;gt; So, I think this is actually improving performance because of some other effect like better cache locality by operating in larger chunks. Admittedly, cache locality is always the fallback excuse for a performance increase, but I don&apos;t have a better explanation yet. Anyone care to hazard a guess?&lt;/p&gt;

&lt;p&gt;hmm... avoiding a copy should save measurable CPU. I will run some experiments as well.&lt;/p&gt;</comment>
                            <comment id="12774163" author="tlipcon" created="Fri, 6 Nov 2009 00:54:46 +0000"  >&lt;p&gt;Yea, DFSClient will need a small edit as well, and I guess it will be a different JIRA since it&apos;s a different project. Annoyingly, the two jiras may have to be committed at the exact same time, since depending on the current implementation in DFSClient it may crash after this core change. I&apos;ll compile a core jar with this change and see whether HDFS still passes tests.&lt;/p&gt;</comment>
                            <comment id="12774170" author="hadoopqa" created="Fri, 6 Nov 2009 01:22:58 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12424169/hadoop-3205.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12424169/hadoop-3205.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision 832590.&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    +1 findbugs.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    +1 core tests.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    +1 contrib tests.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/129/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12774520" author="tlipcon" created="Sat, 7 Nov 2009 01:01:18 +0000"  >&lt;p&gt;New patch adds back the static checksum2Long function, since it was public and in use by HDFS code. HDFS continues to compile and pass tests on my machine using a common jar built from this patch. I made checksum2Long be @Deprecated since it&apos;s no longer used by FSInputChecker and with this patch it makes more sense to use IntBuffer to store multiple checksums.&lt;/p&gt;

&lt;p&gt;I created &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-755&quot; title=&quot;Read multiple checksum chunks at once in DFSInputStream&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-755&quot;&gt;&lt;del&gt;HDFS-755&lt;/del&gt;&lt;/a&gt; to track progress taking advantage of this feature for DFSClient.&lt;/p&gt;</comment>
                            <comment id="12774559" author="hadoopqa" created="Sat, 7 Nov 2009 05:15:37 +0000"  >&lt;p&gt;-1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12424246/hadoop-3205.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12424246/hadoop-3205.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision 833553.&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    -1 tests included.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    +1 findbugs.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    +1 core tests.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    +1 contrib tests.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h4.grid.sp2.yahoo.net/130/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12775179" author="tlipcon" created="Mon, 9 Nov 2009 22:49:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;-1 tests included. The patch doesn&apos;t appear to include any new or modified tests.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I believe this code path to be well exercised by existing tests (almost all the tests that do file IO go through this path and exercise it pretty well - note that existing tests failed when there were bugs in prior versions of the patch)&lt;/p&gt;</comment>
                            <comment id="12777805" author="eli" created="Sat, 14 Nov 2009 02:13:42 +0000"  >&lt;p&gt;Hey Todd,&lt;/p&gt;

&lt;p&gt;Nice change! &lt;/p&gt;

&lt;p&gt;Would be good to add a comment where the checksum array gets created indicating that the size of the  array determines the size of the underlying IO. &lt;/p&gt;

&lt;p&gt;In readChunk an invalid size checksum file currently results in a ChecksumException, is now an EOFException, would be good to revert back to ChecksumException and add a unit test for this.&lt;/p&gt;

&lt;p&gt;For sanity, do the existing tests cover both small, odd-size single block files?&lt;/p&gt;

&lt;p&gt;In readChecksum I&apos;d make expected and calculated ints (know they were longs before your change) since the code deals w 32-bit sums and just truncate sum.getValue() rather than cast and truncate the checksumInts.get() return value. Could assert sum.getValue() == 0xffffffffL &amp;amp; sum.getValue() if we&apos;re feeling paranoid.&lt;/p&gt;

&lt;p&gt;It would be good to test the LocalFs version (appears LocalFs is untested) but since the LocalFs and LocalFileSystem diffs are the same let&apos;s leave that to a separate jira.&lt;/p&gt;

&lt;p&gt;Nits:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Idealy the new illegal argument exception in FSInputChecker.set would be an assert since the given checkSumSize is not configurable or passed in at runtime, however since we don&apos;t enable asserts yet by default perhaps ok to leave as is.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The new code in readChecksum would be more readable if it were left pulled out into a separate function (how verifySum was).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Was this way before your change but the back-to-back if statements on line 252-253 could be combined triviallly.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Thanks,&lt;br/&gt;
Eli&lt;/p&gt;</comment>
                            <comment id="12784691" author="tlipcon" created="Wed, 2 Dec 2009 07:52:41 +0000"  >&lt;p&gt;Circled around on this issue tonight and tried to look into the mysterious behavior with the value of MAX_CHUNKS (the constant that determines how many checksum chunks worth we&apos;ll read in a single go)&lt;/p&gt;

&lt;p&gt;I wrote a quick benchmark which read a 1GB file out of /dev/shm with checksums using different values of MAX_CHUNKS. For each value, I ran 50 separate trials and calculated the average, as well as doing t-tests to figure out which results were within noise of each other.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;../hadoop-3205-bench/mc_64.user          3.0954
../hadoop-3205-bench/mc_128.user         3.1036
../hadoop-3205-bench/mc_8.user   3.1054
../hadoop-3205-bench/mc_256.user         3.1104
../hadoop-3205-bench/mc_32.user          3.1156 ** everything below here is within noise
../hadoop-3205-bench/mc_16.user          3.1214
../hadoop-3205-bench/mc_4.user   3.2896
../hadoop-3205-bench/mc_2.user   3.427
../hadoop-3205-bench/mc_1.user   3.6832

../hadoop-3205-bench/mc_16.elapsed       3.423
../hadoop-3205-bench/mc_64.elapsed       3.425
../hadoop-3205-bench/mc_8.elapsed        3.4288
../hadoop-3205-bench/mc_256.elapsed      3.4294
../hadoop-3205-bench/mc_128.elapsed      3.434
../hadoop-3205-bench/mc_32.elapsed       3.4392 ** everything below here is within noise
../hadoop-3205-bench/mc_4.elapsed        3.6108
../hadoop-3205-bench/mc_2.elapsed        3.7032
../hadoop-3205-bench/mc_1.elapsed        3.9846
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These were all done with a 64KB io.file.buffer.size, which would make us expect an optimal value of 128, since it should eliminate a copy. The results show that there are no gains to be had after 16 or 32 chunks being read at a time (8-16KB). The L1 cache on this machine is 128K, so that&apos;s not the magic number either.&lt;/p&gt;

&lt;p&gt;So basically, the performance improvement here remains a mystery to me, but it&apos;s clear there is one - about 13% for reading out of RAM on the machine above. Given these results, I&apos;d propose hard coding MAX_CHUNKS to 32 rather than basing it on io.file.buffer.size as I earlier figured.&lt;/p&gt;

&lt;p&gt;On a separate note, some review responses:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Was this way before your change but the back-to-back if statements on line 252-253 could be combined triviallly.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think you missed the &quot;chunkPos += read;&quot; outside the inner if? Java seems to occasionally return -1 for EOF for some reason so I was nervous about letting that happen outside the if. I&apos;d be happy to add an assert read &amp;gt;= 0 though for this case and make it part of the contract of readChunks to never return negative.&lt;/p&gt;


&lt;p&gt;The rest of the review makes sense, and I&apos;ll address those things and upload a new patch.&lt;/p&gt;</comment>
                            <comment id="12785171" author="tlipcon" created="Thu, 3 Dec 2009 04:11:40 +0000"  >&lt;p&gt;I did some further investigation on this:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I ran the 1GB cat test using hprof=cpu=times to get accurate invocation counts for the various read calls. Increasing MAX_CHUNKS from 1 to 16 does exactly what&apos;s expected and reduces the number of calls to readChunks (and thus the input stream reads, etc) by exactly a factor of 16. The same is true of 128 - no noticeable differences. This is because System.arraycopy doesn&apos;t get accounted by hprof in this mode, for whatever reason.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I imported a copy of the BufferedInputStream source and made BufferedFSInputStream extend from it rather than from the java.io one. I added a System.err printout right before the System.arraycopy inside read1(). When I changed MAX_CHUNKS over from 127 to 128, I verified that it correctly avoided these copies and read directly into the buffer. So the goal of the JIRA to get rid of a copy was indeed accomplished.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Now the confusing part: eliminating this copy does nothing in terms of performance. Comparing the MAX_CHUNKS=127 to MAX_CHUNKS=128 has no statistically significant effect on the speed of catting 1G from RAM.&lt;/p&gt;

&lt;p&gt;So my best theory right now on why it&apos;s faster is that it&apos;s simply doing fewer function calls, each of which does more work with longer loops. This is better for loop unrolling, instruction cache locality, and avoiding function call overhead. Perhaps it inspires the JIT to work harder as well - who knows what black magic lurks there &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I think at this point I&apos;ve sufficiently investigated this, unless anyone has questions. I&apos;ll make the changes that Eli suggested and upload a new patch.&lt;/p&gt;</comment>
                            <comment id="12785551" author="tlipcon" created="Thu, 3 Dec 2009 21:46:11 +0000"  >&lt;p&gt;New version of the patch. This addresses Eli&apos;s review comments, and adds some extra tests (one for truncated checksum file throwing ChecksumException, another for odd sized read buffers in a file with a few chunks). I also tidied up some of the comments to make it clearer to implementors what&apos;s going on.&lt;/p&gt;

&lt;p&gt;Just to be doubly sure, I reran all the benchmarks overnight and confirmed that reading 32 chunks at once had all the performance improvement benefits of a larger value (and uses less memory). Also reran &lt;a href=&quot;https://issues.apache.org/jira/browse/HDFS-755&quot; title=&quot;Read multiple checksum chunks at once in DFSInputStream&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HDFS-755&quot;&gt;&lt;del&gt;HDFS-755&lt;/del&gt;&lt;/a&gt; tests against this build with assertions on and everything looked good (plenty of assertion failures, but none in the new code!)&lt;/p&gt;</comment>
                            <comment id="12785561" author="hadoopqa" created="Thu, 3 Dec 2009 22:02:47 +0000"  >&lt;p&gt;+1 overall.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12426818/hadoop-3205.txt&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12426818/hadoop-3205.txt&lt;/a&gt;&lt;br/&gt;
  against trunk revision 886645.&lt;/p&gt;

&lt;p&gt;    +1 @author.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    +1 tests included.  The patch appears to include 5 new or modified tests.&lt;/p&gt;

&lt;p&gt;    +1 javadoc.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    +1 javac.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    +1 findbugs.  The patch does not introduce any new Findbugs warnings.&lt;/p&gt;

&lt;p&gt;    +1 release audit.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    +1 core tests.  The patch passed core unit tests.&lt;/p&gt;

&lt;p&gt;    +1 contrib tests.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html&lt;/a&gt;&lt;br/&gt;
Checkstyle results: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/artifact/trunk/build/test/checkstyle-errors.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/artifact/trunk/build/test/checkstyle-errors.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Patch-h1.grid.sp2.yahoo.net/22/console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="12785709" author="eli" created="Fri, 4 Dec 2009 01:45:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think you missed the &quot;chunkPos += read;&quot; outside the inner if? Java seems to occasionally return -1 for EOF for some reason so I was nervous about letting that happen outside the if. I&apos;d be happy to add an assert read &amp;gt;= 0 though for this case and make it part of the contract of readChunks to never return negative.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yup, I think the way it is now is good. &lt;/p&gt;

&lt;p&gt;Patch looks great. Like the new comments and test modifications. Thanks for running the additional experiments. &lt;/p&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="12793123" author="rangadi" created="Mon, 21 Dec 2009 08:55:59 +0000"  >
&lt;p&gt;+1. The patch looks good.  &lt;/p&gt;

&lt;p&gt;&amp;gt; Like the new comments and test modifications. &lt;br/&gt;
thanks for good comments and javadoc.&lt;/p&gt;

&lt;p&gt;I find it surprising as well, that avoiding a buffer copy does not show any improvement (so is 13% for function calls or other java voodoo). &lt;/p&gt;

&lt;p&gt;Limit of 32 chunks : it is true that your tests didn&apos;t show benefit beyond that for LocalFileSystem.. not sure if that justifies limiting fs implementation&apos;s access to user buffer. Is to reduce memory allocated for checksum buffer?&lt;/p&gt;

&lt;p&gt;I will run the tests on my laptop. The jira need not wait for my results.&lt;/p&gt;</comment>
                            <comment id="12796871" author="tomwhite" created="Tue, 5 Jan 2010 22:15:19 +0000"  >&lt;p&gt;I&apos;ve just committed this. Thanks Todd!&lt;/p&gt;</comment>
                            <comment id="12796886" author="hudson" created="Tue, 5 Jan 2010 22:42:43 +0000"  >&lt;p&gt;Integrated in Hadoop-Common-trunk-Commit #133 (See &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/133/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk-Commit/133/&lt;/a&gt;)&lt;br/&gt;
    . Read multiple chunks directly from FSInputChecker subclass into user buffers. Contributed by Todd Lipcon.&lt;/p&gt;</comment>
                            <comment id="12797069" author="hudson" created="Wed, 6 Jan 2010 11:15:21 +0000"  >&lt;p&gt;Integrated in Hadoop-Common-trunk #210 (See &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk/210/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Hadoop-Common-trunk/210/&lt;/a&gt;)&lt;br/&gt;
    . Read multiple chunks directly from FSInputChecker subclass into user buffers. Contributed by Todd Lipcon.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12440081">HDFS-755</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12410156">HDFS-347</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12426818" name="hadoop-3205.txt" size="19100" author="tlipcon" created="Thu, 3 Dec 2009 21:46:11 +0000"/>
                            <attachment id="12424246" name="hadoop-3205.txt" size="11640" author="tlipcon" created="Sat, 7 Nov 2009 01:01:18 +0000"/>
                            <attachment id="12424169" name="hadoop-3205.txt" size="11441" author="tlipcon" created="Fri, 6 Nov 2009 00:12:43 +0000"/>
                            <attachment id="12424165" name="hadoop-3205.txt" size="8934" author="tlipcon" created="Thu, 5 Nov 2009 23:28:25 +0000"/>
                            <attachment id="12424076" name="hadoop-3205.txt" size="8116" author="tlipcon" created="Wed, 4 Nov 2009 23:55:01 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>125834</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10343"><![CDATA[Reviewed]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            15 years, 46 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0id07:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>105201</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>