<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 21:43:32 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HADOOP-9820] RPCv9 wire protocol is insufficient to support multiplexing</title>
                <link>https://issues.apache.org/jira/browse/HADOOP-9820</link>
                <project id="12310240" key="HADOOP">Hadoop Common</project>
                    <description>&lt;p&gt;RPCv9 is intended to allow future support of multiplexing.  This requires all wire messages to be tagged with a RPC header so a demux can decode and route the messages accordingly.&lt;/p&gt;

&lt;p&gt;RPC ping packets and SASL QOP wrapped data is known to not be tagged with a header.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12661292">HADOOP-9820</key>
            <summary>RPCv9 wire protocol is insufficient to support multiplexing</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="daryn">Daryn Sharp</assignee>
                                    <reporter username="daryn">Daryn Sharp</reporter>
                        <labels>
                    </labels>
                <created>Fri, 2 Aug 2013 00:16:31 +0000</created>
                <updated>Thu, 12 May 2016 18:23:34 +0000</updated>
                            <resolved>Fri, 9 Aug 2013 01:20:31 +0000</resolved>
                                    <version>2.1.0-beta</version>
                    <version>3.0.0-alpha1</version>
                                    <fixVersion>2.1.0-beta</fixVersion>
                                    <component>ipc</component>
                    <component>security</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                                                            <comments>
                            <comment id="13727142" author="daryn" created="Fri, 2 Aug 2013 00:16:59 +0000"  >&lt;p&gt;Sanjay and I spoke that this will be incompatible to fix in the future.&lt;/p&gt;</comment>
                            <comment id="13727747" author="acmurthy" created="Fri, 2 Aug 2013 15:53:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=daryn&quot; class=&quot;user-hover&quot; rel=&quot;daryn&quot;&gt;daryn&lt;/a&gt; I&apos;d really appreciate (and buy you a few cases) if you could get this in asap, thanks!&lt;/p&gt;</comment>
                            <comment id="13727756" author="daryn" created="Fri, 2 Aug 2013 16:04:35 +0000"  >&lt;p&gt;#1 priority.&lt;/p&gt;</comment>
                            <comment id="13730042" author="sanjay.radia" created="Mon, 5 Aug 2013 22:17:20 +0000"  >&lt;p&gt;Actually one could argue that ping is part of the connection not the session. However your fix in Hadoop-9832 is an improvement over using a &quot;length&quot; of -1. So I will +1 the proposal in Hadoop-9832 and we can discuss whether the ping should be per-session or per-connection  when we add multiplexed sessions to rpc.&lt;/p&gt;</comment>
                            <comment id="13730188" author="daryn" created="Tue, 6 Aug 2013 00:10:10 +0000"  >&lt;p&gt;Today, once wrapping (encryption) begins all wire communication is [ length + encrypted-payload ].  This makes it impossible to route the packet to the correct multiplexed session.&lt;/p&gt;

&lt;p&gt;Proposal is the encrypted payload is wrapped with a rpc header.  The client and server can distinguish wrapped data from other rpc packets for other sessions or communications (like pings).&lt;/p&gt;</comment>
                            <comment id="13730191" author="daryn" created="Tue, 6 Aug 2013 00:12:53 +0000"  >&lt;p&gt;No tests are included because expanded test suites in prior jiras provide code coverage.&lt;/p&gt;</comment>
                            <comment id="13730225" author="hadoopqa" created="Tue, 6 Aug 2013 00:48:43 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12596247/HADOOP-9820.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12596247/HADOOP-9820.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 eclipse:eclipse&lt;/font&gt;.  The patch built with eclipse:eclipse.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in hadoop-common-project/hadoop-common.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 contrib tests&lt;/font&gt;.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/2929//testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/2929//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/2929//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/2929//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/2929//console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/2929//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13730769" author="daryn" created="Tue, 6 Aug 2013 13:36:52 +0000"  >&lt;p&gt;Findbugs flagged two bad practices completely unrelated to this patch:&lt;br/&gt;
&lt;tt&gt;Class org.apache.hadoop.metrics2.lib.DefaultMetricsSystem defines non-transient non-serializable instance field mBeanNames&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="13730917" author="daryn" created="Tue, 6 Aug 2013 16:34:50 +0000"  >&lt;p&gt;Summary of offline messages to Sanjay who is reviewing:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Sasl wrapped message are sent in RPC/SASL protobufs like other SASL messages&lt;/li&gt;
	&lt;li&gt;Code shifted in Server to decode SASL wrapped packets in the code path invoked by processRpcOutOfBandRequest which handles other SASL packets&lt;/li&gt;
	&lt;li&gt;Client&apos;s SaslInputStream replacement unwraps SASL wrapped messages, leaves others alone&lt;/li&gt;
	&lt;li&gt;Client&apos;s SaslOutputStream replacement adds the RPC header because existing one does length/encrypted-payload only&lt;/li&gt;
	&lt;li&gt;Replacement SaslOutputStream correctly uses a buffered stream of the SASL negotiated size for wrapping.  Existing SaslOutputStream impl was wrong but accidentally worked because of a smaller buffered stream atop it.&lt;/li&gt;
	&lt;li&gt;Slight optimization that Client isn&apos;t unnecessarily given sasl streams (that are no-ops) when wrapping isn&apos;t being done&lt;/li&gt;
	&lt;li&gt;Per comments, would be cleaner to decode all RPC packets in Client and route SASL messages to SaslRpcClient, but decoding is currently split across Client/SaslRpcClient.  SaslRpcClient handles RPC decoding during authentication, but then Client decodes the rest of the stream with no knowledge of SASL.  In the future, Client should decode all RPC packets and route SASL to SaslRpcClient.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13731144" author="sanjay.radia" created="Tue, 6 Aug 2013 19:25:11 +0000"  >&lt;p&gt;In SaslRpcClient#SaslRpcInputStream.readNextRpcPacket line 569:  if (headerBuilder.getCallId() == AuthProtocol.SASL.callId) {...&lt;/p&gt;

&lt;p&gt;Since SaslRpcInputStream is only used when sasl-wrapped, shouldn&apos;t it throw an exception if the callId is not SASL.callId? &lt;/p&gt;</comment>
                            <comment id="13731150" author="sanjay.radia" created="Tue, 6 Aug 2013 19:29:12 +0000"  >&lt;p&gt;You have optimized as per item 6 on your comment. Hence the javadoc for getInputStream and getOutputStream are incorrect. It should say something like &quot;Get SASL wrapped xxxputStreeam if it is sasl wrapped otherwise return original stream&quot;.&lt;/p&gt;</comment>
                            <comment id="13731234" author="daryn" created="Tue, 6 Aug 2013 20:32:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;Since SaslRpcInputStream is only used when sasl-wrapped, shouldn&apos;t it throw an exception if the callId is not SASL.callId?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I did consider if an exception should be thrown.  However, it would preclude the server sending any control messages to a given session.  Non-SASL messages might be something like a server sent ping to see if the client session is still alive.  Or maybe to forcibly close the session, etc.  I erred on the side of future flexibility.  Thoughts?&lt;/p&gt;</comment>
                            <comment id="13731254" author="sanjay.radia" created="Tue, 6 Aug 2013 20:54:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;I did consider if an exception should be thrown. However, it would preclude the server sending any control messages to a given session. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If that is the case then we should enumerate the messages explicitly in the code. &lt;/p&gt;

&lt;p&gt;However, Non-sasl messages will have their own header and will be wrapped - they  will be parsed by the next layer and the SASL  layer will not see them. If you agree then at this stage throw the exception.&lt;/p&gt;</comment>
                            <comment id="13732103" author="daryn" created="Wed, 7 Aug 2013 15:39:41 +0000"  >&lt;p&gt;Currently, yes, all RPC packets are wrapped and processed on the other side after unwrapping.  Therein lies the possible problem with sending control messages.&lt;/p&gt;

&lt;p&gt;Let&apos;s say we do throw an exception as suggested.  If the server cannot unwrap the SASL data, the client unlikely to be able to unwrap.  The server would need to send a non-wrapped exception response to notify the client that wrapping isn&apos;t working, which this patch allows.&lt;/p&gt;

&lt;p&gt;Perhaps a better approach is to only allow out-of-band RPC (negative callIds) to be sent unwrapped over a wrapped stream?&lt;/p&gt;</comment>
                            <comment id="13732137" author="sanjay.radia" created="Wed, 7 Aug 2013 16:33:46 +0000"  >&lt;p&gt;The RPC header and the SASL header after the it (but before the wrapped data) are not wrapped. The wrapped reply also has unwrapped headers (RPC and SASL). So the exception (if say the RPC header or the SASL is incorrect) will pass through fine. Indeed that is the beauty of the headers to the wrapped data - it does allow throwing an exception at the outer layer. The only problem is that if there is an exception at the RPC layer (above the wrapped layer) then the client has to be able to unwrap in order to read the exception. &lt;/p&gt;</comment>
                            <comment id="13732156" author="sanjay.radia" created="Wed, 7 Aug 2013 16:47:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;Let&apos;s say we do throw an exception as suggested.  If the server cannot unwrap the SASL data, ...&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Note the exception is not being thrown because the server cannot unwrap, the exception is being thrown because &lt;b&gt;currently&lt;/b&gt; the only header that is acceptable when wrapping is enabled is the RPC-header-callId=sasl &lt;b&gt;with&lt;/b&gt; the SASL-state=wrapped header. If you don&apos;t get that then throw the exception (which will go with its own response header). Later when we add multiplexing we will allow RPC-header-callId=sasl &lt;b&gt;with&lt;/b&gt; start new rpc-stream and here comes its its SASL-authenticate exchange.&lt;/p&gt;</comment>
                            <comment id="13732175" author="vicaya" created="Wed, 7 Aug 2013 17:11:56 +0000"  >&lt;p&gt;IMO, when the stream is being wrapped, no unwrapped exception should be thrown across RPC for the stream, as it&apos;s a breach of confidentiality. Server should log such exceptions.&lt;/p&gt;

&lt;p&gt;Minor nits:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;SaslRpcClient.SaslRpc*Stream should be named SaslRpcClient.Wrapped*Stream.&lt;/li&gt;
	&lt;li&gt;The default stream buffer size should be configurable instead of hard coded &quot;64*1024&quot;.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13732684" author="daryn" created="Wed, 7 Aug 2013 20:15:25 +0000"  >&lt;blockquote&gt;&lt;p&gt;exception is being thrown because currently the only header that is acceptable when wrapping is enabled is the RPC-header-callId=sasl with the SASL-state=wrapped header. If you don&apos;t get that then throw the exception (which will go with its own response header).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The more specific cases I had in mind:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Client and server are using mismatched ciphers.  The server can&apos;t decode the wrapped data.  The server doesn&apos;t know what cipher is the client is using so it can&apos;t send a wrapped response with the exception.  Sending a fatal non-wrapped RPC exception of &quot;wrong cipher&quot; exposes no sensitive data.    I guess we just close the connection and the client sees EOF.&lt;/li&gt;
	&lt;li&gt;Server wants to send a non-sensitive control messages like &quot;is session alive&quot; or &quot;close session&quot;.  Requiring non-sensitive messages to be wrapped/unwrapped seems overkill.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;All said, I&apos;ll disallow non-wrapped responses.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;SaslRpcClient.SaslRpc*Stream should be named SaslRpcClient.Wrapped*Stream.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ok.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The default stream buffer size should be configurable instead of hard coded &quot;64*1024&quot;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;That&apos;s the spec default if the buffer size isn&apos;t negotiated so it can&apos;t be a configurable option.  There are java properties to request a different buffer size, but if we want to add hadoop config options to override those then that&apos;s a separate feature.&lt;/p&gt;</comment>
                            <comment id="13732752" author="vicaya" created="Wed, 7 Aug 2013 21:15:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;Client and server are using mismatched ciphers.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That should not happen after the SASL negotiation is done. Given that even timing difference can leak information, we should not even tell a potentially adversarial client the fact that unwrap failed. We should log the exception at the server side for debugging purpose and close the connection after waiting for a random interval.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;That&apos;s the spec default if the buffer size isn&apos;t negotiated so it can&apos;t be a configurable option.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It needs to be a constant (with a pointer to the rfc) instead of literals for future maintenance.&lt;/p&gt;
</comment>
                            <comment id="13732965" author="sanjay.radia" created="Wed, 7 Aug 2013 23:59:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;The more specific cases I had in mind: ...  Server wants to send a non-sensitive control messages like &quot;is session alive&quot; or &quot;close session&quot;. Requiring non-sensitive messages to be wrapped/unwrapped seems overkill.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am in agreement with you here. But I was never proposing that we need to wrap such stuff in the future. Since you are responding to an issue I never raised, perhaps you are misreading my concern.&lt;br/&gt;
All I am just saying: in  SaslRpcClient#SaslRpcInputStream.readNextRpcPacket line 569:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (headerBuilder.getCallId() != AuthProtocol.SASL.callId) {...
     &lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; an exception, perhaps close the connection with fatal exception
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;In the future when we have out-of-band messages we can enumerate the ones that are allowed.&lt;/p&gt;</comment>
                            <comment id="13732968" author="sanjay.radia" created="Thu, 8 Aug 2013 00:01:07 +0000"  >&lt;p&gt;+1 modulo my comment on the exception and my comment on the javadoc. I like Luke&apos;s nit.&lt;/p&gt;</comment>
                            <comment id="13733607" author="daryn" created="Thu, 8 Aug 2013 15:40:16 +0000"  >&lt;ol&gt;
	&lt;li&gt;Updated get*Stream comments per Sanjay&lt;/li&gt;
	&lt;li&gt;Renamed classes for wrapped stream&apos;s per Luke&lt;/li&gt;
	&lt;li&gt;Removed default buf size per me
	&lt;ul&gt;
		&lt;li&gt;I was using the RFC max for digest-md5 which is different than gssapi&lt;/li&gt;
		&lt;li&gt;It was the wrong value.  RAW_SEND_BUF is the correct value which is guaranteed to be set.  Just to be sure, verified in the SASL md5 and krb5 clients.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Once the SASL auth completes, &lt;em&gt;only&lt;/em&gt; SASL wrapped packets are allowed per Sanjay/Luke.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13733644" author="hadoopqa" created="Thu, 8 Aug 2013 16:25:41 +0000"  >&lt;p&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;.  Here are the results of testing the latest attachment &lt;br/&gt;
  &lt;a href=&quot;http://issues.apache.org/jira/secure/attachment/12596861/HADOOP-9820.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;http://issues.apache.org/jira/secure/attachment/12596861/HADOOP-9820.patch&lt;/a&gt;&lt;br/&gt;
  against trunk revision .&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 @author&lt;/font&gt;.  The patch does not contain any @author tags.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 tests included&lt;/font&gt;.  The patch doesn&apos;t appear to include any new or modified tests.&lt;br/&gt;
                        Please justify why no new tests are needed for this patch.&lt;br/&gt;
                        Also please list what manual steps were performed to verify this patch.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javac&lt;/font&gt;.  The applied patch does not increase the total number of javac compiler warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 javadoc&lt;/font&gt;.  The javadoc tool did not generate any warning messages.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 eclipse:eclipse&lt;/font&gt;.  The patch built with eclipse:eclipse.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;red&quot;&gt;-1 findbugs&lt;/font&gt;.  The patch appears to introduce 2 new Findbugs (version 1.3.9) warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 release audit&lt;/font&gt;.  The applied patch does not increase the total number of release audit warnings.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 core tests&lt;/font&gt;.  The patch passed unit tests in hadoop-common-project/hadoop-common.&lt;/p&gt;

&lt;p&gt;    &lt;font color=&quot;green&quot;&gt;+1 contrib tests&lt;/font&gt;.  The patch passed contrib unit tests.&lt;/p&gt;

&lt;p&gt;Test results: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/2945//testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/2945//testReport/&lt;/a&gt;&lt;br/&gt;
Findbugs warnings: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/2945//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/2945//artifact/trunk/patchprocess/newPatchFindbugsWarningshadoop-common.html&lt;/a&gt;&lt;br/&gt;
Console output: &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/2945//console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/2945//console&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This message is automatically generated.&lt;/p&gt;</comment>
                            <comment id="13733685" author="daryn" created="Thu, 8 Aug 2013 16:59:07 +0000"  >&lt;p&gt;Findbugs still not caused by this patch.&lt;/p&gt;

&lt;p&gt;I&apos;m supposed to be on vacation today, so please commit if the patch is acceptable.  Thanks!&lt;/p&gt;</comment>
                            <comment id="13734052" author="jnp" created="Thu, 8 Aug 2013 21:47:56 +0000"  >&lt;p&gt;+1. looks good to me.&lt;/p&gt;</comment>
                            <comment id="13734064" author="sanjay.radia" created="Thu, 8 Aug 2013 21:55:26 +0000"  >&lt;p&gt;+1 with a minor nit in java comment.&lt;br/&gt;
 // decode message if it&apos;s SASL wrapped&lt;br/&gt;
should be &lt;br/&gt;
// Must be SASL wrapped, verify and decode.&lt;/p&gt;</comment>
                            <comment id="13734179" author="hudson" created="Thu, 8 Aug 2013 23:32:18 +0000"  >&lt;p&gt;SUCCESS: Integrated in Hadoop-trunk-Commit #4231 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-trunk-Commit/4231/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-trunk-Commit/4231/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9820&quot; title=&quot;RPCv9 wire protocol is insufficient to support multiplexing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9820&quot;&gt;&lt;del&gt;HADOOP-9820&lt;/del&gt;&lt;/a&gt;. RPCv9 wire protocol is insufficient to support multiplexing. Contributed by Daryn Sharp. (jitendra: &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1512091&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1512091&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/proto/RpcHeader.proto&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13734307" author="jnp" created="Fri, 9 Aug 2013 01:18:39 +0000"  >&lt;p&gt;I have addressed Sanjay&apos;s nit and committed this to trunk, branch-2, branch-2.1-beta and branch-2.1.0-beta. Thanks to Daryn.&lt;/p&gt;</comment>
                            <comment id="13734663" author="hudson" created="Fri, 9 Aug 2013 10:55:04 +0000"  >&lt;p&gt;SUCCESS: Integrated in Hadoop-Yarn-trunk #296 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Yarn-trunk/296/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-Yarn-trunk/296/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9820&quot; title=&quot;RPCv9 wire protocol is insufficient to support multiplexing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9820&quot;&gt;&lt;del&gt;HADOOP-9820&lt;/del&gt;&lt;/a&gt;. RPCv9 wire protocol is insufficient to support multiplexing. Contributed by Daryn Sharp. (jitendra: &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1512091&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1512091&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/proto/RpcHeader.proto&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13734730" author="hudson" created="Fri, 9 Aug 2013 12:37:01 +0000"  >&lt;p&gt;ABORTED: Integrated in Hadoop-Hdfs-trunk #1486 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Hdfs-trunk/1486/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-Hdfs-trunk/1486/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9820&quot; title=&quot;RPCv9 wire protocol is insufficient to support multiplexing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9820&quot;&gt;&lt;del&gt;HADOOP-9820&lt;/del&gt;&lt;/a&gt;. RPCv9 wire protocol is insufficient to support multiplexing. Contributed by Daryn Sharp. (jitendra: &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1512091&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1512091&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/proto/RpcHeader.proto&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13734833" author="hudson" created="Fri, 9 Aug 2013 14:25:17 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-Mapreduce-trunk #1513 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1513/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-Mapreduce-trunk/1513/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9820&quot; title=&quot;RPCv9 wire protocol is insufficient to support multiplexing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9820&quot;&gt;&lt;del&gt;HADOOP-9820&lt;/del&gt;&lt;/a&gt;. RPCv9 wire protocol is insufficient to support multiplexing. Contributed by Daryn Sharp. (jitendra: &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1512091&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1512091&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/proto/RpcHeader.proto&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13735860" author="hudson" created="Sat, 10 Aug 2013 11:33:36 +0000"  >&lt;p&gt;FAILURE: Integrated in Hadoop-Hdfs-trunk #1487 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-Hdfs-trunk/1487/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-Hdfs-trunk/1487/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9820&quot; title=&quot;RPCv9 wire protocol is insufficient to support multiplexing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-9820&quot;&gt;&lt;del&gt;HADOOP-9820&lt;/del&gt;&lt;/a&gt;. RPCv9 wire protocol is insufficient to support multiplexing. Contributed by Daryn Sharp. (jitendra: &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1512091&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1512091&lt;/a&gt;)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Client.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/ipc/Server.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/SaslRpcClient.java&lt;/li&gt;
	&lt;li&gt;/hadoop/common/trunk/hadoop-common-project/hadoop-common/src/main/proto/RpcHeader.proto&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                    <attachments>
                            <attachment id="12596861" name="HADOOP-9820.patch" size="14878" author="daryn" created="Thu, 8 Aug 2013 15:40:16 +0000"/>
                            <attachment id="12596247" name="HADOOP-9820.patch" size="14089" author="daryn" created="Tue, 6 Aug 2013 00:10:10 +0000"/>
                    </attachments>
                <subtasks>
                            <subtask id="12661931">HADOOP-9832</subtask>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>341481</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310191" key="com.atlassian.jira.plugin.system.customfieldtypes:multicheckboxes">
                        <customfieldname>Hadoop Flags</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10342"><![CDATA[Incompatible change]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 15 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1mw5r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>341788</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12324030">2.1.0-beta</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>