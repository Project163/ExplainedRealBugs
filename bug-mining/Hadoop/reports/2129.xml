<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 21:52:20 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HADOOP-14204] S3A multipart commit failing, &quot;UnsupportedOperationException at java.util.Collections$UnmodifiableList.sort&quot;</title>
                <link>https://issues.apache.org/jira/browse/HADOOP-14204</link>
                <project id="12310240" key="HADOOP">Hadoop Common</project>
                    <description>&lt;p&gt;Stack trace seen trying to commit a multipart upload, as the EMR code (which takes a &lt;tt&gt;List&amp;lt;String&amp;gt; etags&lt;/tt&gt; is trying to sort that list directly, which it can&apos;t do if the list doesn&apos;t want to be sorted.&lt;/p&gt;

&lt;p&gt;later versions of the SDK clone the list before sorting.&lt;/p&gt;

&lt;p&gt;We need to make sure that the list passed in can be sorted.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13057567">HADOOP-14204</key>
            <summary>S3A multipart commit failing, &quot;UnsupportedOperationException at java.util.Collections$UnmodifiableList.sort&quot;</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="stevel@apache.org">Steve Loughran</assignee>
                                    <reporter username="stevel@apache.org">Steve Loughran</reporter>
                        <labels>
                    </labels>
                <created>Mon, 20 Mar 2017 15:46:51 +0000</created>
                <updated>Thu, 8 Jun 2017 00:13:36 +0000</updated>
                            <resolved>Tue, 21 Mar 2017 13:24:44 +0000</resolved>
                                    <version>2.8.0</version>
                                    <fixVersion>2.9.0</fixVersion>
                    <fixVersion>3.0.0-alpha4</fixVersion>
                    <fixVersion>2.8.2</fixVersion>
                                    <component>fs/s3</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="15932911" author="stevel@apache.org" created="Mon, 20 Mar 2017 15:59:51 +0000"  >&lt;p&gt;Stack. This is my github cloud examples running with Spark master built against hadoop-2.8.0 RC3&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;   org.apache.spark.SparkException: Job aborted.
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:196)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:161)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:161)
      at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:161)
      at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:137)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
      at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
      at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
      at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
      at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:93)
      at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:93)
      at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:442)
      at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:478)
      at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
      at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
      at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
      at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
      at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:93)
      at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:93)
      at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:606)
      at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)
      at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)
      at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)
      at com.hortonworks.spark.cloud.examples.S3DataFrameExample.action(S3DataFrameExample.scala:160)
      at com.hortonworks.spark.cloud.ObjectStoreExample$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;action(ObjectStoreExample.scala:67)
      at com.hortonworks.spark.cloud.examples.S3DataFrameExample.action(S3DataFrameExample.scala:56)
      at com.hortonworks.spark.cloud.examples.S3DataFrameExampleSuite$$anonfun$2.apply$mcV$sp(S3DataFrameExampleSuite.scala:47)
      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply$mcV$sp(CloudSuite.scala:133)
      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply(CloudSuite.scala:131)
      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply(CloudSuite.scala:131)
      at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
      at org.scalatest.OutcomeOf$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;outcomeOf(OutcomeOf.scala:85)
      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
      at org.scalatest.Transformer.apply(Transformer.scala:22)
      at org.scalatest.Transformer.apply(Transformer.scala:20)
      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
      at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;withFixture(Suite.scala:1122)
      at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;invokeWithFixture$1(FunSuiteLike.scala:163)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
      at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTest(FunSuiteLike.scala:175)
      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfterEach$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$runTest(CloudSuite.scala:40)
      at org.scalatest.BeforeAndAfterEach$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTest(BeforeAndAfterEach.scala:255)
      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfter$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$runTest(CloudSuite.scala:40)
      at org.scalatest.BeforeAndAfter$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTest(BeforeAndAfter.scala:200)
      at com.hortonworks.spark.cloud.CloudSuite.runTest(CloudSuite.scala:40)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
      at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
      at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
      at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTests(FunSuiteLike.scala:208)
      at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
      at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(Suite.scala:1424)
      at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
      at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(FunSuiteLike.scala:212)
      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfterAll$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(CloudSuite.scala:40)
      at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;liftedTree1$1(BeforeAndAfterAll.scala:257)
      at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(BeforeAndAfterAll.scala:256)
      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfter$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(CloudSuite.scala:40)
      at org.scalatest.BeforeAndAfter$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(BeforeAndAfter.scala:241)
      at com.hortonworks.spark.cloud.CloudSuite.run(CloudSuite.scala:40)
      at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;callExecuteOnSuite$1(Suite.scala:1492)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
      at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(IndexedSeqOptimized.scala:33)
      at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
      at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runNestedSuites(Suite.scala:1526)
      at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
      at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(Suite.scala:1421)
      at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
      at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
      at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
      at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
      at org.scalatest.tools.Runner$.main(Runner.scala:860)
      at org.scalatest.tools.Runner.main(Runner.scala)
      Cause: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1, localhost, executor driver): org.apache.spark.SparkException: Task failed &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; writing rows
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:253)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:178)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:177)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
	at org.apache.spark.scheduler.Task.run(Task.scala:108)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
Caused by: java.lang.UnsupportedOperationException
	at java.util.Collections$UnmodifiableList.sort(Collections.java:1331)
	at java.util.Collections.sort(Collections.java:175)
	at com.amazonaws.services.s3.model.transform.RequestXmlFactory.convertToXmlByteArray(RequestXmlFactory.java:42)
	at com.amazonaws.services.s3.AmazonS3Client.completeMultipartUpload(AmazonS3Client.java:2692)
	at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelper.completeMultipartUpload(S3AFileSystem.java:2298)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:561)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$200(S3ABlockOutputStream.java:454)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:352)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
	at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
	at org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:639)
	at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:117)
	at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:163)
	at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.releaseResources(FileFormatWriter.scala:317)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:311)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:237)
	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1360)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:242)
	... 8 more
	Suppressed: java.lang.NullPointerException
		at org.apache.parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:160)
		at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:109)
		at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:163)
		at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.releaseResources(FileFormatWriter.scala:317)
		at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$1.apply$mcV$sp(FileFormatWriter.scala:245)
		at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1369)
		... 9 more

Driver stacktrace:
      at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1495)
      at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1483)
      at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1482)
      at scala.collection.mutable.ResizableArray$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(ResizableArray.scala:59)
      at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
      at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1482)
      at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:810)
      at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:810)
      at scala.Option.foreach(Option.scala:257)
      at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:810)
      at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1710)
      at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1665)
      at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1654)
      at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
      at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:635)
      at org.apache.spark.SparkContext.runJob(SparkContext.scala:2019)
      at org.apache.spark.SparkContext.runJob(SparkContext.scala:2040)
      at org.apache.spark.SparkContext.runJob(SparkContext.scala:2072)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply$mcV$sp(FileFormatWriter.scala:176)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:161)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:161)
      at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:161)
      at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:137)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
      at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
      at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
      at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
      at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:93)
      at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:93)
      at org.apache.spark.sql.execution.datasources.DataSource.writeInFileFormat(DataSource.scala:442)
      at org.apache.spark.sql.execution.datasources.DataSource.write(DataSource.scala:478)
      at org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:48)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:58)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:56)
      at org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:74)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:114)
      at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:135)
      at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
      at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:132)
      at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:113)
      at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:93)
      at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:93)
      at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:606)
      at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:233)
      at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:217)
      at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:509)
      at com.hortonworks.spark.cloud.examples.S3DataFrameExample.action(S3DataFrameExample.scala:160)
      at com.hortonworks.spark.cloud.ObjectStoreExample$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;action(ObjectStoreExample.scala:67)
      at com.hortonworks.spark.cloud.examples.S3DataFrameExample.action(S3DataFrameExample.scala:56)
      at com.hortonworks.spark.cloud.examples.S3DataFrameExampleSuite$$anonfun$2.apply$mcV$sp(S3DataFrameExampleSuite.scala:47)
      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply$mcV$sp(CloudSuite.scala:133)
      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply(CloudSuite.scala:131)
      at com.hortonworks.spark.cloud.CloudSuite$$anonfun$ctest$1.apply(CloudSuite.scala:131)
      at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
      at org.scalatest.OutcomeOf$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;outcomeOf(OutcomeOf.scala:85)
      at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
      at org.scalatest.Transformer.apply(Transformer.scala:22)
      at org.scalatest.Transformer.apply(Transformer.scala:20)
      at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
      at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;withFixture(Suite.scala:1122)
      at org.scalatest.FunSuite.withFixture(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;invokeWithFixture$1(FunSuiteLike.scala:163)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
      at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
      at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTest(FunSuiteLike.scala:175)
      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfterEach$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$runTest(CloudSuite.scala:40)
      at org.scalatest.BeforeAndAfterEach$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTest(BeforeAndAfterEach.scala:255)
      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfter$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$runTest(CloudSuite.scala:40)
      at org.scalatest.BeforeAndAfter$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTest(BeforeAndAfter.scala:200)
      at com.hortonworks.spark.cloud.CloudSuite.runTest(CloudSuite.scala:40)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
      at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
      at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
      at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
      at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runTests(FunSuiteLike.scala:208)
      at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
      at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(Suite.scala:1424)
      at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(FunSuite.scala:1555)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
      at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
      at org.scalatest.FunSuiteLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(FunSuiteLike.scala:212)
      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfterAll$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(CloudSuite.scala:40)
      at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;liftedTree1$1(BeforeAndAfterAll.scala:257)
      at org.scalatest.BeforeAndAfterAll$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(BeforeAndAfterAll.scala:256)
      at com.hortonworks.spark.cloud.CloudSuite.org$scalatest$BeforeAndAfter$$&lt;span class=&quot;code-keyword&quot;&gt;super&lt;/span&gt;$run(CloudSuite.scala:40)
      at org.scalatest.BeforeAndAfter$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(BeforeAndAfter.scala:241)
      at com.hortonworks.spark.cloud.CloudSuite.run(CloudSuite.scala:40)
      at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;callExecuteOnSuite$1(Suite.scala:1492)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1528)
      at org.scalatest.Suite$$anonfun$runNestedSuites$1.apply(Suite.scala:1526)
      at scala.collection.IndexedSeqOptimized$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;foreach(IndexedSeqOptimized.scala:33)
      at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
      at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;runNestedSuites(Suite.scala:1526)
      at org.scalatest.tools.DiscoverySuite.runNestedSuites(DiscoverySuite.scala:29)
      at org.scalatest.Suite$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;run(Suite.scala:1421)
      at org.scalatest.tools.DiscoverySuite.run(DiscoverySuite.scala:29)
      at org.scalatest.tools.SuiteRunner.run(SuiteRunner.scala:55)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2563)
      at org.scalatest.tools.Runner$$anonfun$doRunRunRunDaDoRunRun$3.apply(Runner.scala:2557)
      at scala.collection.immutable.List.foreach(List.scala:381)
      at org.scalatest.tools.Runner$.doRunRunRunDaDoRunRun(Runner.scala:2557)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1044)
      at org.scalatest.tools.Runner$$anonfun$runOptionallyWithPassFailReporter$2.apply(Runner.scala:1043)
      at org.scalatest.tools.Runner$.withClassLoaderAndDispatchReporter(Runner.scala:2722)
      at org.scalatest.tools.Runner$.runOptionallyWithPassFailReporter(Runner.scala:1043)
      at org.scalatest.tools.Runner$.main(Runner.scala:860)
      at org.scalatest.tools.Runner.main(Runner.scala)
      Cause: org.apache.spark.SparkException: Task failed &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; writing rows
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:253)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:178)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:177)
      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
      at org.apache.spark.scheduler.Task.run(Task.scala:108)
      at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
      at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
      Cause: java.lang.UnsupportedOperationException
      at java.util.Collections$UnmodifiableList.sort(Collections.java:1331)
      at java.util.Collections.sort(Collections.java:175)
      at com.amazonaws.services.s3.model.transform.RequestXmlFactory.convertToXmlByteArray(RequestXmlFactory.java:42)
      at com.amazonaws.services.s3.AmazonS3Client.completeMultipartUpload(AmazonS3Client.java:2692)
      at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelper.completeMultipartUpload(S3AFileSystem.java:2298)
      at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.complete(S3ABlockOutputStream.java:561)
      at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$200(S3ABlockOutputStream.java:454)
      at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.close(S3ABlockOutputStream.java:352)
      at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:72)
      at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:106)
      at org.apache.parquet.hadoop.ParquetFileWriter.end(ParquetFileWriter.java:639)
      at org.apache.parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:117)
      at org.apache.parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:163)
      at org.apache.spark.sql.execution.datasources.parquet.ParquetOutputWriter.close(ParquetOutputWriter.scala:42)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.releaseResources(FileFormatWriter.scala:317)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$SingleDirectoryWriteTask.execute(FileFormatWriter.scala:311)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:239)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask$3.apply(FileFormatWriter.scala:237)
      at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1360)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:242)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:178)
      at org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1$$anonfun$12.apply(FileFormatWriter.scala:177)
      at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)
      at org.apache.spark.scheduler.Task.run(Task.scala:108)
      at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:317)
      at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
      at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
      at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15933072" author="stevel@apache.org" created="Mon, 20 Mar 2017 17:04:17 +0000"  >&lt;p&gt;Issue is&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;AWS SDK assumes passed in &lt;tt&gt;List&amp;lt;PartETag&amp;gt;&lt;/tt&gt; can be sorted.&lt;/li&gt;
	&lt;li&gt;We are generating it with &lt;tt&gt;Futures.allAsList(partETagsFutures).get();&lt;/tt&gt;, which inside goes &lt;tt&gt;return new ListFuture&amp;lt;V&amp;gt;(ImmutableList.copyOf(futures), true,  MoreExecutors.sameThreadExecutor());&lt;/tt&gt;. That is: returns an immutable list.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Fix is what the later SDKs do internally: copy the list elements into a new ArrayList.&lt;/p&gt;</comment>
                            <comment id="15933310" author="stevel@apache.org" created="Mon, 20 Mar 2017 19:00:28 +0000"  >&lt;p&gt;This is one of those Schoedingbugs: it doesn&apos;t exist until it surfaces, but now you see it, it&apos;s obvious that the code never worked. Except it does, doesn&apos;t it?&lt;/p&gt;</comment>
                            <comment id="15933364" author="stevel@apache.org" created="Mon, 20 Mar 2017 19:34:04 +0000"  >&lt;p&gt;Patch 001; create a new, sortable list. This is what the later AWS sdk does internally, it is mostly harmless on those SDKs, and should prevent the problem on the version in Hadoop 2.7-2.8.&lt;/p&gt;

&lt;p&gt;Testing: s3a frankfurt, also rebuilt spark &amp;amp; ran the tests downstream, as that was where I saw it. No occurrences in repeated test runs.&lt;/p&gt;</comment>
                            <comment id="15933401" author="hadoopqa" created="Mon, 20 Mar 2017 19:54:07 +0000"  >&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/error.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;b&gt;&lt;font color=&quot;red&quot;&gt;-1 overall&lt;/font&gt;&lt;/b&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br class=&quot;atl-forced-newline&quot; /&gt;
&lt;br class=&quot;atl-forced-newline&quot; /&gt;&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Vote &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Subsystem &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Runtime &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Comment &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;blue&quot;&gt;0&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;blue&quot;&gt; reexec &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;blue&quot;&gt;  0m 19s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;blue&quot;&gt; Docker mode activated. &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; @author &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m  0s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; The patch does not contain any @author tags. &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;red&quot;&gt;-1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;red&quot;&gt; test4tests &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;red&quot;&gt;  0m  0s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;red&quot;&gt; The patch doesn&apos;t appear to include any new or modified tests. Please justify why no new tests are needed for this patch. Also please list what manual steps were performed to verify this patch. &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; mvninstall &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  6m 55s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; branch-2.8 passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; compile &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 16s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; branch-2.8 passed with JDK v1.8.0_121 &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; compile &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 19s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; branch-2.8 passed with JDK v1.7.0_121 &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; checkstyle &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 14s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; branch-2.8 passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; mvnsite &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 24s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; branch-2.8 passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; mvneclipse &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 15s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; branch-2.8 passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; findbugs &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 34s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; branch-2.8 passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; javadoc &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 13s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; branch-2.8 passed with JDK v1.8.0_121 &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; javadoc &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 16s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; branch-2.8 passed with JDK v1.7.0_121 &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; mvninstall &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 17s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; the patch passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; compile &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 14s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; the patch passed with JDK v1.8.0_121 &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; javac &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 14s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; the patch passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; compile &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 17s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; the patch passed with JDK v1.7.0_121 &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; javac &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 17s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; the patch passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;orange&quot;&gt;-0&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;orange&quot;&gt; checkstyle &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;orange&quot;&gt;  0m 11s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;orange&quot;&gt; hadoop-tools/hadoop-aws: The patch generated 1 new + 5 unchanged - 1 fixed = 6 total (was 6) &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; mvnsite &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 22s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; the patch passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; mvneclipse &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 11s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; the patch passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; whitespace &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m  0s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; The patch has no whitespace issues. &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; findbugs &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 43s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; the patch passed &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; javadoc &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 11s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; the patch passed with JDK v1.8.0_121 &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; javadoc &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 15s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; the patch passed with JDK v1.7.0_121 &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; unit &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 23s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; hadoop-aws in the patch passed with JDK v1.7.0_121. &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;+1&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; asflicense &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt;  0m 18s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;green&quot;&gt; The patch does not generate ASF License warnings. &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;black&quot;&gt;&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;black&quot;&gt; &lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;black&quot;&gt; 15m 19s&lt;/font&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;font color=&quot;black&quot;&gt; &lt;/font&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br class=&quot;atl-forced-newline&quot; /&gt;
&lt;br class=&quot;atl-forced-newline&quot; /&gt;&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Subsystem &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Report/Notes &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Docker &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  Image:yetus/hadoop:5af2af1 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; JIRA Issue &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-14204&quot; title=&quot;S3A multipart commit failing, &amp;quot;UnsupportedOperationException at java.util.Collections$UnmodifiableList.sort&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-14204&quot;&gt;&lt;del&gt;HADOOP-14204&lt;/del&gt;&lt;/a&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; JIRA Patch URL &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12859617/HADOOP-14204-branch-2.8-001.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/secure/attachment/12859617/HADOOP-14204-branch-2.8-001.patch&lt;/a&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Optional Tests &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  asflicense  compile  javac  javadoc  mvninstall  mvnsite  unit  findbugs  checkstyle  &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; uname &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Linux 3b61ea52345a 3.13.0-103-generic #150-Ubuntu SMP Thu Nov 24 10:34:17 UTC 2016 x86_64 x86_64 x86_64 GNU/Linux &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Build tool &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; maven &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Personality &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; /testptch/hadoop/patchprocess/precommit/personality/provided.sh &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; git revision &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; branch-2.8 / c7b6e0d &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Default Java &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; 1.7.0_121 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Multi-JDK versions &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;  /usr/lib/jvm/java-8-oracle:1.8.0_121 /usr/lib/jvm/java-7-openjdk-amd64:1.7.0_121 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; findbugs &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; v3.0.0 &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; checkstyle &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/11857/artifact/patchprocess/diff-checkstyle-hadoop-tools_hadoop-aws.txt&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/11857/artifact/patchprocess/diff-checkstyle-hadoop-tools_hadoop-aws.txt&lt;/a&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; JDK v1.7.0_121  Test Results &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/11857/testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/11857/testReport/&lt;/a&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; modules &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; C: hadoop-tools/hadoop-aws U: hadoop-tools/hadoop-aws &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Console output &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;https://builds.apache.org/job/PreCommit-HADOOP-Build/11857/console&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/PreCommit-HADOOP-Build/11857/console&lt;/a&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Powered by &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; Apache Yetus 0.5.0-SNAPSHOT   &lt;a href=&quot;http://yetus.apache.org&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://yetus.apache.org&lt;/a&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;



&lt;p&gt;This message was automatically generated.&lt;/p&gt;
</comment>
                            <comment id="15933663" author="liuml07" created="Mon, 20 Mar 2017 22:06:21 +0000"  >&lt;p&gt;+1 on this, Thanks Steve.&lt;/p&gt;</comment>
                            <comment id="15934616" author="hudson" created="Tue, 21 Mar 2017 13:21:07 +0000"  >&lt;p&gt;FAILURE: Integrated in Jenkins build Hadoop-trunk-Commit #11435 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-trunk-Commit/11435/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-trunk-Commit/11435/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-14204&quot; title=&quot;S3A multipart commit failing, &amp;quot;UnsupportedOperationException at java.util.Collections$UnmodifiableList.sort&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-14204&quot;&gt;&lt;del&gt;HADOOP-14204&lt;/del&gt;&lt;/a&gt; S3A multipart commit failing, (stevel: rev 2841666f1f2dec96761a0aa34a69cbb20297aa14)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;(edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3AFileSystem.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15979460" author="andrew.wang" created="Fri, 21 Apr 2017 21:52:28 +0000"  >&lt;p&gt;Please set the appropriate 3.x fix version when committing to trunk, thanks!&lt;/p&gt;</comment>
                            <comment id="16041964" author="vinodkv" created="Thu, 8 Jun 2017 00:13:36 +0000"  >&lt;p&gt;2.8.1 became a security release. Moving fix-version to 2.8.2 after the fact.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12780618">HADOOP-11694</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13057655">SPARK-20038</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12859617" name="HADOOP-14204-branch-2.8-001.patch" size="983" author="stevel@apache.org" created="Mon, 20 Mar 2017 19:34:04 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 23 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3citz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310320" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Target Version/s</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12339167">2.8.1</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>