<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 21:56:06 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[HADOOP-16900] Very large files can be truncated when written through S3AFileSystem</title>
                <link>https://issues.apache.org/jira/browse/HADOOP-16900</link>
                <project id="12310240" key="HADOOP">Hadoop Common</project>
                    <description>&lt;p&gt;If a written file size exceeds 10,000 * &lt;tt&gt;fs.s3a.multipart.size&lt;/tt&gt;, a corrupt truncation of the S3 object will occur as the maximum number of parts in a multipart upload is 10,000 as &lt;a href=&quot;https://docs.aws.amazon.com/AmazonS3/latest/dev/qfacts.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;specified&lt;/a&gt; by the S3 API, and there is an apparent bug where this failure is not fatal allowing the multipart upload operation to be marked as successfully completed without being fully complete.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13289055">HADOOP-16900</key>
            <summary>Very large files can be truncated when written through S3AFileSystem</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mukund-thakur">Mukund Thakur</assignee>
                                    <reporter username="noslowerdna">Andrew Olson</reporter>
                        <labels>
                            <label>s3</label>
                    </labels>
                <created>Mon, 2 Mar 2020 22:16:26 +0000</created>
                <updated>Mon, 17 May 2021 04:16:30 +0000</updated>
                            <resolved>Wed, 20 May 2020 12:45:47 +0000</resolved>
                                    <version>3.2.1</version>
                                    <fixVersion>3.3.1</fixVersion>
                    <fixVersion>3.4.0</fixVersion>
                                    <component>fs/s3</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="17050512" author="stevel@apache.org" created="Tue, 3 Mar 2020 19:55:42 +0000"  >&lt;p&gt;This is bad. you are probably the first person writing quite so much data.&lt;/p&gt;

&lt;p&gt;What should we do? Fail fast without writing anything? OR explicitly save as truncated before failing?&lt;/p&gt;

&lt;p&gt;+&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gabor.bota&quot; class=&quot;user-hover&quot; rel=&quot;gabor.bota&quot;&gt;gabor.bota&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17050513" author="stevel@apache.org" created="Tue, 3 Mar 2020 19:56:23 +0000"  >&lt;p&gt;(note, committers should be limit-aware too, and new multipart uploader. Sigh)&lt;/p&gt;</comment>
                            <comment id="17050575" author="noslowerdna" created="Tue, 3 Mar 2020 21:22:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gabor.bota&quot; class=&quot;user-hover&quot; rel=&quot;gabor.bota&quot;&gt;gabor.bota&lt;/a&gt; I think we should fail fast here without writing anything. The fail fast part already apparently happens as seen by distcp task failures like this,&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;19/12/26 21:45:54 INFO mapreduce.Job: Task Id : attempt_1576854935249_175694_m_000003_0, Status : FAILED
Error: java.io.IOException: File copy failed: hdfs://cluster/path/to/file.avro --&amp;gt; s3a://bucket/path/to/file.avro
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:312)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:270)
	at org.apache.hadoop.tools.mapred.CopyMapper.map(CopyMapper.java:52)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:145)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:793)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:341)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:164)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1917)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:158)
Caused by: java.io.IOException: Couldn&apos;t run retriable-command: Copying hdfs://cluster/path/to/file.avro to s3a://bucket/path/to/file.avro
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:101)
	at org.apache.hadoop.tools.mapred.CopyMapper.copyFileWithRetry(CopyMapper.java:307)
	... 10 more
Caused by: java.lang.IllegalArgumentException: partNumber must be between 1 and 10000 inclusive, but is 10001
	at com.google.common.base.Preconditions.checkArgument(Preconditions.java:115)
	at org.apache.hadoop.fs.s3a.S3AFileSystem$WriteOperationHelper.newUploadPartRequest(S3AFileSystem.java:3086)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.uploadBlockAsync(S3ABlockOutputStream.java:492)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream$MultiPartUpload.access$000(S3ABlockOutputStream.java:469)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.uploadCurrentBlock(S3ABlockOutputStream.java:307)
	at org.apache.hadoop.fs.s3a.S3ABlockOutputStream.write(S3ABlockOutputStream.java:289)
	at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.write(FSDataOutputStream.java:58)
	at java.io.DataOutputStream.write(DataOutputStream.java:107)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyBytes(RetriableFileCopyCommand.java:299)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.copyToFile(RetriableFileCopyCommand.java:216)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doCopy(RetriableFileCopyCommand.java:146)
	at org.apache.hadoop.tools.mapred.RetriableFileCopyCommand.doExecute(RetriableFileCopyCommand.java:116)
	at org.apache.hadoop.tools.util.RetriableCommand.execute(RetriableCommand.java:87)
	... 11 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;However... in distcp there&apos;s an optimization where it skips files that already exist in the destination (I think that match a checksum of some number of initial bytes?), so when the task attempt was retried it found no work to do for this large file, and the distcp job ultimately succeeded. I probably should open a separate issue to have distcp compare the file lengths before deciding that a path already present in the target location can be safely skipped.&lt;/p&gt;

&lt;p&gt;We ran into this because we had tuned the fs.s3a.multipart.size down to 25M. That effectively imposed a 250 GB limit on our S3A file writes due to the 10,000 part limit on the AWS side. Adjusting the multipart chunk size to a larger value (100M) got us past this since all our files are under 1 TB.&lt;/p&gt;

&lt;p&gt;Here&apos;s evidence of the issue we recorded noting the unexpected 289789841890 vs. 262144000000 bytes file size difference, with names changed to protect the innocent.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$ hdfs dfs -ls /path/to/file.avro
-rwxrwxr-x   3 user group 289789841890 2016-12-20 06:37 /path/to/file.avro

$ hdfs dfs -conf conf.xml -ls s3a://bucket/path/to/file.avro
-rw-rw-rw-   1 user 262144000000 2019-12-26 21:45 s3a://bucket/path/to/file.avro
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17051067" author="stevel@apache.org" created="Wed, 4 Mar 2020 10:06:31 +0000"  >&lt;p&gt;I was thinking about this and came up with a very cunning solution&lt;/p&gt;

&lt;p&gt;As the number of blocks written increases, we expand the size of each block&lt;/p&gt;

&lt;p&gt;e.g: up to 4000 blocks, you get the fs.s3a block size value. &lt;/p&gt;

&lt;p&gt;then multiples by 4x every thousand, block size * 4^5 by the last 1000 of the 10K you are at 32GB per block.&lt;/p&gt;


&lt;p&gt;What do people think? It would have some consequences &lt;del&gt;uploads use more memory&lt;/del&gt; but it would get the data up even for very large source files?&lt;/p&gt;


&lt;p&gt;I see we should also look at the part size on copy requests; we should make sure that what we tell the xfer manager to split a file up to also meets the 10K limit.&lt;/p&gt;</comment>
                            <comment id="17051070" author="stevel@apache.org" created="Wed, 4 Mar 2020 10:08:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;I probably should open a separate issue to have distcp compare the file lengths before deciding that a path already present in the target location can be safely skipped.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;thought it did. &lt;/p&gt;

&lt;p&gt;I added the etag -&amp;gt; checksum feature so that a future distcp could track checksums at the dest and look for changes there too, but that broke distcp which assumes that source checksum algorithm == dest, and therefore the values match at both ends, even across filesystem schemas (it holds for hdfs -&amp;gt; webhdfs, roughly)&lt;/p&gt;</comment>
                            <comment id="17051124" author="mukund-thakur" created="Wed, 4 Mar 2020 11:34:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;I probably should open a separate issue to have distcp compare the file lengths before deciding that a path already present in the target location can be safely skipped.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I think DistCp has already length check in place. FYI,&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/hadoop/blob/bbd704bb828577a1f0afe5fb0ac358fb7c5af446/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java#L350&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hadoop/blob/bbd704bb828577a1f0afe5fb0ac358fb7c5af446/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java#L350&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17051330" author="noslowerdna" created="Wed, 4 Mar 2020 15:35:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt;&#160;An adaptive solution like that sounds good to me. Yes depending on the&#160;fs.s3a.fast.upload.buffer and related configurations some moderate amount of additional resources could be required but it should be successful more often that not.&lt;/p&gt;

&lt;p&gt;Since DistCp does have that source vs target length check, I&apos;m not sure how our DistCp job still managed to succeed when this happened. When I have some time I&apos;ll investigate that further to try to clear up the mystery. For what it&apos;s worth we were using the -strategy dynamic option.&lt;/p&gt;</comment>
                            <comment id="17052576" author="noslowerdna" created="Thu, 5 Mar 2020 22:45:35 +0000"  >&lt;p&gt;Looked into the DistCp issue further. We didn&apos;t use the &quot;-update&quot; (aka sync folders) option because we&apos;re copying the contents of a nested directory tree, so the length check logic here &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; in the canSkip method wasn&apos;t reached. The mere existence of the target file caused it to be skipped on the DistCp map task attempt retry allowing the DistCp job to complete successfully.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;2020-03-05 15:56:33,995 INFO [main] org.apache.hadoop.tools.mapred.CopyMapper: Copying hdfs://cluster/path/to/file.avro to s3a://bucket/path/to/file.avro
2020-03-05 15:56:34,163 INFO [main] org.apache.hadoop.tools.mapred.CopyMapper: Skipping copy of hdfs://cluster/path/to/file.avro to s3a://bucket/path/to/file.avro
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the initial failed DistCp map task attempt, the DistCp RetriableFileCopyCommand wrapper did attempt the copy 3 times with the same &quot;partNumber must be between 1 and 10000 inclusive, but is 10001&quot; error each time. I think that&apos;s all what we would expect to see, there&apos;s nothing to fix in DistCp. If S3A had abandoned the multipart upload instead of incorrectly sending a CompleteMultipartUpload request, the DistCp job would have failed appropriately.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;https://github.com/cloudera/hadoop-common/blob/cdh5.14.4-release/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java#L385&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/cloudera/hadoop-common/blob/cdh5.14.4-release/hadoop-tools/hadoop-distcp/src/main/java/org/apache/hadoop/tools/mapred/CopyMapper.java#L385&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17060966" author="stevel@apache.org" created="Tue, 17 Mar 2020 14:44:07 +0000"  >&lt;p&gt;OK. simple is good.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;fail rather than complete&lt;/li&gt;
	&lt;li&gt;always abort MPUs&lt;/li&gt;
	&lt;li&gt;meaningful message with details in docs, &quot;use a bigger block size, especially for distcp&quot;&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="17060983" author="noslowerdna" created="Tue, 17 Mar 2020 15:06:57 +0000"  >&lt;p&gt;That plan sounds good from here &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="17112147" author="stevel@apache.org" created="Wed, 20 May 2020 12:45:47 +0000"  >&lt;p&gt;in trunk; rebuilding and retesting branch-3.3 with it too&lt;/p&gt;</comment>
                            <comment id="17112168" author="hudson" created="Wed, 20 May 2020 13:01:34 +0000"  >&lt;p&gt;SUCCESS: Integrated in Jenkins build Hadoop-trunk-Commit #18280 (See &lt;a href=&quot;https://builds.apache.org/job/Hadoop-trunk-Commit/18280/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Hadoop-trunk-Commit/18280/&lt;/a&gt;)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-16900&quot; title=&quot;Very large files can be truncated when written through S3AFileSystem&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-16900&quot;&gt;&lt;del&gt;HADOOP-16900&lt;/del&gt;&lt;/a&gt;. Very large files can be truncated when written through the (stevel: rev 29b19cd59245c8809b697b3d7d7445813a685aad)&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;(edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/impl/InternalConstants.java&lt;/li&gt;
	&lt;li&gt;(edit) hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/TestS3ABlockOutputStream.java&lt;/li&gt;
	&lt;li&gt;(edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/commit/CommitOperations.java&lt;/li&gt;
	&lt;li&gt;(add) hadoop-tools/hadoop-aws/src/test/java/org/apache/hadoop/fs/s3a/scale/ITestS3AMultipartUploadSizeLimits.java&lt;/li&gt;
	&lt;li&gt;(edit) hadoop-tools/hadoop-aws/src/site/markdown/tools/hadoop-aws/troubleshooting_s3a.md&lt;/li&gt;
	&lt;li&gt;(edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/S3ABlockOutputStream.java&lt;/li&gt;
	&lt;li&gt;(edit) hadoop-tools/hadoop-aws/src/main/java/org/apache/hadoop/fs/s3a/WriteOperationHelper.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17112170" author="noslowerdna" created="Wed, 20 May 2020 13:02:40 +0000"  >&lt;p&gt;Thanks for fixing this, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stevel%40apache.org&quot; class=&quot;user-hover&quot; rel=&quot;stevel@apache.org&quot;&gt;stevel@apache.org&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 25 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0c3ew:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>