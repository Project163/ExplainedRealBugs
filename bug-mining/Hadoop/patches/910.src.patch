diff --git a/hadoop-common-project/hadoop-common/CHANGES.txt b/hadoop-common-project/hadoop-common/CHANGES.txt
index 6cc06cf6b6e..8c1e8970af9 100644
--- a/hadoop-common-project/hadoop-common/CHANGES.txt
+++ b/hadoop-common-project/hadoop-common/CHANGES.txt
@@ -775,6 +775,9 @@ Release 2.1.0-beta - 2013-07-02
     HADOOP-9264. Port change to use Java untar API on Windows from 
     branch-1-win to trunk. (Chris Nauroth via suresh)
 
+    HADOOP-9665. Fixed BlockDecompressorStream#decompress to return -1 rather
+    than throw EOF at end of file. (Zhijie Shen via acmurthy)
+
 Release 2.0.5-alpha - 06/06/2013
 
   INCOMPATIBLE CHANGES
diff --git a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java
index 7d2504e3e22..72509c72c7d 100644
--- a/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java
+++ b/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java
@@ -93,7 +93,13 @@ protected int decompress(byte[] b, int off, int len) throws IOException {
         }
       }
       if (decompressor.needsInput()) {
-        int m = getCompressedData();
+        int m;
+        try {
+          m = getCompressedData();
+        } catch (EOFException e) {
+          eof = true;
+          return -1;
+        }
         // Send the read data to the decompressor
         decompressor.setInput(buffer, 0, m);
       }
diff --git a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestBlockDecompressorStream.java b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestBlockDecompressorStream.java
index b3acede8ee6..43bd413b81a 100644
--- a/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestBlockDecompressorStream.java
+++ b/hadoop-common-project/hadoop-common/src/test/java/org/apache/hadoop/io/compress/TestBlockDecompressorStream.java
@@ -17,14 +17,16 @@
  */
 package org.apache.hadoop.io.compress;
 
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
+
 import java.io.ByteArrayInputStream;
 import java.io.ByteArrayOutputStream;
 import java.io.IOException;
+import java.nio.ByteBuffer;
 
 import org.apache.hadoop.conf.Configuration;
-
 import org.junit.Test;
-import static org.junit.Assert.*;
 
 public class TestBlockDecompressorStream {
   
@@ -33,9 +35,23 @@ public class TestBlockDecompressorStream {
   private ByteArrayOutputStream bytesOut;
 
   @Test
-  public void testRead() throws IOException {
+  public void testRead1() throws IOException {
+    testRead(0);
+  }
+
+  @Test
+  public void testRead2() throws IOException {
+    // Test eof after getting non-zero block size info
+    testRead(4);
+  }
+
+  private void testRead(int bufLen) throws IOException {
     // compress empty stream
     bytesOut = new ByteArrayOutputStream();
+    if (bufLen > 0) {
+      bytesOut.write(ByteBuffer.allocate(bufLen).putInt(1024).array(), 0,
+          bufLen);
+    }
     BlockCompressorStream blockCompressorStream = 
       new BlockCompressorStream(bytesOut, 
           new FakeCompressor(), 1024, 0);
@@ -44,7 +60,8 @@ public void testRead() throws IOException {
     
     // check compressed output 
     buf = bytesOut.toByteArray();
-    assertEquals("empty file compressed output size is not 4", 4, buf.length);
+    assertEquals("empty file compressed output size is not " + (bufLen + 4),
+        bufLen + 4, buf.length);
     
     // use compressed output as input for decompression
     bytesIn = new ByteArrayInputStream(buf);
@@ -57,6 +74,8 @@ public void testRead() throws IOException {
           -1 , blockDecompressorStream.read());
     } catch (IOException e) {
       fail("unexpected IOException : " + e);
+    } finally {
+      blockDecompressorStream.close();
     }
   }
 }
