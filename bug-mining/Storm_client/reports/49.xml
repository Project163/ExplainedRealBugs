<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Sat Nov 08 21:46:29 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[STORM-2853] Deactivated topologies cause high cpu utilization</title>
                <link>https://issues.apache.org/jira/browse/STORM-2853</link>
                <project id="12314820" key="STORM">Apache Storm</project>
                    <description>&lt;p&gt;The issue is there is high cpu usage for deactivated apache storm topologies.  I can reliably re-create the issue using the steps below but I haven&apos;t identified the exact cause or a solution yet.&lt;/p&gt;

&lt;p&gt;The environment is a storm cluster on which 1 topology is running (The topology is extremely simple, I used the exclamation example).  It is INACTIVE.  Initially there is normal CPU usage.  However, when I kill all topology JVM processes on all supervisors and let Storm restart them again, I find that some time later (~9 hours) the CPU usage per JVM process rises to nearly 100%.  I have tested an ACTIVE topology and this does not happen with it.  I have also tested more than one topology and observe the same results when they&apos;re in the INACTIVE state.&lt;/p&gt;

&lt;p&gt;**&lt;b&gt;Steps to re-create:&lt;/b&gt;**&lt;/p&gt;

&lt;p&gt; 1. Run 1 topology on an Apache Storm cluster&lt;br/&gt;
 2. Deactivate it&lt;br/&gt;
 3. Kill *&lt;b&gt;all&lt;/b&gt;* topology JVM processes on all supervisors (Storm will restart them)&lt;br/&gt;
 4. Observe the CPU usage on Supervisors rise to nearly 100% for all *&lt;b&gt;INACTIVE&lt;/b&gt;* topology JVM processes.&lt;/p&gt;

&lt;p&gt;**&lt;b&gt;Environment&lt;/b&gt;**&lt;/p&gt;

&lt;p&gt;Apache Storm 1.1.0 running on 3 VMs (1 nimbus and 2 supervisors).&lt;/p&gt;

&lt;p&gt;Cluster Summary:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Supervisors: 2&lt;/li&gt;
	&lt;li&gt;Used Slots: 2&lt;/li&gt;
	&lt;li&gt;Available Slots: 38&lt;/li&gt;
	&lt;li&gt;Total Slots: 40&lt;/li&gt;
	&lt;li&gt;Executors: 50&lt;/li&gt;
	&lt;li&gt;Tasks: 50&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;the topology has 2 workers and 50 executors/tasks (threads).&lt;/p&gt;


&lt;p&gt;**&lt;b&gt;Investigation so far:&lt;/b&gt;**&lt;/p&gt;

&lt;p&gt;Apart from being able to reliably re-create the issue, I have identified, for the affected topology JVM process, the threads using the most CPU.  There are 102 threads total in the process, 97 blocked, 5 IN_NATIVE.  The threads using the most CPU are identical and there are 23 of them (all in BLOCKED state):&lt;/p&gt;

&lt;p&gt;    Thread 28558: (state = BLOCKED)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)&lt;/li&gt;
	&lt;li&gt;java.util.concurrent.locks.LockSupport.parkNanos(long) @bci=11, line=338 (Compiled frame)&lt;/li&gt;
	&lt;li&gt;com.lmax.disruptor.MultiProducerSequencer.next(int) @bci=82, line=136 (Compiled frame)&lt;/li&gt;
	&lt;li&gt;com.lmax.disruptor.RingBuffer.next(int) @bci=5, line=260 (Interpreted frame)&lt;/li&gt;
	&lt;li&gt;org.apache.storm.utils.DisruptorQueue.publishDirect(java.util.ArrayList, boolean) @bci=18, line=517 (Interpreted frame)&lt;/li&gt;
	&lt;li&gt;org.apache.storm.utils.DisruptorQueue.access$1000(org.apache.storm.utils.DisruptorQueue, java.util.ArrayList, boolean) @bci=3, line=61 (Interpreted frame)&lt;/li&gt;
	&lt;li&gt;org.apache.storm.utils.DisruptorQueue$ThreadLocalBatcher.flush(boolean) @bci=50, line=280 (Interpreted frame)&lt;/li&gt;
	&lt;li&gt;org.apache.storm.utils.DisruptorQueue$Flusher.run() @bci=55, line=303 (Interpreted frame)&lt;/li&gt;
	&lt;li&gt;java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)&lt;/li&gt;
	&lt;li&gt;java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)&lt;/li&gt;
	&lt;li&gt;java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1142 (Compiled frame)&lt;/li&gt;
	&lt;li&gt;java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=617 (Interpreted frame)&lt;/li&gt;
	&lt;li&gt;java.lang.Thread.run() @bci=11, line=745 (Interpreted frame)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;I identified this thread by using `jstack` to get a thread dump for the process:&lt;/p&gt;


&lt;p&gt;    jstack -F &amp;lt;pid&amp;gt; &amp;gt; jstack&amp;lt;pid&amp;gt;.txt&lt;/p&gt;

&lt;p&gt;and `top` to identify the threads within the process using the most CPU:&lt;/p&gt;

&lt;p&gt;    top -H -p &amp;lt;pid&amp;gt; &lt;/p&gt;</description>
                <environment></environment>
        <key id="13124655">STORM-2853</key>
            <summary>Deactivated topologies cause high cpu utilization</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="kabhwan">Jungtaek Lim</assignee>
                                    <reporter username="shannoncode">Stuart</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Wed, 13 Dec 2017 12:17:35 +0000</created>
                <updated>Sun, 26 Jan 2025 09:25:21 +0000</updated>
                            <resolved>Fri, 2 Feb 2018 04:21:03 +0000</resolved>
                                    <version>1.1.0</version>
                                    <fixVersion>2.0.0</fixVersion>
                    <fixVersion>1.2.0</fixVersion>
                    <fixVersion>1.1.2</fixVersion>
                    <fixVersion>1.0.6</fixVersion>
                                    <component>storm-core</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>4</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="4800">1h 20m</timespent>
                                <comments>
                            <comment id="16290696" author="vorin" created="Thu, 14 Dec 2017 10:40:00 +0000"  >&lt;p&gt;I&apos;m having the same issue. It&apos;s pretty troubling. Any help would be appreciated.&lt;/p&gt;</comment>
                            <comment id="16290708" author="shannoncode" created="Thu, 14 Dec 2017 10:50:08 +0000"  >&lt;p&gt;The topology I am using to re-create this issue.&lt;/p&gt;</comment>
                            <comment id="16347172" author="vorin" created="Wed, 31 Jan 2018 16:58:34 +0000"  >&lt;p&gt;The issue happens because the RingBuffer in a &lt;a href=&quot;https://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;DisruptorQueue&lt;/a&gt;&#160;fills up and when publishing threads are trying to claim a slot they effectively get stuck doing &lt;em&gt;LockSupport.parkNanos(1L)&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;The whole scenario looks like this. When a topology is active some objects and tuples get published to the DisruptorQueue and they are consumed at the same time. After deactivating the topology the tick tuples (maybe some other kind of tuples as well) are still published and consumed. But then when the worker process is killed and restored the tuples are still published but are no longer consumed. Then the RingBuffer fills up and publishing threads cause CPU spike. This happens because if the topology starts up as deactivated the bolts are not started. That behaviour is caused by the code here:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/storm/blob/v1.1.0/storm-core/src/clj/org/apache/storm/daemon/executor.clj#L744&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/storm/blob/v1.1.0/storm-core/src/clj/org/apache/storm/daemon/executor.clj#L744&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In my opinion it is a bug, since the behaviour of a deactivated topology is not consistent before and after a JVM&apos;s restart.&lt;/p&gt;

&lt;p&gt;A permanent fix would be potentially to remove the sleeping part all toghether.&lt;/p&gt;

&lt;p&gt;A quick workaround mitigating the issue could be to check the RingBuffer state and stop trying publishing if it is full, e.g. by adding this code:&lt;br/&gt;
&lt;em&gt;while (_buffer.remainingCapacity() == 0) {&lt;/em&gt;&lt;br/&gt;
 &lt;em&gt;Utils.sleep(1);&lt;/em&gt;&lt;br/&gt;
&lt;em&gt;}&lt;/em&gt;&lt;br/&gt;
here &lt;a href=&quot;https://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java#L517&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/storm/blob/v1.1.0/storm-core/src/jvm/org/apache/storm/utils/DisruptorQueue.java#L517&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Any comments or other considerations would be appreciated&lt;/p&gt;</comment>
                            <comment id="16347803" author="kabhwan" created="Wed, 31 Jan 2018 23:59:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vorin&quot; class=&quot;user-hover&quot; rel=&quot;vorin&quot;&gt;vorin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Makes sense. The code might only consider about startup of topology,&#160;which is expected to be finished within small period, but it triggers the issue when there&apos;s a case worker is restarting while topology is inactive. Not sure it really did, since the code was placed at really older version, before 0.9.0 I guess, donated to Apache incubator. Nathan&apos;s repo is no longer exist so I can&apos;t track even why the code was placed for now, so I&apos;m not 100% sure we know the possible risks of getting rid of the sleeping part.&lt;/p&gt;

&lt;p&gt;The thing which would make the issue more complicated is, we can&apos;t disable tick tuple while topology is in inactive state, since from bolt&apos;s perspective there&#160;might be remaining tuples to process and it needs tick tuple to take some action like flush.&lt;/p&gt;

&lt;p&gt;The only way what I can think about for now is defer&#160;activating tick tuple until all tasks in executor are completed calling open()/prepare(). Even if it makes sense, not sure whether it is easy to address or not. Need to take a look at.&lt;/p&gt;</comment>
                            <comment id="16347870" author="kabhwan" created="Thu, 1 Feb 2018 01:40:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vorin&quot; class=&quot;user-hover&quot; rel=&quot;vorin&quot;&gt;vorin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Fortunately the fix&#160;turned out to be really simple. I submitted pull requests against master and 1.x-branch. Please take a look at if you are interested on the patch.&lt;/p&gt;</comment>
                            <comment id="16348759" author="vorin" created="Thu, 1 Feb 2018 15:37:48 +0000"  >&lt;p&gt;I&apos;m very interested as it affects our production env. Thanks for the quick fix. I&apos;ll test it and update you when I&apos;m done.&lt;/p&gt;

&lt;p&gt;BTW I&apos;m trying to figure out how the storm repo is organised. What&apos;s the difference between the master and e.g. 1.x-branch? Currently the most likely explanation for me is that the master branch contains various additional projects, whereas e.g. 1.x-branch contains only what in the final distro?&lt;/p&gt;

&lt;p&gt;Also the 2 PRs you raised. One is for the 1.x-branch in the storm-core, which is the project I was working with so far. The other is for the master branch in the storm-client project, which seems to mirror some of the storm-core functionality but without Clojure part, am I right? Am I right that the storm-client project is not included in standard storm distro? I&apos;m trying to understand that structure so I can properly build a patched version of the storm binaries.&lt;/p&gt;

&lt;p&gt;Is there any doc that would help me understand that repo/code structure? Didn&apos;t find anything useful in the Doc folder&lt;/p&gt;

&lt;p&gt;Thanks&lt;/p&gt;</comment>
                            <comment id="16349447" author="kabhwan" created="Thu, 1 Feb 2018 22:56:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vorin&quot; class=&quot;user-hover&quot; rel=&quot;vorin&quot;&gt;vorin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Storm 2.0.0 was started for porting Clojure to Java, which was one of major change for merging JStorm. While merging JStorm is&#160;not happened, we had many voices (including me) who are in favor of get rid of Clojure in various reasons, so we went ahead and ported most of things to Java in master branch.(Regarding merging JStorm, we can restart merging&#160;works at any time&#160;since they donate the code to ASF, but huge divergence between twos are not easy to cover.)&lt;/p&gt;

&lt;p&gt;We also have several improvements in Storm 2.0.0, and what you see is one of them: we broke storm-core down into multiple modules in upcoming Storm 2.0.0, &quot;storm-client&quot; which is related to client topology side (worker) interfaces and implementations (will have much less dependencies than current), &quot;storm-server&quot; which is related to daemon side interfaces and implementations, &quot;storm-webserver&quot; which is related to HTTP service and UI. We still keep &quot;storm-core&quot; since we didn&apos;t port back some tests yet.&lt;/p&gt;

&lt;p&gt;So if you are really brave to test Storm 2.0.0 SNAPSHOT out, please check out master branch and build your own dist. Most of the time you would want to check out 1.x-branch to test out the change of latest 1.x version line.&lt;/p&gt;</comment>
                            <comment id="16349765" author="kabhwan" created="Fri, 2 Feb 2018 04:21:03 +0000"  >&lt;p&gt;Merged the patch into master, 1.x, 1.1.x, 1.0.x branches.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vorin&quot; class=&quot;user-hover&quot; rel=&quot;vorin&quot;&gt;vorin&lt;/a&gt;, I merged the patch into branches to make sure they&apos;re included to current ongoing RCs. Please reopen the issue if the patch doesn&apos;t resolve your issue. Thanks in advance!&lt;/p&gt;</comment>
                            <comment id="16350212" author="vorin" created="Fri, 2 Feb 2018 12:00:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kabhwan&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Jungtaek Lim&lt;/a&gt;&#160;I tested the patch. In our use case it resolves the issue. Thank you.&lt;/p&gt;

&lt;p&gt;My only reservation is that we removed one of the symptoms but the original inconsistency/root cause remains. Meaning that the behavior of a deactivated topology is not consistent before and after a JVM&apos;s restart. &lt;br/&gt;
 As an example. As you pointed out when a topology is deactivated we want to keep tick tuples coming as there might be remaining tuples to process/flush. Then let&apos;s assume that the worker process dies/is killed before the processing/flush can finish. In that case stateful bolts normally would resume processing using those tick tuples but currently they won&apos;t have a chance to do it.&lt;br/&gt;
 Well, it&apos;s just a consideration. I do understand the bigger picture perspective here and that the risks of making the bigger functional change dealing with the root cause could be too big.&lt;/p&gt;

&lt;p&gt;Also thanks for the explanation. It cleared up a lot. Quite an interesting story as well with Clojure and JStorm&#160;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="16354766" author="kabhwan" created="Wed, 7 Feb 2018 00:10:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vorin&quot; class=&quot;user-hover&quot; rel=&quot;vorin&quot;&gt;vorin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I guess you&apos;re saying stateful implemented by user side: I don&apos;t think they can use intermediate result while crash occurs, since the result is not guaranteed to be not corrupted. If you&apos;re saying stateful supported by Storm, it doesn&apos;t leverage tick tuple from bolt side. Please elaborate your example a bit if my explanation doesn&apos;t fall into your case.&lt;/p&gt;</comment>
                            <comment id="16360579" author="vorin" created="Mon, 12 Feb 2018 10:46:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kabhwan&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;Jungtaek Lim&lt;/a&gt;&#160;&lt;/p&gt;

&lt;p&gt;Yes, stateful bolts implemented&#160;by a user. Our current application fortunately wouldn&apos;t run into such a situation. So it is more of a theoretical scenario.&lt;/p&gt;

&lt;p&gt;So one can imagine a case e.g. of a bolt that emits tuples in batches. Let&apos;s assume it stores them somewhere e.g. in an external cache. Then when the topology is deactivated it&#160;could want to flush the current batch (e.g. after a timeout which&#160;I can imagine is triggered by a tick tuple in most implementations). So if the JVM restart happens before the timeout, the flush will not happen until the topology is activated and receives the first tick tuple. Whereas it would happen if the JVM hasn&apos;t restarted, that&apos;s the inconsistency. It&apos;s is an edge case scenario of course.&lt;/p&gt;</comment>
                            <comment id="17919826" author="githubbot" created="Sun, 26 Jan 2025 09:25:21 +0000"  >&lt;p&gt;For your information, &lt;a href=&quot;https://issues.apache.org/jira/projects/STORM/issues/&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;all STORM issues&lt;/a&gt;  have been transferred to Github: &lt;a href=&quot;https://github.com/apache/storm/issues/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/storm/issues/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Here is the direct link to this issue in Github: &lt;a href=&quot;https://github.com/apache/storm/issues//6635&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/storm/issues//6635&lt;/a&gt;&lt;br/&gt;
And here is the link to a search for related issues: &lt;a href=&quot;https://github.com/apache/storm/issues/?q=%22STORM-2853%22&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/storm/issues/?q=%22STORM-2853%22&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;(Note: this is an automated bulk comment)&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12902049" name="exclamation.zip" size="5584" author="shannoncode" created="Thu, 14 Dec 2017 10:49:52 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[no_permission]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            40 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3nurb:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            </customfields>
    </item>
</channel>
</rss>