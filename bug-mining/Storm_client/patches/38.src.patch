diff --git a/storm-client/src/jvm/org/apache/storm/daemon/Task.java b/storm-client/src/jvm/org/apache/storm/daemon/Task.java
index 0bc56fb01..658aadc37 100644
--- a/storm-client/src/jvm/org/apache/storm/daemon/Task.java
+++ b/storm-client/src/jvm/org/apache/storm/daemon/Task.java
@@ -62,7 +62,6 @@ public class Task {
     private TopologyContext systemTopologyContext;
     private TopologyContext userTopologyContext;
     private WorkerTopologyContext workerTopologyContext;
-    private LoadMapping loadMapping;
     private Integer taskId;
     private String componentId;
     private Object taskObject; // Spout/Bolt object
@@ -84,7 +83,6 @@ public class Task {
         this.builtInMetrics = BuiltinMetricsUtil.mkData(executor.getType(), this.executorStats);
         this.workerTopologyContext = executor.getWorkerTopologyContext();
         this.emitSampler = ConfigUtils.mkStatsSampler(topoConf);
-        this.loadMapping = workerData.getLoadMapping();
         this.systemTopologyContext = mkTopologyContext(workerData.getSystemTopology());
         this.userTopologyContext = mkTopologyContext(workerData.getTopology());
         this.taskObject = mkTaskObject();
diff --git a/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java b/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java
index 378099cb6..9e7bd0b6f 100644
--- a/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java
+++ b/storm-client/src/jvm/org/apache/storm/daemon/worker/Worker.java
@@ -286,7 +286,7 @@ public class Worker implements Shutdownable, DaemonCommon {
     }
 
     public void doRefreshLoad() {
-        workerState.refreshLoad();
+        workerState.refreshLoad(executorsAtom.get());
 
         final List<IRunningExecutor> executors = executorsAtom.get();
         for (IRunningExecutor executor : executors) {
diff --git a/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java b/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java
index d2444916e..33ea5793d 100644
--- a/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java
+++ b/storm-client/src/jvm/org/apache/storm/daemon/worker/WorkerState.java
@@ -28,6 +28,7 @@ import org.apache.storm.cluster.IStormClusterState;
 import org.apache.storm.cluster.VersionedData;
 import org.apache.storm.daemon.StormCommon;
 import org.apache.storm.daemon.supervisor.AdvancedFSOps;
+import org.apache.storm.executor.IRunningExecutor;
 import org.apache.storm.generated.Assignment;
 import org.apache.storm.generated.DebugOptions;
 import org.apache.storm.generated.Grouping;
@@ -444,15 +445,20 @@ public class WorkerState {
         this.throttleOn.set(backpressure);
     }
 
-    public void refreshLoad() {
-        Set<Integer> remoteTasks = Sets.difference(new HashSet<Integer>(outboundTasks), new HashSet<>(taskIds));
+    private static double getQueueLoad(DisruptorQueue q) {
+        DisruptorQueue.QueueMetrics qMetrics = q.getMetrics();
+        return ((double) qMetrics.population()) / qMetrics.capacity();
+    }
+
+    public void refreshLoad(List<IRunningExecutor> execs) {
+        Set<Integer> remoteTasks = Sets.difference(new HashSet<>(outboundTasks), new HashSet<>(taskIds));
         Long now = System.currentTimeMillis();
-        Map<Integer, Double> localLoad = shortExecutorReceiveQueueMap.entrySet().stream().collect(Collectors.toMap(
-            (Function<Map.Entry<Integer, DisruptorQueue>, Integer>) Map.Entry::getKey,
-            (Function<Map.Entry<Integer, DisruptorQueue>, Double>) entry -> {
-                DisruptorQueue.QueueMetrics qMetrics = entry.getValue().getMetrics();
-                return ( (double) qMetrics.population()) / qMetrics.capacity();
-            }));
+        Map<Integer, Double> localLoad = new HashMap<>();
+        for (IRunningExecutor exec: execs) {
+            double receiveLoad = getQueueLoad(exec.getReceiveQueue());
+            double sendLoad = getQueueLoad(exec.getSendQueue());
+            localLoad.put(exec.getExecutorId().get(0).intValue(), Math.max(receiveLoad, sendLoad));
+        }
 
         Map<Integer, Load> remoteLoad = new HashMap<>();
         cachedNodeToPortSocket.get().values().stream().forEach(conn -> remoteLoad.putAll(conn.getLoad(remoteTasks)));
diff --git a/storm-client/src/jvm/org/apache/storm/executor/Executor.java b/storm-client/src/jvm/org/apache/storm/executor/Executor.java
index b787fbbad..e55aca0b2 100644
--- a/storm-client/src/jvm/org/apache/storm/executor/Executor.java
+++ b/storm-client/src/jvm/org/apache/storm/executor/Executor.java
@@ -115,7 +115,7 @@ public abstract class Executor implements Callable, EventHandler<Object> {
 
     protected final IReportError reportError;
     protected final Random rand;
-    protected final DisruptorQueue transferQueue;
+    protected final DisruptorQueue sendQueue;
     protected final DisruptorQueue receiveQueue;
     protected Map<Integer, Task> idToTask;
     protected final Map<String, String> credentials;
@@ -140,8 +140,8 @@ public abstract class Executor implements Callable, EventHandler<Object> {
         this.stormActive = workerData.getIsTopologyActive();
         this.stormComponentDebug = workerData.getStormComponentToDebug();
 
-        this.transferQueue = mkExecutorBatchQueue(topoConf, executorId);
-        this.executorTransfer = new ExecutorTransfer(workerData, transferQueue, topoConf);
+        this.sendQueue = mkExecutorBatchQueue(topoConf, executorId);
+        this.executorTransfer = new ExecutorTransfer(workerData, sendQueue, topoConf);
 
         this.suicideFn = workerData.getSuicideCallback();
         try {
@@ -253,7 +253,7 @@ public abstract class Executor implements Callable, EventHandler<Object> {
         setupTicks(StatsUtil.SPOUT.equals(type));
 
         LOG.info("Finished loading executor " + componentId + ":" + executorId);
-        return new ExecutorShutdown(this, Lists.newArrayList(systemThreads, handlers), idToTask);
+        return new ExecutorShutdown(this, Lists.newArrayList(systemThreads, handlers), idToTask, receiveQueue, sendQueue);
     }
 
     public abstract void tupleActionFn(int taskId, TupleImpl tuple) throws Exception;
@@ -562,7 +562,7 @@ public abstract class Executor implements Callable, EventHandler<Object> {
     }
 
     public DisruptorQueue getTransferWorkerQueue() {
-        return transferQueue;
+        return sendQueue;
     }
 
     public IStormClusterState getStormClusterState() {
diff --git a/storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java b/storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java
index 7ea48b043..c7691e4c1 100644
--- a/storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java
+++ b/storm-client/src/jvm/org/apache/storm/executor/ExecutorShutdown.java
@@ -31,6 +31,7 @@ import org.apache.storm.task.TopologyContext;
 import org.apache.storm.tuple.AddressedTuple;
 import org.apache.storm.tuple.TupleImpl;
 import org.apache.storm.tuple.Values;
+import org.apache.storm.utils.DisruptorQueue;
 import org.apache.storm.utils.Utils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -44,11 +45,16 @@ public class ExecutorShutdown implements Shutdownable, IRunningExecutor {
     private final Executor executor;
     private final List<Utils.SmartThread> threads;
     private final Map<Integer, Task> taskDatas;
+    private final DisruptorQueue receiveQueue;
+    private final DisruptorQueue sendQueue;
 
-    public ExecutorShutdown(Executor executor, List<Utils.SmartThread> threads, Map<Integer, Task> taskDatas) {
+    public ExecutorShutdown(Executor executor, List<Utils.SmartThread> threads, Map<Integer, Task> taskDatas,
+                            DisruptorQueue receiveQueue, DisruptorQueue sendQueue) {
         this.executor = executor;
         this.threads = threads;
         this.taskDatas = taskDatas;
+        this.receiveQueue = receiveQueue;
+        this.sendQueue = sendQueue;
     }
 
     @Override
@@ -79,6 +85,16 @@ public class ExecutorShutdown implements Shutdownable, IRunningExecutor {
         return executor.getBackpressure();
     }
 
+    @Override
+    public DisruptorQueue getReceiveQueue() {
+        return receiveQueue;
+    }
+
+    @Override
+    public DisruptorQueue getSendQueue() {
+        return sendQueue;
+    }
+
     @Override
     public void shutdown() {
         try {
diff --git a/storm-client/src/jvm/org/apache/storm/executor/IRunningExecutor.java b/storm-client/src/jvm/org/apache/storm/executor/IRunningExecutor.java
index e7a4117cc..4b7f483d3 100644
--- a/storm-client/src/jvm/org/apache/storm/executor/IRunningExecutor.java
+++ b/storm-client/src/jvm/org/apache/storm/executor/IRunningExecutor.java
@@ -22,6 +22,7 @@ import org.apache.storm.generated.ExecutorStats;
 import org.apache.storm.grouping.LoadMapping;
 
 import java.util.List;
+import org.apache.storm.utils.DisruptorQueue;
 
 public interface IRunningExecutor {
 
@@ -30,4 +31,6 @@ public interface IRunningExecutor {
     void credentialsChanged(Credentials credentials);
     void loadChanged(LoadMapping loadMapping);
     boolean getBackPressureFlag();
+    DisruptorQueue getReceiveQueue();
+    DisruptorQueue getSendQueue();
 }
diff --git a/storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java b/storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
index 5d9edf198..4e46dc50e 100644
--- a/storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
+++ b/storm-client/src/jvm/org/apache/storm/executor/bolt/BoltExecutor.java
@@ -70,7 +70,7 @@ public class BoltExecutor extends Executor {
                 ((ICredentialsListener) boltObject).setCredentials(credentials);
             }
             if (Constants.SYSTEM_COMPONENT_ID.equals(componentId)) {
-                Map<String, DisruptorQueue> map = ImmutableMap.of("sendqueue", transferQueue, "receive", receiveQueue,
+                Map<String, DisruptorQueue> map = ImmutableMap.of("sendqueue", sendQueue, "receive", receiveQueue,
                         "transfer", workerData.getTransferQueue());
                 BuiltinMetricsUtil.registerQueueMetrics(map, topoConf, userContext);
 
@@ -78,7 +78,7 @@ public class BoltExecutor extends Executor {
                 BuiltinMetricsUtil.registerIconnectionClientMetrics(cachedNodePortToSocket, topoConf, userContext);
                 BuiltinMetricsUtil.registerIconnectionServerMetric(workerData.getReceiver(), topoConf, userContext);
             } else {
-                Map<String, DisruptorQueue> map = ImmutableMap.of("sendqueue", transferQueue, "receive", receiveQueue);
+                Map<String, DisruptorQueue> map = ImmutableMap.of("sendqueue", sendQueue, "receive", receiveQueue);
                 BuiltinMetricsUtil.registerQueueMetrics(map, topoConf, userContext);
             }
 
diff --git a/storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java b/storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
index 6e85f34eb..c465338de 100644
--- a/storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
+++ b/storm-client/src/jvm/org/apache/storm/executor/spout/SpoutExecutor.java
@@ -118,7 +118,7 @@ public class SpoutExecutor extends Executor {
             this.outputCollectors.add(outputCollector);
 
             taskData.getBuiltInMetrics().registerAll(topoConf, taskData.getUserContext());
-            Map<String, DisruptorQueue> map = ImmutableMap.of("sendqueue", transferQueue, "receive", receiveQueue);
+            Map<String, DisruptorQueue> map = ImmutableMap.of("sendqueue", sendQueue, "receive", receiveQueue);
             BuiltinMetricsUtil.registerQueueMetrics(map, topoConf, taskData.getUserContext());
 
             if (spoutObject instanceof ICredentialsListener) {
@@ -152,7 +152,7 @@ public class SpoutExecutor extends Executor {
                             spout.activate();
                         }
                     }
-                    if (!transferQueue.isFull() && !throttleOn && !reachedMaxSpoutPending) {
+                    if (!sendQueue.isFull() && !throttleOn && !reachedMaxSpoutPending) {
                         for (ISpout spout : spouts) {
                             spout.nextTuple();
                         }
diff --git a/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java b/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java
index 0c0560a3a..f5b63ec5e 100644
--- a/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java
+++ b/storm-client/src/jvm/org/apache/storm/grouping/LoadAwareShuffleGrouping.java
@@ -15,61 +15,61 @@
  * See the License for the specific language governing permissions and
  * limitations under the License.
  */
+
 package org.apache.storm.grouping;
 
+import com.google.common.annotations.VisibleForTesting;
 import java.io.Serializable;
-import java.util.ArrayList;
 import java.util.Arrays;
-import java.util.Collections;
+import java.util.HashMap;
 import java.util.List;
+import java.util.Map;
 import java.util.Random;
-import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicInteger;
-
-import com.google.common.annotations.VisibleForTesting;
-import org.apache.commons.lang.ArrayUtils;
 import org.apache.storm.generated.GlobalStreamId;
 import org.apache.storm.task.WorkerTopologyContext;
 
 public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping, Serializable {
-    private static final int CAPACITY = 1000;
+    static final int CAPACITY = 1000;
+    private static final int MAX_WEIGHT = 100;
+    private static class IndexAndWeights {
+        final int index;
+        int weight;
+
+        IndexAndWeights(int index) {
+            this.index = index;
+            weight = MAX_WEIGHT;
+        }
+    }
 
+    private final Map<Integer, IndexAndWeights> orig = new HashMap<>();
     private Random random;
-    private List<Integer>[] rets;
-    private int[] targets;
-    private int[] loads;
-    private int[] unassigned;
-    private int[] choices;
-    private int[] prepareChoices;
+    @VisibleForTesting
+    List<Integer>[] rets;
+    @VisibleForTesting
+    volatile int[] choices;
+    private volatile int[] prepareChoices;
     private AtomicInteger current;
 
     @Override
     public void prepare(WorkerTopologyContext context, GlobalStreamId stream, List<Integer> targetTasks) {
         random = new Random();
 
-        rets = (List<Integer>[])new List<?>[targetTasks.size()];
-        targets = new int[targetTasks.size()];
-        for (int i = 0; i < targets.length; i++) {
-            rets[i] = Arrays.asList(targetTasks.get(i));
-            targets[i] = targetTasks.get(i);
+        rets = (List<Integer>[]) new List<?>[targetTasks.size()];
+        int i = 0;
+        for (int target : targetTasks) {
+            rets[i] = Arrays.asList(target);
+            orig.put(target, new IndexAndWeights(i));
+            i++;
         }
 
         // can't leave choices to be empty, so initiate it similar as ShuffleGrouping
         choices = new int[CAPACITY];
 
-        for (int i = 0 ; i < CAPACITY ; i++) {
-            choices[i] = i % rets.length;
-        }
-
-        shuffleArray(choices);
-        current = new AtomicInteger(-1);
-
+        current = new AtomicInteger(0);
         // allocate another array to be switched
         prepareChoices = new int[CAPACITY];
-
-        // allocating only once
-        loads = new int[targets.length];
-        unassigned = new int[targets.length];
+        updateRing(null);
     }
 
     @Override
@@ -93,48 +93,44 @@ public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping,
         updateRing(loadMapping);
     }
 
-    private void updateRing(LoadMapping load) {
-        int localTotal = 0;
-        for (int i = 0 ; i < targets.length; i++) {
-            int val = (int)(101 - (load.get(targets[i]) * 100));
-            loads[i] = val;
-            localTotal += val;
+    private synchronized void updateRing(LoadMapping load) {
+        //We will adjust weights based off of the minimum load
+        double min = load == null ? 0 : orig.keySet().stream().mapToDouble((key) -> load.get(key)).min().getAsDouble();
+        for (Map.Entry<Integer, IndexAndWeights> target: orig.entrySet()) {
+            IndexAndWeights val = target.getValue();
+            double l = load == null ? 0.0 : load.get(target.getKey());
+            if (l <= min + (0.05)) {
+                //We assume that within 5% of the minimum congestion is still fine.
+                //Not congested we grow (but slowly)
+                val.weight = Math.min(MAX_WEIGHT, val.weight + 1);
+            } else {
+                //Congested we contract much more quickly
+                val.weight = Math.max(0, val.weight - 10);
+            }
         }
+        //Now we need to build the array
+        long weightSum = orig.values().stream().mapToLong((w) -> w.weight).sum();
+        //Now we can calculate a percentage
 
         int currentIdx = 0;
-        int unassignedIdx = 0;
-        for (int i = 0 ; i < loads.length ; i++) {
-            if (currentIdx == CAPACITY) {
-                break;
-            }
-
-            int loadForTask = loads[i];
-            int amount = Math.round(loadForTask * 1.0f * CAPACITY / localTotal);
-            // assign at least one for task
-            if (amount == 0) {
-                unassigned[unassignedIdx++] = i;
-            }
-            for (int j = 0; j < amount; j++) {
-                if (currentIdx == CAPACITY) {
-                    break;
+        if (weightSum > 0) {
+            for (IndexAndWeights indexAndWeights : orig.values()) {
+                int count = (int) ((indexAndWeights.weight / (double) weightSum) * CAPACITY);
+                for (int i = 0; i < count && currentIdx < CAPACITY; i++) {
+                    prepareChoices[currentIdx] = indexAndWeights.index;
+                    currentIdx++;
                 }
-
-                prepareChoices[currentIdx++] = i;
             }
-        }
 
-        if (currentIdx < CAPACITY) {
-            // if there're some rooms, give unassigned tasks a chance to be included
-            // this should be really small amount, so just add them sequentially
-            if (unassignedIdx > 0) {
-                for (int i = currentIdx ; i < CAPACITY ; i++) {
-                    prepareChoices[i] = unassigned[(i - currentIdx) % unassignedIdx];
-                }
-            } else {
-                // just pick random
-                for (int i = currentIdx ; i < CAPACITY ; i++) {
-                    prepareChoices[i] = random.nextInt(loads.length);
-                }
+            //in case we didn't fill in enough
+            for (; currentIdx < CAPACITY; currentIdx++) {
+                prepareChoices[currentIdx] = prepareChoices[random.nextInt(currentIdx)];
+            }
+        } else {
+            //This really should be impossible, because we go off of the min load, and inc anything within 5% of it.
+            // But just to be sure it is never an issue, especially with float rounding etc.
+            for (;currentIdx < CAPACITY; currentIdx++) {
+                prepareChoices[currentIdx] = currentIdx % rets.length;
             }
         }
 
@@ -160,5 +156,4 @@ public class LoadAwareShuffleGrouping implements LoadAwareCustomStreamGrouping,
         arr[i] = arr[j];
         arr[j] = tmp;
     }
-
-}
+}
\ No newline at end of file
diff --git a/storm-client/test/jvm/org/apache/storm/grouping/LoadAwareShuffleGroupingTest.java b/storm-client/test/jvm/org/apache/storm/grouping/LoadAwareShuffleGroupingTest.java
index 53ac404b4..a5f9304c6 100644
--- a/storm-client/test/jvm/org/apache/storm/grouping/LoadAwareShuffleGroupingTest.java
+++ b/storm-client/test/jvm/org/apache/storm/grouping/LoadAwareShuffleGroupingTest.java
@@ -19,12 +19,12 @@ package org.apache.storm.grouping;
 
 import com.google.common.collect.Lists;
 import com.google.common.util.concurrent.MoreExecutors;
-import org.apache.storm.StormTimer;
+import java.util.Arrays;
 import org.apache.storm.daemon.GrouperFactory;
+import org.apache.storm.generated.GlobalStreamId;
 import org.apache.storm.generated.Grouping;
 import org.apache.storm.generated.NullStruct;
 import org.apache.storm.task.WorkerTopologyContext;
-import org.apache.storm.utils.Utils;
 import org.junit.Ignore;
 import org.junit.Test;
 import org.slf4j.Logger;
@@ -54,6 +54,64 @@ public class LoadAwareShuffleGroupingTest {
     public static final double ACCEPTABLE_MARGIN = 0.015;
     private static final Logger LOG = LoggerFactory.getLogger(LoadAwareShuffleGroupingTest.class);
 
+    @Test
+    public void testUnevenLoadOverTime() throws Exception {
+        LoadAwareShuffleGrouping grouping = new LoadAwareShuffleGrouping();
+        WorkerTopologyContext context = mock(WorkerTopologyContext.class);
+        grouping.prepare(context, new GlobalStreamId("a", "default"), Arrays.asList(1, 2));
+        double expectedOneWeight = 100.0;
+        double expectedTwoWeight = 100.0;
+
+        Map<Integer, Double> localLoad = new HashMap<>();
+        localLoad.put(1, 1.0);
+        localLoad.put(2, 0.0);
+        LoadMapping lm = new LoadMapping();
+        lm.setLocal(localLoad);
+        //First verify that if something has a high load it's distribution will drop over time
+        for (int i = 9; i >= 0; i--) {
+            grouping.refreshLoad(lm);
+            expectedOneWeight -= 10.0;
+            Map<Integer, Double> countByType = count(grouping.choices, grouping.rets);
+            LOG.info("contByType = {}", countByType);
+            double expectedOnePercentage = expectedOneWeight / (expectedOneWeight + expectedTwoWeight);
+            double expectedTwoPercentage = expectedTwoWeight / (expectedOneWeight + expectedTwoWeight);
+            assertEquals("i = " + i,
+                expectedOnePercentage, countByType.getOrDefault(1, 0.0) / LoadAwareShuffleGrouping.CAPACITY,
+                0.01);
+            assertEquals("i = " + i,
+                expectedTwoPercentage, countByType.getOrDefault(2, 0.0) / LoadAwareShuffleGrouping.CAPACITY,
+                0.01);
+        }
+
+        //Now verify that when it is switched we can recover
+        localLoad.put(1, 0.0);
+        localLoad.put(2, 1.0);
+        lm.setLocal(localLoad);
+
+        while (expectedOneWeight < 100.0) {
+            grouping.refreshLoad(lm);
+            expectedOneWeight += 1.0;
+            expectedTwoWeight = Math.max(0.0, expectedTwoWeight - 10.0);
+            Map<Integer, Double> countByType = count(grouping.choices, grouping.rets);
+            LOG.info("contByType = {}", countByType);
+            double expectedOnePercentage = expectedOneWeight / (expectedOneWeight + expectedTwoWeight);
+            double expectedTwoPercentage = expectedTwoWeight / (expectedOneWeight + expectedTwoWeight);
+            assertEquals(expectedOnePercentage, countByType.getOrDefault(1, 0.0) / LoadAwareShuffleGrouping.CAPACITY,
+                0.01);
+            assertEquals(expectedTwoPercentage, countByType.getOrDefault(2, 0.0) / LoadAwareShuffleGrouping.CAPACITY,
+                0.01);
+        }
+    }
+
+    private Map<Integer,Double> count(int[] choices, List<Integer>[] rets) {
+        Map<Integer, Double> ret = new HashMap<>();
+        for (int i : choices) {
+            int task = rets[i].get(0);
+            ret.put(task, ret.getOrDefault(task, 0.0) + 1);
+        }
+        return ret;
+    }
+
     @Test
     public void testLoadAwareShuffleGroupingWithEvenLoad() {
         // just pick arbitrary number
@@ -160,36 +218,6 @@ public class LoadAwareShuffleGroupingTest {
         }
     }
 
-    @Test
-    public void testLoadAwareShuffleGroupingWithUnevenLoad() {
-        // just pick arbitrary number
-        final int numTasks = 7;
-        final LoadAwareShuffleGrouping grouper = new LoadAwareShuffleGrouping();
-
-        // Define our taskIds and loads
-        final List<Integer> availableTaskIds = getAvailableTaskIds(numTasks);
-        final LoadMapping loadMapping = buildLocalTasksUnevenLoadMapping(availableTaskIds);
-
-        runDistributionVerificationTestWithUnevenLoad(numTasks, grouper, availableTaskIds,
-            loadMapping);
-    }
-
-    @Test
-    public void testLoadAwareShuffleGroupingWithRandomTasksAndRandomLoad() {
-        for (int trial = 0 ; trial < 200 ; trial++) {
-            // just pick arbitrary number in 5 ~ 100
-            final int numTasks = new Random().nextInt(96) + 5;
-            final LoadAwareShuffleGrouping grouper = new LoadAwareShuffleGrouping();
-
-            // Define our taskIds and loads
-            final List<Integer> availableTaskIds = getAvailableTaskIds(numTasks);
-            final LoadMapping loadMapping = buildLocalTasksRandomLoadMapping(availableTaskIds);
-
-            runDistributionVerificationTestWithUnevenLoad(numTasks, grouper, availableTaskIds,
-                loadMapping);
-        }
-    }
-
     @Test
     public void testShuffleLoadEven() {
         // port test-shuffle-load-even
@@ -228,46 +256,6 @@ public class LoadAwareShuffleGroupingTest {
         assertTrue(load2 <= maxPrCount);
     }
 
-    @Test
-    public void testShuffleLoadUneven() {
-        // port test-shuffle-load-uneven
-        LoadAwareCustomStreamGrouping shuffler = GrouperFactory
-            .mkGrouper(null, "comp", "stream", null, Grouping.shuffle(new NullStruct()),
-                Lists.newArrayList(1, 2), Collections.emptyMap());
-        int numMessages = 100000;
-        int min1PrCount = (int) (numMessages * (0.33 - ACCEPTABLE_MARGIN));
-        int max1PrCount = (int) (numMessages * (0.33 + ACCEPTABLE_MARGIN));
-        int min2PrCount = (int) (numMessages * (0.66 - ACCEPTABLE_MARGIN));
-        int max2PrCount = (int) (numMessages * (0.66 + ACCEPTABLE_MARGIN));
-        LoadMapping load = new LoadMapping();
-        Map<Integer, Double> loadInfoMap = new HashMap<>();
-        loadInfoMap.put(1, 0.5);
-        loadInfoMap.put(2, 0.0);
-        load.setLocal(loadInfoMap);
-
-        // force triggers building ring
-        shuffler.refreshLoad(load);
-
-        List<Object> data = Lists.newArrayList(1, 2);
-        int[] frequencies = new int[3]; // task id starts from 1
-        for (int i = 0 ; i < numMessages ; i++) {
-            List<Integer> tasks = shuffler.chooseTasks(1, data);
-            for (int task : tasks) {
-                frequencies[task]++;
-            }
-        }
-
-        int load1 = frequencies[1];
-        int load2 = frequencies[2];
-
-        LOG.info("Frequency info: load1 = {}, load2 = {}", load1, load2);
-
-        assertTrue(load1 >= min1PrCount);
-        assertTrue(load1 <= max1PrCount);
-        assertTrue(load2 >= min2PrCount);
-        assertTrue(load2 <= max2PrCount);
-    }
-
     @Ignore
     @Test
     public void testBenchmarkLoadAwareShuffleGroupingEvenLoad() {
@@ -374,37 +362,6 @@ public class LoadAwareShuffleGroupingTest {
         return taskCounts;
     }
 
-    private void runDistributionVerificationTestWithUnevenLoad(int numTasks,
-        LoadAwareShuffleGrouping grouper, List<Integer> availableTaskIds,
-        LoadMapping loadMapping) {
-        int[] loads = new int[numTasks];
-        int localTotal = 0;
-        List<Double> loadRate = new ArrayList<>();
-        for (int i = 0; i < numTasks; i++) {
-            int val = (int)(101 - (loadMapping.get(i) * 100));
-            loads[i] = val;
-            localTotal += val;
-        }
-
-        for (int i = 0; i < numTasks; i++) {
-            loadRate.add(loads[i] * 1.0 / localTotal);
-        }
-
-        WorkerTopologyContext context = mock(WorkerTopologyContext.class);
-        grouper.prepare(context, null, availableTaskIds);
-
-        // Keep track of how many times we see each taskId
-        int totalEmits = 5000 * numTasks;
-        int[] taskCounts = runChooseTasksWithVerification(grouper, totalEmits, numTasks, loadMapping);
-
-        int delta = (int) (totalEmits * ACCEPTABLE_MARGIN);
-        for (int i = 0; i < numTasks; i++) {
-            int expected = (int) (totalEmits * loadRate.get(i));
-            assertTrue("Distribution should respect the task load with small delta",
-                taskCounts[i] >= expected - delta && taskCounts[i] <= expected + delta);
-        }
-    }
-
     private void runSimpleBenchmark(LoadAwareCustomStreamGrouping grouper,
         List<Integer> availableTaskIds, LoadMapping loadMapping) {
         // Task Id not used, so just pick a static value
