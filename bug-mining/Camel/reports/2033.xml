<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Wed Nov 12 09:53:48 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF Jira</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CAMEL-5683] JMS connection leak with request/reply producer on temporary queues</title>
                <link>https://issues.apache.org/jira/browse/CAMEL-5683</link>
                <project id="12311211" key="CAMEL">Camel</project>
                    <description>&lt;p&gt;Over time I see the number of temporary queues in ActiveMQ slowly climb. Using JMX information and memory dumps in MAT, I believe the cause is a connection leak in Apache Camel.&lt;/p&gt;

&lt;p&gt;My environment contains 2 ActiveMQ brokers in a network of brokers configuration. There are about 15 separate applications which use Apache Camel to connect to the broker using the ActiveMQ/JMS component. The various applications have different load profiles and route configurations.&lt;/p&gt;

&lt;p&gt;In the more active client applications, I found that ActiveMQ was listing 300+ consumers when, based on my configuration, I would expect no more than 75. The vast majority of the consumers are sitting on a temporary queue. Over time, the 300 number increments by one or two over about a 4 hour period.&lt;/p&gt;

&lt;p&gt;I did a memory dump on one of the more active client applications and found about 275 DefaultMessageListenerContainers. Using MAT, I can see that some of the containers are referenced by JmsProducers in the ProducerCache; however I can also see a large number of listener containers that are no longer being referenced at all. I was also able to match up a soft-references producer/listener endpoint with an unreferenced listener which means a second producer was created at some point.&lt;/p&gt;

&lt;p&gt;Looking through the ProducerCache code, it looks like the LRU cache uses soft-references to producers, in my case a JmsProducer. This seems problematic for two reasons:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;If memory gets constrained and the GC cleans up a producer, it is never properly stopped.&lt;/li&gt;
	&lt;li&gt;If the cache gets full and the map removes the LRU producer, it is never properly stopped.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;What I believe is happening, is that my application is sending a few request/reply messages to a JmsProducer. The producer creates a TemporaryReplyManager which creates a DefaultMessageListenerContainer. At some point, the JmsProducer is claimed by the GC (either via the soft-reference or because the cache is full) and the reply manager is never stopped. This causes the listener container to continue to listen on the temporary queue, consuming local resources and more importantly, consuming resources on the JMS broker.&lt;/p&gt;

&lt;p&gt;I haven&apos;t had a chance to write an application to reproduce this behavior, but I will attach one of my route configurations and a screenshot of the MAT analysis looking at DefaultMessageListenerContainers. If needed, I could provide the entire memory dump for analysis (although I rather not post it publicly). The leak depends on memory usage or producer count in the client application because the ProducerCache must have some churn. Like I said, in our production system we see about 12 temporary queues abandoned per client per day.&lt;/p&gt;

&lt;p&gt;Unless I&apos;m missing something, it looks like the producer cache would need to be much smarter to support stopping a producer when the soft-reference is reclaimed or a member of the cache is ejected from the LRU list.&lt;/p&gt;
</description>
                <environment>&lt;p&gt;Apache Camel 2.10.0&lt;br/&gt;
ActiveMQ 5.6.0&lt;br/&gt;
Spring 3.2.1.RELEASE&lt;br/&gt;
Java 1.6.0_27&lt;br/&gt;
SunOS HOST 5.10 Generic_144488-09 sun4v sparc SUNW,SPARC-Enterprise-T5220&lt;/p&gt;</environment>
        <key id="12610112">CAMEL-5683</key>
            <summary>JMS connection leak with request/reply producer on temporary queues</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="davsclaus">Claus Ibsen</assignee>
                                    <reporter username="mpilone">Michael Pilone</reporter>
                        <labels>
                    </labels>
                <created>Wed, 3 Oct 2012 17:37:36 +0000</created>
                <updated>Thu, 2 May 2013 02:29:56 +0000</updated>
                            <resolved>Sun, 7 Oct 2012 06:39:43 +0000</resolved>
                                    <version>2.10.0</version>
                                    <fixVersion>2.9.4</fixVersion>
                    <fixVersion>2.10.2</fixVersion>
                    <fixVersion>2.11.0</fixVersion>
                                    <component>camel-jms</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="13468695" author="mpilone" created="Wed, 3 Oct 2012 17:40:55 +0000"  >&lt;p&gt;Attached screenshot from MAT analysis.&lt;/p&gt;</comment>
                            <comment id="13468699" author="mpilone" created="Wed, 3 Oct 2012 17:45:24 +0000"  >&lt;p&gt;Attached the route configuration for the JMS client application being analyzed.&lt;/p&gt;

&lt;p&gt;Attached the list of consumers as ActiveMQ sees it for the client application being analyzed.&lt;/p&gt;</comment>
                            <comment id="13469498" author="mpilone" created="Thu, 4 Oct 2012 16:45:20 +0000"  >&lt;p&gt;I attached a test case which reproduces the problem. The test case has 3 JMS request/reply routes. It runs in a loop, sending a message, consuming a bunch of memory, then sending another message. As the GC starts to run, the producer on route 2 is reclaimed and a consumer is leaked. Instructions for running it are in the LeakMain class.&lt;/p&gt;</comment>
                            <comment id="13469499" author="mpilone" created="Thu, 4 Oct 2012 16:46:11 +0000"  >&lt;p&gt;It would be great to get a work-around for this. As of now, we have to restart our services every couple of days to keep them from exhausting ActiveMQ resources with hundreds of temporary queues.&lt;/p&gt;</comment>
                            <comment id="13469681" author="raulvk" created="Thu, 4 Oct 2012 20:53:16 +0000"  >&lt;p&gt;Michael,&lt;/p&gt;

&lt;p&gt;Many thanks for such a detailed description, test case and bug report!&lt;/p&gt;

&lt;p&gt;Have you tried setting the size of the ProducerCache to zero? Check &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; for instructions on how to do this. Beware I haven&apos;t tested it, it&apos;s just a suggestion for a workaround. If you have static endpoint URIs, then I don&apos;t think you should experience any churn or performance hit by having a non-existent ProducerCache. &lt;/p&gt;

&lt;p&gt;Regards,&lt;br/&gt;
Ra&#250;l.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://camel.apache.org/how-do-i-configure-the-default-maximum-cache-size-for-producercache-or-producertemplate.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://camel.apache.org/how-do-i-configure-the-default-maximum-cache-size-for-producercache-or-producertemplate.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13469956" author="mpilone" created="Fri, 5 Oct 2012 01:36:52 +0000"  >&lt;p&gt;Raul, thanks for the suggestion. I gave it a try but it didn&apos;t work. I set a few breakpoints and found that the configuration of my test case creates the ProducerCache in SendProcessor.java line 152. The cache is hard coded to a size of 1.&lt;/p&gt;

&lt;p&gt;If you then set a breakpoint in ProducerCache.java line 385 where the producer is created using the endpoint, you can see that the producer is occasionally no longer in the cache and must be recreated which means it must have been reclaimed via a GC soft-reference.&lt;/p&gt;</comment>
                            <comment id="13470082" author="davsclaus" created="Fri, 5 Oct 2012 07:33:47 +0000"  >&lt;p&gt;Yeah it does not make so much sense to use a producer cache in the send processor as its a single producer based. So if we just store the Producer as a strong reference then there is no issue like this.&lt;/p&gt;</comment>
                            <comment id="13470180" author="davsclaus" created="Fri, 5 Oct 2012 08:56:33 +0000"  >&lt;p&gt;I have committed a fix on trunk, and backporting to 2.10 and 2.9 branches.&lt;br/&gt;
You are welcome to give those a try.&lt;/p&gt;</comment>
                            <comment id="13470300" author="mpilone" created="Fri, 5 Oct 2012 13:18:12 +0000"  >&lt;p&gt;Claus, thanks for the quick fix. I&apos;ll try building the source and verifying the fix. Your change in the SendProcessor looks like it will solve my problem but doesn&apos;t the problem still exist if I was using the DefaultProducerTemplate? I could probably hack my test case to use the template rather than a gateway proxy and route configuration and I think Camel would continue to leak listeners.&lt;/p&gt;

&lt;p&gt;For example, the sample documentation for ProducerTemplate shows:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ProducerTemplate template;&lt;br/&gt;
// send to default endpoint&lt;br/&gt;
template.sendBody(&quot;&amp;lt;hello&amp;gt;world!&amp;lt;/hello&amp;gt;&quot;);&lt;br/&gt;
// send to a specific queue&lt;br/&gt;
template.sendBody(&quot;activemq:MyQueue&quot;, &quot;&amp;lt;hello&amp;gt;world!&amp;lt;/hello&amp;gt;&quot;);&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The second send to ActiveMQ, if it was request/reply, would put a JmsProducer in the ProducerCache with a listener/consumer which could/would later leak.&lt;/p&gt;</comment>
                            <comment id="13470303" author="raulvk" created="Fri, 5 Oct 2012 13:23:08 +0000"  >&lt;p&gt;Maybe we need to override the &lt;tt&gt;finalize()&lt;/tt&gt; method of the JmsProducer (and review all other producers), but take a look at this post which suggests another approach: &lt;a href=&quot;http://stackoverflow.com/questions/1638859/gracefully-finalizing-the-softreference-referent&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://stackoverflow.com/questions/1638859/gracefully-finalizing-the-softreference-referent&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13470327" author="mpilone" created="Fri, 5 Oct 2012 13:59:00 +0000"  >&lt;p&gt;I compiled the code from the 2.10.x branch and confirmed that your change does appear to fix the issue when using the SendProducer. However I also confirmed my previous comment that the problem still exists when using the DefaultProducerTemplate (or any other code that uses the ProducerCache with the LRU map implementation). I&apos;ll attach an update test case which uses the ProducerTemplate to reproduce the problem. The current cache implementation is going to be a problem with any producer that requires a stop call to properly cleanup.&lt;/p&gt;

&lt;p&gt;You might want to look at modifying the ProducerCache to support a ReferenceQueue with the SoftReferences. Then the ProducerCache could drain the queue and stop all the reclaimed producers before creating a new producer.&lt;/p&gt;

&lt;p&gt;Even with that fix, it might be a good idea to have an easy way (e.g. via a context property) to disable the soft-references in the cache and rely only on max cache size. If I know I&apos;m only going to have 3 or 4 producers but a lot of memory churn, it would be nice to know that my producers would stay in the cache until I completely fill it. This could be really valuable if producer construction/teardown were expensive.&lt;/p&gt;</comment>
                            <comment id="13470330" author="mpilone" created="Fri, 5 Oct 2012 14:00:02 +0000"  >&lt;p&gt;Reopening because the problem still exists when using the ProducerTemplate (or anything else using the ProducerCache).&lt;/p&gt;</comment>
                            <comment id="13470332" author="mpilone" created="Fri, 5 Oct 2012 14:00:49 +0000"  >&lt;p&gt;Attached an updated test case that shows the same problem when using the ProducerTemplate.&lt;/p&gt;</comment>
                            <comment id="13470346" author="mpilone" created="Fri, 5 Oct 2012 14:11:27 +0000"  >&lt;p&gt;Raul, I agree. I need to refresh my page before commenting &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;The more I think about it the trickier the problem gets. Using the ReferenceQueue on the SoftReferences would help cleanup producers in the GC case, but you would need to make sure the cache also handles the case where the LRU item is evicted when the capacity is reached. In the eviction case, there is no ReferenceQueue to hold the item for later cleanup.&lt;/p&gt;

&lt;p&gt;It might make sense to remove the SoftReference support and just keep the LRU/capacity behavior. Then add a listener interface or &quot;evicted queue&quot; to the LRU hashmap to collect items (i.e. producers) that have been evicted and are pending cleanup. It seems like the use of SoftReferences undermines the LRU concept because the GC is deciding when to collect it rather than letting the map track the last used time. In theory the GC is supposed to be bias against SoftRef collection but it seems pretty aggressive from my simple tests. &lt;/p&gt;

&lt;p&gt;Something like java.util.LinkedHashMap gives you a removeEldestEntry method which would be a nice place to hook in producer shutdown code and avoids these problems.&lt;/p&gt;</comment>
                            <comment id="13470385" author="raulvk" created="Fri, 5 Oct 2012 15:15:05 +0000"  >&lt;p&gt;Beware that the LRU and the cleanup of the SoftReferences kick in at different times. They cater for different situations:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;LRU logic is valuable when your recipientList can generate many, many different producers. In a hypothetical case, if there are 2000 users and each user has a dedicated JMS topic where you want to publish messages to from your Camel route, you may end up with 2000 items in the ProducerCache, even if 1000 users are no longer active. The LRU allows Camel to vacuum potentially irrelevant producers. There is a max. producer cache size you can set to control the threshold.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;SoftReferences are valuable in near-OOM situations. It allows the JVM to &apos;intelligently&apos; dispose of objects that can be recreated later, once the memory exhaustion subsides.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Both functionalities are thus valuable. We just need to address the memory leak in SoftReferences perhaps by using finalize().&lt;/p&gt;</comment>
                            <comment id="13470422" author="mpilone" created="Fri, 5 Oct 2012 16:04:16 +0000"  >&lt;p&gt;I can understand the need for the two different mechanisms, but I&apos;d suggest that you find an approach where both the ReferenceQueue from collected SoftRef and the LRU evictions end up in the same place to support producer shutdown. Maybe the LRU evictions could be put on the same reference queue.&lt;/p&gt;

&lt;p&gt;Using finalizers means that each stateful producer needs to properly implement a finalizer and ensure that it is safe to call it even if the producer was properly stopped previously. This seems like you&apos;re asking for trouble given the number of disparate producer implementations. Because you already have an API/mechanism for stopping producers, you just want to make sure the cache uses that mechanism in all automatic cache removal cases. Just my opinion though. &lt;/p&gt;</comment>
                            <comment id="13470938" author="davsclaus" created="Sat, 6 Oct 2012 07:07:24 +0000"  >&lt;p&gt;The DefaultProducerTemplate constructor allows you to pass in your own map cache, so you can just pass in the LRUCache (not the soft) or use a unlimited cache etc.&lt;/p&gt;</comment>
                            <comment id="13471016" author="davsclaus" created="Sat, 6 Oct 2012 14:03:42 +0000"  >&lt;p&gt;The LRUCache now stops the service when evicting the entry.&lt;/p&gt;</comment>
                            <comment id="13471019" author="davsclaus" created="Sat, 6 Oct 2012 14:08:57 +0000"  >&lt;p&gt;1)&lt;br/&gt;
I think it may make sense to let the DefaultProducerTemplate / DefaultConsumerTemplate uses a non soft cache (eg just LRUCache) as they are created by end users, and thus they would be able to control this. For example they can lower the cache size to reduce memory occupation if using a lot of different producers. And now the elements that gets evicted will be stopped as well.&lt;/p&gt;

&lt;p&gt;2)&lt;br/&gt;
Then there is some internal caches in Camel such as some based on Class/Method introspections which can safely be soft/weak based, as there is no &quot;stop&quot; logic needed.&lt;/p&gt;

&lt;p&gt;3)&lt;br/&gt;
Whether some of the EIPs which uses a ProducerCache should be non-soft based; we can take a look. It may make sense. &lt;/p&gt;</comment>
                            <comment id="13471159" author="davsclaus" created="Sun, 7 Oct 2012 06:38:41 +0000"  >&lt;p&gt;I have committed a fix for 1+2+3, so we use a non-soft cache for the producer/consumer caches in Camel. And they are stopped on eviction as well.&lt;/p&gt;

&lt;p&gt;Michael, fell free to give it a test run.&lt;/p&gt;</comment>
                            <comment id="13472314" author="mpilone" created="Tue, 9 Oct 2012 12:40:36 +0000"  >&lt;p&gt;I ran 2.10.2-SNAPSHOT through my test cases and everything looks good. Thanks for your attention to the matter and a good, complete solution. Now I just need to decide if I want to run with a SNAPSHOT in production or wait for 2.10.2 final!&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                            <outwardlinks description="depends upon">
                                        <issuelink>
            <issuekey id="12610692">CAMEL-5688</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12547978" name="CamelConnectionLeak-ProducerTemplate.zip" size="10297" author="mpilone" created="Fri, 5 Oct 2012 14:00:49 +0000"/>
                            <attachment id="12547781" name="CamelConnectionLeak.zip" size="10093" author="mpilone" created="Thu, 4 Oct 2012 16:45:20 +0000"/>
                            <attachment id="12547567" name="Consumer List.txt" size="96867" author="mpilone" created="Wed, 3 Oct 2012 17:45:24 +0000"/>
                            <attachment id="12547565" name="MAT Snapshot.png" size="236660" author="mpilone" created="Wed, 3 Oct 2012 17:40:55 +0000"/>
                            <attachment id="12547566" name="Route Configuration.txt" size="8230" author="mpilone" created="Wed, 3 Oct 2012 17:45:24 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12310060" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Estimated Complexity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="10060"><![CDATA[Unknown]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>240495</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 years, 7 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0120v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4094</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>