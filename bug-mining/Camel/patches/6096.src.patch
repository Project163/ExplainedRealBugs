diff --git a/catalog/camel-catalog/src/generated/resources/org/apache/camel/catalog/components/kafka.json b/catalog/camel-catalog/src/generated/resources/org/apache/camel/catalog/components/kafka.json
index 50077912169..b6c4599b059 100644
--- a/catalog/camel-catalog/src/generated/resources/org/apache/camel/catalog/components/kafka.json
+++ b/catalog/camel-catalog/src/generated/resources/org/apache/camel/catalog/components/kafka.json
@@ -60,8 +60,12 @@
     "specificAvroReader": { "kind": "property", "displayName": "Specific Avro Reader", "group": "consumer", "label": "confluent,consumer", "required": false, "type": "boolean", "javaType": "boolean", "deprecated": false, "autowired": false, "secret": false, "defaultValue": false, "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "This enables the use of a specific Avro reader for use with the Confluent Platform schema registry and the io.confluent.kafka.serializers.KafkaAvroDeserializer. This option is only available in the Confluent Platform (not standard Apache Kafka)" },
     "topicIsPattern": { "kind": "property", "displayName": "Topic Is Pattern", "group": "consumer", "label": "consumer", "required": false, "type": "boolean", "javaType": "boolean", "deprecated": false, "autowired": false, "secret": false, "defaultValue": false, "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "Whether the topic is a pattern (regular expression). This can be used to subscribe to dynamic number of topics matching the pattern." },
     "valueDeserializer": { "kind": "property", "displayName": "Value Deserializer", "group": "consumer", "label": "consumer", "required": false, "type": "string", "javaType": "java.lang.String", "deprecated": false, "autowired": false, "secret": false, "defaultValue": "org.apache.kafka.common.serialization.StringDeserializer", "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "Deserializer class for value that implements the Deserializer interface." },
+    "createConsumerBackoffInterval": { "kind": "property", "displayName": "Create Consumer Backoff Interval", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "integer", "javaType": "long", "deprecated": false, "autowired": false, "secret": false, "defaultValue": 5000, "description": "The delay in millis seconds to wait before trying again to create the kafka consumer (kafka-client)." },
+    "createConsumerBackoffMaxAttempts": { "kind": "property", "displayName": "Create Consumer Backoff Max Attempts", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "integer", "javaType": "int", "deprecated": false, "autowired": false, "secret": false, "description": "Maximum attempts to create the kafka consumer (kafka-client), before eventually giving up and failing. Error during creating the consumer may be fatal due to invalid configuration and as such recovery is not possible. However, one part of the validation is DNS resolution of the bootstrap broker hostnames. This may be a temporary networking problem, and could potentially be recoverable. While other errors are fatal such as some invalid kafka configurations. Unfortunately kafka-client does not separate this kind of errors. Camel will by default retry forever, and therefore never give up. If you want to give up after many attempts then set this option and Camel will then when giving up terminate the consumer. You can manually restart the consumer by stopping and starting the route, to try again." },
     "kafkaManualCommitFactory": { "kind": "property", "displayName": "Kafka Manual Commit Factory", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "object", "javaType": "org.apache.camel.component.kafka.consumer.KafkaManualCommitFactory", "deprecated": false, "autowired": true, "secret": false, "description": "Factory to use for creating KafkaManualCommit instances. This allows to plugin a custom factory to create custom KafkaManualCommit instances in case special logic is needed when doing manual commits that deviates from the default implementation that comes out of the box." },
     "pollExceptionStrategy": { "kind": "property", "displayName": "Poll Exception Strategy", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "object", "javaType": "org.apache.camel.component.kafka.PollExceptionStrategy", "deprecated": false, "autowired": true, "secret": false, "description": "To use a custom strategy with the consumer to control how to handle exceptions thrown from the Kafka broker while pooling messages." },
+    "subscribeConsumerBackoffInterval": { "kind": "property", "displayName": "Subscribe Consumer Backoff Interval", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "integer", "javaType": "long", "deprecated": false, "autowired": false, "secret": false, "defaultValue": 5000, "description": "The delay in millis seconds to wait before trying again to subscribe to the kafka broker." },
+    "subscribeConsumerBackoffMaxAttempts": { "kind": "property", "displayName": "Subscribe Consumer Backoff Max Attempts", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "integer", "javaType": "int", "deprecated": false, "autowired": false, "secret": false, "description": "Maximum number the kafka consumer will attempt to subscribe to the kafka broker, before eventually giving up and failing. Error during subscribing the consumer to the kafka topic could be temporary errors due to network issues, and could potentially be recoverable. Camel will by default retry forever, and therefore never give up. If you want to give up after many attempts then set this option and Camel will then when giving up terminate the consumer. You can manually restart the consumer by stopping and starting the route, to try again." },
     "bufferMemorySize": { "kind": "property", "displayName": "Buffer Memory Size", "group": "producer", "label": "producer", "required": false, "type": "integer", "javaType": "java.lang.Integer", "deprecated": false, "autowired": false, "secret": false, "defaultValue": "33554432", "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "The total bytes of memory the producer can use to buffer records waiting to be sent to the server. If records are sent faster than they can be delivered to the server the producer will either block or throw an exception based on the preference specified by block.on.buffer.full.This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests." },
     "compressionCodec": { "kind": "property", "displayName": "Compression Codec", "group": "producer", "label": "producer", "required": false, "type": "string", "javaType": "java.lang.String", "enum": [ "none", "gzip", "snappy", "lz4" ], "deprecated": false, "autowired": false, "secret": false, "defaultValue": "none", "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "This parameter allows you to specify the compression codec for all data generated by this producer. Valid values are none, gzip and snappy." },
     "connectionMaxIdleMs": { "kind": "property", "displayName": "Connection Max Idle Ms", "group": "producer", "label": "producer", "required": false, "type": "integer", "javaType": "java.lang.Integer", "deprecated": false, "autowired": false, "secret": false, "defaultValue": "540000", "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "Close idle connections after the number of milliseconds specified by this config." },
diff --git a/components/camel-kafka/src/generated/java/org/apache/camel/component/kafka/KafkaComponentConfigurer.java b/components/camel-kafka/src/generated/java/org/apache/camel/component/kafka/KafkaComponentConfigurer.java
index e2fa43e19a6..8138ecd5a79 100644
--- a/components/camel-kafka/src/generated/java/org/apache/camel/component/kafka/KafkaComponentConfigurer.java
+++ b/components/camel-kafka/src/generated/java/org/apache/camel/component/kafka/KafkaComponentConfigurer.java
@@ -64,6 +64,10 @@ public class KafkaComponentConfigurer extends PropertyConfigurerSupport implemen
         case "consumerRequestTimeoutMs": getOrCreateConfiguration(target).setConsumerRequestTimeoutMs(property(camelContext, java.lang.Integer.class, value)); return true;
         case "consumerscount":
         case "consumersCount": getOrCreateConfiguration(target).setConsumersCount(property(camelContext, int.class, value)); return true;
+        case "createconsumerbackoffinterval":
+        case "createConsumerBackoffInterval": target.setCreateConsumerBackoffInterval(property(camelContext, long.class, value)); return true;
+        case "createconsumerbackoffmaxattempts":
+        case "createConsumerBackoffMaxAttempts": target.setCreateConsumerBackoffMaxAttempts(property(camelContext, int.class, value)); return true;
         case "deliverytimeoutms":
         case "deliveryTimeoutMs": getOrCreateConfiguration(target).setDeliveryTimeoutMs(property(camelContext, java.lang.Integer.class, value)); return true;
         case "enableidempotence":
@@ -213,6 +217,10 @@ public class KafkaComponentConfigurer extends PropertyConfigurerSupport implemen
         case "sslTruststorePassword": getOrCreateConfiguration(target).setSslTruststorePassword(property(camelContext, java.lang.String.class, value)); return true;
         case "ssltruststoretype":
         case "sslTruststoreType": getOrCreateConfiguration(target).setSslTruststoreType(property(camelContext, java.lang.String.class, value)); return true;
+        case "subscribeconsumerbackoffinterval":
+        case "subscribeConsumerBackoffInterval": target.setSubscribeConsumerBackoffInterval(property(camelContext, long.class, value)); return true;
+        case "subscribeconsumerbackoffmaxattempts":
+        case "subscribeConsumerBackoffMaxAttempts": target.setSubscribeConsumerBackoffMaxAttempts(property(camelContext, int.class, value)); return true;
         case "synchronous": getOrCreateConfiguration(target).setSynchronous(property(camelContext, boolean.class, value)); return true;
         case "topicispattern":
         case "topicIsPattern": getOrCreateConfiguration(target).setTopicIsPattern(property(camelContext, boolean.class, value)); return true;
@@ -276,6 +284,10 @@ public class KafkaComponentConfigurer extends PropertyConfigurerSupport implemen
         case "consumerRequestTimeoutMs": return java.lang.Integer.class;
         case "consumerscount":
         case "consumersCount": return int.class;
+        case "createconsumerbackoffinterval":
+        case "createConsumerBackoffInterval": return long.class;
+        case "createconsumerbackoffmaxattempts":
+        case "createConsumerBackoffMaxAttempts": return int.class;
         case "deliverytimeoutms":
         case "deliveryTimeoutMs": return java.lang.Integer.class;
         case "enableidempotence":
@@ -425,6 +437,10 @@ public class KafkaComponentConfigurer extends PropertyConfigurerSupport implemen
         case "sslTruststorePassword": return java.lang.String.class;
         case "ssltruststoretype":
         case "sslTruststoreType": return java.lang.String.class;
+        case "subscribeconsumerbackoffinterval":
+        case "subscribeConsumerBackoffInterval": return long.class;
+        case "subscribeconsumerbackoffmaxattempts":
+        case "subscribeConsumerBackoffMaxAttempts": return int.class;
         case "synchronous": return boolean.class;
         case "topicispattern":
         case "topicIsPattern": return boolean.class;
@@ -484,6 +500,10 @@ public class KafkaComponentConfigurer extends PropertyConfigurerSupport implemen
         case "consumerRequestTimeoutMs": return getOrCreateConfiguration(target).getConsumerRequestTimeoutMs();
         case "consumerscount":
         case "consumersCount": return getOrCreateConfiguration(target).getConsumersCount();
+        case "createconsumerbackoffinterval":
+        case "createConsumerBackoffInterval": return target.getCreateConsumerBackoffInterval();
+        case "createconsumerbackoffmaxattempts":
+        case "createConsumerBackoffMaxAttempts": return target.getCreateConsumerBackoffMaxAttempts();
         case "deliverytimeoutms":
         case "deliveryTimeoutMs": return getOrCreateConfiguration(target).getDeliveryTimeoutMs();
         case "enableidempotence":
@@ -633,6 +653,10 @@ public class KafkaComponentConfigurer extends PropertyConfigurerSupport implemen
         case "sslTruststorePassword": return getOrCreateConfiguration(target).getSslTruststorePassword();
         case "ssltruststoretype":
         case "sslTruststoreType": return getOrCreateConfiguration(target).getSslTruststoreType();
+        case "subscribeconsumerbackoffinterval":
+        case "subscribeConsumerBackoffInterval": return target.getSubscribeConsumerBackoffInterval();
+        case "subscribeconsumerbackoffmaxattempts":
+        case "subscribeConsumerBackoffMaxAttempts": return target.getSubscribeConsumerBackoffMaxAttempts();
         case "synchronous": return getOrCreateConfiguration(target).isSynchronous();
         case "topicispattern":
         case "topicIsPattern": return getOrCreateConfiguration(target).isTopicIsPattern();
diff --git a/components/camel-kafka/src/generated/resources/org/apache/camel/component/kafka/kafka.json b/components/camel-kafka/src/generated/resources/org/apache/camel/component/kafka/kafka.json
index 50077912169..b6c4599b059 100644
--- a/components/camel-kafka/src/generated/resources/org/apache/camel/component/kafka/kafka.json
+++ b/components/camel-kafka/src/generated/resources/org/apache/camel/component/kafka/kafka.json
@@ -60,8 +60,12 @@
     "specificAvroReader": { "kind": "property", "displayName": "Specific Avro Reader", "group": "consumer", "label": "confluent,consumer", "required": false, "type": "boolean", "javaType": "boolean", "deprecated": false, "autowired": false, "secret": false, "defaultValue": false, "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "This enables the use of a specific Avro reader for use with the Confluent Platform schema registry and the io.confluent.kafka.serializers.KafkaAvroDeserializer. This option is only available in the Confluent Platform (not standard Apache Kafka)" },
     "topicIsPattern": { "kind": "property", "displayName": "Topic Is Pattern", "group": "consumer", "label": "consumer", "required": false, "type": "boolean", "javaType": "boolean", "deprecated": false, "autowired": false, "secret": false, "defaultValue": false, "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "Whether the topic is a pattern (regular expression). This can be used to subscribe to dynamic number of topics matching the pattern." },
     "valueDeserializer": { "kind": "property", "displayName": "Value Deserializer", "group": "consumer", "label": "consumer", "required": false, "type": "string", "javaType": "java.lang.String", "deprecated": false, "autowired": false, "secret": false, "defaultValue": "org.apache.kafka.common.serialization.StringDeserializer", "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "Deserializer class for value that implements the Deserializer interface." },
+    "createConsumerBackoffInterval": { "kind": "property", "displayName": "Create Consumer Backoff Interval", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "integer", "javaType": "long", "deprecated": false, "autowired": false, "secret": false, "defaultValue": 5000, "description": "The delay in millis seconds to wait before trying again to create the kafka consumer (kafka-client)." },
+    "createConsumerBackoffMaxAttempts": { "kind": "property", "displayName": "Create Consumer Backoff Max Attempts", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "integer", "javaType": "int", "deprecated": false, "autowired": false, "secret": false, "description": "Maximum attempts to create the kafka consumer (kafka-client), before eventually giving up and failing. Error during creating the consumer may be fatal due to invalid configuration and as such recovery is not possible. However, one part of the validation is DNS resolution of the bootstrap broker hostnames. This may be a temporary networking problem, and could potentially be recoverable. While other errors are fatal such as some invalid kafka configurations. Unfortunately kafka-client does not separate this kind of errors. Camel will by default retry forever, and therefore never give up. If you want to give up after many attempts then set this option and Camel will then when giving up terminate the consumer. You can manually restart the consumer by stopping and starting the route, to try again." },
     "kafkaManualCommitFactory": { "kind": "property", "displayName": "Kafka Manual Commit Factory", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "object", "javaType": "org.apache.camel.component.kafka.consumer.KafkaManualCommitFactory", "deprecated": false, "autowired": true, "secret": false, "description": "Factory to use for creating KafkaManualCommit instances. This allows to plugin a custom factory to create custom KafkaManualCommit instances in case special logic is needed when doing manual commits that deviates from the default implementation that comes out of the box." },
     "pollExceptionStrategy": { "kind": "property", "displayName": "Poll Exception Strategy", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "object", "javaType": "org.apache.camel.component.kafka.PollExceptionStrategy", "deprecated": false, "autowired": true, "secret": false, "description": "To use a custom strategy with the consumer to control how to handle exceptions thrown from the Kafka broker while pooling messages." },
+    "subscribeConsumerBackoffInterval": { "kind": "property", "displayName": "Subscribe Consumer Backoff Interval", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "integer", "javaType": "long", "deprecated": false, "autowired": false, "secret": false, "defaultValue": 5000, "description": "The delay in millis seconds to wait before trying again to subscribe to the kafka broker." },
+    "subscribeConsumerBackoffMaxAttempts": { "kind": "property", "displayName": "Subscribe Consumer Backoff Max Attempts", "group": "consumer (advanced)", "label": "consumer,advanced", "required": false, "type": "integer", "javaType": "int", "deprecated": false, "autowired": false, "secret": false, "description": "Maximum number the kafka consumer will attempt to subscribe to the kafka broker, before eventually giving up and failing. Error during subscribing the consumer to the kafka topic could be temporary errors due to network issues, and could potentially be recoverable. Camel will by default retry forever, and therefore never give up. If you want to give up after many attempts then set this option and Camel will then when giving up terminate the consumer. You can manually restart the consumer by stopping and starting the route, to try again." },
     "bufferMemorySize": { "kind": "property", "displayName": "Buffer Memory Size", "group": "producer", "label": "producer", "required": false, "type": "integer", "javaType": "java.lang.Integer", "deprecated": false, "autowired": false, "secret": false, "defaultValue": "33554432", "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "The total bytes of memory the producer can use to buffer records waiting to be sent to the server. If records are sent faster than they can be delivered to the server the producer will either block or throw an exception based on the preference specified by block.on.buffer.full.This setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. Some additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests." },
     "compressionCodec": { "kind": "property", "displayName": "Compression Codec", "group": "producer", "label": "producer", "required": false, "type": "string", "javaType": "java.lang.String", "enum": [ "none", "gzip", "snappy", "lz4" ], "deprecated": false, "autowired": false, "secret": false, "defaultValue": "none", "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "This parameter allows you to specify the compression codec for all data generated by this producer. Valid values are none, gzip and snappy." },
     "connectionMaxIdleMs": { "kind": "property", "displayName": "Connection Max Idle Ms", "group": "producer", "label": "producer", "required": false, "type": "integer", "javaType": "java.lang.Integer", "deprecated": false, "autowired": false, "secret": false, "defaultValue": "540000", "configurationClass": "org.apache.camel.component.kafka.KafkaConfiguration", "configurationField": "configuration", "description": "Close idle connections after the number of milliseconds specified by this config." },
diff --git a/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaComponent.java b/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaComponent.java
index dde3327d611..348b60bfbbc 100644
--- a/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaComponent.java
+++ b/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaComponent.java
@@ -42,6 +42,14 @@ public class KafkaComponent extends DefaultComponent implements SSLContextParame
     private KafkaClientFactory kafkaClientFactory;
     @Metadata(autowired = true, label = "consumer,advanced")
     private PollExceptionStrategy pollExceptionStrategy;
+    @Metadata(label = "consumer,advanced")
+    private int createConsumerBackoffMaxAttempts;
+    @Metadata(label = "consumer,advanced", defaultValue = "5000")
+    private long createConsumerBackoffInterval = 5000;
+    @Metadata(label = "consumer,advanced")
+    private int subscribeConsumerBackoffMaxAttempts;
+    @Metadata(label = "consumer,advanced", defaultValue = "5000")
+    private long subscribeConsumerBackoffInterval = 5000;
 
     public KafkaComponent() {
     }
@@ -149,6 +157,67 @@ public class KafkaComponent extends DefaultComponent implements SSLContextParame
         this.pollExceptionStrategy = pollExceptionStrategy;
     }
 
+    public int getCreateConsumerBackoffMaxAttempts() {
+        return createConsumerBackoffMaxAttempts;
+    }
+
+    /**
+     * Maximum attempts to create the kafka consumer (kafka-client), before eventually giving up and failing.
+     *
+     * Error during creating the consumer may be fatal due to invalid configuration and as such recovery is not
+     * possible. However, one part of the validation is DNS resolution of the bootstrap broker hostnames. This may be a
+     * temporary networking problem, and could potentially be recoverable. While other errors are fatal such as some
+     * invalid kafka configurations. Unfortunately kafka-client does not separate this kind of errors.
+     *
+     * Camel will by default retry forever, and therefore never give up. If you want to give up after many attempts then
+     * set this option and Camel will then when giving up terminate the consumer. You can manually restart the consumer
+     * by stopping and starting the route, to try again.
+     */
+    public void setCreateConsumerBackoffMaxAttempts(int createConsumerBackoffMaxAttempts) {
+        this.createConsumerBackoffMaxAttempts = createConsumerBackoffMaxAttempts;
+    }
+
+    public long getCreateConsumerBackoffInterval() {
+        return createConsumerBackoffInterval;
+    }
+
+    /**
+     * The delay in millis seconds to wait before trying again to create the kafka consumer (kafka-client).
+     */
+    public void setCreateConsumerBackoffInterval(long createConsumerBackoffInterval) {
+        this.createConsumerBackoffInterval = createConsumerBackoffInterval;
+    }
+
+    public int getSubscribeConsumerBackoffMaxAttempts() {
+        return subscribeConsumerBackoffMaxAttempts;
+    }
+
+    /**
+     * Maximum number the kafka consumer will attempt to subscribe to the kafka broker, before eventually giving up and
+     * failing.
+     *
+     * Error during subscribing the consumer to the kafka topic could be temporary errors due to network issues, and
+     * could potentially be recoverable.
+     *
+     * Camel will by default retry forever, and therefore never give up. If you want to give up after many attempts then
+     * set this option and Camel will then when giving up terminate the consumer. You can manually restart the consumer
+     * by stopping and starting the route, to try again.
+     */
+    public void setSubscribeConsumerBackoffMaxAttempts(int subscribeConsumerBackoffMaxAttempts) {
+        this.subscribeConsumerBackoffMaxAttempts = subscribeConsumerBackoffMaxAttempts;
+    }
+
+    public long getSubscribeConsumerBackoffInterval() {
+        return subscribeConsumerBackoffInterval;
+    }
+
+    /**
+     * The delay in millis seconds to wait before trying again to subscribe to the kafka broker.
+     */
+    public void setSubscribeConsumerBackoffInterval(long subscribeConsumerBackoffInterval) {
+        this.subscribeConsumerBackoffInterval = subscribeConsumerBackoffInterval;
+    }
+
     @Override
     protected void doInit() throws Exception {
         super.doInit();
diff --git a/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumer.java b/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumer.java
index cb784c17f37..ccb0e68502c 100644
--- a/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumer.java
+++ b/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumer.java
@@ -116,6 +116,14 @@ public class KafkaConsumer extends DefaultConsumer implements ResumeAware<KafkaC
                 endpoint.getConfiguration().isBreakOnFirstError());
         super.doStart();
 
+        // health-check is optional so discover and resolve
+        healthCheckRepository = HealthCheckHelper.getHealthCheckRepository(endpoint.getCamelContext(), "camel-kafka",
+                KafkaHealthCheckRepository.class);
+        if (healthCheckRepository != null) {
+            consumerHealthCheck = new KafkaConsumerHealthCheck(this, getRouteId());
+            healthCheckRepository.addHealthCheck(consumerHealthCheck);
+        }
+
         // is the offset repository already started?
         StateRepository<String, String> repo = endpoint.getConfiguration().getOffsetRepository();
         if (repo instanceof ServiceSupport) {
@@ -144,14 +152,6 @@ public class KafkaConsumer extends DefaultConsumer implements ResumeAware<KafkaC
 
             tasks.add(task);
         }
-
-        // health-check is optional so discover and resolve
-        healthCheckRepository = HealthCheckHelper.getHealthCheckRepository(endpoint.getCamelContext(), "camel-kafka",
-                KafkaHealthCheckRepository.class);
-        if (healthCheckRepository != null) {
-            consumerHealthCheck = new KafkaConsumerHealthCheck(this, getRouteId());
-            healthCheckRepository.addHealthCheck(consumerHealthCheck);
-        }
     }
 
     @Override
diff --git a/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumerFatalException.java b/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumerFatalException.java
new file mode 100644
index 00000000000..8a980876cb9
--- /dev/null
+++ b/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumerFatalException.java
@@ -0,0 +1,29 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *      http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.camel.component.kafka;
+
+/**
+ * A fatal exception such as the kafka consumer is not able to be created and/or subscribed to the kafka brokers within
+ * a given backoff period, leading to camel-kafka giving up and terminating the kafka consumer thread, meaning that the
+ * kafka consumer will not try to recover. To recover requires either to restart the Camel route, or the application.
+ */
+public class KafkaConsumerFatalException extends RuntimeException {
+
+    public KafkaConsumerFatalException(String message, Throwable cause) {
+        super(message, cause);
+    }
+}
diff --git a/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumerHealthCheck.java b/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumerHealthCheck.java
index 0509eeadfbc..e759191793f 100644
--- a/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumerHealthCheck.java
+++ b/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaConsumerHealthCheck.java
@@ -22,6 +22,7 @@ import java.util.Properties;
 
 import org.apache.camel.health.HealthCheckResultBuilder;
 import org.apache.camel.impl.health.AbstractHealthCheck;
+import org.apache.camel.util.TimeUtils;
 import org.apache.kafka.clients.consumer.ConsumerConfig;
 
 /**
@@ -50,7 +51,16 @@ public class KafkaConsumerHealthCheck extends AbstractHealthCheck {
         for (KafkaFetchRecords task : tasks) {
             if (!task.isReady()) {
                 builder.down();
-                builder.message("KafkaConsumer is not ready");
+
+                String msg = "KafkaConsumer is not ready";
+                if (task.isTerminated()) {
+                    msg += " (gave up recovering and terminated the kafka consumer; restart route or application to recover).";
+                } else if (task.isRecoverable()) {
+                    String time = TimeUtils.printDuration(task.getCurrentRecoveryInterval());
+                    msg += " (recovery in progress using " + time + " intervals).";
+                }
+                builder.message(msg);
+
                 // was this caused by consumer not able to connect then this is stored in last error
                 builder.error(task.getLastError());
 
diff --git a/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaFetchRecords.java b/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaFetchRecords.java
index 22d95c15a41..5f4cf41ab35 100644
--- a/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaFetchRecords.java
+++ b/components/camel-kafka/src/main/java/org/apache/camel/component/kafka/KafkaFetchRecords.java
@@ -34,8 +34,12 @@ import org.apache.camel.component.kafka.consumer.support.PartitionAssignmentList
 import org.apache.camel.component.kafka.consumer.support.ProcessingResult;
 import org.apache.camel.component.kafka.consumer.support.ResumeStrategyFactory;
 import org.apache.camel.support.BridgeExceptionHandlerToErrorHandler;
+import org.apache.camel.support.task.ForegroundTask;
+import org.apache.camel.support.task.Tasks;
+import org.apache.camel.support.task.budget.Budgets;
 import org.apache.camel.util.IOHelper;
 import org.apache.camel.util.ReflectionHelper;
+import org.apache.camel.util.TimeUtils;
 import org.apache.kafka.clients.CommonClientConfigs;
 import org.apache.kafka.clients.consumer.ConsumerRecords;
 import org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient;
@@ -62,6 +66,8 @@ class KafkaFetchRecords implements Runnable {
     private CommitManager commitManager;
     private Exception lastError;
 
+    private boolean terminated;
+    private long currentBackoffInterval;
     private boolean retry = true;
     private boolean reconnect; // must be false at init (this is the policy whether to reconnect)
     private boolean connected; // this is the state (connected or not)
@@ -85,23 +91,90 @@ class KafkaFetchRecords implements Runnable {
         }
 
         do {
-            String phase = "General error";
-            try {
-                if (!isConnected()) {
-                    phase = "Error creating";
-                    createConsumer();
-                    commitManager = CommitManagers.createCommitManager(consumer, kafkaConsumer, threadId, getPrintableTopic());
-
-                    phase = "Error initializing";
-                    initializeConsumer();
-                    setConnected(true);
+            terminated = false;
+
+            if (!isConnected()) {
+
+                // task that deals with creating kafka consumer
+                currentBackoffInterval = kafkaConsumer.getEndpoint().getComponent().getCreateConsumerBackoffInterval();
+                ForegroundTask task = Tasks.foregroundTask()
+                        .withName("Create KafkaConsumer")
+                        .withBudget(Budgets.iterationBudget()
+                                .withMaxIterations(
+                                        kafkaConsumer.getEndpoint().getComponent().getCreateConsumerBackoffMaxAttempts())
+                                .withInitialDelay(Duration.ZERO)
+                                .withInterval(Duration.ofMillis(currentBackoffInterval))
+                                .build())
+                        .build();
+                boolean success = task.run(() -> {
+                    try {
+                        createConsumer();
+                        commitManager
+                                = CommitManagers.createCommitManager(consumer, kafkaConsumer, threadId, getPrintableTopic());
+
+                    } catch (Exception e) {
+                        setConnected(false);
+                        // ensure this is logged so users can see the problem
+                        LOG.warn("Error creating org.apache.kafka.clients.consumer.KafkaConsumer due to: {}", e.getMessage(),
+                                e);
+                        lastError = e;
+                        return false;
+                    }
+
+                    return true;
+                });
+                if (!success) {
+                    int max = kafkaConsumer.getEndpoint().getComponent().getCreateConsumerBackoffMaxAttempts();
+                    String time = TimeUtils.printDuration(task.elapsed());
+                    String topic = getPrintableTopic();
+                    String msg = "Gave up creating org.apache.kafka.clients.consumer.KafkaConsumer "
+                                 + threadId + " to " + topic + " after " + max + " attempts (elapsed: " + time + ").";
+                    LOG.warn(msg);
+                    lastError = new KafkaConsumerFatalException(msg, lastError);
+                    // give up and terminate this consumer
+                    terminated = true;
+                    break;
+                }
+
+                // task that deals with subscribing kafka consumer
+                currentBackoffInterval = kafkaConsumer.getEndpoint().getComponent().getSubscribeConsumerBackoffInterval();
+                task = Tasks.foregroundTask()
+                        .withName("Subscribe KafkaConsumer")
+                        .withBudget(Budgets.iterationBudget()
+                                .withMaxIterations(
+                                        kafkaConsumer.getEndpoint().getComponent().getSubscribeConsumerBackoffMaxAttempts())
+                                .withInitialDelay(Duration.ZERO)
+                                .withInterval(Duration.ofMillis(currentBackoffInterval))
+                                .build())
+                        .build();
+                success = task.run(() -> {
+                    try {
+                        initializeConsumer();
+                    } catch (Exception e) {
+                        setConnected(false);
+                        // ensure this is logged so users can see the problem
+                        LOG.warn("Error subscribing org.apache.kafka.clients.consumer.KafkaConsumer due to: {}", e.getMessage(),
+                                e);
+                        lastError = e;
+                        return false;
+                    }
+
+                    return true;
+                });
+                if (!success) {
+                    int max = kafkaConsumer.getEndpoint().getComponent().getCreateConsumerBackoffMaxAttempts();
+                    String time = TimeUtils.printDuration(task.elapsed());
+                    String topic = getPrintableTopic();
+                    String msg = "Gave up subscribing org.apache.kafka.clients.consumer.KafkaConsumer " +
+                                 threadId + " to " + topic + " after " + max + " attempts (elapsed: " + time + ").";
+                    LOG.warn(msg);
+                    lastError = new KafkaConsumerFatalException(msg, lastError);
+                    // give up and terminate this consumer
+                    terminated = true;
+                    break;
                 }
-            } catch (Exception e) {
-                setConnected(false);
-                // ensure this is logged so users can see the problem
-                LOG.warn("{} org.apache.kafka.clients.consumer.KafkaConsumer due to: {}", phase, e.getMessage(), e);
-                lastError = e;
-                continue;
+
+                setConnected(true);
             }
 
             lastError = null;
@@ -109,7 +182,7 @@ class KafkaFetchRecords implements Runnable {
         } while ((isRetrying() || isReconnect()) && isKafkaConsumerRunnable());
 
         if (LOG.isInfoEnabled()) {
-            LOG.info("Terminating KafkaConsumer thread: {} receiving from {}", threadId, getPrintableTopic());
+            LOG.info("Terminating KafkaConsumer thread {} receiving from {}", threadId, getPrintableTopic());
         }
 
         safeUnsubscribe();
@@ -276,12 +349,16 @@ class KafkaFetchRecords implements Runnable {
     }
 
     private void safeUnsubscribe() {
-        final String printableTopic = getPrintableTopic();
+        if (consumer == null) {
+            return;
+        }
 
+        final String printableTopic = getPrintableTopic();
         try {
             consumer.unsubscribe();
         } catch (IllegalStateException e) {
-            LOG.warn("The consumer is likely already closed. Skipping the unsubscription from {}", printableTopic);
+            LOG.warn("The consumer is likely already closed. Skipping unsubscribing thread {} from kafka {}", threadId,
+                    printableTopic);
         } catch (Exception e) {
             kafkaConsumer.getExceptionHandler().handleException(
                     "Error unsubscribing thread " + threadId + " from kafka " + printableTopic, e);
@@ -403,6 +480,10 @@ class KafkaFetchRecords implements Runnable {
      * should be made here besides the wakeUp.
      */
     private void safeStop() {
+        if (consumer == null) {
+            return;
+        }
+
         long timeout = kafkaConsumer.getEndpoint().getConfiguration().getShutdownTimeout();
         try {
             /*
@@ -473,4 +554,16 @@ class KafkaFetchRecords implements Runnable {
     Exception getLastError() {
         return lastError;
     }
+
+    boolean isTerminated() {
+        return terminated;
+    }
+
+    boolean isRecoverable() {
+        return (isRetrying() || isReconnect()) && isKafkaConsumerRunnable();
+    }
+
+    long getCurrentRecoveryInterval() {
+        return currentBackoffInterval;
+    }
 }
diff --git a/core/camel-support/src/main/java/org/apache/camel/support/task/ForegroundTask.java b/core/camel-support/src/main/java/org/apache/camel/support/task/ForegroundTask.java
index e69c68b2896..deefe7e8d78 100644
--- a/core/camel-support/src/main/java/org/apache/camel/support/task/ForegroundTask.java
+++ b/core/camel-support/src/main/java/org/apache/camel/support/task/ForegroundTask.java
@@ -36,13 +36,26 @@ public class ForegroundTask implements BlockingTask {
      * A builder helper for building new foreground tasks
      */
     public static class ForegroundTaskBuilder extends AbstractTaskBuilder<ForegroundTask> {
+        private String name;
         private IterationBudget budget;
 
         /**
-         * Sets an iteration budget for the task (i.e.: the task will not run more than the given number of iterations)
+         * Sets the name of the task
          * 
+         * @param  name the name
+         * @return      an instance of this builder
+         */
+        public ForegroundTaskBuilder withName(String name) {
+            this.name = name;
+
+            return this;
+        }
+
+        /**
+         * Sets an iteration budget for the task (i.e.: the task will not run more than the given number of iterations)
+         *
          * @param  budget the budget
-         * @return        an instance of the this builder
+         * @return        an instance of this builder
          */
         public ForegroundTaskBuilder withBudget(IterationBudget budget) {
             this.budget = budget;
@@ -52,7 +65,7 @@ public class ForegroundTask implements BlockingTask {
 
         @Override
         public ForegroundTask build() {
-            return new ForegroundTask(budget, getName());
+            return new ForegroundTask(budget, name != null ? name : getName());
         }
     }
 
diff --git a/core/camel-support/src/main/java/org/apache/camel/support/task/budget/IterationBoundedBudgetBuilder.java b/core/camel-support/src/main/java/org/apache/camel/support/task/budget/IterationBoundedBudgetBuilder.java
index e410bbe275d..a6ef105eb91 100644
--- a/core/camel-support/src/main/java/org/apache/camel/support/task/budget/IterationBoundedBudgetBuilder.java
+++ b/core/camel-support/src/main/java/org/apache/camel/support/task/budget/IterationBoundedBudgetBuilder.java
@@ -36,19 +36,31 @@ public class IterationBoundedBudgetBuilder implements BudgetBuilder<IterationBud
     protected BackOffStrategy backOffStrategy;
 
     public IterationBoundedBudgetBuilder withInitialDelay(Duration duration) {
-        this.initialDelay = duration.toMillis();
+        if (duration != null) {
+            this.initialDelay = duration.toMillis();
+        }
 
         return this;
     }
 
     public IterationBoundedBudgetBuilder withInterval(Duration duration) {
-        this.interval = duration.toMillis();
+        if (duration != null) {
+            this.interval = duration.toMillis();
+        }
 
         return this;
     }
 
     public IterationBoundedBudgetBuilder withMaxIterations(int maxIterations) {
-        this.maxIterations = maxIterations;
+        if (maxIterations > 0) {
+            this.maxIterations = maxIterations;
+        }
+
+        return this;
+    }
+
+    public IterationBoundedBudgetBuilder withUnlimitedMaxIterations() {
+        this.maxIterations = Integer.MAX_VALUE;
 
         return this;
     }
diff --git a/core/camel-support/src/main/java/org/apache/camel/support/task/budget/IterationTimeBoundedBudgetBuilder.java b/core/camel-support/src/main/java/org/apache/camel/support/task/budget/IterationTimeBoundedBudgetBuilder.java
index c949aa38092..93ca29c4b0b 100644
--- a/core/camel-support/src/main/java/org/apache/camel/support/task/budget/IterationTimeBoundedBudgetBuilder.java
+++ b/core/camel-support/src/main/java/org/apache/camel/support/task/budget/IterationTimeBoundedBudgetBuilder.java
@@ -35,25 +35,33 @@ public class IterationTimeBoundedBudgetBuilder implements BudgetBuilder<Iteratio
     private int maxIterations = DEFAULT_MAX_ITERATIONS;
 
     public IterationTimeBoundedBudgetBuilder withInitialDelay(Duration duration) {
-        this.initialDelay = duration.toMillis();
+        if (duration != null) {
+            this.initialDelay = duration.toMillis();
+        }
 
         return this;
     }
 
     public IterationTimeBoundedBudgetBuilder withInterval(Duration duration) {
-        this.interval = duration.toMillis();
+        if (duration != null) {
+            this.interval = duration.toMillis();
+        }
 
         return this;
     }
 
     public IterationTimeBoundedBudgetBuilder withMaxIterations(int maxIterations) {
-        this.maxIterations = maxIterations;
+        if (maxIterations > 0) {
+            this.maxIterations = maxIterations;
+        }
 
         return this;
     }
 
     public IterationTimeBoundedBudgetBuilder withMaxDuration(Duration duration) {
-        this.maxDuration = duration.toMillis();
+        if (duration != null) {
+            this.maxDuration = duration.toMillis();
+        }
 
         return this;
     }
diff --git a/dsl/camel-componentdsl/src/generated/java/org/apache/camel/builder/component/dsl/KafkaComponentBuilderFactory.java b/dsl/camel-componentdsl/src/generated/java/org/apache/camel/builder/component/dsl/KafkaComponentBuilderFactory.java
index 64703ab0e3c..9f99a727ab4 100644
--- a/dsl/camel-componentdsl/src/generated/java/org/apache/camel/builder/component/dsl/KafkaComponentBuilderFactory.java
+++ b/dsl/camel-componentdsl/src/generated/java/org/apache/camel/builder/component/dsl/KafkaComponentBuilderFactory.java
@@ -757,6 +757,49 @@ public interface KafkaComponentBuilderFactory {
             doSetProperty("valueDeserializer", valueDeserializer);
             return this;
         }
+        /**
+         * The delay in millis seconds to wait before trying again to create the
+         * kafka consumer (kafka-client).
+         * 
+         * The option is a: &lt;code&gt;long&lt;/code&gt; type.
+         * 
+         * Default: 5000
+         * Group: consumer (advanced)
+         * 
+         * @param createConsumerBackoffInterval the value to set
+         * @return the dsl builder
+         */
+        default KafkaComponentBuilder createConsumerBackoffInterval(
+                long createConsumerBackoffInterval) {
+            doSetProperty("createConsumerBackoffInterval", createConsumerBackoffInterval);
+            return this;
+        }
+        /**
+         * Maximum attempts to create the kafka consumer (kafka-client), before
+         * eventually giving up and failing. Error during creating the consumer
+         * may be fatal due to invalid configuration and as such recovery is not
+         * possible. However, one part of the validation is DNS resolution of
+         * the bootstrap broker hostnames. This may be a temporary networking
+         * problem, and could potentially be recoverable. While other errors are
+         * fatal such as some invalid kafka configurations. Unfortunately
+         * kafka-client does not separate this kind of errors. Camel will by
+         * default retry forever, and therefore never give up. If you want to
+         * give up after many attempts then set this option and Camel will then
+         * when giving up terminate the consumer. You can manually restart the
+         * consumer by stopping and starting the route, to try again.
+         * 
+         * The option is a: &lt;code&gt;int&lt;/code&gt; type.
+         * 
+         * Group: consumer (advanced)
+         * 
+         * @param createConsumerBackoffMaxAttempts the value to set
+         * @return the dsl builder
+         */
+        default KafkaComponentBuilder createConsumerBackoffMaxAttempts(
+                int createConsumerBackoffMaxAttempts) {
+            doSetProperty("createConsumerBackoffMaxAttempts", createConsumerBackoffMaxAttempts);
+            return this;
+        }
         /**
          * Factory to use for creating KafkaManualCommit instances. This allows
          * to plugin a custom factory to create custom KafkaManualCommit
@@ -794,6 +837,46 @@ public interface KafkaComponentBuilderFactory {
             doSetProperty("pollExceptionStrategy", pollExceptionStrategy);
             return this;
         }
+        /**
+         * The delay in millis seconds to wait before trying again to subscribe
+         * to the kafka broker.
+         * 
+         * The option is a: &lt;code&gt;long&lt;/code&gt; type.
+         * 
+         * Default: 5000
+         * Group: consumer (advanced)
+         * 
+         * @param subscribeConsumerBackoffInterval the value to set
+         * @return the dsl builder
+         */
+        default KafkaComponentBuilder subscribeConsumerBackoffInterval(
+                long subscribeConsumerBackoffInterval) {
+            doSetProperty("subscribeConsumerBackoffInterval", subscribeConsumerBackoffInterval);
+            return this;
+        }
+        /**
+         * Maximum number the kafka consumer will attempt to subscribe to the
+         * kafka broker, before eventually giving up and failing. Error during
+         * subscribing the consumer to the kafka topic could be temporary errors
+         * due to network issues, and could potentially be recoverable. Camel
+         * will by default retry forever, and therefore never give up. If you
+         * want to give up after many attempts then set this option and Camel
+         * will then when giving up terminate the consumer. You can manually
+         * restart the consumer by stopping and starting the route, to try
+         * again.
+         * 
+         * The option is a: &lt;code&gt;int&lt;/code&gt; type.
+         * 
+         * Group: consumer (advanced)
+         * 
+         * @param subscribeConsumerBackoffMaxAttempts the value to set
+         * @return the dsl builder
+         */
+        default KafkaComponentBuilder subscribeConsumerBackoffMaxAttempts(
+                int subscribeConsumerBackoffMaxAttempts) {
+            doSetProperty("subscribeConsumerBackoffMaxAttempts", subscribeConsumerBackoffMaxAttempts);
+            return this;
+        }
         /**
          * The total bytes of memory the producer can use to buffer records
          * waiting to be sent to the server. If records are sent faster than
@@ -2021,8 +2104,12 @@ public interface KafkaComponentBuilderFactory {
             case "specificAvroReader": getOrCreateConfiguration((KafkaComponent) component).setSpecificAvroReader((boolean) value); return true;
             case "topicIsPattern": getOrCreateConfiguration((KafkaComponent) component).setTopicIsPattern((boolean) value); return true;
             case "valueDeserializer": getOrCreateConfiguration((KafkaComponent) component).setValueDeserializer((java.lang.String) value); return true;
+            case "createConsumerBackoffInterval": ((KafkaComponent) component).setCreateConsumerBackoffInterval((long) value); return true;
+            case "createConsumerBackoffMaxAttempts": ((KafkaComponent) component).setCreateConsumerBackoffMaxAttempts((int) value); return true;
             case "kafkaManualCommitFactory": ((KafkaComponent) component).setKafkaManualCommitFactory((org.apache.camel.component.kafka.consumer.KafkaManualCommitFactory) value); return true;
             case "pollExceptionStrategy": ((KafkaComponent) component).setPollExceptionStrategy((org.apache.camel.component.kafka.PollExceptionStrategy) value); return true;
+            case "subscribeConsumerBackoffInterval": ((KafkaComponent) component).setSubscribeConsumerBackoffInterval((long) value); return true;
+            case "subscribeConsumerBackoffMaxAttempts": ((KafkaComponent) component).setSubscribeConsumerBackoffMaxAttempts((int) value); return true;
             case "bufferMemorySize": getOrCreateConfiguration((KafkaComponent) component).setBufferMemorySize((java.lang.Integer) value); return true;
             case "compressionCodec": getOrCreateConfiguration((KafkaComponent) component).setCompressionCodec((java.lang.String) value); return true;
             case "connectionMaxIdleMs": getOrCreateConfiguration((KafkaComponent) component).setConnectionMaxIdleMs((java.lang.Integer) value); return true;
