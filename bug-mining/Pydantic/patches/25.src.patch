diff --git a/.travis.yml b/.travis.yml
index f8793d108..35883d88b 100644
--- a/.travis.yml
+++ b/.travis.yml
@@ -29,6 +29,7 @@ script:
 - coverage combine
 
 - make mypy
+- make external-mypy
 - make docs
 - BENCHMARK_REPEATS=1 make benchmark-all
 - ./tests/check_tag.py
diff --git a/HISTORY.rst b/HISTORY.rst
index 6ea9dd901..e92929d03 100644
--- a/HISTORY.rst
+++ b/HISTORY.rst
@@ -10,6 +10,7 @@ v0.19.0 (unreleased)
 * Add ``multiple_of`` constraint to ``ConstrainedDecimal``, ``ConstrainedFloat``, ``ConstrainedInt``
   and their related types ``condecimal``, ``confloat``, and ``conint`` #371, thanks @StephenBrown2
 * Deprecated ``ignore_extra`` and ``allow_extra`` Config fields in favor of ``extra``, #352 by @liiight
+* Add type annotations to all functions, test fully with mypy, #373 by @samuelcolvin
 
 v0.18.2 (2019-01-22)
 ....................
diff --git a/Makefile b/Makefile
index df128b9ac..ee8379dec 100644
--- a/Makefile
+++ b/Makefile
@@ -18,20 +18,26 @@ lint:
 	pytest pydantic -p no:sugar -q
 	black -S -l 120 --py36 --check pydantic tests
 
+.PHONY: mypy
+mypy:
+	mypy pydantic
+
 .PHONY: test
 test:
 	pytest --cov=pydantic
 
-.PHONY: mypy
-mypy:
+.PHONY: external-mypy
+external-mypy:
 	@echo "testing simple example with mypy (and python to check it's sane)..."
-	mypy --ignore-missing-imports --follow-imports=skip --strict-optional tests/mypy_test_success.py
 	python tests/mypy_test_success.py
-	@echo "checking code with bad type annotations fails..."
-	@mypy --ignore-missing-imports --follow-imports=skip tests/mypy_test_fails.py 1>/dev/null; \
+	mypy tests/mypy_test_success.py
+	@echo "checking code with incorrect types fails..."
+	@mypy tests/mypy_test_fails1.py 1>/dev/null; \
+	  test $$? -eq 1 || \
+	  (echo "mypy_test_fails1: mypy passed when it should have failed!"; exit 1)
+	@mypy tests/mypy_test_fails2.py 1>/dev/null; \
 	  test $$? -eq 1 || \
-	  (echo "mypy passed when it shouldn't"; exit 1)
-	python tests/mypy_test_fails.py
+	  (echo "mypy_test_fails2: mypy passed when it should have failed!"; exit 1)
 
 .PHONY: testcov
 testcov:
@@ -40,7 +46,7 @@ testcov:
 	@coverage html
 
 .PHONY: all
-all: testcov mypy lint
+all: testcov lint mypy external-mypy
 
 .PHONY: benchmark-all
 benchmark-all:
@@ -57,6 +63,8 @@ clean:
 	rm -f `find . -type f -name '*~' `
 	rm -f `find . -type f -name '.*~' `
 	rm -rf .cache
+	rm -rf .pytest_cache
+	rm -rf .mypy_cache
 	rm -rf htmlcov
 	rm -rf *.egg-info
 	rm -f .coverage
diff --git a/docs/index.rst b/docs/index.rst
index d288a5b3a..a5fb90804 100644
--- a/docs/index.rst
+++ b/docs/index.rst
@@ -130,6 +130,14 @@ created by the standard library ``dataclass`` decorator.
 ``pydantic.dataclasses.dataclass``'s arguments are the same as the standard decorator, except one extra
 key word argument ``config`` which has the same meaning as :ref:`Config <config>`.
 
+.. note::
+
+   As a side effect of getting pydantic dataclasses to play nicely with mypy the ``config`` argument will show
+   as invalid in IDEs and mypy, use ``@dataclass(..., config=Config) # type: ignore`` as a workaround. See
+   `python/mypy#6239 <https://github.com/python/mypy/issues/6239>`_ for an explanation of why this is.
+
+Nested dataclasses
+~~~~~~~~~~~~~~~~~~
 
 Since version ``v0.17`` nested dataclasses are support both in dataclasses and normal models.
 
diff --git a/pydantic/class_validators.py b/pydantic/class_validators.py
index 491ef003e..3fbdf2532 100644
--- a/pydantic/class_validators.py
+++ b/pydantic/class_validators.py
@@ -3,10 +3,10 @@ from dataclasses import dataclass
 from enum import IntEnum
 from itertools import chain
 from types import FunctionType
-from typing import Callable, Dict
+from typing import Any, Callable, Dict, List, Optional, Set
 
 from .errors import ConfigError
-from .utils import in_ipython
+from .utils import AnyCallable, in_ipython
 
 
 class ValidatorSignature(IntEnum):
@@ -18,17 +18,19 @@ class ValidatorSignature(IntEnum):
 
 @dataclass
 class Validator:
-    func: Callable
+    func: AnyCallable
     pre: bool
     whole: bool
     always: bool
     check_fields: bool
 
 
-_FUNCS = set()
+_FUNCS: Set[str] = set()
 
 
-def validator(*fields, pre: bool = False, whole: bool = False, always: bool = False, check_fields: bool = True):
+def validator(
+    *fields: str, pre: bool = False, whole: bool = False, always: bool = False, check_fields: bool = True
+) -> Callable[[AnyCallable], classmethod]:
     """
     Decorate methods on the class indicating that they should be used to validate fields
     :param fields: which field(s) the method should be called on
@@ -45,7 +47,7 @@ def validator(*fields, pre: bool = False, whole: bool = False, always: bool = Fa
             "E.g. usage should be `@validator('<field_name>', ...)`"
         )
 
-    def dec(f):
+    def dec(f: AnyCallable) -> classmethod:
         # avoid validators with duplicated names since without this validators can be overwritten silently
         # which generally isn't the intended behaviour, don't run in ipython - see #312
         if not in_ipython():  # pragma: no branch
@@ -54,26 +56,30 @@ def validator(*fields, pre: bool = False, whole: bool = False, always: bool = Fa
                 raise ConfigError(f'duplicate validator function "{ref}"')
             _FUNCS.add(ref)
         f_cls = classmethod(f)
-        f_cls.__validator_config = fields, Validator(f, pre, whole, always, check_fields)
+        f_cls.__validator_config = fields, Validator(f, pre, whole, always, check_fields)  # type: ignore
         return f_cls
 
     return dec
 
 
+ValidatorListDict = Dict[str, List[Validator]]
+
+
 class ValidatorGroup:
-    def __init__(self, validators):
-        self.validators: Dict[str, Validator] = validators
+    def __init__(self, validators: ValidatorListDict) -> None:
+        self.validators = validators
         self.used_validators = {'*'}
 
-    def get_validators(self, name):
+    def get_validators(self, name: str) -> Optional[Dict[str, Validator]]:
         self.used_validators.add(name)
         specific_validators = self.validators.get(name)
         wildcard_validators = self.validators.get('*')
         if specific_validators or wildcard_validators:
             validators = (specific_validators or []) + (wildcard_validators or [])
             return {v.func.__name__: v for v in validators}
+        return None
 
-    def check_for_unused(self):
+    def check_for_unused(self) -> None:
         unused_validators = set(
             chain(
                 *[
@@ -90,8 +96,8 @@ class ValidatorGroup:
             )
 
 
-def extract_validators(namespace):
-    validators = {}
+def extract_validators(namespace: Dict[str, Any]) -> Dict[str, List[Validator]]:
+    validators: Dict[str, List[Validator]] = {}
     for var_name, value in namespace.items():
         validator_config = getattr(value, '__validator_config', None)
         if validator_config:
@@ -104,7 +110,7 @@ def extract_validators(namespace):
     return validators
 
 
-def inherit_validators(base_validators, validators):
+def inherit_validators(base_validators: ValidatorListDict, validators: ValidatorListDict) -> ValidatorListDict:
     for field, field_validators in base_validators.items():
         if field not in validators:
             validators[field] = []
@@ -112,14 +118,14 @@ def inherit_validators(base_validators, validators):
     return validators
 
 
-def get_validator_signature(validator):
+def get_validator_signature(validator: Any) -> ValidatorSignature:
     signature = inspect.signature(validator)
 
     # bind here will raise a TypeError so:
     # 1. we can deal with it before validation begins
     # 2. (more importantly) it doesn't get confused with a TypeError when executing the validator
     try:
-        if 'cls' in signature._parameters:
+        if 'cls' in signature._parameters:  # type: ignore
             if len(signature.parameters) == 2:
                 signature.bind(object(), 1)
                 return ValidatorSignature.CLS_JUST_VALUE
diff --git a/pydantic/dataclasses.py b/pydantic/dataclasses.py
index 486f4b26e..4ecac89ae 100644
--- a/pydantic/dataclasses.py
+++ b/pydantic/dataclasses.py
@@ -1,10 +1,27 @@
 import dataclasses
+from typing import TYPE_CHECKING, Any, Callable, Dict, Generator, Optional, Type, Union
 
 from . import ValidationError, errors
 from .main import create_model, validate_model
+from .utils import AnyType
 
+if TYPE_CHECKING:  # pragma: no cover
+    from .main import BaseConfig, BaseModel  # noqa: F401
 
-def _pydantic_post_init(self):
+    class DataclassType:
+        __pydantic_model__: Type[BaseModel]
+        __post_init_original__: Callable[..., None]
+        __initialised__: bool
+
+        def __init__(self, *args: Any, **kwargs: Any) -> None:
+            pass
+
+        @classmethod
+        def __validate__(cls, v: Any) -> 'DataclassType':
+            pass
+
+
+def _pydantic_post_init(self: 'DataclassType') -> None:
     d = validate_model(self.__pydantic_model__, self.__dict__)
     object.__setattr__(self, '__dict__', d)
     object.__setattr__(self, '__initialised__', True)
@@ -12,10 +29,10 @@ def _pydantic_post_init(self):
         self.__post_init_original__()
 
 
-def _validate_dataclass(cls, v):
+def _validate_dataclass(cls: Type['DataclassType'], v: Any) -> 'DataclassType':
     if isinstance(v, cls):
         return v
-    elif isinstance(v, (cls, list, tuple)):
+    elif isinstance(v, (list, tuple)):
         return cls(*v)
     elif isinstance(v, dict):
         return cls(**v)
@@ -23,11 +40,11 @@ def _validate_dataclass(cls, v):
         raise errors.DataclassTypeError(class_name=cls.__name__)
 
 
-def _get_validators(cls):
+def _get_validators(cls: Type['DataclassType']) -> Generator[Any, None, None]:
     yield cls.__validate__
 
 
-def setattr_validate_assignment(self, name, value):
+def setattr_validate_assignment(self: 'DataclassType', name: str, value: Any) -> None:
     if self.__initialised__:
         d = dict(self.__dict__)
         d.pop(name)
@@ -38,17 +55,26 @@ def setattr_validate_assignment(self, name, value):
     object.__setattr__(self, name, value)
 
 
-def _process_class(_cls, init, repr, eq, order, unsafe_hash, frozen, config):
+def _process_class(
+    _cls: AnyType,
+    init: bool,
+    repr: bool,
+    eq: bool,
+    order: bool,
+    unsafe_hash: bool,
+    frozen: bool,
+    config: Type['BaseConfig'],
+) -> 'DataclassType':
     post_init_original = getattr(_cls, '__post_init__', None)
     if post_init_original and post_init_original.__name__ == '_pydantic_post_init':
         post_init_original = None
     _cls.__post_init__ = _pydantic_post_init
-    cls = dataclasses._process_class(_cls, init, repr, eq, order, unsafe_hash, frozen)
+    cls = dataclasses._process_class(_cls, init, repr, eq, order, unsafe_hash, frozen)  # type: ignore
 
-    fields = {name: (field.type, field.default) for name, field in cls.__dataclass_fields__.items()}
+    fields: Dict[str, Any] = {name: (field.type, field.default) for name, field in cls.__dataclass_fields__.items()}
     cls.__post_init_original__ = post_init_original
 
-    cls.__pydantic_model__ = create_model(cls.__name__, __config__=config, **fields)
+    cls.__pydantic_model__ = create_model(cls.__name__, __config__=config, __base__=None, **fields)
 
     cls.__initialised__ = False
     cls.__validate__ = classmethod(_validate_dataclass)
@@ -60,18 +86,33 @@ def _process_class(_cls, init, repr, eq, order, unsafe_hash, frozen, config):
     return cls
 
 
-def dataclass(_cls=None, *, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False, config=None):
-    """
-    Like the python standard lib dataclasses but with type validation.
-
-    Arguments are the same as for standard dataclasses, except for validate_assignment which has the same meaning
-    as Config.validate_assignment.
-    """
-
-    def wrap(cls):
-        return _process_class(cls, init, repr, eq, order, unsafe_hash, frozen, config)
-
-    if _cls is None:
-        return wrap
-
-    return wrap(_cls)
+if TYPE_CHECKING:  # pragma: no cover
+    # see https://github.com/python/mypy/issues/6239 for explanation of why we do this
+    from dataclasses import dataclass
+else:
+
+    def dataclass(
+        _cls: Optional[AnyType] = None,
+        *,
+        init: bool = True,
+        repr: bool = True,
+        eq: bool = True,
+        order: bool = False,
+        unsafe_hash: bool = False,
+        frozen: bool = False,
+        config: Type['BaseConfig'] = None,
+    ) -> Union[Callable[[AnyType], 'DataclassType'], 'DataclassType']:
+        """
+        Like the python standard lib dataclasses but with type validation.
+
+        Arguments are the same as for standard dataclasses, except for validate_assignment which has the same meaning
+        as Config.validate_assignment.
+        """
+
+        def wrap(cls: AnyType) -> 'DataclassType':
+            return _process_class(cls, init, repr, eq, order, unsafe_hash, frozen, config)
+
+        if _cls is None:
+            return wrap
+
+        return wrap(_cls)
diff --git a/pydantic/datetime_parse.py b/pydantic/datetime_parse.py
index 079fa9770..3e66aae5c 100644
--- a/pydantic/datetime_parse.py
+++ b/pydantic/datetime_parse.py
@@ -16,7 +16,7 @@ Changed to:
 """
 import re
 from datetime import date, datetime, time, timedelta, timezone
-from typing import Union
+from typing import Dict, Union, cast
 
 from . import errors
 from .utils import change_exception
@@ -62,20 +62,19 @@ MS_WATERSHED = int(1e11)  # if greater than this, the number is in ms (in second
 StrIntFloat = Union[str, int, float]
 
 
-def get_numeric(value: StrIntFloat):
+def get_numeric(value: StrIntFloat) -> Union[None, int, float]:
     if isinstance(value, (int, float)):
         return value
     try:
         return int(value)
     except ValueError:
-        pass
-    try:
-        return float(value)
-    except ValueError:
-        pass
+        try:
+            return float(value)
+        except ValueError:
+            return None
 
 
-def from_unix_seconds(seconds: int) -> datetime:
+def from_unix_seconds(seconds: float) -> datetime:
     while seconds > MS_WATERSHED:
         seconds /= 1000
     dt = EPOCH + timedelta(seconds=seconds)
@@ -99,7 +98,7 @@ def parse_date(value: Union[date, StrIntFloat]) -> date:
     if number is not None:
         return from_unix_seconds(number).date()
 
-    match = date_re.match(value)
+    match = date_re.match(cast(str, value))
     if not match:
         raise errors.DateError()
 
@@ -129,10 +128,10 @@ def parse_time(value: Union[time, str]) -> time:
     if kw['microsecond']:
         kw['microsecond'] = kw['microsecond'].ljust(6, '0')
 
-    kw = {k: int(v) for k, v in kw.items() if v is not None}
+    kw_ = {k: int(v) for k, v in kw.items() if v is not None}
 
     with change_exception(errors.TimeError, ValueError):
-        return time(**kw)
+        return time(**kw_)  # type: ignore
 
 
 def parse_datetime(value: Union[datetime, StrIntFloat]) -> datetime:
@@ -152,7 +151,7 @@ def parse_datetime(value: Union[datetime, StrIntFloat]) -> datetime:
     if number is not None:
         return from_unix_seconds(number)
 
-    match = datetime_re.match(value)
+    match = datetime_re.match(cast(str, value))
     if not match:
         raise errors.DateTimeError()
 
@@ -160,21 +159,23 @@ def parse_datetime(value: Union[datetime, StrIntFloat]) -> datetime:
     if kw['microsecond']:
         kw['microsecond'] = kw['microsecond'].ljust(6, '0')
 
-    tzinfo = kw.pop('tzinfo')
-    if tzinfo == 'Z':
+    tzinfo_str = kw.pop('tzinfo')
+    if tzinfo_str == 'Z':
         tzinfo = timezone.utc
-    elif tzinfo is not None:
-        offset_mins = int(tzinfo[-2:]) if len(tzinfo) > 3 else 0
-        offset = 60 * int(tzinfo[1:3]) + offset_mins
-        if tzinfo[0] == '-':
+    elif tzinfo_str is not None:
+        offset_mins = int(tzinfo_str[-2:]) if len(tzinfo_str) > 3 else 0
+        offset = 60 * int(tzinfo_str[1:3]) + offset_mins
+        if tzinfo_str[0] == '-':
             offset = -offset
         tzinfo = timezone(timedelta(minutes=offset))
+    else:
+        tzinfo = None
 
-    kw = {k: int(v) for k, v in kw.items() if v is not None}
-    kw['tzinfo'] = tzinfo
+    kw_: Dict[str, Union[int, timezone]] = {k: int(v) for k, v in kw.items() if v is not None}
+    kw_['tzinfo'] = tzinfo
 
     with change_exception(errors.DateTimeError, ValueError):
-        return datetime(**kw)
+        return datetime(**kw_)  # type: ignore
 
 
 def parse_duration(value: StrIntFloat) -> timedelta:
@@ -204,6 +205,6 @@ def parse_duration(value: StrIntFloat) -> timedelta:
     if kw.get('seconds') and kw.get('microseconds') and kw['seconds'].startswith('-'):
         kw['microseconds'] = '-' + kw['microseconds']
 
-    kw = {k: float(v) for k, v in kw.items() if v is not None}
+    kw_ = {k: float(v) for k, v in kw.items() if v is not None}
 
-    return sign * timedelta(**kw)
+    return sign * timedelta(**kw_)  # type: ignore
diff --git a/pydantic/env_settings.py b/pydantic/env_settings.py
index 3016c5d05..c94faefde 100644
--- a/pydantic/env_settings.py
+++ b/pydantic/env_settings.py
@@ -1,5 +1,6 @@
 import json
 import os
+from typing import Any, Dict, Optional, cast
 
 from .main import BaseModel, Extra
 
@@ -19,22 +20,22 @@ class BaseSettings(BaseModel):
     Heroku and any 12 factor app design.
     """
 
-    def __init__(self, **values):
+    def __init__(self, **values: Any) -> None:
         super().__init__(**self._build_values(values))
 
-    def _build_values(self, init_kwargs):
+    def _build_values(self, init_kwargs: Dict[str, Any]) -> Dict[str, Any]:
         return {**self._build_environ(), **init_kwargs}
 
-    def _build_environ(self):
+    def _build_environ(self) -> Dict[str, Optional[str]]:
         """
         Build environment variables suitable for passing to the Model.
         """
-        d = {}
+        d: Dict[str, Optional[str]] = {}
 
         if self.__config__.case_insensitive:
             env_vars = {k.lower(): v for k, v in os.environ.items()}
         else:
-            env_vars = os.environ
+            env_vars = cast(Dict[str, str], os.environ)
 
         for field in self.__fields__.values():
             if field.has_alias:
@@ -60,3 +61,5 @@ class BaseSettings(BaseModel):
         extra = Extra.forbid
         arbitrary_types_allowed = True
         case_insensitive = False
+
+    __config__: Config  # type: ignore
diff --git a/pydantic/error_wrappers.py b/pydantic/error_wrappers.py
index 35166cb29..ec863f5f3 100644
--- a/pydantic/error_wrappers.py
+++ b/pydantic/error_wrappers.py
@@ -1,5 +1,9 @@
 import json
 from functools import lru_cache
+from typing import TYPE_CHECKING, Any, Dict, Generator, List, Optional, Sequence, Tuple, Type, Union, cast
+
+if TYPE_CHECKING:  # pragma: no cover
+    from pydantic import BaseConfig  # noqa: F401
 
 __all__ = ('ErrorWrapper', 'ValidationError')
 
@@ -7,17 +11,19 @@ __all__ = ('ErrorWrapper', 'ValidationError')
 class ErrorWrapper:
     __slots__ = 'exc', 'loc', 'msg_template'
 
-    def __init__(self, exc, *, loc, config=None):
+    def __init__(
+        self, exc: Exception, *, loc: Union[Tuple[str, ...], str], config: Optional[Type['BaseConfig']] = None
+    ) -> None:
         self.exc = exc
-        self.loc = loc if isinstance(loc, tuple) else (loc,)
+        self.loc: Tuple[str, ...] = cast(Tuple[str, ...], loc if isinstance(loc, tuple) else (loc,))
         self.msg_template = config.error_msg_templates.get(self.type_) if config else None
 
     @property
-    def ctx(self):
+    def ctx(self) -> Dict[str, Any]:
         return getattr(self.exc, 'ctx', None)
 
     @property
-    def msg(self):
+    def msg(self) -> str:
         default_msg_template = getattr(self.exc, 'msg_template', None)
         msg_template = self.msg_template or default_msg_template
         if msg_template:
@@ -26,13 +32,13 @@ class ErrorWrapper:
         return str(self.exc)
 
     @property
-    def type_(self):
+    def type_(self) -> str:
         return get_exc_type(self.exc)
 
-    def dict(self, *, loc_prefix=None):
+    def dict(self, *, loc_prefix: Optional[Tuple[str, ...]] = None) -> Dict[str, Any]:
         loc = self.loc if loc_prefix is None else loc_prefix + self.loc
 
-        d = {'loc': loc, 'msg': self.msg, 'type': self.type_}
+        d: Dict[str, Any] = {'loc': loc, 'msg': self.msg, 'type': self.type_}
 
         if self.ctx is not None:
             d['ctx'] = self.ctx
@@ -40,34 +46,39 @@ class ErrorWrapper:
         return d
 
 
+# ErrorList is something like Union[List[Union[List[ErrorWrapper], ErrorWrapper]], ErrorWrapper]
+# but recursive, therefore just use:
+ErrorList = Union[Sequence[Any], ErrorWrapper]
+
+
 class ValidationError(ValueError):
     __slots__ = ('raw_errors',)
 
-    def __init__(self, errors):
+    def __init__(self, errors: Sequence[ErrorList]) -> None:
         self.raw_errors = errors
 
     @lru_cache()
-    def errors(self):
+    def errors(self) -> List[Dict[str, Any]]:
         return list(flatten_errors(self.raw_errors))
 
-    def json(self, *, indent=2):
+    def json(self, *, indent: int = 2) -> str:
         return json.dumps(self.errors(), indent=indent)
 
-    def __str__(self):
+    def __str__(self) -> str:
         errors = self.errors()
         no_errors = len(errors)
         return f'{no_errors} validation error{"" if no_errors == 1 else "s"}\n{display_errors(errors)}'
 
 
-def display_errors(errors):
+def display_errors(errors: List[Dict[str, Any]]) -> str:
     return '\n'.join(f'{_display_error_loc(e)}\n  {e["msg"]} ({_display_error_type_and_ctx(e)})' for e in errors)
 
 
-def _display_error_loc(error):
+def _display_error_loc(error: Dict[str, Any]) -> str:
     return ' -> '.join(str(l) for l in error['loc'])
 
 
-def _display_error_type_and_ctx(error):
+def _display_error_type_and_ctx(error: Dict[str, Any]) -> str:
     t = 'type=' + error['type']
     ctx = error.get('ctx')
     if ctx:
@@ -76,7 +87,9 @@ def _display_error_type_and_ctx(error):
         return t
 
 
-def flatten_errors(errors, *, loc=None):
+def flatten_errors(
+    errors: Sequence[Any], *, loc: Optional[Tuple[str, ...]] = None
+) -> Generator[Dict[str, Any], None, None]:
     for error in errors:
         if isinstance(error, ErrorWrapper):
             if isinstance(error.exc, ValidationError):
diff --git a/pydantic/errors.py b/pydantic/errors.py
index cd32101e1..ab9e7e1ff 100644
--- a/pydantic/errors.py
+++ b/pydantic/errors.py
@@ -1,15 +1,15 @@
 from decimal import Decimal
 from pathlib import Path
-from typing import Union
+from typing import Any, Union
 
-from .utils import display_as_type
+from .utils import AnyType, display_as_type
 
 
 class PydanticErrorMixin:
     code: str
     msg_template: str
 
-    def __init__(self, **ctx) -> None:
+    def __init__(self, **ctx: Any) -> None:
         self.ctx = ctx or None
         super().__init__()
 
@@ -194,7 +194,7 @@ class NumberNotMultipleError(PydanticValueError):
     code = 'number.not_multiple'
     msg_template = 'ensure this value is a multiple of {multiple_of}'
 
-    def __init__(self, *, multiple_of: str) -> None:
+    def __init__(self, *, multiple_of: Union[int, float, Decimal]) -> None:
         super().__init__(multiple_of=multiple_of)
 
 
@@ -263,7 +263,7 @@ class ArbitraryTypeError(PydanticTypeError):
     code = 'arbitrary_type'
     msg_template = 'instance of {expected_arbitrary_type} expected'
 
-    def __init__(self, *, expected_arbitrary_type) -> None:
+    def __init__(self, *, expected_arbitrary_type: AnyType) -> None:
         super().__init__(expected_arbitrary_type=display_as_type(expected_arbitrary_type))
 
 
diff --git a/pydantic/fields.py b/pydantic/fields.py
index a944ebe45..018f38fdf 100644
--- a/pydantic/fields.py
+++ b/pydantic/fields.py
@@ -1,16 +1,25 @@
 import warnings
 from enum import IntEnum
-from typing import Any, Dict, List, Mapping, Optional, Pattern, Set, Tuple, Type, Union
+from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Mapping, Optional, Pattern, Set, Tuple, Type, Union, cast
 
 from . import errors as errors_
 from .class_validators import Validator, ValidatorSignature, get_validator_signature
 from .error_wrappers import ErrorWrapper
 from .types import Json, JsonWrapper
-from .utils import Callable, ForwardRef, display_as_type, lenient_issubclass, list_like
+from .utils import AnyCallable, AnyType, Callable, ForwardRef, display_as_type, lenient_issubclass, list_like
 from .validators import NoneType, dict_validator, find_validators
 
 Required: Any = Ellipsis
 
+if TYPE_CHECKING:  # pragma: no cover
+    from .schema import Schema  # noqa: F401
+    from .main import BaseConfig, BaseModel  # noqa: F401
+    from .error_wrappers import ErrorList
+
+    ValidatorTuple = Tuple[Tuple[ValidatorSignature, AnyCallable], ...]
+    ValidateReturn = Tuple[Optional[Any], Optional[ErrorList]]
+    LocType = Union[Tuple[str, ...], str]
+
 
 class Shape(IntEnum):
     SINGLETON = 1
@@ -46,14 +55,14 @@ class Field:
         self,
         *,
         name: str,
-        type_: Type,
+        type_: AnyType,
         class_validators: Optional[Dict[str, Validator]],
-        model_config: Any,
+        model_config: Type['BaseConfig'],
         default: Any = None,
         required: bool = True,
         alias: str = None,
-        schema=None,
-    ):
+        schema: Optional['Schema'] = None,
+    ) -> None:
 
         self.name: str = name
         self.has_alias: bool = bool(alias)
@@ -63,29 +72,37 @@ class Field:
         self.default: Any = default
         self.required: bool = required
         self.model_config = model_config
-        self.schema: 'schema.Schema' = schema
+        self.schema: Optional['Schema'] = schema
 
         self.allow_none: bool = False
         self.validate_always: bool = False
-        self.sub_fields: List[Field] = None
-        self.key_field: Field = None
-        self.validators = []
-        self.whole_pre_validators = None
-        self.whole_post_validators = None
+        self.sub_fields: Optional[List[Field]] = None
+        self.key_field: Optional[Field] = None
+        self.validators: 'ValidatorTuple' = ()
+        self.whole_pre_validators: Optional['ValidatorTuple'] = None
+        self.whole_post_validators: Optional['ValidatorTuple'] = None
         self.parse_json: bool = False
         self.shape: Shape = Shape.SINGLETON
         self.prepare()
 
     @classmethod
-    def infer(cls, *, name, value, annotation, class_validators, config):
+    def infer(
+        cls,
+        *,
+        name: str,
+        value: Any,
+        annotation: Any,
+        class_validators: Optional[Dict[str, Validator]],
+        config: Type['BaseConfig'],
+    ) -> 'Field':
         schema_from_config = config.get_field_schema(name)
-        from .schema import Schema, get_annotation_from_schema
+        from .schema import Schema, get_annotation_from_schema  # noqa: F811
 
         if isinstance(value, Schema):
             schema = value
             value = schema.default
         else:
-            schema = Schema(value, **schema_from_config)
+            schema = Schema(value, **schema_from_config)  # type: ignore
         schema.alias = schema.alias or schema_from_config.get('alias')
         required = value == Required
         annotation = get_annotation_from_schema(annotation, schema)
@@ -100,18 +117,19 @@ class Field:
             schema=schema,
         )
 
-    def set_config(self, config):
+    def set_config(self, config: Type['BaseConfig']) -> None:
         self.model_config = config
         schema_from_config = config.get_field_schema(self.name)
         if schema_from_config:
+            self.schema = cast('Schema', self.schema)
             self.schema.alias = self.schema.alias or schema_from_config.get('alias')
-            self.alias = self.schema.alias
+            self.alias = cast(str, self.schema.alias)
 
     @property
-    def alt_alias(self):
+    def alt_alias(self) -> bool:
         return self.name != self.alias
 
-    def prepare(self):
+    def prepare(self) -> None:
         if self.default is not None and self.type_ is None:
             self.type_ = type(self.default)
 
@@ -133,10 +151,10 @@ class Field:
         self._populate_sub_fields()
         self._populate_validators()
 
-    def _populate_sub_fields(self):
+    def _populate_sub_fields(self) -> None:
         # typing interface is horrible, we have to do some ugly checks
         if lenient_issubclass(self.type_, JsonWrapper):
-            self.type_ = self.type_.inner_type
+            self.type_ = self.type_.inner_type  # type: ignore
             self.parse_json = True
 
         if self.type_ is Pattern:
@@ -150,36 +168,40 @@ class Field:
             return
         if origin is Union:
             types_ = []
-            for type_ in self.type_.__args__:
-                if type_ is NoneType:
+            for type_ in self.type_.__args__:  # type: ignore
+                if type_ is NoneType:  # type: ignore
                     self.allow_none = True
                     self.required = False
                 types_.append(type_)
             self.sub_fields = [self._create_sub_type(t, f'{self.name}_{display_as_type(t)}') for t in types_]
             return
 
-        if issubclass(origin, Tuple):
+        if issubclass(origin, Tuple):  # type: ignore
             self.shape = Shape.TUPLE
-            self.sub_fields = [self._create_sub_type(t, f'{self.name}_{i}') for i, t in enumerate(self.type_.__args__)]
+            self.sub_fields = [
+                self._create_sub_type(t, f'{self.name}_{i}') for i, t in enumerate(self.type_.__args__)  # type: ignore
+            ]
             return
 
         if issubclass(origin, List):
-            self.type_ = self.type_.__args__[0]
+            self.type_ = self.type_.__args__[0]  # type: ignore
             self.shape = Shape.LIST
         elif issubclass(origin, Set):
-            self.type_ = self.type_.__args__[0]
+            self.type_ = self.type_.__args__[0]  # type: ignore
             self.shape = Shape.SET
         else:
             assert issubclass(origin, Mapping)
-            self.key_field = self._create_sub_type(self.type_.__args__[0], 'key_' + self.name, for_keys=True)
-            self.type_ = self.type_.__args__[1]
+            self.key_field = self._create_sub_type(
+                self.type_.__args__[0], 'key_' + self.name, for_keys=True  # type: ignore
+            )
+            self.type_ = self.type_.__args__[1]  # type: ignore
             self.shape = Shape.MAPPING
 
         if getattr(self.type_, '__origin__', None):
             # type_ has been refined eg. as the type of a List and sub_fields needs to be populated
             self.sub_fields = [self._create_sub_type(self.type_, '_' + self.name)]
 
-    def _create_sub_type(self, type_, name, *, for_keys=False):
+    def _create_sub_type(self, type_: AnyType, name: str, *, for_keys: bool = False) -> 'Field':
         return self.__class__(
             type_=type_,
             name=name,
@@ -187,7 +209,7 @@ class Field:
             model_config=self.model_config,
         )
 
-    def _populate_validators(self):
+    def _populate_validators(self) -> None:
         class_validators_ = self.class_validators.values()
         if not self.sub_fields:
             get_validators = getattr(self.type_, '__get_validators__', None)
@@ -213,10 +235,12 @@ class Field:
             self.whole_post_validators = self._prep_vals(v.func for v in class_validators_ if v.whole and not v.pre)
 
     @staticmethod
-    def _prep_vals(v_funcs):
+    def _prep_vals(v_funcs: Iterable[AnyCallable]) -> 'ValidatorTuple':
         return tuple((get_validator_signature(f), f) for f in v_funcs if f)
 
-    def validate(self, v, values, *, loc, cls=None):
+    def validate(
+        self, v: Any, values: Dict[str, Any], *, loc: 'LocType', cls: Optional[Type['BaseModel']] = None
+    ) -> 'ValidateReturn':
         if self.allow_none and not self.validate_always and v is None:
             return None, None
 
@@ -227,6 +251,7 @@ class Field:
             if error:
                 return v, error
 
+        errors: Optional['ErrorList'] = None
         if self.whole_pre_validators:
             v, errors = self._apply_validators(v, values, loc, cls, self.whole_pre_validators)
             if errors:
@@ -248,23 +273,26 @@ class Field:
             v, errors = self._apply_validators(v, values, loc, cls, self.whole_post_validators)
         return v, errors
 
-    def _validate_json(self, v, loc):
+    def _validate_json(self, v: str, loc: Tuple[str, ...]) -> Tuple[Optional[Any], Optional[ErrorWrapper]]:
         try:
             return Json.validate(v), None
         except (ValueError, TypeError) as exc:
             return v, ErrorWrapper(exc, loc=loc, config=self.model_config)
 
-    def _validate_list_set(self, v, values, loc, cls):
+    def _validate_list_set(
+        self, v: Any, values: Dict[str, Any], loc: 'LocType', cls: Optional[Type['BaseModel']]
+    ) -> 'ValidateReturn':
         if not list_like(v):
             e = errors_.ListError() if self.shape is Shape.LIST else errors_.SetError()
             return v, ErrorWrapper(e, loc=loc, config=self.model_config)
 
-        result, errors = [], []
+        result = []
+        errors: List[ErrorList] = []
         for i, v_ in enumerate(v):
             v_loc = *loc, i
-            r, e = self._validate_singleton(v_, values, v_loc, cls)
-            if e:
-                errors.append(e)
+            r, ee = self._validate_singleton(v_, values, v_loc, cls)
+            if ee:
+                errors.append(ee)
             else:
                 result.append(r)
 
@@ -273,24 +301,27 @@ class Field:
         else:
             return result, None
 
-    def _validate_tuple(self, v, values, loc, cls):
-        e = None
+    def _validate_tuple(
+        self, v: Any, values: Dict[str, Any], loc: 'LocType', cls: Optional[Type['BaseModel']]
+    ) -> 'ValidateReturn':
+        e: Optional[Exception] = None
         if not list_like(v):
             e = errors_.TupleError()
         else:
-            actual_length, expected_length = len(v), len(self.sub_fields)
+            actual_length, expected_length = len(v), len(self.sub_fields)  # type: ignore
             if actual_length != expected_length:
                 e = errors_.TupleLengthError(actual_length=actual_length, expected_length=expected_length)
 
         if e:
             return v, ErrorWrapper(e, loc=loc, config=self.model_config)
 
-        result, errors = [], []
-        for i, (v_, field) in enumerate(zip(v, self.sub_fields)):
+        result = []
+        errors: List[ErrorList] = []
+        for i, (v_, field) in enumerate(zip(v, self.sub_fields)):  # type: ignore
             v_loc = *loc, i
-            r, e = field.validate(v_, values, loc=v_loc, cls=cls)
-            if e:
-                errors.append(e)
+            r, ee = field.validate(v_, values, loc=v_loc, cls=cls)
+            if ee:
+                errors.append(ee)
             else:
                 result.append(r)
 
@@ -299,7 +330,9 @@ class Field:
         else:
             return tuple(result), None
 
-    def _validate_mapping(self, v, values, loc, cls):
+    def _validate_mapping(
+        self, v: Any, values: Dict[str, Any], loc: 'LocType', cls: Optional[Type['BaseModel']]
+    ) -> 'ValidateReturn':
         try:
             v_iter = dict_validator(v)
         except TypeError as exc:
@@ -308,7 +341,7 @@ class Field:
         result, errors = {}, []
         for k, v_ in v_iter.items():
             v_loc = *loc, '__key__'
-            key_result, key_errors = self.key_field.validate(k, values, loc=v_loc, cls=cls)
+            key_result, key_errors = self.key_field.validate(k, values, loc=v_loc, cls=cls)  # type: ignore
             if key_errors:
                 errors.append(key_errors)
                 continue
@@ -325,7 +358,9 @@ class Field:
         else:
             return result, None
 
-    def _validate_singleton(self, v, values, loc, cls):
+    def _validate_singleton(
+        self, v: Any, values: Dict[str, Any], loc: 'LocType', cls: Optional[Type['BaseModel']]
+    ) -> 'ValidateReturn':
         if self.sub_fields:
             errors = []
             for field in self.sub_fields:
@@ -338,7 +373,14 @@ class Field:
         else:
             return self._apply_validators(v, values, loc, cls, self.validators)
 
-    def _apply_validators(self, v, values, loc, cls, validators):
+    def _apply_validators(
+        self,
+        v: Any,
+        values: Dict[str, Any],
+        loc: 'LocType',
+        cls: Optional[Type['BaseModel']],
+        validators: 'ValidatorTuple',
+    ) -> 'ValidateReturn':
         for signature, validator in validators:
             try:
                 if signature is ValidatorSignature.JUST_VALUE:
@@ -358,13 +400,13 @@ class Field:
         """
         False if this is a simple field just allowing None as used in Unions/Optional.
         """
-        return self.type_ != NoneType
+        return self.type_ != NoneType  # type: ignore
 
-    def is_complex(self):
+    def is_complex(self) -> bool:
         """
         Whether the field is "complex" eg. env variables should be parsed as JSON.
         """
-        from .main import BaseModel
+        from .main import BaseModel  # noqa: F811
 
         return (
             self.shape != Shape.SINGLETON
@@ -372,10 +414,10 @@ class Field:
             or hasattr(self.type_, '__pydantic_model__')  # pydantic dataclass
         )
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return f'<Field({self})>'
 
-    def __str__(self):
+    def __str__(self) -> str:
         parts = [self.name, 'type=' + display_as_type(self.type_)]
 
         if self.required:
diff --git a/pydantic/json.py b/pydantic/json.py
index 158681c6e..27df306c6 100644
--- a/pydantic/json.py
+++ b/pydantic/json.py
@@ -2,16 +2,17 @@ import datetime
 from decimal import Decimal
 from enum import Enum
 from types import GeneratorType
+from typing import Any, Callable, Dict, Type, Union
 from uuid import UUID
 
 __all__ = 'pydantic_encoder', 'custom_pydantic_encoder', 'timedelta_isoformat'
 
 
-def isoformat(o):
+def isoformat(o: Union[datetime.date, datetime.time]) -> str:
     return o.isoformat()
 
 
-ENCODERS_BY_TYPE = {
+ENCODERS_BY_TYPE: Dict[Type[Any], Callable[[Any], Any]] = {
     UUID: str,
     datetime.datetime: isoformat,
     datetime.date: isoformat,
@@ -25,7 +26,7 @@ ENCODERS_BY_TYPE = {
 }
 
 
-def pydantic_encoder(obj):
+def pydantic_encoder(obj: Any) -> Any:
     from .main import BaseModel
 
     if isinstance(obj, BaseModel):
@@ -41,7 +42,7 @@ def pydantic_encoder(obj):
         return encoder(obj)
 
 
-def custom_pydantic_encoder(type_encoders, obj):
+def custom_pydantic_encoder(type_encoders: Dict[Any, Callable[[Type[Any]], Any]], obj: Any) -> Any:
     encoder = type_encoders.get(type(obj))
     if encoder:
         return encoder(obj)
diff --git a/pydantic/main.py b/pydantic/main.py
index 087507661..307384b91 100644
--- a/pydantic/main.py
+++ b/pydantic/main.py
@@ -7,7 +7,21 @@ from enum import Enum
 from functools import partial
 from pathlib import Path
 from types import FunctionType
-from typing import Any, Callable, ClassVar, Dict, Set, Type, Union
+from typing import (
+    TYPE_CHECKING,
+    Any,
+    Callable,
+    ClassVar,
+    Dict,
+    Generator,
+    Optional,
+    Set,
+    Tuple,
+    Type,
+    Union,
+    cast,
+    no_type_check,
+)
 
 from .class_validators import ValidatorGroup, extract_validators, inherit_validators
 from .error_wrappers import ErrorWrapper, ValidationError
@@ -17,9 +31,18 @@ from .json import custom_pydantic_encoder, pydantic_encoder
 from .parse import Protocol, load_file, load_str_bytes
 from .schema import model_schema
 from .types import StrBytes
-from .utils import ForwardRef, resolve_annotations, truncate, validate_field_name
+from .utils import AnyCallable, AnyType, ForwardRef, resolve_annotations, truncate, validate_field_name
 from .validators import dict_validator
 
+if TYPE_CHECKING:  # pragma: no cover
+    from .types import CallableGenerator
+    from .class_validators import ValidatorListDict
+
+    AnyGenerator = Generator[Any, None, None]
+    TupleGenerator = Generator[Tuple[str, Any], None, None]
+    DictStrAny = Dict[str, Any]
+    ConfigType = Type['BaseConfig']
+
 
 class Extra(str, Enum):
     allow = 'allow'
@@ -37,34 +60,34 @@ class BaseConfig:
     allow_mutation = True
     allow_population_by_alias = False
     use_enum_values = False
-    fields = {}
+    fields: Dict[str, Union[str, Dict[str, str]]] = {}
     validate_assignment = False
     error_msg_templates: Dict[str, str] = {}
     arbitrary_types_allowed = False
-    json_encoders = {}
+    json_encoders: Dict[AnyType, AnyCallable] = {}
 
     @classmethod
-    def get_field_schema(cls, name):
+    def get_field_schema(cls, name: str) -> Dict[str, str]:
         field_config = cls.fields.get(name) or {}
         if isinstance(field_config, str):
             field_config = {'alias': field_config}
         return field_config
 
 
-def inherit_config(self_config: Type, parent_config: Type[BaseConfig]) -> Type[BaseConfig]:
+def inherit_config(self_config: 'ConfigType', parent_config: 'ConfigType') -> 'ConfigType':
     if not self_config:
         base_classes = (parent_config,)
     elif self_config == parent_config:
         base_classes = (self_config,)
     else:
-        base_classes = self_config, parent_config
+        base_classes = self_config, parent_config  # type: ignore
     return type('Config', base_classes, {})
 
 
 EXTRA_LINK = 'https://pydantic-docs.helpmanual.io/#model-config'
 
 
-def set_extra(config, cls_name):
+def set_extra(config: BaseConfig, cls_name: str) -> None:
     has_ignore_extra, has_allow_extra = hasattr(config, 'ignore_extra'), hasattr(config, 'allow_extra')
     if has_ignore_extra or has_allow_extra:
         if getattr(config, 'allow_extra', False):
@@ -100,10 +123,11 @@ TYPE_BLACKLIST = FunctionType, property, type, classmethod, staticmethod
 
 
 class MetaModel(ABCMeta):
+    @no_type_check
     def __new__(mcs, name, bases, namespace):
-        fields: Dict[name, Field] = {}
+        fields: Dict[str, Field] = {}
         config = BaseConfig
-        validators = {}
+        validators: 'ValidatorListDict' = {}
         for base in reversed(bases):
             if issubclass(base, BaseModel) and base != BaseModel:
                 fields.update(deepcopy(base.__fields__))
@@ -173,22 +197,30 @@ _missing = object()
 
 
 class BaseModel(metaclass=MetaModel):
-    # populated by the metaclass, defined here to help IDEs only
-    __fields__: Dict[str, Field] = {}
-    __validators__ = {}
+    if TYPE_CHECKING:  # pragma: no cover
+        # populated by the metaclass, defined here to help IDEs only
+        __fields__: Dict[str, Field] = {}
+        __validators__: Dict[str, AnyCallable] = {}
+        __config__: Type[BaseConfig] = BaseConfig
+        _json_encoder: Callable[[Any], Any] = lambda x: x
+        _schema_cache: Dict[Any, Any] = {}
 
     Config = BaseConfig
     __slots__ = ('__values__',)
 
-    def __init__(self, **data):
+    def __init__(self, **data: Any) -> None:
+        if TYPE_CHECKING:  # pragma: no cover
+            self.__values__: Dict[str, Any] = {}
         self.__setstate__(self._process_values(data))
 
+    @no_type_check
     def __getattr__(self, name):
         try:
             return self.__values__[name]
         except KeyError:
             raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{name}'")
 
+    @no_type_check
     def __setattr__(self, name, value):
         if self.__config__.extra is not Extra.allow and name not in self.__fields__:
             raise ValueError(f'"{self.__class__.__name__}" object has no field "{name}"')
@@ -203,13 +235,13 @@ class BaseModel(metaclass=MetaModel):
         else:
             self.__values__[name] = value
 
-    def __getstate__(self):
+    def __getstate__(self) -> Dict[Any, Any]:
         return self.__values__
 
-    def __setstate__(self, state):
+    def __setstate__(self, state: Dict[Any, Any]) -> None:
         object.__setattr__(self, '__values__', state)
 
-    def dict(self, *, include: Set[str] = None, exclude: Set[str] = set(), by_alias: bool = False) -> Dict[str, Any]:
+    def dict(self, *, include: Set[str] = None, exclude: Set[str] = set(), by_alias: bool = False) -> 'DictStrAny':
         """
         Generate a dictionary representation of the model, optionally specifying which fields to include or exclude.
         """
@@ -222,7 +254,7 @@ class BaseModel(metaclass=MetaModel):
             if k not in exclude and (not include or k in include)
         }
 
-    def _get_key_factory(self, by_alias: bool) -> Callable:
+    def _get_key_factory(self, by_alias: bool) -> Callable[..., str]:
         if by_alias:
             return lambda fields, key: fields[key].alias
 
@@ -231,25 +263,24 @@ class BaseModel(metaclass=MetaModel):
     def json(
         self,
         *,
-        include: Set[str] = None,
+        include: Set[str] = set(),
         exclude: Set[str] = set(),
         by_alias: bool = False,
-        encoder=None,
-        **dumps_kwargs,
+        encoder: Optional[Callable[[Any], Any]] = None,
+        **dumps_kwargs: Any,
     ) -> str:
         """
         Generate a JSON representation of the model, `include` and `exclude` arguments as per `dict()`.
 
         `encoder` is an optional function to supply as `default` to json.dumps(), other arguments as per `json.dumps()`.
         """
+        encoder = cast(Callable[[Any], Any], encoder or self._json_encoder)
         return json.dumps(
-            self.dict(include=include, exclude=exclude, by_alias=by_alias),
-            default=encoder or self._json_encoder,
-            **dumps_kwargs,
+            self.dict(include=include, exclude=exclude, by_alias=by_alias), default=encoder, **dumps_kwargs
         )
 
     @classmethod
-    def parse_obj(cls, obj):
+    def parse_obj(cls, obj: Dict[Any, Any]) -> 'BaseModel':
         if not isinstance(obj, dict):
             exc = TypeError(f'{cls.__name__} expected dict not {type(obj).__name__}')
             raise ValidationError([ErrorWrapper(exc, loc='__obj__')])
@@ -264,7 +295,7 @@ class BaseModel(metaclass=MetaModel):
         encoding: str = 'utf8',
         proto: Protocol = None,
         allow_pickle: bool = False,
-    ):
+    ) -> 'BaseModel':
         try:
             obj = load_str_bytes(
                 b, proto=proto, content_type=content_type, encoding=encoding, allow_pickle=allow_pickle
@@ -282,12 +313,12 @@ class BaseModel(metaclass=MetaModel):
         encoding: str = 'utf8',
         proto: Protocol = None,
         allow_pickle: bool = False,
-    ):
+    ) -> 'BaseModel':
         obj = load_file(path, proto=proto, content_type=content_type, encoding=encoding, allow_pickle=allow_pickle)
         return cls.parse_obj(obj)
 
     @classmethod
-    def construct(cls, **values):
+    def construct(cls, **values: Any) -> 'BaseModel':
         """
         Creates a new model and set __values__ without any validation, thus values should already be trusted.
         Chances are you don't want to use this method directly.
@@ -297,8 +328,8 @@ class BaseModel(metaclass=MetaModel):
         return m
 
     def copy(
-        self, *, include: Set[str] = None, exclude: Set[str] = None, update: Dict[str, Any] = None, deep: bool = False
-    ):
+        self, *, include: Set[str] = None, exclude: Set[str] = None, update: 'DictStrAny' = None, deep: bool = False
+    ) -> 'BaseModel':
         """
         Duplicate a model, optionally choose which fields to include, exclude and change.
 
@@ -323,11 +354,11 @@ class BaseModel(metaclass=MetaModel):
         return self.__class__.construct(**v)
 
     @property
-    def fields(self):
+    def fields(self) -> Dict[str, Field]:
         return self.__fields__
 
     @classmethod
-    def schema(cls, by_alias=True) -> Dict[str, Any]:
+    def schema(cls, by_alias: bool = True) -> 'DictStrAny':
         cached = cls._schema_cache.get(by_alias)
         if cached is not None:
             return cached
@@ -336,25 +367,26 @@ class BaseModel(metaclass=MetaModel):
         return s
 
     @classmethod
-    def schema_json(cls, *, by_alias=True, **dumps_kwargs) -> str:
+    def schema_json(cls, *, by_alias: bool = True, **dumps_kwargs: Any) -> str:
         from .json import pydantic_encoder
 
         return json.dumps(cls.schema(by_alias=by_alias), default=pydantic_encoder, **dumps_kwargs)
 
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield dict_validator
         yield cls.validate
 
     @classmethod
-    def validate(cls, value):
+    def validate(cls, value: 'DictStrAny') -> 'BaseModel':
         return cls(**value)
 
-    def _process_values(self, input_data: dict) -> Dict[str, Any]:
-        return validate_model(self, input_data)
+    def _process_values(self, input_data: Any) -> 'DictStrAny':
+        # (casting here is slow so use ignore)
+        return validate_model(self, input_data)  # type: ignore
 
     @classmethod
-    def _get_value(cls, v, by_alias=False):
+    def _get_value(cls, v: Any, by_alias: bool) -> Any:
         if isinstance(v, BaseModel):
             return v.dict(by_alias=by_alias)
         elif isinstance(v, list):
@@ -369,7 +401,7 @@ class BaseModel(metaclass=MetaModel):
             return v
 
     @classmethod
-    def update_forward_refs(cls, **localns):
+    def update_forward_refs(cls, **localns: Any) -> None:
         """
         Try to update ForwardRefs on fields based on this Model, globalns and localns.
         """
@@ -377,29 +409,29 @@ class BaseModel(metaclass=MetaModel):
         globalns.setdefault(cls.__name__, cls)
         for f in cls.__fields__.values():
             if type(f.type_) == ForwardRef:
-                f.type_ = f.type_._evaluate(globalns, localns or None)
+                f.type_ = f.type_._evaluate(globalns, localns or None)  # type: ignore
                 f.prepare()
 
-    def __iter__(self):
+    def __iter__(self) -> 'AnyGenerator':
         """
         so `dict(model)` works
         """
         yield from self._iter()
 
-    def _iter(self, by_alias=False):
+    def _iter(self, by_alias: bool = False) -> 'TupleGenerator':
         for k, v in self.__values__.items():
             yield k, self._get_value(v, by_alias=by_alias)
 
-    def __eq__(self, other):
+    def __eq__(self, other: Any) -> bool:
         if isinstance(other, BaseModel):
             return self.dict() == other.dict()
         else:
             return self.dict() == other
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return f'<{self}>'
 
-    def to_string(self, pretty=False):
+    def to_string(self, pretty: bool = False) -> str:
         divider = '\n  ' if pretty else ' '
         return '{}{}{}'.format(
             self.__class__.__name__,
@@ -407,11 +439,13 @@ class BaseModel(metaclass=MetaModel):
             divider.join('{}={}'.format(k, truncate(v)) for k, v in self.__values__.items()),
         )
 
-    def __str__(self):
+    def __str__(self) -> str:
         return self.to_string()
 
 
-def create_model(model_name: str, *, __config__: Type = None, __base__: Type[BaseModel] = None, **field_definitions):
+def create_model(
+    model_name: str, *, __config__: Type[BaseConfig] = None, __base__: Type[BaseModel] = None, **field_definitions: Any
+) -> BaseModel:
     """
     Dynamically create a model.
     :param model_name: name of the created model
@@ -448,15 +482,17 @@ def create_model(model_name: str, *, __config__: Type = None, __base__: Type[Bas
             annotations[f_name] = f_annotation
         fields[f_name] = f_value
 
-    namespace = {'__annotations__': annotations}
+    namespace: 'DictStrAny' = {'__annotations__': annotations}
     namespace.update(fields)
     if __config__:
         namespace['Config'] = inherit_config(__config__, BaseConfig)
 
-    return type(model_name, (__base__,), namespace)
+    return type(model_name, (__base__,), namespace)  # type: ignore
 
 
-def validate_model(model: BaseModel, input_data: dict, raise_exc=True):  # noqa: C901 (ignore complexity)
+def validate_model(  # noqa: C901 (ignore complexity)
+    model: Union[BaseModel, Type[BaseModel]], input_data: 'DictStrAny', raise_exc: bool = True
+) -> Union['DictStrAny', Tuple['DictStrAny', Optional[ValidationError]]]:
     """
     validate data against a model.
     """
@@ -491,7 +527,7 @@ def validate_model(model: BaseModel, input_data: dict, raise_exc=True):  # noqa:
         elif check_extra:
             names_used.add(field.name if using_name else field.alias)
 
-        v_, errors_ = field.validate(value, values, loc=field.alias, cls=model.__class__)
+        v_, errors_ = field.validate(value, values, loc=field.alias, cls=model.__class__)  # type: ignore
         if isinstance(errors_, ErrorWrapper):
             errors.append(errors_)
         elif isinstance(errors_, list):
@@ -503,11 +539,11 @@ def validate_model(model: BaseModel, input_data: dict, raise_exc=True):  # noqa:
         extra = input_data.keys() - names_used
         if extra:
             if config.extra is Extra.allow:
-                for field in extra:
-                    values[field] = input_data[field]
+                for f in extra:
+                    values[f] = input_data[f]
             else:
-                for field in sorted(extra):
-                    errors.append(ErrorWrapper(ExtraError(), loc=field, config=config))
+                for f in sorted(extra):
+                    errors.append(ErrorWrapper(ExtraError(), loc=f, config=config))
 
     if not raise_exc:
         return values, ValidationError(errors) if errors else None
diff --git a/pydantic/parse.py b/pydantic/parse.py
index 30d90b8c3..83bb21d99 100644
--- a/pydantic/parse.py
+++ b/pydantic/parse.py
@@ -8,7 +8,7 @@ from .types import StrBytes
 try:
     import ujson as json
 except ImportError:
-    import json
+    import json  # type: ignore
 
 
 class Protocol(str, Enum):
@@ -36,7 +36,8 @@ def load_str_bytes(
     elif proto == Protocol.pickle:
         if not allow_pickle:
             raise RuntimeError('Trying to decode with pickle with allow_pickle=False')
-        return pickle.loads(b)
+        bb = b if isinstance(b, bytes) else b.encode()
+        return pickle.loads(bb)
     else:
         raise TypeError(f'Unknown protocol: {proto}')
 
diff --git a/pydantic/schema.py b/pydantic/schema.py
index c0bf78990..b9c21eeb9 100644
--- a/pydantic/schema.py
+++ b/pydantic/schema.py
@@ -3,7 +3,7 @@ from datetime import date, datetime, time, timedelta
 from decimal import Decimal
 from enum import Enum
 from pathlib import Path
-from typing import Any, Dict, Sequence, Set, Tuple, Type
+from typing import Any, Callable, Dict, List, Optional, Sequence, Set, Tuple, Type, Union, cast
 from uuid import UUID
 
 from . import main
@@ -102,7 +102,7 @@ class Schema:
 
     def __init__(
         self,
-        default,
+        default: Any,
         *,
         alias: str = None,
         title: str = None,
@@ -115,8 +115,8 @@ class Schema:
         min_length: int = None,
         max_length: int = None,
         regex: str = None,
-        **extra,
-    ):
+        **extra: Any,
+    ) -> None:
         self.default = default
         self.alias = alias
         self.title = title
@@ -131,14 +131,19 @@ class Schema:
         self.max_length = max_length
         self.regex = regex
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         attrs = ((s, getattr(self, s)) for s in self.__slots__)
         return 'Schema({})'.format(', '.join(f'{a}: {v!r}' for a, v in attrs if v is not None))
 
 
 def schema(
-    models: Sequence[Type['main.BaseModel']], *, by_alias=True, title=None, description=None, ref_prefix=None
-) -> Dict:
+    models: Sequence[Type['main.BaseModel']],
+    *,
+    by_alias: bool = True,
+    title: Optional[str] = None,
+    description: Optional[str] = None,
+    ref_prefix: Optional[str] = None,
+) -> Dict[str, Any]:
     """
     Process a list of models and generate a single JSON Schema with all of them defined in the ``definitions``
     top-level JSON key, including their sub-models.
@@ -159,7 +164,7 @@ def schema(
     flat_models = get_flat_models_from_models(models)
     model_name_map = get_model_name_map(flat_models)
     definitions = {}
-    output_schema = {}
+    output_schema: Dict[str, Any] = {}
     if title:
         output_schema['title'] = title
     if description:
@@ -176,7 +181,9 @@ def schema(
     return output_schema
 
 
-def model_schema(model: Type['main.BaseModel'], by_alias=True, ref_prefix=None) -> Dict[str, Any]:
+def model_schema(
+    model: Type['main.BaseModel'], by_alias: bool = True, ref_prefix: Optional[str] = None
+) -> Dict[str, Any]:
     """
     Generate a JSON Schema for one model. With all the sub-models defined in the ``definitions`` top-level
     JSON key.
@@ -202,7 +209,11 @@ def model_schema(model: Type['main.BaseModel'], by_alias=True, ref_prefix=None)
 
 
 def field_schema(
-    field: Field, *, by_alias=True, model_name_map: Dict[Type['main.BaseModel'], str], ref_prefix=None
+    field: Field,
+    *,
+    by_alias: bool = True,
+    model_name_map: Dict[Type['main.BaseModel'], str],
+    ref_prefix: Optional[str] = None,
 ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
     """
     Process a Pydantic field and return a tuple with a JSON Schema for it as the first item.
@@ -219,12 +230,13 @@ def field_schema(
     """
     ref_prefix = ref_prefix or default_prefix
     schema_overrides = False
-    s = dict(title=field.schema.title or field.alias.title())
-    if field.schema.title:
+    schema = cast('Schema', field.schema)
+    s = dict(title=schema.title or field.alias.title())
+    if schema.title:
         schema_overrides = True
 
-    if field.schema.description:
-        s['description'] = field.schema.description
+    if schema.description:
+        s['description'] = schema.description
         schema_overrides = True
 
     if not field.required and field.default is not None:
@@ -252,13 +264,13 @@ def field_schema(
 
 
 numeric_types = (int, float, Decimal)
-_str_types_attrs = (
+_str_types_attrs: Tuple[Tuple[str, Union[type, Tuple[type, ...]], str], ...] = (
     ('max_length', numeric_types, 'maxLength'),
     ('min_length', numeric_types, 'minLength'),
     ('regex', str, 'pattern'),
 )
 
-_numeric_types_attrs = (
+_numeric_types_attrs: Tuple[Tuple[str, Union[type, Tuple[type, ...]], str], ...] = (
     ('gt', numeric_types, 'exclusiveMinimum'),
     ('lt', numeric_types, 'exclusiveMaximum'),
     ('ge', numeric_types, 'minimum'),
@@ -267,12 +279,12 @@ _numeric_types_attrs = (
 )
 
 
-def get_field_schema_validations(field):
+def get_field_schema_validations(field: Field) -> Dict[str, Any]:
     """
     Get the JSON Schema validation keywords for a ``field`` with an annotation of
     a Pydantic ``Schema`` with validation arguments.
     """
-    f_schema = {}
+    f_schema: Dict[str, Any] = {}
     if lenient_issubclass(field.type_, (str, bytes)):
         for attr_name, t, keyword in _str_types_attrs:
             attr = getattr(field.schema, attr_name, None)
@@ -283,8 +295,9 @@ def get_field_schema_validations(field):
             attr = getattr(field.schema, attr_name, None)
             if isinstance(attr, t):
                 f_schema[keyword] = attr
-    if field.schema.extra:
-        f_schema.update(field.schema.extra)
+    schema = cast('Schema', field.schema)
+    if schema.extra:
+        f_schema.update(schema.extra)
     return f_schema
 
 
@@ -299,7 +312,7 @@ def get_model_name_map(unique_models: Set[Type['main.BaseModel']]) -> Dict[Type[
     :return: dict mapping models to names
     """
     name_model_map = {}
-    conflicting_names = set()
+    conflicting_names: Set[str] = set()
     for model in unique_models:
         model_name = model.__name__
         if model_name in conflicting_names:
@@ -325,9 +338,10 @@ def get_flat_models_from_model(model: Type['main.BaseModel']) -> Set[Type['main.
     :param model: a Pydantic ``BaseModel`` subclass
     :return: a set with the initial model and all its sub-models
     """
-    flat_models = set()
+    flat_models: Set[Type['main.BaseModel']] = set()
     flat_models.add(model)
-    flat_models |= get_flat_models_from_fields(model.__fields__.values())
+    fields = cast(Sequence[Field], model.__fields__.values())
+    flat_models |= get_flat_models_from_fields(fields)
     return flat_models
 
 
@@ -342,7 +356,7 @@ def get_flat_models_from_field(field: Field) -> Set[Type['main.BaseModel']]:
     :param field: a Pydantic ``Field``
     :return: a set with the model used in the declaration for this field, if any, and all its sub-models
     """
-    flat_models = set()
+    flat_models: Set[Type['main.BaseModel']] = set()
     if field.sub_fields:
         flat_models |= get_flat_models_from_fields(field.sub_fields)
     elif lenient_issubclass(field.type_, main.BaseModel):
@@ -350,7 +364,7 @@ def get_flat_models_from_field(field: Field) -> Set[Type['main.BaseModel']]:
     return flat_models
 
 
-def get_flat_models_from_fields(fields) -> Set[Type['main.BaseModel']]:
+def get_flat_models_from_fields(fields: Sequence[Field]) -> Set[Type['main.BaseModel']]:
     """
     Take a list of Pydantic  ``Field``s (from a model) that could have been declared as sublcasses of ``BaseModel``
     (so, any of them could be a submodel), and generate a set with their models and all the sub-models in the tree.
@@ -361,7 +375,7 @@ def get_flat_models_from_fields(fields) -> Set[Type['main.BaseModel']]:
     :param fields: a list of Pydantic ``Field``s
     :return: a set with any model declared in the fields, and all their sub-models
     """
-    flat_models = set()
+    flat_models: Set[Type['main.BaseModel']] = set()
     for field in fields:
         flat_models |= get_flat_models_from_field(field)
     return flat_models
@@ -373,13 +387,13 @@ def get_flat_models_from_models(models: Sequence[Type['main.BaseModel']]) -> Set
     a list of two models, ``Foo`` and ``Bar``, both subclasses of Pydantic ``BaseModel`` as models, and ``Bar`` has
     a field of type ``Baz`` (also subclass of ``BaseModel``), the return value will be ``set([Foo, Bar, Baz])``.
     """
-    flat_models = set()
+    flat_models: Set[Type['main.BaseModel']] = set()
     for model in models:
         flat_models |= get_flat_models_from_model(model)
     return flat_models
 
 
-def get_long_model_name(model: Type['main.BaseModel']):
+def get_long_model_name(model: Type['main.BaseModel']) -> str:
     return f'{model.__module__}__{model.__name__}'.replace('.', '__')
 
 
@@ -388,8 +402,8 @@ def field_type_schema(
     *,
     by_alias: bool,
     model_name_map: Dict[Type['main.BaseModel'], str],
-    schema_overrides=False,
-    ref_prefix=None,
+    schema_overrides: bool = False,
+    ref_prefix: Optional[str] = None,
 ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
     """
     Used by ``field_schema()``, you probably should be using that function.
@@ -412,8 +426,9 @@ def field_type_schema(
         definitions.update(f_definitions)
         return {'type': 'array', 'uniqueItems': True, 'items': f_schema}, definitions
     elif field.shape is Shape.MAPPING:
-        dict_schema = {'type': 'object'}
-        regex = getattr(field.key_field.type_, 'regex', None)
+        dict_schema: Dict[str, Any] = {'type': 'object'}
+        key_field = cast(Field, field.key_field)
+        regex = getattr(key_field.type_, 'regex', None)
         f_schema, f_definitions = field_singleton_schema(
             field, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix
         )
@@ -428,14 +443,15 @@ def field_type_schema(
         return dict_schema, definitions
     elif field.shape is Shape.TUPLE:
         sub_schema = []
-        for sf in field.sub_fields:
+        sub_fields = cast(List[Field], field.sub_fields)
+        for sf in sub_fields:
             sf_schema, sf_definitions = field_type_schema(
                 sf, by_alias=by_alias, model_name_map=model_name_map, ref_prefix=ref_prefix
             )
             definitions.update(sf_definitions)
             sub_schema.append(sf_schema)
         if len(sub_schema) == 1:
-            sub_schema = sub_schema[0]
+            sub_schema = sub_schema[0]  # type: ignore
         return {'type': 'array', 'items': sub_schema}, definitions
     else:
         assert field.shape is Shape.SINGLETON, field.shape
@@ -451,7 +467,11 @@ def field_type_schema(
 
 
 def model_process_schema(
-    model: Type['main.BaseModel'], *, by_alias=True, model_name_map: Dict[Type['main.BaseModel'], str], ref_prefix=None
+    model: Type['main.BaseModel'],
+    *,
+    by_alias: bool = True,
+    model_name_map: Dict[Type['main.BaseModel'], str],
+    ref_prefix: Optional[str] = None,
 ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
     """
     Used by ``model_schema()``, you probably should be using that function.
@@ -472,8 +492,12 @@ def model_process_schema(
 
 
 def model_type_schema(
-    model: Type['main.BaseModel'], *, by_alias: bool, model_name_map: Dict[Type['main.BaseModel'], str], ref_prefix=None
-):
+    model: Type['main.BaseModel'],
+    *,
+    by_alias: bool,
+    model_name_map: Dict[Type['main.BaseModel'], str],
+    ref_prefix: Optional[str] = None,
+) -> Tuple[Dict[str, Any], Dict[str, Any]]:
     """
     You probably should be using ``model_schema()``, this function is indirectly used by that function.
 
@@ -483,7 +507,7 @@ def model_type_schema(
     ref_prefix = ref_prefix or default_prefix
     properties = {}
     required = []
-    definitions = {}
+    definitions: Dict[str, Any] = {}
     for k, f in model.__fields__.items():
         try:
             f_schema, f_definitions = field_schema(
@@ -512,8 +536,8 @@ def field_singleton_sub_fields_schema(
     *,
     by_alias: bool,
     model_name_map: Dict[Type['main.BaseModel'], str],
-    schema_overrides=False,
-    ref_prefix=None,
+    schema_overrides: bool = False,
+    ref_prefix: Optional[str] = None,
 ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
     """
     This function is indirectly used by ``field_schema()``, you probably should be using that function.
@@ -559,7 +583,7 @@ validation_attribute_to_schema_keyword = {
 }
 
 # Order is important, subclasses of str must go before str, etc
-field_class_to_schema_enum_enabled = (
+field_class_to_schema_enum_enabled: Tuple[Tuple[Any, Dict[str, Any]], ...] = (
     (EmailStr, {'type': 'string', 'format': 'email'}),
     (UrlStr, {'type': 'string', 'format': 'uri'}),
     (DSN, {'type': 'string', 'format': 'dsn'}),
@@ -600,8 +624,8 @@ def field_singleton_schema(  # noqa: C901 (ignore complexity)
     *,
     by_alias: bool,
     model_name_map: Dict[Type['main.BaseModel'], str],
-    schema_overrides=False,
-    ref_prefix=None,
+    schema_overrides: bool = False,
+    ref_prefix: Optional[str] = None,
 ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
     """
     This function is indirectly used by ``field_schema()``, you should probably be using that function.
@@ -610,7 +634,7 @@ def field_singleton_schema(  # noqa: C901 (ignore complexity)
     """
 
     ref_prefix = ref_prefix or default_prefix
-    definitions = {}
+    definitions: Dict[str, Any] = {}
     if field.sub_fields:
         return field_singleton_sub_fields_schema(
             field.sub_fields,
@@ -623,9 +647,9 @@ def field_singleton_schema(  # noqa: C901 (ignore complexity)
         return {}, definitions  # no restrictions
     if is_callable_type(field.type_):
         raise SkipField(f'Callable {field.name} was excluded from schema since JSON schema has no equivalent type.')
-    f_schema = {}
+    f_schema: Dict[str, Any] = {}
     if issubclass(field.type_, Enum):
-        f_schema.update({'enum': [item.value for item in field.type_]})
+        f_schema.update({'enum': [item.value for item in field.type_]})  # type: ignore
         # Don't return immediately, to allow adding specific types
     for field_name, schema_name in validation_attribute_to_schema_keyword.items():
         field_value = getattr(field.type_, field_name, None)
@@ -657,7 +681,7 @@ def field_singleton_schema(  # noqa: C901 (ignore complexity)
     raise ValueError(f'Value not declarable with JSON Schema, field: {field}')
 
 
-def encode_default(dft):
+def encode_default(dft: Any) -> Any:
     if isinstance(dft, (int, float, str)):
         return dft
     elif isinstance(dft, (tuple, list, set)):
@@ -669,10 +693,10 @@ def encode_default(dft):
         return pydantic_encoder(dft)
 
 
-_map_types_constraint = {int: conint, float: confloat, Decimal: condecimal}
+_map_types_constraint: Dict[Any, Callable[..., type]] = {int: conint, float: confloat, Decimal: condecimal}
 
 
-def get_annotation_from_schema(annotation, schema):
+def get_annotation_from_schema(annotation: Any, schema: Schema) -> Type[Any]:
     """
     Get an annotation with validation implemented for numbers and strings based on the schema.
 
@@ -681,7 +705,8 @@ def get_annotation_from_schema(annotation, schema):
     :return: the same ``annotation`` if unmodified or a new annotation with validation in place
     """
     if isinstance(annotation, type):
-        attrs = constraint_func = None
+        attrs: Optional[Tuple[str, ...]] = None
+        constraint_func: Optional[Callable[..., type]] = None
         if issubclass(annotation, str) and not issubclass(annotation, (EmailStr, DSN, UrlStr, ConstrainedStr)):
             attrs = ('max_length', 'min_length', 'regex')
             constraint_func = constr
@@ -700,6 +725,7 @@ def get_annotation_from_schema(annotation, schema):
                 if attr is not None
             }
             if kwargs:
+                constraint_func = cast(Callable[..., type], constraint_func)
                 return constraint_func(**kwargs)
     return annotation
 
@@ -709,5 +735,5 @@ class SkipField(Exception):
     Utility exception used to exclude fields from schema.
     """
 
-    def __init__(self, message):
+    def __init__(self, message: str) -> None:
         self.message = message
diff --git a/pydantic/types.py b/pydantic/types.py
index ee9b248af..f6ca5003a 100644
--- a/pydantic/types.py
+++ b/pydantic/types.py
@@ -2,11 +2,11 @@ import json
 import re
 from decimal import Decimal
 from pathlib import Path
-from typing import Optional, Pattern, Set, Type, Union
+from typing import TYPE_CHECKING, Any, Dict, Generator, Optional, Pattern, Set, Type, Union, cast
 from uuid import UUID
 
 from . import errors
-from .utils import import_string, make_dsn, url_regex_generator, validate_email
+from .utils import AnyType, import_string, make_dsn, url_regex_generator, validate_email
 from .validators import (
     anystr_length_validator,
     anystr_strip_whitespace,
@@ -67,15 +67,23 @@ NoneStr = Optional[str]
 NoneBytes = Optional[bytes]
 StrBytes = Union[str, bytes]
 NoneStrBytes = Optional[StrBytes]
+OptionalInt = Optional[int]
+OptionalIntFloat = Union[OptionalInt, float]
+OptionalIntFloatDecimal = Union[OptionalIntFloat, Decimal]
+
+if TYPE_CHECKING:  # pragma: no cover
+    from .utils import AnyCallable
+
+    CallableGenerator = Generator[AnyCallable, None, None]
 
 
 class StrictStr(str):
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield cls.validate
 
     @classmethod
-    def validate(cls, v):
+    def validate(cls, v: Any) -> str:
         if not isinstance(v, str):
             raise errors.StrError()
         return v
@@ -83,18 +91,18 @@ class StrictStr(str):
 
 class ConstrainedBytes(bytes):
     strip_whitespace = False
-    min_length: Optional[int] = None
-    max_length: Optional[int] = None
+    min_length: OptionalInt = None
+    max_length: OptionalInt = None
 
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield not_none_validator
         yield bytes_validator
         yield anystr_strip_whitespace
         yield anystr_length_validator
 
 
-def conbytes(*, strip_whitespace=False, min_length=None, max_length=None) -> Type[bytes]:
+def conbytes(*, strip_whitespace: bool = False, min_length: int = None, max_length: int = None) -> Type[bytes]:
     # use kwargs then define conf in a dict to aid with IDE type hinting
     namespace = dict(strip_whitespace=strip_whitespace, min_length=min_length, max_length=max_length)
     return type('ConstrainedBytesValue', (ConstrainedBytes,), namespace)
@@ -102,13 +110,13 @@ def conbytes(*, strip_whitespace=False, min_length=None, max_length=None) -> Typ
 
 class ConstrainedStr(str):
     strip_whitespace = False
-    min_length: Optional[int] = None
-    max_length: Optional[int] = None
-    curtail_length: Optional[int] = None
-    regex: Optional[Pattern] = None
+    min_length: OptionalInt = None
+    max_length: OptionalInt = None
+    curtail_length: OptionalInt = None
+    regex: Optional[Pattern[str]] = None
 
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield not_none_validator
         yield str_validator
         yield anystr_strip_whitespace
@@ -127,7 +135,14 @@ class ConstrainedStr(str):
         return value
 
 
-def constr(*, strip_whitespace=False, min_length=None, max_length=None, curtail_length=None, regex=None) -> Type[str]:
+def constr(
+    *,
+    strip_whitespace: bool = False,
+    min_length: int = None,
+    max_length: int = None,
+    curtail_length: int = None,
+    regex: str = None,
+) -> Type[str]:
     # use kwargs then define conf in a dict to aid with IDE type hinting
     namespace = dict(
         strip_whitespace=strip_whitespace,
@@ -141,7 +156,7 @@ def constr(*, strip_whitespace=False, min_length=None, max_length=None, curtail_
 
 class EmailStr(str):
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         # included here and below so the error happens straight away
         if email_validator is None:
             raise ImportError('email-validator is not installed, run `pip install pydantic[email]`')
@@ -150,7 +165,7 @@ class EmailStr(str):
         yield cls.validate
 
     @classmethod
-    def validate(cls, value):
+    def validate(cls, value: str) -> str:
         return validate_email(value)[1]
 
 
@@ -163,7 +178,7 @@ class UrlStr(str):
     require_tld = True  # whether to reject non-FQDN hostnames
 
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield not_none_validator
         yield str_validator
         yield anystr_strip_whitespace
@@ -188,11 +203,11 @@ class UrlStr(str):
 
 def urlstr(
     *,
-    strip_whitespace=True,
-    min_length=1,
-    max_length=2 ** 16,
-    relative=False,
-    require_tld=True,
+    strip_whitespace: bool = True,
+    min_length: int = 1,
+    max_length: int = 2 ** 16,
+    relative: bool = False,
+    require_tld: bool = True,
     schemes: Optional[Set[str]] = None,
 ) -> Type[str]:
     # use kwargs then define conf in a dict to aid with IDE type hinting
@@ -210,12 +225,12 @@ def urlstr(
 class NameEmail:
     __slots__ = 'name', 'email'
 
-    def __init__(self, name, email):
+    def __init__(self, name: str, email: str):
         self.name = name
         self.email = email
 
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         if email_validator is None:
             raise ImportError('email-validator is not installed, run `pip install pydantic[email]`')
 
@@ -223,13 +238,13 @@ class NameEmail:
         yield cls.validate
 
     @classmethod
-    def validate(cls, value):
+    def validate(cls, value: str) -> 'NameEmail':
         return cls(*validate_email(value))
 
-    def __str__(self):
+    def __str__(self) -> str:
         return f'{self.name} <{self.email}>'
 
-    def __repr__(self):
+    def __repr__(self) -> str:
         return f'<NameEmail("{self}")>'
 
 
@@ -237,12 +252,12 @@ class PyObject:
     validate_always = True
 
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield str_validator
         yield cls.validate
 
     @classmethod
-    def validate(cls, value):
+    def validate(cls, value: Any) -> Any:
         if value is not None:
             try:
                 return import_string(value)
@@ -256,12 +271,12 @@ class DSN(str):
     validate_always = True
 
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield str_validator
         yield cls.validate
 
     @classmethod
-    def validate(cls, value, values, **kwarg):
+    def validate(cls, value: str, values: Dict[str, Any], **kwarg: Any) -> str:
         if value:
             return value
 
@@ -269,12 +284,12 @@ class DSN(str):
         if kwargs['driver'] is None:
             raise errors.DSNDriverIsEmptyError()
 
-        return make_dsn(**kwargs)
+        return make_dsn(**kwargs)  # type: ignore
 
 
 class ConstrainedNumberMeta(type):
-    def __new__(cls, name, bases, dct):
-        new_cls = type.__new__(cls, name, bases, dct)
+    def __new__(cls, name: str, bases: Any, dct: Dict[str, Any]) -> 'ConstrainedInt':
+        new_cls = cast('ConstrainedInt', type.__new__(cls, name, bases, dct))
 
         if new_cls.gt is not None and new_cls.ge is not None:
             raise errors.ConfigError('bounds gt and ge cannot be specified at the same time')
@@ -285,20 +300,20 @@ class ConstrainedNumberMeta(type):
 
 
 class ConstrainedInt(int, metaclass=ConstrainedNumberMeta):
-    gt: Optional[int] = None
-    ge: Optional[int] = None
-    lt: Optional[int] = None
-    le: Optional[int] = None
-    multiple_of: Optional[Union[int, float]] = None
+    gt: OptionalInt = None
+    ge: OptionalInt = None
+    lt: OptionalInt = None
+    le: OptionalInt = None
+    multiple_of: OptionalInt = None
 
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield int_validator
         yield number_size_validator
         yield number_multiple_validator
 
 
-def conint(*, gt=None, ge=None, lt=None, le=None, multiple_of=None) -> Type[int]:
+def conint(*, gt: int = None, ge: int = None, lt: int = None, le: int = None, multiple_of: int = None) -> Type[int]:
     # use kwargs then define conf in a dict to aid with IDE type hinting
     namespace = dict(gt=gt, ge=ge, lt=lt, le=le, multiple_of=multiple_of)
     return type('ConstrainedIntValue', (ConstrainedInt,), namespace)
@@ -313,20 +328,22 @@ class NegativeInt(ConstrainedInt):
 
 
 class ConstrainedFloat(float, metaclass=ConstrainedNumberMeta):
-    gt: Union[None, int, float] = None
-    ge: Union[None, int, float] = None
-    lt: Union[None, int, float] = None
-    le: Union[None, int, float] = None
-    multiple_of: Optional[Union[int, float]] = None
+    gt: OptionalIntFloat = None
+    ge: OptionalIntFloat = None
+    lt: OptionalIntFloat = None
+    le: OptionalIntFloat = None
+    multiple_of: OptionalIntFloat = None
 
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield float_validator
         yield number_size_validator
         yield number_multiple_validator
 
 
-def confloat(*, gt=None, ge=None, lt=None, le=None, multiple_of=None) -> Type[float]:
+def confloat(
+    *, gt: float = None, ge: float = None, lt: float = None, le: float = None, multiple_of: float = None
+) -> Type[float]:
     # use kwargs then define conf in a dict to aid with IDE type hinting
     namespace = dict(gt=gt, ge=ge, lt=lt, le=le, multiple_of=multiple_of)
     return type('ConstrainedFloatValue', (ConstrainedFloat,), namespace)
@@ -341,16 +358,16 @@ class NegativeFloat(ConstrainedFloat):
 
 
 class ConstrainedDecimal(Decimal, metaclass=ConstrainedNumberMeta):
-    gt: Union[None, int, float, Decimal] = None
-    ge: Union[None, int, float, Decimal] = None
-    lt: Union[None, int, float, Decimal] = None
-    le: Union[None, int, float, Decimal] = None
-    max_digits: Optional[int] = None
-    decimal_places: Optional[int] = None
-    multiple_of: Optional[Union[int, float]] = None
+    gt: OptionalIntFloatDecimal = None
+    ge: OptionalIntFloatDecimal = None
+    lt: OptionalIntFloatDecimal = None
+    le: OptionalIntFloatDecimal = None
+    max_digits: OptionalInt = None
+    decimal_places: OptionalInt = None
+    multiple_of: OptionalIntFloatDecimal = None
 
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield not_none_validator
         yield decimal_validator
         yield number_size_validator
@@ -395,7 +412,14 @@ class ConstrainedDecimal(Decimal, metaclass=ConstrainedNumberMeta):
 
 
 def condecimal(
-    *, gt=None, ge=None, lt=None, le=None, max_digits=None, decimal_places=None, multiple_of=None
+    *,
+    gt: Decimal = None,
+    ge: Decimal = None,
+    lt: Decimal = None,
+    le: Decimal = None,
+    max_digits: int = None,
+    decimal_places: int = None,
+    multiple_of: Decimal = None,
 ) -> Type[Decimal]:
     # use kwargs then define conf in a dict to aid with IDE type hinting
     namespace = dict(
@@ -422,7 +446,7 @@ class UUID5(UUID):
 
 class FilePath(Path):
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield path_validator
         yield path_exists_validator
         yield cls.validate
@@ -437,7 +461,7 @@ class FilePath(Path):
 
 class DirectoryPath(Path):
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield path_validator
         yield path_exists_validator
         yield cls.validate
@@ -451,22 +475,22 @@ class DirectoryPath(Path):
 
 
 class JsonWrapper:
-    __slots__ = ('inner_type',)
+    pass
 
 
 class JsonMeta(type):
-    def __getitem__(self, t):
+    def __getitem__(self, t: AnyType) -> Type[JsonWrapper]:
         return type('JsonWrapperValue', (JsonWrapper,), {'inner_type': t})
 
 
 class Json(metaclass=JsonMeta):
     @classmethod
-    def __get_validators__(cls):
+    def __get_validators__(cls) -> 'CallableGenerator':
         yield str_validator
         yield cls.validate
 
     @classmethod
-    def validate(cls, v: str):
+    def validate(cls, v: str) -> Any:
         try:
             return json.loads(v)
         except ValueError:
diff --git a/pydantic/utils.py b/pydantic/utils.py
index a6a80d67a..be93f7c56 100644
--- a/pydantic/utils.py
+++ b/pydantic/utils.py
@@ -6,7 +6,8 @@ from enum import Enum
 from functools import lru_cache
 from importlib import import_module
 from textwrap import dedent
-from typing import List, Pattern, Tuple, Type, _eval_type
+from typing import _eval_type  # type: ignore
+from typing import TYPE_CHECKING, Any, Dict, Generator, List, Optional, Pattern, Tuple, Type, Union
 
 from . import errors
 
@@ -16,25 +17,35 @@ except ImportError:
     email_validator = None
 
 try:
-    from typing import _TypingBase as typing_base
+    from typing import _TypingBase as typing_base  # type: ignore
 except ImportError:
-    from typing import _Final as typing_base
+    from typing import _Final as typing_base  # type: ignore
 
 try:
-    from typing import ForwardRef
+    from typing import ForwardRef  # type: ignore
 except ImportError:
     # python 3.6
     ForwardRef = None
 
+if TYPE_CHECKING:  # pragma: no cover
+    from .main import BaseModel  # noqa: F401
+
 if sys.version_info < (3, 7):
     from typing import Callable
+
+    AnyCallable = Callable[..., Any]
 else:
     from collections.abc import Callable
+    from typing import Callable as TypingCallable
+
+    AnyCallable = TypingCallable[..., Any]
+
 
 PRETTY_REGEX = re.compile(r'([\w ]*?) *<(.*)> *')
+AnyType = Type[Any]
 
 
-def validate_email(value) -> Tuple[str, str]:
+def validate_email(value: str) -> Tuple[str, str]:
     """
     Brutally simple email address validation. Note unlike most email address validation
     * raw ip address (literal) domain parts are not allowed.
@@ -49,10 +60,9 @@ def validate_email(value) -> Tuple[str, str]:
         raise ImportError('email-validator is not installed, run `pip install pydantic[email]`')
 
     m = PRETTY_REGEX.fullmatch(value)
+    name: Optional[str] = None
     if m:
         name, value = m.groups()
-    else:
-        name = None
 
     email = value.strip()
 
@@ -64,7 +74,7 @@ def validate_email(value) -> Tuple[str, str]:
     return name or email[: email.index('@')], email.lower()
 
 
-def _rfc_1738_quote(text):
+def _rfc_1738_quote(text: str) -> str:
     return re.sub(r'[:@/]', lambda m: '%{:X}'.format(ord(m.group(0))), text)
 
 
@@ -76,8 +86,8 @@ def make_dsn(
     host: str = None,
     port: str = None,
     name: str = None,
-    query: str = None,
-):
+    query: Dict[str, Any] = None,
+) -> str:
     """
     Create a DSN from from connection settings.
 
@@ -106,7 +116,7 @@ def make_dsn(
     return s
 
 
-def import_string(dotted_path):
+def import_string(dotted_path: str) -> Any:
     """
     Stolen approximately from django. Import a dotted module path and return the attribute/class designated by the
     last name in the path. Raise ImportError if the import fails.
@@ -123,7 +133,7 @@ def import_string(dotted_path):
         raise ImportError(f'Module "{module_path}" does not define a "{class_name}" attribute') from e
 
 
-def truncate(v, *, max_len=80):
+def truncate(v: str, *, max_len: int = 80) -> str:
     """
     Truncate a value and add a unicode ellipsis (three dots) to the end if it was too long
     """
@@ -136,7 +146,7 @@ def truncate(v, *, max_len=80):
     return v
 
 
-def display_as_type(v):
+def display_as_type(v: AnyType) -> str:
     if not isinstance(v, typing_base) and not isinstance(v, type):
         v = type(v)
 
@@ -155,19 +165,22 @@ def display_as_type(v):
         return str(v)
 
 
+ExcType = Type[Exception]
+
+
 @contextmanager
-def change_exception(raise_exc, *except_types):
+def change_exception(raise_exc: ExcType, *except_types: ExcType) -> Generator[None, None, None]:
     try:
         yield
     except except_types as e:
         raise raise_exc from e
 
 
-def clean_docstring(d):
+def clean_docstring(d: str) -> str:
     return dedent(d).strip(' \r\n\t')
 
 
-def list_like(v):
+def list_like(v: AnyType) -> bool:
     return isinstance(v, (list, tuple, set)) or inspect.isgenerator(v)
 
 
@@ -184,7 +197,7 @@ def validate_field_name(bases: List[Type['BaseModel']], field_name: str) -> None
 
 
 @lru_cache(maxsize=None)
-def url_regex_generator(*, relative: bool, require_tld: bool) -> Pattern:
+def url_regex_generator(*, relative: bool, require_tld: bool) -> Pattern[str]:
     """
     Url regex generator taken from Marshmallow library,
     for details please follow library source code:
@@ -214,30 +227,30 @@ def url_regex_generator(*, relative: bool, require_tld: bool) -> Pattern:
     )
 
 
-def lenient_issubclass(cls, class_or_tuple):
+def lenient_issubclass(cls: Any, class_or_tuple: Union[AnyType, Tuple[AnyType, ...]]) -> bool:
     return isinstance(cls, type) and issubclass(cls, class_or_tuple)
 
 
-def in_ipython():
+def in_ipython() -> bool:
     """
     Check whether we're in an ipython environment, including jupyter notebooks.
     """
     try:
-        __IPYTHON__
+        __IPYTHON__  # type: ignore
     except NameError:
         return False
     else:  # pragma: no cover
         return True
 
 
-def resolve_annotations(raw_annotations, module):
+def resolve_annotations(raw_annotations: Dict[str, AnyType], module_name: Optional[str]) -> Dict[str, AnyType]:
     """
     Partially taken from typing.get_type_hints.
 
     Resolve string or ForwardRef annotations into type objects if possible.
     """
-    if module:
-        base_globals = sys.modules[module].__dict__
+    if module_name:
+        base_globals: Optional[Dict[str, Any]] = sys.modules[module_name].__dict__
     else:
         base_globals = None
     annotations = {}
@@ -253,5 +266,5 @@ def resolve_annotations(raw_annotations, module):
     return annotations
 
 
-def is_callable_type(type_):
+def is_callable_type(type_: AnyType) -> bool:
     return type_ is Callable or getattr(type_, '__origin__', None) is Callable
diff --git a/pydantic/validators.py b/pydantic/validators.py
index d619ab390..17623328b 100644
--- a/pydantic/validators.py
+++ b/pydantic/validators.py
@@ -4,29 +4,39 @@ from datetime import date, datetime, time, timedelta
 from decimal import Decimal, DecimalException
 from enum import Enum
 from pathlib import Path
-from typing import Any, Callable, Pattern
+from typing import TYPE_CHECKING, Any, Callable, Dict, List, Optional, Pattern, Set, Tuple, Type, TypeVar, Union, cast
 from uuid import UUID
 
 from . import errors
 from .datetime_parse import parse_date, parse_datetime, parse_duration, parse_time
-from .utils import change_exception, display_as_type, is_callable_type, list_like
+from .utils import AnyCallable, AnyType, change_exception, display_as_type, is_callable_type, list_like
+
+if TYPE_CHECKING:  # pragma: no cover
+    from .fields import Field
+    from .main import BaseConfig
+    from .types import ConstrainedDecimal, ConstrainedFloat, ConstrainedInt
+
+    ConstrainedNumber = Union[ConstrainedDecimal, ConstrainedFloat, ConstrainedInt]
+    AnyOrderedDict = OrderedDict[Any, Any]
+    Number = Union[int, float, Decimal]
+    StrBytes = Union[str, bytes]
 
 NoneType = type(None)
 
 
-def not_none_validator(v):
+def not_none_validator(v: Any) -> Any:
     if v is None:
         raise errors.NoneIsNotAllowedError()
     return v
 
 
-def is_none_validator(v):
+def is_none_validator(v: Any) -> None:
     if v is not None:
         raise errors.NoneIsAllowedError()
 
 
-def str_validator(v) -> str:
-    if isinstance(v, (str, NoneType)):
+def str_validator(v: Any) -> str:
+    if isinstance(v, (str, NoneType)):  # type: ignore
         return v
     elif isinstance(v, (bytes, bytearray)):
         return v.decode()
@@ -37,7 +47,7 @@ def str_validator(v) -> str:
         raise errors.StrError()
 
 
-def bytes_validator(v) -> bytes:
+def bytes_validator(v: Any) -> bytes:
     if isinstance(v, bytes):
         return v
     elif isinstance(v, bytearray):
@@ -53,7 +63,7 @@ def bytes_validator(v) -> bytes:
 BOOL_STRINGS = {'1', 'TRUE', 'ON', 'YES'}
 
 
-def bool_validator(v) -> bool:
+def bool_validator(v: Any) -> bool:
     if isinstance(v, bool):
         return v
     if isinstance(v, bytes):
@@ -63,7 +73,7 @@ def bool_validator(v) -> bool:
     return bool(v)
 
 
-def int_validator(v) -> int:
+def int_validator(v: Any) -> int:
     if not isinstance(v, bool) and isinstance(v, int):
         return v
 
@@ -71,7 +81,7 @@ def int_validator(v) -> int:
         return int(v)
 
 
-def float_validator(v) -> float:
+def float_validator(v: Any) -> float:
     if isinstance(v, float):
         return v
 
@@ -79,28 +89,30 @@ def float_validator(v) -> float:
         return float(v)
 
 
-def number_multiple_validator(v, field, config, **kwargs):
-    if field.type_.multiple_of is not None and v % field.type_.multiple_of != 0:
-        raise errors.NumberNotMultipleError(multiple_of=field.type_.multiple_of)
+def number_multiple_validator(v: 'Number', field: 'Field', config: 'BaseConfig', **kwargs: Any) -> 'Number':
+    field_type = cast('ConstrainedNumber', field.type_)
+    if field_type.multiple_of is not None and v % field_type.multiple_of != 0:  # type: ignore
+        raise errors.NumberNotMultipleError(multiple_of=field_type.multiple_of)
 
     return v
 
 
-def number_size_validator(v, field, config, **kwargs):
-    if field.type_.gt is not None and not v > field.type_.gt:
-        raise errors.NumberNotGtError(limit_value=field.type_.gt)
-    elif field.type_.ge is not None and not v >= field.type_.ge:
-        raise errors.NumberNotGeError(limit_value=field.type_.ge)
+def number_size_validator(v: 'Number', field: 'Field', config: 'BaseConfig', **kwargs: Any) -> 'Number':
+    field_type = cast('ConstrainedNumber', field.type_)
+    if field_type.gt is not None and not v > field_type.gt:
+        raise errors.NumberNotGtError(limit_value=field_type.gt)
+    elif field_type.ge is not None and not v >= field_type.ge:
+        raise errors.NumberNotGeError(limit_value=field_type.ge)
 
-    if field.type_.lt is not None and not v < field.type_.lt:
-        raise errors.NumberNotLtError(limit_value=field.type_.lt)
-    if field.type_.le is not None and not v <= field.type_.le:
-        raise errors.NumberNotLeError(limit_value=field.type_.le)
+    if field_type.lt is not None and not v < field_type.lt:
+        raise errors.NumberNotLtError(limit_value=field_type.lt)
+    if field_type.le is not None and not v <= field_type.le:
+        raise errors.NumberNotLeError(limit_value=field_type.le)
 
     return v
 
 
-def anystr_length_validator(v, field, config, **kwargs):
+def anystr_length_validator(v: 'StrBytes', field: 'Field', config: 'BaseConfig', **kwargs: Any) -> 'StrBytes':
     v_len = len(v)
 
     min_length = getattr(field.type_, 'min_length', config.min_anystr_length)
@@ -114,7 +126,7 @@ def anystr_length_validator(v, field, config, **kwargs):
     return v
 
 
-def anystr_strip_whitespace(v, field, config, **kwargs):
+def anystr_strip_whitespace(v: 'StrBytes', field: 'Field', config: 'BaseConfig', **kwargs: Any) -> 'StrBytes':
     strip_whitespace = getattr(field.type_, 'strip_whitespace', config.anystr_strip_whitespace)
     if strip_whitespace:
         v = v.strip()
@@ -122,7 +134,7 @@ def anystr_strip_whitespace(v, field, config, **kwargs):
     return v
 
 
-def ordered_dict_validator(v) -> OrderedDict:
+def ordered_dict_validator(v: Any) -> 'AnyOrderedDict':
     if isinstance(v, OrderedDict):
         return v
 
@@ -130,7 +142,7 @@ def ordered_dict_validator(v) -> OrderedDict:
         return OrderedDict(v)
 
 
-def dict_validator(v) -> dict:
+def dict_validator(v: Any) -> Dict[Any, Any]:
     if isinstance(v, dict):
         return v
 
@@ -138,7 +150,7 @@ def dict_validator(v) -> dict:
         return dict(v)
 
 
-def list_validator(v) -> list:
+def list_validator(v: Any) -> List[Any]:
     if isinstance(v, list):
         return v
     elif list_like(v):
@@ -147,7 +159,7 @@ def list_validator(v) -> list:
         raise errors.ListError()
 
 
-def tuple_validator(v) -> tuple:
+def tuple_validator(v: Any) -> Tuple[Any, ...]:
     if isinstance(v, tuple):
         return v
     elif list_like(v):
@@ -156,7 +168,7 @@ def tuple_validator(v) -> tuple:
         raise errors.TupleError()
 
 
-def set_validator(v) -> set:
+def set_validator(v: Any) -> Set[Any]:
     if isinstance(v, set):
         return v
     elif list_like(v):
@@ -165,14 +177,14 @@ def set_validator(v) -> set:
         raise errors.SetError()
 
 
-def enum_validator(v, field, config, **kwargs) -> Enum:
+def enum_validator(v: Any, field: 'Field', config: 'BaseConfig', **kwargs: Any) -> Enum:
     with change_exception(errors.EnumError, ValueError):
         enum_v = field.type_(v)
 
     return enum_v.value if config.use_enum_values else enum_v
 
 
-def uuid_validator(v, field, config, **kwargs) -> UUID:
+def uuid_validator(v: Any, field: 'Field', config: 'BaseConfig', **kwargs: Any) -> UUID:
     with change_exception(errors.UUIDError, ValueError):
         if isinstance(v, str):
             v = UUID(v)
@@ -189,7 +201,7 @@ def uuid_validator(v, field, config, **kwargs) -> UUID:
     return v
 
 
-def decimal_validator(v) -> Decimal:
+def decimal_validator(v: Any) -> Decimal:
     if isinstance(v, Decimal):
         return v
     elif isinstance(v, (bytes, bytearray)):
@@ -206,7 +218,7 @@ def decimal_validator(v) -> Decimal:
     return v
 
 
-def path_validator(v) -> Path:
+def path_validator(v: Any) -> Path:
     if isinstance(v, Path):
         return v
 
@@ -214,14 +226,14 @@ def path_validator(v) -> Path:
         return Path(v)
 
 
-def path_exists_validator(v) -> Path:
+def path_exists_validator(v: Any) -> Path:
     if not v.exists():
         raise errors.PathNotExistsError(path=v)
 
     return v
 
 
-def callable_validator(v) -> Callable:
+def callable_validator(v: Any) -> AnyCallable:
     """
     Perform a simple check if the value is callable.
 
@@ -233,8 +245,11 @@ def callable_validator(v) -> Callable:
     raise errors.CallableError(value=v)
 
 
-def make_arbitrary_type_validator(type_):
-    def arbitrary_type_validator(v) -> type_:
+T = TypeVar('T')
+
+
+def make_arbitrary_type_validator(type_: Type[T]) -> Callable[[T], T]:
+    def arbitrary_type_validator(v: Any) -> T:
         if isinstance(v, type_):
             return v
         raise errors.ArbitraryTypeError(expected_arbitrary_type=type_)
@@ -242,21 +257,21 @@ def make_arbitrary_type_validator(type_):
     return arbitrary_type_validator
 
 
-def pattern_validator(v) -> Pattern:
+def pattern_validator(v: Any) -> Pattern[str]:
     with change_exception(errors.PatternError, re.error):
         return re.compile(v)
 
 
 pattern_validators = [not_none_validator, str_validator, pattern_validator]
 # order is important here, for example: bool is a subclass of int so has to come first, datetime before date same
-_VALIDATORS = [
+_VALIDATORS: List[Tuple[AnyType, List[AnyCallable]]] = [
     (Enum, [enum_validator]),
     (str, [not_none_validator, str_validator, anystr_strip_whitespace, anystr_length_validator]),
     (bytes, [not_none_validator, bytes_validator, anystr_strip_whitespace, anystr_length_validator]),
     (bool, [bool_validator]),
     (int, [int_validator]),
     (float, [float_validator]),
-    (NoneType, [is_none_validator]),
+    (NoneType, [is_none_validator]),  # type: ignore
     (Path, [path_validator]),
     (datetime, [parse_datetime]),
     (date, [parse_date]),
@@ -272,7 +287,7 @@ _VALIDATORS = [
 ]
 
 
-def find_validators(type_, arbitrary_types_allowed=False):
+def find_validators(type_: AnyType, arbitrary_types_allowed: bool = False) -> List[AnyCallable]:
     if type_ is Any:
         return []
     if type_ is Pattern:
@@ -297,7 +312,7 @@ def find_validators(type_, arbitrary_types_allowed=False):
         raise RuntimeError(f'no validator found for {type_}')
 
 
-def _find_supertype(type_):
+def _find_supertype(type_: AnyType) -> Optional[AnyType]:
     if not _is_new_type(type_):
         return None
 
@@ -308,5 +323,5 @@ def _find_supertype(type_):
     return supertype
 
 
-def _is_new_type(type_):
+def _is_new_type(type_: AnyType) -> bool:
     return hasattr(type_, '__name__') and hasattr(type_, '__supertype__')
diff --git a/setup.cfg b/setup.cfg
index dff33afd1..1fa59d683 100644
--- a/setup.cfg
+++ b/setup.cfg
@@ -30,3 +30,17 @@ multi_line_output=3
 include_trailing_comma=True
 force_grid_wrap=0
 combine_as_imports=True
+
+[mypy]
+follow_imports = silent
+strict_optional = True
+warn_redundant_casts = True
+warn_unused_ignores = True
+disallow_any_generics = True
+check_untyped_defs = True
+
+# for strict mypy: (this is the tricky one :-))
+disallow_untyped_defs = True
+
+[mypy-email_validator]
+ignore_missing_imports = true
diff --git a/tests/mypy_test_fails1.py b/tests/mypy_test_fails1.py
new file mode 100644
index 000000000..6b1ebea03
--- /dev/null
+++ b/tests/mypy_test_fails1.py
@@ -0,0 +1,20 @@
+"""
+Test mypy failure with missing attribute
+"""
+from datetime import datetime
+from typing import List, Optional
+
+from pydantic import BaseModel, NoneStr
+
+
+class Model(BaseModel):
+    age: int
+    first_name = 'John'
+    last_name: NoneStr = None
+    signup_ts: Optional[datetime] = None
+    list_of_ints: List[int]
+
+
+m = Model(age=42, list_of_ints=[1, '2', b'3'])
+
+print(m.age + 'not integer')
diff --git a/tests/mypy_test_fails.py b/tests/mypy_test_fails2.py
similarity index 88%
rename from tests/mypy_test_fails.py
rename to tests/mypy_test_fails2.py
index e6c6fc856..2a900597f 100644
--- a/tests/mypy_test_fails.py
+++ b/tests/mypy_test_fails2.py
@@ -17,5 +17,4 @@ class Model(BaseModel):
 
 m = Model(age=42, list_of_ints=[1, '2', b'3'])
 
-assert m.age == 42, m.age
-m.age = 'not integer'
+print(m.foobar)
diff --git a/tests/mypy_test_success.py b/tests/mypy_test_success.py
index 2bbb6cdf5..10a61dccb 100644
--- a/tests/mypy_test_success.py
+++ b/tests/mypy_test_success.py
@@ -7,6 +7,7 @@ from datetime import datetime
 from typing import List, Optional
 
 from pydantic import BaseModel, NoneStr
+from pydantic.dataclasses import dataclass
 
 
 class Model(BaseModel):
@@ -44,3 +45,13 @@ assert m.first_name == 'Woof', m.first_name
 assert m.last_name == 'Woof', m.last_name
 assert m.signup_ts == datetime(2017, 6, 7), m.signup_ts
 assert day_of_week(m.signup_ts) == 3
+
+
+@dataclass
+class AddProject:
+    name: str
+    slug: Optional[str]
+    description: Optional[str]
+
+
+p = AddProject(name='x', slug='y', description='z')
