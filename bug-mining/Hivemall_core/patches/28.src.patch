diff --git a/core/src/main/java/hivemall/dataset/LogisticRegressionDataGeneratorUDTF.java b/core/src/main/java/hivemall/dataset/LogisticRegressionDataGeneratorUDTF.java
index 5b871838..24cd9d5e 100644
--- a/core/src/main/java/hivemall/dataset/LogisticRegressionDataGeneratorUDTF.java
+++ b/core/src/main/java/hivemall/dataset/LogisticRegressionDataGeneratorUDTF.java
@@ -155,13 +155,9 @@ public final class LogisticRegressionDataGeneratorUDTF extends UDTFWithOptions {
     public void process(Object[] argOIs) throws HiveException {
         if (rnd1 == null) {
             assert (rnd2 == null);
-            final int taskid = HadoopUtils.getTaskId(-1);
-            final long seed;
-            if (taskid == -1) {
-                seed = r_seed; // Non-MR local task
-            } else {
-                seed = r_seed + taskid;
-            }
+            int threadId = (int) Thread.currentThread().getId();
+            int taskid = HadoopUtils.getTaskId(threadId);
+            long seed = r_seed + taskid;
             this.rnd1 = new Random(seed);
             this.rnd2 = new Random(seed + 1);
         }
diff --git a/core/src/main/java/hivemall/utils/hadoop/HadoopUtils.java b/core/src/main/java/hivemall/utils/hadoop/HadoopUtils.java
index 10a17dc0..a85798a8 100644
--- a/core/src/main/java/hivemall/utils/hadoop/HadoopUtils.java
+++ b/core/src/main/java/hivemall/utils/hadoop/HadoopUtils.java
@@ -27,6 +27,8 @@ import java.io.FileReader;
 import java.io.IOException;
 import java.io.InputStreamReader;
 import java.io.Reader;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
 import java.net.URI;
 import java.util.Iterator;
 import java.util.Map.Entry;
@@ -138,7 +140,12 @@ public final class HadoopUtils {
     public static int getTaskId() {
         MapredContext ctx = MapredContextAccessor.get();
         if (ctx == null) {
-            throw new IllegalStateException("MapredContext is not set");
+            final int sparkTaskId = getSparkTaskId(-1);
+            if (sparkTaskId != -1) {
+                return sparkTaskId;
+            }
+            throw new IllegalStateException(
+                "Both hive.ql.exec.MapredContext and spark.TaskContext is not set");
         }
         JobConf jobconf = ctx.getJobConf();
         if (jobconf == null) {
@@ -175,6 +182,46 @@ public final class HadoopUtils {
         return taskid;
     }
 
+    /**
+     * @return org.apache.spark.TaskContext.get().partitionId()
+     */
+    public static int getSparkTaskId(final int defaultValue) {
+        final Class<?> clazz;
+        try {
+            clazz = Class.forName("org.apache.spark.TaskContext");
+        } catch (ClassNotFoundException e) {
+            return defaultValue;
+        }
+        final Method getMethod;
+        try {
+            getMethod = clazz.getDeclaredMethod("get");
+        } catch (NoSuchMethodException | SecurityException e) {
+            return defaultValue;
+        }
+        final Object taskContextInstance;
+        try {
+            taskContextInstance = getMethod.invoke(null);
+        } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {
+            return defaultValue;
+        }
+        final Method partitionIdMethod;
+        try {
+            partitionIdMethod = clazz.getDeclaredMethod("partitionId");
+        } catch (NoSuchMethodException | SecurityException e) {
+            return defaultValue;
+        }
+        final Object result;
+        try {
+            result = partitionIdMethod.invoke(taskContextInstance);
+        } catch (IllegalAccessException | IllegalArgumentException | InvocationTargetException e) {
+            return defaultValue;
+        }
+        if (result != null && result instanceof Integer) {
+            return ((Integer) result).intValue();
+        }
+        return defaultValue;
+    }
+
     public static String getUniqueTaskIdString() {
         MapredContext ctx = MapredContextAccessor.get();
         if (ctx != null) {
