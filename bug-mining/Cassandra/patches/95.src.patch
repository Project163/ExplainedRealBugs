diff --git a/build.xml b/build.xml
index d120ad7176..b0a0d86a35 100644
--- a/build.xml
+++ b/build.xml
@@ -1,4 +1,4 @@
-<?xml version="1.0" encoding="UTF-8" standalone="no"?>
+<?xml version="1.0" encoding="UTF-8" standalone="no"?>
 <!--
  ~ Licensed to the Apache Software Foundation (ASF) under one
  ~ or more contributor license agreements.  See the NOTICE file
@@ -17,295 +17,295 @@
  ~ specific language governing permissions and limitations
  ~ under the License.
  -->
-<project basedir="." default="build" name="apache-cassandra-incubating">
-    <property environment="env"/>
-    <property name="debuglevel" value="source,lines,vars"/>
-
-    <property name="basedir" value="."/>
-    <property name="build.src" value="${basedir}/src/java"/>
-    <property name="build.src.gen-java" value="${basedir}/src/gen-java"/>
-    <property name="build.lib" value="${basedir}/lib"/>
-    <property name="build.dir" value="${basedir}/build"/>
-    <property name="build.test.dir" value="${build.dir}/test"/>
-    <property name="build.classes" value="${build.dir}/classes"/>
-    <property name="javadoc.dir" value="${build.dir}/javadoc"/>
-    <property name="interface.dir" value="${basedir}/interface"/>
-    <property name="interface.src.dir" value="${interface.dir}/gen-java"/>
-    <property name="test.dir" value="${basedir}/test"/>
-    <property name="test.classes" value="${build.dir}/test/classes"/>
-    <property name="test.conf" value="${test.dir}/conf"/>
-    <property name="test.name" value="*Test"/>
-    <property name="test.unit.src" value="${test.dir}/unit"/>
-    <property name="dist.dir" value="${build.dir}/dist"/>
-    <property name="version" value="0.4.0-dev"/>
-    <property name="final.name" value="${ant.project.name}-${version}"/>
-
-    <!-- http://cobertura.sourceforge.net/ -->
-    <property name="cobertura.build.dir" value="${build.dir}/cobertura"/>
-    <!-- can't include due to licensing, specify jar via command line -->
-    <property name="cobertura.dir" value="/tmp"/>
-    <property name="cobertura.report.dir" value="${cobertura.build.dir}/report"/>
-    <property name="cobertura.classes.dir" value="${cobertura.build.dir}/classes"/>
-    <property name="cobertura.datafile" value="${cobertura.build.dir}/cobertura.ser"/>
-	
-    <!-- 
-	 Add all the dependencies.
-    -->
-    <path id="cassandra.classpath">
-        <pathelement location="${cobertura.classes.dir}"/>
-        <pathelement location="${build.classes}"/>
-        <fileset dir="${build.lib}">
-          <include name="**/*.jar" />
-        </fileset>
-    </path>
-
-    <!--
-	Setup the output directories.
-    -->
-    <target name="init">
-        <mkdir dir="${build.classes}"/>
-        <mkdir dir="${test.classes}"/>
-        <mkdir dir="${build.src.gen-java}"/>
-    </target>
-
-    <target name="clean">
-        <delete dir="${build.dir}" />
-        <delete dir="${build.src.gen-java}" />
-    </target>
-    <target depends="clean" name="cleanall"/>
-
-    <!--
-       This generates the CLI grammar files from Cli.g
-    -->
-    <target name="check-gen-cli-grammar">
-        <uptodate property="cliUpToDate" 
-                srcfile="${build.src}/org/apache/cassandra/cli/Cli.g" 
-                targetfile="${build.src.gen-java}/org/apache/cassandra/cli/Cli.tokens"/>
-    </target>
- 
-    <target name="gen-cli-grammar" depends="check-gen-cli-grammar" unless="cliUpToDate">
-      <echo>Building Grammar ${build.src}/org/apache/cassandra/cli/Cli.g  ....</echo>
-      <java classname="org.antlr.Tool"
-            classpath="${build.lib}/antlr-3.1.3.jar"
-            fork="true">
-         <arg value="${build.src}/org/apache/cassandra/cli/Cli.g" />
-         <arg value="-fo" />
-         <arg value="${build.src.gen-java}/org/apache/cassandra/cli/" />
-      </java> 
-    </target>
-
-    <!--
-       This generates the CQL grammar files from Cql.g
-    -->
-    <target name="check-gen-cql-grammar">
-        <uptodate property="cqlUpToDate" 
-                srcfile="${build.src}/org/apache/cassandra/cql/compiler/parse/Cql.g" 
-                targetfile="${build.src.gen-java}/org/apache/cassandra/cql/compiler/parse/Cql.tokens"/>
-    </target>
- 
-    <target name="gen-cql-grammar" depends="check-gen-cql-grammar" unless="cqlUpToDate">
-      <echo>Building Grammar ${build.src}/org/apache/cassandra/cql/compiler/parse/Cql.g  ....</echo>
-      <java classname="org.antlr.Tool"
-            classpath="${build.lib}/antlr-3.1.3.jar"
-            fork="true">
-         <arg value="${build.src}/org/apache/cassandra/cql/compiler/parse/Cql.g" />
-         <arg value="-fo" />
-         <arg value="${build.src.gen-java}/org/apache/cassandra/cql/compiler/parse/" />
-      </java> 
-    </target>
-
-    <target name="gen-thrift-java">
-      <echo>Generating Thrift Java code from ${basedir}/interface/cassandra.thrift ....</echo>
-      <exec executable="thrift" dir="${basedir}/interface">
-        <arg line="--gen java cassandra.thrift" />
-      </exec>
-    </target>
-    <target name="gen-thrift-py">
-      <echo>Generating Thrift Python code from ${basedir}/interface/cassandra.thrift ....</echo>
-      <exec executable="thrift" dir="${basedir}/interface">
-        <arg line="--gen py cassandra.thrift" />
-      </exec>
-    </target>
-
-    <!--
-	The build target builds all the .class files
-    -->
-    <target name="build" depends="build-subprojects,build-project"/>
-    <target name="build-subprojects"/>
-    <target name="codecoverage" depends="cobertura-instrument,test,cobertura-report"/>
-    	
-    <target depends="init,gen-cli-grammar,gen-cql-grammar" name="build-project">
-        <echo message="${ant.project.name}: ${ant.file}"/>
-        <javac debug="true" debuglevel="${debuglevel}" destdir="${build.classes}">
-            <src path="${build.src}"/>
-            <src path="${build.src.gen-java}"/>
-            <src path="${interface.src.dir}"/>
-            <classpath refid="cassandra.classpath"/>
-        </javac>
-    </target>
-
-    <!--
-	The jar target makes cassandra.jar output.
-    -->
-    <target name="jar" depends="build">
-      <mkdir dir="${build.classes}/META-INF"/>
-      <copy file="LICENSE.txt" tofile="${build.classes}/META-INF/LICENSE.txt"/>
-      <copy file="NOTICE.txt" tofile="${build.classes}/META-INF/NOTICE.txt"/>
-      <jar jarfile="${build.dir}/${final.name}.jar"
-           basedir="${build.classes}">
-        <manifest>
-        <!-- <section name="org/apache/cassandra/infrastructure"> -->
-          <attribute name="Implementation-Title" value="Cassandra"/>
-          <attribute name="Implementation-Version" value="${version}"/>
-          <attribute name="Implementation-Vendor" value="Apache"/>
-          <attribute name="Premain-Class" value="org.apache.cassandra.infrastructure.continuations.CAgent"/>
-        <!-- </section> -->
-        </manifest>
-      </jar>
-    </target>
-
-    <!-- creates a release tarball -->	
-    <target name="release" depends="jar,javadoc">
-      <mkdir dir="${dist.dir}"/>
-      <copy todir="${dist.dir}/lib">
-        <fileset dir="${build.lib}"/>
-        <fileset dir="${build.dir}">
-          <include name="*.jar" />
-        </fileset>
-      </copy>
-      <copy todir="${dist.dir}/javadoc">
-        <fileset dir="${javadoc.dir}"/>
-      </copy>
-      <copy todir="${dist.dir}/bin">
-        <fileset dir="bin"/>
-      </copy>
-      <copy todir="${dist.dir}/conf">
-        <fileset dir="conf"/>
-      </copy>   
-      <copy todir="${dist.dir}/interface">
-        <fileset dir="interface">
-          <include name="**/*.thrift" />
-        </fileset>
-      </copy>      
-      <copy todir="${dist.dir}/">
-        <fileset dir="${basedir}">
-          <include name="*.txt" />
-        </fileset>
-      </copy> 
-      
-      <tar compression="gzip" longfile="gnu"
-        destfile="${build.dir}/${final.name}-bin.tar.gz">
-
-        <!-- Everything but bin/ (default mode) -->
-        <tarfileset dir="${dist.dir}" prefix="${final.name}">
-          <include name="**"/>
-          <exclude name="bin/*" />
-        </tarfileset>
-        <!-- Shell includes in bin/ (default mode) -->
-        <tarfileset dir="${dist.dir}" prefix="${final.name}">
-          <include name="bin/*.in.sh" />
-        </tarfileset>
-        <!-- Executable scripts in bin/ -->
-        <tarfileset dir="${dist.dir}" prefix="${final.name}" mode="755">
-          <include name="bin/*"/>
-          <not>
-        	<filename name="bin/*.in.sh" />
-          </not>
-        </tarfileset>	
-      </tar>
-      <tar compression="gzip" longfile="gnu"
-        destfile="${build.dir}/${final.name}-src.tar.gz">
-
-        <tarfileset dir="${basedir}"
-          prefix="${final.name}-src">
-          <include name="**"/>
-          <exclude name="build/**" />
-        </tarfileset>
-      </tar>
-    </target>
-
-  <target name="build-test" depends="build" description="Build the Cassandra classes">
-    <javac
-     debug="true"
-     debuglevel="${debuglevel}"
-     destdir="${test.classes}"
-    >
-      <classpath refid="cassandra.classpath"/>
-      <src path="${test.unit.src}"/>
-    </javac>
-  </target>
-
-   <target name="test" depends="build-test">
-    <echo message="running tests"/>
-    <mkdir dir="${build.test.dir}/cassandra"/>
-    <mkdir dir="${build.test.dir}/output"/>
-    <junit fork="on" failureproperty="testfailed">
-      <sysproperty key="net.sourceforge.cobertura.datafile" file="${cobertura.datafile}"/>
-      <formatter type="xml" usefile="true"/>
-      <formatter type="brief" usefile="false"/>
-      <jvmarg value="-Dstorage-config=${test.conf}"/>
-      <jvmarg value="-ea"/>
-      <classpath>
-        <path refid="cassandra.classpath" />
-        <pathelement location="${test.classes}"/>
-        <pathelement location="${cobertura.dir}/cobertura.jar"/>
-        <pathelement location="${test.conf}"/>
-      </classpath>
-      <batchtest todir="${build.test.dir}/output">
-        <fileset dir="${test.classes}" includes="**/${test.name}.class" />
-      </batchtest>
-    </junit>
-
-    <fail if="testfailed" message="Some test(s) failed."/>
-  </target>
-	
-  <!-- instruments the classes to later create code coverage reports -->
-  <target name="cobertura-instrument" depends="build,build-test">
-    <taskdef resource="tasks.properties">
-      <classpath>
-        <fileset dir="${cobertura.dir}">
-            <include name="cobertura.jar" />
-            <include name="lib/**/*.jar" />
-        </fileset>
-      </classpath>
-    </taskdef>
-    
-    <delete file="${cobertura.datafile}"/>
-    
-    <cobertura-instrument todir="${cobertura.classes.dir}" datafile="${cobertura.datafile}">
-      <ignore regex="org.apache.log4j.*"/>
-      
-      <fileset dir="${build.classes}">
-        <include name="**/*.class"/>
-        <exclude name="**/*Test.class"/>
-        <exclude name="**/*TestCase.class"/>
-        <exclude name="**/test/*.class"/>
-        <exclude name="${cobertura.excludes}"/>
-      </fileset>
-     
-    </cobertura-instrument>
-  </target>	
-
-  <!-- create both html and xml code coverage reports -->
-  <target name="cobertura-report">
-    <cobertura-report format="html" destdir="${cobertura.report.dir}" srcdir="${build.src}"
-      datafile="${cobertura.datafile}"/>
-    <cobertura-report format="xml" destdir="${cobertura.report.dir}" srcdir="${build.src}"
-      datafile="${cobertura.datafile}"/>
-  </target>	
-	
-  <target name="javadoc" depends="init">
-    <tstamp>
-      <format property="YEAR" pattern="yyyy"/>
-    </tstamp>
-    <javadoc destdir="${javadoc.dir}" author="true" version="true" use="true"
-      windowtitle="${ant.project.name} API" classpathref="cassandra.classpath"
-      bottom="Copyright &amp;copy; ${YEAR} The Apache Software Foundation">
-
-      <fileset dir="${build.src}" defaultexcludes="yes">
-        <include name="org/apache/**/*.java"/>
-      </fileset>
-    </javadoc>
-   </target>
-
-</project>
+<project basedir="." default="build" name="apache-cassandra-incubating">
+    <property environment="env"/>
+    <property name="debuglevel" value="source,lines,vars"/>
+
+    <property name="basedir" value="."/>
+    <property name="build.src" value="${basedir}/src/java"/>
+    <property name="build.src.gen-java" value="${basedir}/src/gen-java"/>
+    <property name="build.lib" value="${basedir}/lib"/>
+    <property name="build.dir" value="${basedir}/build"/>
+    <property name="build.test.dir" value="${build.dir}/test"/>
+    <property name="build.classes" value="${build.dir}/classes"/>
+    <property name="javadoc.dir" value="${build.dir}/javadoc"/>
+    <property name="interface.dir" value="${basedir}/interface"/>
+    <property name="interface.src.dir" value="${interface.dir}/gen-java"/>
+    <property name="test.dir" value="${basedir}/test"/>
+    <property name="test.classes" value="${build.dir}/test/classes"/>
+    <property name="test.conf" value="${test.dir}/conf"/>
+    <property name="test.name" value="*Test"/>
+    <property name="test.unit.src" value="${test.dir}/unit"/>
+    <property name="dist.dir" value="${build.dir}/dist"/>
+    <property name="version" value="0.4.0-dev"/>
+    <property name="final.name" value="${ant.project.name}-${version}"/>
+
+    <!-- http://cobertura.sourceforge.net/ -->
+    <property name="cobertura.build.dir" value="${build.dir}/cobertura"/>
+    <!-- can't include due to licensing, specify jar via command line -->
+    <property name="cobertura.dir" value="/tmp"/>
+    <property name="cobertura.report.dir" value="${cobertura.build.dir}/report"/>
+    <property name="cobertura.classes.dir" value="${cobertura.build.dir}/classes"/>
+    <property name="cobertura.datafile" value="${cobertura.build.dir}/cobertura.ser"/>
+	
+    <!-- 
+	 Add all the dependencies.
+    -->
+    <path id="cassandra.classpath">
+        <pathelement location="${cobertura.classes.dir}"/>
+        <pathelement location="${build.classes}"/>
+        <fileset dir="${build.lib}">
+          <include name="**/*.jar" />
+        </fileset>
+    </path>
+
+    <!--
+	Setup the output directories.
+    -->
+    <target name="init">
+        <mkdir dir="${build.classes}"/>
+        <mkdir dir="${test.classes}"/>
+        <mkdir dir="${build.src.gen-java}"/>
+    </target>
+
+    <target name="clean">
+        <delete dir="${build.dir}" />
+        <delete dir="${build.src.gen-java}" />
+    </target>
+    <target depends="clean" name="cleanall"/>
+
+    <!--
+       This generates the CLI grammar files from Cli.g
+    -->
+    <target name="check-gen-cli-grammar">
+        <uptodate property="cliUpToDate" 
+                srcfile="${build.src}/org/apache/cassandra/cli/Cli.g" 
+                targetfile="${build.src.gen-java}/org/apache/cassandra/cli/Cli.tokens"/>
+    </target>
+ 
+    <target name="gen-cli-grammar" depends="check-gen-cli-grammar" unless="cliUpToDate">
+      <echo>Building Grammar ${build.src}/org/apache/cassandra/cli/Cli.g  ....</echo>
+      <java classname="org.antlr.Tool"
+            classpath="${build.lib}/antlr-3.1.3.jar"
+            fork="true">
+         <arg value="${build.src}/org/apache/cassandra/cli/Cli.g" />
+         <arg value="-fo" />
+         <arg value="${build.src.gen-java}/org/apache/cassandra/cli/" />
+      </java> 
+    </target>
+
+    <!--
+       This generates the CQL grammar files from Cql.g
+    -->
+    <target name="check-gen-cql-grammar">
+        <uptodate property="cqlUpToDate" 
+                srcfile="${build.src}/org/apache/cassandra/cql/compiler/parse/Cql.g" 
+                targetfile="${build.src.gen-java}/org/apache/cassandra/cql/compiler/parse/Cql.tokens"/>
+    </target>
+ 
+    <target name="gen-cql-grammar" depends="check-gen-cql-grammar" unless="cqlUpToDate">
+      <echo>Building Grammar ${build.src}/org/apache/cassandra/cql/compiler/parse/Cql.g  ....</echo>
+      <java classname="org.antlr.Tool"
+            classpath="${build.lib}/antlr-3.1.3.jar"
+            fork="true">
+         <arg value="${build.src}/org/apache/cassandra/cql/compiler/parse/Cql.g" />
+         <arg value="-fo" />
+         <arg value="${build.src.gen-java}/org/apache/cassandra/cql/compiler/parse/" />
+      </java> 
+    </target>
+
+    <target name="gen-thrift-java">
+      <echo>Generating Thrift Java code from ${basedir}/interface/cassandra.thrift ....</echo>
+      <exec executable="thrift" dir="${basedir}/interface">
+        <arg line="--gen java cassandra.thrift" />
+      </exec>
+    </target>
+    <target name="gen-thrift-py">
+      <echo>Generating Thrift Python code from ${basedir}/interface/cassandra.thrift ....</echo>
+      <exec executable="thrift" dir="${basedir}/interface">
+        <arg line="--gen py cassandra.thrift" />
+      </exec>
+    </target>
+
+    <!--
+	The build target builds all the .class files
+    -->
+    <target name="build" depends="build-subprojects,build-project"/>
+    <target name="build-subprojects"/>
+    <target name="codecoverage" depends="cobertura-instrument,test,cobertura-report"/>
+    	
+    <target depends="init,gen-cli-grammar,gen-cql-grammar" name="build-project">
+        <echo message="${ant.project.name}: ${ant.file}"/>
+        <javac debug="true" debuglevel="${debuglevel}" destdir="${build.classes}">
+            <src path="${build.src}"/>
+            <src path="${build.src.gen-java}"/>
+            <src path="${interface.src.dir}"/>
+            <classpath refid="cassandra.classpath"/>
+        </javac>
+    </target>
+
+    <!--
+	The jar target makes cassandra.jar output.
+    -->
+    <target name="jar" depends="build">
+      <mkdir dir="${build.classes}/META-INF"/>
+      <copy file="LICENSE.txt" tofile="${build.classes}/META-INF/LICENSE.txt"/>
+      <copy file="NOTICE.txt" tofile="${build.classes}/META-INF/NOTICE.txt"/>
+      <jar jarfile="${build.dir}/${final.name}.jar"
+           basedir="${build.classes}">
+        <manifest>
+        <!-- <section name="org/apache/cassandra/infrastructure"> -->
+          <attribute name="Implementation-Title" value="Cassandra"/>
+          <attribute name="Implementation-Version" value="${version}"/>
+          <attribute name="Implementation-Vendor" value="Apache"/>
+          <attribute name="Premain-Class" value="org.apache.cassandra.infrastructure.continuations.CAgent"/>
+        <!-- </section> -->
+        </manifest>
+      </jar>
+    </target>
+
+    <!-- creates a release tarball -->	
+    <target name="release" depends="jar,javadoc">
+      <mkdir dir="${dist.dir}"/>
+      <copy todir="${dist.dir}/lib">
+        <fileset dir="${build.lib}"/>
+        <fileset dir="${build.dir}">
+          <include name="*.jar" />
+        </fileset>
+      </copy>
+      <copy todir="${dist.dir}/javadoc">
+        <fileset dir="${javadoc.dir}"/>
+      </copy>
+      <copy todir="${dist.dir}/bin">
+        <fileset dir="bin"/>
+      </copy>
+      <copy todir="${dist.dir}/conf">
+        <fileset dir="conf"/>
+      </copy>   
+      <copy todir="${dist.dir}/interface">
+        <fileset dir="interface">
+          <include name="**/*.thrift" />
+        </fileset>
+      </copy>      
+      <copy todir="${dist.dir}/">
+        <fileset dir="${basedir}">
+          <include name="*.txt" />
+        </fileset>
+      </copy> 
+      
+      <tar compression="gzip" longfile="gnu"
+        destfile="${build.dir}/${final.name}-bin.tar.gz">
+
+        <!-- Everything but bin/ (default mode) -->
+        <tarfileset dir="${dist.dir}" prefix="${final.name}">
+          <include name="**"/>
+          <exclude name="bin/*" />
+        </tarfileset>
+        <!-- Shell includes in bin/ (default mode) -->
+        <tarfileset dir="${dist.dir}" prefix="${final.name}">
+          <include name="bin/*.in.sh" />
+        </tarfileset>
+        <!-- Executable scripts in bin/ -->
+        <tarfileset dir="${dist.dir}" prefix="${final.name}" mode="755">
+          <include name="bin/*"/>
+          <not>
+        	<filename name="bin/*.in.sh" />
+          </not>
+        </tarfileset>	
+      </tar>
+      <tar compression="gzip" longfile="gnu"
+        destfile="${build.dir}/${final.name}-src.tar.gz">
+
+        <tarfileset dir="${basedir}"
+          prefix="${final.name}-src">
+          <include name="**"/>
+          <exclude name="build/**" />
+        </tarfileset>
+      </tar>
+    </target>
+
+  <target name="build-test" depends="build" description="Build the Cassandra classes">
+    <javac
+     debug="true"
+     debuglevel="${debuglevel}"
+     destdir="${test.classes}"
+    >
+      <classpath refid="cassandra.classpath"/>
+      <src path="${test.unit.src}"/>
+    </javac>
+  </target>
+
+   <target name="test" depends="build-test">
+    <echo message="running tests"/>
+    <mkdir dir="${build.test.dir}/cassandra"/>
+    <mkdir dir="${build.test.dir}/output"/>
+    <junit fork="on" failureproperty="testfailed">
+      <sysproperty key="net.sourceforge.cobertura.datafile" file="${cobertura.datafile}"/>
+      <formatter type="xml" usefile="true"/>
+      <formatter type="brief" usefile="false"/>
+      <jvmarg value="-Dstorage-config=${test.conf}"/>
+      <jvmarg value="-ea"/>
+      <classpath>
+        <path refid="cassandra.classpath" />
+        <pathelement location="${test.classes}"/>
+        <pathelement location="${cobertura.dir}/cobertura.jar"/>
+        <pathelement location="${test.conf}"/>
+      </classpath>
+      <batchtest todir="${build.test.dir}/output">
+        <fileset dir="${test.classes}" includes="**/${test.name}.class" />
+      </batchtest>
+    </junit>
+
+    <fail if="testfailed" message="Some test(s) failed."/>
+  </target>
+	
+  <!-- instruments the classes to later create code coverage reports -->
+  <target name="cobertura-instrument" depends="build,build-test">
+    <taskdef resource="tasks.properties">
+      <classpath>
+        <fileset dir="${cobertura.dir}">
+            <include name="cobertura.jar" />
+            <include name="lib/**/*.jar" />
+        </fileset>
+      </classpath>
+    </taskdef>
+    
+    <delete file="${cobertura.datafile}"/>
+    
+    <cobertura-instrument todir="${cobertura.classes.dir}" datafile="${cobertura.datafile}">
+      <ignore regex="org.apache.log4j.*"/>
+      
+      <fileset dir="${build.classes}">
+        <include name="**/*.class"/>
+        <exclude name="**/*Test.class"/>
+        <exclude name="**/*TestCase.class"/>
+        <exclude name="**/test/*.class"/>
+        <exclude name="${cobertura.excludes}"/>
+      </fileset>
+     
+    </cobertura-instrument>
+  </target>	
+
+  <!-- create both html and xml code coverage reports -->
+  <target name="cobertura-report">
+    <cobertura-report format="html" destdir="${cobertura.report.dir}" srcdir="${build.src}"
+      datafile="${cobertura.datafile}"/>
+    <cobertura-report format="xml" destdir="${cobertura.report.dir}" srcdir="${build.src}"
+      datafile="${cobertura.datafile}"/>
+  </target>	
+	
+  <target name="javadoc" depends="init">
+    <tstamp>
+      <format property="YEAR" pattern="yyyy"/>
+    </tstamp>
+    <javadoc destdir="${javadoc.dir}" author="true" version="true" use="true"
+      windowtitle="${ant.project.name} API" classpathref="cassandra.classpath"
+      bottom="Copyright &amp;copy; ${YEAR} The Apache Software Foundation">
+
+      <fileset dir="${build.src}" defaultexcludes="yes">
+        <include name="org/apache/**/*.java"/>
+      </fileset>
+    </javadoc>
+   </target>
+
+</project>
diff --git a/src/java/org/apache/cassandra/concurrent/AIOExecutorService.java b/src/java/org/apache/cassandra/concurrent/AIOExecutorService.java
index bbd5337cdd..1679dfad50 100644
--- a/src/java/org/apache/cassandra/concurrent/AIOExecutorService.java
+++ b/src/java/org/apache/cassandra/concurrent/AIOExecutorService.java
@@ -1,319 +1,319 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import java.util.Collection;
-import java.util.List;
-import java.util.concurrent.BlockingQueue;
-import java.util.concurrent.Callable;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.Future;
-import java.util.concurrent.RejectedExecutionException;
-import java.util.concurrent.ThreadFactory;
-import java.util.concurrent.ThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.TimeoutException;
-
-public class AIOExecutorService implements ExecutorService
-{
-    private ExecutorService executorService_;
-    
-    public AIOExecutorService(int corePoolSize,
-            int maximumPoolSize,
-            long keepAliveTime,
-            TimeUnit unit,
-            BlockingQueue<Runnable> workQueue,
-            ThreadFactory threadFactory)
-    {
-        executorService_ = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory);        
-    }
-    
-    /**
-     * Executes the given command at some time in the future.  The command
-     * may execute in a new thread, in a pooled thread, or in the calling
-     * thread, at the discretion of the <tt>Executor</tt> implementation.
-     *
-     * @param command the runnable task
-     * @throws RejectedExecutionException if this task cannot be
-     * accepted for execution.
-     * @throws NullPointerException if command is null
-     */
-    public void execute(Runnable command)
-    {
-        executorService_.execute(command);
-    }
-    
-    /**
-     * Initiates an orderly shutdown in which previously submitted
-     * tasks are executed, but no new tasks will be accepted.
-     * Invocation has no additional effect if already shut down.
-     *
-     * <p>This method does not wait for previously submitted tasks to
-     * complete execution.  Use {@link #awaitTermination awaitTermination}
-     * to do that.
-     *
-     * @throws SecurityException if a security manager exists and
-     *         shutting down this ExecutorService may manipulate
-     *         threads that the caller is not permitted to modify
-     *         because it does not hold {@link
-     *         java.lang.RuntimePermission}<tt>("modifyThread")</tt>,
-     *         or the security manager's <tt>checkAccess</tt> method
-     *         denies access.
-     */
-    public void shutdown()
-    {    
-        /* This is a noop. */     
-    }
-
-    /**
-     * Attempts to stop all actively executing tasks, halts the
-     * processing of waiting tasks, and returns a list of the tasks
-     * that were awaiting execution.
-     *
-     * <p>This method does not wait for actively executing tasks to
-     * terminate.  Use {@link #awaitTermination awaitTermination} to
-     * do that.
-     *
-     * <p>There are no guarantees beyond best-effort attempts to stop
-     * processing actively executing tasks.  For example, typical
-     * implementations will cancel via {@link Thread#interrupt}, so any
-     * task that fails to respond to interrupts may never terminate.
-     *
-     * @return list of tasks that never commenced execution
-     * @throws SecurityException if a security manager exists and
-     *         shutting down this ExecutorService may manipulate
-     *         threads that the caller is not permitted to modify
-     *         because it does not hold {@link
-     *         java.lang.RuntimePermission}<tt>("modifyThread")</tt>,
-     *         or the security manager's <tt>checkAccess</tt> method
-     *         denies access.
-     */
-    public List<Runnable> shutdownNow()
-    {
-        return executorService_.shutdownNow();
-    }
-
-    /**
-     * Returns <tt>true</tt> if this executor has been shut down.
-     *
-     * @return <tt>true</tt> if this executor has been shut down
-     */
-    public boolean isShutdown()
-    {
-        return executorService_.isShutdown();
-    }
-
-    /**
-     * Returns <tt>true</tt> if all tasks have completed following shut down.
-     * Note that <tt>isTerminated</tt> is never <tt>true</tt> unless
-     * either <tt>shutdown</tt> or <tt>shutdownNow</tt> was called first.
-     *
-     * @return <tt>true</tt> if all tasks have completed following shut down
-     */
-    public boolean isTerminated()
-    {
-        return executorService_.isTerminated();
-    }
-
-    /**
-     * Blocks until all tasks have completed execution after a shutdown
-     * request, or the timeout occurs, or the current thread is
-     * interrupted, whichever happens first.
-     *
-     * @param timeout the maximum time to wait
-     * @param unit the time unit of the timeout argument
-     * @return <tt>true</tt> if this executor terminated and
-     *         <tt>false</tt> if the timeout elapsed before termination
-     * @throws InterruptedException if interrupted while waiting
-     */
-    public boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException
-    {
-        return executorService_.awaitTermination(timeout, unit);
-    }
-
-    /**
-     * Submits a value-returning task for execution and returns a
-     * Future representing the pending results of the task. The
-     * Future's <tt>get</tt> method will return the task's result upon
-     * successful completion.
-     *
-     * <p>
-     * If you would like to immediately block waiting
-     * for a task, you can use constructions of the form
-     * <tt>result = exec.submit(aCallable).get();</tt>
-     *
-     * <p> Note: The {@link Executors} class includes a set of methods
-     * that can convert some other common closure-like objects,
-     * for example, {@link java.security.PrivilegedAction} to
-     * {@link Callable} form so they can be submitted.
-     *
-     * @param task the task to submit
-     * @return a Future representing pending completion of the task
-     * @throws RejectedExecutionException if the task cannot be
-     *         scheduled for execution
-     * @throws NullPointerException if the task is null
-     */
-    public <T> Future<T> submit(Callable<T> task)
-    {
-        return executorService_.submit(task);
-    }
-
-    /**
-     * Submits a Runnable task for execution and returns a Future
-     * representing that task. The Future's <tt>get</tt> method will
-     * return the given result upon successful completion.
-     *
-     * @param task the task to submit
-     * @param result the result to return
-     * @return a Future representing pending completion of the task
-     * @throws RejectedExecutionException if the task cannot be
-     *         scheduled for execution
-     * @throws NullPointerException if the task is null
-     */
-    public <T> Future<T> submit(Runnable task, T result)
-    {
-        return executorService_.submit(task, result);
-    }
-
-    /**
-     * Submits a Runnable task for execution and returns a Future
-     * representing that task. The Future's <tt>get</tt> method will
-     * return <tt>null</tt> upon <em>successful</em> completion.
-     *
-     * @param task the task to submit
-     * @return a Future representing pending completion of the task
-     * @throws RejectedExecutionException if the task cannot be
-     *         scheduled for execution
-     * @throws NullPointerException if the task is null
-     */
-    public Future<?> submit(Runnable task)
-    {
-        return executorService_.submit(task);
-    }
-
-    /**
-     * Executes the given tasks, returning a list of Futures holding
-     * their status and results when all complete.
-     * {@link Future#isDone} is <tt>true</tt> for each
-     * element of the returned list.
-     * Note that a <em>completed</em> task could have
-     * terminated either normally or by throwing an exception.
-     * The results of this method are undefined if the given
-     * collection is modified while this operation is in progress.
-     *
-     * @param tasks the collection of tasks
-     * @return A list of Futures representing the tasks, in the same
-     *         sequential order as produced by the iterator for the
-     *         given task list, each of which has completed.
-     * @throws InterruptedException if interrupted while waiting, in
-     *         which case unfinished tasks are cancelled.
-     * @throws NullPointerException if tasks or any of its elements are <tt>null</tt>
-     * @throws RejectedExecutionException if any task cannot be
-     *         scheduled for execution
-     */
-
-    public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks) throws InterruptedException
-    {
-        return executorService_.invokeAll(tasks);
-    }
-
-    /**
-     * Executes the given tasks, returning a list of Futures holding
-     * their status and results
-     * when all complete or the timeout expires, whichever happens first.
-     * {@link Future#isDone} is <tt>true</tt> for each
-     * element of the returned list.
-     * Upon return, tasks that have not completed are cancelled.
-     * Note that a <em>completed</em> task could have
-     * terminated either normally or by throwing an exception.
-     * The results of this method are undefined if the given
-     * collection is modified while this operation is in progress.
-     *
-     * @param tasks the collection of tasks
-     * @param timeout the maximum time to wait
-     * @param unit the time unit of the timeout argument
-     * @return a list of Futures representing the tasks, in the same
-     *         sequential order as produced by the iterator for the
-     *         given task list. If the operation did not time out,
-     *         each task will have completed. If it did time out, some
-     *         of these tasks will not have completed.
-     * @throws InterruptedException if interrupted while waiting, in
-     *         which case unfinished tasks are cancelled
-     * @throws NullPointerException if tasks, any of its elements, or
-     *         unit are <tt>null</tt>
-     * @throws RejectedExecutionException if any task cannot be scheduled
-     *         for execution
-     */
-    public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks, long timeout, TimeUnit unit) throws InterruptedException
-    {
-        return executorService_.invokeAll(tasks, timeout, unit);
-    }
-    
-    /**
-     * Executes the given tasks, returning the result
-     * of one that has completed successfully (i.e., without throwing
-     * an exception), if any do. Upon normal or exceptional return,
-     * tasks that have not completed are cancelled.
-     * The results of this method are undefined if the given
-     * collection is modified while this operation is in progress.
-     *
-     * @param tasks the collection of tasks
-     * @return the result returned by one of the tasks
-     * @throws InterruptedException if interrupted while waiting
-     * @throws NullPointerException if tasks or any of its elements
-     *         are <tt>null</tt>
-     * @throws IllegalArgumentException if tasks is empty
-     * @throws ExecutionException if no task successfully completes
-     * @throws RejectedExecutionException if tasks cannot be scheduled
-     *         for execution
-     */
-    public <T> T invokeAny(Collection<? extends Callable<T>> tasks) throws InterruptedException, ExecutionException
-    {
-        return executorService_.invokeAny(tasks);
-    }
-
-    /**
-     * Executes the given tasks, returning the result
-     * of one that has completed successfully (i.e., without throwing
-     * an exception), if any do before the given timeout elapses.
-     * Upon normal or exceptional return, tasks that have not
-     * completed are cancelled.
-     * The results of this method are undefined if the given
-     * collection is modified while this operation is in progress.
-     *
-     * @param tasks the collection of tasks
-     * @param timeout the maximum time to wait
-     * @param unit the time unit of the timeout argument
-     * @return the result returned by one of the tasks.
-     * @throws InterruptedException if interrupted while waiting
-     * @throws NullPointerException if tasks, any of its elements, or
-     *         unit are <tt>null</tt>
-     * @throws TimeoutException if the given timeout elapses before
-     *         any task successfully completes
-     * @throws ExecutionException if no task successfully completes
-     * @throws RejectedExecutionException if tasks cannot be scheduled
-     *         for execution
-     */
-    public <T> T invokeAny(Collection<? extends Callable<T>> tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException
-    {
-        return executorService_.invokeAny(tasks, timeout, unit);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.concurrent.BlockingQueue;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.RejectedExecutionException;
+import java.util.concurrent.ThreadFactory;
+import java.util.concurrent.ThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+public class AIOExecutorService implements ExecutorService
+{
+    private ExecutorService executorService_;
+    
+    public AIOExecutorService(int corePoolSize,
+            int maximumPoolSize,
+            long keepAliveTime,
+            TimeUnit unit,
+            BlockingQueue<Runnable> workQueue,
+            ThreadFactory threadFactory)
+    {
+        executorService_ = new ThreadPoolExecutor(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory);        
+    }
+    
+    /**
+     * Executes the given command at some time in the future.  The command
+     * may execute in a new thread, in a pooled thread, or in the calling
+     * thread, at the discretion of the <tt>Executor</tt> implementation.
+     *
+     * @param command the runnable task
+     * @throws RejectedExecutionException if this task cannot be
+     * accepted for execution.
+     * @throws NullPointerException if command is null
+     */
+    public void execute(Runnable command)
+    {
+        executorService_.execute(command);
+    }
+    
+    /**
+     * Initiates an orderly shutdown in which previously submitted
+     * tasks are executed, but no new tasks will be accepted.
+     * Invocation has no additional effect if already shut down.
+     *
+     * <p>This method does not wait for previously submitted tasks to
+     * complete execution.  Use {@link #awaitTermination awaitTermination}
+     * to do that.
+     *
+     * @throws SecurityException if a security manager exists and
+     *         shutting down this ExecutorService may manipulate
+     *         threads that the caller is not permitted to modify
+     *         because it does not hold {@link
+     *         java.lang.RuntimePermission}<tt>("modifyThread")</tt>,
+     *         or the security manager's <tt>checkAccess</tt> method
+     *         denies access.
+     */
+    public void shutdown()
+    {    
+        /* This is a noop. */     
+    }
+
+    /**
+     * Attempts to stop all actively executing tasks, halts the
+     * processing of waiting tasks, and returns a list of the tasks
+     * that were awaiting execution.
+     *
+     * <p>This method does not wait for actively executing tasks to
+     * terminate.  Use {@link #awaitTermination awaitTermination} to
+     * do that.
+     *
+     * <p>There are no guarantees beyond best-effort attempts to stop
+     * processing actively executing tasks.  For example, typical
+     * implementations will cancel via {@link Thread#interrupt}, so any
+     * task that fails to respond to interrupts may never terminate.
+     *
+     * @return list of tasks that never commenced execution
+     * @throws SecurityException if a security manager exists and
+     *         shutting down this ExecutorService may manipulate
+     *         threads that the caller is not permitted to modify
+     *         because it does not hold {@link
+     *         java.lang.RuntimePermission}<tt>("modifyThread")</tt>,
+     *         or the security manager's <tt>checkAccess</tt> method
+     *         denies access.
+     */
+    public List<Runnable> shutdownNow()
+    {
+        return executorService_.shutdownNow();
+    }
+
+    /**
+     * Returns <tt>true</tt> if this executor has been shut down.
+     *
+     * @return <tt>true</tt> if this executor has been shut down
+     */
+    public boolean isShutdown()
+    {
+        return executorService_.isShutdown();
+    }
+
+    /**
+     * Returns <tt>true</tt> if all tasks have completed following shut down.
+     * Note that <tt>isTerminated</tt> is never <tt>true</tt> unless
+     * either <tt>shutdown</tt> or <tt>shutdownNow</tt> was called first.
+     *
+     * @return <tt>true</tt> if all tasks have completed following shut down
+     */
+    public boolean isTerminated()
+    {
+        return executorService_.isTerminated();
+    }
+
+    /**
+     * Blocks until all tasks have completed execution after a shutdown
+     * request, or the timeout occurs, or the current thread is
+     * interrupted, whichever happens first.
+     *
+     * @param timeout the maximum time to wait
+     * @param unit the time unit of the timeout argument
+     * @return <tt>true</tt> if this executor terminated and
+     *         <tt>false</tt> if the timeout elapsed before termination
+     * @throws InterruptedException if interrupted while waiting
+     */
+    public boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException
+    {
+        return executorService_.awaitTermination(timeout, unit);
+    }
+
+    /**
+     * Submits a value-returning task for execution and returns a
+     * Future representing the pending results of the task. The
+     * Future's <tt>get</tt> method will return the task's result upon
+     * successful completion.
+     *
+     * <p>
+     * If you would like to immediately block waiting
+     * for a task, you can use constructions of the form
+     * <tt>result = exec.submit(aCallable).get();</tt>
+     *
+     * <p> Note: The {@link Executors} class includes a set of methods
+     * that can convert some other common closure-like objects,
+     * for example, {@link java.security.PrivilegedAction} to
+     * {@link Callable} form so they can be submitted.
+     *
+     * @param task the task to submit
+     * @return a Future representing pending completion of the task
+     * @throws RejectedExecutionException if the task cannot be
+     *         scheduled for execution
+     * @throws NullPointerException if the task is null
+     */
+    public <T> Future<T> submit(Callable<T> task)
+    {
+        return executorService_.submit(task);
+    }
+
+    /**
+     * Submits a Runnable task for execution and returns a Future
+     * representing that task. The Future's <tt>get</tt> method will
+     * return the given result upon successful completion.
+     *
+     * @param task the task to submit
+     * @param result the result to return
+     * @return a Future representing pending completion of the task
+     * @throws RejectedExecutionException if the task cannot be
+     *         scheduled for execution
+     * @throws NullPointerException if the task is null
+     */
+    public <T> Future<T> submit(Runnable task, T result)
+    {
+        return executorService_.submit(task, result);
+    }
+
+    /**
+     * Submits a Runnable task for execution and returns a Future
+     * representing that task. The Future's <tt>get</tt> method will
+     * return <tt>null</tt> upon <em>successful</em> completion.
+     *
+     * @param task the task to submit
+     * @return a Future representing pending completion of the task
+     * @throws RejectedExecutionException if the task cannot be
+     *         scheduled for execution
+     * @throws NullPointerException if the task is null
+     */
+    public Future<?> submit(Runnable task)
+    {
+        return executorService_.submit(task);
+    }
+
+    /**
+     * Executes the given tasks, returning a list of Futures holding
+     * their status and results when all complete.
+     * {@link Future#isDone} is <tt>true</tt> for each
+     * element of the returned list.
+     * Note that a <em>completed</em> task could have
+     * terminated either normally or by throwing an exception.
+     * The results of this method are undefined if the given
+     * collection is modified while this operation is in progress.
+     *
+     * @param tasks the collection of tasks
+     * @return A list of Futures representing the tasks, in the same
+     *         sequential order as produced by the iterator for the
+     *         given task list, each of which has completed.
+     * @throws InterruptedException if interrupted while waiting, in
+     *         which case unfinished tasks are cancelled.
+     * @throws NullPointerException if tasks or any of its elements are <tt>null</tt>
+     * @throws RejectedExecutionException if any task cannot be
+     *         scheduled for execution
+     */
+
+    public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks) throws InterruptedException
+    {
+        return executorService_.invokeAll(tasks);
+    }
+
+    /**
+     * Executes the given tasks, returning a list of Futures holding
+     * their status and results
+     * when all complete or the timeout expires, whichever happens first.
+     * {@link Future#isDone} is <tt>true</tt> for each
+     * element of the returned list.
+     * Upon return, tasks that have not completed are cancelled.
+     * Note that a <em>completed</em> task could have
+     * terminated either normally or by throwing an exception.
+     * The results of this method are undefined if the given
+     * collection is modified while this operation is in progress.
+     *
+     * @param tasks the collection of tasks
+     * @param timeout the maximum time to wait
+     * @param unit the time unit of the timeout argument
+     * @return a list of Futures representing the tasks, in the same
+     *         sequential order as produced by the iterator for the
+     *         given task list. If the operation did not time out,
+     *         each task will have completed. If it did time out, some
+     *         of these tasks will not have completed.
+     * @throws InterruptedException if interrupted while waiting, in
+     *         which case unfinished tasks are cancelled
+     * @throws NullPointerException if tasks, any of its elements, or
+     *         unit are <tt>null</tt>
+     * @throws RejectedExecutionException if any task cannot be scheduled
+     *         for execution
+     */
+    public <T> List<Future<T>> invokeAll(Collection<? extends Callable<T>> tasks, long timeout, TimeUnit unit) throws InterruptedException
+    {
+        return executorService_.invokeAll(tasks, timeout, unit);
+    }
+    
+    /**
+     * Executes the given tasks, returning the result
+     * of one that has completed successfully (i.e., without throwing
+     * an exception), if any do. Upon normal or exceptional return,
+     * tasks that have not completed are cancelled.
+     * The results of this method are undefined if the given
+     * collection is modified while this operation is in progress.
+     *
+     * @param tasks the collection of tasks
+     * @return the result returned by one of the tasks
+     * @throws InterruptedException if interrupted while waiting
+     * @throws NullPointerException if tasks or any of its elements
+     *         are <tt>null</tt>
+     * @throws IllegalArgumentException if tasks is empty
+     * @throws ExecutionException if no task successfully completes
+     * @throws RejectedExecutionException if tasks cannot be scheduled
+     *         for execution
+     */
+    public <T> T invokeAny(Collection<? extends Callable<T>> tasks) throws InterruptedException, ExecutionException
+    {
+        return executorService_.invokeAny(tasks);
+    }
+
+    /**
+     * Executes the given tasks, returning the result
+     * of one that has completed successfully (i.e., without throwing
+     * an exception), if any do before the given timeout elapses.
+     * Upon normal or exceptional return, tasks that have not
+     * completed are cancelled.
+     * The results of this method are undefined if the given
+     * collection is modified while this operation is in progress.
+     *
+     * @param tasks the collection of tasks
+     * @param timeout the maximum time to wait
+     * @param unit the time unit of the timeout argument
+     * @return the result returned by one of the tasks.
+     * @throws InterruptedException if interrupted while waiting
+     * @throws NullPointerException if tasks, any of its elements, or
+     *         unit are <tt>null</tt>
+     * @throws TimeoutException if the given timeout elapses before
+     *         any task successfully completes
+     * @throws ExecutionException if no task successfully completes
+     * @throws RejectedExecutionException if tasks cannot be scheduled
+     *         for execution
+     */
+    public <T> T invokeAny(Collection<? extends Callable<T>> tasks, long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException
+    {
+        return executorService_.invokeAny(tasks, timeout, unit);
+    }
+}
diff --git a/src/java/org/apache/cassandra/concurrent/Context.java b/src/java/org/apache/cassandra/concurrent/Context.java
index ae62fefa74..cddde14462 100644
--- a/src/java/org/apache/cassandra/concurrent/Context.java
+++ b/src/java/org/apache/cassandra/concurrent/Context.java
@@ -1,52 +1,52 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import java.util.HashMap;
-import java.util.Map;
-
-/**
- * Context object adding a collection of key/value pairs into ThreadLocalContext.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class Context
-{
-    private Map<Object, Object> ht_;
-    
-    public Context()
-    {
-        ht_ = new HashMap<Object, Object>();
-    }
-    
-    public Object put(Object key, Object value)
-    {
-        return ht_.put(key, value);
-    }
-    
-    public Object get(Object key)
-    {
-        return ht_.get(key);
-    }
-    
-    public void remove(Object key)
-    {
-        ht_.remove(key);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.HashMap;
+import java.util.Map;
+
+/**
+ * Context object adding a collection of key/value pairs into ThreadLocalContext.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Context
+{
+    private Map<Object, Object> ht_;
+    
+    public Context()
+    {
+        ht_ = new HashMap<Object, Object>();
+    }
+    
+    public Object put(Object key, Object value)
+    {
+        return ht_.put(key, value);
+    }
+    
+    public Object get(Object key)
+    {
+        return ht_.get(key);
+    }
+    
+    public void remove(Object key)
+    {
+        ht_.remove(key);
+    }
+}
diff --git a/src/java/org/apache/cassandra/concurrent/ContinuationContext.java b/src/java/org/apache/cassandra/concurrent/ContinuationContext.java
index 98e48f0fce..8279617634 100644
--- a/src/java/org/apache/cassandra/concurrent/ContinuationContext.java
+++ b/src/java/org/apache/cassandra/concurrent/ContinuationContext.java
@@ -1,52 +1,52 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import org.apache.commons.javaflow.Continuation;
-
-public class ContinuationContext
-{
-    private Continuation continuation_;
-    private Object result_;
-    
-    public ContinuationContext(Continuation continuation)
-    {
-        continuation_ = continuation;        
-    }
-    
-    public Continuation getContinuation()
-    {
-        return continuation_;
-    }
-    
-    public void setContinuation(Continuation continuation)
-    {
-        continuation_ = continuation;
-    }
-    
-    public Object result()
-    {
-        return result_;
-    }
-    
-    public void result(Object result)
-    {
-        result_ = result;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import org.apache.commons.javaflow.Continuation;
+
+public class ContinuationContext
+{
+    private Continuation continuation_;
+    private Object result_;
+    
+    public ContinuationContext(Continuation continuation)
+    {
+        continuation_ = continuation;        
+    }
+    
+    public Continuation getContinuation()
+    {
+        return continuation_;
+    }
+    
+    public void setContinuation(Continuation continuation)
+    {
+        continuation_ = continuation;
+    }
+    
+    public Object result()
+    {
+        return result_;
+    }
+    
+    public void result(Object result)
+    {
+        result_ = result;
+    }
+}
diff --git a/src/java/org/apache/cassandra/concurrent/ContinuationStage.java b/src/java/org/apache/cassandra/concurrent/ContinuationStage.java
index c1f6189cff..1be9b8957f 100644
--- a/src/java/org/apache/cassandra/concurrent/ContinuationStage.java
+++ b/src/java/org/apache/cassandra/concurrent/ContinuationStage.java
@@ -1,89 +1,89 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import java.util.concurrent.Callable;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Future;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.ScheduledFuture;
-import java.util.concurrent.TimeUnit;
-
-
-public class ContinuationStage implements IStage
-{
-    private String name_;
-    private ContinuationsExecutor executorService_;
-            
-    public ContinuationStage(String name, int numThreads)
-    {        
-        name_ = name;        
-        executorService_ = new ContinuationsExecutor( numThreads,
-                numThreads,
-                Integer.MAX_VALUE,
-                TimeUnit.SECONDS,
-                new LinkedBlockingQueue<Runnable>(),
-                new ThreadFactoryImpl(name)
-                );        
-    }
-    
-    public String getName() 
-    {        
-        return name_;
-    }
-    
-    public ExecutorService getInternalThreadPool()
-    {
-        return executorService_;
-    }
-
-    public Future<Object> execute(Callable<Object> callable) {
-        return executorService_.submit(callable);
-    }
-    
-    public void execute(Runnable runnable) {
-        executorService_.execute(runnable);
-    }
-    
-    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
-    {
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) {
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) {
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public void shutdown() {  
-        executorService_.shutdownNow(); 
-    }
-    
-    public boolean isShutdown()
-    {
-        return executorService_.isShutdown();
-    }
-    
-    public long getPendingTasks(){
-        return (executorService_.getTaskCount() - executorService_.getCompletedTaskCount());
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+
+
+public class ContinuationStage implements IStage
+{
+    private String name_;
+    private ContinuationsExecutor executorService_;
+            
+    public ContinuationStage(String name, int numThreads)
+    {        
+        name_ = name;        
+        executorService_ = new ContinuationsExecutor( numThreads,
+                numThreads,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl(name)
+                );        
+    }
+    
+    public String getName() 
+    {        
+        return name_;
+    }
+    
+    public ExecutorService getInternalThreadPool()
+    {
+        return executorService_;
+    }
+
+    public Future<Object> execute(Callable<Object> callable) {
+        return executorService_.submit(callable);
+    }
+    
+    public void execute(Runnable runnable) {
+        executorService_.execute(runnable);
+    }
+    
+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
+    {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public void shutdown() {  
+        executorService_.shutdownNow(); 
+    }
+    
+    public boolean isShutdown()
+    {
+        return executorService_.isShutdown();
+    }
+    
+    public long getPendingTasks(){
+        return (executorService_.getTaskCount() - executorService_.getCompletedTaskCount());
+    }
+}
diff --git a/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java b/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java
index dc0eed515a..6b2abfc3d2 100644
--- a/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java
+++ b/src/java/org/apache/cassandra/concurrent/DebuggableScheduledThreadPoolExecutor.java
@@ -1,76 +1,76 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import java.util.concurrent.*;
-
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-import org.apache.cassandra.utils.*;
-
-/**
- * This is a wrapper class for the <i>ScheduledThreadPoolExecutor</i>. It provides an implementation
- * for the <i>afterExecute()</i> found in the <i>ThreadPoolExecutor</i> class to log any unexpected 
- * Runtime Exceptions.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public final class DebuggableScheduledThreadPoolExecutor extends ScheduledThreadPoolExecutor
-{
-    private static Logger logger_ = Logger.getLogger(DebuggableScheduledThreadPoolExecutor.class);
-    
-    public DebuggableScheduledThreadPoolExecutor(int threads,
-            ThreadFactory threadFactory)
-    {
-        super(threads, threadFactory);        
-    }
-    
-    /**
-     *  (non-Javadoc)
-     * @see java.util.concurrent.ThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)
-     */
-    public void afterExecute(Runnable r, Throwable t)
-    {
-        super.afterExecute(r,t);
-        if ( t != null )
-        {  
-            Context ctx = ThreadLocalContext.get();
-            if ( ctx != null )
-            {
-                Object object = ctx.get(r.getClass().getName());
-                
-                if ( object != null )
-                {
-                    logger_.info("**** In afterExecute() " + t.getClass().getName() + " occured while working with " + object + " ****");
-                }
-                else
-                {
-                    logger_.info("**** In afterExecute() " + t.getClass().getName() + " occured ****");
-                }
-            }
-            
-            Throwable cause = t.getCause();
-            if ( cause != null )
-            {
-                logger_.info( LogUtil.throwableToString(cause) );
-            }
-            logger_.info( LogUtil.throwableToString(t) );
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.*;
+
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.utils.*;
+
+/**
+ * This is a wrapper class for the <i>ScheduledThreadPoolExecutor</i>. It provides an implementation
+ * for the <i>afterExecute()</i> found in the <i>ThreadPoolExecutor</i> class to log any unexpected 
+ * Runtime Exceptions.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public final class DebuggableScheduledThreadPoolExecutor extends ScheduledThreadPoolExecutor
+{
+    private static Logger logger_ = Logger.getLogger(DebuggableScheduledThreadPoolExecutor.class);
+    
+    public DebuggableScheduledThreadPoolExecutor(int threads,
+            ThreadFactory threadFactory)
+    {
+        super(threads, threadFactory);        
+    }
+    
+    /**
+     *  (non-Javadoc)
+     * @see java.util.concurrent.ThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)
+     */
+    public void afterExecute(Runnable r, Throwable t)
+    {
+        super.afterExecute(r,t);
+        if ( t != null )
+        {  
+            Context ctx = ThreadLocalContext.get();
+            if ( ctx != null )
+            {
+                Object object = ctx.get(r.getClass().getName());
+                
+                if ( object != null )
+                {
+                    logger_.info("**** In afterExecute() " + t.getClass().getName() + " occured while working with " + object + " ****");
+                }
+                else
+                {
+                    logger_.info("**** In afterExecute() " + t.getClass().getName() + " occured ****");
+                }
+            }
+            
+            Throwable cause = t.getCause();
+            if ( cause != null )
+            {
+                logger_.info( LogUtil.throwableToString(cause) );
+            }
+            logger_.info( LogUtil.throwableToString(t) );
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java b/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
index bd3d985471..21d89798f1 100644
--- a/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
+++ b/src/java/org/apache/cassandra/concurrent/DebuggableThreadPoolExecutor.java
@@ -1,128 +1,128 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import java.util.concurrent.*;
-import java.lang.management.ManagementFactory;
-
-import org.apache.log4j.Logger;
-
-import javax.management.MBeanServer;
-import javax.management.ObjectName;
-
-/**
- * This is a wrapper class for the <i>ScheduledThreadPoolExecutor</i>. It provides an implementation
- * for the <i>afterExecute()</i> found in the <i>ThreadPoolExecutor</i> class to log any unexpected 
- * Runtime Exceptions.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class DebuggableThreadPoolExecutor extends ThreadPoolExecutor implements DebuggableThreadPoolExecutorMBean
-{
-    private static Logger logger_ = Logger.getLogger(DebuggableThreadPoolExecutor.class);
-
-    private ObjectName objName;
-    public DebuggableThreadPoolExecutor(String threadPoolName) 
-    {
-        this(1, 1, Integer.MAX_VALUE, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(), new ThreadFactoryImpl(threadPoolName));
-    }
-
-    public DebuggableThreadPoolExecutor(int corePoolSize,
-            int maximumPoolSize,
-            long keepAliveTime,
-            TimeUnit unit,
-            BlockingQueue<Runnable> workQueue,
-            ThreadFactoryImpl threadFactory)
-    {
-        super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory);
-        super.prestartAllCoreThreads();
-        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
-        try
-        {
-            objName = new ObjectName("org.apache.cassandra.concurrent:type=" + threadFactory.id_);
-            mbs.registerMBean(this, objName);
-        }
-        catch (Exception e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-    
-    public void unregisterMBean()
-    {
-        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
-        try
-        {
-            mbs.unregisterMBean(objName);
-        }
-        catch (Exception e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    public long getPendingTasks()
-    {
-        return getTaskCount() - getCompletedTaskCount();
-    }
-
-    /*
-     * 
-     *  (non-Javadoc)
-     * @see java.util.concurrent.ThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)
-     * Helps us in figuring out why sometimes the threads are getting 
-     * killed and replaced by new ones.
-     */
-    public void afterExecute(Runnable r, Throwable t)
-    {
-        super.afterExecute(r,t);
-
-        if (r instanceof FutureTask) {
-            assert t == null;
-            try
-            {
-                ((FutureTask)r).get();
-            }
-            catch (InterruptedException e)
-            {
-                throw new RuntimeException(e);
-            }
-            catch (ExecutionException e)
-            {
-                t = e;
-            }
-        }
-
-        if ( t != null )
-        {  
-            Context ctx = ThreadLocalContext.get();
-            if ( ctx != null )
-            {
-                Object object = ctx.get(r.getClass().getName());
-                
-                if ( object != null )
-                {
-                    logger_.error("In afterExecute() " + t.getClass().getName() + " occured while working with " + object);
-                }
-            }
-            logger_.error("Error in ThreadPoolExecutor", t);
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.*;
+import java.lang.management.ManagementFactory;
+
+import org.apache.log4j.Logger;
+
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+
+/**
+ * This is a wrapper class for the <i>ScheduledThreadPoolExecutor</i>. It provides an implementation
+ * for the <i>afterExecute()</i> found in the <i>ThreadPoolExecutor</i> class to log any unexpected 
+ * Runtime Exceptions.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class DebuggableThreadPoolExecutor extends ThreadPoolExecutor implements DebuggableThreadPoolExecutorMBean
+{
+    private static Logger logger_ = Logger.getLogger(DebuggableThreadPoolExecutor.class);
+
+    private ObjectName objName;
+    public DebuggableThreadPoolExecutor(String threadPoolName) 
+    {
+        this(1, 1, Integer.MAX_VALUE, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>(), new ThreadFactoryImpl(threadPoolName));
+    }
+
+    public DebuggableThreadPoolExecutor(int corePoolSize,
+            int maximumPoolSize,
+            long keepAliveTime,
+            TimeUnit unit,
+            BlockingQueue<Runnable> workQueue,
+            ThreadFactoryImpl threadFactory)
+    {
+        super(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue, threadFactory);
+        super.prestartAllCoreThreads();
+        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
+        try
+        {
+            objName = new ObjectName("org.apache.cassandra.concurrent:type=" + threadFactory.id_);
+            mbs.registerMBean(this, objName);
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+    
+    public void unregisterMBean()
+    {
+        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
+        try
+        {
+            mbs.unregisterMBean(objName);
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    public long getPendingTasks()
+    {
+        return getTaskCount() - getCompletedTaskCount();
+    }
+
+    /*
+     * 
+     *  (non-Javadoc)
+     * @see java.util.concurrent.ThreadPoolExecutor#afterExecute(java.lang.Runnable, java.lang.Throwable)
+     * Helps us in figuring out why sometimes the threads are getting 
+     * killed and replaced by new ones.
+     */
+    public void afterExecute(Runnable r, Throwable t)
+    {
+        super.afterExecute(r,t);
+
+        if (r instanceof FutureTask) {
+            assert t == null;
+            try
+            {
+                ((FutureTask)r).get();
+            }
+            catch (InterruptedException e)
+            {
+                throw new RuntimeException(e);
+            }
+            catch (ExecutionException e)
+            {
+                t = e;
+            }
+        }
+
+        if ( t != null )
+        {  
+            Context ctx = ThreadLocalContext.get();
+            if ( ctx != null )
+            {
+                Object object = ctx.get(r.getClass().getName());
+                
+                if ( object != null )
+                {
+                    logger_.error("In afterExecute() " + t.getClass().getName() + " occured while working with " + object);
+                }
+            }
+            logger_.error("Error in ThreadPoolExecutor", t);
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/concurrent/IContinuable.java b/src/java/org/apache/cassandra/concurrent/IContinuable.java
index f3511121e1..99e61363d9 100644
--- a/src/java/org/apache/cassandra/concurrent/IContinuable.java
+++ b/src/java/org/apache/cassandra/concurrent/IContinuable.java
@@ -1,26 +1,26 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import org.apache.commons.javaflow.Continuation;
-
-public interface IContinuable
-{
-    public void run(Continuation c);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import org.apache.commons.javaflow.Continuation;
+
+public interface IContinuable
+{
+    public void run(Continuation c);
+}
diff --git a/src/java/org/apache/cassandra/concurrent/IStage.java b/src/java/org/apache/cassandra/concurrent/IStage.java
index f33e3cb228..0fb74cac6a 100644
--- a/src/java/org/apache/cassandra/concurrent/IStage.java
+++ b/src/java/org/apache/cassandra/concurrent/IStage.java
@@ -1,120 +1,120 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import java.util.concurrent.*;
-
-/**
- * An abstraction for stages as described in the SEDA paper by Matt Welsh. 
- * For reference to the paper look over here 
- * <a href="http://www.eecs.harvard.edu/~mdw/papers/seda-sosp01.pdf">SEDA: An Architecture for WellConditioned,
-   Scalable Internet Services</a>.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IStage 
-{
-    /**
-     * Get the name of the associated stage.
-     * @return name of the associated stage.
-     */
-    public String getName();
-    
-    /**
-     * Get the thread pool used by this stage 
-     * internally.
-     */
-    public ExecutorService getInternalThreadPool();
-    
-    /**
-     * This method is used to execute a piece of code on
-     * this stage. The idea is that the <i>run()</i> method
-     * of this Runnable instance is invoked on a thread from a
-     * thread pool that belongs to this stage.
-     * @param runnable instance whose run() method needs to be invoked.
-     */
-    public void execute(Runnable runnable);
-    
-    /**
-     * This method is used to execute a piece of code on
-     * this stage which returns a Future pointer. The idea
-     * is that the <i>call()</i> method of this Runnable 
-     * instance is invoked on a thread from a thread pool 
-     * that belongs to this stage.
-     
-     * @param callable instance that needs to be invoked.
-     * @return the future return object from the callable.
-     */
-    public Future<Object> execute(Callable<Object> callable);
-    
-    /**
-     * This method is used to submit tasks to this stage
-     * that execute periodically. 
-     * 
-     * @param command the task to execute.
-     * @param delay the time to delay first execution 
-     * @param unit the time unit of the initialDelay and period parameters 
-     * @return the future return object from the runnable.
-     */
-    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit); 
-      
-    /**
-     * This method is used to submit tasks to this stage
-     * that execute periodically. 
-     * @param command the task to execute.
-     * @param initialDelay the time to delay first execution
-     * @param period the period between successive executions
-     * @param unit the time unit of the initialDelay and period parameters 
-     * @return the future return object from the runnable.
-     */
-    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit); 
-    
-    /**
-     * This method is used to submit tasks to this stage
-     * that execute periodically. 
-     * @param command the task to execute.
-     * @param initialDelay the time to delay first execution
-     * @param delay  the delay between the termination of one execution and the commencement of the next.
-     * @param unit the time unit of the initialDelay and delay parameters 
-     * @return the future return object from the runnable.
-     */
-    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit);
-    
-    /**
-     * Shutdown the stage. All the threads of this stage
-     * are forcefully shutdown. Any pending tasks on this
-     * stage could be dropped or the stage could wait for 
-     * these tasks to be completed. This is however an 
-     * implementation detail.
-     */
-    public void shutdown();  
-    
-    /**
-     * Checks if the stage has been shutdown.
-     * @return true if shut down, otherwise false.
-     */
-    public boolean isShutdown();
-    
-    /**
-     * This method returns the number of tasks that are 
-     * pending on this stage to be executed.
-     * @return task count.
-     */
-    public long getPendingTasks();
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.*;
+
+/**
+ * An abstraction for stages as described in the SEDA paper by Matt Welsh. 
+ * For reference to the paper look over here 
+ * <a href="http://www.eecs.harvard.edu/~mdw/papers/seda-sosp01.pdf">SEDA: An Architecture for WellConditioned,
+   Scalable Internet Services</a>.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IStage 
+{
+    /**
+     * Get the name of the associated stage.
+     * @return name of the associated stage.
+     */
+    public String getName();
+    
+    /**
+     * Get the thread pool used by this stage 
+     * internally.
+     */
+    public ExecutorService getInternalThreadPool();
+    
+    /**
+     * This method is used to execute a piece of code on
+     * this stage. The idea is that the <i>run()</i> method
+     * of this Runnable instance is invoked on a thread from a
+     * thread pool that belongs to this stage.
+     * @param runnable instance whose run() method needs to be invoked.
+     */
+    public void execute(Runnable runnable);
+    
+    /**
+     * This method is used to execute a piece of code on
+     * this stage which returns a Future pointer. The idea
+     * is that the <i>call()</i> method of this Runnable 
+     * instance is invoked on a thread from a thread pool 
+     * that belongs to this stage.
+     
+     * @param callable instance that needs to be invoked.
+     * @return the future return object from the callable.
+     */
+    public Future<Object> execute(Callable<Object> callable);
+    
+    /**
+     * This method is used to submit tasks to this stage
+     * that execute periodically. 
+     * 
+     * @param command the task to execute.
+     * @param delay the time to delay first execution 
+     * @param unit the time unit of the initialDelay and period parameters 
+     * @return the future return object from the runnable.
+     */
+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit); 
+      
+    /**
+     * This method is used to submit tasks to this stage
+     * that execute periodically. 
+     * @param command the task to execute.
+     * @param initialDelay the time to delay first execution
+     * @param period the period between successive executions
+     * @param unit the time unit of the initialDelay and period parameters 
+     * @return the future return object from the runnable.
+     */
+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit); 
+    
+    /**
+     * This method is used to submit tasks to this stage
+     * that execute periodically. 
+     * @param command the task to execute.
+     * @param initialDelay the time to delay first execution
+     * @param delay  the delay between the termination of one execution and the commencement of the next.
+     * @param unit the time unit of the initialDelay and delay parameters 
+     * @return the future return object from the runnable.
+     */
+    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit);
+    
+    /**
+     * Shutdown the stage. All the threads of this stage
+     * are forcefully shutdown. Any pending tasks on this
+     * stage could be dropped or the stage could wait for 
+     * these tasks to be completed. This is however an 
+     * implementation detail.
+     */
+    public void shutdown();  
+    
+    /**
+     * Checks if the stage has been shutdown.
+     * @return true if shut down, otherwise false.
+     */
+    public boolean isShutdown();
+    
+    /**
+     * This method returns the number of tasks that are 
+     * pending on this stage to be executed.
+     * @return task count.
+     */
+    public long getPendingTasks();
+}
diff --git a/src/java/org/apache/cassandra/concurrent/MultiThreadedStage.java b/src/java/org/apache/cassandra/concurrent/MultiThreadedStage.java
index bbdcad62c3..8435bf558b 100644
--- a/src/java/org/apache/cassandra/concurrent/MultiThreadedStage.java
+++ b/src/java/org/apache/cassandra/concurrent/MultiThreadedStage.java
@@ -1,98 +1,98 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-
-package org.apache.cassandra.concurrent;
-
-import java.util.concurrent.*;
-
-import javax.naming.OperationNotSupportedException;
-
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * This class is an implementation of the <i>IStage</i> interface. In particular
- * it is for a stage that has a thread pool with multiple threads. For details 
- * please refer to the <i>IStage</i> documentation.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class MultiThreadedStage implements IStage 
-{    
-    private String name_;
-    private DebuggableThreadPoolExecutor executorService_;
-            
-    public MultiThreadedStage(String name, int numThreads)
-    {        
-        name_ = name;        
-        executorService_ = new DebuggableThreadPoolExecutor( numThreads,
-                numThreads,
-                Integer.MAX_VALUE,
-                TimeUnit.SECONDS,
-                new LinkedBlockingQueue<Runnable>(),
-                new ThreadFactoryImpl(name)
-                );        
-    }
-    
-    public String getName() 
-    {        
-        return name_;
-    }    
-    
-    public ExecutorService getInternalThreadPool()
-    {
-        return executorService_;
-    }
-
-    public Future<Object> execute(Callable<Object> callable) {
-        return executorService_.submit(callable);
-    }
-    
-    public void execute(Runnable runnable) {
-        executorService_.execute(runnable);
-    }
-    
-    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
-    {
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) {
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) {
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public void shutdown() {  
-        executorService_.shutdownNow(); 
-    }
-    
-    public boolean isShutdown()
-    {
-        return executorService_.isShutdown();
-    }
-    
-    public long getPendingTasks(){
-        return executorService_.getPendingTasks();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.*;
+
+import javax.naming.OperationNotSupportedException;
+
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * This class is an implementation of the <i>IStage</i> interface. In particular
+ * it is for a stage that has a thread pool with multiple threads. For details 
+ * please refer to the <i>IStage</i> documentation.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MultiThreadedStage implements IStage 
+{    
+    private String name_;
+    private DebuggableThreadPoolExecutor executorService_;
+            
+    public MultiThreadedStage(String name, int numThreads)
+    {        
+        name_ = name;        
+        executorService_ = new DebuggableThreadPoolExecutor( numThreads,
+                numThreads,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl(name)
+                );        
+    }
+    
+    public String getName() 
+    {        
+        return name_;
+    }    
+    
+    public ExecutorService getInternalThreadPool()
+    {
+        return executorService_;
+    }
+
+    public Future<Object> execute(Callable<Object> callable) {
+        return executorService_.submit(callable);
+    }
+    
+    public void execute(Runnable runnable) {
+        executorService_.execute(runnable);
+    }
+    
+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
+    {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit) {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit) {
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public void shutdown() {  
+        executorService_.shutdownNow(); 
+    }
+    
+    public boolean isShutdown()
+    {
+        return executorService_.isShutdown();
+    }
+    
+    public long getPendingTasks(){
+        return executorService_.getPendingTasks();
+    }
+}
diff --git a/src/java/org/apache/cassandra/concurrent/RejectedExecutionHandler.java b/src/java/org/apache/cassandra/concurrent/RejectedExecutionHandler.java
index b62ea76989..0ac88d4440 100644
--- a/src/java/org/apache/cassandra/concurrent/RejectedExecutionHandler.java
+++ b/src/java/org/apache/cassandra/concurrent/RejectedExecutionHandler.java
@@ -1,24 +1,24 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-interface RejectedExecutionHandler
-{
-    public void rejectedExecution(Runnable r, ContinuationsExecutor executor); 
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+interface RejectedExecutionHandler
+{
+    public void rejectedExecution(Runnable r, ContinuationsExecutor executor); 
+}
diff --git a/src/java/org/apache/cassandra/concurrent/SingleThreadedContinuationStage.java b/src/java/org/apache/cassandra/concurrent/SingleThreadedContinuationStage.java
index fa1fcff2fe..2c858990d3 100644
--- a/src/java/org/apache/cassandra/concurrent/SingleThreadedContinuationStage.java
+++ b/src/java/org/apache/cassandra/concurrent/SingleThreadedContinuationStage.java
@@ -1,100 +1,100 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import java.util.concurrent.Callable;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Future;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.ScheduledFuture;
-import java.util.concurrent.TimeUnit;
-
-public class SingleThreadedContinuationStage implements IStage 
-{
-    protected ContinuationsExecutor executorService_;
-    private String name_;
-
-    public SingleThreadedContinuationStage(String name)
-    {        
-        executorService_ = new ContinuationsExecutor( 1,
-                1,
-                Integer.MAX_VALUE,
-                TimeUnit.SECONDS,
-                new LinkedBlockingQueue<Runnable>(),
-                new ThreadFactoryImpl(name)
-                );        
-        name_ = name;        
-    }
-    
-    /* Implementing the IStage interface methods */
-    
-    public String getName()
-    {
-        return name_;
-    }
-    
-    public ExecutorService getInternalThreadPool()
-    {
-        return executorService_;
-    }
-    
-    public void execute(Runnable runnable)
-    {
-        executorService_.execute(runnable);
-    }
-    
-    public Future<Object> execute(Callable<Object> callable)
-    {
-        return executorService_.submit(callable);
-    }
-    
-    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
-    {
-        //return executorService_.schedule(command, delay, unit);
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit)
-    {
-        //return executorService_.scheduleAtFixedRate(command, initialDelay, period, unit);
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit)
-    {
-        //return executorService_.scheduleWithFixedDelay(command, initialDelay, delay, unit);
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public void shutdown()
-    {
-        executorService_.shutdownNow();
-    }
-    
-    public boolean isShutdown()
-    {
-        return executorService_.isShutdown();
-    }    
-    
-    public long getPendingTasks(){
-        return (executorService_.getTaskCount() - executorService_.getCompletedTaskCount());
-    }
-    /* Finished implementing the IStage interface methods */
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+
+public class SingleThreadedContinuationStage implements IStage 
+{
+    protected ContinuationsExecutor executorService_;
+    private String name_;
+
+    public SingleThreadedContinuationStage(String name)
+    {        
+        executorService_ = new ContinuationsExecutor( 1,
+                1,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl(name)
+                );        
+        name_ = name;        
+    }
+    
+    /* Implementing the IStage interface methods */
+    
+    public String getName()
+    {
+        return name_;
+    }
+    
+    public ExecutorService getInternalThreadPool()
+    {
+        return executorService_;
+    }
+    
+    public void execute(Runnable runnable)
+    {
+        executorService_.execute(runnable);
+    }
+    
+    public Future<Object> execute(Callable<Object> callable)
+    {
+        return executorService_.submit(callable);
+    }
+    
+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
+    {
+        //return executorService_.schedule(command, delay, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit)
+    {
+        //return executorService_.scheduleAtFixedRate(command, initialDelay, period, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit)
+    {
+        //return executorService_.scheduleWithFixedDelay(command, initialDelay, delay, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public void shutdown()
+    {
+        executorService_.shutdownNow();
+    }
+    
+    public boolean isShutdown()
+    {
+        return executorService_.isShutdown();
+    }    
+    
+    public long getPendingTasks(){
+        return (executorService_.getTaskCount() - executorService_.getCompletedTaskCount());
+    }
+    /* Finished implementing the IStage interface methods */
+}
+
diff --git a/src/java/org/apache/cassandra/concurrent/SingleThreadedStage.java b/src/java/org/apache/cassandra/concurrent/SingleThreadedStage.java
index 24480d33f6..9d6e14080f 100644
--- a/src/java/org/apache/cassandra/concurrent/SingleThreadedStage.java
+++ b/src/java/org/apache/cassandra/concurrent/SingleThreadedStage.java
@@ -1,100 +1,100 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import java.util.concurrent.Callable;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Future;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.ScheduledFuture;
-import java.util.concurrent.TimeUnit;
-import org.apache.cassandra.net.*;
-
-/**
- * This class is an implementation of the <i>IStage</i> interface. In particular
- * it is for a stage that has a thread pool with a single thread. For details 
- * please refer to the <i>IStage</i> documentation.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class SingleThreadedStage implements IStage 
-{
-    protected DebuggableThreadPoolExecutor executorService_;
-    private String name_;
-
-	public SingleThreadedStage(String name)
-    {
-        executorService_ = new DebuggableThreadPoolExecutor(name);
-        name_ = name;
-	}
-	
-    /* Implementing the IStage interface methods */
-    
-    public String getName()
-    {
-        return name_;
-    }
-    
-    public ExecutorService getInternalThreadPool()
-    {
-        return executorService_;
-    }
-    
-    public void execute(Runnable runnable)
-    {
-        executorService_.execute(runnable);
-    }
-    
-    public Future<Object> execute(Callable<Object> callable)
-    {
-        return executorService_.submit(callable);
-    }
-    
-    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
-    {
-        //return executorService_.schedule(command, delay, unit);
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit)
-    {
-        //return executorService_.scheduleAtFixedRate(command, initialDelay, period, unit);
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit)
-    {
-        //return executorService_.scheduleWithFixedDelay(command, initialDelay, delay, unit);
-        throw new UnsupportedOperationException("This operation is not supported");
-    }
-    
-    public void shutdown()
-    {
-        executorService_.shutdownNow();
-    }
-    
-    public boolean isShutdown()
-    {
-        return executorService_.isShutdown();
-    }    
-    
-    public long getPendingTasks(){
-        return executorService_.getPendingTasks();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledFuture;
+import java.util.concurrent.TimeUnit;
+import org.apache.cassandra.net.*;
+
+/**
+ * This class is an implementation of the <i>IStage</i> interface. In particular
+ * it is for a stage that has a thread pool with a single thread. For details 
+ * please refer to the <i>IStage</i> documentation.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SingleThreadedStage implements IStage 
+{
+    protected DebuggableThreadPoolExecutor executorService_;
+    private String name_;
+
+	public SingleThreadedStage(String name)
+    {
+        executorService_ = new DebuggableThreadPoolExecutor(name);
+        name_ = name;
+	}
+	
+    /* Implementing the IStage interface methods */
+    
+    public String getName()
+    {
+        return name_;
+    }
+    
+    public ExecutorService getInternalThreadPool()
+    {
+        return executorService_;
+    }
+    
+    public void execute(Runnable runnable)
+    {
+        executorService_.execute(runnable);
+    }
+    
+    public Future<Object> execute(Callable<Object> callable)
+    {
+        return executorService_.submit(callable);
+    }
+    
+    public ScheduledFuture<?> schedule(Runnable command, long delay, TimeUnit unit)
+    {
+        //return executorService_.schedule(command, delay, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleAtFixedRate(Runnable command, long initialDelay, long period, TimeUnit unit)
+    {
+        //return executorService_.scheduleAtFixedRate(command, initialDelay, period, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public ScheduledFuture<?> scheduleWithFixedDelay(Runnable command, long initialDelay, long delay, TimeUnit unit)
+    {
+        //return executorService_.scheduleWithFixedDelay(command, initialDelay, delay, unit);
+        throw new UnsupportedOperationException("This operation is not supported");
+    }
+    
+    public void shutdown()
+    {
+        executorService_.shutdownNow();
+    }
+    
+    public boolean isShutdown()
+    {
+        return executorService_.isShutdown();
+    }    
+    
+    public long getPendingTasks(){
+        return executorService_.getPendingTasks();
+    }
+}
diff --git a/src/java/org/apache/cassandra/concurrent/StageManager.java b/src/java/org/apache/cassandra/concurrent/StageManager.java
index f771588d6d..9762740869 100644
--- a/src/java/org/apache/cassandra/concurrent/StageManager.java
+++ b/src/java/org/apache/cassandra/concurrent/StageManager.java
@@ -1,117 +1,117 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.ExecutorService;
-
-
-/**
- * This class manages all stages that exist within a process. The application registers
- * and de-registers stages with this abstraction. Any component that has the <i>ID</i> 
- * associated with a stage can obtain a handle to actual stage.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class StageManager
-{
-    private static Map<String, IStage > stageQueues_ = new HashMap<String, IStage>();
-    
-    /**
-     * Register a stage with the StageManager
-     * @param stageName stage name.
-     * @param stage stage for the respective message types.
-     */
-    public static void registerStage(String stageName, IStage stage)
-    {
-        stageQueues_.put(stageName, stage);
-    }
-    
-    /**
-     * Returns the stage that we are currently executing on.
-     * This relies on the fact that the thread names in the
-     * stage have the name of the stage as the prefix.
-     * @return Returns the stage that we are currently executing on.
-     */
-    public static IStage getCurrentStage()
-    {
-        String name = Thread.currentThread().getName();
-        String[] peices = name.split(":");
-        IStage stage = getStage(peices[0]);
-        return stage;
-    }
-
-    /**
-     * Retrieve a stage from the StageManager
-     * @param stageName name of the stage to be retrieved.
-    */
-    public static IStage getStage(String stageName)
-    {
-        return stageQueues_.get(stageName);
-    }
-    
-    /**
-     * Retrieve the internal thread pool associated with the
-     * specified stage name.
-     * @param stageName name of the stage.
-     */
-    public static ExecutorService getStageInternalThreadPool(String stageName)
-    {
-        IStage stage = getStage(stageName);
-        if ( stage == null )
-            throw new IllegalArgumentException("No stage registered with name " + stageName);
-        return stage.getInternalThreadPool();
-    }
-
-    /**
-     * Deregister a stage from StageManager
-     * @param stageName stage name.
-     */
-    public static void deregisterStage(String stageName)
-    {
-        stageQueues_.remove(stageName);
-    }
-
-    /**
-     * This method gets the number of tasks on the
-     * stage's internal queue.
-     * @param stage name of the stage
-     * @return stage task count.
-     */
-    public static long getStageTaskCount(String stage)
-    {
-        return stageQueues_.get(stage).getPendingTasks();
-    }
-
-    /**
-     * This method shuts down all registered stages.
-     */
-    public static void shutdown()
-    {
-        Set<String> stages = stageQueues_.keySet();
-        for ( String stage : stages )
-        {
-            IStage registeredStage = stageQueues_.get(stage);
-            registeredStage.shutdown();
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ExecutorService;
+
+
+/**
+ * This class manages all stages that exist within a process. The application registers
+ * and de-registers stages with this abstraction. Any component that has the <i>ID</i> 
+ * associated with a stage can obtain a handle to actual stage.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class StageManager
+{
+    private static Map<String, IStage > stageQueues_ = new HashMap<String, IStage>();
+    
+    /**
+     * Register a stage with the StageManager
+     * @param stageName stage name.
+     * @param stage stage for the respective message types.
+     */
+    public static void registerStage(String stageName, IStage stage)
+    {
+        stageQueues_.put(stageName, stage);
+    }
+    
+    /**
+     * Returns the stage that we are currently executing on.
+     * This relies on the fact that the thread names in the
+     * stage have the name of the stage as the prefix.
+     * @return Returns the stage that we are currently executing on.
+     */
+    public static IStage getCurrentStage()
+    {
+        String name = Thread.currentThread().getName();
+        String[] peices = name.split(":");
+        IStage stage = getStage(peices[0]);
+        return stage;
+    }
+
+    /**
+     * Retrieve a stage from the StageManager
+     * @param stageName name of the stage to be retrieved.
+    */
+    public static IStage getStage(String stageName)
+    {
+        return stageQueues_.get(stageName);
+    }
+    
+    /**
+     * Retrieve the internal thread pool associated with the
+     * specified stage name.
+     * @param stageName name of the stage.
+     */
+    public static ExecutorService getStageInternalThreadPool(String stageName)
+    {
+        IStage stage = getStage(stageName);
+        if ( stage == null )
+            throw new IllegalArgumentException("No stage registered with name " + stageName);
+        return stage.getInternalThreadPool();
+    }
+
+    /**
+     * Deregister a stage from StageManager
+     * @param stageName stage name.
+     */
+    public static void deregisterStage(String stageName)
+    {
+        stageQueues_.remove(stageName);
+    }
+
+    /**
+     * This method gets the number of tasks on the
+     * stage's internal queue.
+     * @param stage name of the stage
+     * @return stage task count.
+     */
+    public static long getStageTaskCount(String stage)
+    {
+        return stageQueues_.get(stage).getPendingTasks();
+    }
+
+    /**
+     * This method shuts down all registered stages.
+     */
+    public static void shutdown()
+    {
+        Set<String> stages = stageQueues_.keySet();
+        for ( String stage : stages )
+        {
+            IStage registeredStage = stageQueues_.get(stage);
+            registeredStage.shutdown();
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/concurrent/ThreadFactoryImpl.java b/src/java/org/apache/cassandra/concurrent/ThreadFactoryImpl.java
index 7a024576c9..ae71d8b5bd 100644
--- a/src/java/org/apache/cassandra/concurrent/ThreadFactoryImpl.java
+++ b/src/java/org/apache/cassandra/concurrent/ThreadFactoryImpl.java
@@ -1,51 +1,51 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-import java.util.concurrent.*;
-import java.util.concurrent.atomic.*;
-import org.apache.cassandra.utils.*;
-
-/**
- * This class is an implementation of the <i>ThreadFactory</i> interface. This 
- * is useful to give Java threads meaningful names which is useful when using 
- * a tool like JConsole.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class ThreadFactoryImpl implements ThreadFactory
-{
-    protected String id_;
-    protected ThreadGroup threadGroup_;
-    protected final AtomicInteger threadNbr_ = new AtomicInteger(1);
-    
-    public ThreadFactoryImpl(String id)
-    {
-        SecurityManager sm = System.getSecurityManager();
-        threadGroup_ = ( sm != null ) ? sm.getThreadGroup() : Thread.currentThread().getThreadGroup();
-        id_ = id;
-    }    
-    
-    public Thread newThread(Runnable runnable)
-    {        
-        String name = id_ + ":" + threadNbr_.getAndIncrement();       
-        Thread thread = new Thread(threadGroup_, runnable, name);        
-        return thread;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+import java.util.concurrent.*;
+import java.util.concurrent.atomic.*;
+import org.apache.cassandra.utils.*;
+
+/**
+ * This class is an implementation of the <i>ThreadFactory</i> interface. This 
+ * is useful to give Java threads meaningful names which is useful when using 
+ * a tool like JConsole.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ThreadFactoryImpl implements ThreadFactory
+{
+    protected String id_;
+    protected ThreadGroup threadGroup_;
+    protected final AtomicInteger threadNbr_ = new AtomicInteger(1);
+    
+    public ThreadFactoryImpl(String id)
+    {
+        SecurityManager sm = System.getSecurityManager();
+        threadGroup_ = ( sm != null ) ? sm.getThreadGroup() : Thread.currentThread().getThreadGroup();
+        id_ = id;
+    }    
+    
+    public Thread newThread(Runnable runnable)
+    {        
+        String name = id_ + ":" + threadNbr_.getAndIncrement();       
+        Thread thread = new Thread(threadGroup_, runnable, name);        
+        return thread;
+    }
+}
diff --git a/src/java/org/apache/cassandra/concurrent/ThreadLocalContext.java b/src/java/org/apache/cassandra/concurrent/ThreadLocalContext.java
index 22acc6915f..479992790f 100644
--- a/src/java/org/apache/cassandra/concurrent/ThreadLocalContext.java
+++ b/src/java/org/apache/cassandra/concurrent/ThreadLocalContext.java
@@ -1,42 +1,42 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.concurrent;
-
-/**
- * Use this implementation over Java's ThreadLocal or InheritableThreadLocal when 
- * you need to add multiple key/value pairs into ThreadLocalContext for a given thread.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-
-public class ThreadLocalContext
-{
-    private static InheritableThreadLocal<Context> tls_ = new InheritableThreadLocal<Context>();
-
-    public static void put(Context value)
-    {
-        tls_.set(value);
-    }
-
-    public static Context get()
-    {
-        return tls_.get();
-    }
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.concurrent;
+
+/**
+ * Use this implementation over Java's ThreadLocal or InheritableThreadLocal when 
+ * you need to add multiple key/value pairs into ThreadLocalContext for a given thread.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+
+public class ThreadLocalContext
+{
+    private static InheritableThreadLocal<Context> tls_ = new InheritableThreadLocal<Context>();
+
+    public static void put(Context value)
+    {
+        tls_.set(value);
+    }
+
+    public static Context get()
+    {
+        return tls_.get();
+    }
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java b/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
index 5ceb0b45a1..2e56015b91 100644
--- a/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
+++ b/src/java/org/apache/cassandra/config/DatabaseDescriptor.java
@@ -1,905 +1,905 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.config;
-
-import java.util.*;
-import java.io.*;
-import java.lang.reflect.InvocationTargetException;
-
-import javax.xml.transform.TransformerException;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.db.*;
-import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.db.marshal.AsciiType;
-import org.apache.cassandra.db.marshal.UTF8Type;
-import org.apache.cassandra.db.marshal.BytesType;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.locator.IEndPointSnitch;
-import org.apache.cassandra.utils.FileUtils;
-import org.apache.cassandra.utils.XMLUtils;
-import org.w3c.dom.Node;
-import org.w3c.dom.NodeList;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class DatabaseDescriptor
-{
-    private static Logger logger_ = Logger.getLogger(DatabaseDescriptor.class);
-
-    public static final String random_ = "RANDOM";
-    public static final String ophf_ = "OPHF";
-    private static int storagePort_ = 7000;
-    private static int controlPort_ = 7001;
-    private static int thriftPort_ = 9160;
-    private static String listenAddress_; // leave null so we can fall through to getLocalHost
-    private static String thriftAddress_;
-    private static String clusterName_ = "Test";
-    private static int replicationFactor_ = 3;
-    private static long rpcTimeoutInMillis_ = 2000;
-    private static Set<String> seeds_ = new HashSet<String>();
-    /* Keeps the list of data file directories */
-    private static String[] dataFileDirectories_;
-    /* Current index into the above list of directories */
-    private static int currentIndex_ = 0;
-    private static String logFileDirectory_;
-    private static String bootstrapFileDirectory_;
-    private static int consistencyThreads_ = 4; // not configurable
-    private static int concurrentReaders_ = 8;
-    private static int concurrentWriters_ = 32;
-    private static List<String> tables_ = new ArrayList<String>();
-    private static Set<String> applicationColumnFamilies_ = new HashSet<String>();
-
-    // Default descriptive names for use in CQL. The user can override
-    // these choices in the config file. These are not case sensitive.
-    // Hence, these are stored in UPPER case for easy comparison.
-    private static String d_rowKey_           = "ROW_KEY";
-    private static String d_superColumnMap_   = "SUPER_COLUMN_MAP";
-    private static String d_superColumnKey_   = "SUPER_COLUMN_KEY";
-    private static String d_columnMap_        = "COLUMN_MAP";
-    private static String d_columnKey_        = "COLUMN_KEY";
-    private static String d_columnValue_      = "COLUMN_VALUE";
-    private static String d_columnTimestamp_  = "COLUMN_TIMESTAMP";
-
-    private static Map<String, Double> tableKeysCachedFractions_;
-    /*
-     * A map from table names to the set of column families for the table and the
-     * corresponding meta data for that column family.
-    */
-    private static Map<String, Map<String, CFMetaData>> tableToCFMetaDataMap_;
-    /* Hashing strategy Random or OPHF */
-    private static IPartitioner partitioner_;
-
-    private static IEndPointSnitch endPointSnitch_;
-
-    private static Class replicaPlacementStrategyClass_;
-
-    /* if the size of columns or super-columns are more than this, indexing will kick in */
-    private static int columnIndexSizeInKB_;
-    /* Number of hours to keep a memtable in memory */
-    private static int memtableLifetime_ = 6;
-    /* Size of the memtable in memory before it is dumped */
-    private static int memtableSize_ = 128;
-    /* Number of objects in millions in the memtable before it is dumped */
-    private static double memtableObjectCount_ = 1;
-    /* 
-     * This parameter enables or disables consistency checks. 
-     * If set to false the read repairs are disable for very
-     * high throughput on reads but at the cost of consistency.
-    */
-    private static boolean doConsistencyCheck_ = true;
-    /* Callout directories */
-    private static String calloutLocation_;
-    /* Job Jar Location */
-    private static String jobJarFileLocation_;
-    /* Address where to run the job tracker */
-    private static String jobTrackerHost_;    
-    /* time to wait before garbage collecting tombstones (deletion markers) */
-    private static int gcGraceInSeconds_ = 10 * 24 * 3600; // 10 days
-
-    // the path qualified config file (storage-conf.xml) name
-    private static String configFileName_;
-    /* initial token in the ring */
-    private static String initialToken_ = null;
-
-    private static boolean commitLogSync_;
-
-    private static int commitLogSyncDelay_;
-
-    static
-    {
-        try
-        {
-            configFileName_ = System.getProperty("storage-config") + File.separator + "storage-conf.xml";
-            if (logger_.isDebugEnabled())
-                logger_.debug("Loading settings from " + configFileName_);
-            XMLUtils xmlUtils = new XMLUtils(configFileName_);
-
-            /* Cluster Name */
-            clusterName_ = xmlUtils.getNodeValue("/Storage/ClusterName");
-
-            String syncRaw = xmlUtils.getNodeValue("/Storage/CommitLogSync");
-            if (!"false".equals(syncRaw) && !"true".equals(syncRaw))
-            {
-                // Bool.valueOf will silently assume false for values it doesn't recognize
-                throw new ConfigurationException("Unrecognized value for CommitLogSync.  Use 'true' or 'false'.");
-            }
-            commitLogSync_ = Boolean.valueOf(xmlUtils.getNodeValue("/Storage/CommitLogSync"));
-
-            commitLogSyncDelay_ = Integer.valueOf(xmlUtils.getNodeValue("/Storage/CommitLogSyncDelay"));
-
-            /* Hashing strategy */
-            String partitionerClassName = xmlUtils.getNodeValue("/Storage/Partitioner");
-            if (partitionerClassName == null)
-            {
-                throw new ConfigurationException("Missing partitioner directive /Storage/Partitioner");
-            }
-            try
-            {
-                Class cls = Class.forName(partitionerClassName);
-                partitioner_ = (IPartitioner) cls.getConstructor().newInstance();
-            }
-            catch (ClassNotFoundException e)
-            {
-                throw new ConfigurationException("Invalid partitioner class " + partitionerClassName);
-            }
-
-            /* end point snitch */
-            String endPointSnitchClassName = xmlUtils.getNodeValue("/Storage/EndPointSnitch");
-            if (endPointSnitchClassName == null)
-            {
-                throw new ConfigurationException("Missing endpointsnitch directive /Storage/EndPointSnitch");
-            }
-            try
-            {
-                Class cls = Class.forName(endPointSnitchClassName);
-                endPointSnitch_ = (IEndPointSnitch) cls.getConstructor().newInstance();
-            }
-            catch (ClassNotFoundException e)
-            {
-                throw new ConfigurationException("Invalid endpointsnitch class " + endPointSnitchClassName);
-            }
-            
-            /* Callout location */
-            calloutLocation_ = xmlUtils.getNodeValue("/Storage/CalloutLocation");
-
-            /* JobTracker address */
-            jobTrackerHost_ = xmlUtils.getNodeValue("/Storage/JobTrackerHost");
-
-            /* Job Jar file location */
-            jobJarFileLocation_ = xmlUtils.getNodeValue("/Storage/JobJarFileLocation");
-
-            String gcGrace = xmlUtils.getNodeValue("/Storage/GCGraceSeconds");
-            if ( gcGrace != null )
-                gcGraceInSeconds_ = Integer.parseInt(gcGrace);
-
-            initialToken_ = xmlUtils.getNodeValue("/Storage/InitialToken");
-
-            /* Data replication factor */
-            String replicationFactor = xmlUtils.getNodeValue("/Storage/ReplicationFactor");
-            if ( replicationFactor != null )
-                replicationFactor_ = Integer.parseInt(replicationFactor);
-
-            /* RPC Timeout */
-            String rpcTimeoutInMillis = xmlUtils.getNodeValue("/Storage/RpcTimeoutInMillis");
-            if ( rpcTimeoutInMillis != null )
-                rpcTimeoutInMillis_ = Integer.parseInt(rpcTimeoutInMillis);
-
-            /* Thread per pool */
-            String rawReaders = xmlUtils.getNodeValue("/Storage/ConcurrentReads");
-            if (rawReaders != null)
-            {
-                concurrentReaders_ = Integer.parseInt(rawReaders);
-            }
-            String rawWriters = xmlUtils.getNodeValue("/Storage/ConcurrentWrites");
-            if (rawWriters != null)
-            {
-                concurrentWriters_ = Integer.parseInt(rawWriters);
-            }
-
-            /* TCP port on which the storage system listens */
-            String port = xmlUtils.getNodeValue("/Storage/StoragePort");
-            if ( port != null )
-                storagePort_ = Integer.parseInt(port);
-
-            /* Local IP or hostname to bind services to */
-            String listenAddress = xmlUtils.getNodeValue("/Storage/ListenAddress");
-            if ( listenAddress != null)
-                listenAddress_ = listenAddress;
-            
-            /* Local IP or hostname to bind thrift server to */
-            String thriftAddress = xmlUtils.getNodeValue("/Storage/ThriftAddress");
-            if ( thriftAddress != null )
-                thriftAddress_ = thriftAddress;
-            
-            /* UDP port for control messages */
-            port = xmlUtils.getNodeValue("/Storage/ControlPort");
-            if ( port != null )
-                controlPort_ = Integer.parseInt(port);
-
-            /* get the thrift port from conf file */
-            port = xmlUtils.getNodeValue("/Storage/ThriftPort");
-            if (port != null)
-                thriftPort_ = Integer.parseInt(port);
-
-            /* Number of days to keep the memtable around w/o flushing */
-            String lifetime = xmlUtils.getNodeValue("/Storage/MemtableLifetimeInDays");
-            if ( lifetime != null )
-                memtableLifetime_ = Integer.parseInt(lifetime);
-
-            /* Size of the memtable in memory in MB before it is dumped */
-            String memtableSize = xmlUtils.getNodeValue("/Storage/MemtableSizeInMB");
-            if ( memtableSize != null )
-                memtableSize_ = Integer.parseInt(memtableSize);
-            /* Number of objects in millions in the memtable before it is dumped */
-            String memtableObjectCount = xmlUtils.getNodeValue("/Storage/MemtableObjectCountInMillions");
-            if ( memtableObjectCount != null )
-                memtableObjectCount_ = Double.parseDouble(memtableObjectCount);
-            if (memtableObjectCount_ <= 0)
-            {
-                throw new ConfigurationException("Memtable object count must be a positive double");
-            }
-
-            /* This parameter enables or disables consistency checks.
-             * If set to false the read repairs are disable for very
-             * high throughput on reads but at the cost of consistency.*/
-            String doConsistencyCheck = xmlUtils.getNodeValue("/Storage/DoConsistencyChecksBoolean");
-            if ( doConsistencyCheck != null )
-                doConsistencyCheck_ = Boolean.parseBoolean(doConsistencyCheck);
-
-            /* read the size at which we should do column indexes */
-            String columnIndexSizeInKB = xmlUtils.getNodeValue("/Storage/ColumnIndexSizeInKB");
-            if(columnIndexSizeInKB == null)
-            {
-                columnIndexSizeInKB_ = 64;
-            }
-            else
-            {
-                columnIndexSizeInKB_ = Integer.parseInt(columnIndexSizeInKB);
-            }
-
-            /* data file directory */
-            dataFileDirectories_ = xmlUtils.getNodeValues("/Storage/DataFileDirectories/DataFileDirectory");
-            if (dataFileDirectories_.length == 0)
-            {
-                throw new ConfigurationException("At least one DataFileDirectory must be specified");
-            }
-            for ( String dataFileDirectory : dataFileDirectories_ )
-                FileUtils.createDirectory(dataFileDirectory);
-
-            /* bootstrap file directory */
-            bootstrapFileDirectory_ = xmlUtils.getNodeValue("/Storage/BootstrapFileDirectory");
-            if (bootstrapFileDirectory_ == null)
-            {
-                throw new ConfigurationException("BootstrapFileDirectory must be specified");
-            }
-            FileUtils.createDirectory(bootstrapFileDirectory_);
-
-            /* commit log directory */
-            logFileDirectory_ = xmlUtils.getNodeValue("/Storage/CommitLogDirectory");
-            if (logFileDirectory_ == null)
-            {
-                throw new ConfigurationException("CommitLogDirectory must be specified");
-            }
-            FileUtils.createDirectory(logFileDirectory_);
-
-            /* threshold after which commit log should be rotated. */
-            String value = xmlUtils.getNodeValue("/Storage/CommitLogRotationThresholdInMB");
-            if ( value != null)
-                CommitLog.setSegmentSize(Integer.parseInt(value) * 1024 * 1024);
-
-            tableToCFMetaDataMap_ = new HashMap<String, Map<String, CFMetaData>>();
-            tableKeysCachedFractions_ = new HashMap<String, Double>();
-
-            /* See which replica placement strategy to use */
-            String replicaPlacementStrategyClassName = xmlUtils.getNodeValue("/Storage/ReplicaPlacementStrategy");
-            if (replicaPlacementStrategyClassName == null)
-            {
-                throw new ConfigurationException("Missing replicaplacementstrategy directive /Storage/ReplicaPlacementStrategy");
-            }
-            try
-            {
-                replicaPlacementStrategyClass_ = Class.forName(replicaPlacementStrategyClassName);
-            }
-            catch (ClassNotFoundException e)
-            {
-                throw new ConfigurationException("Invalid replicaplacementstrategy class " + replicaPlacementStrategyClassName);
-            }
-
-            /* Read the table related stuff from config */
-            NodeList tables = xmlUtils.getRequestedNodeList("/Storage/Keyspaces/Keyspace");
-            int size = tables.getLength();
-            for ( int i = 0; i < size; ++i )
-            {
-                Node table = tables.item(i);
-
-                /* parsing out the table name */
-                String tName = XMLUtils.getAttributeValue(table, "Name");
-                if (tName == null)
-                {
-                    throw new ConfigurationException("Table name attribute is required");
-                }
-                if (tName.equalsIgnoreCase(Table.SYSTEM_TABLE))
-                {
-                    throw new ConfigurationException("'system' is a reserved table name for Cassandra internals");
-                }
-                tables_.add(tName);
-                tableToCFMetaDataMap_.put(tName, new HashMap<String, CFMetaData>());
-
-                String xqlCacheSize = "/Storage/Keyspaces/Keyspace[@Name='" + tName + "']/KeysCachedFraction";
-                value = xmlUtils.getNodeValue(xqlCacheSize);
-                if (value == null)
-                {
-                    tableKeysCachedFractions_.put(tName, 0.01);
-                }
-                else
-                {
-                    tableKeysCachedFractions_.put(tName, Double.valueOf(value));
-                }
-
-                String xqlTable = "/Storage/Keyspaces/Keyspace[@Name='" + tName + "']/";
-                NodeList columnFamilies = xmlUtils.getRequestedNodeList(xqlTable + "ColumnFamily");
-
-                // get name of the rowKey for this table
-                String n_rowKey = xmlUtils.getNodeValue(xqlTable + "RowKey");
-                if (n_rowKey == null)
-                    n_rowKey = d_rowKey_;
-
-                //NodeList columnFamilies = xmlUtils.getRequestedNodeList(table, "ColumnFamily");
-                int size2 = columnFamilies.getLength();
-
-                for ( int j = 0; j < size2; ++j )
-                {
-                    Node columnFamily = columnFamilies.item(j);
-                    String cfName = XMLUtils.getAttributeValue(columnFamily, "Name");
-                    if (cfName == null)
-                    {
-                        throw new ConfigurationException("ColumnFamily name attribute is required");
-                    }
-                    String xqlCF = xqlTable + "ColumnFamily[@Name='" + cfName + "']/";
-
-                    /* squirrel away the application column families */
-                    applicationColumnFamilies_.add(cfName);
-
-                    // Parse out the column type
-                    String rawColumnType = XMLUtils.getAttributeValue(columnFamily, "ColumnType");
-                    String columnType = ColumnFamily.getColumnType(rawColumnType);
-                    if (columnType == null)
-                    {
-                        throw new ConfigurationException("ColumnFamily " + cfName + " has invalid type " + rawColumnType);
-                    }
-
-                    if (XMLUtils.getAttributeValue(columnFamily, "ColumnSort") != null)
-                    {
-                        throw new ConfigurationException("ColumnSort is no longer an accepted attribute.  Use CompareWith instead.");
-                    }
-
-                    // Parse out the column comparator
-                    AbstractType columnComparator = getComparator(columnFamily, "CompareWith");
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.config;
+
+import java.util.*;
+import java.io.*;
+import java.lang.reflect.InvocationTargetException;
+
+import javax.xml.transform.TransformerException;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.db.marshal.AsciiType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.db.marshal.BytesType;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.locator.IEndPointSnitch;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.XMLUtils;
+import org.w3c.dom.Node;
+import org.w3c.dom.NodeList;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class DatabaseDescriptor
+{
+    private static Logger logger_ = Logger.getLogger(DatabaseDescriptor.class);
+
+    public static final String random_ = "RANDOM";
+    public static final String ophf_ = "OPHF";
+    private static int storagePort_ = 7000;
+    private static int controlPort_ = 7001;
+    private static int thriftPort_ = 9160;
+    private static String listenAddress_; // leave null so we can fall through to getLocalHost
+    private static String thriftAddress_;
+    private static String clusterName_ = "Test";
+    private static int replicationFactor_ = 3;
+    private static long rpcTimeoutInMillis_ = 2000;
+    private static Set<String> seeds_ = new HashSet<String>();
+    /* Keeps the list of data file directories */
+    private static String[] dataFileDirectories_;
+    /* Current index into the above list of directories */
+    private static int currentIndex_ = 0;
+    private static String logFileDirectory_;
+    private static String bootstrapFileDirectory_;
+    private static int consistencyThreads_ = 4; // not configurable
+    private static int concurrentReaders_ = 8;
+    private static int concurrentWriters_ = 32;
+    private static List<String> tables_ = new ArrayList<String>();
+    private static Set<String> applicationColumnFamilies_ = new HashSet<String>();
+
+    // Default descriptive names for use in CQL. The user can override
+    // these choices in the config file. These are not case sensitive.
+    // Hence, these are stored in UPPER case for easy comparison.
+    private static String d_rowKey_           = "ROW_KEY";
+    private static String d_superColumnMap_   = "SUPER_COLUMN_MAP";
+    private static String d_superColumnKey_   = "SUPER_COLUMN_KEY";
+    private static String d_columnMap_        = "COLUMN_MAP";
+    private static String d_columnKey_        = "COLUMN_KEY";
+    private static String d_columnValue_      = "COLUMN_VALUE";
+    private static String d_columnTimestamp_  = "COLUMN_TIMESTAMP";
+
+    private static Map<String, Double> tableKeysCachedFractions_;
+    /*
+     * A map from table names to the set of column families for the table and the
+     * corresponding meta data for that column family.
+    */
+    private static Map<String, Map<String, CFMetaData>> tableToCFMetaDataMap_;
+    /* Hashing strategy Random or OPHF */
+    private static IPartitioner partitioner_;
+
+    private static IEndPointSnitch endPointSnitch_;
+
+    private static Class replicaPlacementStrategyClass_;
+
+    /* if the size of columns or super-columns are more than this, indexing will kick in */
+    private static int columnIndexSizeInKB_;
+    /* Number of hours to keep a memtable in memory */
+    private static int memtableLifetime_ = 6;
+    /* Size of the memtable in memory before it is dumped */
+    private static int memtableSize_ = 128;
+    /* Number of objects in millions in the memtable before it is dumped */
+    private static double memtableObjectCount_ = 1;
+    /* 
+     * This parameter enables or disables consistency checks. 
+     * If set to false the read repairs are disable for very
+     * high throughput on reads but at the cost of consistency.
+    */
+    private static boolean doConsistencyCheck_ = true;
+    /* Callout directories */
+    private static String calloutLocation_;
+    /* Job Jar Location */
+    private static String jobJarFileLocation_;
+    /* Address where to run the job tracker */
+    private static String jobTrackerHost_;    
+    /* time to wait before garbage collecting tombstones (deletion markers) */
+    private static int gcGraceInSeconds_ = 10 * 24 * 3600; // 10 days
+
+    // the path qualified config file (storage-conf.xml) name
+    private static String configFileName_;
+    /* initial token in the ring */
+    private static String initialToken_ = null;
+
+    private static boolean commitLogSync_;
+
+    private static int commitLogSyncDelay_;
+
+    static
+    {
+        try
+        {
+            configFileName_ = System.getProperty("storage-config") + File.separator + "storage-conf.xml";
+            if (logger_.isDebugEnabled())
+                logger_.debug("Loading settings from " + configFileName_);
+            XMLUtils xmlUtils = new XMLUtils(configFileName_);
+
+            /* Cluster Name */
+            clusterName_ = xmlUtils.getNodeValue("/Storage/ClusterName");
+
+            String syncRaw = xmlUtils.getNodeValue("/Storage/CommitLogSync");
+            if (!"false".equals(syncRaw) && !"true".equals(syncRaw))
+            {
+                // Bool.valueOf will silently assume false for values it doesn't recognize
+                throw new ConfigurationException("Unrecognized value for CommitLogSync.  Use 'true' or 'false'.");
+            }
+            commitLogSync_ = Boolean.valueOf(xmlUtils.getNodeValue("/Storage/CommitLogSync"));
+
+            commitLogSyncDelay_ = Integer.valueOf(xmlUtils.getNodeValue("/Storage/CommitLogSyncDelay"));
+
+            /* Hashing strategy */
+            String partitionerClassName = xmlUtils.getNodeValue("/Storage/Partitioner");
+            if (partitionerClassName == null)
+            {
+                throw new ConfigurationException("Missing partitioner directive /Storage/Partitioner");
+            }
+            try
+            {
+                Class cls = Class.forName(partitionerClassName);
+                partitioner_ = (IPartitioner) cls.getConstructor().newInstance();
+            }
+            catch (ClassNotFoundException e)
+            {
+                throw new ConfigurationException("Invalid partitioner class " + partitionerClassName);
+            }
+
+            /* end point snitch */
+            String endPointSnitchClassName = xmlUtils.getNodeValue("/Storage/EndPointSnitch");
+            if (endPointSnitchClassName == null)
+            {
+                throw new ConfigurationException("Missing endpointsnitch directive /Storage/EndPointSnitch");
+            }
+            try
+            {
+                Class cls = Class.forName(endPointSnitchClassName);
+                endPointSnitch_ = (IEndPointSnitch) cls.getConstructor().newInstance();
+            }
+            catch (ClassNotFoundException e)
+            {
+                throw new ConfigurationException("Invalid endpointsnitch class " + endPointSnitchClassName);
+            }
+            
+            /* Callout location */
+            calloutLocation_ = xmlUtils.getNodeValue("/Storage/CalloutLocation");
+
+            /* JobTracker address */
+            jobTrackerHost_ = xmlUtils.getNodeValue("/Storage/JobTrackerHost");
+
+            /* Job Jar file location */
+            jobJarFileLocation_ = xmlUtils.getNodeValue("/Storage/JobJarFileLocation");
+
+            String gcGrace = xmlUtils.getNodeValue("/Storage/GCGraceSeconds");
+            if ( gcGrace != null )
+                gcGraceInSeconds_ = Integer.parseInt(gcGrace);
+
+            initialToken_ = xmlUtils.getNodeValue("/Storage/InitialToken");
+
+            /* Data replication factor */
+            String replicationFactor = xmlUtils.getNodeValue("/Storage/ReplicationFactor");
+            if ( replicationFactor != null )
+                replicationFactor_ = Integer.parseInt(replicationFactor);
+
+            /* RPC Timeout */
+            String rpcTimeoutInMillis = xmlUtils.getNodeValue("/Storage/RpcTimeoutInMillis");
+            if ( rpcTimeoutInMillis != null )
+                rpcTimeoutInMillis_ = Integer.parseInt(rpcTimeoutInMillis);
+
+            /* Thread per pool */
+            String rawReaders = xmlUtils.getNodeValue("/Storage/ConcurrentReads");
+            if (rawReaders != null)
+            {
+                concurrentReaders_ = Integer.parseInt(rawReaders);
+            }
+            String rawWriters = xmlUtils.getNodeValue("/Storage/ConcurrentWrites");
+            if (rawWriters != null)
+            {
+                concurrentWriters_ = Integer.parseInt(rawWriters);
+            }
+
+            /* TCP port on which the storage system listens */
+            String port = xmlUtils.getNodeValue("/Storage/StoragePort");
+            if ( port != null )
+                storagePort_ = Integer.parseInt(port);
+
+            /* Local IP or hostname to bind services to */
+            String listenAddress = xmlUtils.getNodeValue("/Storage/ListenAddress");
+            if ( listenAddress != null)
+                listenAddress_ = listenAddress;
+            
+            /* Local IP or hostname to bind thrift server to */
+            String thriftAddress = xmlUtils.getNodeValue("/Storage/ThriftAddress");
+            if ( thriftAddress != null )
+                thriftAddress_ = thriftAddress;
+            
+            /* UDP port for control messages */
+            port = xmlUtils.getNodeValue("/Storage/ControlPort");
+            if ( port != null )
+                controlPort_ = Integer.parseInt(port);
+
+            /* get the thrift port from conf file */
+            port = xmlUtils.getNodeValue("/Storage/ThriftPort");
+            if (port != null)
+                thriftPort_ = Integer.parseInt(port);
+
+            /* Number of days to keep the memtable around w/o flushing */
+            String lifetime = xmlUtils.getNodeValue("/Storage/MemtableLifetimeInDays");
+            if ( lifetime != null )
+                memtableLifetime_ = Integer.parseInt(lifetime);
+
+            /* Size of the memtable in memory in MB before it is dumped */
+            String memtableSize = xmlUtils.getNodeValue("/Storage/MemtableSizeInMB");
+            if ( memtableSize != null )
+                memtableSize_ = Integer.parseInt(memtableSize);
+            /* Number of objects in millions in the memtable before it is dumped */
+            String memtableObjectCount = xmlUtils.getNodeValue("/Storage/MemtableObjectCountInMillions");
+            if ( memtableObjectCount != null )
+                memtableObjectCount_ = Double.parseDouble(memtableObjectCount);
+            if (memtableObjectCount_ <= 0)
+            {
+                throw new ConfigurationException("Memtable object count must be a positive double");
+            }
+
+            /* This parameter enables or disables consistency checks.
+             * If set to false the read repairs are disable for very
+             * high throughput on reads but at the cost of consistency.*/
+            String doConsistencyCheck = xmlUtils.getNodeValue("/Storage/DoConsistencyChecksBoolean");
+            if ( doConsistencyCheck != null )
+                doConsistencyCheck_ = Boolean.parseBoolean(doConsistencyCheck);
+
+            /* read the size at which we should do column indexes */
+            String columnIndexSizeInKB = xmlUtils.getNodeValue("/Storage/ColumnIndexSizeInKB");
+            if(columnIndexSizeInKB == null)
+            {
+                columnIndexSizeInKB_ = 64;
+            }
+            else
+            {
+                columnIndexSizeInKB_ = Integer.parseInt(columnIndexSizeInKB);
+            }
+
+            /* data file directory */
+            dataFileDirectories_ = xmlUtils.getNodeValues("/Storage/DataFileDirectories/DataFileDirectory");
+            if (dataFileDirectories_.length == 0)
+            {
+                throw new ConfigurationException("At least one DataFileDirectory must be specified");
+            }
+            for ( String dataFileDirectory : dataFileDirectories_ )
+                FileUtils.createDirectory(dataFileDirectory);
+
+            /* bootstrap file directory */
+            bootstrapFileDirectory_ = xmlUtils.getNodeValue("/Storage/BootstrapFileDirectory");
+            if (bootstrapFileDirectory_ == null)
+            {
+                throw new ConfigurationException("BootstrapFileDirectory must be specified");
+            }
+            FileUtils.createDirectory(bootstrapFileDirectory_);
+
+            /* commit log directory */
+            logFileDirectory_ = xmlUtils.getNodeValue("/Storage/CommitLogDirectory");
+            if (logFileDirectory_ == null)
+            {
+                throw new ConfigurationException("CommitLogDirectory must be specified");
+            }
+            FileUtils.createDirectory(logFileDirectory_);
+
+            /* threshold after which commit log should be rotated. */
+            String value = xmlUtils.getNodeValue("/Storage/CommitLogRotationThresholdInMB");
+            if ( value != null)
+                CommitLog.setSegmentSize(Integer.parseInt(value) * 1024 * 1024);
+
+            tableToCFMetaDataMap_ = new HashMap<String, Map<String, CFMetaData>>();
+            tableKeysCachedFractions_ = new HashMap<String, Double>();
+
+            /* See which replica placement strategy to use */
+            String replicaPlacementStrategyClassName = xmlUtils.getNodeValue("/Storage/ReplicaPlacementStrategy");
+            if (replicaPlacementStrategyClassName == null)
+            {
+                throw new ConfigurationException("Missing replicaplacementstrategy directive /Storage/ReplicaPlacementStrategy");
+            }
+            try
+            {
+                replicaPlacementStrategyClass_ = Class.forName(replicaPlacementStrategyClassName);
+            }
+            catch (ClassNotFoundException e)
+            {
+                throw new ConfigurationException("Invalid replicaplacementstrategy class " + replicaPlacementStrategyClassName);
+            }
+
+            /* Read the table related stuff from config */
+            NodeList tables = xmlUtils.getRequestedNodeList("/Storage/Keyspaces/Keyspace");
+            int size = tables.getLength();
+            for ( int i = 0; i < size; ++i )
+            {
+                Node table = tables.item(i);
+
+                /* parsing out the table name */
+                String tName = XMLUtils.getAttributeValue(table, "Name");
+                if (tName == null)
+                {
+                    throw new ConfigurationException("Table name attribute is required");
+                }
+                if (tName.equalsIgnoreCase(Table.SYSTEM_TABLE))
+                {
+                    throw new ConfigurationException("'system' is a reserved table name for Cassandra internals");
+                }
+                tables_.add(tName);
+                tableToCFMetaDataMap_.put(tName, new HashMap<String, CFMetaData>());
+
+                String xqlCacheSize = "/Storage/Keyspaces/Keyspace[@Name='" + tName + "']/KeysCachedFraction";
+                value = xmlUtils.getNodeValue(xqlCacheSize);
+                if (value == null)
+                {
+                    tableKeysCachedFractions_.put(tName, 0.01);
+                }
+                else
+                {
+                    tableKeysCachedFractions_.put(tName, Double.valueOf(value));
+                }
+
+                String xqlTable = "/Storage/Keyspaces/Keyspace[@Name='" + tName + "']/";
+                NodeList columnFamilies = xmlUtils.getRequestedNodeList(xqlTable + "ColumnFamily");
+
+                // get name of the rowKey for this table
+                String n_rowKey = xmlUtils.getNodeValue(xqlTable + "RowKey");
+                if (n_rowKey == null)
+                    n_rowKey = d_rowKey_;
+
+                //NodeList columnFamilies = xmlUtils.getRequestedNodeList(table, "ColumnFamily");
+                int size2 = columnFamilies.getLength();
+
+                for ( int j = 0; j < size2; ++j )
+                {
+                    Node columnFamily = columnFamilies.item(j);
+                    String cfName = XMLUtils.getAttributeValue(columnFamily, "Name");
+                    if (cfName == null)
+                    {
+                        throw new ConfigurationException("ColumnFamily name attribute is required");
+                    }
+                    String xqlCF = xqlTable + "ColumnFamily[@Name='" + cfName + "']/";
+
+                    /* squirrel away the application column families */
+                    applicationColumnFamilies_.add(cfName);
+
+                    // Parse out the column type
+                    String rawColumnType = XMLUtils.getAttributeValue(columnFamily, "ColumnType");
+                    String columnType = ColumnFamily.getColumnType(rawColumnType);
+                    if (columnType == null)
+                    {
+                        throw new ConfigurationException("ColumnFamily " + cfName + " has invalid type " + rawColumnType);
+                    }
+
+                    if (XMLUtils.getAttributeValue(columnFamily, "ColumnSort") != null)
+                    {
+                        throw new ConfigurationException("ColumnSort is no longer an accepted attribute.  Use CompareWith instead.");
+                    }
+
+                    // Parse out the column comparator
+                    AbstractType columnComparator = getComparator(columnFamily, "CompareWith");
                     AbstractType subcolumnComparator = null;
-                    if (columnType.equals("Super"))
-                    {
-                        subcolumnComparator = getComparator(columnFamily, "CompareSubcolumnsWith");
-                    }
-                    else if (XMLUtils.getAttributeValue(columnFamily, "CompareSubcolumnsWith") != null)
-                    {
-                        throw new ConfigurationException("CompareSubcolumnsWith is only a valid attribute on super columnfamilies (not regular columnfamily " + cfName + ")");
-                    }
-
-                    // see if flush period is set
-                    String flushPeriodInMinutes = XMLUtils.getAttributeValue(columnFamily, "FlushPeriodInMinutes");
-                    int flushPeriod=0;
-                    if ( flushPeriodInMinutes != null )
-                        flushPeriod = Integer.parseInt(flushPeriodInMinutes);
-
-                    
-                    // Parse out user-specified logical names for the various dimensions
-                    // of a the column family from the config.
-                    String n_superColumnMap = xmlUtils.getNodeValue(xqlCF + "SuperColumnMap");
-                    if (n_superColumnMap == null)
-                        n_superColumnMap = d_superColumnMap_;
-
-                    String n_superColumnKey = xmlUtils.getNodeValue(xqlCF + "SuperColumnKey");
-                    if (n_superColumnKey == null)
-                        n_superColumnKey = d_superColumnKey_;
-
-                    String n_columnMap = xmlUtils.getNodeValue(xqlCF + "ColumnMap");
-                    if (n_columnMap == null)
-                        n_columnMap = d_columnMap_;
-
-                    String n_columnKey = xmlUtils.getNodeValue(xqlCF + "ColumnKey");
-                    if (n_columnKey == null)
-                        n_columnKey = d_columnKey_;
-
-                    String n_columnValue = xmlUtils.getNodeValue(xqlCF + "ColumnValue");
-                    if (n_columnValue == null)
-                        n_columnValue = d_columnValue_;
-
-                    String n_columnTimestamp = xmlUtils.getNodeValue(xqlCF + "ColumnTimestamp");
-                    if (n_columnTimestamp == null)
-                        n_columnTimestamp = d_columnTimestamp_;
-
-                    // now populate the column family meta data and
-                    // insert it into the table dictionary.
-                    CFMetaData cfMetaData = new CFMetaData();
-
-                    cfMetaData.tableName = tName;
-                    cfMetaData.cfName = cfName;
-
-                    cfMetaData.columnType = columnType;
-                    cfMetaData.comparator = columnComparator;
+                    if (columnType.equals("Super"))
+                    {
+                        subcolumnComparator = getComparator(columnFamily, "CompareSubcolumnsWith");
+                    }
+                    else if (XMLUtils.getAttributeValue(columnFamily, "CompareSubcolumnsWith") != null)
+                    {
+                        throw new ConfigurationException("CompareSubcolumnsWith is only a valid attribute on super columnfamilies (not regular columnfamily " + cfName + ")");
+                    }
+
+                    // see if flush period is set
+                    String flushPeriodInMinutes = XMLUtils.getAttributeValue(columnFamily, "FlushPeriodInMinutes");
+                    int flushPeriod=0;
+                    if ( flushPeriodInMinutes != null )
+                        flushPeriod = Integer.parseInt(flushPeriodInMinutes);
+
+                    
+                    // Parse out user-specified logical names for the various dimensions
+                    // of a the column family from the config.
+                    String n_superColumnMap = xmlUtils.getNodeValue(xqlCF + "SuperColumnMap");
+                    if (n_superColumnMap == null)
+                        n_superColumnMap = d_superColumnMap_;
+
+                    String n_superColumnKey = xmlUtils.getNodeValue(xqlCF + "SuperColumnKey");
+                    if (n_superColumnKey == null)
+                        n_superColumnKey = d_superColumnKey_;
+
+                    String n_columnMap = xmlUtils.getNodeValue(xqlCF + "ColumnMap");
+                    if (n_columnMap == null)
+                        n_columnMap = d_columnMap_;
+
+                    String n_columnKey = xmlUtils.getNodeValue(xqlCF + "ColumnKey");
+                    if (n_columnKey == null)
+                        n_columnKey = d_columnKey_;
+
+                    String n_columnValue = xmlUtils.getNodeValue(xqlCF + "ColumnValue");
+                    if (n_columnValue == null)
+                        n_columnValue = d_columnValue_;
+
+                    String n_columnTimestamp = xmlUtils.getNodeValue(xqlCF + "ColumnTimestamp");
+                    if (n_columnTimestamp == null)
+                        n_columnTimestamp = d_columnTimestamp_;
+
+                    // now populate the column family meta data and
+                    // insert it into the table dictionary.
+                    CFMetaData cfMetaData = new CFMetaData();
+
+                    cfMetaData.tableName = tName;
+                    cfMetaData.cfName = cfName;
+
+                    cfMetaData.columnType = columnType;
+                    cfMetaData.comparator = columnComparator;
                     cfMetaData.subcolumnComparator = subcolumnComparator;
-
-                    cfMetaData.n_rowKey = n_rowKey;
-                    cfMetaData.n_columnMap = n_columnMap;
-                    cfMetaData.n_columnKey = n_columnKey;
-                    cfMetaData.n_columnValue = n_columnValue;
-                    cfMetaData.n_columnTimestamp = n_columnTimestamp;
-                    if ("Super".equals(columnType))
-                    {
-                        cfMetaData.n_superColumnKey = n_superColumnKey;
-                        cfMetaData.n_superColumnMap = n_superColumnMap;
-                    }
-                    cfMetaData.flushPeriodInMinutes = flushPeriod;
-                    
-                    tableToCFMetaDataMap_.get(tName).put(cfName, cfMetaData);
-                }
-            }
-
-            // Hardcoded system tables
-            Map<String, CFMetaData> systemMetadata = new HashMap<String, CFMetaData>();
-
-            CFMetaData data = new CFMetaData();
-            data.comparator = new AsciiType();
-            systemMetadata.put(SystemTable.LOCATION_CF, data);
-
-            data = new CFMetaData();
-            data.columnType = "Super";
-            data.comparator = new UTF8Type();
-            data.subcolumnComparator = new BytesType();
-            systemMetadata.put(HintedHandOffManager.HINTS_CF, data);
-
-            tableToCFMetaDataMap_.put("system", systemMetadata);
-
-            /* make sure we have a directory for each table */
-            createTableDirectories();
-
-            /* Load the seeds for node contact points */
-            String[] seeds = xmlUtils.getNodeValues("/Storage/Seeds/Seed");
-            for( int i = 0; i < seeds.length; ++i )
-            {
-                seeds_.add( seeds[i] );
-            }
-        }
-        catch (ConfigurationException e)
-        {
-            logger_.error("Fatal error: " + e.getMessage());
-            System.err.println("Bad configuration; unable to start server");
-            System.exit(1);
-        }
-        catch (Exception e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    private static AbstractType getComparator(Node columnFamily, String attr)
-    throws ConfigurationException, TransformerException, NoSuchMethodException, InvocationTargetException, IllegalAccessException, InstantiationException
-    {
-        Class<? extends AbstractType> typeClass;
-        String compareWith = XMLUtils.getAttributeValue(columnFamily, attr);
-        if (compareWith == null)
-        {
-            typeClass = AsciiType.class;
-        }
-        else
-        {
-            String className = compareWith.contains(".") ? compareWith : "org.apache.cassandra.db.marshal." + compareWith;
-            try
-            {
-                typeClass = (Class<? extends AbstractType>)Class.forName(className);
-            }
-            catch (ClassNotFoundException e)
-            {
-                throw new ConfigurationException("Unable to load class " + className + " for " + attr + " attribute");
-            }
-        }
-        return typeClass.getConstructor().newInstance();
-    }
-
-    /**
-     * Create the table directory in each data directory
-     */
-    public static void createTableDirectories() throws IOException
-    {
-        for (String dataFile : dataFileDirectories_) 
-        {
-            FileUtils.createDirectory(dataFile + File.separator + Table.SYSTEM_TABLE);
-            for (String table : tables_)
-            {
-                FileUtils.createDirectory(dataFile + File.separator + table);
-            }
-        }
-    }
-
-    /**
-     * Create the metadata tables. This table has information about
-     * the table name and the column families that make up the table.
-     * Each column family also has an associated ID which is an int.
-    */
-    // TODO duplicating data b/t tablemetadata and CFMetaData is confusing and error-prone
-    public static void storeMetadata() throws IOException
-    {
-        int cfId = 0;
-        Set<String> tables = tableToCFMetaDataMap_.keySet();
-
-        for (String table : tables)
-        {
-            Table.TableMetadata tmetadata = Table.TableMetadata.instance(table);
-            if (tmetadata.isEmpty())
-            {
-                tmetadata = Table.TableMetadata.instance(table);
-                /* Column families associated with this table */
-                Map<String, CFMetaData> columnFamilies = tableToCFMetaDataMap_.get(table);
-
-                for (String columnFamily : columnFamilies.keySet())
-                {
-                    tmetadata.add(columnFamily, cfId++, DatabaseDescriptor.getColumnType(table, columnFamily));
-                }
-            }
-        }
-    }
-
-    public static int getGcGraceInSeconds()
-    {
-        return gcGraceInSeconds_;
-    }
-
-    public static IPartitioner getPartitioner()
-    {
-        return partitioner_;
-    }
-    
-    public static IEndPointSnitch getEndPointSnitch()
-    {
-        return endPointSnitch_;
-    }
-
-    public static Class getReplicaPlacementStrategyClass()
-    {
-        return replicaPlacementStrategyClass_;
-    }
-    
-    public static String getCalloutLocation()
-    {
-        return calloutLocation_;
-    }
-    
-    public static String getJobTrackerAddress()
-    {
-        return jobTrackerHost_;
-    }
-    
-    public static int getColumnIndexSize()
-    {
-    	return columnIndexSizeInKB_ * 1024;
-    }
-
-    public static int getMemtableLifetime()
-    {
-      return memtableLifetime_;
-    }
-
-    public static String getInitialToken()
-    {
-      return initialToken_;
-    }
-
-    public static int getMemtableSize()
-    {
-      return memtableSize_;
-    }
-
-    public static double getMemtableObjectCount()
-    {
-      return memtableObjectCount_;
-    }
-
-    public static boolean getConsistencyCheck()
-    {
-      return doConsistencyCheck_;
-    }
-
-    public static String getClusterName()
-    {
-        return clusterName_;
-    }
-
-    public static String getConfigFileName() {
-        return configFileName_;
-    }
-    
-    public static boolean isApplicationColumnFamily(String columnFamily)
-    {
-        return applicationColumnFamilies_.contains(columnFamily);
-    }
-
-    public static String getJobJarLocation()
-    {
-        return jobJarFileLocation_;
-    }
-    
-    public static Map<String, CFMetaData> getTableMetaData(String tableName)
-    {
-        assert tableName != null;
-        return tableToCFMetaDataMap_.get(tableName);
-    }
-
-    /*
-     * Given a table name & column family name, get the column family
-     * meta data. If the table name or column family name is not valid
-     * this function returns null.
-     */
-    public static CFMetaData getCFMetaData(String tableName, String cfName)
-    {
-        assert tableName != null;
-        Map<String, CFMetaData> cfInfo = tableToCFMetaDataMap_.get(tableName);
-        if (cfInfo == null)
-            return null;
-        
-        return cfInfo.get(cfName);
-    }
-    
-    public static String getColumnType(String tableName, String cfName)
-    {
-        assert tableName != null;
-        CFMetaData cfMetaData = getCFMetaData(tableName, cfName);
-        
-        if (cfMetaData == null)
-            return null;
-        return cfMetaData.columnType;
-    }
-
-    public static int getFlushPeriod(String tableName, String columnFamilyName)
-    {
-        assert tableName != null;
-        CFMetaData cfMetaData = getCFMetaData(tableName, columnFamilyName);
-        
-        if (cfMetaData == null)
-            return 0;
-        return cfMetaData.flushPeriodInMinutes;
-    }
-
-    public static List<String> getTables()
-    {
-        return tables_;
-    }
-
-    public static String getTable(String tableName)
-    {
-        assert tableName != null;
-        int index = getTables().indexOf(tableName);
-        return index >= 0 ? getTables().get(index) : null;
-    }
-
-    public static void  setTables(String table)
-    {
-        tables_.add(table);
-    }
-
-    public static int getStoragePort()
-    {
-        return storagePort_;
-    }
-
-    public static int getControlPort()
-    {
-        return controlPort_;
-    }
-
-    public static int getThriftPort()
-    {
-        return thriftPort_;
-    }
-
-    public static int getReplicationFactor()
-    {
-        return replicationFactor_;
-    }
-
-    public static int getQuorum()
-    {
-        return (replicationFactor_ / 2) + 1;
-    }
-
-    public static long getRpcTimeout()
-    {
-        return rpcTimeoutInMillis_;
-    }
-
-    public static int getConsistencyThreads()
-    {
-        return consistencyThreads_;
-    }
-
-    public static int getConcurrentReaders()
-    {
-        return concurrentReaders_;
-    }
-
-    public static int getConcurrentWriters()
-    {
-        return concurrentWriters_;
-    }
-
-    public static String[] getAllDataFileLocations()
-    {
-        return dataFileDirectories_;
-    }
-
-    /**
-     * Get a list of data directories for a given table
-     * 
-     * @param table name of the table.
-     * 
-     * @return an array of path to the data directories. 
-     */
-    public static String[] getAllDataFileLocationsForTable(String table)
-    {
-        String[] tableLocations = new String[dataFileDirectories_.length];
-
-        for (int i = 0; i < dataFileDirectories_.length; i++)
-        {
-            tableLocations[i] = dataFileDirectories_[i] + File.separator + table;
-        }
-
-        return tableLocations;
-    }
-
-    public static String getDataFileLocationForTable(String table)
-    {
-        String dataFileDirectory = dataFileDirectories_[currentIndex_] + File.separator + table;
-        currentIndex_ = (currentIndex_ + 1) % dataFileDirectories_.length;
-        return dataFileDirectory;
-    }
-
-    public static String getBootstrapFileLocation()
-    {
-        return bootstrapFileDirectory_;
-    }
-
-    public static void setBootstrapFileLocation(String bfLocation)
-    {
-        bootstrapFileDirectory_ = bfLocation;
-    }
-
-    public static String getLogFileLocation()
-    {
-        return logFileDirectory_;
-    }
-
-    public static void setLogFileLocation(String logLocation)
-    {
-        logFileDirectory_ = logLocation;
-    }
-
-    public static Set<String> getSeeds()
-    {
-        return seeds_;
-    }
-
-    public static String getColumnFamilyType(String tableName, String cfName)
-    {
-        assert tableName != null;
-        String cfType = getColumnType(tableName, cfName);
-        if ( cfType == null )
-            cfType = "Standard";
-    	return cfType;
-    }
-
-    /*
-     * Loop through all the disks to see which disk has the max free space
-     * return the disk with max free space for compactions. If the size of the expected
-     * compacted file is greater than the max disk space available return null, we cannot
-     * do compaction in this case.
-     */
-    public static String getDataFileLocationForTable(String table, long expectedCompactedFileSize)
-    {
-      long maxFreeDisk = 0;
-      int maxDiskIndex = 0;
-      String dataFileDirectory = null;
-      String[] dataDirectoryForTable = getAllDataFileLocationsForTable(table);
-
-      for ( int i = 0 ; i < dataDirectoryForTable.length ; i++ )
-      {
-        File f = new File(dataDirectoryForTable[i]);
-        if( maxFreeDisk < f.getUsableSpace())
-        {
-          maxFreeDisk = f.getUsableSpace();
-          maxDiskIndex = i;
-        }
-      }
-      // Load factor of 0.9 we do not want to use the entire disk that is too risky.
-      maxFreeDisk = (long)(0.9 * maxFreeDisk);
-      if( expectedCompactedFileSize < maxFreeDisk )
-      {
-        dataFileDirectory = dataDirectoryForTable[maxDiskIndex];
-        currentIndex_ = (maxDiskIndex + 1 )%dataDirectoryForTable.length ;
-      }
-      else
-      {
-        currentIndex_ = maxDiskIndex;
-      }
-        return dataFileDirectory;
-    }
-    
-    public static AbstractType getComparator(String tableName, String cfName)
-    {
-        assert tableName != null;
-        return getCFMetaData(tableName, cfName).comparator;
-    }
-
-    public static AbstractType getSubComparator(String tableName, String cfName)
-    {
-        assert tableName != null;
-        return getCFMetaData(tableName, cfName).comparator;
-    }
-
-    public static Map<String, Map<String, CFMetaData>> getTableToColumnFamilyMap()
-    {
-        return tableToCFMetaDataMap_;
-    }
-
-    public static double getKeysCachedFraction(String tableName)
-    {
-        return tableKeysCachedFractions_.get(tableName);
-    }
-
-    private static class ConfigurationException extends Exception
-    {
-        public ConfigurationException(String message)
-        {
-            super(message);
-        }
-    }
-
-    public static String getListenAddress()
-    {
-        return listenAddress_;
-    }
-    
-    public static String getThriftAddress()
-    {
-        return thriftAddress_;
-    }
-
-    public static int getCommitLogSyncDelay()
-    {
-        return commitLogSyncDelay_;
-    }
-
-    public static boolean isCommitLogSyncEnabled()
-    {
-        return commitLogSync_;
-    }
-}
+
+                    cfMetaData.n_rowKey = n_rowKey;
+                    cfMetaData.n_columnMap = n_columnMap;
+                    cfMetaData.n_columnKey = n_columnKey;
+                    cfMetaData.n_columnValue = n_columnValue;
+                    cfMetaData.n_columnTimestamp = n_columnTimestamp;
+                    if ("Super".equals(columnType))
+                    {
+                        cfMetaData.n_superColumnKey = n_superColumnKey;
+                        cfMetaData.n_superColumnMap = n_superColumnMap;
+                    }
+                    cfMetaData.flushPeriodInMinutes = flushPeriod;
+                    
+                    tableToCFMetaDataMap_.get(tName).put(cfName, cfMetaData);
+                }
+            }
+
+            // Hardcoded system tables
+            Map<String, CFMetaData> systemMetadata = new HashMap<String, CFMetaData>();
+
+            CFMetaData data = new CFMetaData();
+            data.comparator = new AsciiType();
+            systemMetadata.put(SystemTable.LOCATION_CF, data);
+
+            data = new CFMetaData();
+            data.columnType = "Super";
+            data.comparator = new UTF8Type();
+            data.subcolumnComparator = new BytesType();
+            systemMetadata.put(HintedHandOffManager.HINTS_CF, data);
+
+            tableToCFMetaDataMap_.put("system", systemMetadata);
+
+            /* make sure we have a directory for each table */
+            createTableDirectories();
+
+            /* Load the seeds for node contact points */
+            String[] seeds = xmlUtils.getNodeValues("/Storage/Seeds/Seed");
+            for( int i = 0; i < seeds.length; ++i )
+            {
+                seeds_.add( seeds[i] );
+            }
+        }
+        catch (ConfigurationException e)
+        {
+            logger_.error("Fatal error: " + e.getMessage());
+            System.err.println("Bad configuration; unable to start server");
+            System.exit(1);
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    private static AbstractType getComparator(Node columnFamily, String attr)
+    throws ConfigurationException, TransformerException, NoSuchMethodException, InvocationTargetException, IllegalAccessException, InstantiationException
+    {
+        Class<? extends AbstractType> typeClass;
+        String compareWith = XMLUtils.getAttributeValue(columnFamily, attr);
+        if (compareWith == null)
+        {
+            typeClass = AsciiType.class;
+        }
+        else
+        {
+            String className = compareWith.contains(".") ? compareWith : "org.apache.cassandra.db.marshal." + compareWith;
+            try
+            {
+                typeClass = (Class<? extends AbstractType>)Class.forName(className);
+            }
+            catch (ClassNotFoundException e)
+            {
+                throw new ConfigurationException("Unable to load class " + className + " for " + attr + " attribute");
+            }
+        }
+        return typeClass.getConstructor().newInstance();
+    }
+
+    /**
+     * Create the table directory in each data directory
+     */
+    public static void createTableDirectories() throws IOException
+    {
+        for (String dataFile : dataFileDirectories_) 
+        {
+            FileUtils.createDirectory(dataFile + File.separator + Table.SYSTEM_TABLE);
+            for (String table : tables_)
+            {
+                FileUtils.createDirectory(dataFile + File.separator + table);
+            }
+        }
+    }
+
+    /**
+     * Create the metadata tables. This table has information about
+     * the table name and the column families that make up the table.
+     * Each column family also has an associated ID which is an int.
+    */
+    // TODO duplicating data b/t tablemetadata and CFMetaData is confusing and error-prone
+    public static void storeMetadata() throws IOException
+    {
+        int cfId = 0;
+        Set<String> tables = tableToCFMetaDataMap_.keySet();
+
+        for (String table : tables)
+        {
+            Table.TableMetadata tmetadata = Table.TableMetadata.instance(table);
+            if (tmetadata.isEmpty())
+            {
+                tmetadata = Table.TableMetadata.instance(table);
+                /* Column families associated with this table */
+                Map<String, CFMetaData> columnFamilies = tableToCFMetaDataMap_.get(table);
+
+                for (String columnFamily : columnFamilies.keySet())
+                {
+                    tmetadata.add(columnFamily, cfId++, DatabaseDescriptor.getColumnType(table, columnFamily));
+                }
+            }
+        }
+    }
+
+    public static int getGcGraceInSeconds()
+    {
+        return gcGraceInSeconds_;
+    }
+
+    public static IPartitioner getPartitioner()
+    {
+        return partitioner_;
+    }
+    
+    public static IEndPointSnitch getEndPointSnitch()
+    {
+        return endPointSnitch_;
+    }
+
+    public static Class getReplicaPlacementStrategyClass()
+    {
+        return replicaPlacementStrategyClass_;
+    }
+    
+    public static String getCalloutLocation()
+    {
+        return calloutLocation_;
+    }
+    
+    public static String getJobTrackerAddress()
+    {
+        return jobTrackerHost_;
+    }
+    
+    public static int getColumnIndexSize()
+    {
+    	return columnIndexSizeInKB_ * 1024;
+    }
+
+    public static int getMemtableLifetime()
+    {
+      return memtableLifetime_;
+    }
+
+    public static String getInitialToken()
+    {
+      return initialToken_;
+    }
+
+    public static int getMemtableSize()
+    {
+      return memtableSize_;
+    }
+
+    public static double getMemtableObjectCount()
+    {
+      return memtableObjectCount_;
+    }
+
+    public static boolean getConsistencyCheck()
+    {
+      return doConsistencyCheck_;
+    }
+
+    public static String getClusterName()
+    {
+        return clusterName_;
+    }
+
+    public static String getConfigFileName() {
+        return configFileName_;
+    }
+    
+    public static boolean isApplicationColumnFamily(String columnFamily)
+    {
+        return applicationColumnFamilies_.contains(columnFamily);
+    }
+
+    public static String getJobJarLocation()
+    {
+        return jobJarFileLocation_;
+    }
+    
+    public static Map<String, CFMetaData> getTableMetaData(String tableName)
+    {
+        assert tableName != null;
+        return tableToCFMetaDataMap_.get(tableName);
+    }
+
+    /*
+     * Given a table name & column family name, get the column family
+     * meta data. If the table name or column family name is not valid
+     * this function returns null.
+     */
+    public static CFMetaData getCFMetaData(String tableName, String cfName)
+    {
+        assert tableName != null;
+        Map<String, CFMetaData> cfInfo = tableToCFMetaDataMap_.get(tableName);
+        if (cfInfo == null)
+            return null;
+        
+        return cfInfo.get(cfName);
+    }
+    
+    public static String getColumnType(String tableName, String cfName)
+    {
+        assert tableName != null;
+        CFMetaData cfMetaData = getCFMetaData(tableName, cfName);
+        
+        if (cfMetaData == null)
+            return null;
+        return cfMetaData.columnType;
+    }
+
+    public static int getFlushPeriod(String tableName, String columnFamilyName)
+    {
+        assert tableName != null;
+        CFMetaData cfMetaData = getCFMetaData(tableName, columnFamilyName);
+        
+        if (cfMetaData == null)
+            return 0;
+        return cfMetaData.flushPeriodInMinutes;
+    }
+
+    public static List<String> getTables()
+    {
+        return tables_;
+    }
+
+    public static String getTable(String tableName)
+    {
+        assert tableName != null;
+        int index = getTables().indexOf(tableName);
+        return index >= 0 ? getTables().get(index) : null;
+    }
+
+    public static void  setTables(String table)
+    {
+        tables_.add(table);
+    }
+
+    public static int getStoragePort()
+    {
+        return storagePort_;
+    }
+
+    public static int getControlPort()
+    {
+        return controlPort_;
+    }
+
+    public static int getThriftPort()
+    {
+        return thriftPort_;
+    }
+
+    public static int getReplicationFactor()
+    {
+        return replicationFactor_;
+    }
+
+    public static int getQuorum()
+    {
+        return (replicationFactor_ / 2) + 1;
+    }
+
+    public static long getRpcTimeout()
+    {
+        return rpcTimeoutInMillis_;
+    }
+
+    public static int getConsistencyThreads()
+    {
+        return consistencyThreads_;
+    }
+
+    public static int getConcurrentReaders()
+    {
+        return concurrentReaders_;
+    }
+
+    public static int getConcurrentWriters()
+    {
+        return concurrentWriters_;
+    }
+
+    public static String[] getAllDataFileLocations()
+    {
+        return dataFileDirectories_;
+    }
+
+    /**
+     * Get a list of data directories for a given table
+     * 
+     * @param table name of the table.
+     * 
+     * @return an array of path to the data directories. 
+     */
+    public static String[] getAllDataFileLocationsForTable(String table)
+    {
+        String[] tableLocations = new String[dataFileDirectories_.length];
+
+        for (int i = 0; i < dataFileDirectories_.length; i++)
+        {
+            tableLocations[i] = dataFileDirectories_[i] + File.separator + table;
+        }
+
+        return tableLocations;
+    }
+
+    public static String getDataFileLocationForTable(String table)
+    {
+        String dataFileDirectory = dataFileDirectories_[currentIndex_] + File.separator + table;
+        currentIndex_ = (currentIndex_ + 1) % dataFileDirectories_.length;
+        return dataFileDirectory;
+    }
+
+    public static String getBootstrapFileLocation()
+    {
+        return bootstrapFileDirectory_;
+    }
+
+    public static void setBootstrapFileLocation(String bfLocation)
+    {
+        bootstrapFileDirectory_ = bfLocation;
+    }
+
+    public static String getLogFileLocation()
+    {
+        return logFileDirectory_;
+    }
+
+    public static void setLogFileLocation(String logLocation)
+    {
+        logFileDirectory_ = logLocation;
+    }
+
+    public static Set<String> getSeeds()
+    {
+        return seeds_;
+    }
+
+    public static String getColumnFamilyType(String tableName, String cfName)
+    {
+        assert tableName != null;
+        String cfType = getColumnType(tableName, cfName);
+        if ( cfType == null )
+            cfType = "Standard";
+    	return cfType;
+    }
+
+    /*
+     * Loop through all the disks to see which disk has the max free space
+     * return the disk with max free space for compactions. If the size of the expected
+     * compacted file is greater than the max disk space available return null, we cannot
+     * do compaction in this case.
+     */
+    public static String getDataFileLocationForTable(String table, long expectedCompactedFileSize)
+    {
+      long maxFreeDisk = 0;
+      int maxDiskIndex = 0;
+      String dataFileDirectory = null;
+      String[] dataDirectoryForTable = getAllDataFileLocationsForTable(table);
+
+      for ( int i = 0 ; i < dataDirectoryForTable.length ; i++ )
+      {
+        File f = new File(dataDirectoryForTable[i]);
+        if( maxFreeDisk < f.getUsableSpace())
+        {
+          maxFreeDisk = f.getUsableSpace();
+          maxDiskIndex = i;
+        }
+      }
+      // Load factor of 0.9 we do not want to use the entire disk that is too risky.
+      maxFreeDisk = (long)(0.9 * maxFreeDisk);
+      if( expectedCompactedFileSize < maxFreeDisk )
+      {
+        dataFileDirectory = dataDirectoryForTable[maxDiskIndex];
+        currentIndex_ = (maxDiskIndex + 1 )%dataDirectoryForTable.length ;
+      }
+      else
+      {
+        currentIndex_ = maxDiskIndex;
+      }
+        return dataFileDirectory;
+    }
+    
+    public static AbstractType getComparator(String tableName, String cfName)
+    {
+        assert tableName != null;
+        return getCFMetaData(tableName, cfName).comparator;
+    }
+
+    public static AbstractType getSubComparator(String tableName, String cfName)
+    {
+        assert tableName != null;
+        return getCFMetaData(tableName, cfName).comparator;
+    }
+
+    public static Map<String, Map<String, CFMetaData>> getTableToColumnFamilyMap()
+    {
+        return tableToCFMetaDataMap_;
+    }
+
+    public static double getKeysCachedFraction(String tableName)
+    {
+        return tableKeysCachedFractions_.get(tableName);
+    }
+
+    private static class ConfigurationException extends Exception
+    {
+        public ConfigurationException(String message)
+        {
+            super(message);
+        }
+    }
+
+    public static String getListenAddress()
+    {
+        return listenAddress_;
+    }
+    
+    public static String getThriftAddress()
+    {
+        return thriftAddress_;
+    }
+
+    public static int getCommitLogSyncDelay()
+    {
+        return commitLogSyncDelay_;
+    }
+
+    public static boolean isCommitLogSyncEnabled()
+    {
+        return commitLogSync_;
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/BinaryMemtable.java b/src/java/org/apache/cassandra/db/BinaryMemtable.java
index c07320ae4d..9ee909c977 100644
--- a/src/java/org/apache/cassandra/db/BinaryMemtable.java
+++ b/src/java/org/apache/cassandra/db/BinaryMemtable.java
@@ -1,162 +1,162 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.locks.Condition;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-
-import org.apache.cassandra.io.SSTableWriter;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.config.DatabaseDescriptor;
-
-import org.apache.log4j.Logger;
-import org.cliffc.high_scale_lib.NonBlockingHashMap;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class BinaryMemtable
-{
-    private static Logger logger_ = Logger.getLogger( Memtable.class );
-    private int threshold_ = 512*1024*1024;
-    private AtomicInteger currentSize_ = new AtomicInteger(0);
-
-    /* Table and ColumnFamily name are used to determine the ColumnFamilyStore */
-    private String table_;
-    private String cfName_;
-    private boolean isFrozen_ = false;
-    private Map<String, byte[]> columnFamilies_ = new NonBlockingHashMap<String, byte[]>();
-    /* Lock and Condition for notifying new clients about Memtable switches */
-    Lock lock_ = new ReentrantLock();
-    Condition condition_;
-
-    BinaryMemtable(String table, String cfName) throws IOException
-    {
-        condition_ = lock_.newCondition();
-        table_ = table;
-        cfName_ = cfName;
-    }
-
-    public int getMemtableThreshold()
-    {
-        return currentSize_.get();
-    }
-
-    void resolveSize(int oldSize, int newSize)
-    {
-        currentSize_.addAndGet(newSize - oldSize);
-    }
-
-
-    boolean isThresholdViolated()
-    {
-        if (currentSize_.get() >= threshold_ || columnFamilies_.size() > 50000)
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("CURRENT SIZE:" + currentSize_.get());
-        	return true;
-        }
-        return false;
-    }
-
-    String getColumnFamily()
-    {
-    	return cfName_;
-    }
-
-    /*
-     * This version is used by the external clients to put data into
-     * the memtable. This version will respect the threshold and flush
-     * the memtable to disk when the size exceeds the threshold.
-    */
-    void put(String key, byte[] buffer) throws IOException
-    {
-        if (isThresholdViolated() )
-        {
-            lock_.lock();
-            try
-            {
-                ColumnFamilyStore cfStore = Table.open(table_).getColumnFamilyStore(cfName_);
-                if (!isFrozen_)
-                {
-                    isFrozen_ = true;
-                    BinaryMemtableManager.instance().submit(cfStore.getColumnFamilyName(), this);
-                    cfStore.switchBinaryMemtable(key, buffer);
-                }
-                else
-                {
-                    cfStore.applyBinary(key, buffer);
-                }
-            }
-            finally
-            {
-                lock_.unlock();
-            }
-        }
-        else
-        {
-            resolve(key, buffer);
-        }
-    }
-
-    private void resolve(String key, byte[] buffer)
-    {
-            columnFamilies_.put(key, buffer);
-            currentSize_.addAndGet(buffer.length + key.length());
-    }
-
-
-    /*
-     * 
-    */
-    void flush() throws IOException
-    {
-        if ( columnFamilies_.size() == 0 )
-            return;
-
-        /*
-         * Use the SSTable to write the contents of the TreeMap
-         * to disk.
-        */
-        ColumnFamilyStore cfStore = Table.open(table_).getColumnFamilyStore(cfName_);
-        List<String> keys = new ArrayList<String>( columnFamilies_.keySet() );
-        SSTableWriter writer = new SSTableWriter(cfStore.getTempSSTablePath(), keys.size(), StorageService.getPartitioner());
-        Collections.sort(keys);
-        /* Use this BloomFilter to decide if a key exists in a SSTable */
-        for ( String key : keys )
-        {           
-            byte[] bytes = columnFamilies_.get(key);
-            if ( bytes.length > 0 )
-            {            	
-                /* Now write the key and value to disk */
-                writer.append(key, bytes);
-            }
-        }
-        cfStore.storeLocation(writer.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_)));
-        columnFamilies_.clear();       
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.List;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.io.SSTableWriter;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+import org.apache.log4j.Logger;
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class BinaryMemtable
+{
+    private static Logger logger_ = Logger.getLogger( Memtable.class );
+    private int threshold_ = 512*1024*1024;
+    private AtomicInteger currentSize_ = new AtomicInteger(0);
+
+    /* Table and ColumnFamily name are used to determine the ColumnFamilyStore */
+    private String table_;
+    private String cfName_;
+    private boolean isFrozen_ = false;
+    private Map<String, byte[]> columnFamilies_ = new NonBlockingHashMap<String, byte[]>();
+    /* Lock and Condition for notifying new clients about Memtable switches */
+    Lock lock_ = new ReentrantLock();
+    Condition condition_;
+
+    BinaryMemtable(String table, String cfName) throws IOException
+    {
+        condition_ = lock_.newCondition();
+        table_ = table;
+        cfName_ = cfName;
+    }
+
+    public int getMemtableThreshold()
+    {
+        return currentSize_.get();
+    }
+
+    void resolveSize(int oldSize, int newSize)
+    {
+        currentSize_.addAndGet(newSize - oldSize);
+    }
+
+
+    boolean isThresholdViolated()
+    {
+        if (currentSize_.get() >= threshold_ || columnFamilies_.size() > 50000)
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("CURRENT SIZE:" + currentSize_.get());
+        	return true;
+        }
+        return false;
+    }
+
+    String getColumnFamily()
+    {
+    	return cfName_;
+    }
+
+    /*
+     * This version is used by the external clients to put data into
+     * the memtable. This version will respect the threshold and flush
+     * the memtable to disk when the size exceeds the threshold.
+    */
+    void put(String key, byte[] buffer) throws IOException
+    {
+        if (isThresholdViolated() )
+        {
+            lock_.lock();
+            try
+            {
+                ColumnFamilyStore cfStore = Table.open(table_).getColumnFamilyStore(cfName_);
+                if (!isFrozen_)
+                {
+                    isFrozen_ = true;
+                    BinaryMemtableManager.instance().submit(cfStore.getColumnFamilyName(), this);
+                    cfStore.switchBinaryMemtable(key, buffer);
+                }
+                else
+                {
+                    cfStore.applyBinary(key, buffer);
+                }
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        else
+        {
+            resolve(key, buffer);
+        }
+    }
+
+    private void resolve(String key, byte[] buffer)
+    {
+            columnFamilies_.put(key, buffer);
+            currentSize_.addAndGet(buffer.length + key.length());
+    }
+
+
+    /*
+     * 
+    */
+    void flush() throws IOException
+    {
+        if ( columnFamilies_.size() == 0 )
+            return;
+
+        /*
+         * Use the SSTable to write the contents of the TreeMap
+         * to disk.
+        */
+        ColumnFamilyStore cfStore = Table.open(table_).getColumnFamilyStore(cfName_);
+        List<String> keys = new ArrayList<String>( columnFamilies_.keySet() );
+        SSTableWriter writer = new SSTableWriter(cfStore.getTempSSTablePath(), keys.size(), StorageService.getPartitioner());
+        Collections.sort(keys);
+        /* Use this BloomFilter to decide if a key exists in a SSTable */
+        for ( String key : keys )
+        {           
+            byte[] bytes = columnFamilies_.get(key);
+            if ( bytes.length > 0 )
+            {            	
+                /* Now write the key and value to disk */
+                writer.append(key, bytes);
+            }
+        }
+        cfStore.storeLocation(writer.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_)));
+        columnFamilies_.clear();       
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/BinaryMemtableManager.java b/src/java/org/apache/cassandra/db/BinaryMemtableManager.java
index f91a839a00..47317f9b87 100644
--- a/src/java/org/apache/cassandra/db/BinaryMemtableManager.java
+++ b/src/java/org/apache/cassandra/db/BinaryMemtableManager.java
@@ -1,92 +1,92 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.IOException;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-
-import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
-import org.apache.cassandra.concurrent.ThreadFactoryImpl;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class BinaryMemtableManager
-{
-    private static BinaryMemtableManager instance_;
-    private static Lock lock_ = new ReentrantLock();
-    private static Logger logger_ = Logger.getLogger(BinaryMemtableManager.class);    
-
-    static BinaryMemtableManager instance() 
-    {
-        if ( instance_ == null )
-        {
-            lock_.lock();
-            try
-            {
-                if ( instance_ == null )
-                    instance_ = new BinaryMemtableManager();
-            }
-            finally
-            {
-                lock_.unlock();
-            }
-        }
-        return instance_;
-    }
-    
-    class BinaryMemtableFlusher implements Runnable
-    {
-        private BinaryMemtable memtable_;
-        
-        BinaryMemtableFlusher(BinaryMemtable memtable)
-        {
-            memtable_ = memtable;
-        }
-        
-        public void run()
-        {
-            try
-            {
-            	memtable_.flush();
-            }
-            catch (IOException e)
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug( LogUtil.throwableToString(e) );
-            }        	
-        }
-    }
-    
-    private ExecutorService flusher_ = new DebuggableThreadPoolExecutor("BINARY-MEMTABLE-FLUSHER-POOL");
-    
-    /* Submit memtables to be flushed to disk */
-    void submit(String cfName, BinaryMemtable memtbl)
-    {
-    	flusher_.submit( new BinaryMemtableFlusher(memtbl) );
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class BinaryMemtableManager
+{
+    private static BinaryMemtableManager instance_;
+    private static Lock lock_ = new ReentrantLock();
+    private static Logger logger_ = Logger.getLogger(BinaryMemtableManager.class);    
+
+    static BinaryMemtableManager instance() 
+    {
+        if ( instance_ == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                    instance_ = new BinaryMemtableManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return instance_;
+    }
+    
+    class BinaryMemtableFlusher implements Runnable
+    {
+        private BinaryMemtable memtable_;
+        
+        BinaryMemtableFlusher(BinaryMemtable memtable)
+        {
+            memtable_ = memtable;
+        }
+        
+        public void run()
+        {
+            try
+            {
+            	memtable_.flush();
+            }
+            catch (IOException e)
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug( LogUtil.throwableToString(e) );
+            }        	
+        }
+    }
+    
+    private ExecutorService flusher_ = new DebuggableThreadPoolExecutor("BINARY-MEMTABLE-FLUSHER-POOL");
+    
+    /* Submit memtables to be flushed to disk */
+    void submit(String cfName, BinaryMemtable memtbl)
+    {
+    	flusher_.submit( new BinaryMemtableFlusher(memtbl) );
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/BinaryVerbHandler.java b/src/java/org/apache/cassandra/db/BinaryVerbHandler.java
index cc0d044c81..6b4526e932 100644
--- a/src/java/org/apache/cassandra/db/BinaryVerbHandler.java
+++ b/src/java/org/apache/cassandra/db/BinaryVerbHandler.java
@@ -1,66 +1,66 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import org.apache.cassandra.db.RowMutationVerbHandler.RowMutationContext;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class BinaryVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger(BinaryVerbHandler.class);    
-    /* We use this so that we can reuse the same row mutation context for the mutation. */
-    private static ThreadLocal<RowMutationContext> tls_ = new InheritableThreadLocal<RowMutationContext>();
-    
-    public void doVerb(Message message)
-    { 
-        byte[] bytes = message.getMessageBody();
-        /* Obtain a Row Mutation Context from TLS */
-        RowMutationContext rowMutationCtx = tls_.get();
-        if ( rowMutationCtx == null )
-        {
-            rowMutationCtx = new RowMutationContext();
-            tls_.set(rowMutationCtx);
-        }                
-        rowMutationCtx.buffer_.reset(bytes, bytes.length);
-        
-	    try
-	    {
-            RowMutationMessage rmMsg = RowMutationMessage.serializer().deserialize(rowMutationCtx.buffer_);
-            RowMutation rm = rmMsg.getRowMutation();            	                
-            rowMutationCtx.row_.setKey(rm.key());
-            rm.applyBinary(rowMutationCtx.row_);
-	
-	    }        
-	    catch ( Exception e )
-	    {
-	        if (logger_.isDebugEnabled())
-                logger_.debug(LogUtil.throwableToString(e));            
-	    }        
-    }
-
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import org.apache.cassandra.db.RowMutationVerbHandler.RowMutationContext;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class BinaryVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(BinaryVerbHandler.class);    
+    /* We use this so that we can reuse the same row mutation context for the mutation. */
+    private static ThreadLocal<RowMutationContext> tls_ = new InheritableThreadLocal<RowMutationContext>();
+    
+    public void doVerb(Message message)
+    { 
+        byte[] bytes = message.getMessageBody();
+        /* Obtain a Row Mutation Context from TLS */
+        RowMutationContext rowMutationCtx = tls_.get();
+        if ( rowMutationCtx == null )
+        {
+            rowMutationCtx = new RowMutationContext();
+            tls_.set(rowMutationCtx);
+        }                
+        rowMutationCtx.buffer_.reset(bytes, bytes.length);
+        
+	    try
+	    {
+            RowMutationMessage rmMsg = RowMutationMessage.serializer().deserialize(rowMutationCtx.buffer_);
+            RowMutation rm = rmMsg.getRowMutation();            	                
+            rowMutationCtx.row_.setKey(rm.key());
+            rm.applyBinary(rowMutationCtx.row_);
+	
+	    }        
+	    catch ( Exception e )
+	    {
+	        if (logger_.isDebugEnabled())
+                logger_.debug(LogUtil.throwableToString(e));            
+	    }        
+    }
+
+}
diff --git a/src/java/org/apache/cassandra/db/CalloutDeployMessage.java b/src/java/org/apache/cassandra/db/CalloutDeployMessage.java
index aa6d470ece..8b15492cae 100644
--- a/src/java/org/apache/cassandra/db/CalloutDeployMessage.java
+++ b/src/java/org/apache/cassandra/db/CalloutDeployMessage.java
@@ -1,89 +1,89 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.cassandra.db;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.service.StorageService;
-
-
-public class CalloutDeployMessage
-{
-    private static ICompactSerializer<CalloutDeployMessage> serializer_;
-    
-    static
-    {
-        serializer_ = new CalloutDeployMessageSerializer();
-    }
-    
-    public static ICompactSerializer<CalloutDeployMessage> serializer()
-    {
-        return serializer_;
-    }
-    
-    public static Message getCalloutDeployMessage(CalloutDeployMessage cdMessage) throws IOException
-    {
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream(bos);
-        serializer_.serialize(cdMessage, dos);
-        Message message = new Message(StorageService.getLocalStorageEndPoint(), "", StorageService.calloutDeployVerbHandler_, bos.toByteArray());
-        return message;
-    }
-    
-    /* Name of the callout */
-    private String callout_;
-    /* The actual procedure */
-    private String script_;
-    
-    public CalloutDeployMessage(String callout, String script)
-    {
-        callout_ = callout;
-        script_ = script;
-    }
-    
-    String getCallout()
-    {
-        return callout_;
-    }
-    
-    String getScript()
-    {
-        return script_;
-    }
-}
-
-class CalloutDeployMessageSerializer implements ICompactSerializer<CalloutDeployMessage>
-{
-    public void serialize(CalloutDeployMessage cdMessage, DataOutputStream dos) throws IOException
-    {
-        dos.writeUTF(cdMessage.getCallout());
-        dos.writeUTF(cdMessage.getScript());
-    }
-    
-    public CalloutDeployMessage deserialize(DataInputStream dis) throws IOException
-    {
-        String callout = dis.readUTF();
-        String script = dis.readUTF();
-        return new CalloutDeployMessage(callout, script);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+
+
+public class CalloutDeployMessage
+{
+    private static ICompactSerializer<CalloutDeployMessage> serializer_;
+    
+    static
+    {
+        serializer_ = new CalloutDeployMessageSerializer();
+    }
+    
+    public static ICompactSerializer<CalloutDeployMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    public static Message getCalloutDeployMessage(CalloutDeployMessage cdMessage) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        serializer_.serialize(cdMessage, dos);
+        Message message = new Message(StorageService.getLocalStorageEndPoint(), "", StorageService.calloutDeployVerbHandler_, bos.toByteArray());
+        return message;
+    }
+    
+    /* Name of the callout */
+    private String callout_;
+    /* The actual procedure */
+    private String script_;
+    
+    public CalloutDeployMessage(String callout, String script)
+    {
+        callout_ = callout;
+        script_ = script;
+    }
+    
+    String getCallout()
+    {
+        return callout_;
+    }
+    
+    String getScript()
+    {
+        return script_;
+    }
+}
+
+class CalloutDeployMessageSerializer implements ICompactSerializer<CalloutDeployMessage>
+{
+    public void serialize(CalloutDeployMessage cdMessage, DataOutputStream dos) throws IOException
+    {
+        dos.writeUTF(cdMessage.getCallout());
+        dos.writeUTF(cdMessage.getScript());
+    }
+    
+    public CalloutDeployMessage deserialize(DataInputStream dis) throws IOException
+    {
+        String callout = dis.readUTF();
+        String script = dis.readUTF();
+        return new CalloutDeployMessage(callout, script);
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/CalloutDeployVerbHandler.java b/src/java/org/apache/cassandra/db/CalloutDeployVerbHandler.java
index 182b104e07..3c50b720c7 100644
--- a/src/java/org/apache/cassandra/db/CalloutDeployVerbHandler.java
+++ b/src/java/org/apache/cassandra/db/CalloutDeployVerbHandler.java
@@ -1,49 +1,49 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.cassandra.db;
-
-import java.io.IOException;
-
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-
-public class CalloutDeployVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger(CalloutDeployVerbHandler.class);
-    
-    public void doVerb(Message message)
-    {
-        byte[] bytes = message.getMessageBody();
-        DataInputBuffer bufIn = new DataInputBuffer();
-        bufIn.reset(bytes, bytes.length);
-        try
-        {
-            CalloutDeployMessage cdMessage = CalloutDeployMessage.serializer().deserialize(bufIn);
-            /* save the callout to callout cache and to disk. */
-            CalloutManager.instance().addCallout( cdMessage.getCallout(), cdMessage.getScript() );
-        }
-        catch ( IOException ex )
-        {
-            logger_.warn(LogUtil.throwableToString(ex));
-        }        
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+public class CalloutDeployVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(CalloutDeployVerbHandler.class);
+    
+    public void doVerb(Message message)
+    {
+        byte[] bytes = message.getMessageBody();
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(bytes, bytes.length);
+        try
+        {
+            CalloutDeployMessage cdMessage = CalloutDeployMessage.serializer().deserialize(bufIn);
+            /* save the callout to callout cache and to disk. */
+            CalloutManager.instance().addCallout( cdMessage.getCallout(), cdMessage.getScript() );
+        }
+        catch ( IOException ex )
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }        
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/CalloutManager.java b/src/java/org/apache/cassandra/db/CalloutManager.java
index b337df0d77..a7e6a9b1d0 100644
--- a/src/java/org/apache/cassandra/db/CalloutManager.java
+++ b/src/java/org/apache/cassandra/db/CalloutManager.java
@@ -1,211 +1,211 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.cassandra.db;
-
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.util.List;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-
-import javax.script.Bindings;
-import javax.script.Invocable;
-import javax.script.ScriptEngine;
-import javax.script.ScriptEngineManager;
-import javax.script.Compilable;
-import javax.script.CompiledScript;
-import javax.script.ScriptException;
-import javax.script.SimpleBindings;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.procedures.GroovyScriptRunner;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.cassandra.utils.FileUtils;
-
-import org.apache.log4j.Logger;
-
-public class CalloutManager
-{
-    private final static Logger logger_ = Logger.getLogger(CalloutManager.class); 
-    private static final String extn_ = ".groovy";
-    /* Used to lock the factory for creation of CalloutManager instance */
-    private static Lock createLock_ = new ReentrantLock();
-    /* An instance of the CalloutManager  */
-    private static CalloutManager instance_;
-    
-    public static CalloutManager instance()
-    {
-        if ( instance_ == null )
-        {
-            CalloutManager.createLock_.lock();
-            try
-            {
-                if ( instance_ == null )
-                {
-                    instance_ = new CalloutManager();
-                }
-            }
-            finally
-            {
-                CalloutManager.createLock_.unlock();
-            }
-        }
-        return instance_;
-    }
-    
-    /* Map containing the name of callout as key and the callout script as value */
-    private Map<String, CompiledScript> calloutCache_ = new HashMap<String, CompiledScript>();    
-    /* The Groovy Script compiler instance */
-    private Compilable compiler_;
-    /* The Groovy script invokable instance */
-    private Invocable invokable_;
-    
-    private CalloutManager()
-    {
-        ScriptEngineManager scriptManager = new ScriptEngineManager();
-        ScriptEngine groovyEngine = scriptManager.getEngineByName("groovy");
-        compiler_ = (Compilable)groovyEngine;
-        invokable_ = (Invocable)groovyEngine;
-    }
-    
-    /**
-     * Compile the script and cache the compiled script.
-     * @param script to be compiled
-     * @throws ScriptException
-     */
-    private void compileAndCache(String scriptId, String script) throws ScriptException
-    {
-        if ( compiler_ != null )
-        {
-            CompiledScript compiledScript = compiler_.compile(script);
-            calloutCache_.put(scriptId, compiledScript);
-        }
-    }
-    
-    /**
-     * Invoked on start up to load all the stored callouts, compile
-     * and cache them.
-     * 
-     * @throws IOException
-     */
-    public void onStart() throws IOException
-    {
-    	String location = DatabaseDescriptor.getCalloutLocation();
-    	if ( location == null )
-    		return;
-    	
-        FileUtils.createDirectory(location);
-        
-        File[] files = new File(location).listFiles();
-        
-        for ( File file : files )
-        {
-            String f = file.getName();
-            /* Get the callout name from the file */
-            String callout = f.split(extn_)[0];
-            FileInputStream fis = new FileInputStream(file);
-            byte[] bytes = new byte[fis.available()];
-            fis.read(bytes);
-            fis.close();
-            /* cache the callout after compiling it */
-            try
-            {
-                compileAndCache(callout, new String(bytes));                    
-            }
-            catch ( ScriptException ex )
-            {
-                logger_.warn(LogUtil.throwableToString(ex));
-            }
-        }
-    }
-    
-    /**
-     * Store the callout in cache and write it out
-     * to disk.
-     * @param callout the name of the callout
-     * @param script actual implementation of the callout
-    */
-    public void addCallout(String callout, String script) throws IOException
-    {
-        /* cache the script */
-        /* cache the callout after compiling it */
-        try
-        {
-            compileAndCache(callout, script);                    
-        }
-        catch ( ScriptException ex )
-        {
-            logger_.warn(LogUtil.throwableToString(ex));
-        }
-        /* save the script to disk */
-        String scriptFile = DatabaseDescriptor.getCalloutLocation() + File.separator + callout + extn_;
-        File file = new File(scriptFile);
-        if ( file.exists() )
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Deleting the old script file ...");
-            file.delete();
-        }
-        FileOutputStream fos = new FileOutputStream(scriptFile);
-        fos.write(script.getBytes());
-        fos.close();
-    }
-    
-    /**
-     * Remove the registered callout and delete the
-     * script on the disk.
-     * @param callout to be removed
-     */
-    public void removeCallout(String callout)
-    {
-        /* remove the script from cache */
-        calloutCache_.remove(callout);
-        String scriptFile = DatabaseDescriptor.getCalloutLocation() + File.separator + callout + ".grv";
-        File file = new File(scriptFile);
-        file.delete();
-    }
-    
-    /**
-     * Execute the specified callout.
-     * @param callout to be executed.
-     * @param args arguments to be passed to the callouts.
-     */
-    public Object executeCallout(String callout, Object ... args)
-    {
-        Object result = null;
-        CompiledScript script = calloutCache_.get(callout);
-        if ( script != null )
-        {
-            try
-            {
-                Bindings binding = new SimpleBindings();
-                binding.put("args", args);
-                result = script.eval(binding);
-            }
-            catch(ScriptException ex)
-            {
-                logger_.warn(LogUtil.throwableToString(ex));
-            }
-        }
-        return result;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.util.List;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import javax.script.Bindings;
+import javax.script.Invocable;
+import javax.script.ScriptEngine;
+import javax.script.ScriptEngineManager;
+import javax.script.Compilable;
+import javax.script.CompiledScript;
+import javax.script.ScriptException;
+import javax.script.SimpleBindings;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.procedures.GroovyScriptRunner;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.cassandra.utils.FileUtils;
+
+import org.apache.log4j.Logger;
+
+public class CalloutManager
+{
+    private final static Logger logger_ = Logger.getLogger(CalloutManager.class); 
+    private static final String extn_ = ".groovy";
+    /* Used to lock the factory for creation of CalloutManager instance */
+    private static Lock createLock_ = new ReentrantLock();
+    /* An instance of the CalloutManager  */
+    private static CalloutManager instance_;
+    
+    public static CalloutManager instance()
+    {
+        if ( instance_ == null )
+        {
+            CalloutManager.createLock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                {
+                    instance_ = new CalloutManager();
+                }
+            }
+            finally
+            {
+                CalloutManager.createLock_.unlock();
+            }
+        }
+        return instance_;
+    }
+    
+    /* Map containing the name of callout as key and the callout script as value */
+    private Map<String, CompiledScript> calloutCache_ = new HashMap<String, CompiledScript>();    
+    /* The Groovy Script compiler instance */
+    private Compilable compiler_;
+    /* The Groovy script invokable instance */
+    private Invocable invokable_;
+    
+    private CalloutManager()
+    {
+        ScriptEngineManager scriptManager = new ScriptEngineManager();
+        ScriptEngine groovyEngine = scriptManager.getEngineByName("groovy");
+        compiler_ = (Compilable)groovyEngine;
+        invokable_ = (Invocable)groovyEngine;
+    }
+    
+    /**
+     * Compile the script and cache the compiled script.
+     * @param script to be compiled
+     * @throws ScriptException
+     */
+    private void compileAndCache(String scriptId, String script) throws ScriptException
+    {
+        if ( compiler_ != null )
+        {
+            CompiledScript compiledScript = compiler_.compile(script);
+            calloutCache_.put(scriptId, compiledScript);
+        }
+    }
+    
+    /**
+     * Invoked on start up to load all the stored callouts, compile
+     * and cache them.
+     * 
+     * @throws IOException
+     */
+    public void onStart() throws IOException
+    {
+    	String location = DatabaseDescriptor.getCalloutLocation();
+    	if ( location == null )
+    		return;
+    	
+        FileUtils.createDirectory(location);
+        
+        File[] files = new File(location).listFiles();
+        
+        for ( File file : files )
+        {
+            String f = file.getName();
+            /* Get the callout name from the file */
+            String callout = f.split(extn_)[0];
+            FileInputStream fis = new FileInputStream(file);
+            byte[] bytes = new byte[fis.available()];
+            fis.read(bytes);
+            fis.close();
+            /* cache the callout after compiling it */
+            try
+            {
+                compileAndCache(callout, new String(bytes));                    
+            }
+            catch ( ScriptException ex )
+            {
+                logger_.warn(LogUtil.throwableToString(ex));
+            }
+        }
+    }
+    
+    /**
+     * Store the callout in cache and write it out
+     * to disk.
+     * @param callout the name of the callout
+     * @param script actual implementation of the callout
+    */
+    public void addCallout(String callout, String script) throws IOException
+    {
+        /* cache the script */
+        /* cache the callout after compiling it */
+        try
+        {
+            compileAndCache(callout, script);                    
+        }
+        catch ( ScriptException ex )
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }
+        /* save the script to disk */
+        String scriptFile = DatabaseDescriptor.getCalloutLocation() + File.separator + callout + extn_;
+        File file = new File(scriptFile);
+        if ( file.exists() )
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Deleting the old script file ...");
+            file.delete();
+        }
+        FileOutputStream fos = new FileOutputStream(scriptFile);
+        fos.write(script.getBytes());
+        fos.close();
+    }
+    
+    /**
+     * Remove the registered callout and delete the
+     * script on the disk.
+     * @param callout to be removed
+     */
+    public void removeCallout(String callout)
+    {
+        /* remove the script from cache */
+        calloutCache_.remove(callout);
+        String scriptFile = DatabaseDescriptor.getCalloutLocation() + File.separator + callout + ".grv";
+        File file = new File(scriptFile);
+        file.delete();
+    }
+    
+    /**
+     * Execute the specified callout.
+     * @param callout to be executed.
+     * @param args arguments to be passed to the callouts.
+     */
+    public Object executeCallout(String callout, Object ... args)
+    {
+        Object result = null;
+        CompiledScript script = calloutCache_.get(callout);
+        if ( script != null )
+        {
+            try
+            {
+                Bindings binding = new SimpleBindings();
+                binding.put("args", args);
+                result = script.eval(binding);
+            }
+            catch(ScriptException ex)
+            {
+                logger_.warn(LogUtil.throwableToString(ex));
+            }
+        }
+        return result;
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/Column.java b/src/java/org/apache/cassandra/db/Column.java
index ed67709830..d2d9db52ac 100644
--- a/src/java/org/apache/cassandra/db/Column.java
+++ b/src/java/org/apache/cassandra/db/Column.java
@@ -1,210 +1,210 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.util.Collection;
-import java.nio.ByteBuffer;
-
-import org.apache.commons.lang.ArrayUtils;
-
-import org.apache.cassandra.db.marshal.AbstractType;
-
-
-/**
- * Column is immutable, which prevents all kinds of confusion in a multithreaded environment.
- * (TODO: look at making SuperColumn immutable too.  This is trickier but is probably doable
- *  with something like PCollections -- http://code.google.com
- *
- * Author : Avinash Lakshman ( alakshman@facebook.com ) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public final class Column implements IColumn
-{
-    private static ColumnSerializer serializer_ = new ColumnSerializer();
-
-    static ColumnSerializer serializer()
-    {
-        return serializer_;
-    }
-
-    private final byte[] name;
-    private final byte[] value;
-    private final long timestamp;
-    private final boolean isMarkedForDelete;
-
-    Column(byte[] name)
-    {
-        this(name, ArrayUtils.EMPTY_BYTE_ARRAY);
-    }
-
-    Column(byte[] name, byte[] value)
-    {
-        this(name, value, 0);
-    }
-
-    public Column(byte[] name, byte[] value, long timestamp)
-    {
-        this(name, value, timestamp, false);
-    }
-
-    public Column(byte[] name, byte[] value, long timestamp, boolean isDeleted)
-    {
-        assert name != null;
-        assert value != null;
-        this.name = name;
-        this.value = value;
-        this.timestamp = timestamp;
-        isMarkedForDelete = isDeleted;
-    }
-
-    public byte[] name()
-    {
-        return name;
-    }
-
-    public Column getSubColumn(byte[] columnName)
-    {
-        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
-    }
-
-    public byte[] value()
-    {
-        return value;
-    }
-
-    public byte[] value(byte[] key)
-    {
-        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
-    }
-
-    public Collection<IColumn> getSubColumns()
-    {
-        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
-    }
-
-    public int getObjectCount()
-    {
-        return 1;
-    }
-
-    public long timestamp()
-    {
-        return timestamp;
-    }
-
-    public long timestamp(byte[] key)
-    {
-        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
-    }
-
-    public boolean isMarkedForDelete()
-    {
-        return isMarkedForDelete;
-    }
-
-    public long getMarkedForDeleteAt()
-    {
-        if (!isMarkedForDelete())
-        {
-            throw new IllegalStateException("column is not marked for delete");
-        }
-        return timestamp;
-    }
-
-    public int size()
-    {
-        /*
-         * Size of a column is =
-         *   size of a name (UtfPrefix + length of the string)
-         * + 1 byte to indicate if the column has been deleted
-         * + 8 bytes for timestamp
-         * + 4 bytes which basically indicates the size of the byte array
-         * + entire byte array.
-        */
-
-        /*
-           * We store the string as UTF-8 encoded, so when we calculate the length, it
-           * should be converted to UTF-8.
-           */
-        return IColumn.UtfPrefix_ + name.length + DBConstants.boolSize_ + DBConstants.tsSize_ + DBConstants.intSize_ + value.length;
-    }
-
-    /*
-     * This returns the size of the column when serialized.
-     * @see com.facebook.infrastructure.db.IColumn#serializedSize()
-    */
-    public int serializedSize()
-    {
-        return size();
-    }
-
-    public void addColumn(IColumn column)
-    {
-        throw new UnsupportedOperationException("This operation is not supported for simple columns.");
-    }
-
-    public IColumn diff(IColumn column)
-    {
-        if (timestamp() < column.timestamp())
-        {
-            return column;
-        }
-        return null;
-    }
-
-    public byte[] digest()
-    {
-        StringBuilder stringBuilder = new StringBuilder();
-        stringBuilder.append(name);
-        stringBuilder.append(":");
-        stringBuilder.append(timestamp);
-        return stringBuilder.toString().getBytes();
-    }
-
-    public int getLocalDeletionTime()
-    {
-        assert isMarkedForDelete;
-        return ByteBuffer.wrap(value).getInt();
-    }
-
-    // note that we do not call this simply compareTo since it also makes sense to compare Columns by name
-    public long comparePriority(Column o)
-    {
-        if (isMarkedForDelete)
-        {
-            // tombstone always wins ties.
-            return timestamp < o.timestamp ? -1 : 1;
-        }
-        return timestamp - o.timestamp;
-    }
-
-    public String getString(AbstractType comparator)
-    {
-        StringBuilder sb = new StringBuilder();
-        sb.append(comparator.getString(name));
-        sb.append(":");
-        sb.append(isMarkedForDelete());
-        sb.append(":");
-        sb.append(value.length);
-        sb.append("@");
-        sb.append(timestamp());
-        return sb.toString();
-    }
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.Collection;
+import java.nio.ByteBuffer;
+
+import org.apache.commons.lang.ArrayUtils;
+
+import org.apache.cassandra.db.marshal.AbstractType;
+
+
+/**
+ * Column is immutable, which prevents all kinds of confusion in a multithreaded environment.
+ * (TODO: look at making SuperColumn immutable too.  This is trickier but is probably doable
+ *  with something like PCollections -- http://code.google.com
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com ) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class Column implements IColumn
+{
+    private static ColumnSerializer serializer_ = new ColumnSerializer();
+
+    static ColumnSerializer serializer()
+    {
+        return serializer_;
+    }
+
+    private final byte[] name;
+    private final byte[] value;
+    private final long timestamp;
+    private final boolean isMarkedForDelete;
+
+    Column(byte[] name)
+    {
+        this(name, ArrayUtils.EMPTY_BYTE_ARRAY);
+    }
+
+    Column(byte[] name, byte[] value)
+    {
+        this(name, value, 0);
+    }
+
+    public Column(byte[] name, byte[] value, long timestamp)
+    {
+        this(name, value, timestamp, false);
+    }
+
+    public Column(byte[] name, byte[] value, long timestamp, boolean isDeleted)
+    {
+        assert name != null;
+        assert value != null;
+        this.name = name;
+        this.value = value;
+        this.timestamp = timestamp;
+        isMarkedForDelete = isDeleted;
+    }
+
+    public byte[] name()
+    {
+        return name;
+    }
+
+    public Column getSubColumn(byte[] columnName)
+    {
+        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
+    }
+
+    public byte[] value()
+    {
+        return value;
+    }
+
+    public byte[] value(byte[] key)
+    {
+        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
+    }
+
+    public Collection<IColumn> getSubColumns()
+    {
+        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
+    }
+
+    public int getObjectCount()
+    {
+        return 1;
+    }
+
+    public long timestamp()
+    {
+        return timestamp;
+    }
+
+    public long timestamp(byte[] key)
+    {
+        throw new UnsupportedOperationException("This operation is unsupported on simple columns.");
+    }
+
+    public boolean isMarkedForDelete()
+    {
+        return isMarkedForDelete;
+    }
+
+    public long getMarkedForDeleteAt()
+    {
+        if (!isMarkedForDelete())
+        {
+            throw new IllegalStateException("column is not marked for delete");
+        }
+        return timestamp;
+    }
+
+    public int size()
+    {
+        /*
+         * Size of a column is =
+         *   size of a name (UtfPrefix + length of the string)
+         * + 1 byte to indicate if the column has been deleted
+         * + 8 bytes for timestamp
+         * + 4 bytes which basically indicates the size of the byte array
+         * + entire byte array.
+        */
+
+        /*
+           * We store the string as UTF-8 encoded, so when we calculate the length, it
+           * should be converted to UTF-8.
+           */
+        return IColumn.UtfPrefix_ + name.length + DBConstants.boolSize_ + DBConstants.tsSize_ + DBConstants.intSize_ + value.length;
+    }
+
+    /*
+     * This returns the size of the column when serialized.
+     * @see com.facebook.infrastructure.db.IColumn#serializedSize()
+    */
+    public int serializedSize()
+    {
+        return size();
+    }
+
+    public void addColumn(IColumn column)
+    {
+        throw new UnsupportedOperationException("This operation is not supported for simple columns.");
+    }
+
+    public IColumn diff(IColumn column)
+    {
+        if (timestamp() < column.timestamp())
+        {
+            return column;
+        }
+        return null;
+    }
+
+    public byte[] digest()
+    {
+        StringBuilder stringBuilder = new StringBuilder();
+        stringBuilder.append(name);
+        stringBuilder.append(":");
+        stringBuilder.append(timestamp);
+        return stringBuilder.toString().getBytes();
+    }
+
+    public int getLocalDeletionTime()
+    {
+        assert isMarkedForDelete;
+        return ByteBuffer.wrap(value).getInt();
+    }
+
+    // note that we do not call this simply compareTo since it also makes sense to compare Columns by name
+    public long comparePriority(Column o)
+    {
+        if (isMarkedForDelete)
+        {
+            // tombstone always wins ties.
+            return timestamp < o.timestamp ? -1 : 1;
+        }
+        return timestamp - o.timestamp;
+    }
+
+    public String getString(AbstractType comparator)
+    {
+        StringBuilder sb = new StringBuilder();
+        sb.append(comparator.getString(name));
+        sb.append(":");
+        sb.append(isMarkedForDelete());
+        sb.append(":");
+        sb.append(value.length);
+        sb.append("@");
+        sb.append(timestamp());
+        return sb.toString();
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/db/ColumnFamily.java b/src/java/org/apache/cassandra/db/ColumnFamily.java
index 6ffdd4fe13..7461ed2e90 100644
--- a/src/java/org/apache/cassandra/db/ColumnFamily.java
+++ b/src/java/org/apache/cassandra/db/ColumnFamily.java
@@ -1,523 +1,523 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.lang.reflect.Proxy;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.SortedSet;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.ConcurrentSkipListMap;
-
-import org.apache.commons.lang.ArrayUtils;
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.db.filter.QueryPath;
-import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.db.marshal.MarshalException;
-import org.apache.cassandra.db.marshal.LongType;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public final class ColumnFamily
-{
-    /* The column serializer for this Column Family. Create based on config. */
-    private static ICompactSerializer<ColumnFamily> serializer_;
-    public static final short utfPrefix_ = 2;   
-
-    private static Logger logger_ = Logger.getLogger( ColumnFamily.class );
-    private static Map<String, String> columnTypes_ = new HashMap<String, String>();
-    private String type_;
-    private String table_;
-
-    static
-    {
-        serializer_ = new ColumnFamilySerializer();
-        /* TODO: These are the various column types. Hard coded for now. */
-        columnTypes_.put("Standard", "Standard");
-        columnTypes_.put("Super", "Super");
-    }
-
-    public static ICompactSerializer<ColumnFamily> serializer()
-    {
-        return serializer_;
-    }
-
-    /*
-     * This method returns the serializer whose methods are
-     * preprocessed by a dynamic proxy.
-    */
-    public static ICompactSerializer<ColumnFamily> serializerWithIndexes()
-    {
-        return (ICompactSerializer<ColumnFamily>)Proxy.newProxyInstance( ColumnFamily.class.getClassLoader(), new Class[]{ICompactSerializer.class}, new CompactSerializerInvocationHandler<ColumnFamily>(serializer_) );
-    }
-
-    public static String getColumnType(String key)
-    {
-    	if ( key == null )
-    		return columnTypes_.get("Standard");
-    	return columnTypes_.get(key);
-    }
-
-    public static ColumnFamily create(String tableName, String cfName)
-    {
-        String columnType = DatabaseDescriptor.getColumnFamilyType(tableName, cfName);
-        AbstractType comparator = DatabaseDescriptor.getComparator(tableName, cfName);
-        AbstractType subcolumnComparator = DatabaseDescriptor.getSubComparator(tableName, cfName);
-        return new ColumnFamily(cfName, columnType, comparator, subcolumnComparator);
-    }
-
-    private String name_;
-
-    private transient ICompactSerializer<IColumn> columnSerializer_;
-    private long markedForDeleteAt = Long.MIN_VALUE;
-    private int localDeletionTime = Integer.MIN_VALUE;
-    private AtomicInteger size_ = new AtomicInteger(0);
-    private ConcurrentSkipListMap<byte[], IColumn> columns_;
-
-    public ColumnFamily(String cfName, String columnType, AbstractType comparator, AbstractType subcolumnComparator)
-    {
-        name_ = cfName;
-        type_ = columnType;
-        columnSerializer_ = columnType.equals("Standard") ? Column.serializer() : SuperColumn.serializer(subcolumnComparator);
-        columns_ = new ConcurrentSkipListMap<byte[], IColumn>(comparator);
-    }
-
-    public ColumnFamily cloneMeShallow()
-    {
-        ColumnFamily cf = new ColumnFamily(name_, type_, getComparator(), getSubComparator());
-        cf.markedForDeleteAt = markedForDeleteAt;
-        cf.localDeletionTime = localDeletionTime;
-        return cf;
-    }
-
-    private AbstractType getSubComparator()
-    {
-        return (columnSerializer_ instanceof SuperColumnSerializer) ? ((SuperColumnSerializer)columnSerializer_).getComparator() : null;
-    }
-
-    ColumnFamily cloneMe()
-    {
-        ColumnFamily cf = cloneMeShallow();
-        cf.columns_ = columns_.clone();
-    	return cf;
-    }
-
-    public String name()
-    {
-        return name_;
-    }
-
-    /*
-     *  We need to go through each column
-     *  in the column family and resolve it before adding
-    */
-    void addColumns(ColumnFamily cf)
-    {
-        for (IColumn column : cf.getSortedColumns())
-        {
-            addColumn(column);
-        }
-    }
-
-    public ICompactSerializer<IColumn> getColumnSerializer()
-    {
-    	return columnSerializer_;
-    }
-
-    int getColumnCount()
-    {
-    	int count = 0;
-        if(!isSuper())
-        {
-            count = columns_.size();
-        }
-        else
-        {
-            for(IColumn column: columns_.values())
-            {
-                count += column.getObjectCount();
-            }
-        }
-    	return count;
-    }
-
-    public boolean isSuper()
-    {
-        return type_.equals("Super");
-    }
-
-    public void addColumn(QueryPath path, byte[] value, long timestamp)
-    {
-        addColumn(path, value, timestamp, false);
-    }
-
-    /** In most places the CF must be part of a QueryPath but here it is ignored. */
-    public void addColumn(QueryPath path, byte[] value, long timestamp, boolean deleted)
-	{
-        assert path.columnName != null : path;
-		IColumn column;
-        if (path.superColumnName == null)
-        {
-            try
-            {
-                getComparator().validate(path.columnName);
-            }
-            catch (Exception e)
-            {
-                throw new MarshalException("Invalid column name in " + path.columnFamilyName + " for " + getComparator().getClass().getName());
-            }
-            column = new Column(path.columnName, value, timestamp, deleted);
-        }
-        else
-        {
-            assert isSuper();
-            try
-            {
-                getComparator().validate(path.superColumnName);
-            }
-            catch (Exception e)
-            {
-                throw new MarshalException("Invalid supercolumn name in " + path.columnFamilyName + " for " + getComparator().getClass().getName());
-            }
-            column = new SuperColumn(path.superColumnName, getSubComparator());
-            column.addColumn(new Column(path.columnName, value, timestamp, deleted)); // checks subcolumn name
-        }
-		addColumn(column);
-    }
-
-    public void clear()
-    {
-    	columns_.clear();
-    	size_.set(0);
-    }
-
-    /*
-     * If we find an old column that has the same name
-     * the ask it to resolve itself else add the new column .
-    */
-    public void addColumn(IColumn column)
-    {
-        byte[] name = column.name();
-        IColumn oldColumn = columns_.get(name);
-        if (oldColumn != null)
-        {
-            if (oldColumn instanceof SuperColumn)
-            {
-                int oldSize = oldColumn.size();
-                ((SuperColumn) oldColumn).putColumn(column);
-                size_.addAndGet(oldColumn.size() - oldSize);
-            }
-            else
-            {
-                if (((Column)oldColumn).comparePriority((Column)column) <= 0)
-                {
-                    columns_.put(name, column);
-                    size_.addAndGet(column.size());
-                }
-            }
-        }
-        else
-        {
-            size_.addAndGet(column.size());
-            columns_.put(name, column);
-        }
-    }
-
-    public IColumn getColumn(byte[] name)
-    {
-        return columns_.get(name);
-    }
-
-    public SortedSet<byte[]> getColumnNames()
-    {
-        return columns_.keySet();
-    }
-
-    public Collection<IColumn> getSortedColumns()
-    {
-        return columns_.values();
-    }
-
-    public Map<byte[], IColumn> getColumnsMap()
-    {
-        return columns_;
-    }
-
-    public void remove(byte[] columnName)
-    {
-    	columns_.remove(columnName);
-    }
-
-    public void delete(int localtime, long timestamp)
-    {
-        localDeletionTime = localtime;
-        markedForDeleteAt = timestamp;
-    }
-
-    public void delete(ColumnFamily cf2)
-    {
-        delete(Math.max(getLocalDeletionTime(), cf2.getLocalDeletionTime()),
-               Math.max(getMarkedForDeleteAt(), cf2.getMarkedForDeleteAt()));
-    }
-
-    public boolean isMarkedForDelete()
-    {
-        return markedForDeleteAt > Long.MIN_VALUE;
-    }
-
-    /*
-     * This function will calculate the difference between 2 column families.
-     * The external input is assumed to be a superset of internal.
-     */
-    ColumnFamily diff(ColumnFamily cfComposite)
-    {
-    	ColumnFamily cfDiff = new ColumnFamily(cfComposite.name(), cfComposite.type_, getComparator(), getSubComparator());
-        if (cfComposite.getMarkedForDeleteAt() > getMarkedForDeleteAt())
-        {
-            cfDiff.delete(cfComposite.getLocalDeletionTime(), cfComposite.getMarkedForDeleteAt());
-        }
-
-        // (don't need to worry about cfNew containing IColumns that are shadowed by
-        // the delete tombstone, since cfNew was generated by CF.resolve, which
-        // takes care of those for us.)
-        Map<byte[], IColumn> columns = cfComposite.getColumnsMap();
-        Set<byte[]> cNames = columns.keySet();
-        for (byte[] cName : cNames)
-        {
-            IColumn columnInternal = columns_.get(cName);
-            IColumn columnExternal = columns.get(cName);
-            if (columnInternal == null)
-            {
-                cfDiff.addColumn(columnExternal);
-            }
-            else
-            {
-                IColumn columnDiff = columnInternal.diff(columnExternal);
-                if (columnDiff != null)
-                {
-                    cfDiff.addColumn(columnDiff);
-                }
-            }
-        }
-
-        if (!cfDiff.getColumnsMap().isEmpty() || cfDiff.isMarkedForDelete())
-        	return cfDiff;
-        else
-        	return null;
-    }
-
-    public AbstractType getComparator()
-    {
-        return (AbstractType)columns_.comparator();
-    }
-
-    int size()
-    {
-        if (size_.get() == 0)
-        {
-            for (IColumn column : columns_.values())
-            {
-                size_.addAndGet(column.size());
-            }
-        }
-        return size_.get();
-    }
-
-    public int hashCode()
-    {
-        return name().hashCode();
-    }
-
-    public boolean equals(Object o)
-    {
-        if ( !(o instanceof ColumnFamily) )
-            return false;
-        ColumnFamily cf = (ColumnFamily)o;
-        return name().equals(cf.name());
-    }
-
-    public String toString()
-    {
-    	StringBuilder sb = new StringBuilder();
-        sb.append("ColumnFamily(");
-    	sb.append(name_);
-
-        if (isMarkedForDelete()) {
-            sb.append(" -delete at " + getMarkedForDeleteAt() + "-");
-        }
-
-    	sb.append(" [");
-        sb.append(getComparator().getColumnsString(getSortedColumns()));
-        sb.append("])");
-
-    	return sb.toString();
-    }
-
-    public byte[] digest()
-    {
-        byte[] xorHash = ArrayUtils.EMPTY_BYTE_ARRAY;
-        for (IColumn column : columns_.values())
-        {
-            if (xorHash.length == 0)
-            {
-                xorHash = column.digest();
-            }
-            else
-            {
-                xorHash = FBUtilities.xor(xorHash, column.digest());
-            }
-        }
-        return xorHash;
-    }
-
-    public long getMarkedForDeleteAt()
-    {
-        return markedForDeleteAt;
-    }
-
-    public int getLocalDeletionTime()
-    {
-        return localDeletionTime;
-    }
-
-    public String type()
-    {
-        return type_;
-    }
-
-    /** merge all columnFamilies into a single instance, with only the newest versions of columns preserved. */
-    static ColumnFamily resolve(List<ColumnFamily> columnFamilies)
-    {
-        int size = columnFamilies.size();
-        if (size == 0)
-            return null;
-
-        // start from nothing so that we don't include potential deleted columns from the first instance
-        ColumnFamily cf0 = columnFamilies.get(0);
-        ColumnFamily cf = cf0.cloneMeShallow();
-
-        // merge
-        for (ColumnFamily cf2 : columnFamilies)
-        {
-            assert cf.name().equals(cf2.name());
-            cf.addColumns(cf2);
-            cf.delete(cf2);
-        }
-        return cf;
-    }
-
-    public static class ColumnFamilySerializer implements ICompactSerializer<ColumnFamily>
-    {
-        /*
-         * We are going to create indexes, and write out that information as well. The format
-         * of the data serialized is as follows.
-         *
-         * 1) Without indexes:
-         *  // written by the data
-         * 	<boolean false (index is not present)>
-         * 	<column family id>
-         * 	<is marked for delete>
-         * 	<total number of columns>
-         * 	<columns data>
-
-         * 	<boolean true (index is present)>
-         *
-         *  This part is written by the column indexer
-         * 	<size of index in bytes>
-         * 	<list of column names and their offsets relative to the first column>
-         *
-         *  <size of the cf in bytes>
-         * 	<column family id>
-         * 	<is marked for delete>
-         * 	<total number of columns>
-         * 	<columns data>
-        */
-        public void serialize(ColumnFamily columnFamily, DataOutputStream dos) throws IOException
-        {
-            // TODO whenever we change this we need to change the code in SequenceFile to match in two places.
-            // This SUCKS and is inefficient to boot.  let's fix this ASAP. 
-            Collection<IColumn> columns = columnFamily.getSortedColumns();
-
-            dos.writeUTF(columnFamily.name());
-            dos.writeUTF(columnFamily.type_);
-            dos.writeUTF(columnFamily.getComparator().getClass().getCanonicalName());
-            AbstractType subcolumnComparator = columnFamily.getSubComparator();
-            dos.writeUTF(subcolumnComparator == null ? "" : subcolumnComparator.getClass().getCanonicalName());
-            dos.writeInt(columnFamily.localDeletionTime);
-            dos.writeLong(columnFamily.markedForDeleteAt);
-
-            dos.writeInt(columns.size());
-            for ( IColumn column : columns )
-            {
-                columnFamily.getColumnSerializer().serialize(column, dos);
-            }
-        }
-
-        public ColumnFamily deserialize(DataInputStream dis) throws IOException
-        {
-            ColumnFamily cf = new ColumnFamily(dis.readUTF(),
-                                               dis.readUTF(),
-                                               readComparator(dis),
-                                               readComparator(dis));
-            cf.delete(dis.readInt(), dis.readLong());
-            int size = dis.readInt();
-            IColumn column;
-            for (int i = 0; i < size; ++i)
-            {
-                column = cf.getColumnSerializer().deserialize(dis);
-                cf.addColumn(column);
-            }
-            return cf;
-        }
-
-        private AbstractType readComparator(DataInputStream dis) throws IOException
-        {
-            String className = dis.readUTF();
-            if (className.equals(""))
-            {
-                return null;
-            }
-
-            try
-            {
-                return (AbstractType)Class.forName(className).getConstructor().newInstance();
-            }
-            catch (ClassNotFoundException e)
-            {
-                throw new RuntimeException("Unable to load comparator class '" + className + "'.  probably this means you have obsolete sstables lying around", e);
-            }
-            catch (Exception e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-    }
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.lang.reflect.Proxy;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.SortedSet;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.ConcurrentSkipListMap;
+
+import org.apache.commons.lang.ArrayUtils;
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.db.marshal.MarshalException;
+import org.apache.cassandra.db.marshal.LongType;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public final class ColumnFamily
+{
+    /* The column serializer for this Column Family. Create based on config. */
+    private static ICompactSerializer<ColumnFamily> serializer_;
+    public static final short utfPrefix_ = 2;   
+
+    private static Logger logger_ = Logger.getLogger( ColumnFamily.class );
+    private static Map<String, String> columnTypes_ = new HashMap<String, String>();
+    private String type_;
+    private String table_;
+
+    static
+    {
+        serializer_ = new ColumnFamilySerializer();
+        /* TODO: These are the various column types. Hard coded for now. */
+        columnTypes_.put("Standard", "Standard");
+        columnTypes_.put("Super", "Super");
+    }
+
+    public static ICompactSerializer<ColumnFamily> serializer()
+    {
+        return serializer_;
+    }
+
+    /*
+     * This method returns the serializer whose methods are
+     * preprocessed by a dynamic proxy.
+    */
+    public static ICompactSerializer<ColumnFamily> serializerWithIndexes()
+    {
+        return (ICompactSerializer<ColumnFamily>)Proxy.newProxyInstance( ColumnFamily.class.getClassLoader(), new Class[]{ICompactSerializer.class}, new CompactSerializerInvocationHandler<ColumnFamily>(serializer_) );
+    }
+
+    public static String getColumnType(String key)
+    {
+    	if ( key == null )
+    		return columnTypes_.get("Standard");
+    	return columnTypes_.get(key);
+    }
+
+    public static ColumnFamily create(String tableName, String cfName)
+    {
+        String columnType = DatabaseDescriptor.getColumnFamilyType(tableName, cfName);
+        AbstractType comparator = DatabaseDescriptor.getComparator(tableName, cfName);
+        AbstractType subcolumnComparator = DatabaseDescriptor.getSubComparator(tableName, cfName);
+        return new ColumnFamily(cfName, columnType, comparator, subcolumnComparator);
+    }
+
+    private String name_;
+
+    private transient ICompactSerializer<IColumn> columnSerializer_;
+    private long markedForDeleteAt = Long.MIN_VALUE;
+    private int localDeletionTime = Integer.MIN_VALUE;
+    private AtomicInteger size_ = new AtomicInteger(0);
+    private ConcurrentSkipListMap<byte[], IColumn> columns_;
+
+    public ColumnFamily(String cfName, String columnType, AbstractType comparator, AbstractType subcolumnComparator)
+    {
+        name_ = cfName;
+        type_ = columnType;
+        columnSerializer_ = columnType.equals("Standard") ? Column.serializer() : SuperColumn.serializer(subcolumnComparator);
+        columns_ = new ConcurrentSkipListMap<byte[], IColumn>(comparator);
+    }
+
+    public ColumnFamily cloneMeShallow()
+    {
+        ColumnFamily cf = new ColumnFamily(name_, type_, getComparator(), getSubComparator());
+        cf.markedForDeleteAt = markedForDeleteAt;
+        cf.localDeletionTime = localDeletionTime;
+        return cf;
+    }
+
+    private AbstractType getSubComparator()
+    {
+        return (columnSerializer_ instanceof SuperColumnSerializer) ? ((SuperColumnSerializer)columnSerializer_).getComparator() : null;
+    }
+
+    ColumnFamily cloneMe()
+    {
+        ColumnFamily cf = cloneMeShallow();
+        cf.columns_ = columns_.clone();
+    	return cf;
+    }
+
+    public String name()
+    {
+        return name_;
+    }
+
+    /*
+     *  We need to go through each column
+     *  in the column family and resolve it before adding
+    */
+    void addColumns(ColumnFamily cf)
+    {
+        for (IColumn column : cf.getSortedColumns())
+        {
+            addColumn(column);
+        }
+    }
+
+    public ICompactSerializer<IColumn> getColumnSerializer()
+    {
+    	return columnSerializer_;
+    }
+
+    int getColumnCount()
+    {
+    	int count = 0;
+        if(!isSuper())
+        {
+            count = columns_.size();
+        }
+        else
+        {
+            for(IColumn column: columns_.values())
+            {
+                count += column.getObjectCount();
+            }
+        }
+    	return count;
+    }
+
+    public boolean isSuper()
+    {
+        return type_.equals("Super");
+    }
+
+    public void addColumn(QueryPath path, byte[] value, long timestamp)
+    {
+        addColumn(path, value, timestamp, false);
+    }
+
+    /** In most places the CF must be part of a QueryPath but here it is ignored. */
+    public void addColumn(QueryPath path, byte[] value, long timestamp, boolean deleted)
+	{
+        assert path.columnName != null : path;
+		IColumn column;
+        if (path.superColumnName == null)
+        {
+            try
+            {
+                getComparator().validate(path.columnName);
+            }
+            catch (Exception e)
+            {
+                throw new MarshalException("Invalid column name in " + path.columnFamilyName + " for " + getComparator().getClass().getName());
+            }
+            column = new Column(path.columnName, value, timestamp, deleted);
+        }
+        else
+        {
+            assert isSuper();
+            try
+            {
+                getComparator().validate(path.superColumnName);
+            }
+            catch (Exception e)
+            {
+                throw new MarshalException("Invalid supercolumn name in " + path.columnFamilyName + " for " + getComparator().getClass().getName());
+            }
+            column = new SuperColumn(path.superColumnName, getSubComparator());
+            column.addColumn(new Column(path.columnName, value, timestamp, deleted)); // checks subcolumn name
+        }
+		addColumn(column);
+    }
+
+    public void clear()
+    {
+    	columns_.clear();
+    	size_.set(0);
+    }
+
+    /*
+     * If we find an old column that has the same name
+     * the ask it to resolve itself else add the new column .
+    */
+    public void addColumn(IColumn column)
+    {
+        byte[] name = column.name();
+        IColumn oldColumn = columns_.get(name);
+        if (oldColumn != null)
+        {
+            if (oldColumn instanceof SuperColumn)
+            {
+                int oldSize = oldColumn.size();
+                ((SuperColumn) oldColumn).putColumn(column);
+                size_.addAndGet(oldColumn.size() - oldSize);
+            }
+            else
+            {
+                if (((Column)oldColumn).comparePriority((Column)column) <= 0)
+                {
+                    columns_.put(name, column);
+                    size_.addAndGet(column.size());
+                }
+            }
+        }
+        else
+        {
+            size_.addAndGet(column.size());
+            columns_.put(name, column);
+        }
+    }
+
+    public IColumn getColumn(byte[] name)
+    {
+        return columns_.get(name);
+    }
+
+    public SortedSet<byte[]> getColumnNames()
+    {
+        return columns_.keySet();
+    }
+
+    public Collection<IColumn> getSortedColumns()
+    {
+        return columns_.values();
+    }
+
+    public Map<byte[], IColumn> getColumnsMap()
+    {
+        return columns_;
+    }
+
+    public void remove(byte[] columnName)
+    {
+    	columns_.remove(columnName);
+    }
+
+    public void delete(int localtime, long timestamp)
+    {
+        localDeletionTime = localtime;
+        markedForDeleteAt = timestamp;
+    }
+
+    public void delete(ColumnFamily cf2)
+    {
+        delete(Math.max(getLocalDeletionTime(), cf2.getLocalDeletionTime()),
+               Math.max(getMarkedForDeleteAt(), cf2.getMarkedForDeleteAt()));
+    }
+
+    public boolean isMarkedForDelete()
+    {
+        return markedForDeleteAt > Long.MIN_VALUE;
+    }
+
+    /*
+     * This function will calculate the difference between 2 column families.
+     * The external input is assumed to be a superset of internal.
+     */
+    ColumnFamily diff(ColumnFamily cfComposite)
+    {
+    	ColumnFamily cfDiff = new ColumnFamily(cfComposite.name(), cfComposite.type_, getComparator(), getSubComparator());
+        if (cfComposite.getMarkedForDeleteAt() > getMarkedForDeleteAt())
+        {
+            cfDiff.delete(cfComposite.getLocalDeletionTime(), cfComposite.getMarkedForDeleteAt());
+        }
+
+        // (don't need to worry about cfNew containing IColumns that are shadowed by
+        // the delete tombstone, since cfNew was generated by CF.resolve, which
+        // takes care of those for us.)
+        Map<byte[], IColumn> columns = cfComposite.getColumnsMap();
+        Set<byte[]> cNames = columns.keySet();
+        for (byte[] cName : cNames)
+        {
+            IColumn columnInternal = columns_.get(cName);
+            IColumn columnExternal = columns.get(cName);
+            if (columnInternal == null)
+            {
+                cfDiff.addColumn(columnExternal);
+            }
+            else
+            {
+                IColumn columnDiff = columnInternal.diff(columnExternal);
+                if (columnDiff != null)
+                {
+                    cfDiff.addColumn(columnDiff);
+                }
+            }
+        }
+
+        if (!cfDiff.getColumnsMap().isEmpty() || cfDiff.isMarkedForDelete())
+        	return cfDiff;
+        else
+        	return null;
+    }
+
+    public AbstractType getComparator()
+    {
+        return (AbstractType)columns_.comparator();
+    }
+
+    int size()
+    {
+        if (size_.get() == 0)
+        {
+            for (IColumn column : columns_.values())
+            {
+                size_.addAndGet(column.size());
+            }
+        }
+        return size_.get();
+    }
+
+    public int hashCode()
+    {
+        return name().hashCode();
+    }
+
+    public boolean equals(Object o)
+    {
+        if ( !(o instanceof ColumnFamily) )
+            return false;
+        ColumnFamily cf = (ColumnFamily)o;
+        return name().equals(cf.name());
+    }
+
+    public String toString()
+    {
+    	StringBuilder sb = new StringBuilder();
+        sb.append("ColumnFamily(");
+    	sb.append(name_);
+
+        if (isMarkedForDelete()) {
+            sb.append(" -delete at " + getMarkedForDeleteAt() + "-");
+        }
+
+    	sb.append(" [");
+        sb.append(getComparator().getColumnsString(getSortedColumns()));
+        sb.append("])");
+
+    	return sb.toString();
+    }
+
+    public byte[] digest()
+    {
+        byte[] xorHash = ArrayUtils.EMPTY_BYTE_ARRAY;
+        for (IColumn column : columns_.values())
+        {
+            if (xorHash.length == 0)
+            {
+                xorHash = column.digest();
+            }
+            else
+            {
+                xorHash = FBUtilities.xor(xorHash, column.digest());
+            }
+        }
+        return xorHash;
+    }
+
+    public long getMarkedForDeleteAt()
+    {
+        return markedForDeleteAt;
+    }
+
+    public int getLocalDeletionTime()
+    {
+        return localDeletionTime;
+    }
+
+    public String type()
+    {
+        return type_;
+    }
+
+    /** merge all columnFamilies into a single instance, with only the newest versions of columns preserved. */
+    static ColumnFamily resolve(List<ColumnFamily> columnFamilies)
+    {
+        int size = columnFamilies.size();
+        if (size == 0)
+            return null;
+
+        // start from nothing so that we don't include potential deleted columns from the first instance
+        ColumnFamily cf0 = columnFamilies.get(0);
+        ColumnFamily cf = cf0.cloneMeShallow();
+
+        // merge
+        for (ColumnFamily cf2 : columnFamilies)
+        {
+            assert cf.name().equals(cf2.name());
+            cf.addColumns(cf2);
+            cf.delete(cf2);
+        }
+        return cf;
+    }
+
+    public static class ColumnFamilySerializer implements ICompactSerializer<ColumnFamily>
+    {
+        /*
+         * We are going to create indexes, and write out that information as well. The format
+         * of the data serialized is as follows.
+         *
+         * 1) Without indexes:
+         *  // written by the data
+         * 	<boolean false (index is not present)>
+         * 	<column family id>
+         * 	<is marked for delete>
+         * 	<total number of columns>
+         * 	<columns data>
+
+         * 	<boolean true (index is present)>
+         *
+         *  This part is written by the column indexer
+         * 	<size of index in bytes>
+         * 	<list of column names and their offsets relative to the first column>
+         *
+         *  <size of the cf in bytes>
+         * 	<column family id>
+         * 	<is marked for delete>
+         * 	<total number of columns>
+         * 	<columns data>
+        */
+        public void serialize(ColumnFamily columnFamily, DataOutputStream dos) throws IOException
+        {
+            // TODO whenever we change this we need to change the code in SequenceFile to match in two places.
+            // This SUCKS and is inefficient to boot.  let's fix this ASAP. 
+            Collection<IColumn> columns = columnFamily.getSortedColumns();
+
+            dos.writeUTF(columnFamily.name());
+            dos.writeUTF(columnFamily.type_);
+            dos.writeUTF(columnFamily.getComparator().getClass().getCanonicalName());
+            AbstractType subcolumnComparator = columnFamily.getSubComparator();
+            dos.writeUTF(subcolumnComparator == null ? "" : subcolumnComparator.getClass().getCanonicalName());
+            dos.writeInt(columnFamily.localDeletionTime);
+            dos.writeLong(columnFamily.markedForDeleteAt);
+
+            dos.writeInt(columns.size());
+            for ( IColumn column : columns )
+            {
+                columnFamily.getColumnSerializer().serialize(column, dos);
+            }
+        }
+
+        public ColumnFamily deserialize(DataInputStream dis) throws IOException
+        {
+            ColumnFamily cf = new ColumnFamily(dis.readUTF(),
+                                               dis.readUTF(),
+                                               readComparator(dis),
+                                               readComparator(dis));
+            cf.delete(dis.readInt(), dis.readLong());
+            int size = dis.readInt();
+            IColumn column;
+            for (int i = 0; i < size; ++i)
+            {
+                column = cf.getColumnSerializer().deserialize(dis);
+                cf.addColumn(column);
+            }
+            return cf;
+        }
+
+        private AbstractType readComparator(DataInputStream dis) throws IOException
+        {
+            String className = dis.readUTF();
+            if (className.equals(""))
+            {
+                return null;
+            }
+
+            try
+            {
+                return (AbstractType)Class.forName(className).getConstructor().newInstance();
+            }
+            catch (ClassNotFoundException e)
+            {
+                throw new RuntimeException("Unable to load comparator class '" + className + "'.  probably this means you have obsolete sstables lying around", e);
+            }
+            catch (Exception e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/db/ColumnFamilyNotDefinedException.java b/src/java/org/apache/cassandra/db/ColumnFamilyNotDefinedException.java
index 35b51b912d..2675a94011 100644
--- a/src/java/org/apache/cassandra/db/ColumnFamilyNotDefinedException.java
+++ b/src/java/org/apache/cassandra/db/ColumnFamilyNotDefinedException.java
@@ -1,34 +1,34 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import org.apache.cassandra.service.InvalidRequestException;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class ColumnFamilyNotDefinedException extends InvalidRequestException
-{
-    public ColumnFamilyNotDefinedException(String message)
-    {
-        super(message);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import org.apache.cassandra.service.InvalidRequestException;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ColumnFamilyNotDefinedException extends InvalidRequestException
+{
+    public ColumnFamilyNotDefinedException(String message)
+    {
+        super(message);
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
index d183d97f08..fe94a4ad17 100644
--- a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
+++ b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
@@ -1,1588 +1,1588 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.File;
-import java.io.IOException;
-import java.lang.management.ManagementFactory;
-import javax.management.MBeanServer;
-import javax.management.ObjectName;
-import java.util.*;
-import java.util.concurrent.*;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.atomic.AtomicReference;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.dht.Range;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.io.*;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.*;
-import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
-import org.apache.cassandra.db.filter.*;
-import org.apache.cassandra.db.marshal.AbstractType;
-
-import org.apache.commons.lang.StringUtils;
-import org.apache.commons.collections.IteratorUtils;
-
-import org.cliffc.high_scale_lib.NonBlockingHashMap;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public final class ColumnFamilyStore implements ColumnFamilyStoreMBean
-{
-    private static Logger logger_ = Logger.getLogger(ColumnFamilyStore.class);
-
-    private static final int BUFSIZE = 128 * 1024 * 1024;
-    private static final int COMPACTION_MEMORY_THRESHOLD = 1 << 30;
-
-    private static NonBlockingHashMap<String, Set<Memtable>> memtablesPendingFlush = new NonBlockingHashMap<String, Set<Memtable>>();
-    private static ExecutorService flusher_ = new DebuggableThreadPoolExecutor("MEMTABLE-FLUSHER-POOL");
-
-    private final String table_;
-    public final String columnFamily_;
-    private final boolean isSuper_;
-
-    private volatile Integer memtableSwitchCount = 0;
-
-    /* This is used to generate the next index for a SSTable */
-    private AtomicInteger fileIndexGenerator_ = new AtomicInteger(0);
-
-    /* active memtable associated with this ColumnFamilyStore. */
-    private Memtable memtable_;
-    // this lock is to (1) serialize puts and
-    // (2) make sure we don't perform puts on a memtable that is queued for flush.
-    // (or conversely, flush a memtable that is mid-put.)
-    // gets may be safely performed on a flushing ("frozen") memtable.
-    private ReentrantReadWriteLock memtableLock_ = new ReentrantReadWriteLock(true);
-
-    // TODO binarymemtable ops are not threadsafe (do they need to be?)
-    private AtomicReference<BinaryMemtable> binaryMemtable_;
-
-    /* SSTables on disk for this column family */
-    private SortedMap<String, SSTableReader> ssTables_ = new TreeMap<String, SSTableReader>(new FileNameComparator(FileNameComparator.Descending));
-
-    /* Modification lock used for protecting reads from compactions. */
-    private ReentrantReadWriteLock sstableLock_ = new ReentrantReadWriteLock(true);
-
-    private TimedStatsDeque readStats_ = new TimedStatsDeque(60000);
-    private TimedStatsDeque diskReadStats_ = new TimedStatsDeque(60000);
-    private TimedStatsDeque writeStats_ = new TimedStatsDeque(60000);
-
-    ColumnFamilyStore(String table, String columnFamilyName, boolean isSuper, int indexValue) throws IOException
-    {
-        table_ = table;
-        columnFamily_ = columnFamilyName;
-        isSuper_ = isSuper;
-        fileIndexGenerator_.set(indexValue);
-        memtable_ = new Memtable(table_, columnFamily_);
-        binaryMemtable_ = new AtomicReference<BinaryMemtable>(new BinaryMemtable(table_, columnFamily_));
-    }
-
-    public static ColumnFamilyStore getColumnFamilyStore(String table, String columnFamily) throws IOException
-    {
-        /*
-         * Get all data files associated with old Memtables for this table.
-         * These files are named as follows <Table>-1.db, ..., <Table>-n.db. Get
-         * the max which in this case is n and increment it to use it for next
-         * index.
-         */
-        List<Integer> indices = new ArrayList<Integer>();
-        String[] dataFileDirectories = DatabaseDescriptor.getAllDataFileLocationsForTable(table);
-        for (String directory : dataFileDirectories)
-        {
-            File fileDir = new File(directory);
-            File[] files = fileDir.listFiles();
-            
-            for (File file : files)
-            {
-                String filename = file.getName();
-                String cfName = getColumnFamilyFromFileName(filename);
-
-                if (cfName.equals(columnFamily))
-                {
-                    int index = getIndexFromFileName(filename);
-                    indices.add(index);
-                }
-            }
-        }
-        Collections.sort(indices);
-        int value = (indices.size() > 0) ? (indices.get(indices.size() - 1)) : 0;
-
-        ColumnFamilyStore cfs = new ColumnFamilyStore(table, columnFamily, "Super".equals(DatabaseDescriptor.getColumnType(table, columnFamily)), value);
-
-        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
-        try
-        {
-            mbs.registerMBean(cfs, new ObjectName(
-                    "org.apache.cassandra.db:type=ColumnFamilyStores,name=" + table + ",columnfamily=" + columnFamily));
-        }
-        catch (Exception e)
-        {
-            throw new RuntimeException(e);
-        }
-
-        return cfs;
-    }
-
-    void onStart() throws IOException
-    {
-        // scan for data files corresponding to this CF
-        List<File> sstableFiles = new ArrayList<File>();
-        String[] dataFileDirectories = DatabaseDescriptor.getAllDataFileLocationsForTable(table_);
-        for (String directory : dataFileDirectories)
-        {
-            File fileDir = new File(directory);
-            File[] files = fileDir.listFiles();
-            for (File file : files)
-            {
-                String filename = file.getName();
-                if (((file.length() == 0) || (filename.contains("-" + SSTable.TEMPFILE_MARKER))) && (filename.contains(columnFamily_)))
-                {
-                    file.delete();
-                    continue;
-                }
-
-                String cfName = getColumnFamilyFromFileName(filename);
-                if (cfName.equals(columnFamily_)
-                    && filename.contains("-Data.db"))
-                {
-                    sstableFiles.add(file.getAbsoluteFile());
-                }
-            }
-        }
-        Collections.sort(sstableFiles, new FileUtils.FileComparator());
-
-        /* Load the index files and the Bloom Filters associated with them. */
-        for (File file : sstableFiles)
-        {
-            String filename = file.getAbsolutePath();
-            try
-            {
-                SSTableReader sstable = SSTableReader.open(filename);
-                ssTables_.put(filename, sstable);
-            }
-            catch (IOException ex)
-            {
-                logger_.error("Corrupt file " + filename, ex);
-                FileUtils.delete(filename);
-            }
-        }
-
-        // submit initial check-for-compaction request
-        MinorCompactionManager.instance().submit(ColumnFamilyStore.this);
-
-        // schedule hinted handoff
-        if (table_.equals(Table.SYSTEM_TABLE) && columnFamily_.equals(HintedHandOffManager.HINTS_CF))
-        {
-            HintedHandOffManager.instance().submit(this);
-        }
-
-        // schedule periodic flusher if required
-        int flushPeriod = DatabaseDescriptor.getFlushPeriod(table_, columnFamily_);
-        if (flushPeriod > 0)
-        {
-            PeriodicFlushManager.instance().submitPeriodicFlusher(this, flushPeriod);
-        }
-    }
-
-    /*
-     * This method is called to obtain statistics about
-     * the Column Family represented by this Column Family
-     * Store. It will report the total number of files on
-     * disk and the total space oocupied by the data files
-     * associated with this Column Family.
-    */
-    public String cfStats(String newLineSeparator)
-    {
-        StringBuilder sb = new StringBuilder();
-        /*
-         * We want to do this so that if there are
-         * no files on disk we do not want to display
-         * something ugly on the admin page.
-        */
-        if (ssTables_.size() == 0)
-        {
-            return sb.toString();
-        }
-        sb.append(columnFamily_ + " statistics :");
-        sb.append(newLineSeparator);
-        sb.append("Number of files on disk : " + ssTables_.size());
-        sb.append(newLineSeparator);
-        double totalSpace = 0d;
-        for (SSTableReader sstable: ssTables_.values())
-        {
-            File f = new File(sstable.getFilename());
-            totalSpace += f.length();
-        }
-        String diskSpace = FileUtils.stringifyFileSize(totalSpace);
-        sb.append("Total disk space : " + diskSpace);
-        sb.append(newLineSeparator);
-        sb.append("--------------------------------------");
-        sb.append(newLineSeparator);
-        return sb.toString();
-    }
-
-    /*
-     * This is called after bootstrap to add the files
-     * to the list of files maintained.
-    */
-    void addToList(SSTableReader file)
-    {
-        sstableLock_.writeLock().lock();
-        try
-        {
-            ssTables_.put(file.getFilename(), file);
-        }
-        finally
-        {
-            sstableLock_.writeLock().unlock();
-        }
-    }
-
-    /*
-     * This method forces a compaction of the SSTables on disk. We wait
-     * for the process to complete by waiting on a future pointer.
-    */
-    boolean forceCompaction(List<Range> ranges, EndPoint target, long skip, List<String> fileList)
-    {
-        Future<Boolean> futurePtr = null;
-        if (ranges != null)
-        {
-            futurePtr = MinorCompactionManager.instance().submit(ColumnFamilyStore.this, ranges, target, fileList);
-        }
-        else
-        {
-            MinorCompactionManager.instance().submitMajor(ColumnFamilyStore.this, skip);
-        }
-
-        boolean result = true;
-        try
-        {
-            /* Waiting for the compaction to complete. */
-            if (futurePtr != null)
-            {
-                result = futurePtr.get();
-            }
-            if (logger_.isDebugEnabled())
-              logger_.debug("Done forcing compaction ...");
-        }
-        catch (ExecutionException ex)
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug(LogUtil.throwableToString(ex));
-        }
-        catch (InterruptedException ex2)
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug(LogUtil.throwableToString(ex2));
-        }
-        return result;
-    }
-
-    /**
-     * @return the name of the column family
-     */
-    public String getColumnFamilyName()
-    {
-        return columnFamily_;
-    }
-
-    private static String getColumnFamilyFromFileName(String filename)
-            {
-        return filename.split("-")[0];
-    }
-
-    protected static int getIndexFromFileName(String filename)
-    {
-        /*
-         * File name is of the form <table>-<column family>-<index>-Data.db.
-         * This tokenizer will strip the .db portion.
-         */
-        StringTokenizer st = new StringTokenizer(filename, "-");
-        /*
-         * Now I want to get the index portion of the filename. We accumulate
-         * the indices and then sort them to get the max index.
-         */
-        int count = st.countTokens();
-        int i = 0;
-        String index = null;
-        while (st.hasMoreElements())
-        {
-            index = (String) st.nextElement();
-            if (i == (count - 2))
-            {
-                break;
-            }
-            ++i;
-        }
-        return Integer.parseInt(index);
-    }
-
-    String getNextFileName()
-    {
-        // Psuedo increment so that we do not generate consecutive numbers
-        fileIndexGenerator_.incrementAndGet();
-        return table_ + "-" + columnFamily_ + "-" + fileIndexGenerator_.incrementAndGet();
-    }
-
-    /*
-     * @return a temporary file name for an sstable.
-     * When the sstable object is closed, it will be renamed to a non-temporary
-     * format, so incomplete sstables can be recognized and removed on startup.
-     */
-    String getTempSSTablePath()
-    {
-        // increment twice so that we do not generate consecutive numbers
-        String fname = getTempSSTableFileName();
-        return new File(DatabaseDescriptor.getDataFileLocationForTable(table_), fname).getAbsolutePath();
-    }
-
-    String getTempSSTableFileName()
-    {
-        fileIndexGenerator_.incrementAndGet();
-
-        return String.format("%s-%s-%s-Data.db",
-                             columnFamily_, SSTable.TEMPFILE_MARKER, fileIndexGenerator_.incrementAndGet());
-    }
-
-    /*
-     * Return a temporary file name. Based on the list of files input 
-     * This fn sorts the list and generates a number between he 2 lowest filenames 
-     * ensuring uniqueness.
-     * Since we do not generate consecutive numbers hence the lowest file number
-     * can just be incremented to generate the next file. 
-     */
-    String getTempFileName(List<String> files)
-    {
-        int lowestIndex;
-        int index;
-        Collections.sort(files, new FileNameComparator(FileNameComparator.Ascending));
-
-        if (files.size() <= 1)
-        {
-            return null;
-        }
-        lowestIndex = getIndexFromFileName(files.get(0));
-
-        index = lowestIndex + 1;
-
-        return String.format("%s-%s-%s-Data.db",
-                             columnFamily_, SSTable.TEMPFILE_MARKER, index);
-    }
-
-    void switchMemtable(Memtable oldMemtable, CommitLog.CommitLogContext ctx)
-    {
-        memtableLock_.writeLock().lock();
-        try
-        {
-            if (oldMemtable.isFrozen())
-            {
-                return;
-            }
-            logger_.info(columnFamily_ + " has reached its threshold; switching in a fresh Memtable");
-            oldMemtable.freeze();
-            getMemtablesPendingFlushNotNull(columnFamily_).add(oldMemtable); // it's ok for the MT to briefly be both active and pendingFlush
-            submitFlush(oldMemtable, ctx);
-            memtable_ = new Memtable(table_, columnFamily_);
-        }
-        finally
-        {
-            memtableLock_.writeLock().unlock();
-        }
-
-        if (memtableSwitchCount == Integer.MAX_VALUE)
-        {
-            memtableSwitchCount = 0;
-        }
-        memtableSwitchCount++;
-    }
-
-    void switchBinaryMemtable(String key, byte[] buffer) throws IOException
-    {
-        binaryMemtable_.set(new BinaryMemtable(table_, columnFamily_));
-        binaryMemtable_.get().put(key, buffer);
-    }
-
-    public void forceFlush()
-    {
-        if (memtable_.isClean())
-            return;
-
-        CommitLog.CommitLogContext ctx = null;
-        try
-        {
-            ctx = CommitLog.open().getContext();
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-        switchMemtable(memtable_, ctx);
-    }
-
-    void forceBlockingFlush() throws IOException, ExecutionException, InterruptedException
-    {
-        Memtable oldMemtable = getMemtableThreadSafe();
-        forceFlush();
-        // block for flush to finish by adding a no-op action to the flush executorservice
-        // and waiting for that to finish.  (this works since flush ES is single-threaded.)
-        Future f = flusher_.submit(new Runnable()
-        {
-            public void run()
-            {
-            }
-        });
-        f.get();
-        /* this assert is not threadsafe -- the memtable could have been clean when forceFlush
-           checked it, but dirty now thanks to another thread.  But as long as we are only
-           calling this from single-threaded test code it is useful to have as a sanity check. */
-        assert oldMemtable.isFlushed() || oldMemtable.isClean(); 
-    }
-
-    void forceFlushBinary()
-    {
-        BinaryMemtableManager.instance().submit(getColumnFamilyName(), binaryMemtable_.get());
-    }
-
-    /**
-     * Insert/Update the column family for this key.
-     * param @ lock - lock that needs to be used.
-     * param @ key - key for update/insert
-     * param @ columnFamily - columnFamily changes
-     */
-    void apply(String key, ColumnFamily columnFamily, CommitLog.CommitLogContext cLogCtx)
-            throws IOException
-    {
-        long start = System.currentTimeMillis();
-        Memtable initialMemtable = getMemtableThreadSafe();
-        if (initialMemtable.isThresholdViolated())
-        {
-            switchMemtable(initialMemtable, cLogCtx);
-        }
-        memtableLock_.writeLock().lock();
-        try
-        {
-            memtable_.put(key, columnFamily);
-        }
-        finally
-        {
-            memtableLock_.writeLock().unlock();
-        }
-        writeStats_.add(System.currentTimeMillis() - start);
-    }
-
-    /*
-     * Insert/Update the column family for this key. param @ lock - lock that
-     * needs to be used. param @ key - key for update/insert param @
-     * columnFamily - columnFamily changes
-     */
-    void applyBinary(String key, byte[] buffer)
-            throws IOException
-    {
-        long start = System.currentTimeMillis();
-        binaryMemtable_.get().put(key, buffer);
-        writeStats_.add(System.currentTimeMillis() - start);
-    }
-
-    /**
-     * like resolve, but leaves the resolved CF as the only item in the list
-     */
-    private static void merge(List<ColumnFamily> columnFamilies)
-    {
-        ColumnFamily cf = ColumnFamily.resolve(columnFamilies);
-        columnFamilies.clear();
-        columnFamilies.add(cf);
-    }
-
-    private static ColumnFamily resolveAndRemoveDeleted(List<ColumnFamily> columnFamilies)
-    {
-        ColumnFamily cf = ColumnFamily.resolve(columnFamilies);
-        return removeDeleted(cf);
-    }
-
-    /*
-     This is complicated because we need to preserve deleted columns, supercolumns, and columnfamilies
-     until they have been deleted for at least GC_GRACE_IN_SECONDS.  But, we do not need to preserve
-     their contents; just the object itself as a "tombstone" that can be used to repair other
-     replicas that do not know about the deletion.
-     */
-    static ColumnFamily removeDeleted(ColumnFamily cf)
-    {
-        return removeDeleted(cf, getDefaultGCBefore());
-    }
-
-    public static int getDefaultGCBefore()
-    {
-        return (int)(System.currentTimeMillis() / 1000) - DatabaseDescriptor.getGcGraceInSeconds();
-    }
-
-    static ColumnFamily removeDeleted(ColumnFamily cf, int gcBefore)
-    {
-        if (cf == null)
-        {
-            return null;
-        }
-
-        // in case of a timestamp tie, tombstones get priority over non-tombstones.
-        // we want this to be deterministic in general to avoid confusion;
-        // either way (tombstone or non- getting priority) would be fine,
-        // but we picked this way because it makes removing delivered hints
-        // easier for HintedHandoffManager.
-        for (byte[] cname : cf.getColumnNames())
-        {
-            IColumn c = cf.getColumnsMap().get(cname);
-            if (c instanceof SuperColumn)
-            {
-                long minTimestamp = Math.max(c.getMarkedForDeleteAt(), cf.getMarkedForDeleteAt());
-                // don't operate directly on the supercolumn, it could be the one in the memtable.
-                // instead, create a new SC and add in the subcolumns that qualify.
-                cf.remove(cname);
-                SuperColumn sc = ((SuperColumn)c).cloneMeShallow();
-                for (IColumn subColumn : c.getSubColumns())
-                {
-                    if (subColumn.timestamp() > minTimestamp)
-                    {
-                        if (!subColumn.isMarkedForDelete() || subColumn.getLocalDeletionTime() > gcBefore)
-                        {
-                            sc.addColumn(subColumn);
-                        }
-                    }
-                }
-                if (sc.getSubColumns().size() > 0 || sc.getLocalDeletionTime() > gcBefore)
-                {
-                    cf.addColumn(sc);
-                }
-            }
-            else if ((c.isMarkedForDelete() && c.getLocalDeletionTime() <= gcBefore)
-                     || c.timestamp() <= cf.getMarkedForDeleteAt())
-            {
-                cf.remove(cname);
-            }
-        }
-
-        if (cf.getColumnCount() == 0 && cf.getLocalDeletionTime() <= gcBefore)
-        {
-            return null;
-        }
-        return cf;
-    }
-
-    /*
-     * This version is used only on start up when we are recovering from logs.
-     * Hence no locking is required since we process logs on the main thread. In
-     * the future we may want to parellelize the log processing for a table by
-     * having a thread per log file present for recovery. Re-visit at that time.
-     */
-    void applyNow(String key, ColumnFamily columnFamily) throws IOException
-    {
-        getMemtableThreadSafe().put(key, columnFamily);
-    }
-
-    /*
-     * This method is called when the Memtable is frozen and ready to be flushed
-     * to disk. This method informs the CommitLog that a particular ColumnFamily
-     * is being flushed to disk.
-     */
-    void onMemtableFlush(CommitLog.CommitLogContext cLogCtx) throws IOException
-    {
-        if (cLogCtx.isValidContext())
-        {
-            CommitLog.open().onMemtableFlush(table_, columnFamily_, cLogCtx);
-        }
-    }
-
-    /*
-     * Called after the Memtable flushes its in-memory data. This information is
-     * cached in the ColumnFamilyStore. This is useful for reads because the
-     * ColumnFamilyStore first looks in the in-memory store and the into the
-     * disk to find the key. If invoked during recoveryMode the
-     * onMemtableFlush() need not be invoked.
-     *
-     * param @ filename - filename just flushed to disk
-     * param @ bf - bloom filter which indicates the keys that are in this file.
-    */
-    void storeLocation(SSTableReader sstable)
-    {
-        int ssTableCount;
-        sstableLock_.writeLock().lock();
-        try
-        {
-            ssTables_.put(sstable.getFilename(), sstable);
-            ssTableCount = ssTables_.size();
-        }
-        finally
-        {
-            sstableLock_.writeLock().unlock();
-        }
-
-        /* it's ok if compaction gets submitted multiple times while one is already in process.
-           worst that happens is, compactor will count the sstable files and decide there are
-           not enough to bother with. */
-        if (ssTableCount >= MinorCompactionManager.COMPACTION_THRESHOLD)
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Submitting " + columnFamily_ + " for compaction");
-            MinorCompactionManager.instance().submit(this);
-        }
-    }
-
-    private PriorityQueue<FileStruct> initializePriorityQueue(List<String> files, List<Range> ranges, int minBufferSize)
-    {
-        PriorityQueue<FileStruct> pq = new PriorityQueue<FileStruct>();
-        if (files.size() > 1 || (ranges != null && files.size() > 0))
-        {
-            int bufferSize = Math.min((ColumnFamilyStore.COMPACTION_MEMORY_THRESHOLD / files.size()), minBufferSize);
-            FileStruct fs = null;
-            for (String file : files)
-            {
-                try
-                {
-                    fs = SSTableReader.get(file).getFileStruct();
-                    fs.advance();
-                    if (fs.isExhausted())
-                    {
-                        continue;
-                    }
-                    pq.add(fs);
-                }
-                catch (Exception ex)
-                {
-                    logger_.warn("corrupt file?  or are you just blowing away data files manually out from under me?", ex);
-                    try
-                    {
-                        if (fs != null)
-                        {
-                            fs.close();
-                        }
-                    }
-                    catch (Exception e)
-                    {
-                        logger_.error("Unable to close file :" + file);
-                    }
-                }
-            }
-        }
-        return pq;
-    }
-
-    /*
-     * Group files of similar size into buckets.
-     */
-    static Set<List<String>> getCompactionBuckets(List<String> files, long min)
-    {
-        Map<List<String>, Long> buckets = new ConcurrentHashMap<List<String>, Long>();
-        for (String fname : files)
-        {
-            File f = new File(fname);
-            long size = f.length();
-
-            boolean bFound = false;
-            // look for a bucket containing similar-sized files:
-            // group in the same bucket if it's w/in 50% of the average for this bucket,
-            // or this file and the bucket are all considered "small" (less than `min`)
-            for (List<String> bucket : buckets.keySet())
-            {
-                long averageSize = buckets.get(bucket);
-                if ((size > averageSize / 2 && size < 3 * averageSize / 2)
-                    || (size < min && averageSize < min))
-                {
-                    // remove and re-add because adding changes the hash
-                    buckets.remove(bucket);
-                    averageSize = (averageSize + size) / 2;
-                    bucket.add(fname);
-                    buckets.put(bucket, averageSize);
-                    bFound = true;
-                    break;
-                }
-            }
-            // no similar bucket found; put it in a new one
-            if (!bFound)
-            {
-                ArrayList<String> bucket = new ArrayList<String>();
-                bucket.add(fname);
-                buckets.put(bucket, size);
-            }
-        }
-
-        return buckets.keySet();
-    }
-
-    /*
-     * Break the files into buckets and then compact.
-     */
-    int doCompaction(int threshold) throws IOException
-    {
-        List<String> files = new ArrayList<String>(ssTables_.keySet());
-        int filesCompacted = 0;
-        Set<List<String>> buckets = getCompactionBuckets(files, 50L * 1024L * 1024L);
-        for (List<String> fileList : buckets)
-        {
-            Collections.sort(fileList, new FileNameComparator(FileNameComparator.Ascending));
-            if (fileList.size() < threshold)
-            {
-                continue;
-            }
-            // For each bucket if it has crossed the threshhold do the compaction
-            // In case of range  compaction merge the counting bloom filters also.
-            files.clear();
-            int count = 0;
-            for (String file : fileList)
-            {
-                files.add(file);
-                count++;
-                if (count == threshold)
-                {
-                    filesCompacted += doFileCompaction(files, BUFSIZE);
-                    break;
-                }
-            }
-        }
-        return filesCompacted;
-    }
-
-    void doMajorCompaction(long skip) throws IOException
-    {
-        doMajorCompactionInternal(skip);
-    }
-
-    /*
-     * Compact all the files irrespective of the size.
-     * skip : is the amount in GB of the files to be skipped
-     * all files greater than skip GB are skipped for this compaction.
-     * Except if skip is 0 , in that case this is ignored and all files are taken.
-     */
-    void doMajorCompactionInternal(long skip) throws IOException
-    {
-        List<String> filesInternal = new ArrayList<String>(ssTables_.keySet());
-        List<String> files;
-        if (skip > 0L)
-        {
-            files = new ArrayList<String>();
-            for (String file : filesInternal)
-            {
-                File f = new File(file);
-                if (f.length() < skip * 1024L * 1024L * 1024L)
-                {
-                    files.add(file);
-                }
-            }
-        }
-        else
-        {
-            files = filesInternal;
-        }
-        doFileCompaction(files, BUFSIZE);
-    }
-
-    /*
-     * Add up all the files sizes this is the worst case file
-     * size for compaction of all the list of files given.
-     */
-    long getExpectedCompactedFileSize(List<String> files)
-    {
-        long expectedFileSize = 0;
-        for (String file : files)
-        {
-            File f = new File(file);
-            long size = f.length();
-            expectedFileSize = expectedFileSize + size;
-        }
-        return expectedFileSize;
-    }
-
-    /*
-     *  Find the maximum size file in the list .
-     */
-    String getMaxSizeFile(List<String> files)
-    {
-        long maxSize = 0L;
-        String maxFile = null;
-        for (String file : files)
-        {
-            File f = new File(file);
-            if (f.length() > maxSize)
-            {
-                maxSize = f.length();
-                maxFile = file;
-            }
-        }
-        return maxFile;
-    }
-
-    boolean doAntiCompaction(List<Range> ranges, EndPoint target, List<String> fileList) throws IOException
-    {
-        List<String> files = new ArrayList<String>(ssTables_.keySet());
-        return doFileAntiCompaction(files, ranges, target, fileList);
-    }
-
-    void forceCleanup()
-    {
-        MinorCompactionManager.instance().submitCleanup(ColumnFamilyStore.this);
-    }
-
-    /**
-     * This function goes over each file and removes the keys that the node is not responsible for
-     * and only keeps keys that this node is responsible for.
-     *
-     * @throws IOException
-     */
-    void doCleanupCompaction() throws IOException
-    {
-        List<String> files = new ArrayList<String>(ssTables_.keySet());
-        for (String file : files)
-        {
-            doCleanup(file);
-        }
-    }
-
-    /**
-     * cleans up one particular file by removing keys that this node is not responsible for.
-     *
-     * @param file
-     * @throws IOException
-     */
-    /* TODO: Take care of the comments later. */
-    void doCleanup(String file) throws IOException
-    {
-        if (file == null)
-        {
-            return;
-        }
-        List<Range> myRanges;
-        List<String> files = new ArrayList<String>();
-        files.add(file);
-        List<String> newFiles = new ArrayList<String>();
-        Map<EndPoint, List<Range>> endPointtoRangeMap = StorageService.instance().constructEndPointToRangesMap();
-        myRanges = endPointtoRangeMap.get(StorageService.getLocalStorageEndPoint());
-        doFileAntiCompaction(files, myRanges, null, newFiles);
-        if (logger_.isDebugEnabled())
-          logger_.debug("Original file : " + file + " of size " + new File(file).length());
-        sstableLock_.writeLock().lock();
-        try
-        {
-            ssTables_.remove(file);
-            for (String newfile : newFiles)
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug("New file : " + newfile + " of size " + new File(newfile).length());
-                assert newfile != null;
-                ssTables_.put(newfile, SSTableReader.open(newfile));
-            }
-            SSTableReader.get(file).delete();
-        }
-        finally
-        {
-            sstableLock_.writeLock().unlock();
-        }
-    }
-
-    /**
-     * This function is used to do the anti compaction process , it spits out the file which has keys that belong to a given range
-     * If the target is not specified it spits out the file as a compacted file with the unecessary ranges wiped out.
-     *
-     * @param files
-     * @param ranges
-     * @param target
-     * @param fileList
-     * @return
-     * @throws IOException
-     */
-    boolean doFileAntiCompaction(List<String> files, List<Range> ranges, EndPoint target, List<String> fileList) throws IOException
-    {
-        boolean result = false;
-        long startTime = System.currentTimeMillis();
-        long totalBytesRead = 0;
-        long totalBytesWritten = 0;
-        long totalkeysRead = 0;
-        long totalkeysWritten = 0;
-        String rangeFileLocation;
-        String mergedFileName;
-        IPartitioner p = StorageService.getPartitioner();
-        // Calculate the expected compacted filesize
-        long expectedRangeFileSize = getExpectedCompactedFileSize(files);
-        /* in the worst case a node will be giving out half of its data so we take a chance */
-        expectedRangeFileSize = expectedRangeFileSize / 2;
-        rangeFileLocation = DatabaseDescriptor.getDataFileLocationForTable(table_, expectedRangeFileSize);
-        // If the compaction file path is null that means we have no space left for this compaction.
-        if (rangeFileLocation == null)
-        {
-            logger_.warn("Total bytes to be written for range compaction  ..."
-                         + expectedRangeFileSize + "   is greater than the safe limit of the disk space available.");
-            return result;
-        }
-        PriorityQueue<FileStruct> pq = initializePriorityQueue(files, ranges, ColumnFamilyStore.BUFSIZE);
-        if (pq.isEmpty())
-        {
-            return result;
-        }
-
-        mergedFileName = getTempSSTableFileName();
-        SSTableWriter rangeWriter = null;
-        String lastkey = null;
-        List<FileStruct> lfs = new ArrayList<FileStruct>();
-        DataOutputBuffer bufOut = new DataOutputBuffer();
-        int expectedBloomFilterSize = SSTableReader.getApproximateKeyCount(files);
-        expectedBloomFilterSize = (expectedBloomFilterSize > 0) ? expectedBloomFilterSize : SSTableReader.indexInterval();
-        if (logger_.isDebugEnabled())
-          logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
-        List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();
-
-        while (pq.size() > 0 || lfs.size() > 0)
-        {
-            FileStruct fs = null;
-            if (pq.size() > 0)
-            {
-                fs = pq.poll();
-            }
-            if (fs != null
-                && (lastkey == null || lastkey.equals(fs.getKey())))
-            {
-                // The keys are the same so we need to add this to the
-                // ldfs list
-                lastkey = fs.getKey();
-                lfs.add(fs);
-            }
-            else
-            {
-                Collections.sort(lfs, new FileStructComparator());
-                ColumnFamily columnFamily;
-                bufOut.reset();
-                if (lfs.size() > 1)
-                {
-                    for (FileStruct filestruct : lfs)
-                    {
-                        try
-                        {
-                            /* read the length although we don't need it */
-                            filestruct.getBufIn().readInt();
-                            // Skip the Index
-                            IndexHelper.skipBloomFilterAndIndex(filestruct.getBufIn());
-                            // We want to add only 2 and resolve them right there in order to save on memory footprint
-                            if (columnFamilies.size() > 1)
-                            {
-                                // Now merge the 2 column families
-                                merge(columnFamilies);
-                            }
-                            // deserialize into column families
-                            columnFamilies.add(ColumnFamily.serializer().deserialize(filestruct.getBufIn()));
-                        }
-                        catch (Exception ex)
-                        {
-                            logger_.warn(LogUtil.throwableToString(ex));
-                        }
-                    }
-                    // Now after merging all crap append to the sstable
-                    columnFamily = resolveAndRemoveDeleted(columnFamilies);
-                    columnFamilies.clear();
-                    if (columnFamily != null)
-                    {
-                        /* serialize the cf with column indexes */
-                        ColumnFamily.serializerWithIndexes().serialize(columnFamily, bufOut);
-                    }
-                }
-                else
-                {
-                    FileStruct filestruct = lfs.get(0);
-                    /* read the length although we don't need it */
-                    int size = filestruct.getBufIn().readInt();
-                    bufOut.write(filestruct.getBufIn(), size);
-                }
-                if (Range.isTokenInRanges(StorageService.getPartitioner().getInitialToken(lastkey), ranges))
-                {
-                    if (rangeWriter == null)
-                    {
-                        if (target != null)
-                        {
-                            rangeFileLocation = rangeFileLocation + File.separator + "bootstrap";
-                        }
-                        FileUtils.createDirectory(rangeFileLocation);
-                        String fname = new File(rangeFileLocation, mergedFileName).getAbsolutePath();
-                        rangeWriter = new SSTableWriter(fname, expectedBloomFilterSize, StorageService.getPartitioner());
-                    }
-                    try
-                    {
-                        rangeWriter.append(lastkey, bufOut);
-                    }
-                    catch (Exception ex)
-                    {
-                        logger_.warn(LogUtil.throwableToString(ex));
-                    }
-                }
-                totalkeysWritten++;
-                for (FileStruct filestruct : lfs)
-                {
-                    try
-                    {
-                        filestruct.advance();
-                        if (filestruct.isExhausted())
-                        {
-                            continue;
-                        }
-                        /* keep on looping until we find a key in the range */
-                        while (!Range.isTokenInRanges(StorageService.getPartitioner().getInitialToken(filestruct.getKey()), ranges))
-                        {
-                            filestruct.advance();
-                            if (filestruct.isExhausted())
-                            {
-                                break;
-                            }
-                        }
-                        if (!filestruct.isExhausted())
-                        {
-                            pq.add(filestruct);
-                        }
-                        totalkeysRead++;
-                    }
-                    catch (Exception ex)
-                    {
-                        // Ignore the exception as it might be a corrupted file
-                        // in any case we have read as far as possible from it
-                        // and it will be deleted after compaction.
-                        logger_.warn("corrupt sstable?", ex);
-                        filestruct.close();
-                    }
-                }
-                lfs.clear();
-                lastkey = null;
-                if (fs != null)
-                {
-                    // Add back the fs since we processed the rest of
-                    // filestructs
-                    pq.add(fs);
-                }
-            }
-        }
-
-        if (rangeWriter != null)
-        {
-            rangeWriter.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_));
-            if (fileList != null)
-            {
-                fileList.add(rangeWriter.getFilename());
-            }
-        }
-
-        if (logger_.isDebugEnabled())
-        {
-            logger_.debug("Total time taken for range split   ..." + (System.currentTimeMillis() - startTime));
-            logger_.debug("Total bytes Read for range split  ..." + totalBytesRead);
-            logger_.debug("Total bytes written for range split  ..."
-                          + totalBytesWritten + "   Total keys read ..." + totalkeysRead);
-        }
-        return result;
-    }
-
-    private void doFill(BloomFilter bf, String decoratedKey)
-    {
-        bf.add(StorageService.getPartitioner().undecorateKey(decoratedKey));
-    }
-
-    /*
-    * This function does the actual compaction for files.
-    * It maintains a priority queue of with the first key from each file
-    * and then removes the top of the queue and adds it to the SStable and
-    * repeats this process while reading the next from each file until its
-    * done with all the files . The SStable to which the keys are written
-    * represents the new compacted file. Before writing if there are keys
-    * that occur in multiple files and are the same then a resolution is done
-    * to get the latest data.
-    *
-    */
-    private int doFileCompaction(List<String> files, int minBufferSize) throws IOException
-    {
-        logger_.info("Compacting [" + StringUtils.join(files, ",") + "]");
-        String compactionFileLocation = DatabaseDescriptor.getDataFileLocationForTable(table_, getExpectedCompactedFileSize(files));
-        // If the compaction file path is null that means we have no space left for this compaction.
-        // try again w/o the largest one.
-        if (compactionFileLocation == null)
-        {
-            String maxFile = getMaxSizeFile(files);
-            files.remove(maxFile);
-            return doFileCompaction(files, minBufferSize);
-        }
-
-        String newfile = null;
-        long startTime = System.currentTimeMillis();
-        long totalBytesRead = 0;
-        long totalBytesWritten = 0;
-        long totalkeysRead = 0;
-        long totalkeysWritten = 0;
-        PriorityQueue<FileStruct> pq = initializePriorityQueue(files, null, minBufferSize);
-
-        if (pq.isEmpty())
-        {
-            logger_.warn("Nothing to compact (all files empty or corrupt)");
-            // TODO clean out bad files, if any
-            return 0;
-        }
-
-        String mergedFileName = getTempFileName(files);
-        SSTableWriter writer = null;
-        SSTableReader ssTable = null;
-        String lastkey = null;
-        List<FileStruct> lfs = new ArrayList<FileStruct>();
-        DataOutputBuffer bufOut = new DataOutputBuffer();
-        int expectedBloomFilterSize = SSTableReader.getApproximateKeyCount(files);
-        expectedBloomFilterSize = (expectedBloomFilterSize > 0) ? expectedBloomFilterSize : SSTableReader.indexInterval();
-        if (logger_.isDebugEnabled())
-          logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
-        List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();
-
-        while (pq.size() > 0 || lfs.size() > 0)
-        {
-            FileStruct fs = null;
-            if (pq.size() > 0)
-            {
-                fs = pq.poll();
-            }
-            if (fs != null
-                && (lastkey == null || lastkey.equals(fs.getKey())))
-            {
-                // The keys are the same so we need to add this to the
-                // ldfs list
-                lastkey = fs.getKey();
-                lfs.add(fs);
-            }
-            else
-            {
-                Collections.sort(lfs, new FileStructComparator());
-                ColumnFamily columnFamily;
-                bufOut.reset();
-                if (lfs.size() > 1)
-                {
-                    for (FileStruct filestruct : lfs)
-                    {
-                        try
-                        {
-                            /* read the length although we don't need it */
-                            filestruct.getBufIn().readInt();
-                            // Skip the Index
-                            IndexHelper.skipBloomFilterAndIndex(filestruct.getBufIn());
-                            // We want to add only 2 and resolve them right there in order to save on memory footprint
-                            if (columnFamilies.size() > 1)
-                            {
-                                merge(columnFamilies);
-                            }
-                            // deserialize into column families
-                            columnFamilies.add(ColumnFamily.serializer().deserialize(filestruct.getBufIn()));
-                        }
-                        catch (Exception ex)
-                        {
-                            logger_.warn("error in filecompaction", ex);
-                        }
-                    }
-                    // Now after merging all crap append to the sstable
-                    columnFamily = resolveAndRemoveDeleted(columnFamilies);
-                    columnFamilies.clear();
-                    if (columnFamily != null)
-                    {
-                        /* serialize the cf with column indexes */
-                        ColumnFamily.serializerWithIndexes().serialize(columnFamily, bufOut);
-                    }
-                }
-                else
-                {
-                    FileStruct filestruct = lfs.get(0);
-                    /* read the length although we don't need it */
-                    int size = filestruct.getBufIn().readInt();
-                    bufOut.write(filestruct.getBufIn(), size);
-                }
-
-                if (writer == null)
-                {
-                    String fname = new File(compactionFileLocation, mergedFileName).getAbsolutePath();
-                    writer = new SSTableWriter(fname, expectedBloomFilterSize, StorageService.getPartitioner());
-                }
-                writer.append(lastkey, bufOut);
-                totalkeysWritten++;
-
-                for (FileStruct filestruct : lfs)
-                {
-                    try
-                    {
-                        filestruct.advance();
-                        if (filestruct.isExhausted())
-                        {
-                            continue;
-                        }
-                        pq.add(filestruct);
-                        totalkeysRead++;
-                    }
-                    catch (Throwable ex)
-                    {
-                        // Ignore the exception as it might be a corrupted file
-                        // in any case we have read as far as possible from it
-                        // and it will be deleted after compaction.
-                        logger_.warn("corrupt sstable?", ex);
-                        filestruct.close();
-                    }
-                }
-                lfs.clear();
-                lastkey = null;
-                if (fs != null)
-                {
-                    /* Add back the fs since we processed the rest of filestructs */
-                    pq.add(fs);
-                }
-            }
-        }
-        if (writer != null)
-        {
-            // TODO if all the keys were the same nothing will be done here
-            ssTable = writer.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_));
-            newfile = writer.getFilename();
-        }
-        sstableLock_.writeLock().lock();
-        try
-        {
-            for (String file : files)
-            {
-                ssTables_.remove(file);
-            }
-            if (newfile != null)
-            {
-                ssTables_.put(newfile, ssTable);
-                totalBytesWritten += (new File(newfile)).length();
-            }
-            for (String file : files)
-            {
-                SSTableReader.get(file).delete();
-            }
-        }
-        finally
-        {
-            sstableLock_.writeLock().unlock();
-        }
-
-        String format = "Compacted to %s.  %d/%d bytes for %d/%d keys read/written.  Time: %dms.";
-        long dTime = System.currentTimeMillis() - startTime;
-        logger_.info(String.format(format, newfile, totalBytesRead, totalBytesWritten, totalkeysRead, totalkeysWritten, dTime));
-        return files.size();
-    }
-
-    public static List<Memtable> getUnflushedMemtables(String cfName)
-    {
-        return new ArrayList<Memtable>(getMemtablesPendingFlushNotNull(cfName));
-    }
-
-    private static Set<Memtable> getMemtablesPendingFlushNotNull(String columnFamilyName)
-    {
-        Set<Memtable> memtables = memtablesPendingFlush.get(columnFamilyName);
-        if (memtables == null)
-        {
-            memtablesPendingFlush.putIfAbsent(columnFamilyName, new ConcurrentSkipListSet<Memtable>());
-            memtables = memtablesPendingFlush.get(columnFamilyName); // might not be the object we just put, if there was a race!
-        }
-        return memtables;
-    }
-
-    /* Submit memtables to be flushed to disk */
-    private static void submitFlush(final Memtable memtable, final CommitLog.CommitLogContext cLogCtx)
-    {
-        logger_.info("Enqueuing flush of " + memtable);
-        flusher_.submit(new Runnable()
-        {
-            public void run()
-            {
-                try
-                {
-                    memtable.flush(cLogCtx);
-                }
-                catch (IOException e)
-                {
-                    throw new RuntimeException(e);
-                }
-                getMemtablesPendingFlushNotNull(memtable.getColumnFamily()).remove(memtable);
-            }
-        });
-    }
-
-    public boolean isSuper()
-    {
-        return isSuper_;
-    }
-
-    public void flushMemtableOnRecovery() throws IOException
-    {
-        getMemtableThreadSafe().flushOnRecovery();
-    }
-
-    public int getMemtableColumnsCount()
-    {
-        return getMemtableThreadSafe().getCurrentObjectCount();
-    }
-
-    public int getMemtableDataSize()
-    {
-        return getMemtableThreadSafe().getCurrentSize();
-    }
-
-    public int getMemtableSwitchCount()
-    {
-        return memtableSwitchCount;
-    }
-
-    /**
-     * get the current memtable in a threadsafe fashion.  note that simply "return memtable_" is
-     * incorrect; you need to lock to introduce a thread safe happens-before ordering.
-     *
-     * do NOT use this method to do either a put or get on the memtable object, since it could be
-     * flushed in the meantime (and its executor terminated).
-     *
-     * also do NOT make this method public or it will really get impossible to reason about these things.
-     * @return
-     */
-    private Memtable getMemtableThreadSafe()
-    {
-        memtableLock_.readLock().lock();
-        try
-        {
-            return memtable_;
-        }
-        finally
-        {
-            memtableLock_.readLock().unlock();
-        }
-    }
-
-    public Iterator<String> memtableKeyIterator() throws ExecutionException, InterruptedException
-    {
-        Set<String> keys;
-        memtableLock_.readLock().lock();
-        try
-        {
-            keys = memtable_.getKeys();
-        }
-        finally
-        {
-            memtableLock_.readLock().unlock();
-        }
-        return Memtable.getKeyIterator(keys);
-    }
-
-    /** not threadsafe.  caller must have lock_ acquired. */
-    public Collection<SSTableReader> getSSTables()
-    {
-        return Collections.unmodifiableCollection(ssTables_.values());
-    }
-
-    public ReentrantReadWriteLock.ReadLock getReadLock()
-    {
-        return sstableLock_.readLock();
-    }
-
-    public int getReadCount()
-    {
-        return readStats_.size();
-    }
-
-    public int getReadDiskHits()
-    {
-        return diskReadStats_.size();
-    }
-
-    public double getReadLatency()
-    {
-        return readStats_.mean();
-    }
-    
-    public int getPendingTasks()
-    {
-        return memtableLock_.getQueueLength();
-    }
-
-    /**
-     * @return the number of write operations on this column family in the last minute
-     */
-    public int getWriteCount() {
-        return writeStats_.size();
-    }
-
-    /**
-     * @return average latency per write operation in the last minute
-     */
-    public double getWriteLatency() {
-        return writeStats_.mean();
-    }
-
-    public ColumnFamily getColumnFamily(String key, QueryPath path, byte[] start, byte[] finish, boolean isAscending, int limit) throws IOException
-    {
-        return getColumnFamily(new SliceQueryFilter(key, path, start, finish, isAscending, limit));
-    }
-
-    public ColumnFamily getColumnFamily(QueryFilter filter) throws IOException
-    {
-        return getColumnFamily(filter, getDefaultGCBefore());
-    }
-
-    /**
-     * get a list of columns starting from a given column, in a specified order.
-     * only the latest version of a column is returned.
-     * @return null if there is no data and no tombstones; otherwise a ColumnFamily
-     */
-    public ColumnFamily getColumnFamily(QueryFilter filter, int gcBefore) throws IOException
-    {
-        assert columnFamily_.equals(filter.getColumnFamilyName());
-
-        // if we are querying subcolumns of a supercolumn, fetch the supercolumn with NQF, then filter in-memory.
-        if (filter.path.superColumnName != null)
-        {
-            AbstractType comparator = DatabaseDescriptor.getComparator(table_, columnFamily_);
-            QueryFilter nameFilter = new NamesQueryFilter(filter.key, new QueryPath(columnFamily_), filter.path.superColumnName);
-            ColumnFamily cf = getColumnFamily(nameFilter);
-            if (cf != null)
-            {
-                for (IColumn column : cf.getSortedColumns())
-                {
-                    filter.filterSuperColumn((SuperColumn)column, gcBefore);
-                }
-            }
-            return removeDeleted(cf, gcBefore);
-        }
-
-        // we are querying top-level columns, do a merging fetch with indexes.
-        sstableLock_.readLock().lock();
-        List<ColumnIterator> iterators = new ArrayList<ColumnIterator>();
-        try
-        {
-            final ColumnFamily returnCF;
-            ColumnIterator iter;
-        
-            /* add the current memtable */
-            memtableLock_.readLock().lock();
-            try
-            {
-                iter = filter.getMemColumnIterator(memtable_, getComparator());
-                returnCF = iter.getColumnFamily();
-            }
-            finally
-            {
-                memtableLock_.readLock().unlock();            
-            }        
-            iterators.add(iter);
-
-            /* add the memtables being flushed */
-            List<Memtable> memtables = getUnflushedMemtables(filter.getColumnFamilyName());
-            for (Memtable memtable:memtables)
-            {
-                iter = filter.getMemColumnIterator(memtable, getComparator());
-                returnCF.delete(iter.getColumnFamily());
-                iterators.add(iter);
-            }
-
-            /* add the SSTables on disk */
-            List<SSTableReader> sstables = new ArrayList<SSTableReader>(ssTables_.values());
-            for (SSTableReader sstable : sstables)
-            {
-                iter = filter.getSSTableColumnIterator(sstable, getComparator());
-                if (iter.hasNext()) // initializes iter.CF
-                {
-                    returnCF.delete(iter.getColumnFamily());
-                }
-                iterators.add(iter);
-            }
-
-            Comparator<IColumn> comparator = filter.getColumnComparator(getComparator());
-            Iterator collated = IteratorUtils.collatedIterator(comparator, iterators);
-            if (!collated.hasNext())
-                return null;
-
-            filter.collectColumns(returnCF, collated, gcBefore);
-
-            return removeDeleted(returnCF, gcBefore); // collect does a first pass but doesn't try to recognize e.g. the entire CF being tombstoned
-        }
-        finally
-        {
-            /* close all cursors */
-            for (ColumnIterator ci : iterators)
-            {
-                try
-                {
-                    ci.close();
-                }
-                catch (Throwable th)
-                {
-                    logger_.error(th);
-                }
-            }
-
-            sstableLock_.readLock().unlock();
-        }
-    }
-
-    public AbstractType getComparator()
-    {
-        return DatabaseDescriptor.getComparator(table_, columnFamily_);
-    }
-
-    /**
-     * Take a snap shot of this columnfamily store.
-     * 
-     * @param snapshotName the name of the associated with the snapshot 
-     */
-    public void snapshot(String snapshotName) throws IOException
-    {
-        sstableLock_.readLock().lock();
-        try
-        {
-            for (String filename : new ArrayList<String>(ssTables_.keySet()))
-            {
-                File sourceFile = new File(filename);
-
-                File dataDirectory = sourceFile.getParentFile().getParentFile();
-                String snapshotDirectoryPath = Table.getSnapshotPath(dataDirectory.getAbsolutePath(), table_, snapshotName);
-                FileUtils.createDirectory(snapshotDirectoryPath);
-
-                File targetLink = new File(snapshotDirectoryPath, sourceFile.getName());
-                FileUtils.createHardLink(new File(filename), targetLink);
-                if (logger_.isDebugEnabled())
-                    logger_.debug("Snapshot for " + table_ + " table data file " + sourceFile.getAbsolutePath() +    
-                        " created as " + targetLink.getAbsolutePath());
-            }
-        }
-        finally
-        {
-            sstableLock_.readLock().unlock();
-        }
-    }
-
-    /**
-     * for testing.  no effort is made to clear historical memtables.
-     */
-    void clearUnsafe()
-    {
-        sstableLock_.writeLock().lock();
-        try
-        {
-            memtable_.clearUnsafe();
-        }
-        finally
-        {
-            sstableLock_.writeLock().unlock();
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.File;
+import java.io.IOException;
+import java.lang.management.ManagementFactory;
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+import java.util.*;
+import java.util.concurrent.*;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.*;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.*;
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.db.filter.*;
+import org.apache.cassandra.db.marshal.AbstractType;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.collections.IteratorUtils;
+
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class ColumnFamilyStore implements ColumnFamilyStoreMBean
+{
+    private static Logger logger_ = Logger.getLogger(ColumnFamilyStore.class);
+
+    private static final int BUFSIZE = 128 * 1024 * 1024;
+    private static final int COMPACTION_MEMORY_THRESHOLD = 1 << 30;
+
+    private static NonBlockingHashMap<String, Set<Memtable>> memtablesPendingFlush = new NonBlockingHashMap<String, Set<Memtable>>();
+    private static ExecutorService flusher_ = new DebuggableThreadPoolExecutor("MEMTABLE-FLUSHER-POOL");
+
+    private final String table_;
+    public final String columnFamily_;
+    private final boolean isSuper_;
+
+    private volatile Integer memtableSwitchCount = 0;
+
+    /* This is used to generate the next index for a SSTable */
+    private AtomicInteger fileIndexGenerator_ = new AtomicInteger(0);
+
+    /* active memtable associated with this ColumnFamilyStore. */
+    private Memtable memtable_;
+    // this lock is to (1) serialize puts and
+    // (2) make sure we don't perform puts on a memtable that is queued for flush.
+    // (or conversely, flush a memtable that is mid-put.)
+    // gets may be safely performed on a flushing ("frozen") memtable.
+    private ReentrantReadWriteLock memtableLock_ = new ReentrantReadWriteLock(true);
+
+    // TODO binarymemtable ops are not threadsafe (do they need to be?)
+    private AtomicReference<BinaryMemtable> binaryMemtable_;
+
+    /* SSTables on disk for this column family */
+    private SortedMap<String, SSTableReader> ssTables_ = new TreeMap<String, SSTableReader>(new FileNameComparator(FileNameComparator.Descending));
+
+    /* Modification lock used for protecting reads from compactions. */
+    private ReentrantReadWriteLock sstableLock_ = new ReentrantReadWriteLock(true);
+
+    private TimedStatsDeque readStats_ = new TimedStatsDeque(60000);
+    private TimedStatsDeque diskReadStats_ = new TimedStatsDeque(60000);
+    private TimedStatsDeque writeStats_ = new TimedStatsDeque(60000);
+
+    ColumnFamilyStore(String table, String columnFamilyName, boolean isSuper, int indexValue) throws IOException
+    {
+        table_ = table;
+        columnFamily_ = columnFamilyName;
+        isSuper_ = isSuper;
+        fileIndexGenerator_.set(indexValue);
+        memtable_ = new Memtable(table_, columnFamily_);
+        binaryMemtable_ = new AtomicReference<BinaryMemtable>(new BinaryMemtable(table_, columnFamily_));
+    }
+
+    public static ColumnFamilyStore getColumnFamilyStore(String table, String columnFamily) throws IOException
+    {
+        /*
+         * Get all data files associated with old Memtables for this table.
+         * These files are named as follows <Table>-1.db, ..., <Table>-n.db. Get
+         * the max which in this case is n and increment it to use it for next
+         * index.
+         */
+        List<Integer> indices = new ArrayList<Integer>();
+        String[] dataFileDirectories = DatabaseDescriptor.getAllDataFileLocationsForTable(table);
+        for (String directory : dataFileDirectories)
+        {
+            File fileDir = new File(directory);
+            File[] files = fileDir.listFiles();
+            
+            for (File file : files)
+            {
+                String filename = file.getName();
+                String cfName = getColumnFamilyFromFileName(filename);
+
+                if (cfName.equals(columnFamily))
+                {
+                    int index = getIndexFromFileName(filename);
+                    indices.add(index);
+                }
+            }
+        }
+        Collections.sort(indices);
+        int value = (indices.size() > 0) ? (indices.get(indices.size() - 1)) : 0;
+
+        ColumnFamilyStore cfs = new ColumnFamilyStore(table, columnFamily, "Super".equals(DatabaseDescriptor.getColumnType(table, columnFamily)), value);
+
+        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
+        try
+        {
+            mbs.registerMBean(cfs, new ObjectName(
+                    "org.apache.cassandra.db:type=ColumnFamilyStores,name=" + table + ",columnfamily=" + columnFamily));
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+
+        return cfs;
+    }
+
+    void onStart() throws IOException
+    {
+        // scan for data files corresponding to this CF
+        List<File> sstableFiles = new ArrayList<File>();
+        String[] dataFileDirectories = DatabaseDescriptor.getAllDataFileLocationsForTable(table_);
+        for (String directory : dataFileDirectories)
+        {
+            File fileDir = new File(directory);
+            File[] files = fileDir.listFiles();
+            for (File file : files)
+            {
+                String filename = file.getName();
+                if (((file.length() == 0) || (filename.contains("-" + SSTable.TEMPFILE_MARKER))) && (filename.contains(columnFamily_)))
+                {
+                    file.delete();
+                    continue;
+                }
+
+                String cfName = getColumnFamilyFromFileName(filename);
+                if (cfName.equals(columnFamily_)
+                    && filename.contains("-Data.db"))
+                {
+                    sstableFiles.add(file.getAbsoluteFile());
+                }
+            }
+        }
+        Collections.sort(sstableFiles, new FileUtils.FileComparator());
+
+        /* Load the index files and the Bloom Filters associated with them. */
+        for (File file : sstableFiles)
+        {
+            String filename = file.getAbsolutePath();
+            try
+            {
+                SSTableReader sstable = SSTableReader.open(filename);
+                ssTables_.put(filename, sstable);
+            }
+            catch (IOException ex)
+            {
+                logger_.error("Corrupt file " + filename, ex);
+                FileUtils.delete(filename);
+            }
+        }
+
+        // submit initial check-for-compaction request
+        MinorCompactionManager.instance().submit(ColumnFamilyStore.this);
+
+        // schedule hinted handoff
+        if (table_.equals(Table.SYSTEM_TABLE) && columnFamily_.equals(HintedHandOffManager.HINTS_CF))
+        {
+            HintedHandOffManager.instance().submit(this);
+        }
+
+        // schedule periodic flusher if required
+        int flushPeriod = DatabaseDescriptor.getFlushPeriod(table_, columnFamily_);
+        if (flushPeriod > 0)
+        {
+            PeriodicFlushManager.instance().submitPeriodicFlusher(this, flushPeriod);
+        }
+    }
+
+    /*
+     * This method is called to obtain statistics about
+     * the Column Family represented by this Column Family
+     * Store. It will report the total number of files on
+     * disk and the total space oocupied by the data files
+     * associated with this Column Family.
+    */
+    public String cfStats(String newLineSeparator)
+    {
+        StringBuilder sb = new StringBuilder();
+        /*
+         * We want to do this so that if there are
+         * no files on disk we do not want to display
+         * something ugly on the admin page.
+        */
+        if (ssTables_.size() == 0)
+        {
+            return sb.toString();
+        }
+        sb.append(columnFamily_ + " statistics :");
+        sb.append(newLineSeparator);
+        sb.append("Number of files on disk : " + ssTables_.size());
+        sb.append(newLineSeparator);
+        double totalSpace = 0d;
+        for (SSTableReader sstable: ssTables_.values())
+        {
+            File f = new File(sstable.getFilename());
+            totalSpace += f.length();
+        }
+        String diskSpace = FileUtils.stringifyFileSize(totalSpace);
+        sb.append("Total disk space : " + diskSpace);
+        sb.append(newLineSeparator);
+        sb.append("--------------------------------------");
+        sb.append(newLineSeparator);
+        return sb.toString();
+    }
+
+    /*
+     * This is called after bootstrap to add the files
+     * to the list of files maintained.
+    */
+    void addToList(SSTableReader file)
+    {
+        sstableLock_.writeLock().lock();
+        try
+        {
+            ssTables_.put(file.getFilename(), file);
+        }
+        finally
+        {
+            sstableLock_.writeLock().unlock();
+        }
+    }
+
+    /*
+     * This method forces a compaction of the SSTables on disk. We wait
+     * for the process to complete by waiting on a future pointer.
+    */
+    boolean forceCompaction(List<Range> ranges, EndPoint target, long skip, List<String> fileList)
+    {
+        Future<Boolean> futurePtr = null;
+        if (ranges != null)
+        {
+            futurePtr = MinorCompactionManager.instance().submit(ColumnFamilyStore.this, ranges, target, fileList);
+        }
+        else
+        {
+            MinorCompactionManager.instance().submitMajor(ColumnFamilyStore.this, skip);
+        }
+
+        boolean result = true;
+        try
+        {
+            /* Waiting for the compaction to complete. */
+            if (futurePtr != null)
+            {
+                result = futurePtr.get();
+            }
+            if (logger_.isDebugEnabled())
+              logger_.debug("Done forcing compaction ...");
+        }
+        catch (ExecutionException ex)
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug(LogUtil.throwableToString(ex));
+        }
+        catch (InterruptedException ex2)
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug(LogUtil.throwableToString(ex2));
+        }
+        return result;
+    }
+
+    /**
+     * @return the name of the column family
+     */
+    public String getColumnFamilyName()
+    {
+        return columnFamily_;
+    }
+
+    private static String getColumnFamilyFromFileName(String filename)
+            {
+        return filename.split("-")[0];
+    }
+
+    protected static int getIndexFromFileName(String filename)
+    {
+        /*
+         * File name is of the form <table>-<column family>-<index>-Data.db.
+         * This tokenizer will strip the .db portion.
+         */
+        StringTokenizer st = new StringTokenizer(filename, "-");
+        /*
+         * Now I want to get the index portion of the filename. We accumulate
+         * the indices and then sort them to get the max index.
+         */
+        int count = st.countTokens();
+        int i = 0;
+        String index = null;
+        while (st.hasMoreElements())
+        {
+            index = (String) st.nextElement();
+            if (i == (count - 2))
+            {
+                break;
+            }
+            ++i;
+        }
+        return Integer.parseInt(index);
+    }
+
+    String getNextFileName()
+    {
+        // Psuedo increment so that we do not generate consecutive numbers
+        fileIndexGenerator_.incrementAndGet();
+        return table_ + "-" + columnFamily_ + "-" + fileIndexGenerator_.incrementAndGet();
+    }
+
+    /*
+     * @return a temporary file name for an sstable.
+     * When the sstable object is closed, it will be renamed to a non-temporary
+     * format, so incomplete sstables can be recognized and removed on startup.
+     */
+    String getTempSSTablePath()
+    {
+        // increment twice so that we do not generate consecutive numbers
+        String fname = getTempSSTableFileName();
+        return new File(DatabaseDescriptor.getDataFileLocationForTable(table_), fname).getAbsolutePath();
+    }
+
+    String getTempSSTableFileName()
+    {
+        fileIndexGenerator_.incrementAndGet();
+
+        return String.format("%s-%s-%s-Data.db",
+                             columnFamily_, SSTable.TEMPFILE_MARKER, fileIndexGenerator_.incrementAndGet());
+    }
+
+    /*
+     * Return a temporary file name. Based on the list of files input 
+     * This fn sorts the list and generates a number between he 2 lowest filenames 
+     * ensuring uniqueness.
+     * Since we do not generate consecutive numbers hence the lowest file number
+     * can just be incremented to generate the next file. 
+     */
+    String getTempFileName(List<String> files)
+    {
+        int lowestIndex;
+        int index;
+        Collections.sort(files, new FileNameComparator(FileNameComparator.Ascending));
+
+        if (files.size() <= 1)
+        {
+            return null;
+        }
+        lowestIndex = getIndexFromFileName(files.get(0));
+
+        index = lowestIndex + 1;
+
+        return String.format("%s-%s-%s-Data.db",
+                             columnFamily_, SSTable.TEMPFILE_MARKER, index);
+    }
+
+    void switchMemtable(Memtable oldMemtable, CommitLog.CommitLogContext ctx)
+    {
+        memtableLock_.writeLock().lock();
+        try
+        {
+            if (oldMemtable.isFrozen())
+            {
+                return;
+            }
+            logger_.info(columnFamily_ + " has reached its threshold; switching in a fresh Memtable");
+            oldMemtable.freeze();
+            getMemtablesPendingFlushNotNull(columnFamily_).add(oldMemtable); // it's ok for the MT to briefly be both active and pendingFlush
+            submitFlush(oldMemtable, ctx);
+            memtable_ = new Memtable(table_, columnFamily_);
+        }
+        finally
+        {
+            memtableLock_.writeLock().unlock();
+        }
+
+        if (memtableSwitchCount == Integer.MAX_VALUE)
+        {
+            memtableSwitchCount = 0;
+        }
+        memtableSwitchCount++;
+    }
+
+    void switchBinaryMemtable(String key, byte[] buffer) throws IOException
+    {
+        binaryMemtable_.set(new BinaryMemtable(table_, columnFamily_));
+        binaryMemtable_.get().put(key, buffer);
+    }
+
+    public void forceFlush()
+    {
+        if (memtable_.isClean())
+            return;
+
+        CommitLog.CommitLogContext ctx = null;
+        try
+        {
+            ctx = CommitLog.open().getContext();
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+        switchMemtable(memtable_, ctx);
+    }
+
+    void forceBlockingFlush() throws IOException, ExecutionException, InterruptedException
+    {
+        Memtable oldMemtable = getMemtableThreadSafe();
+        forceFlush();
+        // block for flush to finish by adding a no-op action to the flush executorservice
+        // and waiting for that to finish.  (this works since flush ES is single-threaded.)
+        Future f = flusher_.submit(new Runnable()
+        {
+            public void run()
+            {
+            }
+        });
+        f.get();
+        /* this assert is not threadsafe -- the memtable could have been clean when forceFlush
+           checked it, but dirty now thanks to another thread.  But as long as we are only
+           calling this from single-threaded test code it is useful to have as a sanity check. */
+        assert oldMemtable.isFlushed() || oldMemtable.isClean(); 
+    }
+
+    void forceFlushBinary()
+    {
+        BinaryMemtableManager.instance().submit(getColumnFamilyName(), binaryMemtable_.get());
+    }
+
+    /**
+     * Insert/Update the column family for this key.
+     * param @ lock - lock that needs to be used.
+     * param @ key - key for update/insert
+     * param @ columnFamily - columnFamily changes
+     */
+    void apply(String key, ColumnFamily columnFamily, CommitLog.CommitLogContext cLogCtx)
+            throws IOException
+    {
+        long start = System.currentTimeMillis();
+        Memtable initialMemtable = getMemtableThreadSafe();
+        if (initialMemtable.isThresholdViolated())
+        {
+            switchMemtable(initialMemtable, cLogCtx);
+        }
+        memtableLock_.writeLock().lock();
+        try
+        {
+            memtable_.put(key, columnFamily);
+        }
+        finally
+        {
+            memtableLock_.writeLock().unlock();
+        }
+        writeStats_.add(System.currentTimeMillis() - start);
+    }
+
+    /*
+     * Insert/Update the column family for this key. param @ lock - lock that
+     * needs to be used. param @ key - key for update/insert param @
+     * columnFamily - columnFamily changes
+     */
+    void applyBinary(String key, byte[] buffer)
+            throws IOException
+    {
+        long start = System.currentTimeMillis();
+        binaryMemtable_.get().put(key, buffer);
+        writeStats_.add(System.currentTimeMillis() - start);
+    }
+
+    /**
+     * like resolve, but leaves the resolved CF as the only item in the list
+     */
+    private static void merge(List<ColumnFamily> columnFamilies)
+    {
+        ColumnFamily cf = ColumnFamily.resolve(columnFamilies);
+        columnFamilies.clear();
+        columnFamilies.add(cf);
+    }
+
+    private static ColumnFamily resolveAndRemoveDeleted(List<ColumnFamily> columnFamilies)
+    {
+        ColumnFamily cf = ColumnFamily.resolve(columnFamilies);
+        return removeDeleted(cf);
+    }
+
+    /*
+     This is complicated because we need to preserve deleted columns, supercolumns, and columnfamilies
+     until they have been deleted for at least GC_GRACE_IN_SECONDS.  But, we do not need to preserve
+     their contents; just the object itself as a "tombstone" that can be used to repair other
+     replicas that do not know about the deletion.
+     */
+    static ColumnFamily removeDeleted(ColumnFamily cf)
+    {
+        return removeDeleted(cf, getDefaultGCBefore());
+    }
+
+    public static int getDefaultGCBefore()
+    {
+        return (int)(System.currentTimeMillis() / 1000) - DatabaseDescriptor.getGcGraceInSeconds();
+    }
+
+    static ColumnFamily removeDeleted(ColumnFamily cf, int gcBefore)
+    {
+        if (cf == null)
+        {
+            return null;
+        }
+
+        // in case of a timestamp tie, tombstones get priority over non-tombstones.
+        // we want this to be deterministic in general to avoid confusion;
+        // either way (tombstone or non- getting priority) would be fine,
+        // but we picked this way because it makes removing delivered hints
+        // easier for HintedHandoffManager.
+        for (byte[] cname : cf.getColumnNames())
+        {
+            IColumn c = cf.getColumnsMap().get(cname);
+            if (c instanceof SuperColumn)
+            {
+                long minTimestamp = Math.max(c.getMarkedForDeleteAt(), cf.getMarkedForDeleteAt());
+                // don't operate directly on the supercolumn, it could be the one in the memtable.
+                // instead, create a new SC and add in the subcolumns that qualify.
+                cf.remove(cname);
+                SuperColumn sc = ((SuperColumn)c).cloneMeShallow();
+                for (IColumn subColumn : c.getSubColumns())
+                {
+                    if (subColumn.timestamp() > minTimestamp)
+                    {
+                        if (!subColumn.isMarkedForDelete() || subColumn.getLocalDeletionTime() > gcBefore)
+                        {
+                            sc.addColumn(subColumn);
+                        }
+                    }
+                }
+                if (sc.getSubColumns().size() > 0 || sc.getLocalDeletionTime() > gcBefore)
+                {
+                    cf.addColumn(sc);
+                }
+            }
+            else if ((c.isMarkedForDelete() && c.getLocalDeletionTime() <= gcBefore)
+                     || c.timestamp() <= cf.getMarkedForDeleteAt())
+            {
+                cf.remove(cname);
+            }
+        }
+
+        if (cf.getColumnCount() == 0 && cf.getLocalDeletionTime() <= gcBefore)
+        {
+            return null;
+        }
+        return cf;
+    }
+
+    /*
+     * This version is used only on start up when we are recovering from logs.
+     * Hence no locking is required since we process logs on the main thread. In
+     * the future we may want to parellelize the log processing for a table by
+     * having a thread per log file present for recovery. Re-visit at that time.
+     */
+    void applyNow(String key, ColumnFamily columnFamily) throws IOException
+    {
+        getMemtableThreadSafe().put(key, columnFamily);
+    }
+
+    /*
+     * This method is called when the Memtable is frozen and ready to be flushed
+     * to disk. This method informs the CommitLog that a particular ColumnFamily
+     * is being flushed to disk.
+     */
+    void onMemtableFlush(CommitLog.CommitLogContext cLogCtx) throws IOException
+    {
+        if (cLogCtx.isValidContext())
+        {
+            CommitLog.open().onMemtableFlush(table_, columnFamily_, cLogCtx);
+        }
+    }
+
+    /*
+     * Called after the Memtable flushes its in-memory data. This information is
+     * cached in the ColumnFamilyStore. This is useful for reads because the
+     * ColumnFamilyStore first looks in the in-memory store and the into the
+     * disk to find the key. If invoked during recoveryMode the
+     * onMemtableFlush() need not be invoked.
+     *
+     * param @ filename - filename just flushed to disk
+     * param @ bf - bloom filter which indicates the keys that are in this file.
+    */
+    void storeLocation(SSTableReader sstable)
+    {
+        int ssTableCount;
+        sstableLock_.writeLock().lock();
+        try
+        {
+            ssTables_.put(sstable.getFilename(), sstable);
+            ssTableCount = ssTables_.size();
+        }
+        finally
+        {
+            sstableLock_.writeLock().unlock();
+        }
+
+        /* it's ok if compaction gets submitted multiple times while one is already in process.
+           worst that happens is, compactor will count the sstable files and decide there are
+           not enough to bother with. */
+        if (ssTableCount >= MinorCompactionManager.COMPACTION_THRESHOLD)
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Submitting " + columnFamily_ + " for compaction");
+            MinorCompactionManager.instance().submit(this);
+        }
+    }
+
+    private PriorityQueue<FileStruct> initializePriorityQueue(List<String> files, List<Range> ranges, int minBufferSize)
+    {
+        PriorityQueue<FileStruct> pq = new PriorityQueue<FileStruct>();
+        if (files.size() > 1 || (ranges != null && files.size() > 0))
+        {
+            int bufferSize = Math.min((ColumnFamilyStore.COMPACTION_MEMORY_THRESHOLD / files.size()), minBufferSize);
+            FileStruct fs = null;
+            for (String file : files)
+            {
+                try
+                {
+                    fs = SSTableReader.get(file).getFileStruct();
+                    fs.advance();
+                    if (fs.isExhausted())
+                    {
+                        continue;
+                    }
+                    pq.add(fs);
+                }
+                catch (Exception ex)
+                {
+                    logger_.warn("corrupt file?  or are you just blowing away data files manually out from under me?", ex);
+                    try
+                    {
+                        if (fs != null)
+                        {
+                            fs.close();
+                        }
+                    }
+                    catch (Exception e)
+                    {
+                        logger_.error("Unable to close file :" + file);
+                    }
+                }
+            }
+        }
+        return pq;
+    }
+
+    /*
+     * Group files of similar size into buckets.
+     */
+    static Set<List<String>> getCompactionBuckets(List<String> files, long min)
+    {
+        Map<List<String>, Long> buckets = new ConcurrentHashMap<List<String>, Long>();
+        for (String fname : files)
+        {
+            File f = new File(fname);
+            long size = f.length();
+
+            boolean bFound = false;
+            // look for a bucket containing similar-sized files:
+            // group in the same bucket if it's w/in 50% of the average for this bucket,
+            // or this file and the bucket are all considered "small" (less than `min`)
+            for (List<String> bucket : buckets.keySet())
+            {
+                long averageSize = buckets.get(bucket);
+                if ((size > averageSize / 2 && size < 3 * averageSize / 2)
+                    || (size < min && averageSize < min))
+                {
+                    // remove and re-add because adding changes the hash
+                    buckets.remove(bucket);
+                    averageSize = (averageSize + size) / 2;
+                    bucket.add(fname);
+                    buckets.put(bucket, averageSize);
+                    bFound = true;
+                    break;
+                }
+            }
+            // no similar bucket found; put it in a new one
+            if (!bFound)
+            {
+                ArrayList<String> bucket = new ArrayList<String>();
+                bucket.add(fname);
+                buckets.put(bucket, size);
+            }
+        }
+
+        return buckets.keySet();
+    }
+
+    /*
+     * Break the files into buckets and then compact.
+     */
+    int doCompaction(int threshold) throws IOException
+    {
+        List<String> files = new ArrayList<String>(ssTables_.keySet());
+        int filesCompacted = 0;
+        Set<List<String>> buckets = getCompactionBuckets(files, 50L * 1024L * 1024L);
+        for (List<String> fileList : buckets)
+        {
+            Collections.sort(fileList, new FileNameComparator(FileNameComparator.Ascending));
+            if (fileList.size() < threshold)
+            {
+                continue;
+            }
+            // For each bucket if it has crossed the threshhold do the compaction
+            // In case of range  compaction merge the counting bloom filters also.
+            files.clear();
+            int count = 0;
+            for (String file : fileList)
+            {
+                files.add(file);
+                count++;
+                if (count == threshold)
+                {
+                    filesCompacted += doFileCompaction(files, BUFSIZE);
+                    break;
+                }
+            }
+        }
+        return filesCompacted;
+    }
+
+    void doMajorCompaction(long skip) throws IOException
+    {
+        doMajorCompactionInternal(skip);
+    }
+
+    /*
+     * Compact all the files irrespective of the size.
+     * skip : is the amount in GB of the files to be skipped
+     * all files greater than skip GB are skipped for this compaction.
+     * Except if skip is 0 , in that case this is ignored and all files are taken.
+     */
+    void doMajorCompactionInternal(long skip) throws IOException
+    {
+        List<String> filesInternal = new ArrayList<String>(ssTables_.keySet());
+        List<String> files;
+        if (skip > 0L)
+        {
+            files = new ArrayList<String>();
+            for (String file : filesInternal)
+            {
+                File f = new File(file);
+                if (f.length() < skip * 1024L * 1024L * 1024L)
+                {
+                    files.add(file);
+                }
+            }
+        }
+        else
+        {
+            files = filesInternal;
+        }
+        doFileCompaction(files, BUFSIZE);
+    }
+
+    /*
+     * Add up all the files sizes this is the worst case file
+     * size for compaction of all the list of files given.
+     */
+    long getExpectedCompactedFileSize(List<String> files)
+    {
+        long expectedFileSize = 0;
+        for (String file : files)
+        {
+            File f = new File(file);
+            long size = f.length();
+            expectedFileSize = expectedFileSize + size;
+        }
+        return expectedFileSize;
+    }
+
+    /*
+     *  Find the maximum size file in the list .
+     */
+    String getMaxSizeFile(List<String> files)
+    {
+        long maxSize = 0L;
+        String maxFile = null;
+        for (String file : files)
+        {
+            File f = new File(file);
+            if (f.length() > maxSize)
+            {
+                maxSize = f.length();
+                maxFile = file;
+            }
+        }
+        return maxFile;
+    }
+
+    boolean doAntiCompaction(List<Range> ranges, EndPoint target, List<String> fileList) throws IOException
+    {
+        List<String> files = new ArrayList<String>(ssTables_.keySet());
+        return doFileAntiCompaction(files, ranges, target, fileList);
+    }
+
+    void forceCleanup()
+    {
+        MinorCompactionManager.instance().submitCleanup(ColumnFamilyStore.this);
+    }
+
+    /**
+     * This function goes over each file and removes the keys that the node is not responsible for
+     * and only keeps keys that this node is responsible for.
+     *
+     * @throws IOException
+     */
+    void doCleanupCompaction() throws IOException
+    {
+        List<String> files = new ArrayList<String>(ssTables_.keySet());
+        for (String file : files)
+        {
+            doCleanup(file);
+        }
+    }
+
+    /**
+     * cleans up one particular file by removing keys that this node is not responsible for.
+     *
+     * @param file
+     * @throws IOException
+     */
+    /* TODO: Take care of the comments later. */
+    void doCleanup(String file) throws IOException
+    {
+        if (file == null)
+        {
+            return;
+        }
+        List<Range> myRanges;
+        List<String> files = new ArrayList<String>();
+        files.add(file);
+        List<String> newFiles = new ArrayList<String>();
+        Map<EndPoint, List<Range>> endPointtoRangeMap = StorageService.instance().constructEndPointToRangesMap();
+        myRanges = endPointtoRangeMap.get(StorageService.getLocalStorageEndPoint());
+        doFileAntiCompaction(files, myRanges, null, newFiles);
+        if (logger_.isDebugEnabled())
+          logger_.debug("Original file : " + file + " of size " + new File(file).length());
+        sstableLock_.writeLock().lock();
+        try
+        {
+            ssTables_.remove(file);
+            for (String newfile : newFiles)
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug("New file : " + newfile + " of size " + new File(newfile).length());
+                assert newfile != null;
+                ssTables_.put(newfile, SSTableReader.open(newfile));
+            }
+            SSTableReader.get(file).delete();
+        }
+        finally
+        {
+            sstableLock_.writeLock().unlock();
+        }
+    }
+
+    /**
+     * This function is used to do the anti compaction process , it spits out the file which has keys that belong to a given range
+     * If the target is not specified it spits out the file as a compacted file with the unecessary ranges wiped out.
+     *
+     * @param files
+     * @param ranges
+     * @param target
+     * @param fileList
+     * @return
+     * @throws IOException
+     */
+    boolean doFileAntiCompaction(List<String> files, List<Range> ranges, EndPoint target, List<String> fileList) throws IOException
+    {
+        boolean result = false;
+        long startTime = System.currentTimeMillis();
+        long totalBytesRead = 0;
+        long totalBytesWritten = 0;
+        long totalkeysRead = 0;
+        long totalkeysWritten = 0;
+        String rangeFileLocation;
+        String mergedFileName;
+        IPartitioner p = StorageService.getPartitioner();
+        // Calculate the expected compacted filesize
+        long expectedRangeFileSize = getExpectedCompactedFileSize(files);
+        /* in the worst case a node will be giving out half of its data so we take a chance */
+        expectedRangeFileSize = expectedRangeFileSize / 2;
+        rangeFileLocation = DatabaseDescriptor.getDataFileLocationForTable(table_, expectedRangeFileSize);
+        // If the compaction file path is null that means we have no space left for this compaction.
+        if (rangeFileLocation == null)
+        {
+            logger_.warn("Total bytes to be written for range compaction  ..."
+                         + expectedRangeFileSize + "   is greater than the safe limit of the disk space available.");
+            return result;
+        }
+        PriorityQueue<FileStruct> pq = initializePriorityQueue(files, ranges, ColumnFamilyStore.BUFSIZE);
+        if (pq.isEmpty())
+        {
+            return result;
+        }
+
+        mergedFileName = getTempSSTableFileName();
+        SSTableWriter rangeWriter = null;
+        String lastkey = null;
+        List<FileStruct> lfs = new ArrayList<FileStruct>();
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        int expectedBloomFilterSize = SSTableReader.getApproximateKeyCount(files);
+        expectedBloomFilterSize = (expectedBloomFilterSize > 0) ? expectedBloomFilterSize : SSTableReader.indexInterval();
+        if (logger_.isDebugEnabled())
+          logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
+        List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();
+
+        while (pq.size() > 0 || lfs.size() > 0)
+        {
+            FileStruct fs = null;
+            if (pq.size() > 0)
+            {
+                fs = pq.poll();
+            }
+            if (fs != null
+                && (lastkey == null || lastkey.equals(fs.getKey())))
+            {
+                // The keys are the same so we need to add this to the
+                // ldfs list
+                lastkey = fs.getKey();
+                lfs.add(fs);
+            }
+            else
+            {
+                Collections.sort(lfs, new FileStructComparator());
+                ColumnFamily columnFamily;
+                bufOut.reset();
+                if (lfs.size() > 1)
+                {
+                    for (FileStruct filestruct : lfs)
+                    {
+                        try
+                        {
+                            /* read the length although we don't need it */
+                            filestruct.getBufIn().readInt();
+                            // Skip the Index
+                            IndexHelper.skipBloomFilterAndIndex(filestruct.getBufIn());
+                            // We want to add only 2 and resolve them right there in order to save on memory footprint
+                            if (columnFamilies.size() > 1)
+                            {
+                                // Now merge the 2 column families
+                                merge(columnFamilies);
+                            }
+                            // deserialize into column families
+                            columnFamilies.add(ColumnFamily.serializer().deserialize(filestruct.getBufIn()));
+                        }
+                        catch (Exception ex)
+                        {
+                            logger_.warn(LogUtil.throwableToString(ex));
+                        }
+                    }
+                    // Now after merging all crap append to the sstable
+                    columnFamily = resolveAndRemoveDeleted(columnFamilies);
+                    columnFamilies.clear();
+                    if (columnFamily != null)
+                    {
+                        /* serialize the cf with column indexes */
+                        ColumnFamily.serializerWithIndexes().serialize(columnFamily, bufOut);
+                    }
+                }
+                else
+                {
+                    FileStruct filestruct = lfs.get(0);
+                    /* read the length although we don't need it */
+                    int size = filestruct.getBufIn().readInt();
+                    bufOut.write(filestruct.getBufIn(), size);
+                }
+                if (Range.isTokenInRanges(StorageService.getPartitioner().getInitialToken(lastkey), ranges))
+                {
+                    if (rangeWriter == null)
+                    {
+                        if (target != null)
+                        {
+                            rangeFileLocation = rangeFileLocation + File.separator + "bootstrap";
+                        }
+                        FileUtils.createDirectory(rangeFileLocation);
+                        String fname = new File(rangeFileLocation, mergedFileName).getAbsolutePath();
+                        rangeWriter = new SSTableWriter(fname, expectedBloomFilterSize, StorageService.getPartitioner());
+                    }
+                    try
+                    {
+                        rangeWriter.append(lastkey, bufOut);
+                    }
+                    catch (Exception ex)
+                    {
+                        logger_.warn(LogUtil.throwableToString(ex));
+                    }
+                }
+                totalkeysWritten++;
+                for (FileStruct filestruct : lfs)
+                {
+                    try
+                    {
+                        filestruct.advance();
+                        if (filestruct.isExhausted())
+                        {
+                            continue;
+                        }
+                        /* keep on looping until we find a key in the range */
+                        while (!Range.isTokenInRanges(StorageService.getPartitioner().getInitialToken(filestruct.getKey()), ranges))
+                        {
+                            filestruct.advance();
+                            if (filestruct.isExhausted())
+                            {
+                                break;
+                            }
+                        }
+                        if (!filestruct.isExhausted())
+                        {
+                            pq.add(filestruct);
+                        }
+                        totalkeysRead++;
+                    }
+                    catch (Exception ex)
+                    {
+                        // Ignore the exception as it might be a corrupted file
+                        // in any case we have read as far as possible from it
+                        // and it will be deleted after compaction.
+                        logger_.warn("corrupt sstable?", ex);
+                        filestruct.close();
+                    }
+                }
+                lfs.clear();
+                lastkey = null;
+                if (fs != null)
+                {
+                    // Add back the fs since we processed the rest of
+                    // filestructs
+                    pq.add(fs);
+                }
+            }
+        }
+
+        if (rangeWriter != null)
+        {
+            rangeWriter.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_));
+            if (fileList != null)
+            {
+                fileList.add(rangeWriter.getFilename());
+            }
+        }
+
+        if (logger_.isDebugEnabled())
+        {
+            logger_.debug("Total time taken for range split   ..." + (System.currentTimeMillis() - startTime));
+            logger_.debug("Total bytes Read for range split  ..." + totalBytesRead);
+            logger_.debug("Total bytes written for range split  ..."
+                          + totalBytesWritten + "   Total keys read ..." + totalkeysRead);
+        }
+        return result;
+    }
+
+    private void doFill(BloomFilter bf, String decoratedKey)
+    {
+        bf.add(StorageService.getPartitioner().undecorateKey(decoratedKey));
+    }
+
+    /*
+    * This function does the actual compaction for files.
+    * It maintains a priority queue of with the first key from each file
+    * and then removes the top of the queue and adds it to the SStable and
+    * repeats this process while reading the next from each file until its
+    * done with all the files . The SStable to which the keys are written
+    * represents the new compacted file. Before writing if there are keys
+    * that occur in multiple files and are the same then a resolution is done
+    * to get the latest data.
+    *
+    */
+    private int doFileCompaction(List<String> files, int minBufferSize) throws IOException
+    {
+        logger_.info("Compacting [" + StringUtils.join(files, ",") + "]");
+        String compactionFileLocation = DatabaseDescriptor.getDataFileLocationForTable(table_, getExpectedCompactedFileSize(files));
+        // If the compaction file path is null that means we have no space left for this compaction.
+        // try again w/o the largest one.
+        if (compactionFileLocation == null)
+        {
+            String maxFile = getMaxSizeFile(files);
+            files.remove(maxFile);
+            return doFileCompaction(files, minBufferSize);
+        }
+
+        String newfile = null;
+        long startTime = System.currentTimeMillis();
+        long totalBytesRead = 0;
+        long totalBytesWritten = 0;
+        long totalkeysRead = 0;
+        long totalkeysWritten = 0;
+        PriorityQueue<FileStruct> pq = initializePriorityQueue(files, null, minBufferSize);
+
+        if (pq.isEmpty())
+        {
+            logger_.warn("Nothing to compact (all files empty or corrupt)");
+            // TODO clean out bad files, if any
+            return 0;
+        }
+
+        String mergedFileName = getTempFileName(files);
+        SSTableWriter writer = null;
+        SSTableReader ssTable = null;
+        String lastkey = null;
+        List<FileStruct> lfs = new ArrayList<FileStruct>();
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        int expectedBloomFilterSize = SSTableReader.getApproximateKeyCount(files);
+        expectedBloomFilterSize = (expectedBloomFilterSize > 0) ? expectedBloomFilterSize : SSTableReader.indexInterval();
+        if (logger_.isDebugEnabled())
+          logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
+        List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();
+
+        while (pq.size() > 0 || lfs.size() > 0)
+        {
+            FileStruct fs = null;
+            if (pq.size() > 0)
+            {
+                fs = pq.poll();
+            }
+            if (fs != null
+                && (lastkey == null || lastkey.equals(fs.getKey())))
+            {
+                // The keys are the same so we need to add this to the
+                // ldfs list
+                lastkey = fs.getKey();
+                lfs.add(fs);
+            }
+            else
+            {
+                Collections.sort(lfs, new FileStructComparator());
+                ColumnFamily columnFamily;
+                bufOut.reset();
+                if (lfs.size() > 1)
+                {
+                    for (FileStruct filestruct : lfs)
+                    {
+                        try
+                        {
+                            /* read the length although we don't need it */
+                            filestruct.getBufIn().readInt();
+                            // Skip the Index
+                            IndexHelper.skipBloomFilterAndIndex(filestruct.getBufIn());
+                            // We want to add only 2 and resolve them right there in order to save on memory footprint
+                            if (columnFamilies.size() > 1)
+                            {
+                                merge(columnFamilies);
+                            }
+                            // deserialize into column families
+                            columnFamilies.add(ColumnFamily.serializer().deserialize(filestruct.getBufIn()));
+                        }
+                        catch (Exception ex)
+                        {
+                            logger_.warn("error in filecompaction", ex);
+                        }
+                    }
+                    // Now after merging all crap append to the sstable
+                    columnFamily = resolveAndRemoveDeleted(columnFamilies);
+                    columnFamilies.clear();
+                    if (columnFamily != null)
+                    {
+                        /* serialize the cf with column indexes */
+                        ColumnFamily.serializerWithIndexes().serialize(columnFamily, bufOut);
+                    }
+                }
+                else
+                {
+                    FileStruct filestruct = lfs.get(0);
+                    /* read the length although we don't need it */
+                    int size = filestruct.getBufIn().readInt();
+                    bufOut.write(filestruct.getBufIn(), size);
+                }
+
+                if (writer == null)
+                {
+                    String fname = new File(compactionFileLocation, mergedFileName).getAbsolutePath();
+                    writer = new SSTableWriter(fname, expectedBloomFilterSize, StorageService.getPartitioner());
+                }
+                writer.append(lastkey, bufOut);
+                totalkeysWritten++;
+
+                for (FileStruct filestruct : lfs)
+                {
+                    try
+                    {
+                        filestruct.advance();
+                        if (filestruct.isExhausted())
+                        {
+                            continue;
+                        }
+                        pq.add(filestruct);
+                        totalkeysRead++;
+                    }
+                    catch (Throwable ex)
+                    {
+                        // Ignore the exception as it might be a corrupted file
+                        // in any case we have read as far as possible from it
+                        // and it will be deleted after compaction.
+                        logger_.warn("corrupt sstable?", ex);
+                        filestruct.close();
+                    }
+                }
+                lfs.clear();
+                lastkey = null;
+                if (fs != null)
+                {
+                    /* Add back the fs since we processed the rest of filestructs */
+                    pq.add(fs);
+                }
+            }
+        }
+        if (writer != null)
+        {
+            // TODO if all the keys were the same nothing will be done here
+            ssTable = writer.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_));
+            newfile = writer.getFilename();
+        }
+        sstableLock_.writeLock().lock();
+        try
+        {
+            for (String file : files)
+            {
+                ssTables_.remove(file);
+            }
+            if (newfile != null)
+            {
+                ssTables_.put(newfile, ssTable);
+                totalBytesWritten += (new File(newfile)).length();
+            }
+            for (String file : files)
+            {
+                SSTableReader.get(file).delete();
+            }
+        }
+        finally
+        {
+            sstableLock_.writeLock().unlock();
+        }
+
+        String format = "Compacted to %s.  %d/%d bytes for %d/%d keys read/written.  Time: %dms.";
+        long dTime = System.currentTimeMillis() - startTime;
+        logger_.info(String.format(format, newfile, totalBytesRead, totalBytesWritten, totalkeysRead, totalkeysWritten, dTime));
+        return files.size();
+    }
+
+    public static List<Memtable> getUnflushedMemtables(String cfName)
+    {
+        return new ArrayList<Memtable>(getMemtablesPendingFlushNotNull(cfName));
+    }
+
+    private static Set<Memtable> getMemtablesPendingFlushNotNull(String columnFamilyName)
+    {
+        Set<Memtable> memtables = memtablesPendingFlush.get(columnFamilyName);
+        if (memtables == null)
+        {
+            memtablesPendingFlush.putIfAbsent(columnFamilyName, new ConcurrentSkipListSet<Memtable>());
+            memtables = memtablesPendingFlush.get(columnFamilyName); // might not be the object we just put, if there was a race!
+        }
+        return memtables;
+    }
+
+    /* Submit memtables to be flushed to disk */
+    private static void submitFlush(final Memtable memtable, final CommitLog.CommitLogContext cLogCtx)
+    {
+        logger_.info("Enqueuing flush of " + memtable);
+        flusher_.submit(new Runnable()
+        {
+            public void run()
+            {
+                try
+                {
+                    memtable.flush(cLogCtx);
+                }
+                catch (IOException e)
+                {
+                    throw new RuntimeException(e);
+                }
+                getMemtablesPendingFlushNotNull(memtable.getColumnFamily()).remove(memtable);
+            }
+        });
+    }
+
+    public boolean isSuper()
+    {
+        return isSuper_;
+    }
+
+    public void flushMemtableOnRecovery() throws IOException
+    {
+        getMemtableThreadSafe().flushOnRecovery();
+    }
+
+    public int getMemtableColumnsCount()
+    {
+        return getMemtableThreadSafe().getCurrentObjectCount();
+    }
+
+    public int getMemtableDataSize()
+    {
+        return getMemtableThreadSafe().getCurrentSize();
+    }
+
+    public int getMemtableSwitchCount()
+    {
+        return memtableSwitchCount;
+    }
+
+    /**
+     * get the current memtable in a threadsafe fashion.  note that simply "return memtable_" is
+     * incorrect; you need to lock to introduce a thread safe happens-before ordering.
+     *
+     * do NOT use this method to do either a put or get on the memtable object, since it could be
+     * flushed in the meantime (and its executor terminated).
+     *
+     * also do NOT make this method public or it will really get impossible to reason about these things.
+     * @return
+     */
+    private Memtable getMemtableThreadSafe()
+    {
+        memtableLock_.readLock().lock();
+        try
+        {
+            return memtable_;
+        }
+        finally
+        {
+            memtableLock_.readLock().unlock();
+        }
+    }
+
+    public Iterator<String> memtableKeyIterator() throws ExecutionException, InterruptedException
+    {
+        Set<String> keys;
+        memtableLock_.readLock().lock();
+        try
+        {
+            keys = memtable_.getKeys();
+        }
+        finally
+        {
+            memtableLock_.readLock().unlock();
+        }
+        return Memtable.getKeyIterator(keys);
+    }
+
+    /** not threadsafe.  caller must have lock_ acquired. */
+    public Collection<SSTableReader> getSSTables()
+    {
+        return Collections.unmodifiableCollection(ssTables_.values());
+    }
+
+    public ReentrantReadWriteLock.ReadLock getReadLock()
+    {
+        return sstableLock_.readLock();
+    }
+
+    public int getReadCount()
+    {
+        return readStats_.size();
+    }
+
+    public int getReadDiskHits()
+    {
+        return diskReadStats_.size();
+    }
+
+    public double getReadLatency()
+    {
+        return readStats_.mean();
+    }
+    
+    public int getPendingTasks()
+    {
+        return memtableLock_.getQueueLength();
+    }
+
+    /**
+     * @return the number of write operations on this column family in the last minute
+     */
+    public int getWriteCount() {
+        return writeStats_.size();
+    }
+
+    /**
+     * @return average latency per write operation in the last minute
+     */
+    public double getWriteLatency() {
+        return writeStats_.mean();
+    }
+
+    public ColumnFamily getColumnFamily(String key, QueryPath path, byte[] start, byte[] finish, boolean isAscending, int limit) throws IOException
+    {
+        return getColumnFamily(new SliceQueryFilter(key, path, start, finish, isAscending, limit));
+    }
+
+    public ColumnFamily getColumnFamily(QueryFilter filter) throws IOException
+    {
+        return getColumnFamily(filter, getDefaultGCBefore());
+    }
+
+    /**
+     * get a list of columns starting from a given column, in a specified order.
+     * only the latest version of a column is returned.
+     * @return null if there is no data and no tombstones; otherwise a ColumnFamily
+     */
+    public ColumnFamily getColumnFamily(QueryFilter filter, int gcBefore) throws IOException
+    {
+        assert columnFamily_.equals(filter.getColumnFamilyName());
+
+        // if we are querying subcolumns of a supercolumn, fetch the supercolumn with NQF, then filter in-memory.
+        if (filter.path.superColumnName != null)
+        {
+            AbstractType comparator = DatabaseDescriptor.getComparator(table_, columnFamily_);
+            QueryFilter nameFilter = new NamesQueryFilter(filter.key, new QueryPath(columnFamily_), filter.path.superColumnName);
+            ColumnFamily cf = getColumnFamily(nameFilter);
+            if (cf != null)
+            {
+                for (IColumn column : cf.getSortedColumns())
+                {
+                    filter.filterSuperColumn((SuperColumn)column, gcBefore);
+                }
+            }
+            return removeDeleted(cf, gcBefore);
+        }
+
+        // we are querying top-level columns, do a merging fetch with indexes.
+        sstableLock_.readLock().lock();
+        List<ColumnIterator> iterators = new ArrayList<ColumnIterator>();
+        try
+        {
+            final ColumnFamily returnCF;
+            ColumnIterator iter;
+        
+            /* add the current memtable */
+            memtableLock_.readLock().lock();
+            try
+            {
+                iter = filter.getMemColumnIterator(memtable_, getComparator());
+                returnCF = iter.getColumnFamily();
+            }
+            finally
+            {
+                memtableLock_.readLock().unlock();            
+            }        
+            iterators.add(iter);
+
+            /* add the memtables being flushed */
+            List<Memtable> memtables = getUnflushedMemtables(filter.getColumnFamilyName());
+            for (Memtable memtable:memtables)
+            {
+                iter = filter.getMemColumnIterator(memtable, getComparator());
+                returnCF.delete(iter.getColumnFamily());
+                iterators.add(iter);
+            }
+
+            /* add the SSTables on disk */
+            List<SSTableReader> sstables = new ArrayList<SSTableReader>(ssTables_.values());
+            for (SSTableReader sstable : sstables)
+            {
+                iter = filter.getSSTableColumnIterator(sstable, getComparator());
+                if (iter.hasNext()) // initializes iter.CF
+                {
+                    returnCF.delete(iter.getColumnFamily());
+                }
+                iterators.add(iter);
+            }
+
+            Comparator<IColumn> comparator = filter.getColumnComparator(getComparator());
+            Iterator collated = IteratorUtils.collatedIterator(comparator, iterators);
+            if (!collated.hasNext())
+                return null;
+
+            filter.collectColumns(returnCF, collated, gcBefore);
+
+            return removeDeleted(returnCF, gcBefore); // collect does a first pass but doesn't try to recognize e.g. the entire CF being tombstoned
+        }
+        finally
+        {
+            /* close all cursors */
+            for (ColumnIterator ci : iterators)
+            {
+                try
+                {
+                    ci.close();
+                }
+                catch (Throwable th)
+                {
+                    logger_.error(th);
+                }
+            }
+
+            sstableLock_.readLock().unlock();
+        }
+    }
+
+    public AbstractType getComparator()
+    {
+        return DatabaseDescriptor.getComparator(table_, columnFamily_);
+    }
+
+    /**
+     * Take a snap shot of this columnfamily store.
+     * 
+     * @param snapshotName the name of the associated with the snapshot 
+     */
+    public void snapshot(String snapshotName) throws IOException
+    {
+        sstableLock_.readLock().lock();
+        try
+        {
+            for (String filename : new ArrayList<String>(ssTables_.keySet()))
+            {
+                File sourceFile = new File(filename);
+
+                File dataDirectory = sourceFile.getParentFile().getParentFile();
+                String snapshotDirectoryPath = Table.getSnapshotPath(dataDirectory.getAbsolutePath(), table_, snapshotName);
+                FileUtils.createDirectory(snapshotDirectoryPath);
+
+                File targetLink = new File(snapshotDirectoryPath, sourceFile.getName());
+                FileUtils.createHardLink(new File(filename), targetLink);
+                if (logger_.isDebugEnabled())
+                    logger_.debug("Snapshot for " + table_ + " table data file " + sourceFile.getAbsolutePath() +    
+                        " created as " + targetLink.getAbsolutePath());
+            }
+        }
+        finally
+        {
+            sstableLock_.readLock().unlock();
+        }
+    }
+
+    /**
+     * for testing.  no effort is made to clear historical memtables.
+     */
+    void clearUnsafe()
+    {
+        sstableLock_.writeLock().lock();
+        try
+        {
+            memtable_.clearUnsafe();
+        }
+        finally
+        {
+            sstableLock_.writeLock().unlock();
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/ColumnIndexer.java b/src/java/org/apache/cassandra/db/ColumnIndexer.java
index ff42ed82f2..a507a3612d 100644
--- a/src/java/org/apache/cassandra/db/ColumnIndexer.java
+++ b/src/java/org/apache/cassandra/db/ColumnIndexer.java
@@ -1,151 +1,151 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.List;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.io.DataOutputBuffer;
-import org.apache.cassandra.io.IndexHelper;
-import org.apache.cassandra.utils.BloomFilter;
-import org.apache.cassandra.db.marshal.AbstractType;
-
-
-/**
- * Help to create an index for a column family based on size of columns
- */
-
-public class ColumnIndexer
-{
-	/**
-	 * Given a column family this, function creates an in-memory structure that represents the
-	 * column index for the column family, and subsequently writes it to disk.
-	 * @param columnFamily Column family to create index for
-	 * @param dos data output stream
-	 * @throws IOException
-	 */
-    public static void serialize(ColumnFamily columnFamily, DataOutputStream dos) throws IOException
-	{
-        Collection<IColumn> columns = columnFamily.getSortedColumns();
-        BloomFilter bf = createColumnBloomFilter(columns);                    
-        /* Write out the bloom filter. */
-        DataOutputBuffer bufOut = new DataOutputBuffer(); 
-        BloomFilter.serializer().serialize(bf, bufOut);
-        /* write the length of the serialized bloom filter. */
-        dos.writeInt(bufOut.getLength());
-        /* write out the serialized bytes. */
-        dos.write(bufOut.getData(), 0, bufOut.getLength());
-
-        /* Do the indexing */
-        doIndexing(columnFamily.getComparator(), columns, dos);
-	}
-    
-    /**
-     * Create a bloom filter that contains the subcolumns and the columns that
-     * make up this Column Family.
-     * @param columns columns of the ColumnFamily
-     * @return BloomFilter with the summarized information.
-     */
-    private static BloomFilter createColumnBloomFilter(Collection<IColumn> columns)
-    {
-        int columnCount = 0;
-        for (IColumn column : columns)
-        {
-            columnCount += column.getObjectCount();
-        }
-
-        BloomFilter bf = new BloomFilter(columnCount, 4);
-        for (IColumn column : columns)
-        {
-            bf.add(column.name());
-            /* If this is SuperColumn type Column Family we need to get the subColumns too. */
-            if (column instanceof SuperColumn)
-            {
-                Collection<IColumn> subColumns = column.getSubColumns();
-                for (IColumn subColumn : subColumns)
-                {
-                    bf.add(subColumn.name());
-                }
-            }
-        }
-        return bf;
-    }
-
-    /**
-     * Given the collection of columns in the Column Family,
-     * the name index is generated and written into the provided
-     * stream
-     * @param columns for whom the name index needs to be generated
-     * @param dos stream into which the serialized name index needs
-     *            to be written.
-     * @throws IOException
-     */
-    private static void doIndexing(AbstractType comparator, Collection<IColumn> columns, DataOutputStream dos) throws IOException
-    {
-        /* we are going to write column indexes */
-        int numColumns = 0;
-        int position = 0;
-        int indexSizeInBytes = 0;
-        int sizeSummarized = 0;
-        
-        /*
-         * Maintains a list of KeyPositionInfo objects for the columns in this
-         * column family. The key is the column name and the position is the
-         * relative offset of that column name from the start of the list.
-         * We do this so that we don't read all the columns into memory.
-        */
-        
-        List<IndexHelper.ColumnIndexInfo> columnIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();        
-        
-        /* column offsets at the right thresholds into the index map. */
-        for ( IColumn column : columns )
-        {
-            /* if we hit the column index size that we have to index after, go ahead and index it */
-            if(position - sizeSummarized >= DatabaseDescriptor.getColumnIndexSize())
-            {      
-                /*
-                 * ColumnSort applies only to columns. So in case of 
-                 * SuperColumn always use the name indexing scheme for
-                 * the SuperColumns. We will fix this later.
-                 */
-                IndexHelper.ColumnIndexInfo cIndexInfo = new IndexHelper.ColumnIndexInfo(column.name(), 0, 0, comparator);
-                cIndexInfo.position(position);
-                cIndexInfo.count(numColumns);                
-                columnIndexList.add(cIndexInfo);
-                /*
-                 * we will be writing this object as a UTF8 string and two ints,
-                 * so calculate the size accordingly. Note that we store the string
-                 * as UTF-8 encoded, so when we calculate the length, it should be
-                 * converted to UTF-8.
-                 */
-                indexSizeInBytes += cIndexInfo.size();
-                sizeSummarized = position;
-                numColumns = 0;
-            }
-            position += column.serializedSize();
-            ++numColumns;
-        }
-        /* write the column index list */
-        IndexHelper.serialize(indexSizeInBytes, columnIndexList, dos);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IndexHelper;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.db.marshal.AbstractType;
+
+
+/**
+ * Help to create an index for a column family based on size of columns
+ */
+
+public class ColumnIndexer
+{
+	/**
+	 * Given a column family this, function creates an in-memory structure that represents the
+	 * column index for the column family, and subsequently writes it to disk.
+	 * @param columnFamily Column family to create index for
+	 * @param dos data output stream
+	 * @throws IOException
+	 */
+    public static void serialize(ColumnFamily columnFamily, DataOutputStream dos) throws IOException
+	{
+        Collection<IColumn> columns = columnFamily.getSortedColumns();
+        BloomFilter bf = createColumnBloomFilter(columns);                    
+        /* Write out the bloom filter. */
+        DataOutputBuffer bufOut = new DataOutputBuffer(); 
+        BloomFilter.serializer().serialize(bf, bufOut);
+        /* write the length of the serialized bloom filter. */
+        dos.writeInt(bufOut.getLength());
+        /* write out the serialized bytes. */
+        dos.write(bufOut.getData(), 0, bufOut.getLength());
+
+        /* Do the indexing */
+        doIndexing(columnFamily.getComparator(), columns, dos);
+	}
+    
+    /**
+     * Create a bloom filter that contains the subcolumns and the columns that
+     * make up this Column Family.
+     * @param columns columns of the ColumnFamily
+     * @return BloomFilter with the summarized information.
+     */
+    private static BloomFilter createColumnBloomFilter(Collection<IColumn> columns)
+    {
+        int columnCount = 0;
+        for (IColumn column : columns)
+        {
+            columnCount += column.getObjectCount();
+        }
+
+        BloomFilter bf = new BloomFilter(columnCount, 4);
+        for (IColumn column : columns)
+        {
+            bf.add(column.name());
+            /* If this is SuperColumn type Column Family we need to get the subColumns too. */
+            if (column instanceof SuperColumn)
+            {
+                Collection<IColumn> subColumns = column.getSubColumns();
+                for (IColumn subColumn : subColumns)
+                {
+                    bf.add(subColumn.name());
+                }
+            }
+        }
+        return bf;
+    }
+
+    /**
+     * Given the collection of columns in the Column Family,
+     * the name index is generated and written into the provided
+     * stream
+     * @param columns for whom the name index needs to be generated
+     * @param dos stream into which the serialized name index needs
+     *            to be written.
+     * @throws IOException
+     */
+    private static void doIndexing(AbstractType comparator, Collection<IColumn> columns, DataOutputStream dos) throws IOException
+    {
+        /* we are going to write column indexes */
+        int numColumns = 0;
+        int position = 0;
+        int indexSizeInBytes = 0;
+        int sizeSummarized = 0;
+        
+        /*
+         * Maintains a list of KeyPositionInfo objects for the columns in this
+         * column family. The key is the column name and the position is the
+         * relative offset of that column name from the start of the list.
+         * We do this so that we don't read all the columns into memory.
+        */
+        
+        List<IndexHelper.ColumnIndexInfo> columnIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();        
+        
+        /* column offsets at the right thresholds into the index map. */
+        for ( IColumn column : columns )
+        {
+            /* if we hit the column index size that we have to index after, go ahead and index it */
+            if(position - sizeSummarized >= DatabaseDescriptor.getColumnIndexSize())
+            {      
+                /*
+                 * ColumnSort applies only to columns. So in case of 
+                 * SuperColumn always use the name indexing scheme for
+                 * the SuperColumns. We will fix this later.
+                 */
+                IndexHelper.ColumnIndexInfo cIndexInfo = new IndexHelper.ColumnIndexInfo(column.name(), 0, 0, comparator);
+                cIndexInfo.position(position);
+                cIndexInfo.count(numColumns);                
+                columnIndexList.add(cIndexInfo);
+                /*
+                 * we will be writing this object as a UTF8 string and two ints,
+                 * so calculate the size accordingly. Note that we store the string
+                 * as UTF-8 encoded, so when we calculate the length, it should be
+                 * converted to UTF-8.
+                 */
+                indexSizeInBytes += cIndexInfo.size();
+                sizeSummarized = position;
+                numColumns = 0;
+            }
+            position += column.serializedSize();
+            ++numColumns;
+        }
+        /* write the column index list */
+        IndexHelper.serialize(indexSizeInBytes, columnIndexList, dos);
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/CommitLog.java b/src/java/org/apache/cassandra/db/CommitLog.java
index 26f7328169..ce890e1659 100644
--- a/src/java/org/apache/cassandra/db/CommitLog.java
+++ b/src/java/org/apache/cassandra/db/CommitLog.java
@@ -1,562 +1,562 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.*;
-import java.util.*;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.io.*;
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.utils.FileUtils;
-
-import org.apache.log4j.Logger;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Callable;
-import java.util.concurrent.ExecutionException;
-
-/*
- * Commit Log tracks every write operation into the system. The aim
- * of the commit log is to be able to successfully recover data that was
- * not stored to disk via the Memtable. Every Commit Log maintains a
- * header represented by the abstraction CommitLogHeader. The header
- * contains a bit array and an array of longs and both the arrays are
- * of size, #column families for the Table, the Commit Log represents.
- *
- * Whenever a ColumnFamily is written to, for the first time its bit flag
- * is set to one in the CommitLogHeader. When it is flushed to disk by the
- * Memtable its corresponding bit in the header is set to zero. This helps
- * track which CommitLogs can be thrown away as a result of Memtable flushes.
- * Additionally, when a ColumnFamily is flushed and written to disk, its
- * entry in the array of longs is updated with the offset in the Commit Log
- * file where it was written. This helps speed up recovery since we can seek
- * to these offsets and start processing the commit log.
- *
- * Every Commit Log is rolled over everytime it reaches its threshold in size;
- * the new log inherits the "dirty" bits from the old.
- *
- * Over time there could be a number of commit logs that would be generated.
- * To allow cleaning up non-active commit logs, whenever we flush a column family and update its bit flag in
- * the active CL, we take the dirty bit array and bitwise & it with the headers of the older logs.
- * If the result is 0, then it is safe to remove the older file.  (Since the new CL
- * inherited the old's dirty bitflags, getting a zero for any given bit in the anding
- * means that either the CF was clean in the old CL or it has been flushed since the
- * switch in the new.)
- *
- * The CommitLog class itself is "mostly a singleton."  open() always returns one
- * instance, but log replay will bypass that.
- */
-public class CommitLog
-{
-    private static volatile int SEGMENT_SIZE = 128*1024*1024; // roll after log gets this big
-    private static volatile CommitLog instance_;
-    private static Lock lock_ = new ReentrantLock();
-    private static Logger logger_ = Logger.getLogger(CommitLog.class);
-    private static Map<String, CommitLogHeader> clHeaders_ = new HashMap<String, CommitLogHeader>();
-
-    private ExecutorService executor;
-
-
-    public static final class CommitLogContext
-    {
-        static CommitLogContext NULL = new CommitLogContext(null, -1L);
-        /* Commit Log associated with this operation */
-        public final String file;
-        /* Offset within the Commit Log where this row as added */
-        public final long position;
-
-        public CommitLogContext(String file, long position)
-        {
-            this.file = file;
-            this.position = position;
-        }
-
-        boolean isValidContext()
-        {
-            return (position != -1L);
-        }
-    }
-
-    public static class CommitLogFileComparator implements Comparator<String>
-    {
-        public int compare(String f, String f2)
-        {
-            return (int)(getCreationTime(f) - getCreationTime(f2));
-        }
-
-        public boolean equals(Object o)
-        {
-            if ( !(o instanceof CommitLogFileComparator) )
-                return false;
-            return true;
-        }
-    }
-
-    public static void setSegmentSize(int size)
-    {
-        SEGMENT_SIZE = size;
-    }
-
-    static int getSegmentCount()
-    {
-        return clHeaders_.size();
-    }
-
-    static long getCreationTime(String file)
-    {
-        String[] entries = FBUtilities.strip(file, "-.");
-        return Long.parseLong(entries[entries.length - 2]);
-    }
-
-    private static AbstractWriter createWriter(String file) throws IOException
-    {        
-        return SequenceFile.writer(file);
-    }
-
-    static CommitLog open() throws IOException
-    {
-        if ( instance_ == null )
-        {
-            CommitLog.lock_.lock();
-            try
-            {
-
-                if ( instance_ == null )
-                {
-                    instance_ = new CommitLog(false);
-                }
-            }
-            finally
-            {
-                CommitLog.lock_.unlock();
-            }
-        }
-        return instance_;
-    }
-
-    /* Current commit log file */
-    private String logFile_;
-    /* header for current commit log */
-    private CommitLogHeader clHeader_;
-    private AbstractWriter logWriter_;
-
-    /*
-     * Generates a file name of the format CommitLog-<table>-<timestamp>.log in the
-     * directory specified by the Database Descriptor.
-    */
-    private void setNextFileName()
-    {
-        logFile_ = DatabaseDescriptor.getLogFileLocation() + File.separator +
-                   "CommitLog-" + System.currentTimeMillis() + ".log";
-    }
-
-    /*
-     * param @ table - name of table for which we are maintaining
-     *                 this commit log.
-     * param @ recoverymode - is commit log being instantiated in
-     *                        in recovery mode.
-    */
-    CommitLog(boolean recoveryMode) throws IOException
-    {
-        if ( !recoveryMode )
-        {
-            executor = new CommitLogExecutorService();
-            setNextFileName();            
-            logWriter_ = CommitLog.createWriter(logFile_);
-            writeCommitLogHeader();
-        }
-    }
-
-    /*
-     * This ctor is currently used only for debugging. We
-     * are now using it to modify the header so that recovery
-     * can be tested in as many scenarios as we could imagine.
-     *
-     * param @ logFile - logfile which we wish to modify.
-    */
-    CommitLog(File logFile) throws IOException
-    {
-        logFile_ = logFile.getAbsolutePath();
-        logWriter_ = CommitLog.createWriter(logFile_);
-    }
-
-    String getLogFile()
-    {
-        return logFile_;
-    }
-    
-    private CommitLogHeader readCommitLogHeader(IFileReader logReader) throws IOException
-    {
-        int size = (int)logReader.readLong();
-        byte[] bytes = new byte[size];
-        logReader.readDirect(bytes);
-        ByteArrayInputStream byteStream = new ByteArrayInputStream(bytes);
-        return CommitLogHeader.serializer().deserialize(new DataInputStream(byteStream));
-    }
-
-    /*
-     * Write the serialized commit log header into the specified commit log.
-    */
-    private static void writeCommitLogHeader(String commitLogFileName, byte[] bytes) throws IOException
-    {
-        AbstractWriter logWriter = CommitLog.createWriter(commitLogFileName);
-        writeCommitLogHeader(logWriter, bytes);
-        logWriter.close();
-    }
-
-    /*
-     * This is invoked on startup via the ctor. It basically
-     * writes a header with all bits set to zero.
-    */
-    private void writeCommitLogHeader() throws IOException
-    {
-        int cfSize = Table.TableMetadata.getColumnFamilyCount();
-        clHeader_ = new CommitLogHeader(cfSize);
-        writeCommitLogHeader(logWriter_, clHeader_.toByteArray());
-    }
-
-    /** writes header at the beginning of the file, then seeks back to current position */
-    private void seekAndWriteCommitLogHeader(byte[] bytes) throws IOException
-    {
-        long currentPos = logWriter_.getCurrentPosition();
-        logWriter_.seek(0);
-
-        writeCommitLogHeader(logWriter_, bytes);
-
-        logWriter_.seek(currentPos);
-    }
-
-    private static void writeCommitLogHeader(AbstractWriter logWriter, byte[] bytes) throws IOException
-    {
-        logWriter.writeLong(bytes.length);
-        logWriter.writeDirect(bytes);
-    }
-
-    void recover(File[] clogs) throws IOException
-    {
-        DataInputBuffer bufIn = new DataInputBuffer();
-
-        for (File file : clogs)
-        {
-            IFileReader reader = SequenceFile.reader(file.getAbsolutePath());
-            CommitLogHeader clHeader = readCommitLogHeader(reader);
-            /* seek to the lowest position */
-            int lowPos = CommitLogHeader.getLowestPosition(clHeader);
-            /*
-             * If lowPos == 0 then we need to skip the processing of this
-             * file.
-            */
-            if (lowPos == 0)
-                break;
-            else
-                reader.seek(lowPos);
-
-            Set<Table> tablesRecovered = new HashSet<Table>();
-
-            /* read the logs populate RowMutation and apply */
-            while ( !reader.isEOF() )
-            {
-                byte[] bytes;
-                try
-                {
-                    bytes = new byte[(int)reader.readLong()];
-                    reader.readDirect(bytes);
-                }
-                catch (EOFException e)
-                {
-                    // last CL entry didn't get completely written.  that's ok.
-                    break;
-                }
-                bufIn.reset(bytes, bytes.length);
-
-                /* read the commit log entry */
-                Row row = Row.serializer().deserialize(bufIn);
-                Table table = Table.open(row.getTable());
-                tablesRecovered.add(table);
-                Collection<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>(row.getColumnFamilies());
-                /* remove column families that have already been flushed */
-                for (ColumnFamily columnFamily : columnFamilies)
-                {
-                    /* TODO: Remove this to not process Hints */
-                    if ( !DatabaseDescriptor.isApplicationColumnFamily(columnFamily.name()) )
-                    {
-                        row.removeColumnFamily(columnFamily);
-                        continue;
-                    }
-                    int id = table.getColumnFamilyId(columnFamily.name());
-                    if ( !clHeader.isDirty(id) || reader.getCurrentPosition() < clHeader.getPosition(id) )
-                        row.removeColumnFamily(columnFamily);
-                }
-                if ( !row.isEmpty() )
-                {
-                    table.applyNow(row);
-                }
-            }
-            reader.close();
-            /* apply the rows read -- success will result in the CL file being discarded */
-            for (Table table : tablesRecovered)
-            {
-                table.flush(true);
-            }
-        }
-    }
-
-    /*
-     * Update the header of the commit log if a new column family
-     * is encountered for the first time.
-    */
-    private void maybeUpdateHeader(Row row) throws IOException
-    {
-        Table table = Table.open(row.getTable());
-        for (ColumnFamily columnFamily : row.getColumnFamilies())
-        {
-            int id = table.getColumnFamilyId(columnFamily.name());
-            if (!clHeader_.isDirty(id) || (clHeader_.isDirty(id) && clHeader_.getPosition(id) == 0))
-            {
-                clHeader_.turnOn(id, logWriter_.getCurrentPosition());
-                seekAndWriteCommitLogHeader(clHeader_.toByteArray());
-            }
-        }
-    }
-    
-    CommitLogContext getContext() throws IOException
-    {
-        Callable<CommitLogContext> task = new Callable<CommitLogContext>()
-        {
-            public CommitLogContext call() throws Exception
-            {
-                return new CommitLogContext(logFile_, logWriter_.getCurrentPosition());
-            }
-        };
-        try
-        {
-            return executor.submit(task).get();
-        }
-        catch (InterruptedException e)
-        {
-            throw new RuntimeException(e);
-        }
-        catch (ExecutionException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /*
-     * Adds the specified row to the commit log. This method will reset the
-     * file offset to what it is before the start of the operation in case
-     * of any problems. This way we can assume that the subsequent commit log
-     * entry will override the garbage left over by the previous write.
-    */
-    CommitLogContext add(final Row row) throws IOException
-    {
-        Callable<CommitLogContext> task = new LogRecordAdder(row);
-
-        try
-        {
-            return executor.submit(task).get();
-        }
-        catch (InterruptedException e)
-        {
-            throw new RuntimeException(e);
-        }
-        catch (ExecutionException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /*
-     * This is called on Memtable flush to add to the commit log
-     * a token indicating that this column family has been flushed.
-     * The bit flag associated with this column family is set in the
-     * header and this is used to decide if the log file can be deleted.
-    */
-    void onMemtableFlush(final String tableName, final String cf, final CommitLog.CommitLogContext cLogCtx) throws IOException
-    {
-        Callable task = new Callable()
-        {
-            public Object call() throws IOException
-            {
-                Table table = Table.open(tableName);
-                int id = table.getColumnFamilyId(cf);
-                discardCompletedSegments(cLogCtx, id);
-                return null;
-            }
-        };
-        try
-        {
-            executor.submit(task).get();
-        }
-        catch (InterruptedException e)
-        {
-            throw new RuntimeException(e);
-        }
-        catch (ExecutionException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /*
-     * Delete log segments whose contents have been turned into SSTables.
-     *
-     * param @ cLogCtx The commitLog context .
-     * param @ id id of the columnFamily being flushed to disk.
-     *
-    */
-    private void discardCompletedSegments(CommitLog.CommitLogContext cLogCtx, int id) throws IOException
-    {
-        /* retrieve the commit log header associated with the file in the context */
-        CommitLogHeader commitLogHeader = clHeaders_.get(cLogCtx.file);
-        if(commitLogHeader == null )
-        {
-            if( logFile_.equals(cLogCtx.file) )
-            {
-                /* this means we are dealing with the current commit log. */
-                commitLogHeader = clHeader_;
-                clHeaders_.put(cLogCtx.file, clHeader_);
-            }
-            else
-                return;
-        }
-
-        /*
-         * log replay assumes that we only have to look at entries past the last
-         * flush position, so verify that this flush happens after the last.
-         * (Currently Memtables are flushed on a single thread so this should be fine.)
-        */
-        assert cLogCtx.position >= commitLogHeader.getPosition(id);
-
-        commitLogHeader.turnOff(id);
-        /* Sort the commit logs based on creation time */
-        List<String> oldFiles = new ArrayList<String>(clHeaders_.keySet());
-        Collections.sort(oldFiles, new CommitLogFileComparator());
-        List<String> listOfDeletedFiles = new ArrayList<String>();
-        /*
-         * Loop through all the commit log files in the history. Now process
-         * all files that are older than the one in the context. For each of
-         * these files the header needs to modified by performing a bitwise &
-         * of the header with the header of the file in the context. If we
-         * encounter the file in the context in our list of old commit log files
-         * then we update the header and write it back to the commit log.
-        */
-        for(String oldFile : oldFiles)
-        {
-            if(oldFile.equals(cLogCtx.file))
-            {
-                /*
-                 * We need to turn on again. This is because we always keep
-                 * the bit turned on and the position indicates from where the
-                 * commit log needs to be read. When a flush occurs we turn off
-                 * perform & operation and then turn on with the new position.
-                */
-                commitLogHeader.turnOn(id, cLogCtx.position);
-                writeCommitLogHeader(cLogCtx.file, commitLogHeader.toByteArray());
-                break;
-            }
-            else
-            {
-                CommitLogHeader oldCommitLogHeader = clHeaders_.get(oldFile);
-                oldCommitLogHeader.and(commitLogHeader);
-                if(oldCommitLogHeader.isSafeToDelete())
-                {
-                	if (logger_.isDebugEnabled())
-                	  logger_.debug("Deleting commit log:"+ oldFile);
-                    FileUtils.deleteAsync(oldFile);
-                    listOfDeletedFiles.add(oldFile);
-                }
-                else
-                {
-                    writeCommitLogHeader(oldFile, oldCommitLogHeader.toByteArray());
-                }
-            }
-        }
-
-        for ( String deletedFile : listOfDeletedFiles)
-        {
-            clHeaders_.remove(deletedFile);
-        }
-    }
-
-    private boolean maybeRollLog() throws IOException
-    {
-        if (logWriter_.getFileSize() >= SEGMENT_SIZE)
-        {
-            /* Rolls the current log file over to a new one. */
-            setNextFileName();
-            String oldLogFile = logWriter_.getFileName();
-            logWriter_.close();
-
-            /* point reader/writer to a new commit log file. */
-            logWriter_ = CommitLog.createWriter(logFile_);
-            /* squirrel away the old commit log header */
-            clHeaders_.put(oldLogFile, new CommitLogHeader(clHeader_));
-            // we leave the old 'dirty' bits alone, so we can test for
-            // whether it's safe to remove a given log segment by and-ing its dirty
-            // with the current one.
-            clHeader_.zeroPositions();
-            writeCommitLogHeader(logWriter_, clHeader_.toByteArray());
-            return true;
-        }
-        return false;
-    }
-
-    void sync() throws IOException
-    {
-        logWriter_.sync();
-    }
-
-    class LogRecordAdder implements Callable<CommitLog.CommitLogContext>
-    {
-        Row row;
-
-        LogRecordAdder(Row row)
-        {
-            this.row = row;
-        }
-
-        public CommitLog.CommitLogContext call() throws Exception
-        {
-            long currentPosition = -1L;
-            DataOutputBuffer cfBuffer = new DataOutputBuffer();
-            try
-            {
-                /* serialize the row */
-                Row.serializer().serialize(row, cfBuffer);
-                currentPosition = logWriter_.getCurrentPosition();
-                CommitLogContext cLogCtx = new CommitLogContext(logFile_, currentPosition);
-                /* Update the header */
-                maybeUpdateHeader(row);
-                logWriter_.writeLong(cfBuffer.getLength());
-                logWriter_.append(cfBuffer);
-                maybeRollLog();
-                return cLogCtx;
-            }
-            catch (IOException e)
-            {
-                if ( currentPosition != -1 )
-                    logWriter_.seek(currentPosition);
-                throw e;
-            }
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.*;
+import java.util.*;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.*;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.FileUtils;
+
+import org.apache.log4j.Logger;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+
+/*
+ * Commit Log tracks every write operation into the system. The aim
+ * of the commit log is to be able to successfully recover data that was
+ * not stored to disk via the Memtable. Every Commit Log maintains a
+ * header represented by the abstraction CommitLogHeader. The header
+ * contains a bit array and an array of longs and both the arrays are
+ * of size, #column families for the Table, the Commit Log represents.
+ *
+ * Whenever a ColumnFamily is written to, for the first time its bit flag
+ * is set to one in the CommitLogHeader. When it is flushed to disk by the
+ * Memtable its corresponding bit in the header is set to zero. This helps
+ * track which CommitLogs can be thrown away as a result of Memtable flushes.
+ * Additionally, when a ColumnFamily is flushed and written to disk, its
+ * entry in the array of longs is updated with the offset in the Commit Log
+ * file where it was written. This helps speed up recovery since we can seek
+ * to these offsets and start processing the commit log.
+ *
+ * Every Commit Log is rolled over everytime it reaches its threshold in size;
+ * the new log inherits the "dirty" bits from the old.
+ *
+ * Over time there could be a number of commit logs that would be generated.
+ * To allow cleaning up non-active commit logs, whenever we flush a column family and update its bit flag in
+ * the active CL, we take the dirty bit array and bitwise & it with the headers of the older logs.
+ * If the result is 0, then it is safe to remove the older file.  (Since the new CL
+ * inherited the old's dirty bitflags, getting a zero for any given bit in the anding
+ * means that either the CF was clean in the old CL or it has been flushed since the
+ * switch in the new.)
+ *
+ * The CommitLog class itself is "mostly a singleton."  open() always returns one
+ * instance, but log replay will bypass that.
+ */
+public class CommitLog
+{
+    private static volatile int SEGMENT_SIZE = 128*1024*1024; // roll after log gets this big
+    private static volatile CommitLog instance_;
+    private static Lock lock_ = new ReentrantLock();
+    private static Logger logger_ = Logger.getLogger(CommitLog.class);
+    private static Map<String, CommitLogHeader> clHeaders_ = new HashMap<String, CommitLogHeader>();
+
+    private ExecutorService executor;
+
+
+    public static final class CommitLogContext
+    {
+        static CommitLogContext NULL = new CommitLogContext(null, -1L);
+        /* Commit Log associated with this operation */
+        public final String file;
+        /* Offset within the Commit Log where this row as added */
+        public final long position;
+
+        public CommitLogContext(String file, long position)
+        {
+            this.file = file;
+            this.position = position;
+        }
+
+        boolean isValidContext()
+        {
+            return (position != -1L);
+        }
+    }
+
+    public static class CommitLogFileComparator implements Comparator<String>
+    {
+        public int compare(String f, String f2)
+        {
+            return (int)(getCreationTime(f) - getCreationTime(f2));
+        }
+
+        public boolean equals(Object o)
+        {
+            if ( !(o instanceof CommitLogFileComparator) )
+                return false;
+            return true;
+        }
+    }
+
+    public static void setSegmentSize(int size)
+    {
+        SEGMENT_SIZE = size;
+    }
+
+    static int getSegmentCount()
+    {
+        return clHeaders_.size();
+    }
+
+    static long getCreationTime(String file)
+    {
+        String[] entries = FBUtilities.strip(file, "-.");
+        return Long.parseLong(entries[entries.length - 2]);
+    }
+
+    private static AbstractWriter createWriter(String file) throws IOException
+    {        
+        return SequenceFile.writer(file);
+    }
+
+    static CommitLog open() throws IOException
+    {
+        if ( instance_ == null )
+        {
+            CommitLog.lock_.lock();
+            try
+            {
+
+                if ( instance_ == null )
+                {
+                    instance_ = new CommitLog(false);
+                }
+            }
+            finally
+            {
+                CommitLog.lock_.unlock();
+            }
+        }
+        return instance_;
+    }
+
+    /* Current commit log file */
+    private String logFile_;
+    /* header for current commit log */
+    private CommitLogHeader clHeader_;
+    private AbstractWriter logWriter_;
+
+    /*
+     * Generates a file name of the format CommitLog-<table>-<timestamp>.log in the
+     * directory specified by the Database Descriptor.
+    */
+    private void setNextFileName()
+    {
+        logFile_ = DatabaseDescriptor.getLogFileLocation() + File.separator +
+                   "CommitLog-" + System.currentTimeMillis() + ".log";
+    }
+
+    /*
+     * param @ table - name of table for which we are maintaining
+     *                 this commit log.
+     * param @ recoverymode - is commit log being instantiated in
+     *                        in recovery mode.
+    */
+    CommitLog(boolean recoveryMode) throws IOException
+    {
+        if ( !recoveryMode )
+        {
+            executor = new CommitLogExecutorService();
+            setNextFileName();            
+            logWriter_ = CommitLog.createWriter(logFile_);
+            writeCommitLogHeader();
+        }
+    }
+
+    /*
+     * This ctor is currently used only for debugging. We
+     * are now using it to modify the header so that recovery
+     * can be tested in as many scenarios as we could imagine.
+     *
+     * param @ logFile - logfile which we wish to modify.
+    */
+    CommitLog(File logFile) throws IOException
+    {
+        logFile_ = logFile.getAbsolutePath();
+        logWriter_ = CommitLog.createWriter(logFile_);
+    }
+
+    String getLogFile()
+    {
+        return logFile_;
+    }
+    
+    private CommitLogHeader readCommitLogHeader(IFileReader logReader) throws IOException
+    {
+        int size = (int)logReader.readLong();
+        byte[] bytes = new byte[size];
+        logReader.readDirect(bytes);
+        ByteArrayInputStream byteStream = new ByteArrayInputStream(bytes);
+        return CommitLogHeader.serializer().deserialize(new DataInputStream(byteStream));
+    }
+
+    /*
+     * Write the serialized commit log header into the specified commit log.
+    */
+    private static void writeCommitLogHeader(String commitLogFileName, byte[] bytes) throws IOException
+    {
+        AbstractWriter logWriter = CommitLog.createWriter(commitLogFileName);
+        writeCommitLogHeader(logWriter, bytes);
+        logWriter.close();
+    }
+
+    /*
+     * This is invoked on startup via the ctor. It basically
+     * writes a header with all bits set to zero.
+    */
+    private void writeCommitLogHeader() throws IOException
+    {
+        int cfSize = Table.TableMetadata.getColumnFamilyCount();
+        clHeader_ = new CommitLogHeader(cfSize);
+        writeCommitLogHeader(logWriter_, clHeader_.toByteArray());
+    }
+
+    /** writes header at the beginning of the file, then seeks back to current position */
+    private void seekAndWriteCommitLogHeader(byte[] bytes) throws IOException
+    {
+        long currentPos = logWriter_.getCurrentPosition();
+        logWriter_.seek(0);
+
+        writeCommitLogHeader(logWriter_, bytes);
+
+        logWriter_.seek(currentPos);
+    }
+
+    private static void writeCommitLogHeader(AbstractWriter logWriter, byte[] bytes) throws IOException
+    {
+        logWriter.writeLong(bytes.length);
+        logWriter.writeDirect(bytes);
+    }
+
+    void recover(File[] clogs) throws IOException
+    {
+        DataInputBuffer bufIn = new DataInputBuffer();
+
+        for (File file : clogs)
+        {
+            IFileReader reader = SequenceFile.reader(file.getAbsolutePath());
+            CommitLogHeader clHeader = readCommitLogHeader(reader);
+            /* seek to the lowest position */
+            int lowPos = CommitLogHeader.getLowestPosition(clHeader);
+            /*
+             * If lowPos == 0 then we need to skip the processing of this
+             * file.
+            */
+            if (lowPos == 0)
+                break;
+            else
+                reader.seek(lowPos);
+
+            Set<Table> tablesRecovered = new HashSet<Table>();
+
+            /* read the logs populate RowMutation and apply */
+            while ( !reader.isEOF() )
+            {
+                byte[] bytes;
+                try
+                {
+                    bytes = new byte[(int)reader.readLong()];
+                    reader.readDirect(bytes);
+                }
+                catch (EOFException e)
+                {
+                    // last CL entry didn't get completely written.  that's ok.
+                    break;
+                }
+                bufIn.reset(bytes, bytes.length);
+
+                /* read the commit log entry */
+                Row row = Row.serializer().deserialize(bufIn);
+                Table table = Table.open(row.getTable());
+                tablesRecovered.add(table);
+                Collection<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>(row.getColumnFamilies());
+                /* remove column families that have already been flushed */
+                for (ColumnFamily columnFamily : columnFamilies)
+                {
+                    /* TODO: Remove this to not process Hints */
+                    if ( !DatabaseDescriptor.isApplicationColumnFamily(columnFamily.name()) )
+                    {
+                        row.removeColumnFamily(columnFamily);
+                        continue;
+                    }
+                    int id = table.getColumnFamilyId(columnFamily.name());
+                    if ( !clHeader.isDirty(id) || reader.getCurrentPosition() < clHeader.getPosition(id) )
+                        row.removeColumnFamily(columnFamily);
+                }
+                if ( !row.isEmpty() )
+                {
+                    table.applyNow(row);
+                }
+            }
+            reader.close();
+            /* apply the rows read -- success will result in the CL file being discarded */
+            for (Table table : tablesRecovered)
+            {
+                table.flush(true);
+            }
+        }
+    }
+
+    /*
+     * Update the header of the commit log if a new column family
+     * is encountered for the first time.
+    */
+    private void maybeUpdateHeader(Row row) throws IOException
+    {
+        Table table = Table.open(row.getTable());
+        for (ColumnFamily columnFamily : row.getColumnFamilies())
+        {
+            int id = table.getColumnFamilyId(columnFamily.name());
+            if (!clHeader_.isDirty(id) || (clHeader_.isDirty(id) && clHeader_.getPosition(id) == 0))
+            {
+                clHeader_.turnOn(id, logWriter_.getCurrentPosition());
+                seekAndWriteCommitLogHeader(clHeader_.toByteArray());
+            }
+        }
+    }
+    
+    CommitLogContext getContext() throws IOException
+    {
+        Callable<CommitLogContext> task = new Callable<CommitLogContext>()
+        {
+            public CommitLogContext call() throws Exception
+            {
+                return new CommitLogContext(logFile_, logWriter_.getCurrentPosition());
+            }
+        };
+        try
+        {
+            return executor.submit(task).get();
+        }
+        catch (InterruptedException e)
+        {
+            throw new RuntimeException(e);
+        }
+        catch (ExecutionException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    /*
+     * Adds the specified row to the commit log. This method will reset the
+     * file offset to what it is before the start of the operation in case
+     * of any problems. This way we can assume that the subsequent commit log
+     * entry will override the garbage left over by the previous write.
+    */
+    CommitLogContext add(final Row row) throws IOException
+    {
+        Callable<CommitLogContext> task = new LogRecordAdder(row);
+
+        try
+        {
+            return executor.submit(task).get();
+        }
+        catch (InterruptedException e)
+        {
+            throw new RuntimeException(e);
+        }
+        catch (ExecutionException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    /*
+     * This is called on Memtable flush to add to the commit log
+     * a token indicating that this column family has been flushed.
+     * The bit flag associated with this column family is set in the
+     * header and this is used to decide if the log file can be deleted.
+    */
+    void onMemtableFlush(final String tableName, final String cf, final CommitLog.CommitLogContext cLogCtx) throws IOException
+    {
+        Callable task = new Callable()
+        {
+            public Object call() throws IOException
+            {
+                Table table = Table.open(tableName);
+                int id = table.getColumnFamilyId(cf);
+                discardCompletedSegments(cLogCtx, id);
+                return null;
+            }
+        };
+        try
+        {
+            executor.submit(task).get();
+        }
+        catch (InterruptedException e)
+        {
+            throw new RuntimeException(e);
+        }
+        catch (ExecutionException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    /*
+     * Delete log segments whose contents have been turned into SSTables.
+     *
+     * param @ cLogCtx The commitLog context .
+     * param @ id id of the columnFamily being flushed to disk.
+     *
+    */
+    private void discardCompletedSegments(CommitLog.CommitLogContext cLogCtx, int id) throws IOException
+    {
+        /* retrieve the commit log header associated with the file in the context */
+        CommitLogHeader commitLogHeader = clHeaders_.get(cLogCtx.file);
+        if(commitLogHeader == null )
+        {
+            if( logFile_.equals(cLogCtx.file) )
+            {
+                /* this means we are dealing with the current commit log. */
+                commitLogHeader = clHeader_;
+                clHeaders_.put(cLogCtx.file, clHeader_);
+            }
+            else
+                return;
+        }
+
+        /*
+         * log replay assumes that we only have to look at entries past the last
+         * flush position, so verify that this flush happens after the last.
+         * (Currently Memtables are flushed on a single thread so this should be fine.)
+        */
+        assert cLogCtx.position >= commitLogHeader.getPosition(id);
+
+        commitLogHeader.turnOff(id);
+        /* Sort the commit logs based on creation time */
+        List<String> oldFiles = new ArrayList<String>(clHeaders_.keySet());
+        Collections.sort(oldFiles, new CommitLogFileComparator());
+        List<String> listOfDeletedFiles = new ArrayList<String>();
+        /*
+         * Loop through all the commit log files in the history. Now process
+         * all files that are older than the one in the context. For each of
+         * these files the header needs to modified by performing a bitwise &
+         * of the header with the header of the file in the context. If we
+         * encounter the file in the context in our list of old commit log files
+         * then we update the header and write it back to the commit log.
+        */
+        for(String oldFile : oldFiles)
+        {
+            if(oldFile.equals(cLogCtx.file))
+            {
+                /*
+                 * We need to turn on again. This is because we always keep
+                 * the bit turned on and the position indicates from where the
+                 * commit log needs to be read. When a flush occurs we turn off
+                 * perform & operation and then turn on with the new position.
+                */
+                commitLogHeader.turnOn(id, cLogCtx.position);
+                writeCommitLogHeader(cLogCtx.file, commitLogHeader.toByteArray());
+                break;
+            }
+            else
+            {
+                CommitLogHeader oldCommitLogHeader = clHeaders_.get(oldFile);
+                oldCommitLogHeader.and(commitLogHeader);
+                if(oldCommitLogHeader.isSafeToDelete())
+                {
+                	if (logger_.isDebugEnabled())
+                	  logger_.debug("Deleting commit log:"+ oldFile);
+                    FileUtils.deleteAsync(oldFile);
+                    listOfDeletedFiles.add(oldFile);
+                }
+                else
+                {
+                    writeCommitLogHeader(oldFile, oldCommitLogHeader.toByteArray());
+                }
+            }
+        }
+
+        for ( String deletedFile : listOfDeletedFiles)
+        {
+            clHeaders_.remove(deletedFile);
+        }
+    }
+
+    private boolean maybeRollLog() throws IOException
+    {
+        if (logWriter_.getFileSize() >= SEGMENT_SIZE)
+        {
+            /* Rolls the current log file over to a new one. */
+            setNextFileName();
+            String oldLogFile = logWriter_.getFileName();
+            logWriter_.close();
+
+            /* point reader/writer to a new commit log file. */
+            logWriter_ = CommitLog.createWriter(logFile_);
+            /* squirrel away the old commit log header */
+            clHeaders_.put(oldLogFile, new CommitLogHeader(clHeader_));
+            // we leave the old 'dirty' bits alone, so we can test for
+            // whether it's safe to remove a given log segment by and-ing its dirty
+            // with the current one.
+            clHeader_.zeroPositions();
+            writeCommitLogHeader(logWriter_, clHeader_.toByteArray());
+            return true;
+        }
+        return false;
+    }
+
+    void sync() throws IOException
+    {
+        logWriter_.sync();
+    }
+
+    class LogRecordAdder implements Callable<CommitLog.CommitLogContext>
+    {
+        Row row;
+
+        LogRecordAdder(Row row)
+        {
+            this.row = row;
+        }
+
+        public CommitLog.CommitLogContext call() throws Exception
+        {
+            long currentPosition = -1L;
+            DataOutputBuffer cfBuffer = new DataOutputBuffer();
+            try
+            {
+                /* serialize the row */
+                Row.serializer().serialize(row, cfBuffer);
+                currentPosition = logWriter_.getCurrentPosition();
+                CommitLogContext cLogCtx = new CommitLogContext(logFile_, currentPosition);
+                /* Update the header */
+                maybeUpdateHeader(row);
+                logWriter_.writeLong(cfBuffer.getLength());
+                logWriter_.append(cfBuffer);
+                maybeRollLog();
+                return cLogCtx;
+            }
+            catch (IOException e)
+            {
+                if ( currentPosition != -1 )
+                    logWriter_.seek(currentPosition);
+                throw e;
+            }
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/CommitLogHeader.java b/src/java/org/apache/cassandra/db/CommitLogHeader.java
index 2b58e3dd7c..8caf3f5774 100644
--- a/src/java/org/apache/cassandra/db/CommitLogHeader.java
+++ b/src/java/org/apache/cassandra/db/CommitLogHeader.java
@@ -1,184 +1,184 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.*;
-import java.util.BitSet;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.utils.BitSetSerializer;
-import org.apache.cassandra.config.DatabaseDescriptor;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class CommitLogHeader
-{
-    private static CommitLogHeaderSerializer serializer = new CommitLogHeaderSerializer();
-
-    static CommitLogHeaderSerializer serializer()
-    {
-        return serializer;
-    }
-        
-    public static BitSet and(byte[] bytes1, byte[] bytes2) throws IOException
-    {
-        DataInputBuffer bufIn = new DataInputBuffer();
-        bufIn.reset(bytes1, 0, bytes1.length);
-        CommitLogHeader header1 = serializer.deserialize(bufIn);
-        bufIn.reset(bytes2, 0, bytes2.length);
-        CommitLogHeader header2 = serializer.deserialize(bufIn);
-        header1.and(header2);
-        return header1.dirty;
-    }
-
-    static int getLowestPosition(CommitLogHeader clHeader)
-    {
-        int minPosition = Integer.MAX_VALUE;
-        for ( int position : clHeader.lastFlushedAt)
-        {
-            if ( position < minPosition && position > 0)
-            {
-                minPosition = position;
-            }
-        }
-        
-        if(minPosition == Integer.MAX_VALUE)
-            minPosition = 0;
-        return minPosition;
-    }
-
-    private BitSet dirty; // columnfamilies with un-flushed data in this CommitLog
-    private int[] lastFlushedAt; // position at which each CF was last flushed
-    
-    CommitLogHeader(int size)
-    {
-        dirty = new BitSet(size);
-        lastFlushedAt = new int[size];
-    }
-    
-    /*
-     * This ctor is used while deserializing. This ctor
-     * also builds an index of position to column family
-     * Id.
-    */
-    CommitLogHeader(BitSet dirty, int[] lastFlushedAt)
-    {
-        this.dirty = dirty;
-        this.lastFlushedAt = lastFlushedAt;
-    }
-    
-    CommitLogHeader(CommitLogHeader clHeader)
-    {
-        dirty = (BitSet)clHeader.dirty.clone();
-        lastFlushedAt = new int[clHeader.lastFlushedAt.length];
-        System.arraycopy(clHeader.lastFlushedAt, 0, lastFlushedAt, 0, lastFlushedAt.length);
-    }
-    
-    boolean isDirty(int index)
-    {
-        return dirty.get(index);
-    } 
-    
-    int getPosition(int index)
-    {
-        return lastFlushedAt[index];
-    }
-    
-    void turnOn(int index, long position)
-    {
-        dirty.set(index);
-        lastFlushedAt[index] = (int) position;
-    }
-
-    void turnOff(int index)
-    {
-        dirty.set(index, false);
-        lastFlushedAt[index] = 0;
-    }
-
-    boolean isSafeToDelete() throws IOException
-    {
-        return dirty.isEmpty();
-    }
-
-    void zeroPositions()
-    {
-        int size = lastFlushedAt.length;
-        lastFlushedAt = new int[size];
-    }
-    
-    void and(CommitLogHeader commitLogHeader)
-    {
-        dirty.and(commitLogHeader.dirty);
-    }
-    
-    byte[] toByteArray() throws IOException
-    {
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream(bos);        
-        CommitLogHeader.serializer().serialize(this, dos);
-        return bos.toByteArray();
-    }
-    
-    public String toString()
-    {
-        StringBuilder sb = new StringBuilder("");        
-        for ( int i = 0; i < dirty.size(); ++i )
-        {
-            sb.append((dirty.get(i) ? 0 : 1));
-            sb.append(":");
-            sb.append(Table.TableMetadata.getColumnFamilyName(i));
-            sb.append(" ");
-        }        
-        sb.append(" | " );        
-        for ( int position : lastFlushedAt)
-        {
-            sb.append(position);
-            sb.append(" ");
-        }        
-        return sb.toString();
-    }
-
-    static class CommitLogHeaderSerializer implements ICompactSerializer<CommitLogHeader>
-    {
-        public void serialize(CommitLogHeader clHeader, DataOutputStream dos) throws IOException
-        {
-            BitSetSerializer.serialize(clHeader.dirty, dos);
-            dos.writeInt(clHeader.lastFlushedAt.length);
-            for (int position : clHeader.lastFlushedAt)
-            {
-                dos.writeInt(position);
-            }
-        }
-
-        public CommitLogHeader deserialize(DataInputStream dis) throws IOException
-        {
-            BitSet bitFlags = BitSetSerializer.deserialize(dis);
-            int[] position = new int[dis.readInt()];
-            for (int i = 0; i < position.length; ++i)
-            {
-                position[i] = dis.readInt();
-            }
-            return new CommitLogHeader(bitFlags, position);
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.*;
+import java.util.BitSet;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.utils.BitSetSerializer;
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class CommitLogHeader
+{
+    private static CommitLogHeaderSerializer serializer = new CommitLogHeaderSerializer();
+
+    static CommitLogHeaderSerializer serializer()
+    {
+        return serializer;
+    }
+        
+    public static BitSet and(byte[] bytes1, byte[] bytes2) throws IOException
+    {
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(bytes1, 0, bytes1.length);
+        CommitLogHeader header1 = serializer.deserialize(bufIn);
+        bufIn.reset(bytes2, 0, bytes2.length);
+        CommitLogHeader header2 = serializer.deserialize(bufIn);
+        header1.and(header2);
+        return header1.dirty;
+    }
+
+    static int getLowestPosition(CommitLogHeader clHeader)
+    {
+        int minPosition = Integer.MAX_VALUE;
+        for ( int position : clHeader.lastFlushedAt)
+        {
+            if ( position < minPosition && position > 0)
+            {
+                minPosition = position;
+            }
+        }
+        
+        if(minPosition == Integer.MAX_VALUE)
+            minPosition = 0;
+        return minPosition;
+    }
+
+    private BitSet dirty; // columnfamilies with un-flushed data in this CommitLog
+    private int[] lastFlushedAt; // position at which each CF was last flushed
+    
+    CommitLogHeader(int size)
+    {
+        dirty = new BitSet(size);
+        lastFlushedAt = new int[size];
+    }
+    
+    /*
+     * This ctor is used while deserializing. This ctor
+     * also builds an index of position to column family
+     * Id.
+    */
+    CommitLogHeader(BitSet dirty, int[] lastFlushedAt)
+    {
+        this.dirty = dirty;
+        this.lastFlushedAt = lastFlushedAt;
+    }
+    
+    CommitLogHeader(CommitLogHeader clHeader)
+    {
+        dirty = (BitSet)clHeader.dirty.clone();
+        lastFlushedAt = new int[clHeader.lastFlushedAt.length];
+        System.arraycopy(clHeader.lastFlushedAt, 0, lastFlushedAt, 0, lastFlushedAt.length);
+    }
+    
+    boolean isDirty(int index)
+    {
+        return dirty.get(index);
+    } 
+    
+    int getPosition(int index)
+    {
+        return lastFlushedAt[index];
+    }
+    
+    void turnOn(int index, long position)
+    {
+        dirty.set(index);
+        lastFlushedAt[index] = (int) position;
+    }
+
+    void turnOff(int index)
+    {
+        dirty.set(index, false);
+        lastFlushedAt[index] = 0;
+    }
+
+    boolean isSafeToDelete() throws IOException
+    {
+        return dirty.isEmpty();
+    }
+
+    void zeroPositions()
+    {
+        int size = lastFlushedAt.length;
+        lastFlushedAt = new int[size];
+    }
+    
+    void and(CommitLogHeader commitLogHeader)
+    {
+        dirty.and(commitLogHeader.dirty);
+    }
+    
+    byte[] toByteArray() throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);        
+        CommitLogHeader.serializer().serialize(this, dos);
+        return bos.toByteArray();
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder("");        
+        for ( int i = 0; i < dirty.size(); ++i )
+        {
+            sb.append((dirty.get(i) ? 0 : 1));
+            sb.append(":");
+            sb.append(Table.TableMetadata.getColumnFamilyName(i));
+            sb.append(" ");
+        }        
+        sb.append(" | " );        
+        for ( int position : lastFlushedAt)
+        {
+            sb.append(position);
+            sb.append(" ");
+        }        
+        return sb.toString();
+    }
+
+    static class CommitLogHeaderSerializer implements ICompactSerializer<CommitLogHeader>
+    {
+        public void serialize(CommitLogHeader clHeader, DataOutputStream dos) throws IOException
+        {
+            BitSetSerializer.serialize(clHeader.dirty, dos);
+            dos.writeInt(clHeader.lastFlushedAt.length);
+            for (int position : clHeader.lastFlushedAt)
+            {
+                dos.writeInt(position);
+            }
+        }
+
+        public CommitLogHeader deserialize(DataInputStream dis) throws IOException
+        {
+            BitSet bitFlags = BitSetSerializer.deserialize(dis);
+            int[] position = new int[dis.readInt()];
+            for (int i = 0; i < position.length; ++i)
+            {
+                position[i] = dis.readInt();
+            }
+            return new CommitLogHeader(bitFlags, position);
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/CompactSerializerInvocationHandler.java b/src/java/org/apache/cassandra/db/CompactSerializerInvocationHandler.java
index 845a3f395b..a84fcf5953 100644
--- a/src/java/org/apache/cassandra/db/CompactSerializerInvocationHandler.java
+++ b/src/java/org/apache/cassandra/db/CompactSerializerInvocationHandler.java
@@ -1,54 +1,54 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.lang.reflect.InvocationHandler;
-import java.lang.reflect.Method;
-
-import org.apache.cassandra.io.DataOutputBuffer;
-import org.apache.cassandra.io.ICompactSerializer;
-
-
-/*
- * This is the abstraction that pre-processes calls to implementations
- * of the ICompactSerializer serialize() via dynamic proxies.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class CompactSerializerInvocationHandler<T> implements InvocationHandler
-{
-    private ICompactSerializer<T> serializer_;
-
-    public CompactSerializerInvocationHandler(ICompactSerializer<T> serializer)
-    {
-        serializer_ = serializer;
-    }
-
-    /*
-     * This dynamic runtime proxy adds the indexes before the actual columns are serialized.
-    */
-    public Object invoke(Object proxy, Method m, Object[] args) throws Throwable
-    {
-        /* Do the pre-processing here. */
-    	ColumnFamily cf = (ColumnFamily)args[0];
-    	DataOutputBuffer bufOut = (DataOutputBuffer)args[1];
-    	ColumnIndexer.serialize(cf, bufOut);
-        return m.invoke(serializer_, args);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.lang.reflect.InvocationHandler;
+import java.lang.reflect.Method;
+
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.ICompactSerializer;
+
+
+/*
+ * This is the abstraction that pre-processes calls to implementations
+ * of the ICompactSerializer serialize() via dynamic proxies.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class CompactSerializerInvocationHandler<T> implements InvocationHandler
+{
+    private ICompactSerializer<T> serializer_;
+
+    public CompactSerializerInvocationHandler(ICompactSerializer<T> serializer)
+    {
+        serializer_ = serializer;
+    }
+
+    /*
+     * This dynamic runtime proxy adds the indexes before the actual columns are serialized.
+    */
+    public Object invoke(Object proxy, Method m, Object[] args) throws Throwable
+    {
+        /* Do the pre-processing here. */
+    	ColumnFamily cf = (ColumnFamily)args[0];
+    	DataOutputBuffer bufOut = (DataOutputBuffer)args[1];
+    	ColumnIndexer.serialize(cf, bufOut);
+        return m.invoke(serializer_, args);
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/DBConstants.java b/src/java/org/apache/cassandra/db/DBConstants.java
index 40b9f977bf..2db146559e 100644
--- a/src/java/org/apache/cassandra/db/DBConstants.java
+++ b/src/java/org/apache/cassandra/db/DBConstants.java
@@ -16,12 +16,12 @@
 * specific language governing permissions and limitations
 * under the License.
 */
-package org.apache.cassandra.db;
-
-final class DBConstants
-{
-	public static final int boolSize_ = 1;
-	public static final int intSize_ = 4;
-	public static final int longSize_ = 8;
-	public static final int tsSize_ = 8;
-}
+package org.apache.cassandra.db;
+
+final class DBConstants
+{
+	public static final int boolSize_ = 1;
+	public static final int intSize_ = 4;
+	public static final int longSize_ = 8;
+	public static final int tsSize_ = 8;
+}
diff --git a/src/java/org/apache/cassandra/db/DataFileVerbHandler.java b/src/java/org/apache/cassandra/db/DataFileVerbHandler.java
index 9258d0cdc0..12c67b5c96 100644
--- a/src/java/org/apache/cassandra/db/DataFileVerbHandler.java
+++ b/src/java/org/apache/cassandra/db/DataFileVerbHandler.java
@@ -1,63 +1,63 @@
-/*
-* Licensed to the Apache Software Foundation (ASF) under one
-* or more contributor license agreements.  See the NOTICE file
-* distributed with this work for additional information
-* regarding copyright ownership.  The ASF licenses this file
-* to you under the Apache License, Version 2.0 (the
-* "License"); you may not use this file except in compliance
-* with the License.  You may obtain a copy of the License at
-*
-*    http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing,
-* software distributed under the License is distributed on an
-* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-* KIND, either express or implied.  See the License for the
-* specific language governing permissions and limitations
-* under the License.
-*/
-package org.apache.cassandra.db;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.io.SSTableReader;
-
-import org.apache.log4j.Logger;
-
-
-public class DataFileVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger( DataFileVerbHandler.class );
-    
-    public void doVerb(Message message)
-    {        
-        byte[] bytes = message.getMessageBody();
-        String table = new String(bytes);
-        logger_.info("**** Received a request from " + message.getFrom());
-        
-        try
-        {
-            List<SSTableReader> ssTables = Table.open(table).getAllSSTablesOnDisk();
-            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-            DataOutputStream dos = new DataOutputStream(bos);
-            dos.writeInt(ssTables.size());
-            for (SSTableReader sstable : ssTables)
-            {
-                dos.writeUTF(sstable.getFilename());
-            }
-            Message response = message.getReply( StorageService.getLocalStorageEndPoint(), bos.toByteArray());
-            MessagingService.getMessagingInstance().sendOneWay(response, message.getFrom());
-        }
-        catch (IOException ex)
-        {
-            logger_.error("Error listing data files", ex);
-        }
-    }
-}
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.io.SSTableReader;
+
+import org.apache.log4j.Logger;
+
+
+public class DataFileVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger( DataFileVerbHandler.class );
+    
+    public void doVerb(Message message)
+    {        
+        byte[] bytes = message.getMessageBody();
+        String table = new String(bytes);
+        logger_.info("**** Received a request from " + message.getFrom());
+        
+        try
+        {
+            List<SSTableReader> ssTables = Table.open(table).getAllSSTablesOnDisk();
+            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+            DataOutputStream dos = new DataOutputStream(bos);
+            dos.writeInt(ssTables.size());
+            for (SSTableReader sstable : ssTables)
+            {
+                dos.writeUTF(sstable.getFilename());
+            }
+            Message response = message.getReply( StorageService.getLocalStorageEndPoint(), bos.toByteArray());
+            MessagingService.getMessagingInstance().sendOneWay(response, message.getFrom());
+        }
+        catch (IOException ex)
+        {
+            logger_.error("Error listing data files", ex);
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/FileNameComparator.java b/src/java/org/apache/cassandra/db/FileNameComparator.java
index 60beec347a..5afc95c36f 100644
--- a/src/java/org/apache/cassandra/db/FileNameComparator.java
+++ b/src/java/org/apache/cassandra/db/FileNameComparator.java
@@ -16,27 +16,27 @@
 * specific language governing permissions and limitations
 * under the License.
 */
-package org.apache.cassandra.db;
-
-import java.util.Comparator;
-
-class FileNameComparator implements Comparator<String>
-{
-	private int order_ = 1 ;
-	
-	public static final int Ascending = 0 ;
-	public static final int Descending = 1 ;
-	
-	FileNameComparator( int order )
-	{
-		order_ = order;
-	}
-	
-    public int compare(String f, String f2)
-    {
-    	if( order_ == 1 )
-    		return ColumnFamilyStore.getIndexFromFileName(f2) - ColumnFamilyStore.getIndexFromFileName(f);
-    	else
-    		return ColumnFamilyStore.getIndexFromFileName(f) - ColumnFamilyStore.getIndexFromFileName(f2);
-    }
+package org.apache.cassandra.db;
+
+import java.util.Comparator;
+
+class FileNameComparator implements Comparator<String>
+{
+	private int order_ = 1 ;
+	
+	public static final int Ascending = 0 ;
+	public static final int Descending = 1 ;
+	
+	FileNameComparator( int order )
+	{
+		order_ = order;
+	}
+	
+    public int compare(String f, String f2)
+    {
+    	if( order_ == 1 )
+    		return ColumnFamilyStore.getIndexFromFileName(f2) - ColumnFamilyStore.getIndexFromFileName(f);
+    	else
+    		return ColumnFamilyStore.getIndexFromFileName(f) - ColumnFamilyStore.getIndexFromFileName(f2);
+    }
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/db/FileStructComparator.java b/src/java/org/apache/cassandra/db/FileStructComparator.java
index a726b065a2..e81a992fad 100644
--- a/src/java/org/apache/cassandra/db/FileStructComparator.java
+++ b/src/java/org/apache/cassandra/db/FileStructComparator.java
@@ -1,31 +1,31 @@
-/*
-* Licensed to the Apache Software Foundation (ASF) under one
-* or more contributor license agreements.  See the NOTICE file
-* distributed with this work for additional information
-* regarding copyright ownership.  The ASF licenses this file
-* to you under the Apache License, Version 2.0 (the
-* "License"); you may not use this file except in compliance
-* with the License.  You may obtain a copy of the License at
-*
-*    http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing,
-* software distributed under the License is distributed on an
-* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-* KIND, either express or implied.  See the License for the
-* specific language governing permissions and limitations
-* under the License.
-*/
-package org.apache.cassandra.db;
-
-import java.util.Comparator;
-
-import org.apache.cassandra.io.FileStruct;
-
-class FileStructComparator implements Comparator<FileStruct>
-{
-    public int compare(FileStruct f, FileStruct f2)
-    {
-        return f.getFileName().compareTo(f2.getFileName());
-    }
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.util.Comparator;
+
+import org.apache.cassandra.io.FileStruct;
+
+class FileStructComparator implements Comparator<FileStruct>
+{
+    public int compare(FileStruct f, FileStruct f2)
+    {
+        return f.getFileName().compareTo(f2.getFileName());
+    }
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/db/HintedHandOffManager.java b/src/java/org/apache/cassandra/db/HintedHandOffManager.java
index 8921641a83..7898fe891f 100644
--- a/src/java/org/apache/cassandra/db/HintedHandOffManager.java
+++ b/src/java/org/apache/cassandra/db/HintedHandOffManager.java
@@ -1,280 +1,280 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.util.Collection;
-import java.util.Set;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.TimeoutException;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-import java.io.IOException;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
-import org.apache.cassandra.concurrent.ThreadFactoryImpl;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.gms.FailureDetector;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.*;
-import org.apache.cassandra.db.filter.IdentityQueryFilter;
-import org.apache.cassandra.db.filter.QueryPath;
-
-
-/**
- * There are two ways hinted data gets delivered to the intended nodes.
- *
- * runHints() runs periodically and pushes the hinted data on this node to
- * every intended node.
- *
- * runDelieverHints() is called when some other node starts up (potentially
- * from a failure) and delivers the hinted data just to that node.
- */
-public class HintedHandOffManager
-{
-    private static HintedHandOffManager instance_;
-    private static Lock lock_ = new ReentrantLock();
-    private static Logger logger_ = Logger.getLogger(HintedHandOffManager.class);
-    final static long intervalInMins_ = 60;
-    private ScheduledExecutorService executor_ = new DebuggableScheduledThreadPoolExecutor(1, new ThreadFactoryImpl("HINTED-HANDOFF-POOL"));
-    public static final String HINTS_CF = "HintsColumnFamily";
-
-
-    public static HintedHandOffManager instance()
-    {
-        if ( instance_ == null )
-        {
-            lock_.lock();
-            try
-            {
-                if ( instance_ == null )
-                    instance_ = new HintedHandOffManager();
-            }
-            finally
-            {
-                lock_.unlock();
-            }
-        }
-        return instance_;
-    }
-
-    private static boolean sendMessage(String endpointAddress, String tableName, String key) throws DigestMismatchException, TimeoutException, IOException, InvalidRequestException
-    {
-        EndPoint endPoint = new EndPoint(endpointAddress, DatabaseDescriptor.getStoragePort());
-        if (!FailureDetector.instance().isAlive(endPoint))
-        {
-            return false;
-        }
-
-        Table table = Table.open(tableName);
-        Row row = table.get(key);
-        Row purgedRow = new Row(tableName,key);
-        for (ColumnFamily cf : row.getColumnFamilies())
-        {
-            purgedRow.addColumnFamily(ColumnFamilyStore.removeDeleted(cf));
-        }
-        RowMutation rm = new RowMutation(tableName, purgedRow);
-        Message message = rm.makeRowMutationMessage();
-        QuorumResponseHandler<Boolean> quorumResponseHandler = new QuorumResponseHandler<Boolean>(1, new WriteResponseResolver());
-        MessagingService.getMessagingInstance().sendRR(message, new EndPoint[]{ endPoint }, quorumResponseHandler);
-
-        return quorumResponseHandler.get();
-    }
-
-    private static void deleteEndPoint(byte[] endpointAddress, String tableName, byte[] key, long timestamp) throws IOException
-    {
-        RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, tableName);
-        rm.delete(new QueryPath(HINTS_CF, key, endpointAddress), timestamp);
-        rm.apply();
-    }
-
-    private static void deleteHintedData(String tableName, String key) throws IOException
-    {
-        // delete the row from Application CFs: find the largest timestamp in any of
-        // the data columns, and delete the entire CF with that value for the tombstone.
-
-        // Note that we delete all data associated with the key: this may be more than
-        // we sent earlier in sendMessage, since HH is not serialized with writes.
-        // This is sub-optimal but okay, since HH is just an effort to make a recovering
-        // node more consistent than it would have been; we can rely on the other
-        // consistency mechanisms to finish the job in this corner case.
-        RowMutation rm = new RowMutation(tableName, key);
-        Table table = Table.open(tableName);
-        Row row = table.get(key); // not necessary to do removeDeleted here
-        Collection<ColumnFamily> cfs = row.getColumnFamilies();
-        for (ColumnFamily cf : cfs)
-        {
-            long maxTS = Long.MIN_VALUE;
-            if (!cf.isSuper())
-            {
-                for (IColumn col : cf.getSortedColumns())
-                    maxTS = Math.max(maxTS, col.timestamp());
-            }
-            else
-            {
-                for (IColumn col : cf.getSortedColumns())
-                {
-                    maxTS = Math.max(maxTS, col.timestamp());
-                    Collection<IColumn> subColumns = col.getSubColumns();
-                    for (IColumn subCol : subColumns)
-                        maxTS = Math.max(maxTS, subCol.timestamp());
-                }
-            }
-            rm.delete(new QueryPath(cf.name()), maxTS);
-        }
-        rm.apply();
-    }
-
-    /** hintStore must be the hints columnfamily from the system table */
-    private static void deliverAllHints(ColumnFamilyStore hintStore) throws DigestMismatchException, IOException, InvalidRequestException, TimeoutException
-    {
-        if (logger_.isDebugEnabled())
-          logger_.debug("Started deliverAllHints");
-
-        // 1. Scan through all the keys that we need to handoff
-        // 2. For each key read the list of recipients and send
-        // 3. Delete that recipient from the key if write was successful
-        // 4. If all writes were success for a given key we can even delete the key .
-        // 5. Now force a flush
-        // 6. Do major compaction to clean up all deletes etc.
-        // 7. I guess we are done
-        for (String tableName : DatabaseDescriptor.getTables())
-        {
-            ColumnFamily hintColumnFamily = ColumnFamilyStore.removeDeleted(hintStore.getColumnFamily(new IdentityQueryFilter(tableName, new QueryPath(HINTS_CF))), Integer.MAX_VALUE);
-            if (hintColumnFamily == null)
-            {
-                continue;
-            }
-            Collection<IColumn> keys = hintColumnFamily.getSortedColumns();
-
-            for (IColumn keyColumn : keys)
-            {
-                Collection<IColumn> endpoints = keyColumn.getSubColumns();
-                String keyStr = new String(keyColumn.name(), "UTF-8");
-                int deleted = 0;
-                for (IColumn endpoint : endpoints)
-                {
-                    String endpointStr = new String(endpoint.name(), "UTF-8");
-                    if (sendMessage(endpointStr, tableName, keyStr))
-                    {
-                        deleteEndPoint(endpoint.name(), tableName, keyColumn.name(), keyColumn.timestamp());
-                        deleted++;
-                    }
-                }
-                if (deleted == endpoints.size())
-                {
-                    deleteHintedData(tableName, keyStr);
-                }
-            }
-        }
-        hintStore.forceFlush();
-        hintStore.forceCompaction(null, null, 0, null);
-
-        if (logger_.isDebugEnabled())
-          logger_.debug("Finished deliverAllHints");
-    }
-
-    private static void deliverHintsToEndpoint(EndPoint endPoint) throws IOException, DigestMismatchException, InvalidRequestException, TimeoutException
-    {
-        if (logger_.isDebugEnabled())
-          logger_.debug("Started hinted handoff for endPoint " + endPoint.getHost());
-
-        String targetEPBytes = endPoint.getHost();
-        // 1. Scan through all the keys that we need to handoff
-        // 2. For each key read the list of recipients if the endpoint matches send
-        // 3. Delete that recipient from the key if write was successful
-        Table systemTable = Table.open(Table.SYSTEM_TABLE);
-        for (String tableName : DatabaseDescriptor.getTables())
-        {
-            ColumnFamily hintedColumnFamily = systemTable.get(tableName, HINTS_CF);
-            if (hintedColumnFamily == null)
-            {
-                continue;
-            }
-            Collection<IColumn> keys = hintedColumnFamily.getSortedColumns();
-
-            for (IColumn keyColumn : keys)
-            {
-                String keyStr = new String(keyColumn.name(), "UTF-8");
-                Collection<IColumn> endpoints = keyColumn.getSubColumns();
-                for (IColumn hintEndPoint : endpoints)
-                {
-                    if (hintEndPoint.name().equals(targetEPBytes) && sendMessage(endPoint.getHost(), null, keyStr))
-                    {
-                        deleteEndPoint(hintEndPoint.name(), tableName, keyColumn.name(), keyColumn.timestamp());
-                        if (endpoints.size() == 1)
-                        {
-                            deleteHintedData(tableName, keyStr);
-                        }
-                    }
-                }
-            }
-        }
-
-        if (logger_.isDebugEnabled())
-          logger_.debug("Finished hinted handoff for endpoint " + endPoint.getHost());
-    }
-
-    public void submit(final ColumnFamilyStore columnFamilyStore)
-    {
-        Runnable r = new Runnable()
-        {
-            public void run()
-            {
-                try
-                {
-                    deliverAllHints(columnFamilyStore);
-                }
-                catch (Exception e)
-                {
-                    throw new RuntimeException(e);
-                }
-            }
-        };
-    	executor_.scheduleWithFixedDelay(r, HintedHandOffManager.intervalInMins_, HintedHandOffManager.intervalInMins_, TimeUnit.MINUTES);
-    }
-
-    /*
-     * This method is used to deliver hints to a particular endpoint.
-     * When we learn that some endpoint is back up we deliver the data
-     * to him via an event driven mechanism.
-    */
-    public void deliverHints(final EndPoint to)
-    {
-        Runnable r = new Runnable()
-        {
-            public void run()
-            {
-                try
-                {
-                    deliverHintsToEndpoint(to);
-                }
-                catch (Exception e)
-                {
-                    throw new RuntimeException(e);
-                }
-            }
-        };
-    	executor_.submit(r);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.Collection;
+import java.util.Set;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.io.IOException;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.*;
+import org.apache.cassandra.db.filter.IdentityQueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+
+
+/**
+ * There are two ways hinted data gets delivered to the intended nodes.
+ *
+ * runHints() runs periodically and pushes the hinted data on this node to
+ * every intended node.
+ *
+ * runDelieverHints() is called when some other node starts up (potentially
+ * from a failure) and delivers the hinted data just to that node.
+ */
+public class HintedHandOffManager
+{
+    private static HintedHandOffManager instance_;
+    private static Lock lock_ = new ReentrantLock();
+    private static Logger logger_ = Logger.getLogger(HintedHandOffManager.class);
+    final static long intervalInMins_ = 60;
+    private ScheduledExecutorService executor_ = new DebuggableScheduledThreadPoolExecutor(1, new ThreadFactoryImpl("HINTED-HANDOFF-POOL"));
+    public static final String HINTS_CF = "HintsColumnFamily";
+
+
+    public static HintedHandOffManager instance()
+    {
+        if ( instance_ == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                    instance_ = new HintedHandOffManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return instance_;
+    }
+
+    private static boolean sendMessage(String endpointAddress, String tableName, String key) throws DigestMismatchException, TimeoutException, IOException, InvalidRequestException
+    {
+        EndPoint endPoint = new EndPoint(endpointAddress, DatabaseDescriptor.getStoragePort());
+        if (!FailureDetector.instance().isAlive(endPoint))
+        {
+            return false;
+        }
+
+        Table table = Table.open(tableName);
+        Row row = table.get(key);
+        Row purgedRow = new Row(tableName,key);
+        for (ColumnFamily cf : row.getColumnFamilies())
+        {
+            purgedRow.addColumnFamily(ColumnFamilyStore.removeDeleted(cf));
+        }
+        RowMutation rm = new RowMutation(tableName, purgedRow);
+        Message message = rm.makeRowMutationMessage();
+        QuorumResponseHandler<Boolean> quorumResponseHandler = new QuorumResponseHandler<Boolean>(1, new WriteResponseResolver());
+        MessagingService.getMessagingInstance().sendRR(message, new EndPoint[]{ endPoint }, quorumResponseHandler);
+
+        return quorumResponseHandler.get();
+    }
+
+    private static void deleteEndPoint(byte[] endpointAddress, String tableName, byte[] key, long timestamp) throws IOException
+    {
+        RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, tableName);
+        rm.delete(new QueryPath(HINTS_CF, key, endpointAddress), timestamp);
+        rm.apply();
+    }
+
+    private static void deleteHintedData(String tableName, String key) throws IOException
+    {
+        // delete the row from Application CFs: find the largest timestamp in any of
+        // the data columns, and delete the entire CF with that value for the tombstone.
+
+        // Note that we delete all data associated with the key: this may be more than
+        // we sent earlier in sendMessage, since HH is not serialized with writes.
+        // This is sub-optimal but okay, since HH is just an effort to make a recovering
+        // node more consistent than it would have been; we can rely on the other
+        // consistency mechanisms to finish the job in this corner case.
+        RowMutation rm = new RowMutation(tableName, key);
+        Table table = Table.open(tableName);
+        Row row = table.get(key); // not necessary to do removeDeleted here
+        Collection<ColumnFamily> cfs = row.getColumnFamilies();
+        for (ColumnFamily cf : cfs)
+        {
+            long maxTS = Long.MIN_VALUE;
+            if (!cf.isSuper())
+            {
+                for (IColumn col : cf.getSortedColumns())
+                    maxTS = Math.max(maxTS, col.timestamp());
+            }
+            else
+            {
+                for (IColumn col : cf.getSortedColumns())
+                {
+                    maxTS = Math.max(maxTS, col.timestamp());
+                    Collection<IColumn> subColumns = col.getSubColumns();
+                    for (IColumn subCol : subColumns)
+                        maxTS = Math.max(maxTS, subCol.timestamp());
+                }
+            }
+            rm.delete(new QueryPath(cf.name()), maxTS);
+        }
+        rm.apply();
+    }
+
+    /** hintStore must be the hints columnfamily from the system table */
+    private static void deliverAllHints(ColumnFamilyStore hintStore) throws DigestMismatchException, IOException, InvalidRequestException, TimeoutException
+    {
+        if (logger_.isDebugEnabled())
+          logger_.debug("Started deliverAllHints");
+
+        // 1. Scan through all the keys that we need to handoff
+        // 2. For each key read the list of recipients and send
+        // 3. Delete that recipient from the key if write was successful
+        // 4. If all writes were success for a given key we can even delete the key .
+        // 5. Now force a flush
+        // 6. Do major compaction to clean up all deletes etc.
+        // 7. I guess we are done
+        for (String tableName : DatabaseDescriptor.getTables())
+        {
+            ColumnFamily hintColumnFamily = ColumnFamilyStore.removeDeleted(hintStore.getColumnFamily(new IdentityQueryFilter(tableName, new QueryPath(HINTS_CF))), Integer.MAX_VALUE);
+            if (hintColumnFamily == null)
+            {
+                continue;
+            }
+            Collection<IColumn> keys = hintColumnFamily.getSortedColumns();
+
+            for (IColumn keyColumn : keys)
+            {
+                Collection<IColumn> endpoints = keyColumn.getSubColumns();
+                String keyStr = new String(keyColumn.name(), "UTF-8");
+                int deleted = 0;
+                for (IColumn endpoint : endpoints)
+                {
+                    String endpointStr = new String(endpoint.name(), "UTF-8");
+                    if (sendMessage(endpointStr, tableName, keyStr))
+                    {
+                        deleteEndPoint(endpoint.name(), tableName, keyColumn.name(), keyColumn.timestamp());
+                        deleted++;
+                    }
+                }
+                if (deleted == endpoints.size())
+                {
+                    deleteHintedData(tableName, keyStr);
+                }
+            }
+        }
+        hintStore.forceFlush();
+        hintStore.forceCompaction(null, null, 0, null);
+
+        if (logger_.isDebugEnabled())
+          logger_.debug("Finished deliverAllHints");
+    }
+
+    private static void deliverHintsToEndpoint(EndPoint endPoint) throws IOException, DigestMismatchException, InvalidRequestException, TimeoutException
+    {
+        if (logger_.isDebugEnabled())
+          logger_.debug("Started hinted handoff for endPoint " + endPoint.getHost());
+
+        String targetEPBytes = endPoint.getHost();
+        // 1. Scan through all the keys that we need to handoff
+        // 2. For each key read the list of recipients if the endpoint matches send
+        // 3. Delete that recipient from the key if write was successful
+        Table systemTable = Table.open(Table.SYSTEM_TABLE);
+        for (String tableName : DatabaseDescriptor.getTables())
+        {
+            ColumnFamily hintedColumnFamily = systemTable.get(tableName, HINTS_CF);
+            if (hintedColumnFamily == null)
+            {
+                continue;
+            }
+            Collection<IColumn> keys = hintedColumnFamily.getSortedColumns();
+
+            for (IColumn keyColumn : keys)
+            {
+                String keyStr = new String(keyColumn.name(), "UTF-8");
+                Collection<IColumn> endpoints = keyColumn.getSubColumns();
+                for (IColumn hintEndPoint : endpoints)
+                {
+                    if (hintEndPoint.name().equals(targetEPBytes) && sendMessage(endPoint.getHost(), null, keyStr))
+                    {
+                        deleteEndPoint(hintEndPoint.name(), tableName, keyColumn.name(), keyColumn.timestamp());
+                        if (endpoints.size() == 1)
+                        {
+                            deleteHintedData(tableName, keyStr);
+                        }
+                    }
+                }
+            }
+        }
+
+        if (logger_.isDebugEnabled())
+          logger_.debug("Finished hinted handoff for endpoint " + endPoint.getHost());
+    }
+
+    public void submit(final ColumnFamilyStore columnFamilyStore)
+    {
+        Runnable r = new Runnable()
+        {
+            public void run()
+            {
+                try
+                {
+                    deliverAllHints(columnFamilyStore);
+                }
+                catch (Exception e)
+                {
+                    throw new RuntimeException(e);
+                }
+            }
+        };
+    	executor_.scheduleWithFixedDelay(r, HintedHandOffManager.intervalInMins_, HintedHandOffManager.intervalInMins_, TimeUnit.MINUTES);
+    }
+
+    /*
+     * This method is used to deliver hints to a particular endpoint.
+     * When we learn that some endpoint is back up we deliver the data
+     * to him via an event driven mechanism.
+    */
+    public void deliverHints(final EndPoint to)
+    {
+        Runnable r = new Runnable()
+        {
+            public void run()
+            {
+                try
+                {
+                    deliverHintsToEndpoint(to);
+                }
+                catch (Exception e)
+                {
+                    throw new RuntimeException(e);
+                }
+            }
+        };
+    	executor_.submit(r);
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/IColumn.java b/src/java/org/apache/cassandra/db/IColumn.java
index adef39d7ce..bf04cf9934 100644
--- a/src/java/org/apache/cassandra/db/IColumn.java
+++ b/src/java/org/apache/cassandra/db/IColumn.java
@@ -1,49 +1,49 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.util.Collection;
-
-import org.apache.cassandra.db.marshal.AbstractType;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IColumn
-{
-    public static short UtfPrefix_ = 2;
-    public boolean isMarkedForDelete();
-    public long getMarkedForDeleteAt();
-    public byte[] name();
-    public int size();
-    public int serializedSize();
-    public long timestamp();
-    public long timestamp(byte[] columnName);
-    public byte[] value();
-    public byte[] value(byte[] columnName);
-    public Collection<IColumn> getSubColumns();
-    public IColumn getSubColumn(byte[] columnName);
-    public void addColumn(IColumn column);
-    public IColumn diff(IColumn column);
-    public int getObjectCount();
-    public byte[] digest();
-    public int getLocalDeletionTime(); // for tombstone GC, so int is sufficient granularity
-    public String getString(AbstractType comparator);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.Collection;
+
+import org.apache.cassandra.db.marshal.AbstractType;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IColumn
+{
+    public static short UtfPrefix_ = 2;
+    public boolean isMarkedForDelete();
+    public long getMarkedForDeleteAt();
+    public byte[] name();
+    public int size();
+    public int serializedSize();
+    public long timestamp();
+    public long timestamp(byte[] columnName);
+    public byte[] value();
+    public byte[] value(byte[] columnName);
+    public Collection<IColumn> getSubColumns();
+    public IColumn getSubColumn(byte[] columnName);
+    public void addColumn(IColumn column);
+    public IColumn diff(IColumn column);
+    public int getObjectCount();
+    public byte[] digest();
+    public int getLocalDeletionTime(); // for tombstone GC, so int is sufficient granularity
+    public String getString(AbstractType comparator);
+}
diff --git a/src/java/org/apache/cassandra/db/IScanner.java b/src/java/org/apache/cassandra/db/IScanner.java
index 0d5a34182a..7582947cbf 100644
--- a/src/java/org/apache/cassandra/db/IScanner.java
+++ b/src/java/org/apache/cassandra/db/IScanner.java
@@ -16,14 +16,14 @@
 * specific language governing permissions and limitations
 * under the License.
 */
-package org.apache.cassandra.db;
-
-import java.io.Closeable;
-import java.io.IOException;
-
-public interface IScanner<T> extends Closeable
-{
-    public boolean hasNext() throws IOException;
-    public T next() throws IOException;
-    public void fetch(String key, String cf) throws IOException;    
-}
+package org.apache.cassandra.db;
+
+import java.io.Closeable;
+import java.io.IOException;
+
+public interface IScanner<T> extends Closeable
+{
+    public boolean hasNext() throws IOException;
+    public T next() throws IOException;
+    public void fetch(String key, String cf) throws IOException;    
+}
diff --git a/src/java/org/apache/cassandra/db/LoadVerbHandler.java b/src/java/org/apache/cassandra/db/LoadVerbHandler.java
index d585b5465c..682a92c917 100644
--- a/src/java/org/apache/cassandra/db/LoadVerbHandler.java
+++ b/src/java/org/apache/cassandra/db/LoadVerbHandler.java
@@ -1,71 +1,71 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class LoadVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger(LoadVerbHandler.class);    
-    
-    public void doVerb(Message message)
-    { 
-        try
-        {
-	        byte[] body = message.getMessageBody();
-            DataInputBuffer buffer = new DataInputBuffer();
-            buffer.reset(body, body.length);
-	        RowMutationMessage rmMsg = RowMutationMessage.serializer().deserialize(buffer);
-
-            EndPoint[] endpoints = StorageService.instance().getNStorageEndPoint(rmMsg.getRowMutation().key());
-
-			Message messageInternal = new Message(StorageService.getLocalStorageEndPoint(), 
-	                StorageService.mutationStage_,
-					StorageService.mutationVerbHandler_, 
-	                body
-	        );
-            
-            StringBuilder sb = new StringBuilder();
-			for(EndPoint endPoint : endpoints)
-			{                
-                sb.append(endPoint);
-				MessagingService.getMessagingInstance().sendOneWay(messageInternal, endPoint);
-			}
-            if (logger_.isDebugEnabled())
-                logger_.debug("Sent data to " + sb.toString());
-        }        
-        catch ( Exception e )
-        {
-            if (logger_.isDebugEnabled())
-                logger_.debug(LogUtil.throwableToString(e));            
-        }        
-    }
-
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class LoadVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(LoadVerbHandler.class);    
+    
+    public void doVerb(Message message)
+    { 
+        try
+        {
+	        byte[] body = message.getMessageBody();
+            DataInputBuffer buffer = new DataInputBuffer();
+            buffer.reset(body, body.length);
+	        RowMutationMessage rmMsg = RowMutationMessage.serializer().deserialize(buffer);
+
+            EndPoint[] endpoints = StorageService.instance().getNStorageEndPoint(rmMsg.getRowMutation().key());
+
+			Message messageInternal = new Message(StorageService.getLocalStorageEndPoint(), 
+	                StorageService.mutationStage_,
+					StorageService.mutationVerbHandler_, 
+	                body
+	        );
+            
+            StringBuilder sb = new StringBuilder();
+			for(EndPoint endPoint : endpoints)
+			{                
+                sb.append(endPoint);
+				MessagingService.getMessagingInstance().sendOneWay(messageInternal, endPoint);
+			}
+            if (logger_.isDebugEnabled())
+                logger_.debug("Sent data to " + sb.toString());
+        }        
+        catch ( Exception e )
+        {
+            if (logger_.isDebugEnabled())
+                logger_.debug(LogUtil.throwableToString(e));            
+        }        
+    }
+
+}
diff --git a/src/java/org/apache/cassandra/db/Memtable.java b/src/java/org/apache/cassandra/db/Memtable.java
index d599adf08e..5148bd79a0 100644
--- a/src/java/org/apache/cassandra/db/Memtable.java
+++ b/src/java/org/apache/cassandra/db/Memtable.java
@@ -1,342 +1,342 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.IOException;
-import java.util.*;
-import java.util.concurrent.ExecutionException;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.commons.lang.ArrayUtils;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.io.DataOutputBuffer;
-import org.apache.cassandra.io.SSTableReader;
-import org.apache.cassandra.io.SSTableWriter;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.DestructivePQIterator;
-import org.apache.cassandra.db.filter.*;
-import org.apache.cassandra.db.marshal.AbstractType;
-
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class Memtable implements Comparable<Memtable>
-{
-	private static Logger logger_ = Logger.getLogger( Memtable.class );
-
-    private boolean isFrozen_;
-    private volatile boolean isDirty_;
-    private volatile boolean isFlushed_; // for tests, in particular forceBlockingFlush asserts this
-
-    private int threshold_ = DatabaseDescriptor.getMemtableSize()*1024*1024;
-    private int thresholdCount_ = (int)(DatabaseDescriptor.getMemtableObjectCount()*1024*1024);
-    private AtomicInteger currentSize_ = new AtomicInteger(0);
-    private AtomicInteger currentObjectCount_ = new AtomicInteger(0);
-
-    /* Table and ColumnFamily name are used to determine the ColumnFamilyStore */
-    private String table_;
-    private String cfName_;
-    /* Creation time of this Memtable */
-    private long creationTime_;
-    private Map<String, ColumnFamily> columnFamilies_ = new HashMap<String, ColumnFamily>();
-    /* Lock and Condition for notifying new clients about Memtable switches */
-
-    Memtable(String table, String cfName)
-    {
-        table_ = table;
-        cfName_ = cfName;
-        creationTime_ = System.currentTimeMillis();
-    }
-
-    public boolean isFlushed()
-    {
-        return isFlushed_;
-    }
-
-    /**
-     * Compares two Memtable based on creation time.
-     * @param rhs Memtable to compare to.
-     * @return a negative integer, zero, or a positive integer as this object
-     * is less than, equal to, or greater than the specified object.
-     */
-    public int compareTo(Memtable rhs)
-    {
-    	long diff = creationTime_ - rhs.creationTime_;
-    	if ( diff > 0 )
-    		return 1;
-    	else if ( diff < 0 )
-    		return -1;
-    	else
-    		return 0;
-    }
-
-    public int getCurrentSize()
-    {
-        return currentSize_.get();
-    }
-    
-    public int getCurrentObjectCount()
-    {
-        return currentObjectCount_.get();
-    }
-
-    void resolveSize(int oldSize, int newSize)
-    {
-        currentSize_.addAndGet(newSize - oldSize);
-    }
-
-    void resolveCount(int oldCount, int newCount)
-    {
-        currentObjectCount_.addAndGet(newCount - oldCount);
-    }
-
-    boolean isThresholdViolated()
-    {
-        return currentSize_.get() >= threshold_ ||  currentObjectCount_.get() >= thresholdCount_;
-    }
-
-    String getColumnFamily()
-    {
-    	return cfName_;
-    }
-
-    boolean isFrozen()
-    {
-        return isFrozen_;
-    }
-
-    void freeze()
-    {
-        isFrozen_ = true;
-    }
-
-    /**
-     * Should only be called by ColumnFamilyStore.apply.  NOT a public API.
-     * (CFS handles locking to avoid submitting an op
-     *  to a flushing memtable.  Any other way is unsafe.)
-    */
-    void put(String key, ColumnFamily columnFamily)
-    {
-        assert !isFrozen_; // not 100% foolproof but hell, it's an assert
-        isDirty_ = true;
-        resolve(key, columnFamily);
-    }
-
-    /** flush synchronously (in the current thread, not on the executor).
-     *  only the recover code should call this. */
-    void flushOnRecovery() throws IOException {
-        if (!isClean())
-            flush(CommitLog.CommitLogContext.NULL);
-    }
-
-    private void resolve(String key, ColumnFamily columnFamily)
-    {
-    	ColumnFamily oldCf = columnFamilies_.get(key);
-        if ( oldCf != null )
-        {
-            int oldSize = oldCf.size();
-            int oldObjectCount = oldCf.getColumnCount();
-            oldCf.addColumns(columnFamily);
-            int newSize = oldCf.size();
-            int newObjectCount = oldCf.getColumnCount();
-            resolveSize(oldSize, newSize);
-            resolveCount(oldObjectCount, newObjectCount);
-            oldCf.delete(columnFamily);
-        }
-        else
-        {
-            columnFamilies_.put(key, columnFamily);
-            currentSize_.addAndGet(columnFamily.size() + key.length());
-            currentObjectCount_.addAndGet(columnFamily.getColumnCount());
-        }
-    }
-
-    // for debugging
-    public String contents()
-    {
-        StringBuilder builder = new StringBuilder();
-        builder.append("{");
-        for (Map.Entry<String, ColumnFamily> entry : columnFamilies_.entrySet())
-        {
-            builder.append(entry.getKey()).append(": ").append(entry.getValue()).append(", ");
-        }
-        builder.append("}");
-        return builder.toString();
-    }
-
-    void flush(CommitLog.CommitLogContext cLogCtx) throws IOException
-    {
-        logger_.info("Flushing " + this);
-        ColumnFamilyStore cfStore = Table.open(table_).getColumnFamilyStore(cfName_);
-
-        SSTableWriter writer = new SSTableWriter(cfStore.getTempSSTablePath(), columnFamilies_.size(), StorageService.getPartitioner());
-
-        // sort keys in the order they would be in when decorated
-        final IPartitioner partitioner = StorageService.getPartitioner();
-        final Comparator<String> dc = partitioner.getDecoratedKeyComparator();
-        ArrayList<String> orderedKeys = new ArrayList<String>(columnFamilies_.keySet());
-        Collections.sort(orderedKeys, new Comparator<String>()
-        {
-            public int compare(String o1, String o2)
-            {
-                return dc.compare(partitioner.decorateKey(o1), partitioner.decorateKey(o2));
-            }
-        });
-        DataOutputBuffer buffer = new DataOutputBuffer();
-        for (String key : orderedKeys)
-        {
-            buffer.reset();
-            ColumnFamily columnFamily = columnFamilies_.get(key);
-            if ( columnFamily != null )
-            {
-                /* serialize the cf with column indexes */
-                ColumnFamily.serializerWithIndexes().serialize( columnFamily, buffer );
-                /* Now write the key and value to disk */
-                writer.append(partitioner.decorateKey(key), buffer);
-            }
-        }
-        SSTableReader ssTable = writer.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_));
-        cfStore.onMemtableFlush(cLogCtx);
-        cfStore.storeLocation(ssTable);
-        buffer.close();
-        isFlushed_ = true;
-        logger_.info("Completed flushing " + this);
-    }
-
-    public String toString()
-    {
-        return "Memtable(" + cfName_ + ")@" + hashCode();
-    }
-
-    /**
-     * there does not appear to be any data structure that we can pass to PriorityQueue that will
-     * get it to heapify in-place instead of copying first, so we might as well return a Set.
-    */
-    Set<String> getKeys() throws ExecutionException, InterruptedException
-    {
-        return new HashSet<String>(columnFamilies_.keySet());
-    }
-
-    public static Iterator<String> getKeyIterator(Set<String> keys)
-    {
-        if (keys.size() == 0)
-        {
-            // cannot create a PQ of size zero (wtf?)
-            return Arrays.asList(new String[0]).iterator();
-        }
-        PriorityQueue<String> pq = new PriorityQueue<String>(keys.size(), StorageService.getPartitioner().getDecoratedKeyComparator());
-        pq.addAll(keys);
-        return new DestructivePQIterator<String>(pq);
-    }
-
-    public boolean isClean()
-    {
-        // executor taskcount is inadequate for our needs here -- it can return zero under certain
-        // race conditions even though a task has been processed.
-        return !isDirty_;
-    }
-
-    /**
-     * obtain an iterator of columns in this memtable in the specified order starting from a given column.
-     */
-    public ColumnIterator getSliceIterator(SliceQueryFilter filter, AbstractType typeComparator)
-    {
-        ColumnFamily cf = columnFamilies_.get(filter.key);
-        final ColumnFamily columnFamily = cf == null ? ColumnFamily.create(table_, filter.getColumnFamilyName()) : cf.cloneMeShallow();
-
-        final IColumn columns[] = (cf == null ? columnFamily : cf).getSortedColumns().toArray(new IColumn[columnFamily.getSortedColumns().size()]);
-        // TODO if we are dealing with supercolumns, we need to clone them while we have the read lock since they can be modified later
-        if (!filter.isAscending)
-            ArrayUtils.reverse(columns);
-        IColumn startIColumn;
-        if (DatabaseDescriptor.getColumnFamilyType(table_, filter.getColumnFamilyName()).equals("Standard"))
-            startIColumn = new Column(filter.start);
-        else
-            startIColumn = new SuperColumn(filter.start, null); // ok to not have subcolumnComparator since we won't be adding columns to this object
-
-        // can't use a ColumnComparatorFactory comparator since those compare on both name and time (and thus will fail to match
-        // our dummy column, since the time there is arbitrary).
-        Comparator<IColumn> comparator = filter.getColumnComparator(typeComparator);
-        int index = Arrays.binarySearch(columns, startIColumn, comparator);
-        final int startIndex = index < 0 ? -(index + 1) : index;
-
-        return new AbstractColumnIterator()
-        {
-            private int curIndex_ = startIndex;
-
-            public ColumnFamily getColumnFamily()
-            {
-                return columnFamily;
-            }
-
-            public boolean hasNext()
-            {
-                return curIndex_ < columns.length;
-            }
-
-            public IColumn next()
-            {
-                return columns[curIndex_++];
-            }
-        };
-    }
-
-    public ColumnIterator getNamesIterator(final NamesQueryFilter filter)
-    {
-        final ColumnFamily cf = columnFamilies_.get(filter.key);
-        final ColumnFamily columnFamily = cf == null ? ColumnFamily.create(table_, filter.getColumnFamilyName()) : cf.cloneMeShallow();
-
-        return new SimpleAbstractColumnIterator()
-        {
-            private Iterator<byte[]> iter = filter.columns.iterator();
-            private byte[] current;
-
-            public ColumnFamily getColumnFamily()
-            {
-                return columnFamily;
-            }
-
-            protected IColumn computeNext()
-            {
-                if (cf == null)
-                {
-                    return endOfData();
-                }
-                while (iter.hasNext())
-                {
-                    current = iter.next();
-                    IColumn column = cf.getColumn(current);
-                    if (column != null)
-                        return column;
-                }
-                return endOfData();
-            }
-        };
-    }
-    
-    void clearUnsafe()
-    {
-        columnFamilies_.clear();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.commons.lang.ArrayUtils;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.SSTableWriter;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.DestructivePQIterator;
+import org.apache.cassandra.db.filter.*;
+import org.apache.cassandra.db.marshal.AbstractType;
+
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Memtable implements Comparable<Memtable>
+{
+	private static Logger logger_ = Logger.getLogger( Memtable.class );
+
+    private boolean isFrozen_;
+    private volatile boolean isDirty_;
+    private volatile boolean isFlushed_; // for tests, in particular forceBlockingFlush asserts this
+
+    private int threshold_ = DatabaseDescriptor.getMemtableSize()*1024*1024;
+    private int thresholdCount_ = (int)(DatabaseDescriptor.getMemtableObjectCount()*1024*1024);
+    private AtomicInteger currentSize_ = new AtomicInteger(0);
+    private AtomicInteger currentObjectCount_ = new AtomicInteger(0);
+
+    /* Table and ColumnFamily name are used to determine the ColumnFamilyStore */
+    private String table_;
+    private String cfName_;
+    /* Creation time of this Memtable */
+    private long creationTime_;
+    private Map<String, ColumnFamily> columnFamilies_ = new HashMap<String, ColumnFamily>();
+    /* Lock and Condition for notifying new clients about Memtable switches */
+
+    Memtable(String table, String cfName)
+    {
+        table_ = table;
+        cfName_ = cfName;
+        creationTime_ = System.currentTimeMillis();
+    }
+
+    public boolean isFlushed()
+    {
+        return isFlushed_;
+    }
+
+    /**
+     * Compares two Memtable based on creation time.
+     * @param rhs Memtable to compare to.
+     * @return a negative integer, zero, or a positive integer as this object
+     * is less than, equal to, or greater than the specified object.
+     */
+    public int compareTo(Memtable rhs)
+    {
+    	long diff = creationTime_ - rhs.creationTime_;
+    	if ( diff > 0 )
+    		return 1;
+    	else if ( diff < 0 )
+    		return -1;
+    	else
+    		return 0;
+    }
+
+    public int getCurrentSize()
+    {
+        return currentSize_.get();
+    }
+    
+    public int getCurrentObjectCount()
+    {
+        return currentObjectCount_.get();
+    }
+
+    void resolveSize(int oldSize, int newSize)
+    {
+        currentSize_.addAndGet(newSize - oldSize);
+    }
+
+    void resolveCount(int oldCount, int newCount)
+    {
+        currentObjectCount_.addAndGet(newCount - oldCount);
+    }
+
+    boolean isThresholdViolated()
+    {
+        return currentSize_.get() >= threshold_ ||  currentObjectCount_.get() >= thresholdCount_;
+    }
+
+    String getColumnFamily()
+    {
+    	return cfName_;
+    }
+
+    boolean isFrozen()
+    {
+        return isFrozen_;
+    }
+
+    void freeze()
+    {
+        isFrozen_ = true;
+    }
+
+    /**
+     * Should only be called by ColumnFamilyStore.apply.  NOT a public API.
+     * (CFS handles locking to avoid submitting an op
+     *  to a flushing memtable.  Any other way is unsafe.)
+    */
+    void put(String key, ColumnFamily columnFamily)
+    {
+        assert !isFrozen_; // not 100% foolproof but hell, it's an assert
+        isDirty_ = true;
+        resolve(key, columnFamily);
+    }
+
+    /** flush synchronously (in the current thread, not on the executor).
+     *  only the recover code should call this. */
+    void flushOnRecovery() throws IOException {
+        if (!isClean())
+            flush(CommitLog.CommitLogContext.NULL);
+    }
+
+    private void resolve(String key, ColumnFamily columnFamily)
+    {
+    	ColumnFamily oldCf = columnFamilies_.get(key);
+        if ( oldCf != null )
+        {
+            int oldSize = oldCf.size();
+            int oldObjectCount = oldCf.getColumnCount();
+            oldCf.addColumns(columnFamily);
+            int newSize = oldCf.size();
+            int newObjectCount = oldCf.getColumnCount();
+            resolveSize(oldSize, newSize);
+            resolveCount(oldObjectCount, newObjectCount);
+            oldCf.delete(columnFamily);
+        }
+        else
+        {
+            columnFamilies_.put(key, columnFamily);
+            currentSize_.addAndGet(columnFamily.size() + key.length());
+            currentObjectCount_.addAndGet(columnFamily.getColumnCount());
+        }
+    }
+
+    // for debugging
+    public String contents()
+    {
+        StringBuilder builder = new StringBuilder();
+        builder.append("{");
+        for (Map.Entry<String, ColumnFamily> entry : columnFamilies_.entrySet())
+        {
+            builder.append(entry.getKey()).append(": ").append(entry.getValue()).append(", ");
+        }
+        builder.append("}");
+        return builder.toString();
+    }
+
+    void flush(CommitLog.CommitLogContext cLogCtx) throws IOException
+    {
+        logger_.info("Flushing " + this);
+        ColumnFamilyStore cfStore = Table.open(table_).getColumnFamilyStore(cfName_);
+
+        SSTableWriter writer = new SSTableWriter(cfStore.getTempSSTablePath(), columnFamilies_.size(), StorageService.getPartitioner());
+
+        // sort keys in the order they would be in when decorated
+        final IPartitioner partitioner = StorageService.getPartitioner();
+        final Comparator<String> dc = partitioner.getDecoratedKeyComparator();
+        ArrayList<String> orderedKeys = new ArrayList<String>(columnFamilies_.keySet());
+        Collections.sort(orderedKeys, new Comparator<String>()
+        {
+            public int compare(String o1, String o2)
+            {
+                return dc.compare(partitioner.decorateKey(o1), partitioner.decorateKey(o2));
+            }
+        });
+        DataOutputBuffer buffer = new DataOutputBuffer();
+        for (String key : orderedKeys)
+        {
+            buffer.reset();
+            ColumnFamily columnFamily = columnFamilies_.get(key);
+            if ( columnFamily != null )
+            {
+                /* serialize the cf with column indexes */
+                ColumnFamily.serializerWithIndexes().serialize( columnFamily, buffer );
+                /* Now write the key and value to disk */
+                writer.append(partitioner.decorateKey(key), buffer);
+            }
+        }
+        SSTableReader ssTable = writer.closeAndOpenReader(DatabaseDescriptor.getKeysCachedFraction(table_));
+        cfStore.onMemtableFlush(cLogCtx);
+        cfStore.storeLocation(ssTable);
+        buffer.close();
+        isFlushed_ = true;
+        logger_.info("Completed flushing " + this);
+    }
+
+    public String toString()
+    {
+        return "Memtable(" + cfName_ + ")@" + hashCode();
+    }
+
+    /**
+     * there does not appear to be any data structure that we can pass to PriorityQueue that will
+     * get it to heapify in-place instead of copying first, so we might as well return a Set.
+    */
+    Set<String> getKeys() throws ExecutionException, InterruptedException
+    {
+        return new HashSet<String>(columnFamilies_.keySet());
+    }
+
+    public static Iterator<String> getKeyIterator(Set<String> keys)
+    {
+        if (keys.size() == 0)
+        {
+            // cannot create a PQ of size zero (wtf?)
+            return Arrays.asList(new String[0]).iterator();
+        }
+        PriorityQueue<String> pq = new PriorityQueue<String>(keys.size(), StorageService.getPartitioner().getDecoratedKeyComparator());
+        pq.addAll(keys);
+        return new DestructivePQIterator<String>(pq);
+    }
+
+    public boolean isClean()
+    {
+        // executor taskcount is inadequate for our needs here -- it can return zero under certain
+        // race conditions even though a task has been processed.
+        return !isDirty_;
+    }
+
+    /**
+     * obtain an iterator of columns in this memtable in the specified order starting from a given column.
+     */
+    public ColumnIterator getSliceIterator(SliceQueryFilter filter, AbstractType typeComparator)
+    {
+        ColumnFamily cf = columnFamilies_.get(filter.key);
+        final ColumnFamily columnFamily = cf == null ? ColumnFamily.create(table_, filter.getColumnFamilyName()) : cf.cloneMeShallow();
+
+        final IColumn columns[] = (cf == null ? columnFamily : cf).getSortedColumns().toArray(new IColumn[columnFamily.getSortedColumns().size()]);
+        // TODO if we are dealing with supercolumns, we need to clone them while we have the read lock since they can be modified later
+        if (!filter.isAscending)
+            ArrayUtils.reverse(columns);
+        IColumn startIColumn;
+        if (DatabaseDescriptor.getColumnFamilyType(table_, filter.getColumnFamilyName()).equals("Standard"))
+            startIColumn = new Column(filter.start);
+        else
+            startIColumn = new SuperColumn(filter.start, null); // ok to not have subcolumnComparator since we won't be adding columns to this object
+
+        // can't use a ColumnComparatorFactory comparator since those compare on both name and time (and thus will fail to match
+        // our dummy column, since the time there is arbitrary).
+        Comparator<IColumn> comparator = filter.getColumnComparator(typeComparator);
+        int index = Arrays.binarySearch(columns, startIColumn, comparator);
+        final int startIndex = index < 0 ? -(index + 1) : index;
+
+        return new AbstractColumnIterator()
+        {
+            private int curIndex_ = startIndex;
+
+            public ColumnFamily getColumnFamily()
+            {
+                return columnFamily;
+            }
+
+            public boolean hasNext()
+            {
+                return curIndex_ < columns.length;
+            }
+
+            public IColumn next()
+            {
+                return columns[curIndex_++];
+            }
+        };
+    }
+
+    public ColumnIterator getNamesIterator(final NamesQueryFilter filter)
+    {
+        final ColumnFamily cf = columnFamilies_.get(filter.key);
+        final ColumnFamily columnFamily = cf == null ? ColumnFamily.create(table_, filter.getColumnFamilyName()) : cf.cloneMeShallow();
+
+        return new SimpleAbstractColumnIterator()
+        {
+            private Iterator<byte[]> iter = filter.columns.iterator();
+            private byte[] current;
+
+            public ColumnFamily getColumnFamily()
+            {
+                return columnFamily;
+            }
+
+            protected IColumn computeNext()
+            {
+                if (cf == null)
+                {
+                    return endOfData();
+                }
+                while (iter.hasNext())
+                {
+                    current = iter.next();
+                    IColumn column = cf.getColumn(current);
+                    if (column != null)
+                        return column;
+                }
+                return endOfData();
+            }
+        };
+    }
+    
+    void clearUnsafe()
+    {
+        columnFamilies_.clear();
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/MinorCompactionManager.java b/src/java/org/apache/cassandra/db/MinorCompactionManager.java
index 68a40f523b..f6ef4bb51e 100644
--- a/src/java/org/apache/cassandra/db/MinorCompactionManager.java
+++ b/src/java/org/apache/cassandra/db/MinorCompactionManager.java
@@ -1,191 +1,191 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.concurrent.Callable;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Future;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-
-import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
-import org.apache.cassandra.concurrent.ThreadFactoryImpl;
-import org.apache.cassandra.dht.Range;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.service.StorageService;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class MinorCompactionManager
-{
-    private static MinorCompactionManager instance_;
-    private static Lock lock_ = new ReentrantLock();
-    private static Logger logger_ = Logger.getLogger(MinorCompactionManager.class);
-    private static final long intervalInMins_ = 5;
-    static final int COMPACTION_THRESHOLD = 4; // compact this many sstables at a time
-
-    public static MinorCompactionManager instance()
-    {
-        if ( instance_ == null )
-        {
-            lock_.lock();
-            try
-            {
-                if ( instance_ == null )
-                    instance_ = new MinorCompactionManager();
-            }
-            finally
-            {
-                lock_.unlock();
-            }
-        }
-        return instance_;
-    }
-
-    class FileCompactor2 implements Callable<Boolean>
-    {
-        private ColumnFamilyStore columnFamilyStore_;
-        private List<Range> ranges_;
-        private EndPoint target_;
-        private List<String> fileList_;
-
-        FileCompactor2(ColumnFamilyStore columnFamilyStore, List<Range> ranges, EndPoint target,List<String> fileList)
-        {
-            columnFamilyStore_ = columnFamilyStore;
-            ranges_ = ranges;
-            target_ = target;
-            fileList_ = fileList;
-        }
-
-        public Boolean call()
-        {
-        	boolean result;
-            if (logger_.isDebugEnabled())
-              logger_.debug("Started  compaction ..."+columnFamilyStore_.columnFamily_);
-            try
-            {
-                result = columnFamilyStore_.doAntiCompaction(ranges_, target_,fileList_);
-            }
-            catch (IOException e)
-            {
-                throw new RuntimeException(e);
-            }
-            if (logger_.isDebugEnabled())
-              logger_.debug("Finished compaction ..."+columnFamilyStore_.columnFamily_);
-            return result;
-        }
-    }
-
-    class OnDemandCompactor implements Runnable
-    {
-        private ColumnFamilyStore columnFamilyStore_;
-        private long skip_ = 0L;
-
-        OnDemandCompactor(ColumnFamilyStore columnFamilyStore, long skip)
-        {
-            columnFamilyStore_ = columnFamilyStore;
-            skip_ = skip;
-        }
-
-        public void run()
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Started  Major compaction for " + columnFamilyStore_.columnFamily_);
-            try
-            {
-                columnFamilyStore_.doMajorCompaction(skip_);
-            }
-            catch (IOException e)
-            {
-                throw new RuntimeException(e);
-            }
-            if (logger_.isDebugEnabled())
-              logger_.debug("Finished Major compaction for " + columnFamilyStore_.columnFamily_);
-        }
-    }
-
-    class CleanupCompactor implements Runnable
-    {
-        private ColumnFamilyStore columnFamilyStore_;
-
-        CleanupCompactor(ColumnFamilyStore columnFamilyStore)
-        {
-        	columnFamilyStore_ = columnFamilyStore;
-        }
-
-        public void run()
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Started  compaction ..."+columnFamilyStore_.columnFamily_);
-            try
-            {
-                columnFamilyStore_.doCleanupCompaction();
-            }
-            catch (IOException e)
-            {
-                throw new RuntimeException(e);
-            }
-            if (logger_.isDebugEnabled())
-              logger_.debug("Finished compaction ..."+columnFamilyStore_.columnFamily_);
-        }
-    }
-    
-    
-    private ScheduledExecutorService compactor_ = new DebuggableScheduledThreadPoolExecutor(1, new ThreadFactoryImpl("MINOR-COMPACTION-POOL"));
-
-    public Future<Integer> submit(final ColumnFamilyStore columnFamilyStore)
-    {
-        return submit(columnFamilyStore, COMPACTION_THRESHOLD);
-    }
-
-    Future<Integer> submit(final ColumnFamilyStore columnFamilyStore, final int threshold)
-    {
-        Callable<Integer> callable = new Callable<Integer>()
-        {
-            public Integer call() throws IOException
-            {
-                return columnFamilyStore.doCompaction(threshold);
-            }
-        };
-        return compactor_.submit(callable);
-    }
-
-    public void submitCleanup(ColumnFamilyStore columnFamilyStore)
-    {
-        compactor_.submit(new CleanupCompactor(columnFamilyStore));
-    }
-
-    public Future<Boolean> submit(ColumnFamilyStore columnFamilyStore, List<Range> ranges, EndPoint target, List<String> fileList)
-    {
-        return compactor_.submit( new FileCompactor2(columnFamilyStore, ranges, target, fileList) );
-    }
-
-    public void  submitMajor(ColumnFamilyStore columnFamilyStore, long skip)
-    {
-        compactor_.submit( new OnDemandCompactor(columnFamilyStore, skip) );
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Future;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class MinorCompactionManager
+{
+    private static MinorCompactionManager instance_;
+    private static Lock lock_ = new ReentrantLock();
+    private static Logger logger_ = Logger.getLogger(MinorCompactionManager.class);
+    private static final long intervalInMins_ = 5;
+    static final int COMPACTION_THRESHOLD = 4; // compact this many sstables at a time
+
+    public static MinorCompactionManager instance()
+    {
+        if ( instance_ == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                    instance_ = new MinorCompactionManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return instance_;
+    }
+
+    class FileCompactor2 implements Callable<Boolean>
+    {
+        private ColumnFamilyStore columnFamilyStore_;
+        private List<Range> ranges_;
+        private EndPoint target_;
+        private List<String> fileList_;
+
+        FileCompactor2(ColumnFamilyStore columnFamilyStore, List<Range> ranges, EndPoint target,List<String> fileList)
+        {
+            columnFamilyStore_ = columnFamilyStore;
+            ranges_ = ranges;
+            target_ = target;
+            fileList_ = fileList;
+        }
+
+        public Boolean call()
+        {
+        	boolean result;
+            if (logger_.isDebugEnabled())
+              logger_.debug("Started  compaction ..."+columnFamilyStore_.columnFamily_);
+            try
+            {
+                result = columnFamilyStore_.doAntiCompaction(ranges_, target_,fileList_);
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+            if (logger_.isDebugEnabled())
+              logger_.debug("Finished compaction ..."+columnFamilyStore_.columnFamily_);
+            return result;
+        }
+    }
+
+    class OnDemandCompactor implements Runnable
+    {
+        private ColumnFamilyStore columnFamilyStore_;
+        private long skip_ = 0L;
+
+        OnDemandCompactor(ColumnFamilyStore columnFamilyStore, long skip)
+        {
+            columnFamilyStore_ = columnFamilyStore;
+            skip_ = skip;
+        }
+
+        public void run()
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Started  Major compaction for " + columnFamilyStore_.columnFamily_);
+            try
+            {
+                columnFamilyStore_.doMajorCompaction(skip_);
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+            if (logger_.isDebugEnabled())
+              logger_.debug("Finished Major compaction for " + columnFamilyStore_.columnFamily_);
+        }
+    }
+
+    class CleanupCompactor implements Runnable
+    {
+        private ColumnFamilyStore columnFamilyStore_;
+
+        CleanupCompactor(ColumnFamilyStore columnFamilyStore)
+        {
+        	columnFamilyStore_ = columnFamilyStore;
+        }
+
+        public void run()
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Started  compaction ..."+columnFamilyStore_.columnFamily_);
+            try
+            {
+                columnFamilyStore_.doCleanupCompaction();
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+            if (logger_.isDebugEnabled())
+              logger_.debug("Finished compaction ..."+columnFamilyStore_.columnFamily_);
+        }
+    }
+    
+    
+    private ScheduledExecutorService compactor_ = new DebuggableScheduledThreadPoolExecutor(1, new ThreadFactoryImpl("MINOR-COMPACTION-POOL"));
+
+    public Future<Integer> submit(final ColumnFamilyStore columnFamilyStore)
+    {
+        return submit(columnFamilyStore, COMPACTION_THRESHOLD);
+    }
+
+    Future<Integer> submit(final ColumnFamilyStore columnFamilyStore, final int threshold)
+    {
+        Callable<Integer> callable = new Callable<Integer>()
+        {
+            public Integer call() throws IOException
+            {
+                return columnFamilyStore.doCompaction(threshold);
+            }
+        };
+        return compactor_.submit(callable);
+    }
+
+    public void submitCleanup(ColumnFamilyStore columnFamilyStore)
+    {
+        compactor_.submit(new CleanupCompactor(columnFamilyStore));
+    }
+
+    public Future<Boolean> submit(ColumnFamilyStore columnFamilyStore, List<Range> ranges, EndPoint target, List<String> fileList)
+    {
+        return compactor_.submit( new FileCompactor2(columnFamilyStore, ranges, target, fileList) );
+    }
+
+    public void  submitMajor(ColumnFamilyStore columnFamilyStore, long skip)
+    {
+        compactor_.submit( new OnDemandCompactor(columnFamilyStore, skip) );
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/PeriodicFlushManager.java b/src/java/org/apache/cassandra/db/PeriodicFlushManager.java
index 733c88fc90..e633e24722 100644
--- a/src/java/org/apache/cassandra/db/PeriodicFlushManager.java
+++ b/src/java/org/apache/cassandra/db/PeriodicFlushManager.java
@@ -1,70 +1,70 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.util.concurrent.ScheduledExecutorService;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.concurrent.*;
-import org.apache.cassandra.service.StorageService;
-
-/**
- *  Background flusher that force-flushes a column family periodically.
- */
-class PeriodicFlushManager
-{
-    private static Logger logger_ = Logger.getLogger(PeriodicFlushManager.class);
-    private static PeriodicFlushManager instance_;
-    private static Lock lock_ = new ReentrantLock();
-    private ScheduledExecutorService flusher_ = new DebuggableScheduledThreadPoolExecutor(1, new ThreadFactoryImpl("PERIODIC-FLUSHER-POOL"));
-
-    public static PeriodicFlushManager instance()
-    {
-        if ( instance_ == null )
-        {
-            lock_.lock();
-            try
-            {
-                if ( instance_ == null )
-                    instance_ = new PeriodicFlushManager();
-            }
-            finally
-            {
-                lock_.unlock();
-            }
-        }
-        return instance_;
-    }
-
-    public void submitPeriodicFlusher(final ColumnFamilyStore columnFamilyStore, int flushPeriodInMinutes)
-    {        
-        Runnable runnable= new Runnable()
-        {
-            public void run()
-            {
-                columnFamilyStore.forceFlush();
-            }
-        };
-        flusher_.scheduleWithFixedDelay(runnable, flushPeriodInMinutes, flushPeriodInMinutes, TimeUnit.MINUTES);       
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.concurrent.ScheduledExecutorService;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.concurrent.*;
+import org.apache.cassandra.service.StorageService;
+
+/**
+ *  Background flusher that force-flushes a column family periodically.
+ */
+class PeriodicFlushManager
+{
+    private static Logger logger_ = Logger.getLogger(PeriodicFlushManager.class);
+    private static PeriodicFlushManager instance_;
+    private static Lock lock_ = new ReentrantLock();
+    private ScheduledExecutorService flusher_ = new DebuggableScheduledThreadPoolExecutor(1, new ThreadFactoryImpl("PERIODIC-FLUSHER-POOL"));
+
+    public static PeriodicFlushManager instance()
+    {
+        if ( instance_ == null )
+        {
+            lock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                    instance_ = new PeriodicFlushManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return instance_;
+    }
+
+    public void submitPeriodicFlusher(final ColumnFamilyStore columnFamilyStore, int flushPeriodInMinutes)
+    {        
+        Runnable runnable= new Runnable()
+        {
+            public void run()
+            {
+                columnFamilyStore.forceFlush();
+            }
+        };
+        flusher_.scheduleWithFixedDelay(runnable, flushPeriodInMinutes, flushPeriodInMinutes, TimeUnit.MINUTES);       
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/ReadCommand.java b/src/java/org/apache/cassandra/db/ReadCommand.java
index 07ad7ada9f..25c0e3e789 100644
--- a/src/java/org/apache/cassandra/db/ReadCommand.java
+++ b/src/java/org/apache/cassandra/db/ReadCommand.java
@@ -1,115 +1,115 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.config.DatabaseDescriptor;
-
-
-public abstract class ReadCommand
-{
-    public static final String DO_REPAIR = "READ-REPAIR";
-    public static final byte CMD_TYPE_GET_SLICE_BY_NAMES = 1;
-    public static final byte CMD_TYPE_GET_SLICE = 2;
-
-    public static final String EMPTY_CF = "";
-    
-    private static ReadCommandSerializer serializer = new ReadCommandSerializer();
-
-    public static ReadCommandSerializer serializer()
-    {
-        return serializer;
-    }
-
-    public Message makeReadMessage() throws IOException
-    {
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream(bos);
-        ReadCommand.serializer().serialize(this, dos);
-        return new Message(StorageService.getLocalStorageEndPoint(), StorageService.readStage_, StorageService.readVerbHandler_, bos.toByteArray());
-    }
-
-    public final String table;
-    public final String key;
-    private boolean isDigestQuery = false;    
-    protected final byte commandType;
-
-    protected ReadCommand(String table, String key, byte cmdType)
-    {
-        this.table = table;
-        this.key = key;
-        this.commandType = cmdType;
-    }
-    
-    public boolean isDigestQuery()
-    {
-        return isDigestQuery;
-    }
-
-    public void setDigestQuery(boolean isDigestQuery)
-    {
-        this.isDigestQuery = isDigestQuery;
-    }
-
-    public abstract String getColumnFamilyName();
-    
-    public abstract ReadCommand copy();
-
-    public abstract Row getRow(Table table) throws IOException;
-
-    protected AbstractType getComparator()
-    {
-        return DatabaseDescriptor.getComparator(table, getColumnFamilyName());
-    }
-}
-
-class ReadCommandSerializer implements ICompactSerializer<ReadCommand>
-{
-    private static final Map<Byte, ReadCommandSerializer> CMD_SERIALIZER_MAP = new HashMap<Byte, ReadCommandSerializer>(); 
-    static 
-    {
-        CMD_SERIALIZER_MAP.put(ReadCommand.CMD_TYPE_GET_SLICE_BY_NAMES, new SliceByNamesReadCommandSerializer());
-        CMD_SERIALIZER_MAP.put(ReadCommand.CMD_TYPE_GET_SLICE, new SliceFromReadCommandSerializer());
-    }
-
-
-    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
-    {
-        dos.writeByte(rm.commandType);
-        ReadCommandSerializer ser = CMD_SERIALIZER_MAP.get(rm.commandType);
-        ser.serialize(rm, dos);
-    }
-
-    public ReadCommand deserialize(DataInputStream dis) throws IOException
-    {
-        byte msgType = dis.readByte();
-        return CMD_SERIALIZER_MAP.get(msgType).deserialize(dis);
-    }
-        
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+
+public abstract class ReadCommand
+{
+    public static final String DO_REPAIR = "READ-REPAIR";
+    public static final byte CMD_TYPE_GET_SLICE_BY_NAMES = 1;
+    public static final byte CMD_TYPE_GET_SLICE = 2;
+
+    public static final String EMPTY_CF = "";
+    
+    private static ReadCommandSerializer serializer = new ReadCommandSerializer();
+
+    public static ReadCommandSerializer serializer()
+    {
+        return serializer;
+    }
+
+    public Message makeReadMessage() throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        ReadCommand.serializer().serialize(this, dos);
+        return new Message(StorageService.getLocalStorageEndPoint(), StorageService.readStage_, StorageService.readVerbHandler_, bos.toByteArray());
+    }
+
+    public final String table;
+    public final String key;
+    private boolean isDigestQuery = false;    
+    protected final byte commandType;
+
+    protected ReadCommand(String table, String key, byte cmdType)
+    {
+        this.table = table;
+        this.key = key;
+        this.commandType = cmdType;
+    }
+    
+    public boolean isDigestQuery()
+    {
+        return isDigestQuery;
+    }
+
+    public void setDigestQuery(boolean isDigestQuery)
+    {
+        this.isDigestQuery = isDigestQuery;
+    }
+
+    public abstract String getColumnFamilyName();
+    
+    public abstract ReadCommand copy();
+
+    public abstract Row getRow(Table table) throws IOException;
+
+    protected AbstractType getComparator()
+    {
+        return DatabaseDescriptor.getComparator(table, getColumnFamilyName());
+    }
+}
+
+class ReadCommandSerializer implements ICompactSerializer<ReadCommand>
+{
+    private static final Map<Byte, ReadCommandSerializer> CMD_SERIALIZER_MAP = new HashMap<Byte, ReadCommandSerializer>(); 
+    static 
+    {
+        CMD_SERIALIZER_MAP.put(ReadCommand.CMD_TYPE_GET_SLICE_BY_NAMES, new SliceByNamesReadCommandSerializer());
+        CMD_SERIALIZER_MAP.put(ReadCommand.CMD_TYPE_GET_SLICE, new SliceFromReadCommandSerializer());
+    }
+
+
+    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
+    {
+        dos.writeByte(rm.commandType);
+        ReadCommandSerializer ser = CMD_SERIALIZER_MAP.get(rm.commandType);
+        ser.serialize(rm, dos);
+    }
+
+    public ReadCommand deserialize(DataInputStream dis) throws IOException
+    {
+        byte msgType = dis.readByte();
+        return CMD_SERIALIZER_MAP.get(msgType).deserialize(dis);
+    }
+        
+}
diff --git a/src/java/org/apache/cassandra/db/ReadRepairVerbHandler.java b/src/java/org/apache/cassandra/db/ReadRepairVerbHandler.java
index 280bb7a799..66c4ee5fa7 100644
--- a/src/java/org/apache/cassandra/db/ReadRepairVerbHandler.java
+++ b/src/java/org/apache/cassandra/db/ReadRepairVerbHandler.java
@@ -1,59 +1,59 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.*;
-
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-import org.apache.cassandra.service.*;
-import org.apache.cassandra.utils.*;
-import org.apache.cassandra.concurrent.*;
-import org.apache.cassandra.net.*;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class ReadRepairVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger(ReadRepairVerbHandler.class);    
-    
-    public void doVerb(Message message)
-    {          
-        byte[] body = message.getMessageBody();
-        DataInputBuffer buffer = new DataInputBuffer();
-        buffer.reset(body, body.length);        
-        
-        try
-        {
-            RowMutationMessage rmMsg = RowMutationMessage.serializer().deserialize(buffer);
-            RowMutation rm = rmMsg.getRowMutation();
-            rm.apply();                                   
-        }
-        catch ( IOException e )
-        {
-            if (logger_.isDebugEnabled())
-                logger_.debug(LogUtil.throwableToString(e));            
-        }        
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.*;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.service.*;
+import org.apache.cassandra.utils.*;
+import org.apache.cassandra.concurrent.*;
+import org.apache.cassandra.net.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ReadRepairVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(ReadRepairVerbHandler.class);    
+    
+    public void doVerb(Message message)
+    {          
+        byte[] body = message.getMessageBody();
+        DataInputBuffer buffer = new DataInputBuffer();
+        buffer.reset(body, body.length);        
+        
+        try
+        {
+            RowMutationMessage rmMsg = RowMutationMessage.serializer().deserialize(buffer);
+            RowMutation rm = rmMsg.getRowMutation();
+            rm.apply();                                   
+        }
+        catch ( IOException e )
+        {
+            if (logger_.isDebugEnabled())
+                logger_.debug(LogUtil.throwableToString(e));            
+        }        
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/ReadResponse.java b/src/java/org/apache/cassandra/db/ReadResponse.java
index e9b301be75..f8899e5a19 100644
--- a/src/java/org/apache/cassandra/db/ReadResponse.java
+++ b/src/java/org/apache/cassandra/db/ReadResponse.java
@@ -1,137 +1,137 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.Serializable;
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.StorageService;
-import org.apache.commons.lang.ArrayUtils;
-
-
-/*
- * The read response message is sent by the server when reading data 
- * this encapsulates the tablename and the row that has been read.
- * The table name is needed so that we can use it to create repairs.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public class ReadResponse implements Serializable 
-{
-private static ICompactSerializer<ReadResponse> serializer_;
-
-    static
-    {
-        serializer_ = new ReadResponseSerializer();
-    }
-
-    public static ICompactSerializer<ReadResponse> serializer()
-    {
-        return serializer_;
-    }
-    
-	public static Message makeReadResponseMessage(ReadResponse readResponse) throws IOException
-    {
-    	ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream( bos );
-        ReadResponse.serializer().serialize(readResponse, dos);
-        Message message = new Message(StorageService.getLocalStorageEndPoint(), MessagingService.responseStage_, MessagingService.responseVerbHandler_, bos.toByteArray());         
-        return message;
-    }
-	
-	private Row row_;
-	private byte[] digest_ = ArrayUtils.EMPTY_BYTE_ARRAY;
-    private boolean isDigestQuery_ = false;
-
-	public ReadResponse(byte[] digest )
-    {
-        assert digest != null;
-		digest_= digest;
-	}
-
-	public ReadResponse(Row row)
-    {
-		row_ = row;
-	}
-
-	public Row row() 
-    {
-		return row_;
-    }
-        
-	public byte[] digest() 
-    {
-		return digest_;
-	}
-
-	public boolean isDigestQuery()
-    {
-    	return isDigestQuery_;
-    }
-    
-    public void setIsDigestQuery(boolean isDigestQuery)
-    {
-    	isDigestQuery_ = isDigestQuery;
-    }
-}
-
-class ReadResponseSerializer implements ICompactSerializer<ReadResponse>
-{
-	public void serialize(ReadResponse rm, DataOutputStream dos) throws IOException
-	{
-        dos.writeInt(rm.digest().length);
-        dos.write(rm.digest());
-        dos.writeBoolean(rm.isDigestQuery());
-        
-        if( !rm.isDigestQuery() && rm.row() != null )
-        {            
-            Row.serializer().serialize(rm.row(), dos);
-        }				
-	}
-	
-    public ReadResponse deserialize(DataInputStream dis) throws IOException
-    {
-        int digestSize = dis.readInt();
-        byte[] digest = new byte[digestSize];
-        dis.read(digest, 0 , digestSize);
-        boolean isDigest = dis.readBoolean();
-        
-        Row row = null;
-        if ( !isDigest )
-        {
-            row = Row.serializer().deserialize(dis);
-        }
-		
-		ReadResponse rmsg = null;
-    	if( isDigest  )
-        {
-    		rmsg =  new ReadResponse(digest);
-        }
-    	else
-        {
-    		rmsg =  new ReadResponse(row);
-        }
-        rmsg.setIsDigestQuery(isDigest);
-    	return rmsg;
-    } 
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.commons.lang.ArrayUtils;
+
+
+/*
+ * The read response message is sent by the server when reading data 
+ * this encapsulates the tablename and the row that has been read.
+ * The table name is needed so that we can use it to create repairs.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class ReadResponse implements Serializable 
+{
+private static ICompactSerializer<ReadResponse> serializer_;
+
+    static
+    {
+        serializer_ = new ReadResponseSerializer();
+    }
+
+    public static ICompactSerializer<ReadResponse> serializer()
+    {
+        return serializer_;
+    }
+    
+	public static Message makeReadResponseMessage(ReadResponse readResponse) throws IOException
+    {
+    	ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        ReadResponse.serializer().serialize(readResponse, dos);
+        Message message = new Message(StorageService.getLocalStorageEndPoint(), MessagingService.responseStage_, MessagingService.responseVerbHandler_, bos.toByteArray());         
+        return message;
+    }
+	
+	private Row row_;
+	private byte[] digest_ = ArrayUtils.EMPTY_BYTE_ARRAY;
+    private boolean isDigestQuery_ = false;
+
+	public ReadResponse(byte[] digest )
+    {
+        assert digest != null;
+		digest_= digest;
+	}
+
+	public ReadResponse(Row row)
+    {
+		row_ = row;
+	}
+
+	public Row row() 
+    {
+		return row_;
+    }
+        
+	public byte[] digest() 
+    {
+		return digest_;
+	}
+
+	public boolean isDigestQuery()
+    {
+    	return isDigestQuery_;
+    }
+    
+    public void setIsDigestQuery(boolean isDigestQuery)
+    {
+    	isDigestQuery_ = isDigestQuery;
+    }
+}
+
+class ReadResponseSerializer implements ICompactSerializer<ReadResponse>
+{
+	public void serialize(ReadResponse rm, DataOutputStream dos) throws IOException
+	{
+        dos.writeInt(rm.digest().length);
+        dos.write(rm.digest());
+        dos.writeBoolean(rm.isDigestQuery());
+        
+        if( !rm.isDigestQuery() && rm.row() != null )
+        {            
+            Row.serializer().serialize(rm.row(), dos);
+        }				
+	}
+	
+    public ReadResponse deserialize(DataInputStream dis) throws IOException
+    {
+        int digestSize = dis.readInt();
+        byte[] digest = new byte[digestSize];
+        dis.read(digest, 0 , digestSize);
+        boolean isDigest = dis.readBoolean();
+        
+        Row row = null;
+        if ( !isDigest )
+        {
+            row = Row.serializer().deserialize(dis);
+        }
+		
+		ReadResponse rmsg = null;
+    	if( isDigest  )
+        {
+    		rmsg =  new ReadResponse(digest);
+        }
+    	else
+        {
+    		rmsg =  new ReadResponse(row);
+        }
+        rmsg.setIsDigestQuery(isDigest);
+    	return rmsg;
+    } 
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/db/ReadVerbHandler.java b/src/java/org/apache/cassandra/db/ReadVerbHandler.java
index f08a13dc19..c4831e5a0e 100644
--- a/src/java/org/apache/cassandra/db/ReadVerbHandler.java
+++ b/src/java/org/apache/cassandra/db/ReadVerbHandler.java
@@ -1,123 +1,123 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.io.DataOutputBuffer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class ReadVerbHandler implements IVerbHandler
-{
-    protected static class ReadContext
-    {
-        protected DataInputBuffer bufIn_ = new DataInputBuffer();
-        protected DataOutputBuffer bufOut_ = new DataOutputBuffer();
-    }
-
-    private static Logger logger_ = Logger.getLogger( ReadVerbHandler.class );
-    /* We use this so that we can reuse the same row mutation context for the mutation. */
-    private static ThreadLocal<ReadVerbHandler.ReadContext> tls_ = new InheritableThreadLocal<ReadVerbHandler.ReadContext>();
-    
-    protected static ReadVerbHandler.ReadContext getCurrentReadContext()
-    {
-        return tls_.get();
-    }
-    
-    protected static void setCurrentReadContext(ReadVerbHandler.ReadContext readContext)
-    {
-        tls_.set(readContext);
-    }
-
-    public void doVerb(Message message)
-    {
-        byte[] body = message.getMessageBody();
-        /* Obtain a Read Context from TLS */
-        ReadContext readCtx = tls_.get();
-        if ( readCtx == null )
-        {
-            readCtx = new ReadContext();
-            tls_.set(readCtx);
-        }
-        readCtx.bufIn_.reset(body, body.length);
-
-        try
-        {
-            ReadCommand readCommand = ReadCommand.serializer().deserialize(readCtx.bufIn_);
-            Table table = Table.open(readCommand.table);
-            Row row = null;
-            row = readCommand.getRow(table);
-            ReadResponse readResponse = null;
-            if (readCommand.isDigestQuery())
-            {
-                readResponse = new ReadResponse(row.digest());
-            }
-            else
-            {
-                readResponse = new ReadResponse(row);
-            }
-            readResponse.setIsDigestQuery(readCommand.isDigestQuery());
-            /* serialize the ReadResponseMessage. */
-            readCtx.bufOut_.reset();
-
-            ReadResponse.serializer().serialize(readResponse, readCtx.bufOut_);
-
-            byte[] bytes = new byte[readCtx.bufOut_.getLength()];
-            System.arraycopy(readCtx.bufOut_.getData(), 0, bytes, 0, bytes.length);
-
-            Message response = message.getReply(StorageService.getLocalStorageEndPoint(), bytes);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Read key " + readCommand.key + "; sending response to " + message.getMessageId() + "@" + message.getFrom());
-            MessagingService.getMessagingInstance().sendOneWay(response, message.getFrom());
-
-            /* Do read repair if header of the message says so */
-            if (message.getHeader(ReadCommand.DO_REPAIR) != null)
-            {
-                doReadRepair(row, readCommand);
-            }
-        }
-        catch (IOException ex)
-        {
-            throw new RuntimeException(ex);
-        }
-    }
-    
-    private void doReadRepair(Row row, ReadCommand readCommand)
-    {
-        List<EndPoint> endpoints = StorageService.instance().getNLiveStorageEndPoint(readCommand.key);
-        /* Remove the local storage endpoint from the list. */ 
-        endpoints.remove( StorageService.getLocalStorageEndPoint() );
-            
-        if (endpoints.size() > 0 && DatabaseDescriptor.getConsistencyCheck())
-            StorageService.instance().doConsistencyCheck(row, endpoints, readCommand);
-    }     
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ReadVerbHandler implements IVerbHandler
+{
+    protected static class ReadContext
+    {
+        protected DataInputBuffer bufIn_ = new DataInputBuffer();
+        protected DataOutputBuffer bufOut_ = new DataOutputBuffer();
+    }
+
+    private static Logger logger_ = Logger.getLogger( ReadVerbHandler.class );
+    /* We use this so that we can reuse the same row mutation context for the mutation. */
+    private static ThreadLocal<ReadVerbHandler.ReadContext> tls_ = new InheritableThreadLocal<ReadVerbHandler.ReadContext>();
+    
+    protected static ReadVerbHandler.ReadContext getCurrentReadContext()
+    {
+        return tls_.get();
+    }
+    
+    protected static void setCurrentReadContext(ReadVerbHandler.ReadContext readContext)
+    {
+        tls_.set(readContext);
+    }
+
+    public void doVerb(Message message)
+    {
+        byte[] body = message.getMessageBody();
+        /* Obtain a Read Context from TLS */
+        ReadContext readCtx = tls_.get();
+        if ( readCtx == null )
+        {
+            readCtx = new ReadContext();
+            tls_.set(readCtx);
+        }
+        readCtx.bufIn_.reset(body, body.length);
+
+        try
+        {
+            ReadCommand readCommand = ReadCommand.serializer().deserialize(readCtx.bufIn_);
+            Table table = Table.open(readCommand.table);
+            Row row = null;
+            row = readCommand.getRow(table);
+            ReadResponse readResponse = null;
+            if (readCommand.isDigestQuery())
+            {
+                readResponse = new ReadResponse(row.digest());
+            }
+            else
+            {
+                readResponse = new ReadResponse(row);
+            }
+            readResponse.setIsDigestQuery(readCommand.isDigestQuery());
+            /* serialize the ReadResponseMessage. */
+            readCtx.bufOut_.reset();
+
+            ReadResponse.serializer().serialize(readResponse, readCtx.bufOut_);
+
+            byte[] bytes = new byte[readCtx.bufOut_.getLength()];
+            System.arraycopy(readCtx.bufOut_.getData(), 0, bytes, 0, bytes.length);
+
+            Message response = message.getReply(StorageService.getLocalStorageEndPoint(), bytes);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Read key " + readCommand.key + "; sending response to " + message.getMessageId() + "@" + message.getFrom());
+            MessagingService.getMessagingInstance().sendOneWay(response, message.getFrom());
+
+            /* Do read repair if header of the message says so */
+            if (message.getHeader(ReadCommand.DO_REPAIR) != null)
+            {
+                doReadRepair(row, readCommand);
+            }
+        }
+        catch (IOException ex)
+        {
+            throw new RuntimeException(ex);
+        }
+    }
+    
+    private void doReadRepair(Row row, ReadCommand readCommand)
+    {
+        List<EndPoint> endpoints = StorageService.instance().getNLiveStorageEndPoint(readCommand.key);
+        /* Remove the local storage endpoint from the list. */ 
+        endpoints.remove( StorageService.getLocalStorageEndPoint() );
+            
+        if (endpoints.size() > 0 && DatabaseDescriptor.getConsistencyCheck())
+            StorageService.instance().doConsistencyCheck(row, endpoints, readCommand);
+    }     
+}
diff --git a/src/java/org/apache/cassandra/db/RecoveryManager.java b/src/java/org/apache/cassandra/db/RecoveryManager.java
index 87257c9cc5..66548514bb 100644
--- a/src/java/org/apache/cassandra/db/RecoveryManager.java
+++ b/src/java/org/apache/cassandra/db/RecoveryManager.java
@@ -1,66 +1,66 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.util.*;
-import java.io.*;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.utils.FileUtils;
-import org.apache.log4j.Logger;
-import org.apache.commons.lang.StringUtils;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class RecoveryManager
-{
-    private static RecoveryManager instance_;
-    private static Logger logger_ = Logger.getLogger(RecoveryManager.class);
-
-    public synchronized static RecoveryManager instance() throws IOException
-    {
-        if (instance_ == null)
-        {
-            instance_ = new RecoveryManager();
-        }
-        return instance_;
-    }
-
-    public static File[] getListofCommitLogs()
-    {
-        String directory = DatabaseDescriptor.getLogFileLocation();
-        File file = new File(directory);
-        return file.listFiles();
-    }
-
-    public static void doRecovery() throws IOException
-    {
-        File[] files = getListofCommitLogs();
-        if (files.length == 0)
-            return;
-
-        Arrays.sort(files, new FileUtils.FileComparator());
-        logger_.info("Replaying " + StringUtils.join(files, ", "));
-        new CommitLog(true).recover(files);
-        FileUtils.delete(files);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.*;
+import java.io.*;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.log4j.Logger;
+import org.apache.commons.lang.StringUtils;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class RecoveryManager
+{
+    private static RecoveryManager instance_;
+    private static Logger logger_ = Logger.getLogger(RecoveryManager.class);
+
+    public synchronized static RecoveryManager instance() throws IOException
+    {
+        if (instance_ == null)
+        {
+            instance_ = new RecoveryManager();
+        }
+        return instance_;
+    }
+
+    public static File[] getListofCommitLogs()
+    {
+        String directory = DatabaseDescriptor.getLogFileLocation();
+        File file = new File(directory);
+        return file.listFiles();
+    }
+
+    public static void doRecovery() throws IOException
+    {
+        File[] files = getListofCommitLogs();
+        if (files.length == 0)
+            return;
+
+        Arrays.sort(files, new FileUtils.FileComparator());
+        logger_.info("Replaying " + StringUtils.join(files, ", "));
+        new CommitLog(true).recover(files);
+        FileUtils.delete(files);
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/Row.java b/src/java/org/apache/cassandra/db/Row.java
index d56dde88c4..ee32eba722 100644
--- a/src/java/org/apache/cassandra/db/Row.java
+++ b/src/java/org/apache/cassandra/db/Row.java
@@ -1,231 +1,231 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.Collection;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Set;
-import java.util.Arrays;
-
-import org.apache.commons.lang.ArrayUtils;
-import org.apache.commons.lang.StringUtils;
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.utils.FBUtilities;
-
-public class Row
-{
-    private static Logger logger_ = Logger.getLogger(Row.class);
-    private String table_;
-    private static RowSerializer serializer = new RowSerializer();
-
-    static RowSerializer serializer()
-    {
-        return serializer;
-    }
-
-    public Row(String table, String key) {
-        assert table != null;
-        this.table_ = table;
-        this.key_ = key;
-    }
-
-    // only for use by RMVH
-    Row()
-    {
-    }
-
-    public String getTable() {
-        return table_;
-    }
-
-    private String key_;
-
-    private Map<String, ColumnFamily> columnFamilies_ = new HashMap<String, ColumnFamily>();
-
-    public String key()
-    {
-        return key_;
-    }
-
-    void setKey(String key)
-    {
-        key_ = key;
-    }
-
-    public void setTable(String table)
-    {
-        table_ = table;
-    }
-
-    public Set<String> getColumnFamilyNames()
-    {
-        return columnFamilies_.keySet();
-    }
-
-    public Collection<ColumnFamily> getColumnFamilies()
-    {
-        return columnFamilies_.values();
-    }
-
-    public ColumnFamily getColumnFamily(String cfName)
-    {
-        return columnFamilies_.get(cfName);
-    }
-
-    void addColumnFamily(ColumnFamily columnFamily)
-    {
-        columnFamilies_.put(columnFamily.name(), columnFamily);
-    }
-
-    void removeColumnFamily(ColumnFamily columnFamily)
-    {
-        columnFamilies_.remove(columnFamily.name());
-        int delta = (-1) * columnFamily.size();
-    }
-
-    public boolean isEmpty()
-    {
-        return (columnFamilies_.size() == 0);
-    }
-
-    /*
-     * This function will repair the current row with the input row
-     * what that means is that if there are any differences between the 2 rows then
-     * this function will make the current row take the latest changes.
-     */
-    public void repair(Row rowOther)
-    {
-        for (ColumnFamily cfOld : rowOther.getColumnFamilies())
-        {
-            ColumnFamily cf = columnFamilies_.get(cfOld.name());
-            if (cf == null)
-            {
-                addColumnFamily(cfOld);
-            }
-            else
-            {
-                columnFamilies_.remove(cf.name());
-                addColumnFamily(ColumnFamily.resolve(Arrays.asList(cfOld, cf)));
-            }
-        }
-    }
-
-    /*
-     * This function will calculate the difference between 2 rows
-     * and return the resultant row. This assumes that the row that
-     * is being submitted is a super set of the current row so
-     * it only calculates additional
-     * difference and does not take care of what needs to be removed from the current row to make
-     * it same as the input row.
-     */
-    public Row diff(Row rowComposite)
-    {
-        Row rowDiff = new Row(table_, key_);
-
-        for (ColumnFamily cfComposite : rowComposite.getColumnFamilies())
-        {
-            ColumnFamily cf = columnFamilies_.get(cfComposite.name());
-            if (cf == null)
-                rowDiff.addColumnFamily(cfComposite);
-            else
-            {
-                ColumnFamily cfDiff = cf.diff(cfComposite);
-                if (cfDiff != null)
-                    rowDiff.addColumnFamily(cfDiff);
-            }
-        }
-        if (rowDiff.getColumnFamilies().isEmpty())
-            return null;
-        else
-            return rowDiff;
-    }
-
-    public Row cloneMe()
-    {
-        Row row = new Row(table_, key_);
-        row.columnFamilies_ = new HashMap<String, ColumnFamily>(columnFamilies_);
-        return row;
-    }
-
-    public byte[] digest()
-    {
-        Set<String> cfamilies = columnFamilies_.keySet();
-        byte[] xorHash = ArrayUtils.EMPTY_BYTE_ARRAY;
-        for (String cFamily : cfamilies)
-        {
-            if (xorHash.length == 0)
-            {
-                xorHash = columnFamilies_.get(cFamily).digest();
-            }
-            else
-            {
-                xorHash = FBUtilities.xor(xorHash, columnFamilies_.get(cFamily).digest());
-            }
-        }
-        return xorHash;
-    }
-
-    void clear()
-    {
-        columnFamilies_.clear();
-    }
-
-    public String toString()
-    {
-        return "Row(" + key_ + " [" + StringUtils.join(columnFamilies_.values(), ", ") + ")]";
-    }
-}
-
-class RowSerializer implements ICompactSerializer<Row>
-{
-    public void serialize(Row row, DataOutputStream dos) throws IOException
-    {
-        dos.writeUTF(row.getTable());
-        dos.writeUTF(row.key());
-        Collection<ColumnFamily> columnFamilies = row.getColumnFamilies();
-        int size = columnFamilies.size();
-        dos.writeInt(size);
-
-        for (ColumnFamily cf : columnFamilies)
-        {
-            ColumnFamily.serializer().serialize(cf, dos);
-        }
-    }
-
-    public Row deserialize(DataInputStream dis) throws IOException
-    {
-        String table = dis.readUTF();
-        String key = dis.readUTF();
-        Row row = new Row(table, key);
-        int size = dis.readInt();
-
-        for (int i = 0; i < size; ++i)
-        {
-            ColumnFamily cf = ColumnFamily.serializer().deserialize(dis);
-            row.addColumnFamily(cf);
-        }
-        return row;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.Arrays;
+
+import org.apache.commons.lang.ArrayUtils;
+import org.apache.commons.lang.StringUtils;
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class Row
+{
+    private static Logger logger_ = Logger.getLogger(Row.class);
+    private String table_;
+    private static RowSerializer serializer = new RowSerializer();
+
+    static RowSerializer serializer()
+    {
+        return serializer;
+    }
+
+    public Row(String table, String key) {
+        assert table != null;
+        this.table_ = table;
+        this.key_ = key;
+    }
+
+    // only for use by RMVH
+    Row()
+    {
+    }
+
+    public String getTable() {
+        return table_;
+    }
+
+    private String key_;
+
+    private Map<String, ColumnFamily> columnFamilies_ = new HashMap<String, ColumnFamily>();
+
+    public String key()
+    {
+        return key_;
+    }
+
+    void setKey(String key)
+    {
+        key_ = key;
+    }
+
+    public void setTable(String table)
+    {
+        table_ = table;
+    }
+
+    public Set<String> getColumnFamilyNames()
+    {
+        return columnFamilies_.keySet();
+    }
+
+    public Collection<ColumnFamily> getColumnFamilies()
+    {
+        return columnFamilies_.values();
+    }
+
+    public ColumnFamily getColumnFamily(String cfName)
+    {
+        return columnFamilies_.get(cfName);
+    }
+
+    void addColumnFamily(ColumnFamily columnFamily)
+    {
+        columnFamilies_.put(columnFamily.name(), columnFamily);
+    }
+
+    void removeColumnFamily(ColumnFamily columnFamily)
+    {
+        columnFamilies_.remove(columnFamily.name());
+        int delta = (-1) * columnFamily.size();
+    }
+
+    public boolean isEmpty()
+    {
+        return (columnFamilies_.size() == 0);
+    }
+
+    /*
+     * This function will repair the current row with the input row
+     * what that means is that if there are any differences between the 2 rows then
+     * this function will make the current row take the latest changes.
+     */
+    public void repair(Row rowOther)
+    {
+        for (ColumnFamily cfOld : rowOther.getColumnFamilies())
+        {
+            ColumnFamily cf = columnFamilies_.get(cfOld.name());
+            if (cf == null)
+            {
+                addColumnFamily(cfOld);
+            }
+            else
+            {
+                columnFamilies_.remove(cf.name());
+                addColumnFamily(ColumnFamily.resolve(Arrays.asList(cfOld, cf)));
+            }
+        }
+    }
+
+    /*
+     * This function will calculate the difference between 2 rows
+     * and return the resultant row. This assumes that the row that
+     * is being submitted is a super set of the current row so
+     * it only calculates additional
+     * difference and does not take care of what needs to be removed from the current row to make
+     * it same as the input row.
+     */
+    public Row diff(Row rowComposite)
+    {
+        Row rowDiff = new Row(table_, key_);
+
+        for (ColumnFamily cfComposite : rowComposite.getColumnFamilies())
+        {
+            ColumnFamily cf = columnFamilies_.get(cfComposite.name());
+            if (cf == null)
+                rowDiff.addColumnFamily(cfComposite);
+            else
+            {
+                ColumnFamily cfDiff = cf.diff(cfComposite);
+                if (cfDiff != null)
+                    rowDiff.addColumnFamily(cfDiff);
+            }
+        }
+        if (rowDiff.getColumnFamilies().isEmpty())
+            return null;
+        else
+            return rowDiff;
+    }
+
+    public Row cloneMe()
+    {
+        Row row = new Row(table_, key_);
+        row.columnFamilies_ = new HashMap<String, ColumnFamily>(columnFamilies_);
+        return row;
+    }
+
+    public byte[] digest()
+    {
+        Set<String> cfamilies = columnFamilies_.keySet();
+        byte[] xorHash = ArrayUtils.EMPTY_BYTE_ARRAY;
+        for (String cFamily : cfamilies)
+        {
+            if (xorHash.length == 0)
+            {
+                xorHash = columnFamilies_.get(cFamily).digest();
+            }
+            else
+            {
+                xorHash = FBUtilities.xor(xorHash, columnFamilies_.get(cFamily).digest());
+            }
+        }
+        return xorHash;
+    }
+
+    void clear()
+    {
+        columnFamilies_.clear();
+    }
+
+    public String toString()
+    {
+        return "Row(" + key_ + " [" + StringUtils.join(columnFamilies_.values(), ", ") + ")]";
+    }
+}
+
+class RowSerializer implements ICompactSerializer<Row>
+{
+    public void serialize(Row row, DataOutputStream dos) throws IOException
+    {
+        dos.writeUTF(row.getTable());
+        dos.writeUTF(row.key());
+        Collection<ColumnFamily> columnFamilies = row.getColumnFamilies();
+        int size = columnFamilies.size();
+        dos.writeInt(size);
+
+        for (ColumnFamily cf : columnFamilies)
+        {
+            ColumnFamily.serializer().serialize(cf, dos);
+        }
+    }
+
+    public Row deserialize(DataInputStream dis) throws IOException
+    {
+        String table = dis.readUTF();
+        String key = dis.readUTF();
+        Row row = new Row(table, key);
+        int size = dis.readInt();
+
+        for (int i = 0; i < size; ++i)
+        {
+            ColumnFamily cf = ColumnFamily.serializer().deserialize(dis);
+            row.addColumnFamily(cf);
+        }
+        return row;
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/RowMutation.java b/src/java/org/apache/cassandra/db/RowMutation.java
index 11acf34707..c22e5071db 100644
--- a/src/java/org/apache/cassandra/db/RowMutation.java
+++ b/src/java/org/apache/cassandra/db/RowMutation.java
@@ -1,345 +1,345 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.Serializable;
-import java.util.HashMap;
-import java.util.Iterator;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.ExecutionException;
-import java.nio.ByteBuffer;
-
-import org.apache.commons.lang.ArrayUtils;
-import org.apache.commons.lang.StringUtils;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.service.BatchMutationSuper;
-import org.apache.cassandra.service.BatchMutation;
-import org.apache.cassandra.service.InvalidRequestException;
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.db.filter.QueryPath;
-import org.apache.cassandra.db.marshal.MarshalException;
-import org.apache.cassandra.config.DatabaseDescriptor;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class RowMutation implements Serializable
-{
-    private static ICompactSerializer<RowMutation> serializer_;
-    public static final String HINT = "HINT";
-
-    static
-    {
-        serializer_ = new RowMutationSerializer();
-    }   
-
-    static ICompactSerializer<RowMutation> serializer()
-    {
-        return serializer_;
-    }
-
-    private String table_;
-    private String key_;
-    protected Map<String, ColumnFamily> modifications_ = new HashMap<String, ColumnFamily>();
-
-    public RowMutation(String table, String key)
-    {
-        table_ = table;
-        key_ = key;
-    }
-
-    public RowMutation(String table, Row row)
-    {
-        table_ = table;
-        key_ = row.key();
-        for (ColumnFamily cf : row.getColumnFamilies())
-        {
-            add(cf);
-        }
-    }
-
-    protected RowMutation(String table, String key, Map<String, ColumnFamily> modifications)
-    {
-        table_ = table;
-        key_ = key;
-        modifications_ = modifications;
-    }
-
-    public String table()
-    {
-        return table_;
-    }
-
-    public String key()
-    {
-        return key_;
-    }
-
-    public Set<String> columnFamilyNames()
-    {
-        return modifications_.keySet();
-    }
-
-    void addHints(String key, String host) throws IOException
-    {
-        QueryPath path = new QueryPath(HintedHandOffManager.HINTS_CF, key.getBytes("UTF-8"), host.getBytes("UTF-8"));
-        add(path, ArrayUtils.EMPTY_BYTE_ARRAY, System.currentTimeMillis());
-    }
-
-    /*
-     * Specify a column family name and the corresponding column
-     * family object.
-     * param @ cf - column family name
-     * param @ columnFamily - the column family.
-    */
-    public void add(ColumnFamily columnFamily)
-    {
-        if (modifications_.containsKey(columnFamily.name()))
-        {
-            throw new IllegalArgumentException("ColumnFamily " + columnFamily.name() + " is already being modified");
-        }
-        modifications_.put(columnFamily.name(), columnFamily);
-    }
-
-    /*
-     * Specify a column name and a corresponding value for
-     * the column. Column name is specified as <column family>:column.
-     * This will result in a ColumnFamily associated with
-     * <column family> as name and a Column with <column>
-     * as name. The column can be further broken up
-     * as super column name : columnname  in case of super columns
-     *
-     * param @ cf - column name as <column family>:<column>
-     * param @ value - value associated with the column
-     * param @ timestamp - timestamp associated with this data.
-    */
-    public void add(QueryPath path, byte[] value, long timestamp)
-    {
-        ColumnFamily columnFamily = modifications_.get(path.columnFamilyName);
-        if (columnFamily == null)
-        {
-            columnFamily = ColumnFamily.create(table_, path.columnFamilyName);
-        }
-        columnFamily.addColumn(path, value, timestamp);
-        modifications_.put(path.columnFamilyName, columnFamily);
-    }
-
-    public void delete(QueryPath path, long timestamp)
-    {
-        assert path.columnFamilyName != null;
-        String cfName = path.columnFamilyName;
-
-        if (modifications_.containsKey(cfName))
-        {
-            throw new IllegalArgumentException("ColumnFamily " + cfName + " is already being modified");
-        }
-
-        int localDeleteTime = (int) (System.currentTimeMillis() / 1000);
-
-        ColumnFamily columnFamily = modifications_.get(cfName);
-        if (columnFamily == null)
-            columnFamily = ColumnFamily.create(table_, cfName);
-
-        if (path.superColumnName == null && path.columnName == null)
-        {
-            columnFamily.delete(localDeleteTime, timestamp);
-        }
-        else if (path.columnName == null)
-        {
-            SuperColumn sc = new SuperColumn(path.superColumnName, DatabaseDescriptor.getSubComparator(table_, cfName));
-            sc.markForDeleteAt(localDeleteTime, timestamp);
-            columnFamily.addColumn(sc);
-        }
-        else
-        {
-            ByteBuffer bytes = ByteBuffer.allocate(4);
-            bytes.putInt(localDeleteTime);
-            columnFamily.addColumn(path, bytes.array(), timestamp, true);
-        }
-
-        modifications_.put(cfName, columnFamily);
-    }
-
-    /*
-     * This is equivalent to calling commit. Applies the changes to
-     * to the table that is obtained by calling Table.open().
-    */
-    public void apply() throws IOException
-    {
-        Row row = new Row(table_, key_);
-        apply(row);
-    }
-
-    /*
-     * Allows RowMutationVerbHandler to optimize by re-using a single Row object.
-    */
-    void apply(Row emptyRow) throws IOException
-    {
-        assert emptyRow.getColumnFamilies().size() == 0;
-        Table table = Table.open(table_);
-        for (String cfName : modifications_.keySet())
-        {
-            assert table.isValidColumnFamily(cfName);
-            emptyRow.addColumnFamily(modifications_.get(cfName));
-        }
-        table.apply(emptyRow);
-    }
-
-    /*
-     * This is equivalent to calling commit. Applies the changes to
-     * to the table that is obtained by calling Table.open().
-    */
-    void applyBinary(Row emptyRow) throws IOException, ExecutionException, InterruptedException
-    {
-        assert emptyRow.getColumnFamilies().size() == 0;
-        Table table = Table.open(table_);
-        Set<String> cfNames = modifications_.keySet();
-        for (String cfName : cfNames)
-        {
-            assert table.isValidColumnFamily(cfName);
-            emptyRow.addColumnFamily(modifications_.get(cfName));
-        }
-        table.load(emptyRow);
-    }
-
-    public Message makeRowMutationMessage() throws IOException
-    {
-        return makeRowMutationMessage(StorageService.mutationVerbHandler_);
-    }
-
-    public Message makeRowMutationMessage(String verbHandlerName) throws IOException
-    {
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream(bos);
-        serializer().serialize(this, dos);
-        EndPoint local = StorageService.getLocalStorageEndPoint();
-        EndPoint from = (local != null) ? local : new EndPoint(FBUtilities.getHostAddress(), 7000);
-        return new Message(from, StorageService.mutationStage_, verbHandlerName, bos.toByteArray());
-    }
-
-    public static RowMutation getRowMutation(String table, BatchMutation batchMutation)
-    {
-        RowMutation rm = new RowMutation(table, batchMutation.key.trim());
-        for (String cfname : batchMutation.cfmap.keySet())
-        {
-            List<org.apache.cassandra.service.Column> list = batchMutation.cfmap.get(cfname);
-            for (org.apache.cassandra.service.Column column : list)
-            {
-                rm.add(new QueryPath(cfname, null, column.name), column.value, column.timestamp);
-            }
-        }
-        return rm;
-    }
-
-    public static RowMutation getRowMutation(String table, BatchMutationSuper batchMutationSuper) throws InvalidRequestException
-    {
-        RowMutation rm = new RowMutation(table, batchMutationSuper.key.trim());
-        for (String cfName : batchMutationSuper.cfmap.keySet())
-        {
-            for (org.apache.cassandra.service.SuperColumn super_column : batchMutationSuper.cfmap.get(cfName))
-            {
-                for (org.apache.cassandra.service.Column column : super_column.columns)
-                {
-                    try
-                    {
-                        rm.add(new QueryPath(cfName, super_column.name, column.name), column.value, column.timestamp);
-                    }
-                    catch (MarshalException e)
-                    {
-                        throw new InvalidRequestException(e.getMessage());
-                    }
-                }
-            }
-        }
-        return rm;
-    }
-
-    public String toString()
-    {
-        return "RowMutation(" +
-               "table='" + table_ + '\'' +
-               ", key='" + key_ + '\'' +
-               ", modifications=[" + StringUtils.join(modifications_.values(), ", ") + "]" +
-               ')';
-    }
-}
-
-class RowMutationSerializer implements ICompactSerializer<RowMutation>
-{
-    private void freezeTheMaps(Map<String, ColumnFamily> map, DataOutputStream dos) throws IOException
-    {
-        int size = map.size();
-        dos.writeInt(size);
-        if (size > 0)
-        {
-            Set<String> keys = map.keySet();
-            for (String key : keys)
-            {
-                dos.writeUTF(key);
-                ColumnFamily cf = map.get(key);
-                if (cf != null)
-                {
-                    ColumnFamily.serializer().serialize(cf, dos);
-                }
-            }
-        }
-    }
-
-    public void serialize(RowMutation rm, DataOutputStream dos) throws IOException
-    {
-        dos.writeUTF(rm.table());
-        dos.writeUTF(rm.key());
-
-        /* serialize the modifications_ in the mutation */
-        freezeTheMaps(rm.modifications_, dos);
-    }
-
-    private Map<String, ColumnFamily> defreezeTheMaps(DataInputStream dis) throws IOException
-    {
-        Map<String, ColumnFamily> map = new HashMap<String, ColumnFamily>();
-        int size = dis.readInt();
-        for (int i = 0; i < size; ++i)
-        {
-            String key = dis.readUTF();
-            ColumnFamily cf = ColumnFamily.serializer().deserialize(dis);
-            map.put(key, cf);
-        }
-        return map;
-    }
-
-    public RowMutation deserialize(DataInputStream dis) throws IOException
-    {
-        String table = dis.readUTF();
-        String key = dis.readUTF();
-        Map<String, ColumnFamily> modifications = defreezeTheMaps(dis);
-        return new RowMutation(table, key, modifications);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.ExecutionException;
+import java.nio.ByteBuffer;
+
+import org.apache.commons.lang.ArrayUtils;
+import org.apache.commons.lang.StringUtils;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.service.BatchMutationSuper;
+import org.apache.cassandra.service.BatchMutation;
+import org.apache.cassandra.service.InvalidRequestException;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.marshal.MarshalException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class RowMutation implements Serializable
+{
+    private static ICompactSerializer<RowMutation> serializer_;
+    public static final String HINT = "HINT";
+
+    static
+    {
+        serializer_ = new RowMutationSerializer();
+    }   
+
+    static ICompactSerializer<RowMutation> serializer()
+    {
+        return serializer_;
+    }
+
+    private String table_;
+    private String key_;
+    protected Map<String, ColumnFamily> modifications_ = new HashMap<String, ColumnFamily>();
+
+    public RowMutation(String table, String key)
+    {
+        table_ = table;
+        key_ = key;
+    }
+
+    public RowMutation(String table, Row row)
+    {
+        table_ = table;
+        key_ = row.key();
+        for (ColumnFamily cf : row.getColumnFamilies())
+        {
+            add(cf);
+        }
+    }
+
+    protected RowMutation(String table, String key, Map<String, ColumnFamily> modifications)
+    {
+        table_ = table;
+        key_ = key;
+        modifications_ = modifications;
+    }
+
+    public String table()
+    {
+        return table_;
+    }
+
+    public String key()
+    {
+        return key_;
+    }
+
+    public Set<String> columnFamilyNames()
+    {
+        return modifications_.keySet();
+    }
+
+    void addHints(String key, String host) throws IOException
+    {
+        QueryPath path = new QueryPath(HintedHandOffManager.HINTS_CF, key.getBytes("UTF-8"), host.getBytes("UTF-8"));
+        add(path, ArrayUtils.EMPTY_BYTE_ARRAY, System.currentTimeMillis());
+    }
+
+    /*
+     * Specify a column family name and the corresponding column
+     * family object.
+     * param @ cf - column family name
+     * param @ columnFamily - the column family.
+    */
+    public void add(ColumnFamily columnFamily)
+    {
+        if (modifications_.containsKey(columnFamily.name()))
+        {
+            throw new IllegalArgumentException("ColumnFamily " + columnFamily.name() + " is already being modified");
+        }
+        modifications_.put(columnFamily.name(), columnFamily);
+    }
+
+    /*
+     * Specify a column name and a corresponding value for
+     * the column. Column name is specified as <column family>:column.
+     * This will result in a ColumnFamily associated with
+     * <column family> as name and a Column with <column>
+     * as name. The column can be further broken up
+     * as super column name : columnname  in case of super columns
+     *
+     * param @ cf - column name as <column family>:<column>
+     * param @ value - value associated with the column
+     * param @ timestamp - timestamp associated with this data.
+    */
+    public void add(QueryPath path, byte[] value, long timestamp)
+    {
+        ColumnFamily columnFamily = modifications_.get(path.columnFamilyName);
+        if (columnFamily == null)
+        {
+            columnFamily = ColumnFamily.create(table_, path.columnFamilyName);
+        }
+        columnFamily.addColumn(path, value, timestamp);
+        modifications_.put(path.columnFamilyName, columnFamily);
+    }
+
+    public void delete(QueryPath path, long timestamp)
+    {
+        assert path.columnFamilyName != null;
+        String cfName = path.columnFamilyName;
+
+        if (modifications_.containsKey(cfName))
+        {
+            throw new IllegalArgumentException("ColumnFamily " + cfName + " is already being modified");
+        }
+
+        int localDeleteTime = (int) (System.currentTimeMillis() / 1000);
+
+        ColumnFamily columnFamily = modifications_.get(cfName);
+        if (columnFamily == null)
+            columnFamily = ColumnFamily.create(table_, cfName);
+
+        if (path.superColumnName == null && path.columnName == null)
+        {
+            columnFamily.delete(localDeleteTime, timestamp);
+        }
+        else if (path.columnName == null)
+        {
+            SuperColumn sc = new SuperColumn(path.superColumnName, DatabaseDescriptor.getSubComparator(table_, cfName));
+            sc.markForDeleteAt(localDeleteTime, timestamp);
+            columnFamily.addColumn(sc);
+        }
+        else
+        {
+            ByteBuffer bytes = ByteBuffer.allocate(4);
+            bytes.putInt(localDeleteTime);
+            columnFamily.addColumn(path, bytes.array(), timestamp, true);
+        }
+
+        modifications_.put(cfName, columnFamily);
+    }
+
+    /*
+     * This is equivalent to calling commit. Applies the changes to
+     * to the table that is obtained by calling Table.open().
+    */
+    public void apply() throws IOException
+    {
+        Row row = new Row(table_, key_);
+        apply(row);
+    }
+
+    /*
+     * Allows RowMutationVerbHandler to optimize by re-using a single Row object.
+    */
+    void apply(Row emptyRow) throws IOException
+    {
+        assert emptyRow.getColumnFamilies().size() == 0;
+        Table table = Table.open(table_);
+        for (String cfName : modifications_.keySet())
+        {
+            assert table.isValidColumnFamily(cfName);
+            emptyRow.addColumnFamily(modifications_.get(cfName));
+        }
+        table.apply(emptyRow);
+    }
+
+    /*
+     * This is equivalent to calling commit. Applies the changes to
+     * to the table that is obtained by calling Table.open().
+    */
+    void applyBinary(Row emptyRow) throws IOException, ExecutionException, InterruptedException
+    {
+        assert emptyRow.getColumnFamilies().size() == 0;
+        Table table = Table.open(table_);
+        Set<String> cfNames = modifications_.keySet();
+        for (String cfName : cfNames)
+        {
+            assert table.isValidColumnFamily(cfName);
+            emptyRow.addColumnFamily(modifications_.get(cfName));
+        }
+        table.load(emptyRow);
+    }
+
+    public Message makeRowMutationMessage() throws IOException
+    {
+        return makeRowMutationMessage(StorageService.mutationVerbHandler_);
+    }
+
+    public Message makeRowMutationMessage(String verbHandlerName) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        serializer().serialize(this, dos);
+        EndPoint local = StorageService.getLocalStorageEndPoint();
+        EndPoint from = (local != null) ? local : new EndPoint(FBUtilities.getHostAddress(), 7000);
+        return new Message(from, StorageService.mutationStage_, verbHandlerName, bos.toByteArray());
+    }
+
+    public static RowMutation getRowMutation(String table, BatchMutation batchMutation)
+    {
+        RowMutation rm = new RowMutation(table, batchMutation.key.trim());
+        for (String cfname : batchMutation.cfmap.keySet())
+        {
+            List<org.apache.cassandra.service.Column> list = batchMutation.cfmap.get(cfname);
+            for (org.apache.cassandra.service.Column column : list)
+            {
+                rm.add(new QueryPath(cfname, null, column.name), column.value, column.timestamp);
+            }
+        }
+        return rm;
+    }
+
+    public static RowMutation getRowMutation(String table, BatchMutationSuper batchMutationSuper) throws InvalidRequestException
+    {
+        RowMutation rm = new RowMutation(table, batchMutationSuper.key.trim());
+        for (String cfName : batchMutationSuper.cfmap.keySet())
+        {
+            for (org.apache.cassandra.service.SuperColumn super_column : batchMutationSuper.cfmap.get(cfName))
+            {
+                for (org.apache.cassandra.service.Column column : super_column.columns)
+                {
+                    try
+                    {
+                        rm.add(new QueryPath(cfName, super_column.name, column.name), column.value, column.timestamp);
+                    }
+                    catch (MarshalException e)
+                    {
+                        throw new InvalidRequestException(e.getMessage());
+                    }
+                }
+            }
+        }
+        return rm;
+    }
+
+    public String toString()
+    {
+        return "RowMutation(" +
+               "table='" + table_ + '\'' +
+               ", key='" + key_ + '\'' +
+               ", modifications=[" + StringUtils.join(modifications_.values(), ", ") + "]" +
+               ')';
+    }
+}
+
+class RowMutationSerializer implements ICompactSerializer<RowMutation>
+{
+    private void freezeTheMaps(Map<String, ColumnFamily> map, DataOutputStream dos) throws IOException
+    {
+        int size = map.size();
+        dos.writeInt(size);
+        if (size > 0)
+        {
+            Set<String> keys = map.keySet();
+            for (String key : keys)
+            {
+                dos.writeUTF(key);
+                ColumnFamily cf = map.get(key);
+                if (cf != null)
+                {
+                    ColumnFamily.serializer().serialize(cf, dos);
+                }
+            }
+        }
+    }
+
+    public void serialize(RowMutation rm, DataOutputStream dos) throws IOException
+    {
+        dos.writeUTF(rm.table());
+        dos.writeUTF(rm.key());
+
+        /* serialize the modifications_ in the mutation */
+        freezeTheMaps(rm.modifications_, dos);
+    }
+
+    private Map<String, ColumnFamily> defreezeTheMaps(DataInputStream dis) throws IOException
+    {
+        Map<String, ColumnFamily> map = new HashMap<String, ColumnFamily>();
+        int size = dis.readInt();
+        for (int i = 0; i < size; ++i)
+        {
+            String key = dis.readUTF();
+            ColumnFamily cf = ColumnFamily.serializer().deserialize(dis);
+            map.put(key, cf);
+        }
+        return map;
+    }
+
+    public RowMutation deserialize(DataInputStream dis) throws IOException
+    {
+        String table = dis.readUTF();
+        String key = dis.readUTF();
+        Map<String, ColumnFamily> modifications = defreezeTheMaps(dis);
+        return new RowMutation(table, key, modifications);
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/RowMutationMessage.java b/src/java/org/apache/cassandra/db/RowMutationMessage.java
index bce749ee5a..66f228f509 100644
--- a/src/java/org/apache/cassandra/db/RowMutationMessage.java
+++ b/src/java/org/apache/cassandra/db/RowMutationMessage.java
@@ -1,95 +1,95 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.Serializable;
-
-import javax.xml.bind.annotation.XmlElement;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.FBUtilities;
-
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class RowMutationMessage implements Serializable
-{   
-    public static final String hint_ = "HINT";
-    private static RowMutationMessageSerializer serializer_ = new RowMutationMessageSerializer();
-	
-    static RowMutationMessageSerializer serializer()
-    {
-        return serializer_;
-    }
-
-    public Message makeRowMutationMessage() throws IOException
-    {         
-        return makeRowMutationMessage(StorageService.mutationVerbHandler_);
-    }
-    
-    public Message makeRowMutationMessage(String verbHandlerName) throws IOException
-    {
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream( bos );
-        RowMutationMessage.serializer().serialize(this, dos);
-        EndPoint local = StorageService.getLocalStorageEndPoint();
-        EndPoint from = ( local != null ) ? local : new EndPoint(FBUtilities.getHostAddress(), 7000);
-        return new Message(from, StorageService.mutationStage_, verbHandlerName, bos.toByteArray());         
-    }
-    
-    @XmlElement(name="RowMutation")
-    private RowMutation rowMutation_;
-    
-    private RowMutationMessage()
-    {}
-    
-    public RowMutationMessage(RowMutation rowMutation)
-    {
-        rowMutation_ = rowMutation;
-    }
-    
-   public RowMutation getRowMutation()
-   {
-       return rowMutation_;
-   }
-}
-
-class RowMutationMessageSerializer implements ICompactSerializer<RowMutationMessage>
-{
-	public void serialize(RowMutationMessage rm, DataOutputStream dos) throws IOException
-	{
-		RowMutation.serializer().serialize(rm.getRowMutation(), dos);
-	}
-	
-    public RowMutationMessage deserialize(DataInputStream dis) throws IOException
-    {
-    	RowMutation rm = RowMutation.serializer().deserialize(dis);
-    	return new RowMutationMessage(rm);
-    }
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+
+import javax.xml.bind.annotation.XmlElement;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class RowMutationMessage implements Serializable
+{   
+    public static final String hint_ = "HINT";
+    private static RowMutationMessageSerializer serializer_ = new RowMutationMessageSerializer();
+	
+    static RowMutationMessageSerializer serializer()
+    {
+        return serializer_;
+    }
+
+    public Message makeRowMutationMessage() throws IOException
+    {         
+        return makeRowMutationMessage(StorageService.mutationVerbHandler_);
+    }
+    
+    public Message makeRowMutationMessage(String verbHandlerName) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        RowMutationMessage.serializer().serialize(this, dos);
+        EndPoint local = StorageService.getLocalStorageEndPoint();
+        EndPoint from = ( local != null ) ? local : new EndPoint(FBUtilities.getHostAddress(), 7000);
+        return new Message(from, StorageService.mutationStage_, verbHandlerName, bos.toByteArray());         
+    }
+    
+    @XmlElement(name="RowMutation")
+    private RowMutation rowMutation_;
+    
+    private RowMutationMessage()
+    {}
+    
+    public RowMutationMessage(RowMutation rowMutation)
+    {
+        rowMutation_ = rowMutation;
+    }
+    
+   public RowMutation getRowMutation()
+   {
+       return rowMutation_;
+   }
+}
+
+class RowMutationMessageSerializer implements ICompactSerializer<RowMutationMessage>
+{
+	public void serialize(RowMutationMessage rm, DataOutputStream dos) throws IOException
+	{
+		RowMutation.serializer().serialize(rm.getRowMutation(), dos);
+	}
+	
+    public RowMutationMessage deserialize(DataInputStream dis) throws IOException
+    {
+    	RowMutation rm = RowMutation.serializer().deserialize(dis);
+    	return new RowMutationMessage(rm);
+    }
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java b/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java
index fc57d0d834..aac0885ea7 100644
--- a/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java
+++ b/src/java/org/apache/cassandra/db/RowMutationVerbHandler.java
@@ -1,96 +1,96 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.*;
-
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.net.*;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class RowMutationVerbHandler implements IVerbHandler
-{
-    protected static class RowMutationContext
-    {
-        protected Row row_ = new Row();
-        protected DataInputBuffer buffer_ = new DataInputBuffer();
-    }
-
-    private static Logger logger_ = Logger.getLogger(RowMutationVerbHandler.class);
-    /* We use this so that we can reuse the same row mutation context for the mutation. */
-    private static ThreadLocal<RowMutationContext> tls_ = new InheritableThreadLocal<RowMutationContext>();
-
-    public void doVerb(Message message)
-    {
-        byte[] bytes = message.getMessageBody();
-        /* Obtain a Row Mutation Context from TLS */
-        RowMutationContext rowMutationCtx = tls_.get();
-        if ( rowMutationCtx == null )
-        {
-            rowMutationCtx = new RowMutationContext();
-            tls_.set(rowMutationCtx);
-        }
-
-        rowMutationCtx.buffer_.reset(bytes, bytes.length);
-
-        try
-        {
-            RowMutation rm = RowMutation.serializer().deserialize(rowMutationCtx.buffer_);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Applying " + rm);
-
-            /* Check if there were any hints in this message */
-            byte[] hintedBytes = message.getHeader(RowMutation.HINT);
-            if ( hintedBytes != null && hintedBytes.length > 0 )
-            {
-            	EndPoint hint = EndPoint.fromBytes(hintedBytes);
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Adding hint for " + hint);
-                /* add necessary hints to this mutation */
-                RowMutation hintedMutation = new RowMutation(Table.SYSTEM_TABLE, rm.table());
-                hintedMutation.addHints(rm.key(), hint.getHost());
-                hintedMutation.apply();
-            }
-
-            rowMutationCtx.row_.clear();
-            rowMutationCtx.row_.setTable(rm.table());
-            rowMutationCtx.row_.setKey(rm.key());
-            rm.apply(rowMutationCtx.row_);
-
-            WriteResponse response = new WriteResponse(rm.table(), rm.key(), true);
-            Message responseMessage = WriteResponse.makeWriteResponseMessage(message, response);
-            if (logger_.isDebugEnabled())
-              logger_.debug(rm + " applied.  Sending response to " + message.getMessageId() + "@" + message.getFrom());
-            MessagingService.getMessagingInstance().sendOneWay(responseMessage, message.getFrom());
-        }
-        catch (IOException e)
-        {
-            logger_.error("Error in row mutation", e);
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.*;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.net.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class RowMutationVerbHandler implements IVerbHandler
+{
+    protected static class RowMutationContext
+    {
+        protected Row row_ = new Row();
+        protected DataInputBuffer buffer_ = new DataInputBuffer();
+    }
+
+    private static Logger logger_ = Logger.getLogger(RowMutationVerbHandler.class);
+    /* We use this so that we can reuse the same row mutation context for the mutation. */
+    private static ThreadLocal<RowMutationContext> tls_ = new InheritableThreadLocal<RowMutationContext>();
+
+    public void doVerb(Message message)
+    {
+        byte[] bytes = message.getMessageBody();
+        /* Obtain a Row Mutation Context from TLS */
+        RowMutationContext rowMutationCtx = tls_.get();
+        if ( rowMutationCtx == null )
+        {
+            rowMutationCtx = new RowMutationContext();
+            tls_.set(rowMutationCtx);
+        }
+
+        rowMutationCtx.buffer_.reset(bytes, bytes.length);
+
+        try
+        {
+            RowMutation rm = RowMutation.serializer().deserialize(rowMutationCtx.buffer_);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Applying " + rm);
+
+            /* Check if there were any hints in this message */
+            byte[] hintedBytes = message.getHeader(RowMutation.HINT);
+            if ( hintedBytes != null && hintedBytes.length > 0 )
+            {
+            	EndPoint hint = EndPoint.fromBytes(hintedBytes);
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Adding hint for " + hint);
+                /* add necessary hints to this mutation */
+                RowMutation hintedMutation = new RowMutation(Table.SYSTEM_TABLE, rm.table());
+                hintedMutation.addHints(rm.key(), hint.getHost());
+                hintedMutation.apply();
+            }
+
+            rowMutationCtx.row_.clear();
+            rowMutationCtx.row_.setTable(rm.table());
+            rowMutationCtx.row_.setKey(rm.key());
+            rm.apply(rowMutationCtx.row_);
+
+            WriteResponse response = new WriteResponse(rm.table(), rm.key(), true);
+            Message responseMessage = WriteResponse.makeWriteResponseMessage(message, response);
+            if (logger_.isDebugEnabled())
+              logger_.debug(rm + " applied.  Sending response to " + message.getMessageId() + "@" + message.getFrom());
+            MessagingService.getMessagingInstance().sendOneWay(responseMessage, message.getFrom());
+        }
+        catch (IOException e)
+        {
+            logger_.error("Error in row mutation", e);
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/Scanner.java b/src/java/org/apache/cassandra/db/Scanner.java
index e04b1347f1..2c35ee6c10 100644
--- a/src/java/org/apache/cassandra/db/Scanner.java
+++ b/src/java/org/apache/cassandra/db/Scanner.java
@@ -1,87 +1,87 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.util.*;
-import java.io.IOException;
-
-
-/**
- * This class is used to loop through a retrieved column family
- * to get all columns in Iterator style. Usage is as follows:
- * Scanner scanner = new Scanner("table");
- * scanner.fetchColumnfamily(key, "column-family");
- * 
- * while ( scanner.hasNext() )
- * {
- *     Column column = scanner.next();
- *     // Do something with the column
- * }
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class Scanner implements IScanner<IColumn>
-{
-    /* Table over which we are scanning. */
-    private String table_; 
-    /* Iterator when iterating over the columns of a given key in a column family */
-    private Iterator<IColumn> columnIt_;
-        
-    public Scanner(String table)
-    {
-        table_ = table;
-    }
-    
-    /**
-     * Fetch the columns associated with this key for the specified column family.
-     * This method basically sets up an iterator internally and then provides an 
-     * iterator like interface to iterate over the columns.
-     * @param key key we are interested in.
-     * @param cf column family we are interested in.
-     * @throws IOException
-     */
-    public void fetch(String key, String cf) throws IOException
-    {        
-        if ( cf != null )
-        {
-            Table table = Table.open(table_);
-            ColumnFamily columnFamily = table.get(key, cf);
-            if ( columnFamily != null )
-            {
-                Collection<IColumn> columns = columnFamily.getSortedColumns();
-                columnIt_ = columns.iterator();
-            }
-        }
-    }        
-    
-    public boolean hasNext() throws IOException
-    {
-        return columnIt_.hasNext();
-    }
-    
-    public IColumn next()
-    {
-        return columnIt_.next();
-    }
-    
-    public void close() throws IOException
-    {
-        throw new UnsupportedOperationException("This operation is not supported in the Scanner");
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.*;
+import java.io.IOException;
+
+
+/**
+ * This class is used to loop through a retrieved column family
+ * to get all columns in Iterator style. Usage is as follows:
+ * Scanner scanner = new Scanner("table");
+ * scanner.fetchColumnfamily(key, "column-family");
+ * 
+ * while ( scanner.hasNext() )
+ * {
+ *     Column column = scanner.next();
+ *     // Do something with the column
+ * }
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Scanner implements IScanner<IColumn>
+{
+    /* Table over which we are scanning. */
+    private String table_; 
+    /* Iterator when iterating over the columns of a given key in a column family */
+    private Iterator<IColumn> columnIt_;
+        
+    public Scanner(String table)
+    {
+        table_ = table;
+    }
+    
+    /**
+     * Fetch the columns associated with this key for the specified column family.
+     * This method basically sets up an iterator internally and then provides an 
+     * iterator like interface to iterate over the columns.
+     * @param key key we are interested in.
+     * @param cf column family we are interested in.
+     * @throws IOException
+     */
+    public void fetch(String key, String cf) throws IOException
+    {        
+        if ( cf != null )
+        {
+            Table table = Table.open(table_);
+            ColumnFamily columnFamily = table.get(key, cf);
+            if ( columnFamily != null )
+            {
+                Collection<IColumn> columns = columnFamily.getSortedColumns();
+                columnIt_ = columns.iterator();
+            }
+        }
+    }        
+    
+    public boolean hasNext() throws IOException
+    {
+        return columnIt_.hasNext();
+    }
+    
+    public IColumn next()
+    {
+        return columnIt_.next();
+    }
+    
+    public void close() throws IOException
+    {
+        throw new UnsupportedOperationException("This operation is not supported in the Scanner");
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/SliceByNamesReadCommand.java b/src/java/org/apache/cassandra/db/SliceByNamesReadCommand.java
index 6bc18a63eb..725dae9cb7 100644
--- a/src/java/org/apache/cassandra/db/SliceByNamesReadCommand.java
+++ b/src/java/org/apache/cassandra/db/SliceByNamesReadCommand.java
@@ -1,122 +1,122 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.cassandra.db;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.*;
-
-import org.apache.commons.lang.StringUtils;
-
-import org.apache.cassandra.service.ColumnParent;
-import org.apache.cassandra.db.filter.QueryPath;
-import org.apache.cassandra.db.filter.NamesQueryFilter;
-import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.config.DatabaseDescriptor;
-
-public class SliceByNamesReadCommand extends ReadCommand
-{
-    public final QueryPath columnParent;
-    public final SortedSet<byte[]> columnNames;
-
-    public SliceByNamesReadCommand(String table, String key, ColumnParent column_parent, Collection<byte[]> columnNames)
-    {
-        this(table, key, new QueryPath(column_parent), columnNames);
-    }
-
-    public SliceByNamesReadCommand(String table, String key, QueryPath path, Collection<byte[]> columnNames)
-    {
-        super(table, key, CMD_TYPE_GET_SLICE_BY_NAMES);
-        this.columnParent = path;
-        this.columnNames = new TreeSet<byte[]>(getComparator());
-        this.columnNames.addAll(columnNames);
-    }
-
-    @Override
-    public String getColumnFamilyName()
-    {
-        return columnParent.columnFamilyName;
-    }
-
-    @Override
-    public ReadCommand copy()
-    {
-        ReadCommand readCommand= new SliceByNamesReadCommand(table, key, columnParent, columnNames);
-        readCommand.setDigestQuery(isDigestQuery());
-        return readCommand;
-    }
-    
-    @Override
-    public Row getRow(Table table) throws IOException
-    {        
-        return table.getRow(new NamesQueryFilter(key, columnParent, columnNames));
-    }
-
-    @Override
-    public String toString()
-    {
-        return "SliceByNamesReadCommand(" +
-               "table='" + table + '\'' +
-               ", key='" + key + '\'' +
-               ", columnParent='" + columnParent + '\'' +
-               ", columns=[" + getComparator().getString(columnNames) + "]" +
-               ')';
-    }
-
-}
-
-class SliceByNamesReadCommandSerializer extends ReadCommandSerializer
-{
-    @Override
-    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
-    {
-        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
-        dos.writeBoolean(realRM.isDigestQuery());
-        dos.writeUTF(realRM.table);
-        dos.writeUTF(realRM.key);
-        realRM.columnParent.serialize(dos);
-        dos.writeInt(realRM.columnNames.size());
-        if (realRM.columnNames.size() > 0)
-        {
-            for (byte[] cName : realRM.columnNames)
-            {
-                ColumnSerializer.writeName(cName, dos);
-            }
-        }
-    }
-
-    @Override
-    public ReadCommand deserialize(DataInputStream dis) throws IOException
-    {
-        boolean isDigest = dis.readBoolean();
-        String table = dis.readUTF();
-        String key = dis.readUTF();
-        QueryPath columnParent = QueryPath.deserialize(dis);
-
-        int size = dis.readInt();
-        List<byte[]> columns = new ArrayList<byte[]>();
-        for (int i = 0; i < size; ++i)
-        {
-            columns.add(ColumnSerializer.readName(dis));
-        }
-        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
-        rm.setDigestQuery(isDigest);
-        return rm;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+
+import org.apache.commons.lang.StringUtils;
+
+import org.apache.cassandra.service.ColumnParent;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.filter.NamesQueryFilter;
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+public class SliceByNamesReadCommand extends ReadCommand
+{
+    public final QueryPath columnParent;
+    public final SortedSet<byte[]> columnNames;
+
+    public SliceByNamesReadCommand(String table, String key, ColumnParent column_parent, Collection<byte[]> columnNames)
+    {
+        this(table, key, new QueryPath(column_parent), columnNames);
+    }
+
+    public SliceByNamesReadCommand(String table, String key, QueryPath path, Collection<byte[]> columnNames)
+    {
+        super(table, key, CMD_TYPE_GET_SLICE_BY_NAMES);
+        this.columnParent = path;
+        this.columnNames = new TreeSet<byte[]>(getComparator());
+        this.columnNames.addAll(columnNames);
+    }
+
+    @Override
+    public String getColumnFamilyName()
+    {
+        return columnParent.columnFamilyName;
+    }
+
+    @Override
+    public ReadCommand copy()
+    {
+        ReadCommand readCommand= new SliceByNamesReadCommand(table, key, columnParent, columnNames);
+        readCommand.setDigestQuery(isDigestQuery());
+        return readCommand;
+    }
+    
+    @Override
+    public Row getRow(Table table) throws IOException
+    {        
+        return table.getRow(new NamesQueryFilter(key, columnParent, columnNames));
+    }
+
+    @Override
+    public String toString()
+    {
+        return "SliceByNamesReadCommand(" +
+               "table='" + table + '\'' +
+               ", key='" + key + '\'' +
+               ", columnParent='" + columnParent + '\'' +
+               ", columns=[" + getComparator().getString(columnNames) + "]" +
+               ')';
+    }
+
+}
+
+class SliceByNamesReadCommandSerializer extends ReadCommandSerializer
+{
+    @Override
+    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
+    {
+        SliceByNamesReadCommand realRM = (SliceByNamesReadCommand)rm;
+        dos.writeBoolean(realRM.isDigestQuery());
+        dos.writeUTF(realRM.table);
+        dos.writeUTF(realRM.key);
+        realRM.columnParent.serialize(dos);
+        dos.writeInt(realRM.columnNames.size());
+        if (realRM.columnNames.size() > 0)
+        {
+            for (byte[] cName : realRM.columnNames)
+            {
+                ColumnSerializer.writeName(cName, dos);
+            }
+        }
+    }
+
+    @Override
+    public ReadCommand deserialize(DataInputStream dis) throws IOException
+    {
+        boolean isDigest = dis.readBoolean();
+        String table = dis.readUTF();
+        String key = dis.readUTF();
+        QueryPath columnParent = QueryPath.deserialize(dis);
+
+        int size = dis.readInt();
+        List<byte[]> columns = new ArrayList<byte[]>();
+        for (int i = 0; i < size; ++i)
+        {
+            columns.add(ColumnSerializer.readName(dis));
+        }
+        SliceByNamesReadCommand rm = new SliceByNamesReadCommand(table, key, columnParent, columns);
+        rm.setDigestQuery(isDigest);
+        return rm;
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/SliceFromReadCommand.java b/src/java/org/apache/cassandra/db/SliceFromReadCommand.java
index a1eb6c84b9..f9029f735a 100644
--- a/src/java/org/apache/cassandra/db/SliceFromReadCommand.java
+++ b/src/java/org/apache/cassandra/db/SliceFromReadCommand.java
@@ -1,115 +1,115 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.cassandra.db;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.cassandra.db.filter.QueryPath;
-import org.apache.cassandra.db.filter.SliceQueryFilter;
-import org.apache.cassandra.service.ColumnParent;
-
-public class SliceFromReadCommand extends ReadCommand
-{
-    public final QueryPath column_parent;
-    public final byte[] start, finish;
-    public final boolean isAscending;
-    public final int count;
-
-    public SliceFromReadCommand(String table, String key, ColumnParent column_parent, byte[] start, byte[] finish, boolean isAscending, int count)
-    {
-        this(table, key, new QueryPath(column_parent), start, finish, isAscending, count);
-    }
-
-    public SliceFromReadCommand(String table, String key, QueryPath columnParent, byte[] start, byte[] finish, boolean isAscending, int count)
-    {
-        super(table, key, CMD_TYPE_GET_SLICE);
-        this.column_parent = columnParent;
-        this.start = start;
-        this.finish = finish;
-        this.isAscending = isAscending;
-        this.count = count;
-    }
-
-    @Override
-    public String getColumnFamilyName()
-    {
-        return column_parent.columnFamilyName;
-    }
-
-    @Override
-    public ReadCommand copy()
-    {
-        ReadCommand readCommand = new SliceFromReadCommand(table, key, column_parent, start, finish, isAscending, count);
-        readCommand.setDigestQuery(isDigestQuery());
-        return readCommand;
-    }
-
-    @Override
-    public Row getRow(Table table) throws IOException
-    {
-        return table.getRow(new SliceQueryFilter(key, column_parent, start, finish, isAscending, count));
-    }
-
-    @Override
-    public String toString()
-    {
-        return "SliceFromReadCommand(" +
-               "table='" + table + '\'' +
-               ", key='" + key + '\'' +
-               ", column_parent='" + column_parent + '\'' +
-               ", start='" + getComparator().getString(start) + '\'' +
-               ", finish='" + getComparator().getString(finish) + '\'' +
-               ", isAscending=" + isAscending +
-               ", count=" + count +
-               ')';
-    }
-}
-
-class SliceFromReadCommandSerializer extends ReadCommandSerializer
-{
-    @Override
-    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
-    {
-        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
-        dos.writeBoolean(realRM.isDigestQuery());
-        dos.writeUTF(realRM.table);
-        dos.writeUTF(realRM.key);
-        realRM.column_parent.serialize(dos);
-        ColumnSerializer.writeName(realRM.start, dos);
-        ColumnSerializer.writeName(realRM.finish, dos);
-        dos.writeBoolean(realRM.isAscending);
-        dos.writeInt(realRM.count);
-    }
-
-    @Override
-    public ReadCommand deserialize(DataInputStream dis) throws IOException
-    {
-        boolean isDigest = dis.readBoolean();
-        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
-                                                           dis.readUTF(),
-                                                           QueryPath.deserialize(dis),
-                                                           ColumnSerializer.readName(dis),
-                                                           ColumnSerializer.readName(dis),
-                                                           dis.readBoolean(), 
-                                                           dis.readInt());
-        rm.setDigestQuery(isDigest);
-        return rm;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.filter.SliceQueryFilter;
+import org.apache.cassandra.service.ColumnParent;
+
+public class SliceFromReadCommand extends ReadCommand
+{
+    public final QueryPath column_parent;
+    public final byte[] start, finish;
+    public final boolean isAscending;
+    public final int count;
+
+    public SliceFromReadCommand(String table, String key, ColumnParent column_parent, byte[] start, byte[] finish, boolean isAscending, int count)
+    {
+        this(table, key, new QueryPath(column_parent), start, finish, isAscending, count);
+    }
+
+    public SliceFromReadCommand(String table, String key, QueryPath columnParent, byte[] start, byte[] finish, boolean isAscending, int count)
+    {
+        super(table, key, CMD_TYPE_GET_SLICE);
+        this.column_parent = columnParent;
+        this.start = start;
+        this.finish = finish;
+        this.isAscending = isAscending;
+        this.count = count;
+    }
+
+    @Override
+    public String getColumnFamilyName()
+    {
+        return column_parent.columnFamilyName;
+    }
+
+    @Override
+    public ReadCommand copy()
+    {
+        ReadCommand readCommand = new SliceFromReadCommand(table, key, column_parent, start, finish, isAscending, count);
+        readCommand.setDigestQuery(isDigestQuery());
+        return readCommand;
+    }
+
+    @Override
+    public Row getRow(Table table) throws IOException
+    {
+        return table.getRow(new SliceQueryFilter(key, column_parent, start, finish, isAscending, count));
+    }
+
+    @Override
+    public String toString()
+    {
+        return "SliceFromReadCommand(" +
+               "table='" + table + '\'' +
+               ", key='" + key + '\'' +
+               ", column_parent='" + column_parent + '\'' +
+               ", start='" + getComparator().getString(start) + '\'' +
+               ", finish='" + getComparator().getString(finish) + '\'' +
+               ", isAscending=" + isAscending +
+               ", count=" + count +
+               ')';
+    }
+}
+
+class SliceFromReadCommandSerializer extends ReadCommandSerializer
+{
+    @Override
+    public void serialize(ReadCommand rm, DataOutputStream dos) throws IOException
+    {
+        SliceFromReadCommand realRM = (SliceFromReadCommand)rm;
+        dos.writeBoolean(realRM.isDigestQuery());
+        dos.writeUTF(realRM.table);
+        dos.writeUTF(realRM.key);
+        realRM.column_parent.serialize(dos);
+        ColumnSerializer.writeName(realRM.start, dos);
+        ColumnSerializer.writeName(realRM.finish, dos);
+        dos.writeBoolean(realRM.isAscending);
+        dos.writeInt(realRM.count);
+    }
+
+    @Override
+    public ReadCommand deserialize(DataInputStream dis) throws IOException
+    {
+        boolean isDigest = dis.readBoolean();
+        SliceFromReadCommand rm = new SliceFromReadCommand(dis.readUTF(),
+                                                           dis.readUTF(),
+                                                           QueryPath.deserialize(dis),
+                                                           ColumnSerializer.readName(dis),
+                                                           ColumnSerializer.readName(dis),
+                                                           dis.readBoolean(), 
+                                                           dis.readInt());
+        rm.setDigestQuery(isDigest);
+        return rm;
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/SuperColumn.java b/src/java/org/apache/cassandra/db/SuperColumn.java
index f7e51ae6ed..22dd854dfb 100644
--- a/src/java/org/apache/cassandra/db/SuperColumn.java
+++ b/src/java/org/apache/cassandra/db/SuperColumn.java
@@ -1,379 +1,379 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.Serializable;
-import java.util.Collection;
-import java.util.Arrays;
-import java.util.concurrent.atomic.AtomicInteger;
-import java.util.concurrent.ConcurrentSkipListMap;
-
-import org.apache.commons.lang.ArrayUtils;
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.db.marshal.MarshalException;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public final class SuperColumn implements IColumn
-{
-	private static Logger logger_ = Logger.getLogger(SuperColumn.class);
-
-    static SuperColumnSerializer serializer(AbstractType comparator)
-    {
-        return new SuperColumnSerializer(comparator);
-    }
-
-    private byte[] name_;
-    // TODO make subcolumn comparator configurable
-    private ConcurrentSkipListMap<byte[], IColumn> columns_;
-    private int localDeletionTime = Integer.MIN_VALUE;
-	private long markedForDeleteAt = Long.MIN_VALUE;
-    private AtomicInteger size_ = new AtomicInteger(0);
-
-    SuperColumn(byte[] name, AbstractType comparator)
-    {
-    	name_ = name;
-        columns_ = new ConcurrentSkipListMap<byte[], IColumn>(comparator);
-    }
-
-    public AbstractType getComparator()
-    {
-        return (AbstractType)columns_.comparator();
-    }
-
-    public SuperColumn cloneMeShallow()
-    {
-        SuperColumn sc = new SuperColumn(name_, getComparator());
-        sc.markForDeleteAt(localDeletionTime, markedForDeleteAt);
-        return sc;
-    }
-
-	public boolean isMarkedForDelete()
-	{
-		return markedForDeleteAt > Long.MIN_VALUE;
-	}
-
-    public byte[] name()
-    {
-    	return name_;
-    }
-
-    public Collection<IColumn> getSubColumns()
-    {
-    	return columns_.values();
-    }
-
-    public IColumn getSubColumn(byte[] columnName)
-    {
-        IColumn column = columns_.get(columnName);
-        assert column == null || column instanceof Column;
-        return column;
-    }
-
-    public int size()
-    {
-        /*
-         * return the size of the individual columns
-         * that make up the super column. This is an
-         * APPROXIMATION of the size used only from the
-         * Memtable.
-        */
-        return size_.get();
-    }
-
-    /**
-     * This returns the size of the super-column when serialized.
-     * @see org.apache.cassandra.db.IColumn#serializedSize()
-    */
-    public int serializedSize()
-    {
-        /*
-         * Size of a super-column is =
-         *   size of a name (UtfPrefix + length of the string)
-         * + 1 byte to indicate if the super-column has been deleted
-         * + 4 bytes for size of the sub-columns
-         * + 4 bytes for the number of sub-columns
-         * + size of all the sub-columns.
-        */
-
-    	/*
-    	 * We store the string as UTF-8 encoded, so when we calculate the length, it
-    	 * should be converted to UTF-8.
-    	 */
-    	/*
-    	 * We need to keep the way we are calculating the column size in sync with the
-    	 * way we are calculating the size for the column family serializer.
-    	 */
-    	return IColumn.UtfPrefix_ + name_.length + DBConstants.boolSize_ + DBConstants.intSize_ + DBConstants.intSize_ + getSizeOfAllColumns();
-    }
-
-    /**
-     * This calculates the exact size of the sub columns on the fly
-     */
-    int getSizeOfAllColumns()
-    {
-        int size = 0;
-        Collection<IColumn> subColumns = getSubColumns();
-        for ( IColumn subColumn : subColumns )
-        {
-            size += subColumn.serializedSize();
-        }
-        return size;
-    }
-
-    public void remove(byte[] columnName)
-    {
-    	columns_.remove(columnName);
-    }
-
-    public long timestamp()
-    {
-    	throw new UnsupportedOperationException("This operation is not supported for Super Columns.");
-    }
-
-    public long timestamp(byte[] columnName)
-    {
-    	IColumn column = columns_.get(columnName);
-    	if ( column instanceof SuperColumn )
-    		throw new UnsupportedOperationException("A super column cannot hold other super columns.");
-    	if ( column != null )
-    		return column.timestamp();
-    	throw new IllegalArgumentException("Timestamp was requested for a column that does not exist.");
-    }
-
-    public byte[] value()
-    {
-    	throw new UnsupportedOperationException("This operation is not supported for Super Columns.");
-    }
-
-    public byte[] value(byte[] columnName)
-    {
-    	IColumn column = columns_.get(columnName);
-    	if ( column != null )
-    		return column.value();
-    	throw new IllegalArgumentException("Value was requested for a column that does not exist.");
-    }
-
-    public void addColumn(IColumn column)
-    {
-    	if (!(column instanceof Column))
-    		throw new UnsupportedOperationException("A super column can only contain simple columns.");
-        try
-        {
-            getComparator().validate(column.name());
-        }
-        catch (Exception e)
-        {
-            throw new MarshalException("Invalid column name in supercolumn for " + getComparator().getClass().getName());
-        }
-    	IColumn oldColumn = columns_.get(column.name());
-    	if ( oldColumn == null )
-        {
-    		columns_.put(column.name(), column);
-            size_.addAndGet(column.size());
-        }
-    	else
-    	{
-    		if (((Column)oldColumn).comparePriority((Column)column) <= 0)
-            {
-    			columns_.put(column.name(), column);
-                int delta = (-1)*oldColumn.size();
-                /* subtract the size of the oldColumn */
-                size_.addAndGet(delta);
-                /* add the size of the new column */
-                size_.addAndGet(column.size());
-            }
-    	}
-    }
-
-    /*
-     * Go through each sub column if it exists then as it to resolve itself
-     * if the column does not exist then create it.
-     */
-    public void putColumn(IColumn column)
-    {
-        if (!(column instanceof SuperColumn))
-        {
-            throw new UnsupportedOperationException("Only Super column objects should be put here");
-        }
-        if (!Arrays.equals(name_, column.name()))
-        {
-            throw new IllegalArgumentException("The name should match the name of the current column or super column");
-        }
-
-        for (IColumn subColumn : column.getSubColumns())
-        {
-        	addColumn(subColumn);
-        }
-        if (column.getMarkedForDeleteAt() > markedForDeleteAt)
-        {
-            markForDeleteAt(column.getLocalDeletionTime(),  column.getMarkedForDeleteAt());
-        }
-    }
-
-    public int getObjectCount()
-    {
-    	return 1 + columns_.size();
-    }
-
-    public long getMarkedForDeleteAt() {
-        return markedForDeleteAt;
-    }
-
-    int getColumnCount()
-    {
-    	return columns_.size();
-    }
-
-    public IColumn diff(IColumn columnNew)
-    {
-    	IColumn columnDiff = new SuperColumn(columnNew.name(), ((SuperColumn)columnNew).getComparator());
-        if (columnNew.getMarkedForDeleteAt() > getMarkedForDeleteAt())
-        {
-            ((SuperColumn)columnDiff).markForDeleteAt(columnNew.getLocalDeletionTime(), columnNew.getMarkedForDeleteAt());
-        }
-
-        // (don't need to worry about columnNew containing subColumns that are shadowed by
-        // the delete tombstone, since columnNew was generated by CF.resolve, which
-        // takes care of those for us.)
-        for (IColumn subColumn : columnNew.getSubColumns())
-        {
-        	IColumn columnInternal = columns_.get(subColumn.name());
-        	if(columnInternal == null )
-        	{
-        		columnDiff.addColumn(subColumn);
-        	}
-        	else
-        	{
-            	IColumn subColumnDiff = columnInternal.diff(subColumn);
-        		if(subColumnDiff != null)
-        		{
-            		columnDiff.addColumn(subColumnDiff);
-        		}
-        	}
-        }
-
-        if (!columnDiff.getSubColumns().isEmpty() || columnNew.isMarkedForDelete())
-        	return columnDiff;
-        else
-        	return null;
-    }
-
-    public byte[] digest()
-    {
-    	byte[] xorHash = ArrayUtils.EMPTY_BYTE_ARRAY;
-    	if(name_ == null)
-    		return xorHash;
-    	xorHash = name_.clone();
-    	for(IColumn column : columns_.values())
-    	{
-			xorHash = FBUtilities.xor(xorHash, column.digest());
-    	}
-    	return xorHash;
-    }
-
-    public String getString(AbstractType comparator)
-    {
-    	StringBuilder sb = new StringBuilder();
-        sb.append("SuperColumn(");
-    	sb.append(comparator.getString(name_));
-
-        if (isMarkedForDelete()) {
-            sb.append(" -delete at ").append(getMarkedForDeleteAt()).append("-");
-        }
-
-        sb.append(" [");
-        sb.append(getComparator().getColumnsString(columns_.values()));
-        sb.append("])");
-
-        return sb.toString();
-    }
-
-    public int getLocalDeletionTime()
-    {
-        return localDeletionTime;
-    }
-
-    public void markForDeleteAt(int localDeleteTime, long timestamp)
-    {
-        this.localDeletionTime = localDeleteTime;
-        this.markedForDeleteAt = timestamp;
-    }
-}
-
-class SuperColumnSerializer implements ICompactSerializer<IColumn>
-{
-    private AbstractType comparator;
-
-    public SuperColumnSerializer(AbstractType comparator)
-    {
-        this.comparator = comparator;
-    }
-
-    public AbstractType getComparator()
-    {
-        return comparator;
-    }
-
-    public void serialize(IColumn column, DataOutputStream dos) throws IOException
-    {
-    	SuperColumn superColumn = (SuperColumn)column;
-        ColumnSerializer.writeName(column.name(), dos);
-        dos.writeInt(superColumn.getLocalDeletionTime());
-        dos.writeLong(superColumn.getMarkedForDeleteAt());
-
-        Collection<IColumn> columns  = column.getSubColumns();
-        int size = columns.size();
-        dos.writeInt(size);
-
-        dos.writeInt(superColumn.getSizeOfAllColumns());
-        for ( IColumn subColumn : columns )
-        {
-            Column.serializer().serialize(subColumn, dos);
-        }
-    }
-
-    public IColumn deserialize(DataInputStream dis) throws IOException
-    {
-        byte[] name = ColumnSerializer.readName(dis);
-        SuperColumn superColumn = new SuperColumn(name, comparator);
-        superColumn.markForDeleteAt(dis.readInt(), dis.readLong());
-        assert dis.available() > 0;
-
-        /* read the number of columns */
-        int size = dis.readInt();
-        /* read the size of all columns */
-        dis.readInt();
-        for ( int i = 0; i < size; ++i )
-        {
-            IColumn subColumn = Column.serializer().deserialize(dis);
-            superColumn.addColumn(subColumn);
-        }
-        return superColumn;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.Collection;
+import java.util.Arrays;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.ConcurrentSkipListMap;
+
+import org.apache.commons.lang.ArrayUtils;
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.db.marshal.MarshalException;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class SuperColumn implements IColumn
+{
+	private static Logger logger_ = Logger.getLogger(SuperColumn.class);
+
+    static SuperColumnSerializer serializer(AbstractType comparator)
+    {
+        return new SuperColumnSerializer(comparator);
+    }
+
+    private byte[] name_;
+    // TODO make subcolumn comparator configurable
+    private ConcurrentSkipListMap<byte[], IColumn> columns_;
+    private int localDeletionTime = Integer.MIN_VALUE;
+	private long markedForDeleteAt = Long.MIN_VALUE;
+    private AtomicInteger size_ = new AtomicInteger(0);
+
+    SuperColumn(byte[] name, AbstractType comparator)
+    {
+    	name_ = name;
+        columns_ = new ConcurrentSkipListMap<byte[], IColumn>(comparator);
+    }
+
+    public AbstractType getComparator()
+    {
+        return (AbstractType)columns_.comparator();
+    }
+
+    public SuperColumn cloneMeShallow()
+    {
+        SuperColumn sc = new SuperColumn(name_, getComparator());
+        sc.markForDeleteAt(localDeletionTime, markedForDeleteAt);
+        return sc;
+    }
+
+	public boolean isMarkedForDelete()
+	{
+		return markedForDeleteAt > Long.MIN_VALUE;
+	}
+
+    public byte[] name()
+    {
+    	return name_;
+    }
+
+    public Collection<IColumn> getSubColumns()
+    {
+    	return columns_.values();
+    }
+
+    public IColumn getSubColumn(byte[] columnName)
+    {
+        IColumn column = columns_.get(columnName);
+        assert column == null || column instanceof Column;
+        return column;
+    }
+
+    public int size()
+    {
+        /*
+         * return the size of the individual columns
+         * that make up the super column. This is an
+         * APPROXIMATION of the size used only from the
+         * Memtable.
+        */
+        return size_.get();
+    }
+
+    /**
+     * This returns the size of the super-column when serialized.
+     * @see org.apache.cassandra.db.IColumn#serializedSize()
+    */
+    public int serializedSize()
+    {
+        /*
+         * Size of a super-column is =
+         *   size of a name (UtfPrefix + length of the string)
+         * + 1 byte to indicate if the super-column has been deleted
+         * + 4 bytes for size of the sub-columns
+         * + 4 bytes for the number of sub-columns
+         * + size of all the sub-columns.
+        */
+
+    	/*
+    	 * We store the string as UTF-8 encoded, so when we calculate the length, it
+    	 * should be converted to UTF-8.
+    	 */
+    	/*
+    	 * We need to keep the way we are calculating the column size in sync with the
+    	 * way we are calculating the size for the column family serializer.
+    	 */
+    	return IColumn.UtfPrefix_ + name_.length + DBConstants.boolSize_ + DBConstants.intSize_ + DBConstants.intSize_ + getSizeOfAllColumns();
+    }
+
+    /**
+     * This calculates the exact size of the sub columns on the fly
+     */
+    int getSizeOfAllColumns()
+    {
+        int size = 0;
+        Collection<IColumn> subColumns = getSubColumns();
+        for ( IColumn subColumn : subColumns )
+        {
+            size += subColumn.serializedSize();
+        }
+        return size;
+    }
+
+    public void remove(byte[] columnName)
+    {
+    	columns_.remove(columnName);
+    }
+
+    public long timestamp()
+    {
+    	throw new UnsupportedOperationException("This operation is not supported for Super Columns.");
+    }
+
+    public long timestamp(byte[] columnName)
+    {
+    	IColumn column = columns_.get(columnName);
+    	if ( column instanceof SuperColumn )
+    		throw new UnsupportedOperationException("A super column cannot hold other super columns.");
+    	if ( column != null )
+    		return column.timestamp();
+    	throw new IllegalArgumentException("Timestamp was requested for a column that does not exist.");
+    }
+
+    public byte[] value()
+    {
+    	throw new UnsupportedOperationException("This operation is not supported for Super Columns.");
+    }
+
+    public byte[] value(byte[] columnName)
+    {
+    	IColumn column = columns_.get(columnName);
+    	if ( column != null )
+    		return column.value();
+    	throw new IllegalArgumentException("Value was requested for a column that does not exist.");
+    }
+
+    public void addColumn(IColumn column)
+    {
+    	if (!(column instanceof Column))
+    		throw new UnsupportedOperationException("A super column can only contain simple columns.");
+        try
+        {
+            getComparator().validate(column.name());
+        }
+        catch (Exception e)
+        {
+            throw new MarshalException("Invalid column name in supercolumn for " + getComparator().getClass().getName());
+        }
+    	IColumn oldColumn = columns_.get(column.name());
+    	if ( oldColumn == null )
+        {
+    		columns_.put(column.name(), column);
+            size_.addAndGet(column.size());
+        }
+    	else
+    	{
+    		if (((Column)oldColumn).comparePriority((Column)column) <= 0)
+            {
+    			columns_.put(column.name(), column);
+                int delta = (-1)*oldColumn.size();
+                /* subtract the size of the oldColumn */
+                size_.addAndGet(delta);
+                /* add the size of the new column */
+                size_.addAndGet(column.size());
+            }
+    	}
+    }
+
+    /*
+     * Go through each sub column if it exists then as it to resolve itself
+     * if the column does not exist then create it.
+     */
+    public void putColumn(IColumn column)
+    {
+        if (!(column instanceof SuperColumn))
+        {
+            throw new UnsupportedOperationException("Only Super column objects should be put here");
+        }
+        if (!Arrays.equals(name_, column.name()))
+        {
+            throw new IllegalArgumentException("The name should match the name of the current column or super column");
+        }
+
+        for (IColumn subColumn : column.getSubColumns())
+        {
+        	addColumn(subColumn);
+        }
+        if (column.getMarkedForDeleteAt() > markedForDeleteAt)
+        {
+            markForDeleteAt(column.getLocalDeletionTime(),  column.getMarkedForDeleteAt());
+        }
+    }
+
+    public int getObjectCount()
+    {
+    	return 1 + columns_.size();
+    }
+
+    public long getMarkedForDeleteAt() {
+        return markedForDeleteAt;
+    }
+
+    int getColumnCount()
+    {
+    	return columns_.size();
+    }
+
+    public IColumn diff(IColumn columnNew)
+    {
+    	IColumn columnDiff = new SuperColumn(columnNew.name(), ((SuperColumn)columnNew).getComparator());
+        if (columnNew.getMarkedForDeleteAt() > getMarkedForDeleteAt())
+        {
+            ((SuperColumn)columnDiff).markForDeleteAt(columnNew.getLocalDeletionTime(), columnNew.getMarkedForDeleteAt());
+        }
+
+        // (don't need to worry about columnNew containing subColumns that are shadowed by
+        // the delete tombstone, since columnNew was generated by CF.resolve, which
+        // takes care of those for us.)
+        for (IColumn subColumn : columnNew.getSubColumns())
+        {
+        	IColumn columnInternal = columns_.get(subColumn.name());
+        	if(columnInternal == null )
+        	{
+        		columnDiff.addColumn(subColumn);
+        	}
+        	else
+        	{
+            	IColumn subColumnDiff = columnInternal.diff(subColumn);
+        		if(subColumnDiff != null)
+        		{
+            		columnDiff.addColumn(subColumnDiff);
+        		}
+        	}
+        }
+
+        if (!columnDiff.getSubColumns().isEmpty() || columnNew.isMarkedForDelete())
+        	return columnDiff;
+        else
+        	return null;
+    }
+
+    public byte[] digest()
+    {
+    	byte[] xorHash = ArrayUtils.EMPTY_BYTE_ARRAY;
+    	if(name_ == null)
+    		return xorHash;
+    	xorHash = name_.clone();
+    	for(IColumn column : columns_.values())
+    	{
+			xorHash = FBUtilities.xor(xorHash, column.digest());
+    	}
+    	return xorHash;
+    }
+
+    public String getString(AbstractType comparator)
+    {
+    	StringBuilder sb = new StringBuilder();
+        sb.append("SuperColumn(");
+    	sb.append(comparator.getString(name_));
+
+        if (isMarkedForDelete()) {
+            sb.append(" -delete at ").append(getMarkedForDeleteAt()).append("-");
+        }
+
+        sb.append(" [");
+        sb.append(getComparator().getColumnsString(columns_.values()));
+        sb.append("])");
+
+        return sb.toString();
+    }
+
+    public int getLocalDeletionTime()
+    {
+        return localDeletionTime;
+    }
+
+    public void markForDeleteAt(int localDeleteTime, long timestamp)
+    {
+        this.localDeletionTime = localDeleteTime;
+        this.markedForDeleteAt = timestamp;
+    }
+}
+
+class SuperColumnSerializer implements ICompactSerializer<IColumn>
+{
+    private AbstractType comparator;
+
+    public SuperColumnSerializer(AbstractType comparator)
+    {
+        this.comparator = comparator;
+    }
+
+    public AbstractType getComparator()
+    {
+        return comparator;
+    }
+
+    public void serialize(IColumn column, DataOutputStream dos) throws IOException
+    {
+    	SuperColumn superColumn = (SuperColumn)column;
+        ColumnSerializer.writeName(column.name(), dos);
+        dos.writeInt(superColumn.getLocalDeletionTime());
+        dos.writeLong(superColumn.getMarkedForDeleteAt());
+
+        Collection<IColumn> columns  = column.getSubColumns();
+        int size = columns.size();
+        dos.writeInt(size);
+
+        dos.writeInt(superColumn.getSizeOfAllColumns());
+        for ( IColumn subColumn : columns )
+        {
+            Column.serializer().serialize(subColumn, dos);
+        }
+    }
+
+    public IColumn deserialize(DataInputStream dis) throws IOException
+    {
+        byte[] name = ColumnSerializer.readName(dis);
+        SuperColumn superColumn = new SuperColumn(name, comparator);
+        superColumn.markForDeleteAt(dis.readInt(), dis.readLong());
+        assert dis.available() > 0;
+
+        /* read the number of columns */
+        int size = dis.readInt();
+        /* read the size of all columns */
+        dis.readInt();
+        for ( int i = 0; i < size; ++i )
+        {
+            IColumn subColumn = Column.serializer().deserialize(dis);
+            superColumn.addColumn(subColumn);
+        }
+        return superColumn;
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/SystemTable.java b/src/java/org/apache/cassandra/db/SystemTable.java
index 3c6e8e8106..cb5776d31b 100644
--- a/src/java/org/apache/cassandra/db/SystemTable.java
+++ b/src/java/org/apache/cassandra/db/SystemTable.java
@@ -1,151 +1,151 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.IOException;
-import java.io.UnsupportedEncodingException;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.utils.BasicUtilities;
-import org.apache.cassandra.db.filter.NamesQueryFilter;
-import org.apache.cassandra.db.filter.QueryPath;
-import org.apache.cassandra.db.filter.QueryFilter;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class SystemTable
-{
-    private static Logger logger_ = Logger.getLogger(SystemTable.class);
-    public static final String LOCATION_CF = "LocationInfo";
-    private static final String LOCATION_KEY = "L"; // only one row in Location CF
-    private static final byte[] TOKEN = utf8("Token");
-    private static final byte[] GENERATION = utf8("Generation");
-
-    private static byte[] utf8(String str)
-    {
-        try
-        {
-            return str.getBytes("UTF-8");
-        }
-        catch (UnsupportedEncodingException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /*
-     * This method is used to update the SystemTable with the new token.
-    */
-    public static void updateToken(Token token) throws IOException
-    {
-        IPartitioner p = StorageService.getPartitioner();
-        Table table = Table.open(Table.SYSTEM_TABLE);
-        /* Retrieve the "LocationInfo" column family */
-        QueryFilter filter = new NamesQueryFilter(LOCATION_KEY, new QueryPath(LOCATION_CF), TOKEN);
-        ColumnFamily cf = table.getColumnFamilyStore(LOCATION_CF).getColumnFamily(filter);
-        long oldTokenColumnTimestamp = cf.getColumn(SystemTable.TOKEN).timestamp();
-        /* create the "Token" whose value is the new token. */
-        IColumn tokenColumn = new Column(SystemTable.TOKEN, p.getTokenFactory().toByteArray(token), oldTokenColumnTimestamp + 1);
-        /* replace the old "Token" column with this new one. */
-        if (logger_.isDebugEnabled())
-          logger_.debug("Replacing old token " + p.getTokenFactory().fromByteArray(cf.getColumn(SystemTable.TOKEN).value()) + " with " + token);
-        RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, LOCATION_KEY);
-        cf.addColumn(tokenColumn);
-        rm.add(cf);
-        rm.apply();
-    }
-
-    /*
-     * This method reads the system table and retrieves the metadata
-     * associated with this storage instance. Currently we store the
-     * metadata in a Column Family called LocatioInfo which has two
-     * columns namely "Token" and "Generation". This is the token that
-     * gets gossiped around and the generation info is used for FD.
-    */
-    public static StorageMetadata initMetadata() throws IOException
-    {
-        /* Read the system table to retrieve the storage ID and the generation */
-        Table table = Table.open(Table.SYSTEM_TABLE);
-        QueryFilter filter = new NamesQueryFilter(LOCATION_KEY, new QueryPath(LOCATION_CF), GENERATION);
-        ColumnFamily cf = table.getColumnFamilyStore(LOCATION_CF).getColumnFamily(filter);
-
-        IPartitioner p = StorageService.getPartitioner();
-        if (cf == null)
-        {
-            Token token = p.getDefaultToken();
-            int generation = 1;
-
-            RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, LOCATION_KEY);
-            cf = ColumnFamily.create(Table.SYSTEM_TABLE, SystemTable.LOCATION_CF);
-            cf.addColumn(new Column(TOKEN, p.getTokenFactory().toByteArray(token)));
-            cf.addColumn(new Column(GENERATION, BasicUtilities.intToByteArray(generation)) );
-            rm.add(cf);
-            rm.apply();
-            return new StorageMetadata(token, generation);
-        }
-
-        /* we crashed and came back up need to bump generation # */
-        IColumn tokenColumn = cf.getColumn(TOKEN);
-        Token token = p.getTokenFactory().fromByteArray(tokenColumn.value());
-
-        IColumn generation = cf.getColumn(GENERATION);
-        int gen = BasicUtilities.byteArrayToInt(generation.value()) + 1;
-
-        RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, LOCATION_KEY);
-        cf = ColumnFamily.create(Table.SYSTEM_TABLE, SystemTable.LOCATION_CF);
-        Column generation2 = new Column(GENERATION, BasicUtilities.intToByteArray(gen), generation.timestamp() + 1);
-        cf.addColumn(generation2);
-        rm.add(cf);
-        rm.apply();
-        return new StorageMetadata(token, gen);
-    }
-
-    public static class StorageMetadata
-    {
-        private Token myToken;
-        private int generation_;
-
-        StorageMetadata(Token storageId, int generation)
-        {
-            myToken = storageId;
-            generation_ = generation;
-        }
-
-        public Token getStorageId()
-        {
-            return myToken;
-        }
-
-        public void setStorageId(Token storageId)
-        {
-            myToken = storageId;
-        }
-
-        public int getGeneration()
-        {
-            return generation_;
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.IOException;
+import java.io.UnsupportedEncodingException;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.utils.BasicUtilities;
+import org.apache.cassandra.db.filter.NamesQueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.filter.QueryFilter;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SystemTable
+{
+    private static Logger logger_ = Logger.getLogger(SystemTable.class);
+    public static final String LOCATION_CF = "LocationInfo";
+    private static final String LOCATION_KEY = "L"; // only one row in Location CF
+    private static final byte[] TOKEN = utf8("Token");
+    private static final byte[] GENERATION = utf8("Generation");
+
+    private static byte[] utf8(String str)
+    {
+        try
+        {
+            return str.getBytes("UTF-8");
+        }
+        catch (UnsupportedEncodingException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    /*
+     * This method is used to update the SystemTable with the new token.
+    */
+    public static void updateToken(Token token) throws IOException
+    {
+        IPartitioner p = StorageService.getPartitioner();
+        Table table = Table.open(Table.SYSTEM_TABLE);
+        /* Retrieve the "LocationInfo" column family */
+        QueryFilter filter = new NamesQueryFilter(LOCATION_KEY, new QueryPath(LOCATION_CF), TOKEN);
+        ColumnFamily cf = table.getColumnFamilyStore(LOCATION_CF).getColumnFamily(filter);
+        long oldTokenColumnTimestamp = cf.getColumn(SystemTable.TOKEN).timestamp();
+        /* create the "Token" whose value is the new token. */
+        IColumn tokenColumn = new Column(SystemTable.TOKEN, p.getTokenFactory().toByteArray(token), oldTokenColumnTimestamp + 1);
+        /* replace the old "Token" column with this new one. */
+        if (logger_.isDebugEnabled())
+          logger_.debug("Replacing old token " + p.getTokenFactory().fromByteArray(cf.getColumn(SystemTable.TOKEN).value()) + " with " + token);
+        RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, LOCATION_KEY);
+        cf.addColumn(tokenColumn);
+        rm.add(cf);
+        rm.apply();
+    }
+
+    /*
+     * This method reads the system table and retrieves the metadata
+     * associated with this storage instance. Currently we store the
+     * metadata in a Column Family called LocatioInfo which has two
+     * columns namely "Token" and "Generation". This is the token that
+     * gets gossiped around and the generation info is used for FD.
+    */
+    public static StorageMetadata initMetadata() throws IOException
+    {
+        /* Read the system table to retrieve the storage ID and the generation */
+        Table table = Table.open(Table.SYSTEM_TABLE);
+        QueryFilter filter = new NamesQueryFilter(LOCATION_KEY, new QueryPath(LOCATION_CF), GENERATION);
+        ColumnFamily cf = table.getColumnFamilyStore(LOCATION_CF).getColumnFamily(filter);
+
+        IPartitioner p = StorageService.getPartitioner();
+        if (cf == null)
+        {
+            Token token = p.getDefaultToken();
+            int generation = 1;
+
+            RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, LOCATION_KEY);
+            cf = ColumnFamily.create(Table.SYSTEM_TABLE, SystemTable.LOCATION_CF);
+            cf.addColumn(new Column(TOKEN, p.getTokenFactory().toByteArray(token)));
+            cf.addColumn(new Column(GENERATION, BasicUtilities.intToByteArray(generation)) );
+            rm.add(cf);
+            rm.apply();
+            return new StorageMetadata(token, generation);
+        }
+
+        /* we crashed and came back up need to bump generation # */
+        IColumn tokenColumn = cf.getColumn(TOKEN);
+        Token token = p.getTokenFactory().fromByteArray(tokenColumn.value());
+
+        IColumn generation = cf.getColumn(GENERATION);
+        int gen = BasicUtilities.byteArrayToInt(generation.value()) + 1;
+
+        RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, LOCATION_KEY);
+        cf = ColumnFamily.create(Table.SYSTEM_TABLE, SystemTable.LOCATION_CF);
+        Column generation2 = new Column(GENERATION, BasicUtilities.intToByteArray(gen), generation.timestamp() + 1);
+        cf.addColumn(generation2);
+        rm.add(cf);
+        rm.apply();
+        return new StorageMetadata(token, gen);
+    }
+
+    public static class StorageMetadata
+    {
+        private Token myToken;
+        private int generation_;
+
+        StorageMetadata(Token storageId, int generation)
+        {
+            myToken = storageId;
+            generation_ = generation;
+        }
+
+        public Token getStorageId()
+        {
+            return myToken;
+        }
+
+        public void setStorageId(Token storageId)
+        {
+            myToken = storageId;
+        }
+
+        public int getGeneration()
+        {
+            return generation_;
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/Table.java b/src/java/org/apache/cassandra/db/Table.java
index 57a58e433d..52ac459783 100644
--- a/src/java/org/apache/cassandra/db/Table.java
+++ b/src/java/org/apache/cassandra/db/Table.java
@@ -1,768 +1,768 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.util.*;
-import java.io.IOException;
-import java.io.File;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-import java.util.concurrent.ExecutionException;
-
-import org.apache.commons.collections.IteratorUtils;
-import org.apache.commons.collections.Predicate;
-import org.apache.commons.lang.ArrayUtils;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.dht.BootstrapInitiateMessage;
-import org.apache.cassandra.dht.Range;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.io.SSTableReader;
-import org.apache.cassandra.io.FileStruct;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.net.io.IStreamComplete;
-import org.apache.cassandra.net.io.StreamContextManager;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.*;
-import org.apache.cassandra.db.filter.*;
-
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
-*/
-
-public class Table 
-{
-    public static final String SYSTEM_TABLE = "system";
-
-    private static Logger logger_ = Logger.getLogger(Table.class);
-    private static final String SNAPSHOT_SUBDIR_NAME = "snapshots";
-
-    /*
-     * This class represents the metadata of this Table. The metadata
-     * is basically the column family name and the ID associated with
-     * this column family. We use this ID in the Commit Log header to
-     * determine when a log file that has been rolled can be deleted.
-    */
-    public static class TableMetadata
-    {
-        private static HashMap<String,TableMetadata> tableMetadataMap_ = new HashMap<String,TableMetadata>();
-        private static Map<Integer, String> idCfMap_ = new HashMap<Integer, String>();
-        static
-        {
-            try
-            {
-                DatabaseDescriptor.storeMetadata();
-            }
-            catch (IOException e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-
-        public static synchronized Table.TableMetadata instance(String tableName) throws IOException
-        {
-            if ( tableMetadataMap_.get(tableName) == null )
-            {
-                tableMetadataMap_.put(tableName, new Table.TableMetadata());
-            }
-            return tableMetadataMap_.get(tableName);
-        }
-
-        /* The mapping between column family and the column type. */
-        private Map<String, String> cfTypeMap_ = new HashMap<String, String>();
-        private Map<String, Integer> cfIdMap_ = new HashMap<String, Integer>();
-
-        public void add(String cf, int id)
-        {
-            add(cf, id, "Standard");
-        }
-        
-        public void add(String cf, int id, String type)
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("adding " + cf + " as " + id);
-            assert !idCfMap_.containsKey(id);
-            cfIdMap_.put(cf, id);
-            idCfMap_.put(id, cf);
-            cfTypeMap_.put(cf, type);
-        }
-        
-        public boolean isEmpty()
-        {
-            return cfIdMap_.isEmpty();
-        }
-
-        int getColumnFamilyId(String columnFamily)
-        {
-            return cfIdMap_.get(columnFamily);
-        }
-
-        public static String getColumnFamilyName(int id)
-        {
-            return idCfMap_.get(id);
-        }
-        
-        String getColumnFamilyType(String cfName)
-        {
-            return cfTypeMap_.get(cfName);
-        }
-
-        Set<String> getColumnFamilies()
-        {
-            return cfIdMap_.keySet();
-        }
-        
-        int size()
-        {
-            return cfIdMap_.size();
-        }
-        
-        boolean isValidColumnFamily(String cfName)
-        {
-            return cfIdMap_.containsKey(cfName);
-        }
-
-        public String toString()
-        {
-            StringBuilder sb = new StringBuilder("");
-            Set<String> cfNames = cfIdMap_.keySet();
-            
-            for ( String cfName : cfNames )
-            {
-                sb.append(cfName);
-                sb.append("---->");
-                sb.append(cfIdMap_.get(cfName));
-                sb.append(System.getProperty("line.separator"));
-            }
-            
-            return sb.toString();
-        }
-
-        public static int getColumnFamilyCount()
-        {
-            return idCfMap_.size();
-        }
-    }
-
-    /**
-     * This is the callback handler that is invoked when we have
-     * completely been bootstrapped for a single file by a remote host.
-    */
-    public static class BootstrapCompletionHandler implements IStreamComplete
-    {                
-        public void onStreamCompletion(String host, StreamContextManager.StreamContext streamContext, StreamContextManager.StreamStatus streamStatus) throws IOException
-        {                        
-            /* Parse the stream context and the file to the list of SSTables in the associated Column Family Store. */            
-            if (streamContext.getTargetFile().contains("-Data.db"))
-            {
-                File file = new File( streamContext.getTargetFile() );
-                String fileName = file.getName();
-                String [] temp = null;
-                String tableName;
-                temp = fileName.split("-");
-                tableName = temp[0];
-                /*
-                 * If the file is a Data File we need to load the indicies associated
-                 * with this file. We also need to cache the file name in the SSTables
-                 * list of the associated Column Family. Also merge the CBF into the
-                 * sampler.
-                */                
-                SSTableReader sstable = SSTableReader.open(streamContext.getTargetFile());
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Merging the counting bloom filter in the sampler ...");                
-                String[] peices = FBUtilities.strip(fileName, "-");
-                Table.open(peices[0]).getColumnFamilyStore(peices[1]).addToList(sstable);                
-            }
-            
-            EndPoint to = new EndPoint(host, DatabaseDescriptor.getStoragePort());
-            if (logger_.isDebugEnabled())
-              logger_.debug("Sending a bootstrap terminate message with " + streamStatus + " to " + to);
-            /* Send a StreamStatusMessage object which may require the source node to re-stream certain files. */
-            StreamContextManager.StreamStatusMessage streamStatusMessage = new StreamContextManager.StreamStatusMessage(streamStatus);
-            Message message = StreamContextManager.StreamStatusMessage.makeStreamStatusMessage(streamStatusMessage);
-            MessagingService.getMessagingInstance().sendOneWay(message, to);           
-        }
-    }
-
-    public static class BootStrapInitiateVerbHandler implements IVerbHandler
-    {
-        /*
-         * Here we handle the BootstrapInitiateMessage. Here we get the
-         * array of StreamContexts. We get file names for the column
-         * families associated with the files and replace them with the
-         * file names as obtained from the column family store on the
-         * receiving end.
-        */
-        public void doVerb(Message message)
-        {
-            byte[] body = message.getMessageBody();
-            DataInputBuffer bufIn = new DataInputBuffer();
-            bufIn.reset(body, body.length); 
-            
-            try
-            {
-                BootstrapInitiateMessage biMsg = BootstrapInitiateMessage.serializer().deserialize(bufIn);
-                StreamContextManager.StreamContext[] streamContexts = biMsg.getStreamContext();                
-                
-                Map<String, String> fileNames = getNewNames(streamContexts);
-                /*
-                 * For each of stream context's in the incoming message
-                 * generate the new file names and store the new file names
-                 * in the StreamContextManager.
-                */
-                for (StreamContextManager.StreamContext streamContext : streamContexts )
-                {                    
-                    StreamContextManager.StreamStatus streamStatus = new StreamContextManager.StreamStatus(streamContext.getTargetFile(), streamContext.getExpectedBytes() );
-                    File sourceFile = new File( streamContext.getTargetFile() );
-                    String[] peices = FBUtilities.strip(sourceFile.getName(), "-");
-                    String newFileName = fileNames.get( peices[1] + "-" + peices[2] );
-                    
-                    String file = DatabaseDescriptor.getDataFileLocationForTable(streamContext.getTable()) + File.separator + newFileName + "-Data.db";
-                    if (logger_.isDebugEnabled())
-                      logger_.debug("Received Data from  : " + message.getFrom() + " " + streamContext.getTargetFile() + " " + file);
-                    streamContext.setTargetFile(file);
-                    addStreamContext(message.getFrom().getHost(), streamContext, streamStatus);                                            
-                }    
-                                             
-                StreamContextManager.registerStreamCompletionHandler(message.getFrom().getHost(), new Table.BootstrapCompletionHandler());
-                /* Send a bootstrap initiation done message to execute on default stage. */
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Sending a bootstrap initiate done message ...");
-                Message doneMessage = new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapInitiateDoneVerbHandler_, new byte[0] );
-                MessagingService.getMessagingInstance().sendOneWay(doneMessage, message.getFrom());
-            }
-            catch ( IOException ex )
-            {
-                logger_.info(LogUtil.throwableToString(ex));
-            }
-        }
-        
-        private Map<String, String> getNewNames(StreamContextManager.StreamContext[] streamContexts) throws IOException
-        {
-            /* 
-             * Mapping for each file with unique CF-i ---> new file name. For eg.
-             * for a file with name <Table>-<CF>-<i>-Data.db there is a corresponding
-             * <Table>-<CF>-<i>-Index.db. We maintain a mapping from <CF>-<i> to a newly
-             * generated file name.
-            */
-            Map<String, String> fileNames = new HashMap<String, String>();
-            /* Get the distinct entries from StreamContexts i.e have one entry per Data/Index file combination */
-            Set<String> distinctEntries = new HashSet<String>();
-            for ( StreamContextManager.StreamContext streamContext : streamContexts )
-            {
-                String[] peices = FBUtilities.strip(streamContext.getTargetFile(), "-");
-                distinctEntries.add(peices[0] + "-" + peices[1] + "-" + peices[2]);
-            }
-            
-            /* Generate unique file names per entry */
-            for ( String distinctEntry : distinctEntries )
-            {
-                String tableName;
-                String[] peices = FBUtilities.strip(distinctEntry, "-");
-                tableName = peices[0];
-                Table table = Table.open( tableName );
-                Map<String, ColumnFamilyStore> columnFamilyStores = table.getColumnFamilyStores();
-
-                ColumnFamilyStore cfStore = columnFamilyStores.get(peices[1]);
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Generating file name for " + distinctEntry + " ...");
-                fileNames.put(distinctEntry, cfStore.getNextFileName());
-            }
-            
-            return fileNames;
-        }
-
-        private void addStreamContext(String host, StreamContextManager.StreamContext streamContext, StreamContextManager.StreamStatus streamStatus)
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Adding stream context " + streamContext + " for " + host + " ...");
-            StreamContextManager.addStreamContext(host, streamContext, streamStatus);
-        }
-    }
-    
-    /* Used to lock the factory for creation of Table instance */
-    private static Lock createLock_ = new ReentrantLock();
-    private static Map<String, Table> instances_ = new HashMap<String, Table>();
-    /* Table name. */
-    private String table_;
-    /* Handle to the Table Metadata */
-    private Table.TableMetadata tableMetadata_;
-    /* ColumnFamilyStore per column family */
-    private Map<String, ColumnFamilyStore> columnFamilyStores_ = new HashMap<String, ColumnFamilyStore>();
-    // cache application CFs since Range queries ask for them a _lot_
-    private SortedSet<String> applicationColumnFamilies_;
-
-    public static Table open(String table) throws IOException
-    {
-        Table tableInstance = instances_.get(table);
-        /*
-         * Read the config and figure the column families for this table.
-         * Set the isConfigured flag so that we do not read config all the
-         * time.
-        */
-        if ( tableInstance == null )
-        {
-            Table.createLock_.lock();
-            try
-            {
-                if ( tableInstance == null )
-                {
-                    tableInstance = new Table(table);
-                    instances_.put(table, tableInstance);
-                }
-            }
-            finally
-            {
-                createLock_.unlock();
-            }
-        }
-        return tableInstance;
-    }
-        
-    public Set<String> getColumnFamilies()
-    {
-        return tableMetadata_.getColumnFamilies();
-    }
-
-    Map<String, ColumnFamilyStore> getColumnFamilyStores()
-    {
-        return columnFamilyStores_;
-    }
-
-    public ColumnFamilyStore getColumnFamilyStore(String cfName)
-    {
-        return columnFamilyStores_.get(cfName);
-    }
-
-    /*
-     * This method is called to obtain statistics about
-     * the table. It will return statistics about all
-     * the column families that make up this table. 
-    */
-    public String tableStats(String newLineSeparator, java.text.DecimalFormat df)
-    {
-        StringBuilder sb = new StringBuilder();
-        sb.append(table_ + " statistics :");
-        sb.append(newLineSeparator);
-        int oldLength = sb.toString().length();
-        
-        Set<String> cfNames = columnFamilyStores_.keySet();
-        for ( String cfName : cfNames )
-        {
-            ColumnFamilyStore cfStore = columnFamilyStores_.get(cfName);
-            sb.append(cfStore.cfStats(newLineSeparator));
-        }
-        int newLength = sb.toString().length();
-        
-        /* Don't show anything if there is nothing to show. */
-        if ( newLength == oldLength )
-            return "";
-        
-        return sb.toString();
-    }
-
-    public void onStart() throws IOException
-    {
-        for (String columnFamily : tableMetadata_.getColumnFamilies())
-        {
-            columnFamilyStores_.get(columnFamily).onStart();
-        }
-    }
-    
-    /** 
-     * Do a cleanup of keys that do not belong locally.
-     */
-    public void forceCleanup()
-    {
-        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
-        for ( String columnFamily : columnFamilies )
-        {
-            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
-            if ( cfStore != null )
-                cfStore.forceCleanup();
-        }   
-    }
-    
-    
-    /**
-     * Take a snapshot of the entire set of column families with a given timestamp.
-     * 
-     * @param clientSuppliedName the tag associated with the name of the snapshot.  This
-     *                           value can be null.
-     */
-    public void snapshot(String clientSuppliedName) throws IOException
-    {
-        String snapshotName = Long.toString(System.currentTimeMillis());
-        if (clientSuppliedName != null && !clientSuppliedName.equals(""))
-        {
-            snapshotName = snapshotName + "-" + clientSuppliedName;
-        }
-
-        for (ColumnFamilyStore cfStore : columnFamilyStores_.values())
-        {
-            cfStore.snapshot(snapshotName);
-        }
-    }
-
-
-    /**
-     * Clear all the snapshots for a given table.
-     */
-    public void clearSnapshot() throws IOException
-    {
-        for (String dataDirPath : DatabaseDescriptor.getAllDataFileLocations())
-        {
-            String snapshotPath = dataDirPath + File.separator + table_ + File.separator + SNAPSHOT_SUBDIR_NAME;
-            File snapshotDir = new File(snapshotPath);
-            if (snapshotDir.exists())
-            {
-                if (logger_.isDebugEnabled())
-                    logger_.debug("Removing snapshot directory " + snapshotPath);
-                if (!FileUtils.deleteDir(snapshotDir))
-                    throw new IOException("Could not clear snapshot directory " + snapshotPath);
-            }
-        }
-    }
-
-    /*
-     * This method is invoked only during a bootstrap process. We basically
-     * do a complete compaction since we can figure out based on the ranges
-     * whether the files need to be split.
-    */
-    public boolean forceCompaction(List<Range> ranges, EndPoint target, List<String> fileList)
-    {
-        boolean result = true;
-        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
-        for ( String columnFamily : columnFamilies )
-        {
-            if ( !isApplicationColumnFamily(columnFamily) )
-                continue;
-            
-            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
-            if ( cfStore != null )
-            {
-                /* Counting Bloom Filter for the Column Family */
-                cfStore.forceCompaction(ranges, target, 0, fileList);                
-            }
-        }
-        return result;
-    }
-    
-    /*
-     * This method is an ADMIN operation to force compaction
-     * of all SSTables on disk. 
-    */
-    public void forceCompaction()
-    {
-        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
-        for ( String columnFamily : columnFamilies )
-        {
-            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
-            if ( cfStore != null )
-                MinorCompactionManager.instance().submitMajor(cfStore, 0);
-        }
-    }
-
-    /*
-     * Get the list of all SSTables on disk.  Not safe unless you aquire the CFS readlocks!
-    */
-    public List<SSTableReader> getAllSSTablesOnDisk()
-    {
-        List<SSTableReader> list = new ArrayList<SSTableReader>();
-        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
-        for ( String columnFamily : columnFamilies )
-        {
-            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
-            if ( cfStore != null )
-                list.addAll(cfStore.getSSTables());
-        }
-        return list;
-    }
-
-    private Table(String table) throws IOException
-    {
-        table_ = table;
-        tableMetadata_ = Table.TableMetadata.instance(table);
-        for (String columnFamily : tableMetadata_.getColumnFamilies())
-        {
-            columnFamilyStores_.put(columnFamily, ColumnFamilyStore.getColumnFamilyStore(table, columnFamily));
-        }
-    }
-
-    boolean isApplicationColumnFamily(String columnFamily)
-    {
-        return DatabaseDescriptor.isApplicationColumnFamily(columnFamily);
-    }
-
-    int getColumnFamilyId(String columnFamily)
-    {
-        return tableMetadata_.getColumnFamilyId(columnFamily);
-    }
-
-    boolean isValidColumnFamily(String columnFamily)
-    {
-        return tableMetadata_.isValidColumnFamily(columnFamily);
-    }
-
-    /**
-     * Selects the row associated with the given key.
-    */
-    @Deprecated // CF should be our atom of work, not Row
-    public Row get(String key) throws IOException
-    {
-        Row row = new Row(table_, key);
-        for (String columnFamily : getColumnFamilies())
-        {
-            ColumnFamily cf = get(key, columnFamily);
-            if (cf != null)
-            {
-                row.addColumnFamily(cf);
-            }
-        }
-        return row;
-    }
-
-
-    /**
-     * Selects the specified column family for the specified key.
-    */
-    @Deprecated // single CFs could be larger than memory
-    public ColumnFamily get(String key, String cfName) throws IOException
-    {
-        ColumnFamilyStore cfStore = columnFamilyStores_.get(cfName);
-        assert cfStore != null : "Column family " + cfName + " has not been defined";
-        return cfStore.getColumnFamily(new IdentityQueryFilter(key, new QueryPath(cfName)));
-    }
-
-    /**
-     * Selects only the specified column family for the specified key.
-    */
-    @Deprecated
-    public Row getRow(String key, String cfName) throws IOException
-    {
-        Row row = new Row(table_, key);
-        ColumnFamily columnFamily = get(key, cfName);
-        if ( columnFamily != null )
-        	row.addColumnFamily(columnFamily);
-        return row;
-    }
-    
-    public Row getRow(QueryFilter filter) throws IOException
-    {
-        ColumnFamilyStore cfStore = columnFamilyStores_.get(filter.getColumnFamilyName());
-        Row row = new Row(table_, filter.key);
-        ColumnFamily columnFamily = cfStore.getColumnFamily(filter);
-        if (columnFamily != null)
-            row.addColumnFamily(columnFamily);
-        return row;
-    }
-
-    /**
-     * This method adds the row to the Commit Log associated with this table.
-     * Once this happens the data associated with the individual column families
-     * is also written to the column family store's memtable.
-    */
-    void apply(Row row) throws IOException
-    {
-        CommitLog.CommitLogContext cLogCtx = CommitLog.open().add(row);
-
-        for (ColumnFamily columnFamily : row.getColumnFamilies())
-        {
-            ColumnFamilyStore cfStore = columnFamilyStores_.get(columnFamily.name());
-            cfStore.apply(row.key(), columnFamily, cLogCtx);
-        }
-    }
-
-    void applyNow(Row row) throws IOException
-    {
-        String key = row.key();
-        for (ColumnFamily columnFamily : row.getColumnFamilies())
-        {
-            ColumnFamilyStore cfStore = columnFamilyStores_.get(columnFamily.name());
-            cfStore.applyNow( key, columnFamily );
-        }
-    }
-
-    public void flush(boolean fRecovery) throws IOException
-    {
-        Set<String> cfNames = columnFamilyStores_.keySet();
-        for ( String cfName : cfNames )
-        {
-            if (fRecovery) {
-                columnFamilyStores_.get(cfName).flushMemtableOnRecovery();
-            } else {
-                columnFamilyStores_.get(cfName).forceFlush();
-            }
-        }
-    }
-
-    // for binary load path.  skips commitlog.
-    void load(Row row) throws IOException
-    {
-        String key = row.key();
-                
-        for (ColumnFamily columnFamily : row.getColumnFamilies())
-        {
-            Collection<IColumn> columns = columnFamily.getSortedColumns();
-            for(IColumn column : columns)
-            {
-                ColumnFamilyStore cfStore = columnFamilyStores_.get(column.name());
-                cfStore.applyBinary(key, column.value());
-        	}
-        }
-        row.clear();
-    }
-
-    public SortedSet<String> getApplicationColumnFamilies()
-    {
-        if (applicationColumnFamilies_ == null)
-        {
-            applicationColumnFamilies_ = new TreeSet<String>();
-            for (String cfName : getColumnFamilies())
-            {
-                if (DatabaseDescriptor.isApplicationColumnFamily(cfName))
-                {
-                    applicationColumnFamilies_.add(cfName);
-                }
-            }
-        }
-        return applicationColumnFamilies_;
-    }
-
-    /**
-     * @param startWith key to start with, inclusive.  empty string = start at beginning.
-     * @param stopAt key to stop at, inclusive.  empty string = stop only when keys are exhausted.
-     * @param maxResults
-     * @return list of keys between startWith and stopAt
-     */
-    public List<String> getKeyRange(String columnFamily, final String startWith, final String stopAt, int maxResults)
-    throws IOException, ExecutionException, InterruptedException
-    {
-        assert getColumnFamilyStore(columnFamily) != null : columnFamily;
-
-        getColumnFamilyStore(columnFamily).getReadLock().lock();
-        try
-        {
-            return getKeyRangeUnsafe(columnFamily, startWith, stopAt, maxResults);
-        }
-        finally
-        {
-            getColumnFamilyStore(columnFamily).getReadLock().unlock();
-        }
-    }
-
-    private List<String> getKeyRangeUnsafe(final String cfName, final String startWith, final String stopAt, int maxResults) throws IOException, ExecutionException, InterruptedException
-    {
-        // (OPP key decoration is a no-op so using the "decorated" comparator against raw keys is fine)
-        final Comparator<String> comparator = StorageService.getPartitioner().getDecoratedKeyComparator();
-
-        // create a CollatedIterator that will return unique keys from different sources
-        // (current memtable, historical memtables, and SSTables) in the correct order.
-        List<Iterator<String>> iterators = new ArrayList<Iterator<String>>();
-        ColumnFamilyStore cfs = getColumnFamilyStore(cfName);
-
-        // we iterate through memtables with a priority queue to avoid more sorting than necessary.
-        // this predicate throws out the keys before the start of our range.
-        Predicate p = new Predicate()
-        {
-            public boolean evaluate(Object key)
-            {
-                String st = (String)key;
-                return comparator.compare(startWith, st) <= 0 && (stopAt.isEmpty() || comparator.compare(st, stopAt) <= 0);
-            }
-        };
-
-        // current memtable keys.  have to go through the CFS api for locking.
-        iterators.add(IteratorUtils.filteredIterator(cfs.memtableKeyIterator(), p));
-        // historical memtables
-        for (Memtable memtable : ColumnFamilyStore.getUnflushedMemtables(cfName))
-        {
-            iterators.add(IteratorUtils.filteredIterator(Memtable.getKeyIterator(memtable.getKeys()), p));
-        }
-
-        // sstables
-        for (SSTableReader sstable : cfs.getSSTables())
-        {
-            FileStruct fs = sstable.getFileStruct();
-            fs.seekTo(startWith);
-            iterators.add(fs);
-        }
-
-        Iterator<String> collated = IteratorUtils.collatedIterator(comparator, iterators);
-        Iterable<String> reduced = new ReducingIterator<String>(collated) {
-            String current;
-
-            public void reduce(String current)
-            {
-                 this.current = current;
-            }
-
-            protected String getReduced()
-            {
-                return current;
-            }
-        };
-
-        try
-        {
-            // pull keys out of the CollatedIterator.  checking tombstone status is expensive,
-            // so we set an arbitrary limit on how many we'll do at once.
-            List<String> keys = new ArrayList<String>();
-            for (String current : reduced)
-            {
-                if (!stopAt.isEmpty() && comparator.compare(stopAt, current) < 0)
-                {
-                    break;
-                }
-                // make sure there is actually non-tombstone content associated w/ this key
-                // TODO record the key source(s) somehow and only check that source (e.g., memtable or sstable)
-                QueryFilter filter = new SliceQueryFilter(current, new QueryPath(cfName), ArrayUtils.EMPTY_BYTE_ARRAY, ArrayUtils.EMPTY_BYTE_ARRAY, true, 1);
-                if (cfs.getColumnFamily(filter, Integer.MAX_VALUE) != null)
-                {
-                    keys.add(current);
-                }
-                if (keys.size() >= maxResults)
-                {
-                    break;
-                }
-            }
-            return keys;
-        }
-        finally
-        {
-            for (Iterator iter : iterators)
-            {
-                if (iter instanceof FileStruct)
-                {
-                    ((FileStruct)iter).close();
-                }
-            }
-        }
-    }
-
-    public static String getSnapshotPath(String dataDirPath, String tableName, String snapshotName)
-    {
-        return dataDirPath + File.separator + tableName + File.separator + SNAPSHOT_SUBDIR_NAME + File.separator + snapshotName;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.*;
+import java.io.IOException;
+import java.io.File;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.commons.collections.IteratorUtils;
+import org.apache.commons.collections.Predicate;
+import org.apache.commons.lang.ArrayUtils;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.BootstrapInitiateMessage;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.SSTableReader;
+import org.apache.cassandra.io.FileStruct;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.io.IStreamComplete;
+import org.apache.cassandra.net.io.StreamContextManager;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.*;
+import org.apache.cassandra.db.filter.*;
+
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+*/
+
+public class Table 
+{
+    public static final String SYSTEM_TABLE = "system";
+
+    private static Logger logger_ = Logger.getLogger(Table.class);
+    private static final String SNAPSHOT_SUBDIR_NAME = "snapshots";
+
+    /*
+     * This class represents the metadata of this Table. The metadata
+     * is basically the column family name and the ID associated with
+     * this column family. We use this ID in the Commit Log header to
+     * determine when a log file that has been rolled can be deleted.
+    */
+    public static class TableMetadata
+    {
+        private static HashMap<String,TableMetadata> tableMetadataMap_ = new HashMap<String,TableMetadata>();
+        private static Map<Integer, String> idCfMap_ = new HashMap<Integer, String>();
+        static
+        {
+            try
+            {
+                DatabaseDescriptor.storeMetadata();
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+
+        public static synchronized Table.TableMetadata instance(String tableName) throws IOException
+        {
+            if ( tableMetadataMap_.get(tableName) == null )
+            {
+                tableMetadataMap_.put(tableName, new Table.TableMetadata());
+            }
+            return tableMetadataMap_.get(tableName);
+        }
+
+        /* The mapping between column family and the column type. */
+        private Map<String, String> cfTypeMap_ = new HashMap<String, String>();
+        private Map<String, Integer> cfIdMap_ = new HashMap<String, Integer>();
+
+        public void add(String cf, int id)
+        {
+            add(cf, id, "Standard");
+        }
+        
+        public void add(String cf, int id, String type)
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("adding " + cf + " as " + id);
+            assert !idCfMap_.containsKey(id);
+            cfIdMap_.put(cf, id);
+            idCfMap_.put(id, cf);
+            cfTypeMap_.put(cf, type);
+        }
+        
+        public boolean isEmpty()
+        {
+            return cfIdMap_.isEmpty();
+        }
+
+        int getColumnFamilyId(String columnFamily)
+        {
+            return cfIdMap_.get(columnFamily);
+        }
+
+        public static String getColumnFamilyName(int id)
+        {
+            return idCfMap_.get(id);
+        }
+        
+        String getColumnFamilyType(String cfName)
+        {
+            return cfTypeMap_.get(cfName);
+        }
+
+        Set<String> getColumnFamilies()
+        {
+            return cfIdMap_.keySet();
+        }
+        
+        int size()
+        {
+            return cfIdMap_.size();
+        }
+        
+        boolean isValidColumnFamily(String cfName)
+        {
+            return cfIdMap_.containsKey(cfName);
+        }
+
+        public String toString()
+        {
+            StringBuilder sb = new StringBuilder("");
+            Set<String> cfNames = cfIdMap_.keySet();
+            
+            for ( String cfName : cfNames )
+            {
+                sb.append(cfName);
+                sb.append("---->");
+                sb.append(cfIdMap_.get(cfName));
+                sb.append(System.getProperty("line.separator"));
+            }
+            
+            return sb.toString();
+        }
+
+        public static int getColumnFamilyCount()
+        {
+            return idCfMap_.size();
+        }
+    }
+
+    /**
+     * This is the callback handler that is invoked when we have
+     * completely been bootstrapped for a single file by a remote host.
+    */
+    public static class BootstrapCompletionHandler implements IStreamComplete
+    {                
+        public void onStreamCompletion(String host, StreamContextManager.StreamContext streamContext, StreamContextManager.StreamStatus streamStatus) throws IOException
+        {                        
+            /* Parse the stream context and the file to the list of SSTables in the associated Column Family Store. */            
+            if (streamContext.getTargetFile().contains("-Data.db"))
+            {
+                File file = new File( streamContext.getTargetFile() );
+                String fileName = file.getName();
+                String [] temp = null;
+                String tableName;
+                temp = fileName.split("-");
+                tableName = temp[0];
+                /*
+                 * If the file is a Data File we need to load the indicies associated
+                 * with this file. We also need to cache the file name in the SSTables
+                 * list of the associated Column Family. Also merge the CBF into the
+                 * sampler.
+                */                
+                SSTableReader sstable = SSTableReader.open(streamContext.getTargetFile());
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Merging the counting bloom filter in the sampler ...");                
+                String[] peices = FBUtilities.strip(fileName, "-");
+                Table.open(peices[0]).getColumnFamilyStore(peices[1]).addToList(sstable);                
+            }
+            
+            EndPoint to = new EndPoint(host, DatabaseDescriptor.getStoragePort());
+            if (logger_.isDebugEnabled())
+              logger_.debug("Sending a bootstrap terminate message with " + streamStatus + " to " + to);
+            /* Send a StreamStatusMessage object which may require the source node to re-stream certain files. */
+            StreamContextManager.StreamStatusMessage streamStatusMessage = new StreamContextManager.StreamStatusMessage(streamStatus);
+            Message message = StreamContextManager.StreamStatusMessage.makeStreamStatusMessage(streamStatusMessage);
+            MessagingService.getMessagingInstance().sendOneWay(message, to);           
+        }
+    }
+
+    public static class BootStrapInitiateVerbHandler implements IVerbHandler
+    {
+        /*
+         * Here we handle the BootstrapInitiateMessage. Here we get the
+         * array of StreamContexts. We get file names for the column
+         * families associated with the files and replace them with the
+         * file names as obtained from the column family store on the
+         * receiving end.
+        */
+        public void doVerb(Message message)
+        {
+            byte[] body = message.getMessageBody();
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length); 
+            
+            try
+            {
+                BootstrapInitiateMessage biMsg = BootstrapInitiateMessage.serializer().deserialize(bufIn);
+                StreamContextManager.StreamContext[] streamContexts = biMsg.getStreamContext();                
+                
+                Map<String, String> fileNames = getNewNames(streamContexts);
+                /*
+                 * For each of stream context's in the incoming message
+                 * generate the new file names and store the new file names
+                 * in the StreamContextManager.
+                */
+                for (StreamContextManager.StreamContext streamContext : streamContexts )
+                {                    
+                    StreamContextManager.StreamStatus streamStatus = new StreamContextManager.StreamStatus(streamContext.getTargetFile(), streamContext.getExpectedBytes() );
+                    File sourceFile = new File( streamContext.getTargetFile() );
+                    String[] peices = FBUtilities.strip(sourceFile.getName(), "-");
+                    String newFileName = fileNames.get( peices[1] + "-" + peices[2] );
+                    
+                    String file = DatabaseDescriptor.getDataFileLocationForTable(streamContext.getTable()) + File.separator + newFileName + "-Data.db";
+                    if (logger_.isDebugEnabled())
+                      logger_.debug("Received Data from  : " + message.getFrom() + " " + streamContext.getTargetFile() + " " + file);
+                    streamContext.setTargetFile(file);
+                    addStreamContext(message.getFrom().getHost(), streamContext, streamStatus);                                            
+                }    
+                                             
+                StreamContextManager.registerStreamCompletionHandler(message.getFrom().getHost(), new Table.BootstrapCompletionHandler());
+                /* Send a bootstrap initiation done message to execute on default stage. */
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Sending a bootstrap initiate done message ...");
+                Message doneMessage = new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapInitiateDoneVerbHandler_, new byte[0] );
+                MessagingService.getMessagingInstance().sendOneWay(doneMessage, message.getFrom());
+            }
+            catch ( IOException ex )
+            {
+                logger_.info(LogUtil.throwableToString(ex));
+            }
+        }
+        
+        private Map<String, String> getNewNames(StreamContextManager.StreamContext[] streamContexts) throws IOException
+        {
+            /* 
+             * Mapping for each file with unique CF-i ---> new file name. For eg.
+             * for a file with name <Table>-<CF>-<i>-Data.db there is a corresponding
+             * <Table>-<CF>-<i>-Index.db. We maintain a mapping from <CF>-<i> to a newly
+             * generated file name.
+            */
+            Map<String, String> fileNames = new HashMap<String, String>();
+            /* Get the distinct entries from StreamContexts i.e have one entry per Data/Index file combination */
+            Set<String> distinctEntries = new HashSet<String>();
+            for ( StreamContextManager.StreamContext streamContext : streamContexts )
+            {
+                String[] peices = FBUtilities.strip(streamContext.getTargetFile(), "-");
+                distinctEntries.add(peices[0] + "-" + peices[1] + "-" + peices[2]);
+            }
+            
+            /* Generate unique file names per entry */
+            for ( String distinctEntry : distinctEntries )
+            {
+                String tableName;
+                String[] peices = FBUtilities.strip(distinctEntry, "-");
+                tableName = peices[0];
+                Table table = Table.open( tableName );
+                Map<String, ColumnFamilyStore> columnFamilyStores = table.getColumnFamilyStores();
+
+                ColumnFamilyStore cfStore = columnFamilyStores.get(peices[1]);
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Generating file name for " + distinctEntry + " ...");
+                fileNames.put(distinctEntry, cfStore.getNextFileName());
+            }
+            
+            return fileNames;
+        }
+
+        private void addStreamContext(String host, StreamContextManager.StreamContext streamContext, StreamContextManager.StreamStatus streamStatus)
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Adding stream context " + streamContext + " for " + host + " ...");
+            StreamContextManager.addStreamContext(host, streamContext, streamStatus);
+        }
+    }
+    
+    /* Used to lock the factory for creation of Table instance */
+    private static Lock createLock_ = new ReentrantLock();
+    private static Map<String, Table> instances_ = new HashMap<String, Table>();
+    /* Table name. */
+    private String table_;
+    /* Handle to the Table Metadata */
+    private Table.TableMetadata tableMetadata_;
+    /* ColumnFamilyStore per column family */
+    private Map<String, ColumnFamilyStore> columnFamilyStores_ = new HashMap<String, ColumnFamilyStore>();
+    // cache application CFs since Range queries ask for them a _lot_
+    private SortedSet<String> applicationColumnFamilies_;
+
+    public static Table open(String table) throws IOException
+    {
+        Table tableInstance = instances_.get(table);
+        /*
+         * Read the config and figure the column families for this table.
+         * Set the isConfigured flag so that we do not read config all the
+         * time.
+        */
+        if ( tableInstance == null )
+        {
+            Table.createLock_.lock();
+            try
+            {
+                if ( tableInstance == null )
+                {
+                    tableInstance = new Table(table);
+                    instances_.put(table, tableInstance);
+                }
+            }
+            finally
+            {
+                createLock_.unlock();
+            }
+        }
+        return tableInstance;
+    }
+        
+    public Set<String> getColumnFamilies()
+    {
+        return tableMetadata_.getColumnFamilies();
+    }
+
+    Map<String, ColumnFamilyStore> getColumnFamilyStores()
+    {
+        return columnFamilyStores_;
+    }
+
+    public ColumnFamilyStore getColumnFamilyStore(String cfName)
+    {
+        return columnFamilyStores_.get(cfName);
+    }
+
+    /*
+     * This method is called to obtain statistics about
+     * the table. It will return statistics about all
+     * the column families that make up this table. 
+    */
+    public String tableStats(String newLineSeparator, java.text.DecimalFormat df)
+    {
+        StringBuilder sb = new StringBuilder();
+        sb.append(table_ + " statistics :");
+        sb.append(newLineSeparator);
+        int oldLength = sb.toString().length();
+        
+        Set<String> cfNames = columnFamilyStores_.keySet();
+        for ( String cfName : cfNames )
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get(cfName);
+            sb.append(cfStore.cfStats(newLineSeparator));
+        }
+        int newLength = sb.toString().length();
+        
+        /* Don't show anything if there is nothing to show. */
+        if ( newLength == oldLength )
+            return "";
+        
+        return sb.toString();
+    }
+
+    public void onStart() throws IOException
+    {
+        for (String columnFamily : tableMetadata_.getColumnFamilies())
+        {
+            columnFamilyStores_.get(columnFamily).onStart();
+        }
+    }
+    
+    /** 
+     * Do a cleanup of keys that do not belong locally.
+     */
+    public void forceCleanup()
+    {
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        for ( String columnFamily : columnFamilies )
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
+            if ( cfStore != null )
+                cfStore.forceCleanup();
+        }   
+    }
+    
+    
+    /**
+     * Take a snapshot of the entire set of column families with a given timestamp.
+     * 
+     * @param clientSuppliedName the tag associated with the name of the snapshot.  This
+     *                           value can be null.
+     */
+    public void snapshot(String clientSuppliedName) throws IOException
+    {
+        String snapshotName = Long.toString(System.currentTimeMillis());
+        if (clientSuppliedName != null && !clientSuppliedName.equals(""))
+        {
+            snapshotName = snapshotName + "-" + clientSuppliedName;
+        }
+
+        for (ColumnFamilyStore cfStore : columnFamilyStores_.values())
+        {
+            cfStore.snapshot(snapshotName);
+        }
+    }
+
+
+    /**
+     * Clear all the snapshots for a given table.
+     */
+    public void clearSnapshot() throws IOException
+    {
+        for (String dataDirPath : DatabaseDescriptor.getAllDataFileLocations())
+        {
+            String snapshotPath = dataDirPath + File.separator + table_ + File.separator + SNAPSHOT_SUBDIR_NAME;
+            File snapshotDir = new File(snapshotPath);
+            if (snapshotDir.exists())
+            {
+                if (logger_.isDebugEnabled())
+                    logger_.debug("Removing snapshot directory " + snapshotPath);
+                if (!FileUtils.deleteDir(snapshotDir))
+                    throw new IOException("Could not clear snapshot directory " + snapshotPath);
+            }
+        }
+    }
+
+    /*
+     * This method is invoked only during a bootstrap process. We basically
+     * do a complete compaction since we can figure out based on the ranges
+     * whether the files need to be split.
+    */
+    public boolean forceCompaction(List<Range> ranges, EndPoint target, List<String> fileList)
+    {
+        boolean result = true;
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        for ( String columnFamily : columnFamilies )
+        {
+            if ( !isApplicationColumnFamily(columnFamily) )
+                continue;
+            
+            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
+            if ( cfStore != null )
+            {
+                /* Counting Bloom Filter for the Column Family */
+                cfStore.forceCompaction(ranges, target, 0, fileList);                
+            }
+        }
+        return result;
+    }
+    
+    /*
+     * This method is an ADMIN operation to force compaction
+     * of all SSTables on disk. 
+    */
+    public void forceCompaction()
+    {
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        for ( String columnFamily : columnFamilies )
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
+            if ( cfStore != null )
+                MinorCompactionManager.instance().submitMajor(cfStore, 0);
+        }
+    }
+
+    /*
+     * Get the list of all SSTables on disk.  Not safe unless you aquire the CFS readlocks!
+    */
+    public List<SSTableReader> getAllSSTablesOnDisk()
+    {
+        List<SSTableReader> list = new ArrayList<SSTableReader>();
+        Set<String> columnFamilies = tableMetadata_.getColumnFamilies();
+        for ( String columnFamily : columnFamilies )
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get( columnFamily );
+            if ( cfStore != null )
+                list.addAll(cfStore.getSSTables());
+        }
+        return list;
+    }
+
+    private Table(String table) throws IOException
+    {
+        table_ = table;
+        tableMetadata_ = Table.TableMetadata.instance(table);
+        for (String columnFamily : tableMetadata_.getColumnFamilies())
+        {
+            columnFamilyStores_.put(columnFamily, ColumnFamilyStore.getColumnFamilyStore(table, columnFamily));
+        }
+    }
+
+    boolean isApplicationColumnFamily(String columnFamily)
+    {
+        return DatabaseDescriptor.isApplicationColumnFamily(columnFamily);
+    }
+
+    int getColumnFamilyId(String columnFamily)
+    {
+        return tableMetadata_.getColumnFamilyId(columnFamily);
+    }
+
+    boolean isValidColumnFamily(String columnFamily)
+    {
+        return tableMetadata_.isValidColumnFamily(columnFamily);
+    }
+
+    /**
+     * Selects the row associated with the given key.
+    */
+    @Deprecated // CF should be our atom of work, not Row
+    public Row get(String key) throws IOException
+    {
+        Row row = new Row(table_, key);
+        for (String columnFamily : getColumnFamilies())
+        {
+            ColumnFamily cf = get(key, columnFamily);
+            if (cf != null)
+            {
+                row.addColumnFamily(cf);
+            }
+        }
+        return row;
+    }
+
+
+    /**
+     * Selects the specified column family for the specified key.
+    */
+    @Deprecated // single CFs could be larger than memory
+    public ColumnFamily get(String key, String cfName) throws IOException
+    {
+        ColumnFamilyStore cfStore = columnFamilyStores_.get(cfName);
+        assert cfStore != null : "Column family " + cfName + " has not been defined";
+        return cfStore.getColumnFamily(new IdentityQueryFilter(key, new QueryPath(cfName)));
+    }
+
+    /**
+     * Selects only the specified column family for the specified key.
+    */
+    @Deprecated
+    public Row getRow(String key, String cfName) throws IOException
+    {
+        Row row = new Row(table_, key);
+        ColumnFamily columnFamily = get(key, cfName);
+        if ( columnFamily != null )
+        	row.addColumnFamily(columnFamily);
+        return row;
+    }
+    
+    public Row getRow(QueryFilter filter) throws IOException
+    {
+        ColumnFamilyStore cfStore = columnFamilyStores_.get(filter.getColumnFamilyName());
+        Row row = new Row(table_, filter.key);
+        ColumnFamily columnFamily = cfStore.getColumnFamily(filter);
+        if (columnFamily != null)
+            row.addColumnFamily(columnFamily);
+        return row;
+    }
+
+    /**
+     * This method adds the row to the Commit Log associated with this table.
+     * Once this happens the data associated with the individual column families
+     * is also written to the column family store's memtable.
+    */
+    void apply(Row row) throws IOException
+    {
+        CommitLog.CommitLogContext cLogCtx = CommitLog.open().add(row);
+
+        for (ColumnFamily columnFamily : row.getColumnFamilies())
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get(columnFamily.name());
+            cfStore.apply(row.key(), columnFamily, cLogCtx);
+        }
+    }
+
+    void applyNow(Row row) throws IOException
+    {
+        String key = row.key();
+        for (ColumnFamily columnFamily : row.getColumnFamilies())
+        {
+            ColumnFamilyStore cfStore = columnFamilyStores_.get(columnFamily.name());
+            cfStore.applyNow( key, columnFamily );
+        }
+    }
+
+    public void flush(boolean fRecovery) throws IOException
+    {
+        Set<String> cfNames = columnFamilyStores_.keySet();
+        for ( String cfName : cfNames )
+        {
+            if (fRecovery) {
+                columnFamilyStores_.get(cfName).flushMemtableOnRecovery();
+            } else {
+                columnFamilyStores_.get(cfName).forceFlush();
+            }
+        }
+    }
+
+    // for binary load path.  skips commitlog.
+    void load(Row row) throws IOException
+    {
+        String key = row.key();
+                
+        for (ColumnFamily columnFamily : row.getColumnFamilies())
+        {
+            Collection<IColumn> columns = columnFamily.getSortedColumns();
+            for(IColumn column : columns)
+            {
+                ColumnFamilyStore cfStore = columnFamilyStores_.get(column.name());
+                cfStore.applyBinary(key, column.value());
+        	}
+        }
+        row.clear();
+    }
+
+    public SortedSet<String> getApplicationColumnFamilies()
+    {
+        if (applicationColumnFamilies_ == null)
+        {
+            applicationColumnFamilies_ = new TreeSet<String>();
+            for (String cfName : getColumnFamilies())
+            {
+                if (DatabaseDescriptor.isApplicationColumnFamily(cfName))
+                {
+                    applicationColumnFamilies_.add(cfName);
+                }
+            }
+        }
+        return applicationColumnFamilies_;
+    }
+
+    /**
+     * @param startWith key to start with, inclusive.  empty string = start at beginning.
+     * @param stopAt key to stop at, inclusive.  empty string = stop only when keys are exhausted.
+     * @param maxResults
+     * @return list of keys between startWith and stopAt
+     */
+    public List<String> getKeyRange(String columnFamily, final String startWith, final String stopAt, int maxResults)
+    throws IOException, ExecutionException, InterruptedException
+    {
+        assert getColumnFamilyStore(columnFamily) != null : columnFamily;
+
+        getColumnFamilyStore(columnFamily).getReadLock().lock();
+        try
+        {
+            return getKeyRangeUnsafe(columnFamily, startWith, stopAt, maxResults);
+        }
+        finally
+        {
+            getColumnFamilyStore(columnFamily).getReadLock().unlock();
+        }
+    }
+
+    private List<String> getKeyRangeUnsafe(final String cfName, final String startWith, final String stopAt, int maxResults) throws IOException, ExecutionException, InterruptedException
+    {
+        // (OPP key decoration is a no-op so using the "decorated" comparator against raw keys is fine)
+        final Comparator<String> comparator = StorageService.getPartitioner().getDecoratedKeyComparator();
+
+        // create a CollatedIterator that will return unique keys from different sources
+        // (current memtable, historical memtables, and SSTables) in the correct order.
+        List<Iterator<String>> iterators = new ArrayList<Iterator<String>>();
+        ColumnFamilyStore cfs = getColumnFamilyStore(cfName);
+
+        // we iterate through memtables with a priority queue to avoid more sorting than necessary.
+        // this predicate throws out the keys before the start of our range.
+        Predicate p = new Predicate()
+        {
+            public boolean evaluate(Object key)
+            {
+                String st = (String)key;
+                return comparator.compare(startWith, st) <= 0 && (stopAt.isEmpty() || comparator.compare(st, stopAt) <= 0);
+            }
+        };
+
+        // current memtable keys.  have to go through the CFS api for locking.
+        iterators.add(IteratorUtils.filteredIterator(cfs.memtableKeyIterator(), p));
+        // historical memtables
+        for (Memtable memtable : ColumnFamilyStore.getUnflushedMemtables(cfName))
+        {
+            iterators.add(IteratorUtils.filteredIterator(Memtable.getKeyIterator(memtable.getKeys()), p));
+        }
+
+        // sstables
+        for (SSTableReader sstable : cfs.getSSTables())
+        {
+            FileStruct fs = sstable.getFileStruct();
+            fs.seekTo(startWith);
+            iterators.add(fs);
+        }
+
+        Iterator<String> collated = IteratorUtils.collatedIterator(comparator, iterators);
+        Iterable<String> reduced = new ReducingIterator<String>(collated) {
+            String current;
+
+            public void reduce(String current)
+            {
+                 this.current = current;
+            }
+
+            protected String getReduced()
+            {
+                return current;
+            }
+        };
+
+        try
+        {
+            // pull keys out of the CollatedIterator.  checking tombstone status is expensive,
+            // so we set an arbitrary limit on how many we'll do at once.
+            List<String> keys = new ArrayList<String>();
+            for (String current : reduced)
+            {
+                if (!stopAt.isEmpty() && comparator.compare(stopAt, current) < 0)
+                {
+                    break;
+                }
+                // make sure there is actually non-tombstone content associated w/ this key
+                // TODO record the key source(s) somehow and only check that source (e.g., memtable or sstable)
+                QueryFilter filter = new SliceQueryFilter(current, new QueryPath(cfName), ArrayUtils.EMPTY_BYTE_ARRAY, ArrayUtils.EMPTY_BYTE_ARRAY, true, 1);
+                if (cfs.getColumnFamily(filter, Integer.MAX_VALUE) != null)
+                {
+                    keys.add(current);
+                }
+                if (keys.size() >= maxResults)
+                {
+                    break;
+                }
+            }
+            return keys;
+        }
+        finally
+        {
+            for (Iterator iter : iterators)
+            {
+                if (iter instanceof FileStruct)
+                {
+                    ((FileStruct)iter).close();
+                }
+            }
+        }
+    }
+
+    public static String getSnapshotPath(String dataDirPath, String tableName, String snapshotName)
+    {
+        return dataDirPath + File.separator + tableName + File.separator + SNAPSHOT_SUBDIR_NAME + File.separator + snapshotName;
+    }
+}
diff --git a/src/java/org/apache/cassandra/db/WriteResponse.java b/src/java/org/apache/cassandra/db/WriteResponse.java
index 7af8ad7bc3..96737b1580 100644
--- a/src/java/org/apache/cassandra/db/WriteResponse.java
+++ b/src/java/org/apache/cassandra/db/WriteResponse.java
@@ -1,100 +1,100 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.Serializable;
-
-import javax.xml.bind.annotation.XmlElement;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.StorageService;
-
-
-/*
- * This message is sent back the row mutation verb handler 
- * and basically specifies if the write succeeded or not for a particular 
- * key in a table
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public class WriteResponse 
-{
-    private static WriteResponseSerializer serializer_ = new WriteResponseSerializer();
-
-    public static WriteResponseSerializer serializer()
-    {
-        return serializer_;
-    }
-
-    public static Message makeWriteResponseMessage(Message original, WriteResponse writeResponseMessage) throws IOException
-    {
-    	ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream( bos );
-        WriteResponse.serializer().serialize(writeResponseMessage, dos);
-        return original.getReply(StorageService.getLocalStorageEndPoint(), bos.toByteArray());
-    }
-
-	private final String table_;
-	private final String key_;
-	private final boolean status_;
-
-	public WriteResponse(String table, String key, boolean bVal) {
-		table_ = table;
-		key_ = key;
-		status_ = bVal;
-	}
-
-	public String table()
-	{
-		return table_;
-	}
-
-	public String key()
-	{
-		return key_;
-	}
-
-	public boolean isSuccess()
-	{
-		return status_;
-	}
-
-    public static class WriteResponseSerializer implements ICompactSerializer<WriteResponse>
-    {
-        public void serialize(WriteResponse wm, DataOutputStream dos) throws IOException
-        {
-            dos.writeUTF(wm.table());
-            dos.writeUTF(wm.key());
-            dos.writeBoolean(wm.isSuccess());
-        }
-
-        public WriteResponse deserialize(DataInputStream dis) throws IOException
-        {
-            String table = dis.readUTF();
-            String key = dis.readUTF();
-            boolean status = dis.readBoolean();
-            return new WriteResponse(table, key, status);
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+
+import javax.xml.bind.annotation.XmlElement;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+
+
+/*
+ * This message is sent back the row mutation verb handler 
+ * and basically specifies if the write succeeded or not for a particular 
+ * key in a table
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class WriteResponse 
+{
+    private static WriteResponseSerializer serializer_ = new WriteResponseSerializer();
+
+    public static WriteResponseSerializer serializer()
+    {
+        return serializer_;
+    }
+
+    public static Message makeWriteResponseMessage(Message original, WriteResponse writeResponseMessage) throws IOException
+    {
+    	ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        WriteResponse.serializer().serialize(writeResponseMessage, dos);
+        return original.getReply(StorageService.getLocalStorageEndPoint(), bos.toByteArray());
+    }
+
+	private final String table_;
+	private final String key_;
+	private final boolean status_;
+
+	public WriteResponse(String table, String key, boolean bVal) {
+		table_ = table;
+		key_ = key;
+		status_ = bVal;
+	}
+
+	public String table()
+	{
+		return table_;
+	}
+
+	public String key()
+	{
+		return key_;
+	}
+
+	public boolean isSuccess()
+	{
+		return status_;
+	}
+
+    public static class WriteResponseSerializer implements ICompactSerializer<WriteResponse>
+    {
+        public void serialize(WriteResponse wm, DataOutputStream dos) throws IOException
+        {
+            dos.writeUTF(wm.table());
+            dos.writeUTF(wm.key());
+            dos.writeBoolean(wm.isSuccess());
+        }
+
+        public WriteResponse deserialize(DataInputStream dis) throws IOException
+        {
+            String table = dis.readUTF();
+            String key = dis.readUTF();
+            boolean status = dis.readBoolean();
+            return new WriteResponse(table, key, status);
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/dht/BootStrapper.java b/src/java/org/apache/cassandra/dht/BootStrapper.java
index 190119cb34..62b1b282ab 100644
--- a/src/java/org/apache/cassandra/dht/BootStrapper.java
+++ b/src/java/org/apache/cassandra/dht/BootStrapper.java
@@ -1,134 +1,134 @@
- /**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
- import java.util.ArrayList;
- import java.util.Collections;
- import java.util.HashMap;
- import java.util.HashSet;
- import java.util.List;
- import java.util.Map;
- import java.util.Set;
-
- import org.apache.log4j.Logger;
-
- import org.apache.cassandra.locator.TokenMetadata;
- import org.apache.cassandra.net.EndPoint;
- import org.apache.cassandra.service.StorageService;
- import org.apache.cassandra.utils.LogUtil;
-
-
-/**
- * This class handles the bootstrapping responsibilities for
- * any new endpoint.
-*/
-public class BootStrapper implements Runnable
-{
-    private static Logger logger_ = Logger.getLogger(BootStrapper.class);
-    /* endpoints that need to be bootstrapped */
-    protected EndPoint[] targets_ = new EndPoint[0];
-    /* tokens of the nodes being bootstrapped. */
-    protected final Token[] tokens_;
-    protected TokenMetadata tokenMetadata_ = null;
-    private List<EndPoint> filters_ = new ArrayList<EndPoint>();
-
-    public BootStrapper(EndPoint[] target, Token... token)
-    {
-        targets_ = target;
-        tokens_ = token;
-        tokenMetadata_ = StorageService.instance().getTokenMetadata();
-    }
-    
-    public BootStrapper(EndPoint[] target, Token[] token, EndPoint[] filters)
-    {
-        this(target, token);
-        Collections.addAll(filters_, filters);
-    }
-
-    public void run()
-    {
-        try
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Beginning bootstrap process for " + targets_ + " ...");                                                               
-            /* copy the token to endpoint map */
-            Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-            /* remove the tokens associated with the endpoints being bootstrapped */                
-            for (Token token : tokens_)
-            {
-                tokenToEndPointMap.remove(token);                    
-            }
-
-            Set<Token> oldTokens = new HashSet<Token>( tokenToEndPointMap.keySet() );
-            Range[] oldRanges = StorageService.instance().getAllRanges(oldTokens);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Total number of old ranges " + oldRanges.length);
-            /* 
-             * Find the ranges that are split. Maintain a mapping between
-             * the range being split and the list of subranges.
-            */                
-            Map<Range, List<Range>> splitRanges = LeaveJoinProtocolHelper.getRangeSplitRangeMapping(oldRanges, tokens_);                                                      
-            /* Calculate the list of nodes that handle the old ranges */
-            Map<Range, List<EndPoint>> oldRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(oldRanges, tokenToEndPointMap);
-            /* Mapping of split ranges to the list of endpoints responsible for the range */                
-            Map<Range, List<EndPoint>> replicasForSplitRanges = new HashMap<Range, List<EndPoint>>();                                
-            Set<Range> rangesSplit = splitRanges.keySet();                
-            for ( Range splitRange : rangesSplit )
-            {
-                replicasForSplitRanges.put( splitRange, oldRangeToEndPointMap.get(splitRange) );
-            }                
-            /* Remove the ranges that are split. */
-            for ( Range splitRange : rangesSplit )
-            {
-                oldRangeToEndPointMap.remove(splitRange);
-            }
-            
-            /* Add the subranges of the split range to the map with the same replica set. */
-            for ( Range splitRange : rangesSplit )
-            {
-                List<Range> subRanges = splitRanges.get(splitRange);
-                List<EndPoint> replicas = replicasForSplitRanges.get(splitRange);
-                for ( Range subRange : subRanges )
-                {
-                    /* Make sure we clone or else we are hammered. */
-                    oldRangeToEndPointMap.put(subRange, new ArrayList<EndPoint>(replicas));
-                }
-            }                
-            
-            /* Add the new token and re-calculate the range assignments */
-            Collections.addAll( oldTokens, tokens_ );
-            Range[] newRanges = StorageService.instance().getAllRanges(oldTokens);
-
-            if (logger_.isDebugEnabled())
-              logger_.debug("Total number of new ranges " + newRanges.length);
-            /* Calculate the list of nodes that handle the new ranges */
-            Map<Range, List<EndPoint>> newRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(newRanges);
-            /* Calculate ranges that need to be sent and from whom to where */
-            Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget = LeaveJoinProtocolHelper.getRangeSourceTargetInfo(oldRangeToEndPointMap, newRangeToEndPointMap);
-            /* Send messages to respective folks to stream data over to the new nodes being bootstrapped */
-            LeaveJoinProtocolHelper.assignWork(rangesWithSourceTarget, filters_);                
-        }
-        catch ( Throwable th )
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug( LogUtil.throwableToString(th) );
-        }
-    }
-
-}
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+ import java.util.ArrayList;
+ import java.util.Collections;
+ import java.util.HashMap;
+ import java.util.HashSet;
+ import java.util.List;
+ import java.util.Map;
+ import java.util.Set;
+
+ import org.apache.log4j.Logger;
+
+ import org.apache.cassandra.locator.TokenMetadata;
+ import org.apache.cassandra.net.EndPoint;
+ import org.apache.cassandra.service.StorageService;
+ import org.apache.cassandra.utils.LogUtil;
+
+
+/**
+ * This class handles the bootstrapping responsibilities for
+ * any new endpoint.
+*/
+public class BootStrapper implements Runnable
+{
+    private static Logger logger_ = Logger.getLogger(BootStrapper.class);
+    /* endpoints that need to be bootstrapped */
+    protected EndPoint[] targets_ = new EndPoint[0];
+    /* tokens of the nodes being bootstrapped. */
+    protected final Token[] tokens_;
+    protected TokenMetadata tokenMetadata_ = null;
+    private List<EndPoint> filters_ = new ArrayList<EndPoint>();
+
+    public BootStrapper(EndPoint[] target, Token... token)
+    {
+        targets_ = target;
+        tokens_ = token;
+        tokenMetadata_ = StorageService.instance().getTokenMetadata();
+    }
+    
+    public BootStrapper(EndPoint[] target, Token[] token, EndPoint[] filters)
+    {
+        this(target, token);
+        Collections.addAll(filters_, filters);
+    }
+
+    public void run()
+    {
+        try
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Beginning bootstrap process for " + targets_ + " ...");                                                               
+            /* copy the token to endpoint map */
+            Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+            /* remove the tokens associated with the endpoints being bootstrapped */                
+            for (Token token : tokens_)
+            {
+                tokenToEndPointMap.remove(token);                    
+            }
+
+            Set<Token> oldTokens = new HashSet<Token>( tokenToEndPointMap.keySet() );
+            Range[] oldRanges = StorageService.instance().getAllRanges(oldTokens);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Total number of old ranges " + oldRanges.length);
+            /* 
+             * Find the ranges that are split. Maintain a mapping between
+             * the range being split and the list of subranges.
+            */                
+            Map<Range, List<Range>> splitRanges = LeaveJoinProtocolHelper.getRangeSplitRangeMapping(oldRanges, tokens_);                                                      
+            /* Calculate the list of nodes that handle the old ranges */
+            Map<Range, List<EndPoint>> oldRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(oldRanges, tokenToEndPointMap);
+            /* Mapping of split ranges to the list of endpoints responsible for the range */                
+            Map<Range, List<EndPoint>> replicasForSplitRanges = new HashMap<Range, List<EndPoint>>();                                
+            Set<Range> rangesSplit = splitRanges.keySet();                
+            for ( Range splitRange : rangesSplit )
+            {
+                replicasForSplitRanges.put( splitRange, oldRangeToEndPointMap.get(splitRange) );
+            }                
+            /* Remove the ranges that are split. */
+            for ( Range splitRange : rangesSplit )
+            {
+                oldRangeToEndPointMap.remove(splitRange);
+            }
+            
+            /* Add the subranges of the split range to the map with the same replica set. */
+            for ( Range splitRange : rangesSplit )
+            {
+                List<Range> subRanges = splitRanges.get(splitRange);
+                List<EndPoint> replicas = replicasForSplitRanges.get(splitRange);
+                for ( Range subRange : subRanges )
+                {
+                    /* Make sure we clone or else we are hammered. */
+                    oldRangeToEndPointMap.put(subRange, new ArrayList<EndPoint>(replicas));
+                }
+            }                
+            
+            /* Add the new token and re-calculate the range assignments */
+            Collections.addAll( oldTokens, tokens_ );
+            Range[] newRanges = StorageService.instance().getAllRanges(oldTokens);
+
+            if (logger_.isDebugEnabled())
+              logger_.debug("Total number of new ranges " + newRanges.length);
+            /* Calculate the list of nodes that handle the new ranges */
+            Map<Range, List<EndPoint>> newRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(newRanges);
+            /* Calculate ranges that need to be sent and from whom to where */
+            Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget = LeaveJoinProtocolHelper.getRangeSourceTargetInfo(oldRangeToEndPointMap, newRangeToEndPointMap);
+            /* Send messages to respective folks to stream data over to the new nodes being bootstrapped */
+            LeaveJoinProtocolHelper.assignWork(rangesWithSourceTarget, filters_);                
+        }
+        catch ( Throwable th )
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug( LogUtil.throwableToString(th) );
+        }
+    }
+
+}
diff --git a/src/java/org/apache/cassandra/dht/BootstrapInitiateMessage.java b/src/java/org/apache/cassandra/dht/BootstrapInitiateMessage.java
index 27d6531aa0..386d68fd83 100644
--- a/src/java/org/apache/cassandra/dht/BootstrapInitiateMessage.java
+++ b/src/java/org/apache/cassandra/dht/BootstrapInitiateMessage.java
@@ -1,99 +1,99 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.Serializable;
-
-import org.apache.cassandra.db.ColumnFamily;
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.io.StreamContextManager;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.net.io.*;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class BootstrapInitiateMessage implements Serializable
-{
-    private static ICompactSerializer<BootstrapInitiateMessage> serializer_;
-    
-    static
-    {
-        serializer_ = new BootstrapInitiateMessageSerializer();
-    }
-    
-    public static ICompactSerializer<BootstrapInitiateMessage> serializer()
-    {
-        return serializer_;
-    }
-    
-    public static Message makeBootstrapInitiateMessage(BootstrapInitiateMessage biMessage) throws IOException
-    {
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream( bos );
-        BootstrapInitiateMessage.serializer().serialize(biMessage, dos);
-        return new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapInitiateVerbHandler_, bos.toByteArray() );
-    }
-    
-    protected StreamContextManager.StreamContext[] streamContexts_ = new StreamContextManager.StreamContext[0];
-   
-    public BootstrapInitiateMessage(StreamContextManager.StreamContext[] streamContexts)
-    {
-        streamContexts_ = streamContexts;
-    }
-    
-    public StreamContextManager.StreamContext[] getStreamContext()
-    {
-        return streamContexts_;
-    }
-}
-
-class BootstrapInitiateMessageSerializer implements ICompactSerializer<BootstrapInitiateMessage>
-{
-    public void serialize(BootstrapInitiateMessage bim, DataOutputStream dos) throws IOException
-    {
-        dos.writeInt(bim.streamContexts_.length);
-        for ( StreamContextManager.StreamContext streamContext : bim.streamContexts_ )
-        {
-            StreamContextManager.StreamContext.serializer().serialize(streamContext, dos);
-        }
-    }
-    
-    public BootstrapInitiateMessage deserialize(DataInputStream dis) throws IOException
-    {
-        int size = dis.readInt();
-        StreamContextManager.StreamContext[] streamContexts = new StreamContextManager.StreamContext[0];
-        if ( size > 0 )
-        {
-            streamContexts = new StreamContextManager.StreamContext[size];
-            for ( int i = 0; i < size; ++i )
-            {
-                streamContexts[i] = StreamContextManager.StreamContext.serializer().deserialize(dis);
-            }
-        }
-        return new BootstrapInitiateMessage(streamContexts);
-    }
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.io.StreamContextManager;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.net.io.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class BootstrapInitiateMessage implements Serializable
+{
+    private static ICompactSerializer<BootstrapInitiateMessage> serializer_;
+    
+    static
+    {
+        serializer_ = new BootstrapInitiateMessageSerializer();
+    }
+    
+    public static ICompactSerializer<BootstrapInitiateMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    public static Message makeBootstrapInitiateMessage(BootstrapInitiateMessage biMessage) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        BootstrapInitiateMessage.serializer().serialize(biMessage, dos);
+        return new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapInitiateVerbHandler_, bos.toByteArray() );
+    }
+    
+    protected StreamContextManager.StreamContext[] streamContexts_ = new StreamContextManager.StreamContext[0];
+   
+    public BootstrapInitiateMessage(StreamContextManager.StreamContext[] streamContexts)
+    {
+        streamContexts_ = streamContexts;
+    }
+    
+    public StreamContextManager.StreamContext[] getStreamContext()
+    {
+        return streamContexts_;
+    }
+}
+
+class BootstrapInitiateMessageSerializer implements ICompactSerializer<BootstrapInitiateMessage>
+{
+    public void serialize(BootstrapInitiateMessage bim, DataOutputStream dos) throws IOException
+    {
+        dos.writeInt(bim.streamContexts_.length);
+        for ( StreamContextManager.StreamContext streamContext : bim.streamContexts_ )
+        {
+            StreamContextManager.StreamContext.serializer().serialize(streamContext, dos);
+        }
+    }
+    
+    public BootstrapInitiateMessage deserialize(DataInputStream dis) throws IOException
+    {
+        int size = dis.readInt();
+        StreamContextManager.StreamContext[] streamContexts = new StreamContextManager.StreamContext[0];
+        if ( size > 0 )
+        {
+            streamContexts = new StreamContextManager.StreamContext[size];
+            for ( int i = 0; i < size; ++i )
+            {
+                streamContexts[i] = StreamContextManager.StreamContext.serializer().deserialize(dis);
+            }
+        }
+        return new BootstrapInitiateMessage(streamContexts);
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/dht/BootstrapMetadata.java b/src/java/org/apache/cassandra/dht/BootstrapMetadata.java
index 307e1da23d..34126b2416 100644
--- a/src/java/org/apache/cassandra/dht/BootstrapMetadata.java
+++ b/src/java/org/apache/cassandra/dht/BootstrapMetadata.java
@@ -1,102 +1,102 @@
- /**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.CompactEndPointSerializationHelper;
-import org.apache.cassandra.net.EndPoint;
-
-
-
-/**
- * This encapsulates information of the list of 
- * ranges that a target node requires in order to 
- * be bootstrapped. This will be bundled in a 
- * BootstrapMetadataMessage and sent to nodes that
- * are going to handoff the data.
-*/
-class BootstrapMetadata
-{
-    private static ICompactSerializer<BootstrapMetadata> serializer_;
-    static
-    {
-        serializer_ = new BootstrapMetadataSerializer();
-    }
-    
-    protected static ICompactSerializer<BootstrapMetadata> serializer()
-    {
-        return serializer_;
-    }
-    
-    protected EndPoint target_;
-    protected List<Range> ranges_;
-    
-    BootstrapMetadata(EndPoint target, List<Range> ranges)
-    {
-        target_ = target;
-        ranges_ = ranges;
-    }
-    
-    public String toString()
-    {
-        StringBuilder sb = new StringBuilder("");
-        sb.append(target_);
-        sb.append("------->");
-        for ( Range range : ranges_ )
-        {
-            sb.append(range);
-            sb.append(" ");
-        }
-        return sb.toString();
-    }
-}
-
-class BootstrapMetadataSerializer implements ICompactSerializer<BootstrapMetadata>
-{
-    public void serialize(BootstrapMetadata bsMetadata, DataOutputStream dos) throws IOException
-    {
-        CompactEndPointSerializationHelper.serialize(bsMetadata.target_, dos);
-        int size = (bsMetadata.ranges_ == null) ? 0 : bsMetadata.ranges_.size();            
-        dos.writeInt(size);
-        
-        for ( Range range : bsMetadata.ranges_ )
-        {
-            Range.serializer().serialize(range, dos);
-        }            
-    }
-
-    public BootstrapMetadata deserialize(DataInputStream dis) throws IOException
-    {            
-        EndPoint target = CompactEndPointSerializationHelper.deserialize(dis);
-        int size = dis.readInt();
-        List<Range> ranges = (size == 0) ? null : new ArrayList<Range>();
-        for( int i = 0; i < size; ++i )
-        {
-            ranges.add(Range.serializer().deserialize(dis));
-        }            
-        return new BootstrapMetadata( target, ranges );
-    }
-}
-
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.CompactEndPointSerializationHelper;
+import org.apache.cassandra.net.EndPoint;
+
+
+
+/**
+ * This encapsulates information of the list of 
+ * ranges that a target node requires in order to 
+ * be bootstrapped. This will be bundled in a 
+ * BootstrapMetadataMessage and sent to nodes that
+ * are going to handoff the data.
+*/
+class BootstrapMetadata
+{
+    private static ICompactSerializer<BootstrapMetadata> serializer_;
+    static
+    {
+        serializer_ = new BootstrapMetadataSerializer();
+    }
+    
+    protected static ICompactSerializer<BootstrapMetadata> serializer()
+    {
+        return serializer_;
+    }
+    
+    protected EndPoint target_;
+    protected List<Range> ranges_;
+    
+    BootstrapMetadata(EndPoint target, List<Range> ranges)
+    {
+        target_ = target;
+        ranges_ = ranges;
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder("");
+        sb.append(target_);
+        sb.append("------->");
+        for ( Range range : ranges_ )
+        {
+            sb.append(range);
+            sb.append(" ");
+        }
+        return sb.toString();
+    }
+}
+
+class BootstrapMetadataSerializer implements ICompactSerializer<BootstrapMetadata>
+{
+    public void serialize(BootstrapMetadata bsMetadata, DataOutputStream dos) throws IOException
+    {
+        CompactEndPointSerializationHelper.serialize(bsMetadata.target_, dos);
+        int size = (bsMetadata.ranges_ == null) ? 0 : bsMetadata.ranges_.size();            
+        dos.writeInt(size);
+        
+        for ( Range range : bsMetadata.ranges_ )
+        {
+            Range.serializer().serialize(range, dos);
+        }            
+    }
+
+    public BootstrapMetadata deserialize(DataInputStream dis) throws IOException
+    {            
+        EndPoint target = CompactEndPointSerializationHelper.deserialize(dis);
+        int size = dis.readInt();
+        List<Range> ranges = (size == 0) ? null : new ArrayList<Range>();
+        for( int i = 0; i < size; ++i )
+        {
+            ranges.add(Range.serializer().deserialize(dis));
+        }            
+        return new BootstrapMetadata( target, ranges );
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/dht/BootstrapMetadataMessage.java b/src/java/org/apache/cassandra/dht/BootstrapMetadataMessage.java
index f46ab4146c..341d72e163 100644
--- a/src/java/org/apache/cassandra/dht/BootstrapMetadataMessage.java
+++ b/src/java/org/apache/cassandra/dht/BootstrapMetadataMessage.java
@@ -1,90 +1,90 @@
- /**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.service.StorageService;
-
-
-
-/**
- * This class encapsulates the message that needs to be sent
- * to nodes that handoff data. The message contains information
- * about the node to be bootstrapped and the ranges with which
- * it needs to be bootstrapped.
-*/
-class BootstrapMetadataMessage
-{
-    private static ICompactSerializer<BootstrapMetadataMessage> serializer_;
-    static
-    {
-        serializer_ = new BootstrapMetadataMessageSerializer();
-    }
-    
-    protected static ICompactSerializer<BootstrapMetadataMessage> serializer()
-    {
-        return serializer_;
-    }
-    
-    protected static Message makeBootstrapMetadataMessage(BootstrapMetadataMessage bsMetadataMessage) throws IOException
-    {
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream( bos );
-        BootstrapMetadataMessage.serializer().serialize(bsMetadataMessage, dos);
-        return new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bsMetadataVerbHandler_, bos.toByteArray() );
-    }        
-    
-    protected BootstrapMetadata[] bsMetadata_ = new BootstrapMetadata[0];
-    
-    BootstrapMetadataMessage(BootstrapMetadata[] bsMetadata)
-    {
-        bsMetadata_ = bsMetadata;
-    }
-}
-
-class BootstrapMetadataMessageSerializer implements ICompactSerializer<BootstrapMetadataMessage>
-{
-    public void serialize(BootstrapMetadataMessage bsMetadataMessage, DataOutputStream dos) throws IOException
-    {
-        BootstrapMetadata[] bsMetadata = bsMetadataMessage.bsMetadata_;
-        int size = (bsMetadata == null) ? 0 : bsMetadata.length;
-        dos.writeInt(size);
-        for ( BootstrapMetadata bsmd : bsMetadata )
-        {
-            BootstrapMetadata.serializer().serialize(bsmd, dos);
-        }
-    }
-
-    public BootstrapMetadataMessage deserialize(DataInputStream dis) throws IOException
-    {            
-        int size = dis.readInt();
-        BootstrapMetadata[] bsMetadata = new BootstrapMetadata[size];
-        for ( int i = 0; i < size; ++i )
-        {
-            bsMetadata[i] = BootstrapMetadata.serializer().deserialize(dis);
-        }
-        return new BootstrapMetadataMessage(bsMetadata);
-    }
-}
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+
+
+
+/**
+ * This class encapsulates the message that needs to be sent
+ * to nodes that handoff data. The message contains information
+ * about the node to be bootstrapped and the ranges with which
+ * it needs to be bootstrapped.
+*/
+class BootstrapMetadataMessage
+{
+    private static ICompactSerializer<BootstrapMetadataMessage> serializer_;
+    static
+    {
+        serializer_ = new BootstrapMetadataMessageSerializer();
+    }
+    
+    protected static ICompactSerializer<BootstrapMetadataMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    protected static Message makeBootstrapMetadataMessage(BootstrapMetadataMessage bsMetadataMessage) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream( bos );
+        BootstrapMetadataMessage.serializer().serialize(bsMetadataMessage, dos);
+        return new Message( StorageService.getLocalStorageEndPoint(), "", StorageService.bsMetadataVerbHandler_, bos.toByteArray() );
+    }        
+    
+    protected BootstrapMetadata[] bsMetadata_ = new BootstrapMetadata[0];
+    
+    BootstrapMetadataMessage(BootstrapMetadata[] bsMetadata)
+    {
+        bsMetadata_ = bsMetadata;
+    }
+}
+
+class BootstrapMetadataMessageSerializer implements ICompactSerializer<BootstrapMetadataMessage>
+{
+    public void serialize(BootstrapMetadataMessage bsMetadataMessage, DataOutputStream dos) throws IOException
+    {
+        BootstrapMetadata[] bsMetadata = bsMetadataMessage.bsMetadata_;
+        int size = (bsMetadata == null) ? 0 : bsMetadata.length;
+        dos.writeInt(size);
+        for ( BootstrapMetadata bsmd : bsMetadata )
+        {
+            BootstrapMetadata.serializer().serialize(bsmd, dos);
+        }
+    }
+
+    public BootstrapMetadataMessage deserialize(DataInputStream dis) throws IOException
+    {            
+        int size = dis.readInt();
+        BootstrapMetadata[] bsMetadata = new BootstrapMetadata[size];
+        for ( int i = 0; i < size; ++i )
+        {
+            bsMetadata[i] = BootstrapMetadata.serializer().deserialize(dis);
+        }
+        return new BootstrapMetadataMessage(bsMetadata);
+    }
+}
diff --git a/src/java/org/apache/cassandra/dht/BootstrapMetadataVerbHandler.java b/src/java/org/apache/cassandra/dht/BootstrapMetadataVerbHandler.java
index c36252ac7c..0956a80969 100644
--- a/src/java/org/apache/cassandra/dht/BootstrapMetadataVerbHandler.java
+++ b/src/java/org/apache/cassandra/dht/BootstrapMetadataVerbHandler.java
@@ -1,173 +1,173 @@
- /**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.Table;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.net.io.StreamContextManager;
-import org.apache.cassandra.service.StreamManager;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * This verb handler handles the BootstrapMetadataMessage that is sent
- * by the leader to the nodes that are responsible for handing off data. 
-*/
-public class BootstrapMetadataVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger(BootstrapMetadataVerbHandler.class);
-    
-    public void doVerb(Message message)
-    {
-        if (logger_.isDebugEnabled())
-          logger_.debug("Received a BootstrapMetadataMessage from " + message.getFrom());
-        byte[] body = message.getMessageBody();
-        DataInputBuffer bufIn = new DataInputBuffer();
-        bufIn.reset(body, body.length);
-        try
-        {
-            BootstrapMetadataMessage bsMetadataMessage = BootstrapMetadataMessage.serializer().deserialize(bufIn);
-            BootstrapMetadata[] bsMetadata = bsMetadataMessage.bsMetadata_;
-            
-            /*
-             * This is for debugging purposes. Remove later.
-            */
-            for ( BootstrapMetadata bsmd : bsMetadata )
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug(bsmd.toString());                                      
-            }
-            
-            for ( BootstrapMetadata bsmd : bsMetadata )
-            {
-                long startTime = System.currentTimeMillis();
-                doTransfer(bsmd.target_, bsmd.ranges_);     
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Time taken to boostrap " + 
-                        bsmd.target_ + 
-                        " is " + 
-                        (System.currentTimeMillis() - startTime) +
-                        " msecs.");
-            }
-        }
-        catch ( IOException ex )
-        {
-            logger_.info(LogUtil.throwableToString(ex));
-        }
-    }
-    
-    /*
-     * This method needs to figure out the files on disk
-     * locally for each range and then stream them using
-     * the Bootstrap protocol to the target endpoint.
-    */
-    private void doTransfer(EndPoint target, List<Range> ranges) throws IOException
-    {
-        if ( ranges.size() == 0 )
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("No ranges to give scram ...");
-            return;
-        }
-        
-        /* Just for debugging process - remove later */            
-        for ( Range range : ranges )
-        {
-            StringBuilder sb = new StringBuilder("");                
-            sb.append(range.toString());
-            sb.append(" ");            
-            if (logger_.isDebugEnabled())
-              logger_.debug("Beginning transfer process to " + target + " for ranges " + sb.toString());                
-        }
-      
-        /*
-         * (1) First we dump all the memtables to disk.
-         * (2) Run a version of compaction which will basically
-         *     put the keys in the range specified into a directory
-         *     named as per the endpoint it is destined for inside the
-         *     bootstrap directory.
-         * (3) Handoff the data.
-        */
-        List<String> tables = DatabaseDescriptor.getTables();
-        for ( String tName : tables )
-        {
-            Table table = Table.open(tName);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Flushing memtables ...");
-            table.flush(false);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Forcing compaction ...");
-            /* Get the counting bloom filter for each endpoint and the list of files that need to be streamed */
-            List<String> fileList = new ArrayList<String>();
-            boolean bVal = table.forceCompaction(ranges, target, fileList);                
-            doHandoff(target, fileList, tName);
-        }
-    }
-
-    /**
-     * Stream the files in the bootstrap directory over to the
-     * node being bootstrapped.
-    */
-    private void doHandoff(EndPoint target, List<String> fileList, String table) throws IOException
-    {
-        List<File> filesList = new ArrayList<File>();
-        for(String file : fileList)
-        {
-            filesList.add(new File(file));
-        }
-        File[] files = filesList.toArray(new File[0]);
-        StreamContextManager.StreamContext[] streamContexts = new StreamContextManager.StreamContext[files.length];
-        int i = 0;
-        for ( File file : files )
-        {
-            streamContexts[i] = new StreamContextManager.StreamContext(file.getAbsolutePath(), file.length(), table);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Stream context metadata " + streamContexts[i]);
-            ++i;
-        }
-        
-        if ( files.length > 0 )
-        {
-            /* Set up the stream manager with the files that need to streamed */
-            StreamManager.instance(target).addFilesToStream(streamContexts);
-            /* Send the bootstrap initiate message */
-            BootstrapInitiateMessage biMessage = new BootstrapInitiateMessage(streamContexts);
-            Message message = BootstrapInitiateMessage.makeBootstrapInitiateMessage(biMessage);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Sending a bootstrap initiate message to " + target + " ...");
-            MessagingService.getMessagingInstance().sendOneWay(message, target);                
-            if (logger_.isDebugEnabled())
-              logger_.debug("Waiting for transfer to " + target + " to complete");
-            StreamManager.instance(target).waitForStreamCompletion();
-            if (logger_.isDebugEnabled())
-              logger_.debug("Done with transfer to " + target);  
-        }
-    }
-}
-
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.io.StreamContextManager;
+import org.apache.cassandra.service.StreamManager;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * This verb handler handles the BootstrapMetadataMessage that is sent
+ * by the leader to the nodes that are responsible for handing off data. 
+*/
+public class BootstrapMetadataVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(BootstrapMetadataVerbHandler.class);
+    
+    public void doVerb(Message message)
+    {
+        if (logger_.isDebugEnabled())
+          logger_.debug("Received a BootstrapMetadataMessage from " + message.getFrom());
+        byte[] body = message.getMessageBody();
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(body, body.length);
+        try
+        {
+            BootstrapMetadataMessage bsMetadataMessage = BootstrapMetadataMessage.serializer().deserialize(bufIn);
+            BootstrapMetadata[] bsMetadata = bsMetadataMessage.bsMetadata_;
+            
+            /*
+             * This is for debugging purposes. Remove later.
+            */
+            for ( BootstrapMetadata bsmd : bsMetadata )
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug(bsmd.toString());                                      
+            }
+            
+            for ( BootstrapMetadata bsmd : bsMetadata )
+            {
+                long startTime = System.currentTimeMillis();
+                doTransfer(bsmd.target_, bsmd.ranges_);     
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Time taken to boostrap " + 
+                        bsmd.target_ + 
+                        " is " + 
+                        (System.currentTimeMillis() - startTime) +
+                        " msecs.");
+            }
+        }
+        catch ( IOException ex )
+        {
+            logger_.info(LogUtil.throwableToString(ex));
+        }
+    }
+    
+    /*
+     * This method needs to figure out the files on disk
+     * locally for each range and then stream them using
+     * the Bootstrap protocol to the target endpoint.
+    */
+    private void doTransfer(EndPoint target, List<Range> ranges) throws IOException
+    {
+        if ( ranges.size() == 0 )
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("No ranges to give scram ...");
+            return;
+        }
+        
+        /* Just for debugging process - remove later */            
+        for ( Range range : ranges )
+        {
+            StringBuilder sb = new StringBuilder("");                
+            sb.append(range.toString());
+            sb.append(" ");            
+            if (logger_.isDebugEnabled())
+              logger_.debug("Beginning transfer process to " + target + " for ranges " + sb.toString());                
+        }
+      
+        /*
+         * (1) First we dump all the memtables to disk.
+         * (2) Run a version of compaction which will basically
+         *     put the keys in the range specified into a directory
+         *     named as per the endpoint it is destined for inside the
+         *     bootstrap directory.
+         * (3) Handoff the data.
+        */
+        List<String> tables = DatabaseDescriptor.getTables();
+        for ( String tName : tables )
+        {
+            Table table = Table.open(tName);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Flushing memtables ...");
+            table.flush(false);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Forcing compaction ...");
+            /* Get the counting bloom filter for each endpoint and the list of files that need to be streamed */
+            List<String> fileList = new ArrayList<String>();
+            boolean bVal = table.forceCompaction(ranges, target, fileList);                
+            doHandoff(target, fileList, tName);
+        }
+    }
+
+    /**
+     * Stream the files in the bootstrap directory over to the
+     * node being bootstrapped.
+    */
+    private void doHandoff(EndPoint target, List<String> fileList, String table) throws IOException
+    {
+        List<File> filesList = new ArrayList<File>();
+        for(String file : fileList)
+        {
+            filesList.add(new File(file));
+        }
+        File[] files = filesList.toArray(new File[0]);
+        StreamContextManager.StreamContext[] streamContexts = new StreamContextManager.StreamContext[files.length];
+        int i = 0;
+        for ( File file : files )
+        {
+            streamContexts[i] = new StreamContextManager.StreamContext(file.getAbsolutePath(), file.length(), table);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Stream context metadata " + streamContexts[i]);
+            ++i;
+        }
+        
+        if ( files.length > 0 )
+        {
+            /* Set up the stream manager with the files that need to streamed */
+            StreamManager.instance(target).addFilesToStream(streamContexts);
+            /* Send the bootstrap initiate message */
+            BootstrapInitiateMessage biMessage = new BootstrapInitiateMessage(streamContexts);
+            Message message = BootstrapInitiateMessage.makeBootstrapInitiateMessage(biMessage);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Sending a bootstrap initiate message to " + target + " ...");
+            MessagingService.getMessagingInstance().sendOneWay(message, target);                
+            if (logger_.isDebugEnabled())
+              logger_.debug("Waiting for transfer to " + target + " to complete");
+            StreamManager.instance(target).waitForStreamCompletion();
+            if (logger_.isDebugEnabled())
+              logger_.debug("Done with transfer to " + target);  
+        }
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/dht/BootstrapSourceTarget.java b/src/java/org/apache/cassandra/dht/BootstrapSourceTarget.java
index 20198fa175..4f70c9fea0 100644
--- a/src/java/org/apache/cassandra/dht/BootstrapSourceTarget.java
+++ b/src/java/org/apache/cassandra/dht/BootstrapSourceTarget.java
@@ -1,49 +1,49 @@
- /**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import org.apache.cassandra.net.EndPoint;
-
-/**
- * This class encapsulates who is the source and the
- * target of a bootstrap for a particular range.
- */
-class BootstrapSourceTarget
-{
-    protected EndPoint source_;
-    protected EndPoint target_;
-    
-    BootstrapSourceTarget(EndPoint source, EndPoint target)
-    {
-        source_ = source;
-        target_ = target;
-    }
-    
-    public String toString()
-    {
-        StringBuilder sb = new StringBuilder("");
-        sb.append("SOURCE: ");
-        sb.append(source_);
-        sb.append(" ----> ");
-        sb.append("TARGET: ");
-        sb.append(target_);
-        sb.append(" ");
-        return sb.toString();
-    }
-}
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * This class encapsulates who is the source and the
+ * target of a bootstrap for a particular range.
+ */
+class BootstrapSourceTarget
+{
+    protected EndPoint source_;
+    protected EndPoint target_;
+    
+    BootstrapSourceTarget(EndPoint source, EndPoint target)
+    {
+        source_ = source;
+        target_ = target;
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder("");
+        sb.append("SOURCE: ");
+        sb.append(source_);
+        sb.append(" ----> ");
+        sb.append("TARGET: ");
+        sb.append(target_);
+        sb.append(" ");
+        return sb.toString();
+    }
+}
diff --git a/src/java/org/apache/cassandra/dht/IPartitioner.java b/src/java/org/apache/cassandra/dht/IPartitioner.java
index dcb2b00e93..5154b6d508 100644
--- a/src/java/org/apache/cassandra/dht/IPartitioner.java
+++ b/src/java/org/apache/cassandra/dht/IPartitioner.java
@@ -1,48 +1,48 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.util.Comparator;
-
-public interface IPartitioner
-{
-    /**
-     * transform key to on-disk format s.t. keys are stored in node comparison order.
-     * this lets bootstrap rip out parts of the sstable sequentially instead of doing random seeks.
-     *
-     * @param key the raw, client-facing key
-     * @return decorated on-disk version of key
-     */
-    public String decorateKey(String key);
-
-    public String undecorateKey(String decoratedKey);
-
-    public Comparator<String> getDecoratedKeyComparator();
-
-    public Comparator<String> getReverseDecoratedKeyComparator();
-
-    /**
-     * @return the token to use for this node if none was saved
-     */
-    public Token getInitialToken(String key);
-
-    public Token getDefaultToken();
-
-    public Token.TokenFactory getTokenFactory();
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.util.Comparator;
+
+public interface IPartitioner
+{
+    /**
+     * transform key to on-disk format s.t. keys are stored in node comparison order.
+     * this lets bootstrap rip out parts of the sstable sequentially instead of doing random seeks.
+     *
+     * @param key the raw, client-facing key
+     * @return decorated on-disk version of key
+     */
+    public String decorateKey(String key);
+
+    public String undecorateKey(String decoratedKey);
+
+    public Comparator<String> getDecoratedKeyComparator();
+
+    public Comparator<String> getReverseDecoratedKeyComparator();
+
+    /**
+     * @return the token to use for this node if none was saved
+     */
+    public Token getInitialToken(String key);
+
+    public Token getDefaultToken();
+
+    public Token.TokenFactory getTokenFactory();
+}
diff --git a/src/java/org/apache/cassandra/dht/LeaveJoinProtocolHelper.java b/src/java/org/apache/cassandra/dht/LeaveJoinProtocolHelper.java
index 4ab8f41728..d5a3bb9d79 100644
--- a/src/java/org/apache/cassandra/dht/LeaveJoinProtocolHelper.java
+++ b/src/java/org/apache/cassandra/dht/LeaveJoinProtocolHelper.java
@@ -1,229 +1,229 @@
- /**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
- import java.io.IOException;
- import java.util.ArrayList;
- import java.util.Arrays;
- import java.util.HashMap;
- import java.util.List;
- import java.util.Map;
- import java.util.Set;
-
- import org.apache.log4j.Logger;
-
- import org.apache.cassandra.net.EndPoint;
- import org.apache.cassandra.net.Message;
- import org.apache.cassandra.net.MessagingService;
-
-
-class LeaveJoinProtocolHelper
-{
-    private static Logger logger_ = Logger.getLogger(LeaveJoinProtocolHelper.class);
-    
-    /**
-     * Give a range a-------------b which is being split as
-     * a-----x-----y-----b then we want a mapping from 
-     * (a, b] --> (a, x], (x, y], (y, b] 
-    */
-    protected static Map<Range, List<Range>> getRangeSplitRangeMapping(Range[] oldRanges, Token[] allTokens)
-    {
-        Map<Range, List<Range>> splitRanges = new HashMap<Range, List<Range>>();
-        Token[] tokens = new Token[allTokens.length];
-        System.arraycopy(allTokens, 0, tokens, 0, tokens.length);
-        Arrays.sort(tokens);
-        
-        Range prevRange = null;
-        Token prevToken = null;
-        boolean bVal = false;
-        
-        for ( Range oldRange : oldRanges )
-        {
-            if (bVal)
-            {
-                bVal = false; 
-                List<Range> subRanges = splitRanges.get(prevRange);
-                if ( subRanges != null )
-                    subRanges.add( new Range(prevToken, prevRange.right()) );     
-            }
-            
-            prevRange = oldRange;
-            prevToken = oldRange.left();                
-            for (Token token : tokens)
-            {     
-                List<Range> subRanges = splitRanges.get(oldRange);
-                if ( oldRange.contains(token) )
-                {                        
-                    if ( subRanges == null )
-                    {
-                        subRanges = new ArrayList<Range>();
-                        splitRanges.put(oldRange, subRanges);
-                    }                            
-                    subRanges.add( new Range(prevToken, token) );
-                    prevToken = token;
-                    bVal = true;
-                }
-                else
-                {
-                    if ( bVal )
-                    {
-                        bVal = false;                                                                                
-                        subRanges.add( new Range(prevToken, oldRange.right()) );                            
-                    }
-                }
-            }
-        }
-        /* This is to handle the last range being processed. */
-        if ( bVal )
-        {
-            bVal = false; 
-            List<Range> subRanges = splitRanges.get(prevRange);
-            subRanges.add( new Range(prevToken, prevRange.right()) );                            
-        }
-        return splitRanges;
-    }
-    
-    protected static Map<Range, List<BootstrapSourceTarget>> getRangeSourceTargetInfo(Map<Range, List<EndPoint>> oldRangeToEndPointMap, Map<Range, List<EndPoint>> newRangeToEndPointMap)
-    {
-        Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget = new HashMap<Range, List<BootstrapSourceTarget>>();
-        /*
-         * Basically calculate for each range the endpoints handling the
-         * range in the old token set and in the new token set. Whoever
-         * gets bumped out of the top N will have to hand off that range
-         * to the new dude.
-        */
-        Set<Range> oldRangeSet = oldRangeToEndPointMap.keySet();
-        for(Range range : oldRangeSet)
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Attempting to figure out the dudes who are bumped out for " + range + " ...");
-            List<EndPoint> oldEndPoints = oldRangeToEndPointMap.get(range);
-            List<EndPoint> newEndPoints = newRangeToEndPointMap.get(range);
-            if ( newEndPoints != null )
-            {                        
-                List<EndPoint> newEndPoints2 = new ArrayList<EndPoint>(newEndPoints);
-                for ( EndPoint newEndPoint : newEndPoints2 )
-                {
-                    if ( oldEndPoints.contains(newEndPoint) )
-                    {
-                        oldEndPoints.remove(newEndPoint);
-                        newEndPoints.remove(newEndPoint);
-                    }
-                }                        
-            }
-            else
-            {
-                logger_.warn("Trespassing - scram");
-            }
-            if (logger_.isDebugEnabled())
-              logger_.debug("Done figuring out the dudes who are bumped out for range " + range + " ...");
-        }
-        for ( Range range : oldRangeSet )
-        {                    
-            List<EndPoint> oldEndPoints = oldRangeToEndPointMap.get(range);
-            List<EndPoint> newEndPoints = newRangeToEndPointMap.get(range);
-            List<BootstrapSourceTarget> srcTarget = rangesWithSourceTarget.get(range);
-            if ( srcTarget == null )
-            {
-                srcTarget = new ArrayList<BootstrapSourceTarget>();
-                rangesWithSourceTarget.put(range, srcTarget);
-            }
-            int i = 0;
-            for ( EndPoint oldEndPoint : oldEndPoints )
-            {                        
-                srcTarget.add( new BootstrapSourceTarget(oldEndPoint, newEndPoints.get(i++)) );
-            }
-        }
-        return rangesWithSourceTarget;
-    }
-    
-    /**
-     * This method sends messages out to nodes instructing them 
-     * to stream the specified ranges to specified target nodes. 
-    */
-    protected static void assignWork(Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget) throws IOException
-    {
-        assignWork(rangesWithSourceTarget, null);
-    }
-    
-    /**
-     * This method sends messages out to nodes instructing them 
-     * to stream the specified ranges to specified target nodes. 
-    */
-    protected static void assignWork(Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget, List<EndPoint> filters) throws IOException
-    {
-        /*
-         * Map whose key is the source node and the value is a map whose key is the
-         * target and value is the list of ranges to be sent to it. 
-        */
-        Map<EndPoint, Map<EndPoint, List<Range>>> rangeInfo = new HashMap<EndPoint, Map<EndPoint, List<Range>>>();
-        Set<Range> ranges = rangesWithSourceTarget.keySet();
-        
-        for ( Range range : ranges )
-        {
-            List<BootstrapSourceTarget> rangeSourceTargets = rangesWithSourceTarget.get(range);
-            for ( BootstrapSourceTarget rangeSourceTarget : rangeSourceTargets )
-            {
-                Map<EndPoint, List<Range>> targetRangeMap = rangeInfo.get(rangeSourceTarget.source_);
-                if ( targetRangeMap == null )
-                {
-                    targetRangeMap = new HashMap<EndPoint, List<Range>>();
-                    rangeInfo.put(rangeSourceTarget.source_, targetRangeMap);
-                }
-                List<Range> rangesToGive = targetRangeMap.get(rangeSourceTarget.target_);
-                if ( rangesToGive == null )
-                {
-                    rangesToGive = new ArrayList<Range>();
-                    targetRangeMap.put(rangeSourceTarget.target_, rangesToGive);
-                }
-                rangesToGive.add(range);
-            }
-        }
-        
-        Set<EndPoint> sources = rangeInfo.keySet();
-        for ( EndPoint source : sources )
-        {
-            /* only send the message to the nodes that are in the filter. */
-            if ( filters != null && filters.size() > 0 && !filters.contains(source) )
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Filtering endpoint " + source + " as source ...");
-                continue;
-            }
-            
-            Map<EndPoint, List<Range>> targetRangesMap = rangeInfo.get(source);
-            Set<EndPoint> targets = targetRangesMap.keySet();
-            List<BootstrapMetadata> bsmdList = new ArrayList<BootstrapMetadata>();
-            
-            for ( EndPoint target : targets )
-            {
-                List<Range> rangeForTarget = targetRangesMap.get(target);
-                BootstrapMetadata bsMetadata = new BootstrapMetadata(target, rangeForTarget);
-                bsmdList.add(bsMetadata);
-            }
-            
-            BootstrapMetadataMessage bsMetadataMessage = new BootstrapMetadataMessage(bsmdList.toArray( new BootstrapMetadata[0] ) );
-            /* Send this message to the source to do his shit. */
-            Message message = BootstrapMetadataMessage.makeBootstrapMetadataMessage(bsMetadataMessage); 
-            if (logger_.isDebugEnabled())
-              logger_.debug("Sending the BootstrapMetadataMessage to " + source);
-            MessagingService.getMessagingInstance().sendOneWay(message, source);
-        }
-    }
-}
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+ import java.io.IOException;
+ import java.util.ArrayList;
+ import java.util.Arrays;
+ import java.util.HashMap;
+ import java.util.List;
+ import java.util.Map;
+ import java.util.Set;
+
+ import org.apache.log4j.Logger;
+
+ import org.apache.cassandra.net.EndPoint;
+ import org.apache.cassandra.net.Message;
+ import org.apache.cassandra.net.MessagingService;
+
+
+class LeaveJoinProtocolHelper
+{
+    private static Logger logger_ = Logger.getLogger(LeaveJoinProtocolHelper.class);
+    
+    /**
+     * Give a range a-------------b which is being split as
+     * a-----x-----y-----b then we want a mapping from 
+     * (a, b] --> (a, x], (x, y], (y, b] 
+    */
+    protected static Map<Range, List<Range>> getRangeSplitRangeMapping(Range[] oldRanges, Token[] allTokens)
+    {
+        Map<Range, List<Range>> splitRanges = new HashMap<Range, List<Range>>();
+        Token[] tokens = new Token[allTokens.length];
+        System.arraycopy(allTokens, 0, tokens, 0, tokens.length);
+        Arrays.sort(tokens);
+        
+        Range prevRange = null;
+        Token prevToken = null;
+        boolean bVal = false;
+        
+        for ( Range oldRange : oldRanges )
+        {
+            if (bVal)
+            {
+                bVal = false; 
+                List<Range> subRanges = splitRanges.get(prevRange);
+                if ( subRanges != null )
+                    subRanges.add( new Range(prevToken, prevRange.right()) );     
+            }
+            
+            prevRange = oldRange;
+            prevToken = oldRange.left();                
+            for (Token token : tokens)
+            {     
+                List<Range> subRanges = splitRanges.get(oldRange);
+                if ( oldRange.contains(token) )
+                {                        
+                    if ( subRanges == null )
+                    {
+                        subRanges = new ArrayList<Range>();
+                        splitRanges.put(oldRange, subRanges);
+                    }                            
+                    subRanges.add( new Range(prevToken, token) );
+                    prevToken = token;
+                    bVal = true;
+                }
+                else
+                {
+                    if ( bVal )
+                    {
+                        bVal = false;                                                                                
+                        subRanges.add( new Range(prevToken, oldRange.right()) );                            
+                    }
+                }
+            }
+        }
+        /* This is to handle the last range being processed. */
+        if ( bVal )
+        {
+            bVal = false; 
+            List<Range> subRanges = splitRanges.get(prevRange);
+            subRanges.add( new Range(prevToken, prevRange.right()) );                            
+        }
+        return splitRanges;
+    }
+    
+    protected static Map<Range, List<BootstrapSourceTarget>> getRangeSourceTargetInfo(Map<Range, List<EndPoint>> oldRangeToEndPointMap, Map<Range, List<EndPoint>> newRangeToEndPointMap)
+    {
+        Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget = new HashMap<Range, List<BootstrapSourceTarget>>();
+        /*
+         * Basically calculate for each range the endpoints handling the
+         * range in the old token set and in the new token set. Whoever
+         * gets bumped out of the top N will have to hand off that range
+         * to the new dude.
+        */
+        Set<Range> oldRangeSet = oldRangeToEndPointMap.keySet();
+        for(Range range : oldRangeSet)
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Attempting to figure out the dudes who are bumped out for " + range + " ...");
+            List<EndPoint> oldEndPoints = oldRangeToEndPointMap.get(range);
+            List<EndPoint> newEndPoints = newRangeToEndPointMap.get(range);
+            if ( newEndPoints != null )
+            {                        
+                List<EndPoint> newEndPoints2 = new ArrayList<EndPoint>(newEndPoints);
+                for ( EndPoint newEndPoint : newEndPoints2 )
+                {
+                    if ( oldEndPoints.contains(newEndPoint) )
+                    {
+                        oldEndPoints.remove(newEndPoint);
+                        newEndPoints.remove(newEndPoint);
+                    }
+                }                        
+            }
+            else
+            {
+                logger_.warn("Trespassing - scram");
+            }
+            if (logger_.isDebugEnabled())
+              logger_.debug("Done figuring out the dudes who are bumped out for range " + range + " ...");
+        }
+        for ( Range range : oldRangeSet )
+        {                    
+            List<EndPoint> oldEndPoints = oldRangeToEndPointMap.get(range);
+            List<EndPoint> newEndPoints = newRangeToEndPointMap.get(range);
+            List<BootstrapSourceTarget> srcTarget = rangesWithSourceTarget.get(range);
+            if ( srcTarget == null )
+            {
+                srcTarget = new ArrayList<BootstrapSourceTarget>();
+                rangesWithSourceTarget.put(range, srcTarget);
+            }
+            int i = 0;
+            for ( EndPoint oldEndPoint : oldEndPoints )
+            {                        
+                srcTarget.add( new BootstrapSourceTarget(oldEndPoint, newEndPoints.get(i++)) );
+            }
+        }
+        return rangesWithSourceTarget;
+    }
+    
+    /**
+     * This method sends messages out to nodes instructing them 
+     * to stream the specified ranges to specified target nodes. 
+    */
+    protected static void assignWork(Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget) throws IOException
+    {
+        assignWork(rangesWithSourceTarget, null);
+    }
+    
+    /**
+     * This method sends messages out to nodes instructing them 
+     * to stream the specified ranges to specified target nodes. 
+    */
+    protected static void assignWork(Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget, List<EndPoint> filters) throws IOException
+    {
+        /*
+         * Map whose key is the source node and the value is a map whose key is the
+         * target and value is the list of ranges to be sent to it. 
+        */
+        Map<EndPoint, Map<EndPoint, List<Range>>> rangeInfo = new HashMap<EndPoint, Map<EndPoint, List<Range>>>();
+        Set<Range> ranges = rangesWithSourceTarget.keySet();
+        
+        for ( Range range : ranges )
+        {
+            List<BootstrapSourceTarget> rangeSourceTargets = rangesWithSourceTarget.get(range);
+            for ( BootstrapSourceTarget rangeSourceTarget : rangeSourceTargets )
+            {
+                Map<EndPoint, List<Range>> targetRangeMap = rangeInfo.get(rangeSourceTarget.source_);
+                if ( targetRangeMap == null )
+                {
+                    targetRangeMap = new HashMap<EndPoint, List<Range>>();
+                    rangeInfo.put(rangeSourceTarget.source_, targetRangeMap);
+                }
+                List<Range> rangesToGive = targetRangeMap.get(rangeSourceTarget.target_);
+                if ( rangesToGive == null )
+                {
+                    rangesToGive = new ArrayList<Range>();
+                    targetRangeMap.put(rangeSourceTarget.target_, rangesToGive);
+                }
+                rangesToGive.add(range);
+            }
+        }
+        
+        Set<EndPoint> sources = rangeInfo.keySet();
+        for ( EndPoint source : sources )
+        {
+            /* only send the message to the nodes that are in the filter. */
+            if ( filters != null && filters.size() > 0 && !filters.contains(source) )
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Filtering endpoint " + source + " as source ...");
+                continue;
+            }
+            
+            Map<EndPoint, List<Range>> targetRangesMap = rangeInfo.get(source);
+            Set<EndPoint> targets = targetRangesMap.keySet();
+            List<BootstrapMetadata> bsmdList = new ArrayList<BootstrapMetadata>();
+            
+            for ( EndPoint target : targets )
+            {
+                List<Range> rangeForTarget = targetRangesMap.get(target);
+                BootstrapMetadata bsMetadata = new BootstrapMetadata(target, rangeForTarget);
+                bsmdList.add(bsMetadata);
+            }
+            
+            BootstrapMetadataMessage bsMetadataMessage = new BootstrapMetadataMessage(bsmdList.toArray( new BootstrapMetadata[0] ) );
+            /* Send this message to the source to do his shit. */
+            Message message = BootstrapMetadataMessage.makeBootstrapMetadataMessage(bsMetadataMessage); 
+            if (logger_.isDebugEnabled())
+              logger_.debug("Sending the BootstrapMetadataMessage to " + source);
+            MessagingService.getMessagingInstance().sendOneWay(message, source);
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/dht/LeaveJoinProtocolImpl.java b/src/java/org/apache/cassandra/dht/LeaveJoinProtocolImpl.java
index 4cb5eeced4..5f66e89a1f 100644
--- a/src/java/org/apache/cassandra/dht/LeaveJoinProtocolImpl.java
+++ b/src/java/org/apache/cassandra/dht/LeaveJoinProtocolImpl.java
@@ -1,292 +1,292 @@
- /**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
- import java.util.ArrayList;
- import java.util.Collections;
- import java.util.HashMap;
- import java.util.HashSet;
- import java.util.List;
- import java.util.Map;
- import java.util.Set;
-
- import org.apache.log4j.Logger;
-
- import org.apache.cassandra.locator.TokenMetadata;
- import org.apache.cassandra.net.EndPoint;
- import org.apache.cassandra.service.StorageService;
- import org.apache.cassandra.utils.LogUtil;
-
-
-/**
- * This class performs the exact opposite of the
- * operations of the BootStrapper class. Given 
- * a bunch of nodes that need to move it determines 
- * who they need to hand off data in terms of ranges.
-*/
-public class LeaveJoinProtocolImpl implements Runnable
-{
-    private static Logger logger_ = Logger.getLogger(LeaveJoinProtocolImpl.class);    
-    
-    /* endpoints that are to be moved. */
-    protected EndPoint[] targets_ = new EndPoint[0];
-    /* position where they need to be moved */
-    protected final Token[] tokens_;
-    /* token metadata information */
-    protected TokenMetadata tokenMetadata_ = null;
-
-    public LeaveJoinProtocolImpl(EndPoint[] targets, Token[] tokens)
-    {
-        targets_ = targets;
-        tokens_ = tokens;
-        tokenMetadata_ = StorageService.instance().getTokenMetadata();
-    }
-
-    public void run()
-    {  
-        try
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Beginning leave/join process for ...");                                                               
-            /* copy the token to endpoint map */
-            Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-            /* copy the endpoint to token map */
-            Map<EndPoint, Token> endpointToTokenMap = tokenMetadata_.cloneEndPointTokenMap();
-            
-            Set<Token> oldTokens = new HashSet<Token>( tokenToEndPointMap.keySet() );
-            Range[] oldRanges = StorageService.instance().getAllRanges(oldTokens);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Total number of old ranges " + oldRanges.length);
-            /* Calculate the list of nodes that handle the old ranges */
-            Map<Range, List<EndPoint>> oldRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(oldRanges);
-            
-            /* Remove the tokens of the nodes leaving the ring */
-            Set<Token> tokens = getTokensForLeavingNodes();
-            oldTokens.removeAll(tokens);
-            Range[] rangesAfterNodesLeave = StorageService.instance().getAllRanges(oldTokens);
-            /* Get expanded range to initial range mapping */
-            Map<Range, List<Range>> expandedRangeToOldRangeMap = getExpandedRangeToOldRangeMapping(oldRanges, rangesAfterNodesLeave);
-            /* add the new token positions to the old tokens set */
-            for (Token token : tokens_)
-                oldTokens.add(token);
-            Range[] rangesAfterNodesJoin = StorageService.instance().getAllRanges(oldTokens);
-            /* replace the ranges that were split with the split ranges in the old configuration */
-            addSplitRangesToOldConfiguration(oldRangeToEndPointMap, rangesAfterNodesJoin);
-            
-            /* Re-calculate the new ranges after the new token positions are added */
-            Range[] newRanges = StorageService.instance().getAllRanges(oldTokens);
-            /* Remove the old locations from tokenToEndPointMap and add the new locations they are moving to */
-            for ( int i = 0; i < targets_.length; ++i )
-            {
-                tokenToEndPointMap.remove( endpointToTokenMap.get(targets_[i]) );
-                tokenToEndPointMap.put(tokens_[i], targets_[i]);
-            }            
-            /* Calculate the list of nodes that handle the new ranges */            
-            Map<Range, List<EndPoint>> newRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(newRanges, tokenToEndPointMap);
-            /* Remove any expanded ranges and replace them with ranges whose aggregate is the expanded range in the new configuration. */
-            removeExpandedRangesFromNewConfiguration(newRangeToEndPointMap, expandedRangeToOldRangeMap);
-            /* Calculate ranges that need to be sent and from whom to where */
-            Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget = LeaveJoinProtocolHelper.getRangeSourceTargetInfo(oldRangeToEndPointMap, newRangeToEndPointMap);
-            /* For debug purposes only */
-            Set<Range> ranges = rangesWithSourceTarget.keySet();
-            for ( Range range : ranges )
-            {
-                System.out.print("RANGE: " + range + ":: ");
-                List<BootstrapSourceTarget> infos = rangesWithSourceTarget.get(range);
-                for ( BootstrapSourceTarget info : infos )
-                {
-                    System.out.print(info);
-                    System.out.print(" ");
-                }
-                System.out.println(System.getProperty("line.separator"));
-            }
-            /* Send messages to respective folks to stream data over to the new nodes being bootstrapped */
-            LeaveJoinProtocolHelper.assignWork(rangesWithSourceTarget);
-        }
-        catch ( Throwable th )
-        {
-            logger_.warn(LogUtil.throwableToString(th));
-        }
-    }
-    
-    /**
-     * This method figures out the ranges that have been split and
-     * replaces them with the split range.
-     * @param oldRangeToEndPointMap old range mapped to their replicas.
-     * @param rangesAfterNodesJoin ranges after the nodes have joined at
-     *        their respective position.
-     */
-    private void addSplitRangesToOldConfiguration(Map<Range, List<EndPoint>> oldRangeToEndPointMap, Range[] rangesAfterNodesJoin)
-    {
-        /* 
-         * Find the ranges that are split. Maintain a mapping between
-         * the range being split and the list of subranges.
-        */                
-        Map<Range, List<Range>> splitRanges = LeaveJoinProtocolHelper.getRangeSplitRangeMapping(oldRangeToEndPointMap.keySet().toArray( new Range[0] ), tokens_);
-        /* Mapping of split ranges to the list of endpoints responsible for the range */                
-        Map<Range, List<EndPoint>> replicasForSplitRanges = new HashMap<Range, List<EndPoint>>();                                
-        Set<Range> rangesSplit = splitRanges.keySet();                
-        for ( Range splitRange : rangesSplit )
-        {
-            replicasForSplitRanges.put( splitRange, oldRangeToEndPointMap.get(splitRange) );
-        }
-        /* Remove the ranges that are split. */
-        for ( Range splitRange : rangesSplit )
-        {
-            oldRangeToEndPointMap.remove(splitRange);
-        }
-        
-        /* Add the subranges of the split range to the map with the same replica set. */
-        for ( Range splitRange : rangesSplit )
-        {
-            List<Range> subRanges = splitRanges.get(splitRange);
-            List<EndPoint> replicas = replicasForSplitRanges.get(splitRange);
-            for ( Range subRange : subRanges )
-            {
-                /* Make sure we clone or else we are hammered. */
-                oldRangeToEndPointMap.put(subRange, new ArrayList<EndPoint>(replicas));
-            }
-        }
-    }
-    
-    /**
-     * Reset the newRangeToEndPointMap and replace the expanded range
-     * with the ranges whose aggregate is the expanded range. This happens
-     * only when nodes leave the ring to migrate to a different position.
-     * 
-     * @param newRangeToEndPointMap all new ranges mapped to the replicas 
-     *        responsible for those ranges.
-     * @param expandedRangeToOldRangeMap mapping between the expanded ranges
-     *        and the ranges whose aggregate is the expanded range.
-     */
-    private void removeExpandedRangesFromNewConfiguration(Map<Range, List<EndPoint>> newRangeToEndPointMap, Map<Range, List<Range>> expandedRangeToOldRangeMap)
-    {
-        /* Get the replicas for the expanded ranges */
-        Map<Range, List<EndPoint>> replicasForExpandedRanges = new HashMap<Range, List<EndPoint>>();
-        Set<Range> expandedRanges = expandedRangeToOldRangeMap.keySet();
-        for ( Range expandedRange : expandedRanges )
-        {            
-            replicasForExpandedRanges.put( expandedRange, newRangeToEndPointMap.get(expandedRange) );
-            newRangeToEndPointMap.remove(expandedRange);            
-        }
-        /* replace the expanded ranges in the newRangeToEndPointMap with the subRanges */
-        for ( Range expandedRange : expandedRanges )
-        {
-            List<Range> subRanges = expandedRangeToOldRangeMap.get(expandedRange);
-            List<EndPoint> replicas = replicasForExpandedRanges.get(expandedRange);          
-            for ( Range subRange : subRanges )
-            {
-                newRangeToEndPointMap.put(subRange, new ArrayList<EndPoint>(replicas));
-            }
-        }        
-    }
-    
-    private Set<Token> getTokensForLeavingNodes()
-    {
-        Set<Token> tokens = new HashSet<Token>();
-        for ( EndPoint target : targets_ )
-        {
-            tokens.add(tokenMetadata_.getToken(target));
-        }        
-        return tokens;
-    }
-    
-    /**
-     * Here we are removing the nodes that need to leave the
-     * ring and trying to calculate what the ranges would look
-     * like w/o them. e.g. if we remove two nodes A and D from
-     * the ring and the order of nodes on the ring is A, B, C
-     * and D. When B is removed the range of C is the old range 
-     * of C and the old range of B. We want a mapping from old
-     * range of B to new range of B. We have 
-     * A----B----C----D----E----F----G and we remove b and e
-     * then we want a mapping from (a, c] --> (a,b], (b, c] and 
-     * (d, f] --> (d, e], (d,f].
-     * @param oldRanges ranges with the previous configuration
-     * @param newRanges ranges with the target endpoints removed.
-     * @return map of expanded range to the list whose aggregate is
-     *             the expanded range.
-     */
-    protected static Map<Range, List<Range>> getExpandedRangeToOldRangeMapping(Range[] oldRanges, Range[] newRanges)
-    {
-        Map<Range, List<Range>> map = new HashMap<Range, List<Range>>();   
-        List<Range> oRanges = new ArrayList<Range>();
-        Collections.addAll(oRanges, oldRanges);
-        List<Range> nRanges = new ArrayList<Range>();
-        Collections.addAll(nRanges, newRanges);
-        
-        /*
-         * Remove the ranges that are the same. 
-         * Now we will be left with the expanded 
-         * ranges in the nRanges list and the 
-         * smaller ranges in the oRanges list. 
-        */
-        for( Range oRange : oldRanges )
-        {            
-            boolean bVal = nRanges.remove(oRange);
-            if ( bVal )
-                oRanges.remove(oRange);
-        }
-        
-        int nSize = nRanges.size();
-        int oSize = oRanges.size();
-        /*
-         * Establish the mapping between expanded ranges
-         * to the smaller ranges whose aggregate is the
-         * expanded range. 
-        */
-        for ( int i = 0; i < nSize; ++i )
-        {
-            Range nRange = nRanges.get(i);
-            for ( int j = 0; j < oSize; ++j )
-            {
-                Range oRange = oRanges.get(j);
-                if ( nRange.contains(oRange.right()) )
-                {
-                    List<Range> smallerRanges = map.get(nRange);
-                    if ( smallerRanges == null )
-                    {
-                        smallerRanges = new ArrayList<Range>();
-                        map.put(nRange, smallerRanges);
-                    }
-                    smallerRanges.add(oRange);
-                    continue;
-                }
-            }
-        }
-        
-        return map;
-    }
-
-    public static void main(String[] args) throws Throwable
-    {
-        StorageService ss = StorageService.instance();
-        ss.updateTokenMetadata(new BigIntegerToken("3"), new EndPoint("A", 7000));
-        ss.updateTokenMetadata(new BigIntegerToken("6"), new EndPoint("B", 7000));
-        ss.updateTokenMetadata(new BigIntegerToken("9"), new EndPoint("C", 7000));
-        ss.updateTokenMetadata(new BigIntegerToken("12"), new EndPoint("D", 7000));
-        ss.updateTokenMetadata(new BigIntegerToken("15"), new EndPoint("E", 7000));
-        ss.updateTokenMetadata(new BigIntegerToken("18"), new EndPoint("F", 7000));
-        ss.updateTokenMetadata(new BigIntegerToken("21"), new EndPoint("G", 7000));
-        ss.updateTokenMetadata(new BigIntegerToken("24"), new EndPoint("H", 7000));
-        
-        Runnable runnable = new LeaveJoinProtocolImpl( new EndPoint[]{new EndPoint("C", 7000), new EndPoint("D", 7000)}, new Token[]{new BigIntegerToken("22"), new BigIntegerToken("23")} );
-        runnable.run();
-    }
-}
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+ import java.util.ArrayList;
+ import java.util.Collections;
+ import java.util.HashMap;
+ import java.util.HashSet;
+ import java.util.List;
+ import java.util.Map;
+ import java.util.Set;
+
+ import org.apache.log4j.Logger;
+
+ import org.apache.cassandra.locator.TokenMetadata;
+ import org.apache.cassandra.net.EndPoint;
+ import org.apache.cassandra.service.StorageService;
+ import org.apache.cassandra.utils.LogUtil;
+
+
+/**
+ * This class performs the exact opposite of the
+ * operations of the BootStrapper class. Given 
+ * a bunch of nodes that need to move it determines 
+ * who they need to hand off data in terms of ranges.
+*/
+public class LeaveJoinProtocolImpl implements Runnable
+{
+    private static Logger logger_ = Logger.getLogger(LeaveJoinProtocolImpl.class);    
+    
+    /* endpoints that are to be moved. */
+    protected EndPoint[] targets_ = new EndPoint[0];
+    /* position where they need to be moved */
+    protected final Token[] tokens_;
+    /* token metadata information */
+    protected TokenMetadata tokenMetadata_ = null;
+
+    public LeaveJoinProtocolImpl(EndPoint[] targets, Token[] tokens)
+    {
+        targets_ = targets;
+        tokens_ = tokens;
+        tokenMetadata_ = StorageService.instance().getTokenMetadata();
+    }
+
+    public void run()
+    {  
+        try
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Beginning leave/join process for ...");                                                               
+            /* copy the token to endpoint map */
+            Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+            /* copy the endpoint to token map */
+            Map<EndPoint, Token> endpointToTokenMap = tokenMetadata_.cloneEndPointTokenMap();
+            
+            Set<Token> oldTokens = new HashSet<Token>( tokenToEndPointMap.keySet() );
+            Range[] oldRanges = StorageService.instance().getAllRanges(oldTokens);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Total number of old ranges " + oldRanges.length);
+            /* Calculate the list of nodes that handle the old ranges */
+            Map<Range, List<EndPoint>> oldRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(oldRanges);
+            
+            /* Remove the tokens of the nodes leaving the ring */
+            Set<Token> tokens = getTokensForLeavingNodes();
+            oldTokens.removeAll(tokens);
+            Range[] rangesAfterNodesLeave = StorageService.instance().getAllRanges(oldTokens);
+            /* Get expanded range to initial range mapping */
+            Map<Range, List<Range>> expandedRangeToOldRangeMap = getExpandedRangeToOldRangeMapping(oldRanges, rangesAfterNodesLeave);
+            /* add the new token positions to the old tokens set */
+            for (Token token : tokens_)
+                oldTokens.add(token);
+            Range[] rangesAfterNodesJoin = StorageService.instance().getAllRanges(oldTokens);
+            /* replace the ranges that were split with the split ranges in the old configuration */
+            addSplitRangesToOldConfiguration(oldRangeToEndPointMap, rangesAfterNodesJoin);
+            
+            /* Re-calculate the new ranges after the new token positions are added */
+            Range[] newRanges = StorageService.instance().getAllRanges(oldTokens);
+            /* Remove the old locations from tokenToEndPointMap and add the new locations they are moving to */
+            for ( int i = 0; i < targets_.length; ++i )
+            {
+                tokenToEndPointMap.remove( endpointToTokenMap.get(targets_[i]) );
+                tokenToEndPointMap.put(tokens_[i], targets_[i]);
+            }            
+            /* Calculate the list of nodes that handle the new ranges */            
+            Map<Range, List<EndPoint>> newRangeToEndPointMap = StorageService.instance().constructRangeToEndPointMap(newRanges, tokenToEndPointMap);
+            /* Remove any expanded ranges and replace them with ranges whose aggregate is the expanded range in the new configuration. */
+            removeExpandedRangesFromNewConfiguration(newRangeToEndPointMap, expandedRangeToOldRangeMap);
+            /* Calculate ranges that need to be sent and from whom to where */
+            Map<Range, List<BootstrapSourceTarget>> rangesWithSourceTarget = LeaveJoinProtocolHelper.getRangeSourceTargetInfo(oldRangeToEndPointMap, newRangeToEndPointMap);
+            /* For debug purposes only */
+            Set<Range> ranges = rangesWithSourceTarget.keySet();
+            for ( Range range : ranges )
+            {
+                System.out.print("RANGE: " + range + ":: ");
+                List<BootstrapSourceTarget> infos = rangesWithSourceTarget.get(range);
+                for ( BootstrapSourceTarget info : infos )
+                {
+                    System.out.print(info);
+                    System.out.print(" ");
+                }
+                System.out.println(System.getProperty("line.separator"));
+            }
+            /* Send messages to respective folks to stream data over to the new nodes being bootstrapped */
+            LeaveJoinProtocolHelper.assignWork(rangesWithSourceTarget);
+        }
+        catch ( Throwable th )
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }
+    }
+    
+    /**
+     * This method figures out the ranges that have been split and
+     * replaces them with the split range.
+     * @param oldRangeToEndPointMap old range mapped to their replicas.
+     * @param rangesAfterNodesJoin ranges after the nodes have joined at
+     *        their respective position.
+     */
+    private void addSplitRangesToOldConfiguration(Map<Range, List<EndPoint>> oldRangeToEndPointMap, Range[] rangesAfterNodesJoin)
+    {
+        /* 
+         * Find the ranges that are split. Maintain a mapping between
+         * the range being split and the list of subranges.
+        */                
+        Map<Range, List<Range>> splitRanges = LeaveJoinProtocolHelper.getRangeSplitRangeMapping(oldRangeToEndPointMap.keySet().toArray( new Range[0] ), tokens_);
+        /* Mapping of split ranges to the list of endpoints responsible for the range */                
+        Map<Range, List<EndPoint>> replicasForSplitRanges = new HashMap<Range, List<EndPoint>>();                                
+        Set<Range> rangesSplit = splitRanges.keySet();                
+        for ( Range splitRange : rangesSplit )
+        {
+            replicasForSplitRanges.put( splitRange, oldRangeToEndPointMap.get(splitRange) );
+        }
+        /* Remove the ranges that are split. */
+        for ( Range splitRange : rangesSplit )
+        {
+            oldRangeToEndPointMap.remove(splitRange);
+        }
+        
+        /* Add the subranges of the split range to the map with the same replica set. */
+        for ( Range splitRange : rangesSplit )
+        {
+            List<Range> subRanges = splitRanges.get(splitRange);
+            List<EndPoint> replicas = replicasForSplitRanges.get(splitRange);
+            for ( Range subRange : subRanges )
+            {
+                /* Make sure we clone or else we are hammered. */
+                oldRangeToEndPointMap.put(subRange, new ArrayList<EndPoint>(replicas));
+            }
+        }
+    }
+    
+    /**
+     * Reset the newRangeToEndPointMap and replace the expanded range
+     * with the ranges whose aggregate is the expanded range. This happens
+     * only when nodes leave the ring to migrate to a different position.
+     * 
+     * @param newRangeToEndPointMap all new ranges mapped to the replicas 
+     *        responsible for those ranges.
+     * @param expandedRangeToOldRangeMap mapping between the expanded ranges
+     *        and the ranges whose aggregate is the expanded range.
+     */
+    private void removeExpandedRangesFromNewConfiguration(Map<Range, List<EndPoint>> newRangeToEndPointMap, Map<Range, List<Range>> expandedRangeToOldRangeMap)
+    {
+        /* Get the replicas for the expanded ranges */
+        Map<Range, List<EndPoint>> replicasForExpandedRanges = new HashMap<Range, List<EndPoint>>();
+        Set<Range> expandedRanges = expandedRangeToOldRangeMap.keySet();
+        for ( Range expandedRange : expandedRanges )
+        {            
+            replicasForExpandedRanges.put( expandedRange, newRangeToEndPointMap.get(expandedRange) );
+            newRangeToEndPointMap.remove(expandedRange);            
+        }
+        /* replace the expanded ranges in the newRangeToEndPointMap with the subRanges */
+        for ( Range expandedRange : expandedRanges )
+        {
+            List<Range> subRanges = expandedRangeToOldRangeMap.get(expandedRange);
+            List<EndPoint> replicas = replicasForExpandedRanges.get(expandedRange);          
+            for ( Range subRange : subRanges )
+            {
+                newRangeToEndPointMap.put(subRange, new ArrayList<EndPoint>(replicas));
+            }
+        }        
+    }
+    
+    private Set<Token> getTokensForLeavingNodes()
+    {
+        Set<Token> tokens = new HashSet<Token>();
+        for ( EndPoint target : targets_ )
+        {
+            tokens.add(tokenMetadata_.getToken(target));
+        }        
+        return tokens;
+    }
+    
+    /**
+     * Here we are removing the nodes that need to leave the
+     * ring and trying to calculate what the ranges would look
+     * like w/o them. e.g. if we remove two nodes A and D from
+     * the ring and the order of nodes on the ring is A, B, C
+     * and D. When B is removed the range of C is the old range 
+     * of C and the old range of B. We want a mapping from old
+     * range of B to new range of B. We have 
+     * A----B----C----D----E----F----G and we remove b and e
+     * then we want a mapping from (a, c] --> (a,b], (b, c] and 
+     * (d, f] --> (d, e], (d,f].
+     * @param oldRanges ranges with the previous configuration
+     * @param newRanges ranges with the target endpoints removed.
+     * @return map of expanded range to the list whose aggregate is
+     *             the expanded range.
+     */
+    protected static Map<Range, List<Range>> getExpandedRangeToOldRangeMapping(Range[] oldRanges, Range[] newRanges)
+    {
+        Map<Range, List<Range>> map = new HashMap<Range, List<Range>>();   
+        List<Range> oRanges = new ArrayList<Range>();
+        Collections.addAll(oRanges, oldRanges);
+        List<Range> nRanges = new ArrayList<Range>();
+        Collections.addAll(nRanges, newRanges);
+        
+        /*
+         * Remove the ranges that are the same. 
+         * Now we will be left with the expanded 
+         * ranges in the nRanges list and the 
+         * smaller ranges in the oRanges list. 
+        */
+        for( Range oRange : oldRanges )
+        {            
+            boolean bVal = nRanges.remove(oRange);
+            if ( bVal )
+                oRanges.remove(oRange);
+        }
+        
+        int nSize = nRanges.size();
+        int oSize = oRanges.size();
+        /*
+         * Establish the mapping between expanded ranges
+         * to the smaller ranges whose aggregate is the
+         * expanded range. 
+        */
+        for ( int i = 0; i < nSize; ++i )
+        {
+            Range nRange = nRanges.get(i);
+            for ( int j = 0; j < oSize; ++j )
+            {
+                Range oRange = oRanges.get(j);
+                if ( nRange.contains(oRange.right()) )
+                {
+                    List<Range> smallerRanges = map.get(nRange);
+                    if ( smallerRanges == null )
+                    {
+                        smallerRanges = new ArrayList<Range>();
+                        map.put(nRange, smallerRanges);
+                    }
+                    smallerRanges.add(oRange);
+                    continue;
+                }
+            }
+        }
+        
+        return map;
+    }
+
+    public static void main(String[] args) throws Throwable
+    {
+        StorageService ss = StorageService.instance();
+        ss.updateTokenMetadata(new BigIntegerToken("3"), new EndPoint("A", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("6"), new EndPoint("B", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("9"), new EndPoint("C", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("12"), new EndPoint("D", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("15"), new EndPoint("E", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("18"), new EndPoint("F", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("21"), new EndPoint("G", 7000));
+        ss.updateTokenMetadata(new BigIntegerToken("24"), new EndPoint("H", 7000));
+        
+        Runnable runnable = new LeaveJoinProtocolImpl( new EndPoint[]{new EndPoint("C", 7000), new EndPoint("D", 7000)}, new Token[]{new BigIntegerToken("22"), new BigIntegerToken("23")} );
+        runnable.run();
+    }
+}
diff --git a/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java b/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
index c44c73b882..b747850608 100644
--- a/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
+++ b/src/java/org/apache/cassandra/dht/OrderPreservingPartitioner.java
@@ -1,129 +1,129 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.io.UnsupportedEncodingException;
-import java.text.Collator;
-import java.util.Comparator;
-import java.util.Locale;
-import java.util.Random;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-
-public class OrderPreservingPartitioner implements IPartitioner
-{
-    // TODO make locale configurable.  But don't just leave it up to the OS or you could really screw
-    // people over if they deploy on nodes with different OS locales.
-    static final Collator collator = Collator.getInstance(new Locale("en", "US")); 
-
-    private static final Comparator<String> comparator = new Comparator<String>() {
-        public int compare(String o1, String o2)
-        {
-            return collator.compare(o1, o2);
-        }
-    };
-    private static final Comparator<String> reverseComparator = new Comparator<String>() {
-        public int compare(String o1, String o2)
-        {
-            return -comparator.compare(o1, o2);
-        }
-    };
-
-    public String decorateKey(String key)
-    {
-        return key;
-    }
-
-    public String undecorateKey(String decoratedKey)
-    {
-        return decoratedKey;
-    }
-
-    public Comparator<String> getDecoratedKeyComparator()
-    {
-        return comparator;
-    }
-
-    public Comparator<String> getReverseDecoratedKeyComparator()
-    {
-        return reverseComparator;
-    }
-
-    public StringToken getDefaultToken()
-    {
-        String initialToken = DatabaseDescriptor.getInitialToken();
-        if (initialToken != null)
-            return new StringToken(initialToken);
-
-        // generate random token
-        String chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";
-        Random r = new Random();
-        StringBuilder buffer = new StringBuilder();
-        for (int j = 0; j < 16; j++) {
-            buffer.append(chars.charAt(r.nextInt(chars.length())));
-        }
-        return new StringToken(buffer.toString());
-    }
-
-    private final Token.TokenFactory<String> tokenFactory = new Token.TokenFactory<String>() {
-        public byte[] toByteArray(Token<String> stringToken)
-        {
-            try
-            {
-                return stringToken.token.getBytes("UTF-8");
-            }
-            catch (UnsupportedEncodingException e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-
-        public Token<String> fromByteArray(byte[] bytes)
-        {
-            try
-            {
-                return new StringToken(new String(bytes, "UTF-8"));
-            }
-            catch (UnsupportedEncodingException e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-
-        public String toString(Token<String> stringToken)
-        {
-            return stringToken.token;
-        }
-
-        public Token<String> fromString(String string)
-        {
-            return new StringToken(string);
-        }
-    };
-
-    public Token.TokenFactory<String> getTokenFactory()
-    {
-        return tokenFactory;
-    }
-
-    public Token getInitialToken(String key)
-    {
-        return new StringToken(key);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.UnsupportedEncodingException;
+import java.text.Collator;
+import java.util.Comparator;
+import java.util.Locale;
+import java.util.Random;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+public class OrderPreservingPartitioner implements IPartitioner
+{
+    // TODO make locale configurable.  But don't just leave it up to the OS or you could really screw
+    // people over if they deploy on nodes with different OS locales.
+    static final Collator collator = Collator.getInstance(new Locale("en", "US")); 
+
+    private static final Comparator<String> comparator = new Comparator<String>() {
+        public int compare(String o1, String o2)
+        {
+            return collator.compare(o1, o2);
+        }
+    };
+    private static final Comparator<String> reverseComparator = new Comparator<String>() {
+        public int compare(String o1, String o2)
+        {
+            return -comparator.compare(o1, o2);
+        }
+    };
+
+    public String decorateKey(String key)
+    {
+        return key;
+    }
+
+    public String undecorateKey(String decoratedKey)
+    {
+        return decoratedKey;
+    }
+
+    public Comparator<String> getDecoratedKeyComparator()
+    {
+        return comparator;
+    }
+
+    public Comparator<String> getReverseDecoratedKeyComparator()
+    {
+        return reverseComparator;
+    }
+
+    public StringToken getDefaultToken()
+    {
+        String initialToken = DatabaseDescriptor.getInitialToken();
+        if (initialToken != null)
+            return new StringToken(initialToken);
+
+        // generate random token
+        String chars = "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789";
+        Random r = new Random();
+        StringBuilder buffer = new StringBuilder();
+        for (int j = 0; j < 16; j++) {
+            buffer.append(chars.charAt(r.nextInt(chars.length())));
+        }
+        return new StringToken(buffer.toString());
+    }
+
+    private final Token.TokenFactory<String> tokenFactory = new Token.TokenFactory<String>() {
+        public byte[] toByteArray(Token<String> stringToken)
+        {
+            try
+            {
+                return stringToken.token.getBytes("UTF-8");
+            }
+            catch (UnsupportedEncodingException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+
+        public Token<String> fromByteArray(byte[] bytes)
+        {
+            try
+            {
+                return new StringToken(new String(bytes, "UTF-8"));
+            }
+            catch (UnsupportedEncodingException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+
+        public String toString(Token<String> stringToken)
+        {
+            return stringToken.token;
+        }
+
+        public Token<String> fromString(String string)
+        {
+            return new StringToken(string);
+        }
+    };
+
+    public Token.TokenFactory<String> getTokenFactory()
+    {
+        return tokenFactory;
+    }
+
+    public Token getInitialToken(String key)
+    {
+        return new StringToken(key);
+    }
+}
diff --git a/src/java/org/apache/cassandra/dht/RandomPartitioner.java b/src/java/org/apache/cassandra/dht/RandomPartitioner.java
index 80d3e9ae32..7eb927dd8c 100644
--- a/src/java/org/apache/cassandra/dht/RandomPartitioner.java
+++ b/src/java/org/apache/cassandra/dht/RandomPartitioner.java
@@ -1,121 +1,121 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.math.BigInteger;
-import java.util.Comparator;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.utils.GuidGenerator;
-
-/**
- * This class generates a BigIntegerToken using MD5 hash.
- */
-public class RandomPartitioner implements IPartitioner
-{
-    private static final Comparator<String> comparator = new Comparator<String>()
-    {
-        public int compare(String o1, String o2)
-        {
-            String[] split1 = o1.split(":", 2);
-            String[] split2 = o2.split(":", 2);
-            BigInteger i1 = new BigInteger(split1[0]);
-            BigInteger i2 = new BigInteger(split2[0]);
-            int v = i1.compareTo(i2);
-            if (v != 0) {
-                return v;
-            }
-            return split1[1].compareTo(split2[1]);
-        }
-    };
-    private static final Comparator<String> rcomparator = new Comparator<String>()
-    {
-        public int compare(String o1, String o2)
-        {
-            return -comparator.compare(o1, o2);
-        }
-    };
-
-    public String decorateKey(String key)
-    {
-        return FBUtilities.hash(key).toString() + ":" + key;
-    }
-
-    public String undecorateKey(String decoratedKey)
-    {
-        return decoratedKey.split(":", 2)[1];
-    }
-
-    public Comparator<String> getDecoratedKeyComparator()
-    {
-        return comparator;
-    }
-
-    public Comparator<String> getReverseDecoratedKeyComparator()
-    {
-        return rcomparator;
-    }
-
-    public BigIntegerToken getDefaultToken()
-    {
-        String initialToken = DatabaseDescriptor.getInitialToken();
-        if (initialToken != null)
-            return new BigIntegerToken(new BigInteger(initialToken));
-
-        // generate random token
-        String guid = GuidGenerator.guid();
-        BigInteger token = FBUtilities.hash(guid);
-        if ( token.signum() == -1 )
-            token = token.multiply(BigInteger.valueOf(-1L));
-        return new BigIntegerToken(token);
-    }
-
-    private final Token.TokenFactory<BigInteger> tokenFactory = new Token.TokenFactory<BigInteger>() {
-        public byte[] toByteArray(Token<BigInteger> bigIntegerToken)
-        {
-            return bigIntegerToken.token.toByteArray();
-        }
-
-        public Token<BigInteger> fromByteArray(byte[] bytes)
-        {
-            return new BigIntegerToken(new BigInteger(bytes));
-        }
-
-        public String toString(Token<BigInteger> bigIntegerToken)
-        {
-            return bigIntegerToken.token.toString();
-        }
-
-        public Token<BigInteger> fromString(String string)
-        {
-            return new BigIntegerToken(new BigInteger(string));
-        }
-    };
-
-    public Token.TokenFactory<BigInteger> getTokenFactory()
-    {
-        return tokenFactory;
-    }
-
-    public Token getInitialToken(String key)
-    {
-        return new BigIntegerToken(FBUtilities.hash(key));
-    }
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.math.BigInteger;
+import java.util.Comparator;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.GuidGenerator;
+
+/**
+ * This class generates a BigIntegerToken using MD5 hash.
+ */
+public class RandomPartitioner implements IPartitioner
+{
+    private static final Comparator<String> comparator = new Comparator<String>()
+    {
+        public int compare(String o1, String o2)
+        {
+            String[] split1 = o1.split(":", 2);
+            String[] split2 = o2.split(":", 2);
+            BigInteger i1 = new BigInteger(split1[0]);
+            BigInteger i2 = new BigInteger(split2[0]);
+            int v = i1.compareTo(i2);
+            if (v != 0) {
+                return v;
+            }
+            return split1[1].compareTo(split2[1]);
+        }
+    };
+    private static final Comparator<String> rcomparator = new Comparator<String>()
+    {
+        public int compare(String o1, String o2)
+        {
+            return -comparator.compare(o1, o2);
+        }
+    };
+
+    public String decorateKey(String key)
+    {
+        return FBUtilities.hash(key).toString() + ":" + key;
+    }
+
+    public String undecorateKey(String decoratedKey)
+    {
+        return decoratedKey.split(":", 2)[1];
+    }
+
+    public Comparator<String> getDecoratedKeyComparator()
+    {
+        return comparator;
+    }
+
+    public Comparator<String> getReverseDecoratedKeyComparator()
+    {
+        return rcomparator;
+    }
+
+    public BigIntegerToken getDefaultToken()
+    {
+        String initialToken = DatabaseDescriptor.getInitialToken();
+        if (initialToken != null)
+            return new BigIntegerToken(new BigInteger(initialToken));
+
+        // generate random token
+        String guid = GuidGenerator.guid();
+        BigInteger token = FBUtilities.hash(guid);
+        if ( token.signum() == -1 )
+            token = token.multiply(BigInteger.valueOf(-1L));
+        return new BigIntegerToken(token);
+    }
+
+    private final Token.TokenFactory<BigInteger> tokenFactory = new Token.TokenFactory<BigInteger>() {
+        public byte[] toByteArray(Token<BigInteger> bigIntegerToken)
+        {
+            return bigIntegerToken.token.toByteArray();
+        }
+
+        public Token<BigInteger> fromByteArray(byte[] bytes)
+        {
+            return new BigIntegerToken(new BigInteger(bytes));
+        }
+
+        public String toString(Token<BigInteger> bigIntegerToken)
+        {
+            return bigIntegerToken.token.toString();
+        }
+
+        public Token<BigInteger> fromString(String string)
+        {
+            return new BigIntegerToken(new BigInteger(string));
+        }
+    };
+
+    public Token.TokenFactory<BigInteger> getTokenFactory()
+    {
+        return tokenFactory;
+    }
+
+    public Token getInitialToken(String key)
+    {
+        return new BigIntegerToken(FBUtilities.hash(key));
+    }
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/dht/Range.java b/src/java/org/apache/cassandra/dht/Range.java
index 9eb3dfcd20..ec6698ec81 100644
--- a/src/java/org/apache/cassandra/dht/Range.java
+++ b/src/java/org/apache/cassandra/dht/Range.java
@@ -1,186 +1,186 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.dht;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.Serializable;
-import java.util.List;
-import java.math.BigInteger;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.service.StorageService;
-
-
-/**
- * A representation of the range that a node is responsible for on the DHT ring.
- *
- * A Range is responsible for the tokens between [left, right).
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class Range implements Comparable<Range>, Serializable
-{
-    private static ICompactSerializer<Range> serializer_;
-    static
-    {
-        serializer_ = new RangeSerializer();
-    }
-    
-    public static ICompactSerializer<Range> serializer()
-    {
-        return serializer_;
-    }
-
-    private final Token left_;
-    private final Token right_;
-
-    public Range(Token left, Token right)
-    {
-        left_ = left;
-        right_ = right;
-    }
-
-    /**
-     * Returns the left endpoint of a range.
-     * @return left endpoint
-     */
-    public Token left()
-    {
-        return left_;
-    }
-    
-    /**
-     * Returns the right endpoint of a range.
-     * @return right endpoint
-     */
-    public Token right()
-    {
-        return right_;
-    }
-
-    /**
-     * Helps determine if a given point on the DHT ring is contained
-     * in the range in question.
-     * @param bi point in question
-     * @return true if the point contains within the range else false.
-     */
-    public boolean contains(Token bi)
-    {
-        if ( left_.compareTo(right_) > 0 )
-        {
-            /* 
-             * left is greater than right we are wrapping around.
-             * So if the interval is [a,b) where a > b then we have
-             * 3 cases one of which holds for any given token k.
-             * (1) k > a -- return true
-             * (2) k < b -- return true
-             * (3) b < k < a -- return false
-            */
-            if ( bi.compareTo(left_) >= 0 )
-                return true;
-            else return right_.compareTo(bi) > 0;
-        }
-        else if ( left_.compareTo(right_) < 0 )
-        {
-            /*
-             * This is the range [a, b) where a < b. 
-            */
-            return ( bi.compareTo(left_) >= 0 && right_.compareTo(bi) > 0 );
-        }        
-        else
-    	{
-    		return true;
-    	}    	
-    }
-
-    /**
-     * Tells if the given range is a wrap around.
-     * @param range
-     * @return
-     */
-    private static boolean isWrapAround(Range range)
-    {
-        return range.left_.compareTo(range.right_) > 0;
-    }
-    
-    public int compareTo(Range rhs)
-    {
-        /* 
-         * If the range represented by the "this" pointer
-         * is a wrap around then it is the smaller one.
-        */
-        if ( isWrapAround(this) )
-            return -1;
-        
-        if ( isWrapAround(rhs) )
-            return 1;
-        
-        return right_.compareTo(rhs.right_);
-    }
-    
-
-    public static boolean isTokenInRanges(Token token, List<Range> ranges)
-    {
-        assert ranges != null;
-
-        for (Range range : ranges)
-        {
-            if(range.contains(token))
-            {
-                return true;
-            }
-        }
-        return false;
-    }
-
-    public boolean equals(Object o)
-    {
-        if ( !(o instanceof Range) )
-            return false;
-        Range rhs = (Range)o;
-        return left_.equals(rhs.left_) && right_.equals(rhs.right_);
-    }
-    
-    public int hashCode()
-    {
-        return toString().hashCode();
-    }
-    
-    public String toString()
-    {
-        return "(" + left_ + "," + right_ + "]";
-    }
-}
-
-class RangeSerializer implements ICompactSerializer<Range>
-{
-    public void serialize(Range range, DataOutputStream dos) throws IOException
-    {
-        Token.serializer().serialize(range.left(), dos);
-        Token.serializer().serialize(range.right(), dos);
-    }
-
-    public Range deserialize(DataInputStream dis) throws IOException
-    {
-        return new Range(Token.serializer().deserialize(dis), Token.serializer().deserialize(dis));
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.dht;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.List;
+import java.math.BigInteger;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.service.StorageService;
+
+
+/**
+ * A representation of the range that a node is responsible for on the DHT ring.
+ *
+ * A Range is responsible for the tokens between [left, right).
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Range implements Comparable<Range>, Serializable
+{
+    private static ICompactSerializer<Range> serializer_;
+    static
+    {
+        serializer_ = new RangeSerializer();
+    }
+    
+    public static ICompactSerializer<Range> serializer()
+    {
+        return serializer_;
+    }
+
+    private final Token left_;
+    private final Token right_;
+
+    public Range(Token left, Token right)
+    {
+        left_ = left;
+        right_ = right;
+    }
+
+    /**
+     * Returns the left endpoint of a range.
+     * @return left endpoint
+     */
+    public Token left()
+    {
+        return left_;
+    }
+    
+    /**
+     * Returns the right endpoint of a range.
+     * @return right endpoint
+     */
+    public Token right()
+    {
+        return right_;
+    }
+
+    /**
+     * Helps determine if a given point on the DHT ring is contained
+     * in the range in question.
+     * @param bi point in question
+     * @return true if the point contains within the range else false.
+     */
+    public boolean contains(Token bi)
+    {
+        if ( left_.compareTo(right_) > 0 )
+        {
+            /* 
+             * left is greater than right we are wrapping around.
+             * So if the interval is [a,b) where a > b then we have
+             * 3 cases one of which holds for any given token k.
+             * (1) k > a -- return true
+             * (2) k < b -- return true
+             * (3) b < k < a -- return false
+            */
+            if ( bi.compareTo(left_) >= 0 )
+                return true;
+            else return right_.compareTo(bi) > 0;
+        }
+        else if ( left_.compareTo(right_) < 0 )
+        {
+            /*
+             * This is the range [a, b) where a < b. 
+            */
+            return ( bi.compareTo(left_) >= 0 && right_.compareTo(bi) > 0 );
+        }        
+        else
+    	{
+    		return true;
+    	}    	
+    }
+
+    /**
+     * Tells if the given range is a wrap around.
+     * @param range
+     * @return
+     */
+    private static boolean isWrapAround(Range range)
+    {
+        return range.left_.compareTo(range.right_) > 0;
+    }
+    
+    public int compareTo(Range rhs)
+    {
+        /* 
+         * If the range represented by the "this" pointer
+         * is a wrap around then it is the smaller one.
+        */
+        if ( isWrapAround(this) )
+            return -1;
+        
+        if ( isWrapAround(rhs) )
+            return 1;
+        
+        return right_.compareTo(rhs.right_);
+    }
+    
+
+    public static boolean isTokenInRanges(Token token, List<Range> ranges)
+    {
+        assert ranges != null;
+
+        for (Range range : ranges)
+        {
+            if(range.contains(token))
+            {
+                return true;
+            }
+        }
+        return false;
+    }
+
+    public boolean equals(Object o)
+    {
+        if ( !(o instanceof Range) )
+            return false;
+        Range rhs = (Range)o;
+        return left_.equals(rhs.left_) && right_.equals(rhs.right_);
+    }
+    
+    public int hashCode()
+    {
+        return toString().hashCode();
+    }
+    
+    public String toString()
+    {
+        return "(" + left_ + "," + right_ + "]";
+    }
+}
+
+class RangeSerializer implements ICompactSerializer<Range>
+{
+    public void serialize(Range range, DataOutputStream dos) throws IOException
+    {
+        Token.serializer().serialize(range.left(), dos);
+        Token.serializer().serialize(range.right(), dos);
+    }
+
+    public Range deserialize(DataInputStream dis) throws IOException
+    {
+        return new Range(Token.serializer().deserialize(dis), Token.serializer().deserialize(dis));
+    }
+}
diff --git a/src/java/org/apache/cassandra/gms/ApplicationState.java b/src/java/org/apache/cassandra/gms/ApplicationState.java
index f7c247f1c5..2d5dac4ee4 100644
--- a/src/java/org/apache/cassandra/gms/ApplicationState.java
+++ b/src/java/org/apache/cassandra/gms/ApplicationState.java
@@ -1,101 +1,101 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.cassandra.io.ICompactSerializer;
-
-
-/**
- * This abstraction represents the state associated with a particular node which an
- * application wants to make available to the rest of the nodes in the cluster. 
- * Whenever a piece of state needs to be disseminated to the rest of cluster wrap
- * the state in an instance of <i>ApplicationState</i> and add it to the Gossiper.
- *  
- * e.g. if we want to disseminate load information for node A do the following:
- * 
- *      ApplicationState loadState = new ApplicationState(<string representation of load>);
- *      Gossiper.instance().addApplicationState("LOAD STATE", loadState);
- *  
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class ApplicationState
-{
-    private static ICompactSerializer<ApplicationState> serializer_;
-    static
-    {
-        serializer_ = new ApplicationStateSerializer();
-    }
-    
-    int version_;
-    String state_;
-
-        
-    ApplicationState(String state, int version)
-    {
-        state_ = state;
-        version_ = version;
-    }
-
-    public static ICompactSerializer<ApplicationState> serializer()
-    {
-        return serializer_;
-    }
-    
-    /**
-     * Wraps the specified state into a ApplicationState instance.
-     * @param state string representation of arbitrary state.
-     */
-    public ApplicationState(String state)
-    {
-        state_ = state;
-        version_ = VersionGenerator.getNextVersion();
-    }
-        
-    public String getState()
-    {
-        return state_;
-    }
-    
-    int getStateVersion()
-    {
-        return version_;
-    }
-}
-
-class ApplicationStateSerializer implements ICompactSerializer<ApplicationState>
-{
-    public void serialize(ApplicationState appState, DataOutputStream dos) throws IOException
-    {
-        dos.writeUTF(appState.state_);
-        dos.writeInt(appState.version_);
-    }
-
-    public ApplicationState deserialize(DataInputStream dis) throws IOException
-    {
-        String state = dis.readUTF();
-        int version = dis.readInt();
-        return new ApplicationState(state, version);
-    }
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+
+
+/**
+ * This abstraction represents the state associated with a particular node which an
+ * application wants to make available to the rest of the nodes in the cluster. 
+ * Whenever a piece of state needs to be disseminated to the rest of cluster wrap
+ * the state in an instance of <i>ApplicationState</i> and add it to the Gossiper.
+ *  
+ * e.g. if we want to disseminate load information for node A do the following:
+ * 
+ *      ApplicationState loadState = new ApplicationState(<string representation of load>);
+ *      Gossiper.instance().addApplicationState("LOAD STATE", loadState);
+ *  
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ApplicationState
+{
+    private static ICompactSerializer<ApplicationState> serializer_;
+    static
+    {
+        serializer_ = new ApplicationStateSerializer();
+    }
+    
+    int version_;
+    String state_;
+
+        
+    ApplicationState(String state, int version)
+    {
+        state_ = state;
+        version_ = version;
+    }
+
+    public static ICompactSerializer<ApplicationState> serializer()
+    {
+        return serializer_;
+    }
+    
+    /**
+     * Wraps the specified state into a ApplicationState instance.
+     * @param state string representation of arbitrary state.
+     */
+    public ApplicationState(String state)
+    {
+        state_ = state;
+        version_ = VersionGenerator.getNextVersion();
+    }
+        
+    public String getState()
+    {
+        return state_;
+    }
+    
+    int getStateVersion()
+    {
+        return version_;
+    }
+}
+
+class ApplicationStateSerializer implements ICompactSerializer<ApplicationState>
+{
+    public void serialize(ApplicationState appState, DataOutputStream dos) throws IOException
+    {
+        dos.writeUTF(appState.state_);
+        dos.writeInt(appState.version_);
+    }
+
+    public ApplicationState deserialize(DataInputStream dis) throws IOException
+    {
+        String state = dis.readUTF();
+        int version = dis.readInt();
+        return new ApplicationState(state, version);
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/gms/EndPointState.java b/src/java/org/apache/cassandra/gms/EndPointState.java
index 82bb15d798..c8d8060441 100644
--- a/src/java/org/apache/cassandra/gms/EndPointState.java
+++ b/src/java/org/apache/cassandra/gms/EndPointState.java
@@ -1,184 +1,184 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.*;
-import org.apache.cassandra.io.ICompactSerializer;
-
-import org.apache.log4j.Logger;
-
-/**
- * This abstraction represents both the HeartBeatState and the ApplicationState in an EndPointState
- * instance. Any state for a given endpoint can be retrieved from this instance.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class EndPointState
-{
-    private static ICompactSerializer<EndPointState> serializer_;
-    static
-    {
-        serializer_ = new EndPointStateSerializer();
-    }
-    
-    HeartBeatState hbState_;
-    Map<String, ApplicationState> applicationState_ = new Hashtable<String, ApplicationState>();
-    
-    /* fields below do not get serialized */
-    long updateTimestamp_;
-    boolean isAlive_;
-    boolean isAGossiper_;
-
-    public static ICompactSerializer<EndPointState> serializer()
-    {
-        return serializer_;
-    }
-    
-    EndPointState(HeartBeatState hbState) 
-    { 
-        hbState_ = hbState; 
-        updateTimestamp_ = System.currentTimeMillis(); 
-        isAlive_ = true; 
-        isAGossiper_ = false;
-    }
-        
-    HeartBeatState getHeartBeatState()
-    {
-        return hbState_;
-    }
-    
-    synchronized void setHeartBeatState(HeartBeatState hbState)
-    {
-        updateTimestamp();
-        hbState_ = hbState;
-    }
-    
-    public ApplicationState getApplicationState(String key)
-    {
-        return applicationState_.get(key);
-    }
-    
-    public Map<String, ApplicationState> getApplicationState()
-    {
-        return applicationState_;
-    }
-    
-    void addApplicationState(String key, ApplicationState appState)
-    {        
-        applicationState_.put(key, appState);        
-    }
-
-    /* getters and setters */
-    long getUpdateTimestamp()
-    {
-        return updateTimestamp_;
-    }
-    
-    synchronized void updateTimestamp()
-    {
-        updateTimestamp_ = System.currentTimeMillis();
-    }
-    
-    public boolean isAlive()
-    {        
-        return isAlive_;
-    }
-
-    synchronized void isAlive(boolean value)
-    {        
-        isAlive_ = value;        
-    }
-
-    
-    boolean isAGossiper()
-    {        
-        return isAGossiper_;
-    }
-
-    synchronized void isAGossiper(boolean value)
-    {                
-        //isAlive_ = false;
-        isAGossiper_ = value;        
-    }
-}
-
-class EndPointStateSerializer implements ICompactSerializer<EndPointState>
-{
-    private static Logger logger_ = Logger.getLogger(EndPointStateSerializer.class);
-    
-    public void serialize(EndPointState epState, DataOutputStream dos) throws IOException
-    {
-        /* These are for estimating whether we overshoot the MTU limit */
-        int estimate = 0;
-
-        /* serialize the HeartBeatState */
-        HeartBeatState hbState = epState.getHeartBeatState();
-        HeartBeatState.serializer().serialize(hbState, dos);
-
-        /* serialize the map of ApplicationState objects */
-        int size = epState.applicationState_.size();
-        dos.writeInt(size);
-        if ( size > 0 )
-        {   
-            Set<String> keys = epState.applicationState_.keySet();
-            for( String key : keys )
-            {
-                if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
-                {
-                    logger_.info("@@@@ Breaking out to respect the MTU size in EndPointState serializer. Estimate is " + estimate + " @@@@");
-                    break;
-                }
-            
-                ApplicationState appState = epState.applicationState_.get(key);
-                if ( appState != null )
-                {
-                    int pre = dos.size();
-                    dos.writeUTF(key);
-                    ApplicationState.serializer().serialize(appState, dos);                    
-                    int post = dos.size();
-                    estimate = post - pre;
-                }                
-            }
-        }
-    }
-
-    public EndPointState deserialize(DataInputStream dis) throws IOException
-    {
-        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis);
-        EndPointState epState = new EndPointState(hbState);               
-
-        int appStateSize = dis.readInt();
-        for ( int i = 0; i < appStateSize; ++i )
-        {
-            if ( dis.available() == 0 )
-            {
-                break;
-            }
-            
-            String key = dis.readUTF();    
-            ApplicationState appState = ApplicationState.serializer().deserialize(dis);            
-            epState.addApplicationState(key, appState);            
-        }
-        return epState;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+import org.apache.cassandra.io.ICompactSerializer;
+
+import org.apache.log4j.Logger;
+
+/**
+ * This abstraction represents both the HeartBeatState and the ApplicationState in an EndPointState
+ * instance. Any state for a given endpoint can be retrieved from this instance.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class EndPointState
+{
+    private static ICompactSerializer<EndPointState> serializer_;
+    static
+    {
+        serializer_ = new EndPointStateSerializer();
+    }
+    
+    HeartBeatState hbState_;
+    Map<String, ApplicationState> applicationState_ = new Hashtable<String, ApplicationState>();
+    
+    /* fields below do not get serialized */
+    long updateTimestamp_;
+    boolean isAlive_;
+    boolean isAGossiper_;
+
+    public static ICompactSerializer<EndPointState> serializer()
+    {
+        return serializer_;
+    }
+    
+    EndPointState(HeartBeatState hbState) 
+    { 
+        hbState_ = hbState; 
+        updateTimestamp_ = System.currentTimeMillis(); 
+        isAlive_ = true; 
+        isAGossiper_ = false;
+    }
+        
+    HeartBeatState getHeartBeatState()
+    {
+        return hbState_;
+    }
+    
+    synchronized void setHeartBeatState(HeartBeatState hbState)
+    {
+        updateTimestamp();
+        hbState_ = hbState;
+    }
+    
+    public ApplicationState getApplicationState(String key)
+    {
+        return applicationState_.get(key);
+    }
+    
+    public Map<String, ApplicationState> getApplicationState()
+    {
+        return applicationState_;
+    }
+    
+    void addApplicationState(String key, ApplicationState appState)
+    {        
+        applicationState_.put(key, appState);        
+    }
+
+    /* getters and setters */
+    long getUpdateTimestamp()
+    {
+        return updateTimestamp_;
+    }
+    
+    synchronized void updateTimestamp()
+    {
+        updateTimestamp_ = System.currentTimeMillis();
+    }
+    
+    public boolean isAlive()
+    {        
+        return isAlive_;
+    }
+
+    synchronized void isAlive(boolean value)
+    {        
+        isAlive_ = value;        
+    }
+
+    
+    boolean isAGossiper()
+    {        
+        return isAGossiper_;
+    }
+
+    synchronized void isAGossiper(boolean value)
+    {                
+        //isAlive_ = false;
+        isAGossiper_ = value;        
+    }
+}
+
+class EndPointStateSerializer implements ICompactSerializer<EndPointState>
+{
+    private static Logger logger_ = Logger.getLogger(EndPointStateSerializer.class);
+    
+    public void serialize(EndPointState epState, DataOutputStream dos) throws IOException
+    {
+        /* These are for estimating whether we overshoot the MTU limit */
+        int estimate = 0;
+
+        /* serialize the HeartBeatState */
+        HeartBeatState hbState = epState.getHeartBeatState();
+        HeartBeatState.serializer().serialize(hbState, dos);
+
+        /* serialize the map of ApplicationState objects */
+        int size = epState.applicationState_.size();
+        dos.writeInt(size);
+        if ( size > 0 )
+        {   
+            Set<String> keys = epState.applicationState_.keySet();
+            for( String key : keys )
+            {
+                if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
+                {
+                    logger_.info("@@@@ Breaking out to respect the MTU size in EndPointState serializer. Estimate is " + estimate + " @@@@");
+                    break;
+                }
+            
+                ApplicationState appState = epState.applicationState_.get(key);
+                if ( appState != null )
+                {
+                    int pre = dos.size();
+                    dos.writeUTF(key);
+                    ApplicationState.serializer().serialize(appState, dos);                    
+                    int post = dos.size();
+                    estimate = post - pre;
+                }                
+            }
+        }
+    }
+
+    public EndPointState deserialize(DataInputStream dis) throws IOException
+    {
+        HeartBeatState hbState = HeartBeatState.serializer().deserialize(dis);
+        EndPointState epState = new EndPointState(hbState);               
+
+        int appStateSize = dis.readInt();
+        for ( int i = 0; i < appStateSize; ++i )
+        {
+            if ( dis.available() == 0 )
+            {
+                break;
+            }
+            
+            String key = dis.readUTF();    
+            ApplicationState appState = ApplicationState.serializer().deserialize(dis);            
+            epState.addApplicationState(key, appState);            
+        }
+        return epState;
+    }
+}
diff --git a/src/java/org/apache/cassandra/gms/FailureDetector.java b/src/java/org/apache/cassandra/gms/FailureDetector.java
index 541abfc04d..462acd2ff0 100644
--- a/src/java/org/apache/cassandra/gms/FailureDetector.java
+++ b/src/java/org/apache/cassandra/gms/FailureDetector.java
@@ -1,319 +1,319 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.io.FileOutputStream;
-import java.lang.management.ManagementFactory;
-import java.net.UnknownHostException;
-import java.util.*;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-import javax.management.MBeanServer;
-import javax.management.ObjectName;
-
-import org.apache.commons.lang.StringUtils;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.cassandra.utils.BoundedStatsDeque;
-import org.apache.log4j.Logger;
-
-/**
- * This FailureDetector is an implementation of the paper titled
- * "The Phi Accrual Failure Detector" by Hayashibara. 
- * Check the paper and the <i>IFailureDetector</i> interface for details.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public class FailureDetector implements IFailureDetector, FailureDetectorMBean
-{
-    private static Logger logger_ = Logger.getLogger(FailureDetector.class);
-    private static final int sampleSize_ = 1000;
-    private static final int phiSuspectThreshold_ = 5;
-    private static final int phiConvictThreshold_ = 8;
-    /* The Failure Detector has to have been up for at least 1 min. */
-    private static final long uptimeThreshold_ = 60000;
-    private static IFailureDetector failureDetector_;
-    /* Used to lock the factory for creation of FailureDetector instance */
-    private static Lock createLock_ = new ReentrantLock();
-    /* The time when the module was instantiated. */
-    private static long creationTime_;
-    
-    public static IFailureDetector instance()
-    {        
-        if ( failureDetector_ == null )
-        {
-            FailureDetector.createLock_.lock();
-            try
-            {
-                if ( failureDetector_ == null )
-                {
-                    failureDetector_ = new FailureDetector();
-                }
-            }
-            finally
-            {
-                createLock_.unlock();
-            }
-        }        
-        return failureDetector_;
-    }
-    
-    private Map<EndPoint, ArrivalWindow> arrivalSamples_ = new Hashtable<EndPoint, ArrivalWindow>();
-    private List<IFailureDetectionEventListener> fdEvntListeners_ = new ArrayList<IFailureDetectionEventListener>();
-    
-    public FailureDetector()
-    {
-        creationTime_ = System.currentTimeMillis();
-        // Register this instance with JMX
-        try
-        {
-            MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
-            mbs.registerMBean(this, new ObjectName("org.apache.cassandra.gms:type=FailureDetector"));
-        }
-        catch (Exception e)
-        {
-            logger_.error(LogUtil.throwableToString(e));
-        }
-    }
-    
-    /**
-     * Dump the inter arrival times for examination if necessary.
-     */
-    public void dumpInterArrivalTimes()
-    {
-        try
-        {
-            FileOutputStream fos = new FileOutputStream("/var/tmp/output-" + System.currentTimeMillis() + ".dat", true);
-            fos.write(toString().getBytes());
-            fos.close();
-        }
-        catch(Throwable th)
-        {
-            logger_.warn(LogUtil.throwableToString(th));
-        }
-    }
-    
-    /**
-     * We dump the arrival window for any endpoint only if the 
-     * local Failure Detector module has been up for more than a 
-     * minute.
-     * 
-     * @param ep for which the arrival window needs to be dumped.
-     */
-    private void dumpInterArrivalTimes(EndPoint ep)
-    {
-        long now = System.currentTimeMillis();
-        if ( (now - FailureDetector.creationTime_) <= FailureDetector.uptimeThreshold_ )
-            return;
-        try
-        {
-            FileOutputStream fos = new FileOutputStream("/var/tmp/output-" + System.currentTimeMillis() + "-" + ep + ".dat", true);
-            ArrivalWindow hWnd = arrivalSamples_.get(ep);
-            fos.write(hWnd.toString().getBytes());
-            fos.close();
-        }
-        catch(Throwable th)
-        {
-            logger_.warn(LogUtil.throwableToString(th));
-        }
-    }
-    
-    public boolean isAlive(EndPoint ep)
-    {
-        try
-        {
-            /* If the endpoint in question is the local endpoint return true. */
-            String localHost = FBUtilities.getHostAddress();
-            if ( localHost.equals( ep.getHost() ) )
-                    return true;
-        }
-        catch( UnknownHostException ex )
-        {
-            logger_.info( LogUtil.throwableToString(ex) );
-        }
-    	/* Incoming port is assumed to be the Storage port. We need to change it to the control port */
-    	EndPoint ep2 = new EndPoint(ep.getHost(), DatabaseDescriptor.getControlPort());        
-        EndPointState epState = Gossiper.instance().getEndPointStateForEndPoint(ep2);
-        return epState.isAlive();
-    }
-    
-    public void report(EndPoint ep)
-    {
-        if (logger_.isTraceEnabled())
-            logger_.trace("reporting " + ep);
-        long now = System.currentTimeMillis();
-        ArrivalWindow heartbeatWindow = arrivalSamples_.get(ep);
-        if ( heartbeatWindow == null )
-        {
-            heartbeatWindow = new ArrivalWindow(sampleSize_);
-            arrivalSamples_.put(ep, heartbeatWindow);
-        }
-        heartbeatWindow.add(now);
-    }
-    
-    public void interpret(EndPoint ep)
-    {
-        ArrivalWindow hbWnd = arrivalSamples_.get(ep);
-        if ( hbWnd == null )
-        {            
-            return;
-        }
-        long now = System.currentTimeMillis();
-        /* We need this so that we do not suspect a convict. */
-        boolean isConvicted = false;
-        double phi = hbWnd.phi(now);
-        if (logger_.isTraceEnabled())
-            logger_.trace("PHI for " + ep + " : " + phi);
-        
-        /*
-        if ( phi > phiConvictThreshold_ )
-        {            
-            isConvicted = true;     
-            for ( IFailureDetectionEventListener listener : fdEvntListeners_ )
-            {
-                listener.convict(ep);                
-            }
-        }
-        */
-        if ( !isConvicted && phi > phiSuspectThreshold_ )
-        {     
-            for ( IFailureDetectionEventListener listener : fdEvntListeners_ )
-            {
-                listener.suspect(ep);
-            }
-        }        
-    }
-    
-    public void registerFailureDetectionEventListener(IFailureDetectionEventListener listener)
-    {
-        fdEvntListeners_.add(listener);
-    }
-    
-    public void unregisterFailureDetectionEventListener(IFailureDetectionEventListener listener)
-    {
-        fdEvntListeners_.remove(listener);
-    }
-    
-    public String toString()
-    {
-        StringBuilder sb = new StringBuilder();
-        Set<EndPoint> eps = arrivalSamples_.keySet();
-        
-        sb.append("-----------------------------------------------------------------------");
-        for ( EndPoint ep : eps )
-        {
-            ArrivalWindow hWnd = arrivalSamples_.get(ep);
-            sb.append(ep + " : ");
-            sb.append(hWnd.toString());
-            sb.append( System.getProperty("line.separator") );
-        }
-        sb.append("-----------------------------------------------------------------------");
-        return sb.toString();
-    }
-    
-    public static void main(String[] args) throws Throwable
-    {           
-    }
-}
-
-class ArrivalWindow
-{
-    private static Logger logger_ = Logger.getLogger(ArrivalWindow.class);
-    private double tLast_ = 0L;
-    private BoundedStatsDeque arrivalIntervals_;
-
-    ArrivalWindow(int size)
-    {
-        arrivalIntervals_ = new BoundedStatsDeque(size);
-    }
-    
-    synchronized void add(double value)
-    {
-        double interArrivalTime;
-        if ( tLast_ > 0L )
-        {                        
-            interArrivalTime = (value - tLast_);            
-        }
-        else
-        {
-            interArrivalTime = Gossiper.intervalInMillis_ / 2;
-        }
-        tLast_ = value;            
-        arrivalIntervals_.add(interArrivalTime);        
-    }
-    
-    synchronized double sum()
-    {
-        return arrivalIntervals_.sum();
-    }
-    
-    synchronized double sumOfDeviations()
-    {
-        return arrivalIntervals_.sumOfDeviations();
-    }
-    
-    synchronized double mean()
-    {
-        return arrivalIntervals_.mean();
-    }
-    
-    synchronized double variance()
-    {
-        return arrivalIntervals_.variance();
-    }
-    
-    double stdev()
-    {
-        return arrivalIntervals_.stdev();
-    }
-    
-    void clear()
-    {
-        arrivalIntervals_.clear();
-    }
-    
-    double p(double t)
-    {
-        double mean = mean();
-        double exponent = (-1)*(t)/mean;
-        return 1 - ( 1 - Math.pow(Math.E, exponent) );
-    }
-    
-    double phi(long tnow)
-    {            
-        int size = arrivalIntervals_.size();
-        double log = 0d;
-        if ( size > 0 )
-        {
-            double t = tnow - tLast_;                
-            double probability = p(t);       
-            log = (-1) * Math.log10( probability );                                 
-        }
-        return log;           
-    } 
-    
-    public String toString()
-    {
-        return StringUtils.join(arrivalIntervals_.iterator(), " ");
-    }
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.FileOutputStream;
+import java.lang.management.ManagementFactory;
+import java.net.UnknownHostException;
+import java.util.*;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+
+import org.apache.commons.lang.StringUtils;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.cassandra.utils.BoundedStatsDeque;
+import org.apache.log4j.Logger;
+
+/**
+ * This FailureDetector is an implementation of the paper titled
+ * "The Phi Accrual Failure Detector" by Hayashibara. 
+ * Check the paper and the <i>IFailureDetector</i> interface for details.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class FailureDetector implements IFailureDetector, FailureDetectorMBean
+{
+    private static Logger logger_ = Logger.getLogger(FailureDetector.class);
+    private static final int sampleSize_ = 1000;
+    private static final int phiSuspectThreshold_ = 5;
+    private static final int phiConvictThreshold_ = 8;
+    /* The Failure Detector has to have been up for at least 1 min. */
+    private static final long uptimeThreshold_ = 60000;
+    private static IFailureDetector failureDetector_;
+    /* Used to lock the factory for creation of FailureDetector instance */
+    private static Lock createLock_ = new ReentrantLock();
+    /* The time when the module was instantiated. */
+    private static long creationTime_;
+    
+    public static IFailureDetector instance()
+    {        
+        if ( failureDetector_ == null )
+        {
+            FailureDetector.createLock_.lock();
+            try
+            {
+                if ( failureDetector_ == null )
+                {
+                    failureDetector_ = new FailureDetector();
+                }
+            }
+            finally
+            {
+                createLock_.unlock();
+            }
+        }        
+        return failureDetector_;
+    }
+    
+    private Map<EndPoint, ArrivalWindow> arrivalSamples_ = new Hashtable<EndPoint, ArrivalWindow>();
+    private List<IFailureDetectionEventListener> fdEvntListeners_ = new ArrayList<IFailureDetectionEventListener>();
+    
+    public FailureDetector()
+    {
+        creationTime_ = System.currentTimeMillis();
+        // Register this instance with JMX
+        try
+        {
+            MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
+            mbs.registerMBean(this, new ObjectName("org.apache.cassandra.gms:type=FailureDetector"));
+        }
+        catch (Exception e)
+        {
+            logger_.error(LogUtil.throwableToString(e));
+        }
+    }
+    
+    /**
+     * Dump the inter arrival times for examination if necessary.
+     */
+    public void dumpInterArrivalTimes()
+    {
+        try
+        {
+            FileOutputStream fos = new FileOutputStream("/var/tmp/output-" + System.currentTimeMillis() + ".dat", true);
+            fos.write(toString().getBytes());
+            fos.close();
+        }
+        catch(Throwable th)
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }
+    }
+    
+    /**
+     * We dump the arrival window for any endpoint only if the 
+     * local Failure Detector module has been up for more than a 
+     * minute.
+     * 
+     * @param ep for which the arrival window needs to be dumped.
+     */
+    private void dumpInterArrivalTimes(EndPoint ep)
+    {
+        long now = System.currentTimeMillis();
+        if ( (now - FailureDetector.creationTime_) <= FailureDetector.uptimeThreshold_ )
+            return;
+        try
+        {
+            FileOutputStream fos = new FileOutputStream("/var/tmp/output-" + System.currentTimeMillis() + "-" + ep + ".dat", true);
+            ArrivalWindow hWnd = arrivalSamples_.get(ep);
+            fos.write(hWnd.toString().getBytes());
+            fos.close();
+        }
+        catch(Throwable th)
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }
+    }
+    
+    public boolean isAlive(EndPoint ep)
+    {
+        try
+        {
+            /* If the endpoint in question is the local endpoint return true. */
+            String localHost = FBUtilities.getHostAddress();
+            if ( localHost.equals( ep.getHost() ) )
+                    return true;
+        }
+        catch( UnknownHostException ex )
+        {
+            logger_.info( LogUtil.throwableToString(ex) );
+        }
+    	/* Incoming port is assumed to be the Storage port. We need to change it to the control port */
+    	EndPoint ep2 = new EndPoint(ep.getHost(), DatabaseDescriptor.getControlPort());        
+        EndPointState epState = Gossiper.instance().getEndPointStateForEndPoint(ep2);
+        return epState.isAlive();
+    }
+    
+    public void report(EndPoint ep)
+    {
+        if (logger_.isTraceEnabled())
+            logger_.trace("reporting " + ep);
+        long now = System.currentTimeMillis();
+        ArrivalWindow heartbeatWindow = arrivalSamples_.get(ep);
+        if ( heartbeatWindow == null )
+        {
+            heartbeatWindow = new ArrivalWindow(sampleSize_);
+            arrivalSamples_.put(ep, heartbeatWindow);
+        }
+        heartbeatWindow.add(now);
+    }
+    
+    public void interpret(EndPoint ep)
+    {
+        ArrivalWindow hbWnd = arrivalSamples_.get(ep);
+        if ( hbWnd == null )
+        {            
+            return;
+        }
+        long now = System.currentTimeMillis();
+        /* We need this so that we do not suspect a convict. */
+        boolean isConvicted = false;
+        double phi = hbWnd.phi(now);
+        if (logger_.isTraceEnabled())
+            logger_.trace("PHI for " + ep + " : " + phi);
+        
+        /*
+        if ( phi > phiConvictThreshold_ )
+        {            
+            isConvicted = true;     
+            for ( IFailureDetectionEventListener listener : fdEvntListeners_ )
+            {
+                listener.convict(ep);                
+            }
+        }
+        */
+        if ( !isConvicted && phi > phiSuspectThreshold_ )
+        {     
+            for ( IFailureDetectionEventListener listener : fdEvntListeners_ )
+            {
+                listener.suspect(ep);
+            }
+        }        
+    }
+    
+    public void registerFailureDetectionEventListener(IFailureDetectionEventListener listener)
+    {
+        fdEvntListeners_.add(listener);
+    }
+    
+    public void unregisterFailureDetectionEventListener(IFailureDetectionEventListener listener)
+    {
+        fdEvntListeners_.remove(listener);
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder();
+        Set<EndPoint> eps = arrivalSamples_.keySet();
+        
+        sb.append("-----------------------------------------------------------------------");
+        for ( EndPoint ep : eps )
+        {
+            ArrivalWindow hWnd = arrivalSamples_.get(ep);
+            sb.append(ep + " : ");
+            sb.append(hWnd.toString());
+            sb.append( System.getProperty("line.separator") );
+        }
+        sb.append("-----------------------------------------------------------------------");
+        return sb.toString();
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {           
+    }
+}
+
+class ArrivalWindow
+{
+    private static Logger logger_ = Logger.getLogger(ArrivalWindow.class);
+    private double tLast_ = 0L;
+    private BoundedStatsDeque arrivalIntervals_;
+
+    ArrivalWindow(int size)
+    {
+        arrivalIntervals_ = new BoundedStatsDeque(size);
+    }
+    
+    synchronized void add(double value)
+    {
+        double interArrivalTime;
+        if ( tLast_ > 0L )
+        {                        
+            interArrivalTime = (value - tLast_);            
+        }
+        else
+        {
+            interArrivalTime = Gossiper.intervalInMillis_ / 2;
+        }
+        tLast_ = value;            
+        arrivalIntervals_.add(interArrivalTime);        
+    }
+    
+    synchronized double sum()
+    {
+        return arrivalIntervals_.sum();
+    }
+    
+    synchronized double sumOfDeviations()
+    {
+        return arrivalIntervals_.sumOfDeviations();
+    }
+    
+    synchronized double mean()
+    {
+        return arrivalIntervals_.mean();
+    }
+    
+    synchronized double variance()
+    {
+        return arrivalIntervals_.variance();
+    }
+    
+    double stdev()
+    {
+        return arrivalIntervals_.stdev();
+    }
+    
+    void clear()
+    {
+        arrivalIntervals_.clear();
+    }
+    
+    double p(double t)
+    {
+        double mean = mean();
+        double exponent = (-1)*(t)/mean;
+        return 1 - ( 1 - Math.pow(Math.E, exponent) );
+    }
+    
+    double phi(long tnow)
+    {            
+        int size = arrivalIntervals_.size();
+        double log = 0d;
+        if ( size > 0 )
+        {
+            double t = tnow - tLast_;                
+            double probability = p(t);       
+            log = (-1) * Math.log10( probability );                                 
+        }
+        return log;           
+    } 
+    
+    public String toString()
+    {
+        return StringUtils.join(arrivalIntervals_.iterator(), " ");
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/gms/FailureDetectorMBean.java b/src/java/org/apache/cassandra/gms/FailureDetectorMBean.java
index 0deb3134a4..3c9f7e5a6a 100644
--- a/src/java/org/apache/cassandra/gms/FailureDetectorMBean.java
+++ b/src/java/org/apache/cassandra/gms/FailureDetectorMBean.java
@@ -1,24 +1,24 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-public interface FailureDetectorMBean
-{
-    public void dumpInterArrivalTimes();
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+public interface FailureDetectorMBean
+{
+    public void dumpInterArrivalTimes();
+}
diff --git a/src/java/org/apache/cassandra/gms/GossipDigest.java b/src/java/org/apache/cassandra/gms/GossipDigest.java
index d2c5c490f8..0f62a45241 100644
--- a/src/java/org/apache/cassandra/gms/GossipDigest.java
+++ b/src/java/org/apache/cassandra/gms/GossipDigest.java
@@ -1,110 +1,110 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.CompactEndPointSerializationHelper;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.*;
-
-/**
- * Contains information about a specified list of EndPoints and the largest version 
- * of the state they have generated as known by the local endpoint.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class GossipDigest implements Comparable<GossipDigest>
-{
-    private static ICompactSerializer<GossipDigest> serializer_;
-    static
-    {
-        serializer_ = new GossipDigestSerializer();
-    }
-    
-    EndPoint endPoint_;
-    int generation_;
-    int maxVersion_;
-
-    public static ICompactSerializer<GossipDigest> serializer()
-    {
-        return serializer_;
-    }
-    
-    GossipDigest(EndPoint endPoint, int generation, int maxVersion)
-    {
-        endPoint_ = endPoint;
-        generation_ = generation; 
-        maxVersion_ = maxVersion;
-    }
-    
-    EndPoint getEndPoint()
-    {
-        return endPoint_;
-    }
-    
-    int getGeneration()
-    {
-        return generation_;
-    }
-    
-    int getMaxVersion()
-    {
-        return maxVersion_;
-    }
-    
-    public int compareTo(GossipDigest gDigest)
-    {
-        if ( generation_ != gDigest.generation_ )
-            return ( generation_ - gDigest.generation_ );
-        return (maxVersion_ - gDigest.maxVersion_);
-    }
-    
-    public String toString()
-    {
-        StringBuilder sb = new StringBuilder();
-        sb.append(endPoint_);
-        sb.append(":");
-        sb.append(generation_);
-        sb.append(":");
-        sb.append(maxVersion_);
-        return sb.toString();
-    }
-}
-
-class GossipDigestSerializer implements ICompactSerializer<GossipDigest>
-{       
-    public void serialize(GossipDigest gDigest, DataOutputStream dos) throws IOException
-    {        
-        CompactEndPointSerializationHelper.serialize(gDigest.endPoint_, dos);
-        dos.writeInt(gDigest.generation_);
-        dos.writeInt(gDigest.maxVersion_);
-    }
-
-    public GossipDigest deserialize(DataInputStream dis) throws IOException
-    {
-        EndPoint endPoint = CompactEndPointSerializationHelper.deserialize(dis);
-        int generation = dis.readInt();
-        int version = dis.readInt();
-        return new GossipDigest(endPoint, generation, version);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.CompactEndPointSerializationHelper;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.*;
+
+/**
+ * Contains information about a specified list of EndPoints and the largest version 
+ * of the state they have generated as known by the local endpoint.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class GossipDigest implements Comparable<GossipDigest>
+{
+    private static ICompactSerializer<GossipDigest> serializer_;
+    static
+    {
+        serializer_ = new GossipDigestSerializer();
+    }
+    
+    EndPoint endPoint_;
+    int generation_;
+    int maxVersion_;
+
+    public static ICompactSerializer<GossipDigest> serializer()
+    {
+        return serializer_;
+    }
+    
+    GossipDigest(EndPoint endPoint, int generation, int maxVersion)
+    {
+        endPoint_ = endPoint;
+        generation_ = generation; 
+        maxVersion_ = maxVersion;
+    }
+    
+    EndPoint getEndPoint()
+    {
+        return endPoint_;
+    }
+    
+    int getGeneration()
+    {
+        return generation_;
+    }
+    
+    int getMaxVersion()
+    {
+        return maxVersion_;
+    }
+    
+    public int compareTo(GossipDigest gDigest)
+    {
+        if ( generation_ != gDigest.generation_ )
+            return ( generation_ - gDigest.generation_ );
+        return (maxVersion_ - gDigest.maxVersion_);
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder();
+        sb.append(endPoint_);
+        sb.append(":");
+        sb.append(generation_);
+        sb.append(":");
+        sb.append(maxVersion_);
+        return sb.toString();
+    }
+}
+
+class GossipDigestSerializer implements ICompactSerializer<GossipDigest>
+{       
+    public void serialize(GossipDigest gDigest, DataOutputStream dos) throws IOException
+    {        
+        CompactEndPointSerializationHelper.serialize(gDigest.endPoint_, dos);
+        dos.writeInt(gDigest.generation_);
+        dos.writeInt(gDigest.maxVersion_);
+    }
+
+    public GossipDigest deserialize(DataInputStream dis) throws IOException
+    {
+        EndPoint endPoint = CompactEndPointSerializationHelper.deserialize(dis);
+        int generation = dis.readInt();
+        int version = dis.readInt();
+        return new GossipDigest(endPoint, generation, version);
+    }
+}
diff --git a/src/java/org/apache/cassandra/gms/GossipDigestAck2Message.java b/src/java/org/apache/cassandra/gms/GossipDigestAck2Message.java
index 53d0e87f71..4ba023eed0 100644
--- a/src/java/org/apache/cassandra/gms/GossipDigestAck2Message.java
+++ b/src/java/org/apache/cassandra/gms/GossipDigestAck2Message.java
@@ -1,77 +1,77 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.*;
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.*;
-
-
-/**
- * This message gets sent out as a result of the receipt of a GossipDigestAckMessage. This the 
- * last stage of the 3 way messaging of the Gossip protocol.
- *  
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class GossipDigestAck2Message
-{
-    private static  ICompactSerializer<GossipDigestAck2Message> serializer_;
-    static
-    {
-        serializer_ = new GossipDigestAck2MessageSerializer();
-    }
-    
-    Map<EndPoint, EndPointState> epStateMap_ = new HashMap<EndPoint, EndPointState>();
-
-    public static ICompactSerializer<GossipDigestAck2Message> serializer()
-    {
-        return serializer_;
-    }
-    
-    GossipDigestAck2Message(Map<EndPoint, EndPointState> epStateMap)
-    {
-        epStateMap_ = epStateMap;
-    }
-        
-    Map<EndPoint, EndPointState> getEndPointStateMap()
-    {
-         return epStateMap_;
-    }
-}
-
-class GossipDigestAck2MessageSerializer implements ICompactSerializer<GossipDigestAck2Message>
-{
-    public void serialize(GossipDigestAck2Message gDigestAck2Message, DataOutputStream dos) throws IOException
-    {
-        /* Use the EndPointState */
-        EndPointStatesSerializationHelper.serialize(gDigestAck2Message.epStateMap_, dos);
-    }
-
-    public GossipDigestAck2Message deserialize(DataInputStream dis) throws IOException
-    {
-        Map<EndPoint, EndPointState> epStateMap = EndPointStatesSerializationHelper.deserialize(dis);
-        return new GossipDigestAck2Message(epStateMap);        
-    }
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.*;
+
+
+/**
+ * This message gets sent out as a result of the receipt of a GossipDigestAckMessage. This the 
+ * last stage of the 3 way messaging of the Gossip protocol.
+ *  
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class GossipDigestAck2Message
+{
+    private static  ICompactSerializer<GossipDigestAck2Message> serializer_;
+    static
+    {
+        serializer_ = new GossipDigestAck2MessageSerializer();
+    }
+    
+    Map<EndPoint, EndPointState> epStateMap_ = new HashMap<EndPoint, EndPointState>();
+
+    public static ICompactSerializer<GossipDigestAck2Message> serializer()
+    {
+        return serializer_;
+    }
+    
+    GossipDigestAck2Message(Map<EndPoint, EndPointState> epStateMap)
+    {
+        epStateMap_ = epStateMap;
+    }
+        
+    Map<EndPoint, EndPointState> getEndPointStateMap()
+    {
+         return epStateMap_;
+    }
+}
+
+class GossipDigestAck2MessageSerializer implements ICompactSerializer<GossipDigestAck2Message>
+{
+    public void serialize(GossipDigestAck2Message gDigestAck2Message, DataOutputStream dos) throws IOException
+    {
+        /* Use the EndPointState */
+        EndPointStatesSerializationHelper.serialize(gDigestAck2Message.epStateMap_, dos);
+    }
+
+    public GossipDigestAck2Message deserialize(DataInputStream dis) throws IOException
+    {
+        Map<EndPoint, EndPointState> epStateMap = EndPointStatesSerializationHelper.deserialize(dis);
+        return new GossipDigestAck2Message(epStateMap);        
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/gms/GossipDigestAckMessage.java b/src/java/org/apache/cassandra/gms/GossipDigestAckMessage.java
index 57e54f89cc..ab38db0aa4 100644
--- a/src/java/org/apache/cassandra/gms/GossipDigestAckMessage.java
+++ b/src/java/org/apache/cassandra/gms/GossipDigestAckMessage.java
@@ -1,102 +1,102 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.*;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.EndPoint;
-
-
-
-/**
- * This message gets sent out as a result of the receipt of a GossipDigestSynMessage by an
- * endpoint. This is the 2 stage of the 3 way messaging in the Gossip protocol.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class GossipDigestAckMessage
-{
-    private static ICompactSerializer<GossipDigestAckMessage> serializer_;
-    static
-    {
-        serializer_ = new GossipDigestAckMessageSerializer();
-    }
-    
-    List<GossipDigest> gDigestList_ = new ArrayList<GossipDigest>();
-    Map<EndPoint, EndPointState> epStateMap_ = new HashMap<EndPoint, EndPointState>();
-    
-    static ICompactSerializer<GossipDigestAckMessage> serializer()
-    {
-        return serializer_;
-    }
-    
-    GossipDigestAckMessage(List<GossipDigest> gDigestList, Map<EndPoint, EndPointState> epStateMap)
-    {
-        gDigestList_ = gDigestList;
-        epStateMap_ = epStateMap;
-    }
-    
-    void addGossipDigest(EndPoint ep, int generation, int version)
-    {
-        gDigestList_.add( new GossipDigest(ep, generation, version) );
-    }
-    
-    List<GossipDigest> getGossipDigestList()
-    {
-        return gDigestList_;
-    }
-    
-    Map<EndPoint, EndPointState> getEndPointStateMap()
-    {
-        return epStateMap_;
-    }
-}
-
-class GossipDigestAckMessageSerializer implements ICompactSerializer<GossipDigestAckMessage>
-{
-    public void serialize(GossipDigestAckMessage gDigestAckMessage, DataOutputStream dos) throws IOException
-    {
-        /* Use the helper to serialize the GossipDigestList */
-        boolean bContinue = GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList_, dos);
-        dos.writeBoolean(bContinue);
-        /* Use the EndPointState */
-        if ( bContinue )
-        {
-            EndPointStatesSerializationHelper.serialize(gDigestAckMessage.epStateMap_, dos);            
-        }
-    }
-
-    public GossipDigestAckMessage deserialize(DataInputStream dis) throws IOException
-    {
-        Map<EndPoint, EndPointState> epStateMap = new HashMap<EndPoint, EndPointState>();
-        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(dis);                
-        boolean bContinue = dis.readBoolean();
-
-        if ( bContinue )
-        {
-            epStateMap = EndPointStatesSerializationHelper.deserialize(dis);                                    
-        }
-        return new GossipDigestAckMessage(gDigestList, epStateMap);
-    }
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+
+
+
+/**
+ * This message gets sent out as a result of the receipt of a GossipDigestSynMessage by an
+ * endpoint. This is the 2 stage of the 3 way messaging in the Gossip protocol.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class GossipDigestAckMessage
+{
+    private static ICompactSerializer<GossipDigestAckMessage> serializer_;
+    static
+    {
+        serializer_ = new GossipDigestAckMessageSerializer();
+    }
+    
+    List<GossipDigest> gDigestList_ = new ArrayList<GossipDigest>();
+    Map<EndPoint, EndPointState> epStateMap_ = new HashMap<EndPoint, EndPointState>();
+    
+    static ICompactSerializer<GossipDigestAckMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    GossipDigestAckMessage(List<GossipDigest> gDigestList, Map<EndPoint, EndPointState> epStateMap)
+    {
+        gDigestList_ = gDigestList;
+        epStateMap_ = epStateMap;
+    }
+    
+    void addGossipDigest(EndPoint ep, int generation, int version)
+    {
+        gDigestList_.add( new GossipDigest(ep, generation, version) );
+    }
+    
+    List<GossipDigest> getGossipDigestList()
+    {
+        return gDigestList_;
+    }
+    
+    Map<EndPoint, EndPointState> getEndPointStateMap()
+    {
+        return epStateMap_;
+    }
+}
+
+class GossipDigestAckMessageSerializer implements ICompactSerializer<GossipDigestAckMessage>
+{
+    public void serialize(GossipDigestAckMessage gDigestAckMessage, DataOutputStream dos) throws IOException
+    {
+        /* Use the helper to serialize the GossipDigestList */
+        boolean bContinue = GossipDigestSerializationHelper.serialize(gDigestAckMessage.gDigestList_, dos);
+        dos.writeBoolean(bContinue);
+        /* Use the EndPointState */
+        if ( bContinue )
+        {
+            EndPointStatesSerializationHelper.serialize(gDigestAckMessage.epStateMap_, dos);            
+        }
+    }
+
+    public GossipDigestAckMessage deserialize(DataInputStream dis) throws IOException
+    {
+        Map<EndPoint, EndPointState> epStateMap = new HashMap<EndPoint, EndPointState>();
+        List<GossipDigest> gDigestList = GossipDigestSerializationHelper.deserialize(dis);                
+        boolean bContinue = dis.readBoolean();
+
+        if ( bContinue )
+        {
+            epStateMap = EndPointStatesSerializationHelper.deserialize(dis);                                    
+        }
+        return new GossipDigestAckMessage(gDigestList, epStateMap);
+    }
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/gms/GossipDigestSynMessage.java b/src/java/org/apache/cassandra/gms/GossipDigestSynMessage.java
index f52c4f114e..96ddb3023d 100644
--- a/src/java/org/apache/cassandra/gms/GossipDigestSynMessage.java
+++ b/src/java/org/apache/cassandra/gms/GossipDigestSynMessage.java
@@ -1,184 +1,184 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.*;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.CompactEndPointSerializationHelper;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.utils.Log4jLogger;
-import org.apache.log4j.Logger;
-import org.apache.cassandra.utils.*;
-
-
-/**
- * This is the first message that gets sent out as a start of the Gossip protocol in a 
- * round. 
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class GossipDigestSynMessage
-{
-    private static ICompactSerializer<GossipDigestSynMessage> serializer_;
-    static
-    {
-        serializer_ = new GossipDigestSynMessageSerializer();
-    }
-    
-    String clusterId_;
-    List<GossipDigest> gDigests_ = new ArrayList<GossipDigest>();
-
-    public static ICompactSerializer<GossipDigestSynMessage> serializer()
-    {
-        return serializer_;
-    }
- 
-    public GossipDigestSynMessage(String clusterId, List<GossipDigest> gDigests)
-    {      
-        clusterId_ = clusterId;
-        gDigests_ = gDigests;
-    }
-    
-    List<GossipDigest> getGossipDigests()
-    {
-        return gDigests_;
-    }
-}
-
-class GossipDigestSerializationHelper
-{
-    private static Logger logger_ = Logger.getLogger(GossipDigestSerializationHelper.class);
-    
-    static boolean serialize(List<GossipDigest> gDigestList, DataOutputStream dos) throws IOException
-    {
-        boolean bVal = true;
-        int size = gDigestList.size();                        
-        dos.writeInt(size);
-        
-        int estimate = 0;            
-        for ( GossipDigest gDigest : gDigestList )
-        {
-            if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
-            {
-                logger_.info("@@@@ Breaking out to respect the MTU size in GD @@@@");
-                bVal = false;
-                break;
-            }
-            int pre = dos.size();               
-            GossipDigest.serializer().serialize( gDigest, dos );
-            int post = dos.size();
-            estimate = post - pre;
-        }
-        return bVal;
-    }
-
-    static List<GossipDigest> deserialize(DataInputStream dis) throws IOException
-    {
-        int size = dis.readInt();            
-        List<GossipDigest> gDigests = new ArrayList<GossipDigest>();
-        
-        for ( int i = 0; i < size; ++i )
-        {
-            if ( dis.available() == 0 )
-            {
-                logger_.info("Remaining bytes zero. Stopping deserialization of GossipDigests.");
-                break;
-            }
-                            
-            GossipDigest gDigest = GossipDigest.serializer().deserialize(dis);                
-            gDigests.add( gDigest );                
-        }        
-        return gDigests;
-    }
-}
-
-class EndPointStatesSerializationHelper
-{
-    private static Log4jLogger logger_ = new Log4jLogger(EndPointStatesSerializationHelper.class.getName());
-    
-    static boolean serialize(Map<EndPoint, EndPointState> epStateMap, DataOutputStream dos) throws IOException
-    {
-        boolean bVal = true;
-        int estimate = 0;                
-        int size = epStateMap.size();
-        dos.writeInt(size);
-    
-        Set<EndPoint> eps = epStateMap.keySet();
-        for( EndPoint ep : eps )
-        {
-            if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
-            {
-                logger_.info("@@@@ Breaking out to respect the MTU size in EPS. Estimate is " + estimate + " @@@@");
-                bVal = false;
-                break;
-            }
-    
-            int pre = dos.size();
-            CompactEndPointSerializationHelper.serialize(ep, dos);
-            EndPointState epState = epStateMap.get(ep);            
-            EndPointState.serializer().serialize(epState, dos);
-            int post = dos.size();
-            estimate = post - pre;
-        }
-        return bVal;
-    }
-
-    static Map<EndPoint, EndPointState> deserialize(DataInputStream dis) throws IOException
-    {
-        int size = dis.readInt();            
-        Map<EndPoint, EndPointState> epStateMap = new HashMap<EndPoint, EndPointState>();
-        
-        for ( int i = 0; i < size; ++i )
-        {
-            if ( dis.available() == 0 )
-            {
-                logger_.info("Remaining bytes zero. Stopping deserialization in EndPointState.");
-                break;
-            }
-            // int length = dis.readInt();            
-            EndPoint ep = CompactEndPointSerializationHelper.deserialize(dis);
-            EndPointState epState = EndPointState.serializer().deserialize(dis);            
-            epStateMap.put(ep, epState);
-        }        
-        return epStateMap;
-    }
-}
-
-class GossipDigestSynMessageSerializer implements ICompactSerializer<GossipDigestSynMessage>
-{   
-    public void serialize(GossipDigestSynMessage gDigestSynMessage, DataOutputStream dos) throws IOException
-    {    
-        dos.writeUTF(gDigestSynMessage.clusterId_);
-        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests_, dos);
-    }
-
-    public GossipDigestSynMessage deserialize(DataInputStream dis) throws IOException
-    {
-        String clusterId = dis.readUTF();
-        List<GossipDigest> gDigests = GossipDigestSerializationHelper.deserialize(dis);
-        return new GossipDigestSynMessage(clusterId, gDigests);
-    }
-
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.CompactEndPointSerializationHelper;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.utils.Log4jLogger;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.utils.*;
+
+
+/**
+ * This is the first message that gets sent out as a start of the Gossip protocol in a 
+ * round. 
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class GossipDigestSynMessage
+{
+    private static ICompactSerializer<GossipDigestSynMessage> serializer_;
+    static
+    {
+        serializer_ = new GossipDigestSynMessageSerializer();
+    }
+    
+    String clusterId_;
+    List<GossipDigest> gDigests_ = new ArrayList<GossipDigest>();
+
+    public static ICompactSerializer<GossipDigestSynMessage> serializer()
+    {
+        return serializer_;
+    }
+ 
+    public GossipDigestSynMessage(String clusterId, List<GossipDigest> gDigests)
+    {      
+        clusterId_ = clusterId;
+        gDigests_ = gDigests;
+    }
+    
+    List<GossipDigest> getGossipDigests()
+    {
+        return gDigests_;
+    }
+}
+
+class GossipDigestSerializationHelper
+{
+    private static Logger logger_ = Logger.getLogger(GossipDigestSerializationHelper.class);
+    
+    static boolean serialize(List<GossipDigest> gDigestList, DataOutputStream dos) throws IOException
+    {
+        boolean bVal = true;
+        int size = gDigestList.size();                        
+        dos.writeInt(size);
+        
+        int estimate = 0;            
+        for ( GossipDigest gDigest : gDigestList )
+        {
+            if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
+            {
+                logger_.info("@@@@ Breaking out to respect the MTU size in GD @@@@");
+                bVal = false;
+                break;
+            }
+            int pre = dos.size();               
+            GossipDigest.serializer().serialize( gDigest, dos );
+            int post = dos.size();
+            estimate = post - pre;
+        }
+        return bVal;
+    }
+
+    static List<GossipDigest> deserialize(DataInputStream dis) throws IOException
+    {
+        int size = dis.readInt();            
+        List<GossipDigest> gDigests = new ArrayList<GossipDigest>();
+        
+        for ( int i = 0; i < size; ++i )
+        {
+            if ( dis.available() == 0 )
+            {
+                logger_.info("Remaining bytes zero. Stopping deserialization of GossipDigests.");
+                break;
+            }
+                            
+            GossipDigest gDigest = GossipDigest.serializer().deserialize(dis);                
+            gDigests.add( gDigest );                
+        }        
+        return gDigests;
+    }
+}
+
+class EndPointStatesSerializationHelper
+{
+    private static Log4jLogger logger_ = new Log4jLogger(EndPointStatesSerializationHelper.class.getName());
+    
+    static boolean serialize(Map<EndPoint, EndPointState> epStateMap, DataOutputStream dos) throws IOException
+    {
+        boolean bVal = true;
+        int estimate = 0;                
+        int size = epStateMap.size();
+        dos.writeInt(size);
+    
+        Set<EndPoint> eps = epStateMap.keySet();
+        for( EndPoint ep : eps )
+        {
+            if ( Gossiper.MAX_GOSSIP_PACKET_SIZE - dos.size() < estimate )
+            {
+                logger_.info("@@@@ Breaking out to respect the MTU size in EPS. Estimate is " + estimate + " @@@@");
+                bVal = false;
+                break;
+            }
+    
+            int pre = dos.size();
+            CompactEndPointSerializationHelper.serialize(ep, dos);
+            EndPointState epState = epStateMap.get(ep);            
+            EndPointState.serializer().serialize(epState, dos);
+            int post = dos.size();
+            estimate = post - pre;
+        }
+        return bVal;
+    }
+
+    static Map<EndPoint, EndPointState> deserialize(DataInputStream dis) throws IOException
+    {
+        int size = dis.readInt();            
+        Map<EndPoint, EndPointState> epStateMap = new HashMap<EndPoint, EndPointState>();
+        
+        for ( int i = 0; i < size; ++i )
+        {
+            if ( dis.available() == 0 )
+            {
+                logger_.info("Remaining bytes zero. Stopping deserialization in EndPointState.");
+                break;
+            }
+            // int length = dis.readInt();            
+            EndPoint ep = CompactEndPointSerializationHelper.deserialize(dis);
+            EndPointState epState = EndPointState.serializer().deserialize(dis);            
+            epStateMap.put(ep, epState);
+        }        
+        return epStateMap;
+    }
+}
+
+class GossipDigestSynMessageSerializer implements ICompactSerializer<GossipDigestSynMessage>
+{   
+    public void serialize(GossipDigestSynMessage gDigestSynMessage, DataOutputStream dos) throws IOException
+    {    
+        dos.writeUTF(gDigestSynMessage.clusterId_);
+        GossipDigestSerializationHelper.serialize(gDigestSynMessage.gDigests_, dos);
+    }
+
+    public GossipDigestSynMessage deserialize(DataInputStream dis) throws IOException
+    {
+        String clusterId = dis.readUTF();
+        List<GossipDigest> gDigests = GossipDigestSerializationHelper.deserialize(dis);
+        return new GossipDigestSynMessage(clusterId, gDigests);
+    }
+
+}
+
diff --git a/src/java/org/apache/cassandra/gms/Gossiper.java b/src/java/org/apache/cassandra/gms/Gossiper.java
index b3127d825f..a3ff064f8a 100644
--- a/src/java/org/apache/cassandra/gms/Gossiper.java
+++ b/src/java/org/apache/cassandra/gms/Gossiper.java
@@ -1,1121 +1,1121 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.io.*;
-import java.util.*;
-import java.net.InetAddress;
-
-import org.apache.cassandra.concurrent.SingleThreadedStage;
-import org.apache.cassandra.concurrent.StageManager;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * This module is responsible for Gossiping information for the local endpoint. This abstraction
- * maintains the list of live and dead endpoints. Periodically i.e. every 1 second this module
- * chooses a random node and initiates a round of Gossip with it. A round of Gossip involves 3
- * rounds of messaging. For instance if node A wants to initiate a round of Gossip with node B
- * it starts off by sending node B a GossipDigestSynMessage. Node B on receipt of this message
- * sends node A a GossipDigestAckMessage. On receipt of this message node A sends node B a
- * GossipDigestAck2Message which completes a round of Gossip. This module as and when it hears one
- * of the three above mentioned messages updates the Failure Detector with the liveness information.
- *
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class Gossiper implements IFailureDetectionEventListener, IEndPointStateChangePublisher
-{
-    private class GossipTimerTask extends TimerTask
-    {
-        public void run()
-        {
-            try
-            {
-                synchronized( Gossiper.instance() )
-                {
-                	/* Update the local heartbeat counter. */
-                    endPointStateMap_.get(localEndPoint_).getHeartBeatState().updateHeartBeat();
-                    List<GossipDigest> gDigests = new ArrayList<GossipDigest>();
-                    Gossiper.instance().makeRandomGossipDigest(gDigests);
-
-                    if ( gDigests.size() > 0 )
-                    {
-                        Message message = makeGossipDigestSynMessage(gDigests);
-                        /* Gossip to some random live member */
-                        boolean bVal = doGossipToLiveMember(message);
-
-                        /* Gossip to some unreachable member with some probability to check if he is back up */
-                        doGossipToUnreachableMember(message);
-
-                        /* Gossip to the seed. */
-                        if ( !bVal )
-                            doGossipToSeed(message);
-
-                        if (logger_.isTraceEnabled())
-                            logger_.trace("Performing status check ...");
-                        doStatusCheck();
-                    }
-                }
-            }
-            catch ( Throwable th )
-            {
-                logger_.info( LogUtil.throwableToString(th) );
-            }
-        }
-    }
-
-    final static int MAX_GOSSIP_PACKET_SIZE = 1428;
-    /* GS - abbreviation for GOSSIPER_STAGE */
-    final static String GOSSIP_STAGE = "GS";
-    /* GSV - abbreviation for GOSSIP-DIGEST-SYN-VERB */
-    final static String JOIN_VERB_HANDLER = "JVH";
-    /* GSV - abbreviation for GOSSIP-DIGEST-SYN-VERB */
-    final static String GOSSIP_DIGEST_SYN_VERB = "GSV";
-    /* GAV - abbreviation for GOSSIP-DIGEST-ACK-VERB */
-    final static String GOSSIP_DIGEST_ACK_VERB = "GAV";
-    /* GA2V - abbreviation for GOSSIP-DIGEST-ACK2-VERB */
-    final static String GOSSIP_DIGEST_ACK2_VERB = "GA2V";
-    final static int intervalInMillis_ = 1000;
-    private static Logger logger_ = Logger.getLogger(Gossiper.class);
-    static Gossiper gossiper_;
-
-    public synchronized static Gossiper instance()
-    {
-        if ( gossiper_ == null )
-        {
-            gossiper_ = new Gossiper();
-        }
-        return gossiper_;
-    }
-
-    private Timer gossipTimer_ = new Timer(false);
-    private EndPoint localEndPoint_;
-    private long aVeryLongTime_;
-    private Random random_ = new Random();
-    /* round robin index through live endpoint set */
-    private int rrIndex_ = 0;
-
-    /* subscribers for interest in EndPointState change */
-    private List<IEndPointStateChangeSubscriber> subscribers_ = new ArrayList<IEndPointStateChangeSubscriber>();
-
-    /* live member set */
-    private Set<EndPoint> liveEndpoints_ = new HashSet<EndPoint>();
-
-    /* unreachable member set */
-    private Set<EndPoint> unreachableEndpoints_ = new HashSet<EndPoint>();
-
-    /* initial seeds for joining the cluster */
-    private Set<EndPoint> seeds_ = new HashSet<EndPoint>();
-
-    /* map where key is the endpoint and value is the state associated with the endpoint */
-    Map<EndPoint, EndPointState> endPointStateMap_ = new Hashtable<EndPoint, EndPointState>();
-
-    /* private CTOR */
-    Gossiper()
-    {
-        aVeryLongTime_ = 259200 * 1000;
-        /* register with the Failure Detector for receiving Failure detector events */
-        FailureDetector.instance().registerFailureDetectionEventListener(this);
-        /* register the verbs */
-        MessagingService.getMessagingInstance().registerVerbHandlers(JOIN_VERB_HANDLER, new JoinVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(GOSSIP_DIGEST_SYN_VERB, new GossipDigestSynVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(GOSSIP_DIGEST_ACK_VERB, new GossipDigestAckVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(GOSSIP_DIGEST_ACK2_VERB, new GossipDigestAck2VerbHandler());
-        /* register the Gossip stage */
-        StageManager.registerStage( Gossiper.GOSSIP_STAGE, new SingleThreadedStage("GMFD") );
-    }
-
-    public void register(IEndPointStateChangeSubscriber subscriber)
-    {
-        subscribers_.add(subscriber);
-    }
-
-    public void unregister(IEndPointStateChangeSubscriber subscriber)
-    {
-        subscribers_.remove(subscriber);
-    }
-
-    public Set<EndPoint> getAllMembers()
-    {
-        Set<EndPoint> allMbrs = new HashSet<EndPoint>();
-        allMbrs.addAll(getLiveMembers());
-        allMbrs.addAll(getUnreachableMembers());
-        return allMbrs;
-    }
-
-    public Set<EndPoint> getLiveMembers()
-    {
-        Set<EndPoint> liveMbrs = new HashSet<EndPoint>(liveEndpoints_);
-        liveMbrs.add( new EndPoint( localEndPoint_.getHost(), localEndPoint_.getPort() ) );
-        return liveMbrs;
-    }
-
-    public Set<EndPoint> getUnreachableMembers()
-    {
-        return new HashSet<EndPoint>(unreachableEndpoints_);
-    }
-
-    /**
-     * This method is used to forcibly remove a node from the membership
-     * set. He is forgotten locally immediately.
-     *
-     * param@ ep the endpoint to be removed from membership.
-     */
-    public synchronized void removeFromMembership(EndPoint ep)
-    {
-        endPointStateMap_.remove(ep);
-        liveEndpoints_.remove(ep);
-        unreachableEndpoints_ .remove(ep);
-    }
-
-    /**
-     * This method is part of IFailureDetectionEventListener interface. This is invoked
-     * by the Failure Detector when it convicts an end point.
-     *
-     * param @ endpoint end point that is convicted.
-    */
-
-    public void convict(EndPoint endpoint)
-    {
-        EndPointState epState = endPointStateMap_.get(endpoint);
-        if ( epState != null )
-        {
-            if ( !epState.isAlive() && epState.isAGossiper() )
-            {
-                /*
-                 * just to be sure - is invoked just to make sure that
-                 * it was called at least once.
-                */
-                if ( liveEndpoints_.contains(endpoint) )
-                {
-                    logger_.info("EndPoint " + endpoint + " is now dead.");
-                    isAlive(endpoint, epState, false);
-
-                    /* Notify an endpoint is dead to interested parties. */
-                    EndPointState deltaState = new EndPointState(epState.getHeartBeatState());
-                    doNotifications(endpoint, deltaState);
-                }
-                epState.isAGossiper(false);
-            }
-        }
-    }
-
-    /**
-     * This method is part of IFailureDetectionEventListener interface. This is invoked
-     * by the Failure Detector when it suspects an end point.
-     *
-     * param @ endpoint end point that is suspected.
-    */
-    public void suspect(EndPoint endpoint)
-    {
-        EndPointState epState = endPointStateMap_.get(endpoint);
-        if ( epState.isAlive() )
-        {
-            logger_.info("EndPoint " + endpoint + " is now dead.");
-            isAlive(endpoint, epState, false);
-
-            /* Notify an endpoint is dead to interested parties. */
-            EndPointState deltaState = new EndPointState(epState.getHeartBeatState());
-            doNotifications(endpoint, deltaState);
-        }
-    }
-
-    int getMaxEndPointStateVersion(EndPointState epState)
-    {
-        List<Integer> versions = new ArrayList<Integer>();
-        versions.add( epState.getHeartBeatState().getHeartBeatVersion() );
-        Map<String, ApplicationState> appStateMap = epState.getApplicationState();
-
-        Set<String> keys = appStateMap.keySet();
-        for ( String key : keys )
-        {
-            int stateVersion = appStateMap.get(key).getStateVersion();
-            versions.add( stateVersion );
-        }
-
-        /* sort to get the max version to build GossipDigest for this endpoint */
-        Collections.sort(versions);
-        int maxVersion = versions.get(versions.size() - 1);
-        versions.clear();
-        return maxVersion;
-    }
-
-    /**
-     * Removes the endpoint from unreachable endpoint set
-     *
-     * @param endpoint endpoint to be removed from the current membership.
-    */
-    void evictFromMembership(EndPoint endpoint)
-    {
-        unreachableEndpoints_.remove(endpoint);
-    }
-
-    /* No locking required since it is called from a method that already has acquired a lock */
-    @Deprecated
-    void makeGossipDigest(List<GossipDigest> gDigests)
-    {
-        /* Add the local endpoint state */
-        EndPointState epState = endPointStateMap_.get(localEndPoint_);
-        int generation = epState.getHeartBeatState().getGeneration();
-        int maxVersion = getMaxEndPointStateVersion(epState);
-        gDigests.add( new GossipDigest(localEndPoint_, generation, maxVersion) );
-
-        for ( EndPoint liveEndPoint : liveEndpoints_ )
-        {
-            epState = endPointStateMap_.get(liveEndPoint);
-            if ( epState != null )
-            {
-                generation = epState.getHeartBeatState().getGeneration();
-                maxVersion = getMaxEndPointStateVersion(epState);
-                gDigests.add( new GossipDigest(liveEndPoint, generation, maxVersion) );
-            }
-            else
-            {
-            	gDigests.add( new GossipDigest(liveEndPoint, 0, 0) );
-            }
-        }
-    }
-
-    /**
-     * No locking required since it is called from a method that already
-     * has acquired a lock. The gossip digest is built based on randomization
-     * rather than just looping through the collection of live endpoints.
-     *
-     * @param gDigests list of Gossip Digests.
-    */
-    void makeRandomGossipDigest(List<GossipDigest> gDigests)
-    {
-        /* Add the local endpoint state */
-        EndPointState epState = endPointStateMap_.get(localEndPoint_);
-        int generation = epState.getHeartBeatState().getGeneration();
-        int maxVersion = getMaxEndPointStateVersion(epState);
-        gDigests.add( new GossipDigest(localEndPoint_, generation, maxVersion) );
-
-        List<EndPoint> endpoints = new ArrayList<EndPoint>( liveEndpoints_ );
-        Collections.shuffle(endpoints, random_);
-        for ( EndPoint liveEndPoint : endpoints )
-        {
-            epState = endPointStateMap_.get(liveEndPoint);
-            if ( epState != null )
-            {
-                generation = epState.getHeartBeatState().getGeneration();
-                maxVersion = getMaxEndPointStateVersion(epState);
-                gDigests.add( new GossipDigest(liveEndPoint, generation, maxVersion) );
-            }
-            else
-            {
-            	gDigests.add( new GossipDigest(liveEndPoint, 0, 0) );
-            }
-        }
-
-        /* FOR DEBUG ONLY - remove later */
-        StringBuilder sb = new StringBuilder();
-        for ( GossipDigest gDigest : gDigests )
-        {
-            sb.append(gDigest);
-            sb.append(" ");
-        }
-        if (logger_.isTraceEnabled())
-            logger_.trace("Gossip Digests are : " + sb.toString());
-    }
-
-    public int getCurrentGenerationNumber(EndPoint endpoint)
-    {
-    	return endPointStateMap_.get(endpoint).getHeartBeatState().getGeneration();
-    }
-
-    Message makeGossipDigestSynMessage(List<GossipDigest> gDigests) throws IOException
-    {
-        GossipDigestSynMessage gDigestMessage = new GossipDigestSynMessage(DatabaseDescriptor.getClusterName(), gDigests);
-        ByteArrayOutputStream bos = new ByteArrayOutputStream(Gossiper.MAX_GOSSIP_PACKET_SIZE);
-        DataOutputStream dos = new DataOutputStream( bos );
-        GossipDigestSynMessage.serializer().serialize(gDigestMessage, dos);
-        Message message = new Message(localEndPoint_, Gossiper.GOSSIP_STAGE, GOSSIP_DIGEST_SYN_VERB, bos.toByteArray());
-        return message;
-    }
-
-    Message makeGossipDigestAckMessage(GossipDigestAckMessage gDigestAckMessage) throws IOException
-    {
-        ByteArrayOutputStream bos = new ByteArrayOutputStream(Gossiper.MAX_GOSSIP_PACKET_SIZE);
-        DataOutputStream dos = new DataOutputStream(bos);
-        GossipDigestAckMessage.serializer().serialize(gDigestAckMessage, dos);
-        if (logger_.isTraceEnabled())
-            logger_.trace("@@@@ Size of GossipDigestAckMessage is " + bos.toByteArray().length);
-        Message message = new Message(localEndPoint_, Gossiper.GOSSIP_STAGE, GOSSIP_DIGEST_ACK_VERB, bos.toByteArray());
-        return message;
-    }
-
-    Message makeGossipDigestAck2Message(GossipDigestAck2Message gDigestAck2Message) throws IOException
-    {
-        ByteArrayOutputStream bos = new ByteArrayOutputStream(Gossiper.MAX_GOSSIP_PACKET_SIZE);
-        DataOutputStream dos = new DataOutputStream(bos);
-        GossipDigestAck2Message.serializer().serialize(gDigestAck2Message, dos);
-        Message message = new Message(localEndPoint_, Gossiper.GOSSIP_STAGE, GOSSIP_DIGEST_ACK2_VERB, bos.toByteArray());
-        return message;
-    }
-
-    boolean sendGossipToLiveNode(Message message)
-    {
-        int size = liveEndpoints_.size();
-        List<EndPoint> eps = new ArrayList<EndPoint>(liveEndpoints_);
-
-        if ( rrIndex_ >= size )
-        {
-            rrIndex_ = -1;
-        }
-
-        EndPoint to = eps.get(++rrIndex_);
-        if (logger_.isTraceEnabled())
-            logger_.trace("Sending a GossipDigestSynMessage to " + to + " ...");
-        MessagingService.getMessagingInstance().sendUdpOneWay(message, to);
-        return seeds_.contains(to);
-    }
-
-    /**
-     * Returns true if the chosen target was also a seed. False otherwise
-     *
-     *  @param message message to sent
-     *  @param epSet a set of endpoint from which a random endpoint is chosen.
-     *  @return true if the chosen endpoint is also a seed.
-     */
-    boolean sendGossip(Message message, Set<EndPoint> epSet)
-    {
-        int size = epSet.size();
-        /* Generate a random number from 0 -> size */
-        List<EndPoint> liveEndPoints = new ArrayList<EndPoint>(epSet);
-        int index = (size == 1) ? 0 : random_.nextInt(size);
-        EndPoint to = liveEndPoints.get(index);
-        if (logger_.isTraceEnabled())
-            logger_.trace("Sending a GossipDigestSynMessage to " + to + " ...");
-        MessagingService.getMessagingInstance().sendUdpOneWay(message, to);
-        return seeds_.contains(to);
-    }
-
-    /* Sends a Gossip message to a live member and returns a reference to the member */
-    boolean doGossipToLiveMember(Message message)
-    {
-        int size = liveEndpoints_.size();
-        if ( size == 0 )
-            return false;
-        // return sendGossipToLiveNode(message);
-        /* Use this for a cluster size >= 30 */
-        return sendGossip(message, liveEndpoints_);
-    }
-
-    /* Sends a Gossip message to an unreachable member */
-    void doGossipToUnreachableMember(Message message)
-    {
-        double liveEndPoints = liveEndpoints_.size();
-        double unreachableEndPoints = unreachableEndpoints_.size();
-        if ( unreachableEndPoints > 0 )
-        {
-            /* based on some probability */
-            double prob = unreachableEndPoints / (liveEndPoints + 1);
-            double randDbl = random_.nextDouble();
-            if ( randDbl < prob )
-                sendGossip(message, unreachableEndpoints_);
-        }
-    }
-
-    /* Gossip to a seed for facilitating partition healing */
-    void doGossipToSeed(Message message)
-    {
-        int size = seeds_.size();
-        if ( size > 0 )
-        {
-            if ( size == 1 && seeds_.contains(localEndPoint_) )
-            {
-                return;
-            }
-
-            if ( liveEndpoints_.size() == 0 )
-            {
-                sendGossip(message, seeds_);
-            }
-            else
-            {
-                /* Gossip with the seed with some probability. */
-                double probability = seeds_.size() / ( liveEndpoints_.size() + unreachableEndpoints_.size() );
-                double randDbl = random_.nextDouble();
-                if ( randDbl <= probability )
-                    sendGossip(message, seeds_);
-            }
-        }
-    }
-
-    void doStatusCheck()
-    {
-        Set<EndPoint> eps = endPointStateMap_.keySet();
-
-        for ( EndPoint endpoint : eps )
-        {
-            if ( endpoint.equals(localEndPoint_) )
-                continue;
-
-            FailureDetector.instance().interpret(endpoint);
-            EndPointState epState = endPointStateMap_.get(endpoint);
-            if ( epState != null )
-            {
-                long duration = System.currentTimeMillis() - epState.getUpdateTimestamp();
-                if ( !epState.isAlive() && (duration > aVeryLongTime_) )
-                {
-                    evictFromMembership(endpoint);
-                }
-            }
-        }
-    }
-
-    EndPointState getEndPointStateForEndPoint(EndPoint ep)
-    {
-        return endPointStateMap_.get(ep);
-    }
-
-    synchronized EndPointState getStateForVersionBiggerThan(EndPoint forEndpoint, int version)
-    {
-        EndPointState epState = endPointStateMap_.get(forEndpoint);
-        EndPointState reqdEndPointState = null;
-
-        if ( epState != null )
-        {
-            /*
-             * Here we try to include the Heart Beat state only if it is
-             * greater than the version passed in. It might happen that
-             * the heart beat version maybe lesser than the version passed
-             * in and some application state has a version that is greater
-             * than the version passed in. In this case we also send the old
-             * heart beat and throw it away on the receiver if it is redundant.
-            */
-            int localHbVersion = epState.getHeartBeatState().getHeartBeatVersion();
-            if ( localHbVersion > version )
-            {
-                reqdEndPointState = new EndPointState(epState.getHeartBeatState());
-            }
-            Map<String, ApplicationState> appStateMap = epState.getApplicationState();
-            /* Accumulate all application states whose versions are greater than "version" variable */
-            Set<String> keys = appStateMap.keySet();
-            for ( String key : keys )
-            {
-                ApplicationState appState = appStateMap.get(key);
-                if ( appState.getStateVersion() > version )
-                {
-                    if ( reqdEndPointState == null )
-                    {
-                        reqdEndPointState = new EndPointState(epState.getHeartBeatState());
-                    }
-                    reqdEndPointState.addApplicationState(key, appState);
-                }
-            }
-        }
-        return reqdEndPointState;
-    }
-
-    /*
-     * This method is called only from the JoinVerbHandler. This happens
-     * when a new node coming up multicasts the JoinMessage. Here we need
-     * to add the endPoint to the list of live endpoints.
-    */
-    synchronized void join(EndPoint from)
-    {
-        if ( !from.equals( localEndPoint_ ) )
-        {
-            /* Mark this endpoint as "live" */
-        	liveEndpoints_.add(from);
-            unreachableEndpoints_.remove(from);
-        }
-    }
-
-    void notifyFailureDetector(List<GossipDigest> gDigests)
-    {
-        IFailureDetector fd = FailureDetector.instance();
-        for ( GossipDigest gDigest : gDigests )
-        {
-            EndPointState localEndPointState = endPointStateMap_.get(gDigest.endPoint_);
-            /*
-             * If the local endpoint state exists then report to the FD only
-             * if the versions workout.
-            */
-            if ( localEndPointState != null )
-            {
-                int localGeneration = endPointStateMap_.get(gDigest.endPoint_).getHeartBeatState().generation_;
-                int remoteGeneration = gDigest.generation_;
-                if ( remoteGeneration > localGeneration )
-                {
-                    fd.report(gDigest.endPoint_);
-                    continue;
-                }
-
-                if ( remoteGeneration == localGeneration )
-                {
-                    int localVersion = getMaxEndPointStateVersion(localEndPointState);
-                    //int localVersion = endPointStateMap_.get(gDigest.endPoint_).getHeartBeatState().getHeartBeatVersion();
-                    int remoteVersion = gDigest.maxVersion_;
-                    if ( remoteVersion > localVersion )
-                    {
-                        fd.report(gDigest.endPoint_);
-                    }
-                }
-            }
-        }
-    }
-
-    void notifyFailureDetector(Map<EndPoint, EndPointState> remoteEpStateMap)
-    {
-        IFailureDetector fd = FailureDetector.instance();
-        Set<EndPoint> endpoints = remoteEpStateMap.keySet();
-        for ( EndPoint endpoint : endpoints )
-        {
-            EndPointState remoteEndPointState = remoteEpStateMap.get(endpoint);
-            EndPointState localEndPointState = endPointStateMap_.get(endpoint);
-            /*
-             * If the local endpoint state exists then report to the FD only
-             * if the versions workout.
-            */
-            if ( localEndPointState != null )
-            {
-                int localGeneration = localEndPointState.getHeartBeatState().generation_;
-                int remoteGeneration = remoteEndPointState.getHeartBeatState().generation_;
-                if ( remoteGeneration > localGeneration )
-                {
-                    fd.report(endpoint);
-                    continue;
-                }
-
-                if ( remoteGeneration == localGeneration )
-                {
-                    int localVersion = getMaxEndPointStateVersion(localEndPointState);
-                    //int localVersion = localEndPointState.getHeartBeatState().getHeartBeatVersion();
-                    int remoteVersion = remoteEndPointState.getHeartBeatState().getHeartBeatVersion();
-                    if ( remoteVersion > localVersion )
-                    {
-                        fd.report(endpoint);
-                    }
-                }
-            }
-        }
-    }
-
-    void markAlive(EndPoint addr, EndPointState localState)
-    {
-        if (logger_.isTraceEnabled())
-            logger_.trace("marking as alive " + addr);
-        if ( !localState.isAlive() )
-        {
-            isAlive(addr, localState, true);
-            logger_.info("EndPoint " + addr + " is now UP");
-        }
-    }
-
-    private void handleNewJoin(EndPoint ep, EndPointState epState)
-    {
-    	logger_.info("Node " + ep + " has now joined.");
-        /* Mark this endpoint as "live" */
-        endPointStateMap_.put(ep, epState);
-        isAlive(ep, epState, true);
-        /* Notify interested parties about endpoint state change */
-        doNotifications(ep, epState);
-    }
-
-    synchronized void applyStateLocally(Map<EndPoint, EndPointState> epStateMap)
-    {
-        Set<EndPoint> eps = epStateMap.keySet();
-        for( EndPoint ep : eps )
-        {
-            if ( ep.equals( localEndPoint_ ) )
-                continue;
-
-            EndPointState localEpStatePtr = endPointStateMap_.get(ep);
-            EndPointState remoteState = epStateMap.get(ep);
-            /*
-                If state does not exist just add it. If it does then add it only if the version
-                of the remote copy is greater than the local copy.
-            */
-            if ( localEpStatePtr != null )
-            {
-            	int localGeneration = localEpStatePtr.getHeartBeatState().getGeneration();
-            	int remoteGeneration = remoteState.getHeartBeatState().getGeneration();
-
-            	if (remoteGeneration > localGeneration)
-            	{
-            		handleNewJoin(ep, remoteState);
-            	}
-            	else if ( remoteGeneration == localGeneration )
-            	{
-	                /* manage the membership state */
-	                int localMaxVersion = getMaxEndPointStateVersion(localEpStatePtr);
-	                int remoteMaxVersion = getMaxEndPointStateVersion(remoteState);
-	                if ( remoteMaxVersion > localMaxVersion )
-	                {
-	                    markAlive(ep, localEpStatePtr);
-	                    applyHeartBeatStateLocally(ep, localEpStatePtr, remoteState);
-	                    /* apply ApplicationState */
-	                    applyApplicationStateLocally(ep, localEpStatePtr, remoteState);
-	                }
-            	}
-            }
-            else
-            {
-            	handleNewJoin(ep, remoteState);
-            }
-        }
-    }
-
-    void applyHeartBeatStateLocally(EndPoint addr, EndPointState localState, EndPointState remoteState)
-    {
-        HeartBeatState localHbState = localState.getHeartBeatState();
-        HeartBeatState remoteHbState = remoteState.getHeartBeatState();
-
-        if ( remoteHbState.getGeneration() > localHbState.getGeneration() )
-        {
-            markAlive(addr, localState);
-            localState.setHeartBeatState(remoteHbState);
-        }
-        if ( localHbState.getGeneration() == remoteHbState.getGeneration() )
-        {
-            if ( remoteHbState.getHeartBeatVersion() > localHbState.getHeartBeatVersion() )
-            {
-                int oldVersion = localHbState.getHeartBeatVersion();
-                localState.setHeartBeatState(remoteHbState);
-                if (logger_.isTraceEnabled())
-                    logger_.trace("Updating heartbeat state version to " + localState.getHeartBeatState().getHeartBeatVersion() + " from " + oldVersion + " for " + addr + " ...");
-            }
-        }
-    }
-
-    void applyApplicationStateLocally(EndPoint addr, EndPointState localStatePtr, EndPointState remoteStatePtr)
-    {
-        Map<String, ApplicationState> localAppStateMap = localStatePtr.getApplicationState();
-        Map<String, ApplicationState> remoteAppStateMap = remoteStatePtr.getApplicationState();
-
-        Set<String> remoteKeys = remoteAppStateMap.keySet();
-        for ( String remoteKey : remoteKeys )
-        {
-            ApplicationState remoteAppState = remoteAppStateMap.get(remoteKey);
-            ApplicationState localAppState = localAppStateMap.get(remoteKey);
-
-            /* If state doesn't exist locally for this key then just apply it */
-            if ( localAppState == null )
-            {
-                localStatePtr.addApplicationState(remoteKey, remoteAppState);
-                /* notify interested parties of endpoint state change */
-                EndPointState deltaState = new EndPointState(localStatePtr.getHeartBeatState());
-                deltaState.addApplicationState(remoteKey, remoteAppState);
-                doNotifications(addr, deltaState);
-                continue;
-            }
-
-            int remoteGeneration = remoteStatePtr.getHeartBeatState().getGeneration();
-            int localGeneration = localStatePtr.getHeartBeatState().getGeneration();
-
-            /* If the remoteGeneration is greater than localGeneration then apply state blindly */
-            if ( remoteGeneration > localGeneration )
-            {
-                localStatePtr.addApplicationState(remoteKey, remoteAppState);
-                /* notify interested parties of endpoint state change */
-                EndPointState deltaState = new EndPointState(localStatePtr.getHeartBeatState());
-                deltaState.addApplicationState(remoteKey, remoteAppState);
-                doNotifications(addr, deltaState);
-                continue;
-            }
-
-            /* If the generations are the same then apply state if the remote version is greater than local version. */
-            if ( remoteGeneration == localGeneration )
-            {
-                int remoteVersion = remoteAppState.getStateVersion();
-                int localVersion = localAppState.getStateVersion();
-
-                if ( remoteVersion > localVersion )
-                {
-                    localStatePtr.addApplicationState(remoteKey, remoteAppState);
-                    /* notify interested parties of endpoint state change */
-                    EndPointState deltaState = new EndPointState(localStatePtr.getHeartBeatState());
-                    deltaState.addApplicationState(remoteKey, remoteAppState);
-                    doNotifications(addr, deltaState);
-                }
-            }
-        }
-    }
-
-    void doNotifications(EndPoint addr, EndPointState epState)
-    {
-        for ( IEndPointStateChangeSubscriber subscriber : subscribers_ )
-        {
-            subscriber.onChange(addr, epState);
-        }
-    }
-
-    synchronized void isAlive(EndPoint addr, EndPointState epState, boolean value)
-    {
-        epState.isAlive(value);
-        if ( value )
-        {
-            liveEndpoints_.add(addr);
-            unreachableEndpoints_.remove(addr);
-        }
-        else
-        {
-            liveEndpoints_.remove(addr);
-            unreachableEndpoints_.add(addr);
-        }
-        if ( epState.isAGossiper() )
-            return;
-        epState.isAGossiper(true);
-    }
-
-    /* These are helper methods used from GossipDigestSynVerbHandler */
-    Map<EndPoint, GossipDigest> getEndPointGossipDigestMap(List<GossipDigest> gDigestList)
-    {
-        Map<EndPoint, GossipDigest> epMap = new HashMap<EndPoint, GossipDigest>();
-        for( GossipDigest gDigest : gDigestList )
-        {
-            epMap.put( gDigest.getEndPoint(), gDigest );
-        }
-        return epMap;
-    }
-
-    /* This is a helper method to get all EndPoints from a list of GossipDigests */
-    EndPoint[] getEndPointsFromGossipDigest(List<GossipDigest> gDigestList)
-    {
-        Set<EndPoint> set = new HashSet<EndPoint>();
-        for ( GossipDigest gDigest : gDigestList )
-        {
-            set.add( gDigest.getEndPoint() );
-        }
-        return set.toArray( new EndPoint[0] );
-    }
-
-    /* Request all the state for the endpoint in the gDigest */
-    void requestAll(GossipDigest gDigest, List<GossipDigest> deltaGossipDigestList, int remoteGeneration)
-    {
-        /* We are here since we have no data for this endpoint locally so request everthing. */
-        deltaGossipDigestList.add( new GossipDigest(gDigest.getEndPoint(), remoteGeneration, 0) );
-    }
-
-    /* Send all the data with version greater than maxRemoteVersion */
-    void sendAll(GossipDigest gDigest, Map<EndPoint, EndPointState> deltaEpStateMap, int maxRemoteVersion)
-    {
-        EndPointState localEpStatePtr = getStateForVersionBiggerThan(gDigest.getEndPoint(), maxRemoteVersion) ;
-        if ( localEpStatePtr != null )
-            deltaEpStateMap.put(gDigest.getEndPoint(), localEpStatePtr);
-    }
-
-    /*
-        This method is used to figure the state that the Gossiper has but Gossipee doesn't. The delta digests
-        and the delta state are built up.
-    */
-    synchronized void examineGossiper(List<GossipDigest> gDigestList, List<GossipDigest> deltaGossipDigestList, Map<EndPoint, EndPointState> deltaEpStateMap)
-    {
-        for ( GossipDigest gDigest : gDigestList )
-        {
-            int remoteGeneration = gDigest.getGeneration();
-            int maxRemoteVersion = gDigest.getMaxVersion();
-            /* Get state associated with the end point in digest */
-            EndPointState epStatePtr = endPointStateMap_.get(gDigest.getEndPoint());
-            /*
-                Here we need to fire a GossipDigestAckMessage. If we have some data associated with this endpoint locally
-                then we follow the "if" path of the logic. If we have absolutely nothing for this endpoint we need to
-                request all the data for this endpoint.
-            */
-            if ( epStatePtr != null )
-            {
-                int localGeneration = epStatePtr.getHeartBeatState().getGeneration();
-                /* get the max version of all keys in the state associated with this endpoint */
-                int maxLocalVersion = getMaxEndPointStateVersion(epStatePtr);
-                if ( remoteGeneration == localGeneration && maxRemoteVersion == maxLocalVersion )
-                    continue;
-
-                if ( remoteGeneration > localGeneration )
-                {
-                    /* we request everything from the gossiper */
-                    requestAll(gDigest, deltaGossipDigestList, remoteGeneration);
-                }
-                if ( remoteGeneration < localGeneration )
-                {
-                    /* send all data with generation = localgeneration and version > 0 */
-                    sendAll(gDigest, deltaEpStateMap, 0);
-                }
-                if ( remoteGeneration == localGeneration )
-                {
-                    /*
-                        If the max remote version is greater then we request the remote endpoint send us all the data
-                        for this endpoint with version greater than the max version number we have locally for this
-                        endpoint.
-                        If the max remote version is lesser, then we send all the data we have locally for this endpoint
-                        with version greater than the max remote version.
-                    */
-                    if ( maxRemoteVersion > maxLocalVersion )
-                    {
-                        deltaGossipDigestList.add( new GossipDigest(gDigest.getEndPoint(), remoteGeneration, maxLocalVersion) );
-                    }
-                    if ( maxRemoteVersion < maxLocalVersion )
-                    {
-                        /* send all data with generation = localgeneration and version > maxRemoteVersion */
-                        sendAll(gDigest, deltaEpStateMap, maxRemoteVersion);
-                    }
-                }
-            }
-            else
-            {
-                /* We are here since we have no data for this endpoint locally so request everything. */
-                requestAll(gDigest, deltaGossipDigestList, remoteGeneration);
-            }
-        }
-    }
-
-    public void start(EndPoint localEndPoint, int generationNbr) throws IOException
-    {
-        localEndPoint_ = localEndPoint;
-        /* Get the seeds from the config and initialize them. */
-        Set<String> seedHosts = DatabaseDescriptor.getSeeds();
-        for( String seedHost : seedHosts )
-        {
-            EndPoint seed = new EndPoint(InetAddress.getByName(seedHost).getHostAddress(),
-                                         DatabaseDescriptor.getControlPort());
-            if ( seed.equals(localEndPoint) )
-                continue;
-            seeds_.add(seed);
-        }
-
-        /* initialize the heartbeat state for this localEndPoint */
-        EndPointState localState = endPointStateMap_.get(localEndPoint_);
-        if ( localState == null )
-        {
-            HeartBeatState hbState = new HeartBeatState(generationNbr, 0);
-            localState = new EndPointState(hbState);
-            localState.isAlive(true);
-            localState.isAGossiper(true);
-            endPointStateMap_.put(localEndPoint_, localState);
-        }
-
-        /* starts a timer thread */
-        gossipTimer_.schedule( new GossipTimerTask(), Gossiper.intervalInMillis_, Gossiper.intervalInMillis_);
-    }
-
-    public synchronized void addApplicationState(String key, ApplicationState appState)
-    {
-        EndPointState epState = endPointStateMap_.get(localEndPoint_);
-        if ( epState != null )
-        {
-            epState.addApplicationState(key, appState);
-        }
-    }
-
-    public void stop()
-    {
-        gossipTimer_.cancel();
-    }
-}
-
-class JoinVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger( JoinVerbHandler.class);
-
-    public void doVerb(Message message)
-    {
-        EndPoint from = message.getFrom();
-        if (logger_.isDebugEnabled())
-          logger_.debug("Received a JoinMessage from " + from);
-
-        byte[] bytes = message.getMessageBody();
-        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
-
-        JoinMessage joinMessage = null;
-        try
-        {
-            joinMessage = JoinMessage.serializer().deserialize(dis);
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-        if ( joinMessage.clusterId_.equals( DatabaseDescriptor.getClusterName() ) )
-        {
-            Gossiper.instance().join(from);
-        }
-    }
-}
-
-class GossipDigestSynVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger( GossipDigestSynVerbHandler.class);
-
-    public void doVerb(Message message)
-    {
-        EndPoint from = message.getFrom();
-        if (logger_.isTraceEnabled())
-            logger_.trace("Received a GossipDigestSynMessage from " + from);
-
-        byte[] bytes = message.getMessageBody();
-        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
-
-        try
-        {
-            GossipDigestSynMessage gDigestMessage = GossipDigestSynMessage.serializer().deserialize(dis);
-            /* If the message is from a different cluster throw it away. */
-            if ( !gDigestMessage.clusterId_.equals(DatabaseDescriptor.getClusterName()) )
-                return;
-
-            List<GossipDigest> gDigestList = gDigestMessage.getGossipDigests();
-            /* Notify the Failure Detector */
-            Gossiper.instance().notifyFailureDetector(gDigestList);
-
-            doSort(gDigestList);
-
-            List<GossipDigest> deltaGossipDigestList = new ArrayList<GossipDigest>();
-            Map<EndPoint, EndPointState> deltaEpStateMap = new HashMap<EndPoint, EndPointState>();
-            Gossiper.instance().examineGossiper(gDigestList, deltaGossipDigestList, deltaEpStateMap);
-
-            GossipDigestAckMessage gDigestAck = new GossipDigestAckMessage(deltaGossipDigestList, deltaEpStateMap);
-            Message gDigestAckMessage = Gossiper.instance().makeGossipDigestAckMessage(gDigestAck);
-            if (logger_.isTraceEnabled())
-                logger_.trace("Sending a GossipDigestAckMessage to " + from);
-            MessagingService.getMessagingInstance().sendUdpOneWay(gDigestAckMessage, from);
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /*
-     * First construct a map whose key is the endpoint in the GossipDigest and the value is the
-     * GossipDigest itself. Then build a list of version differences i.e difference between the
-     * version in the GossipDigest and the version in the local state for a given EndPoint.
-     * Sort this list. Now loop through the sorted list and retrieve the GossipDigest corresponding
-     * to the endpoint from the map that was initially constructed.
-    */
-    private void doSort(List<GossipDigest> gDigestList)
-    {
-        /* Construct a map of endpoint to GossipDigest. */
-        Map<EndPoint, GossipDigest> epToDigestMap = new HashMap<EndPoint, GossipDigest>();
-        for ( GossipDigest gDigest : gDigestList )
-        {
-            epToDigestMap.put(gDigest.getEndPoint(), gDigest);
-        }
-
-        /*
-         * These digests have their maxVersion set to the difference of the version
-         * of the local EndPointState and the version found in the GossipDigest.
-        */
-        List<GossipDigest> diffDigests = new ArrayList<GossipDigest>();
-        for ( GossipDigest gDigest : gDigestList )
-        {
-            EndPoint ep = gDigest.getEndPoint();
-            EndPointState epState = Gossiper.instance().getEndPointStateForEndPoint(ep);
-            int version = (epState != null) ? Gossiper.instance().getMaxEndPointStateVersion( epState ) : 0;
-            int diffVersion = Math.abs(version - gDigest.getMaxVersion() );
-            diffDigests.add( new GossipDigest(ep, gDigest.getGeneration(), diffVersion) );
-        }
-
-        gDigestList.clear();
-        Collections.sort(diffDigests);
-        int size = diffDigests.size();
-        /*
-         * Report the digests in descending order. This takes care of the endpoints
-         * that are far behind w.r.t this local endpoint
-        */
-        for ( int i = size - 1; i >= 0; --i )
-        {
-            gDigestList.add( epToDigestMap.get(diffDigests.get(i).getEndPoint()) );
-        }
-    }
-}
-
-class GossipDigestAckVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger(GossipDigestAckVerbHandler.class);
-
-    public void doVerb(Message message)
-    {
-        EndPoint from = message.getFrom();
-        if (logger_.isTraceEnabled())
-            logger_.trace("Received a GossipDigestAckMessage from " + from);
-
-        byte[] bytes = message.getMessageBody();
-        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
-
-        try
-        {
-            GossipDigestAckMessage gDigestAckMessage = GossipDigestAckMessage.serializer().deserialize(dis);
-            List<GossipDigest> gDigestList = gDigestAckMessage.getGossipDigestList();
-            Map<EndPoint, EndPointState> epStateMap = gDigestAckMessage.getEndPointStateMap();
-
-            if ( epStateMap.size() > 0 )
-            {
-                /* Notify the Failure Detector */
-                Gossiper.instance().notifyFailureDetector(epStateMap);
-                Gossiper.instance().applyStateLocally(epStateMap);
-            }
-
-            /* Get the state required to send to this gossipee - construct GossipDigestAck2Message */
-            Map<EndPoint, EndPointState> deltaEpStateMap = new HashMap<EndPoint, EndPointState>();
-            for( GossipDigest gDigest : gDigestList )
-            {
-                EndPoint addr = gDigest.getEndPoint();
-                EndPointState localEpStatePtr = Gossiper.instance().getStateForVersionBiggerThan(addr, gDigest.getMaxVersion());
-                if ( localEpStatePtr != null )
-                    deltaEpStateMap.put(addr, localEpStatePtr);
-            }
-
-            GossipDigestAck2Message gDigestAck2 = new GossipDigestAck2Message(deltaEpStateMap);
-            Message gDigestAck2Message = Gossiper.instance().makeGossipDigestAck2Message(gDigestAck2);
-            if (logger_.isTraceEnabled())
-                logger_.trace("Sending a GossipDigestAck2Message to " + from);
-            MessagingService.getMessagingInstance().sendUdpOneWay(gDigestAck2Message, from);
-        }
-        catch ( IOException e )
-        {
-            throw new RuntimeException(e);
-        }
-    }
-}
-
-class GossipDigestAck2VerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger(GossipDigestAck2VerbHandler.class);
-
-    public void doVerb(Message message)
-    {
-        EndPoint from = message.getFrom();
-        if (logger_.isTraceEnabled())
-            logger_.trace("Received a GossipDigestAck2Message from " + from);
-
-        byte[] bytes = message.getMessageBody();
-        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
-        GossipDigestAck2Message gDigestAck2Message = null;
-        try
-        {
-            gDigestAck2Message = GossipDigestAck2Message.serializer().deserialize(dis);
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-        Map<EndPoint, EndPointState> remoteEpStateMap = gDigestAck2Message.getEndPointStateMap();
-        /* Notify the Failure Detector */
-        Gossiper.instance().notifyFailureDetector(remoteEpStateMap);
-        Gossiper.instance().applyStateLocally(remoteEpStateMap);
-    }
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.*;
+import java.util.*;
+import java.net.InetAddress;
+
+import org.apache.cassandra.concurrent.SingleThreadedStage;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * This module is responsible for Gossiping information for the local endpoint. This abstraction
+ * maintains the list of live and dead endpoints. Periodically i.e. every 1 second this module
+ * chooses a random node and initiates a round of Gossip with it. A round of Gossip involves 3
+ * rounds of messaging. For instance if node A wants to initiate a round of Gossip with node B
+ * it starts off by sending node B a GossipDigestSynMessage. Node B on receipt of this message
+ * sends node A a GossipDigestAckMessage. On receipt of this message node A sends node B a
+ * GossipDigestAck2Message which completes a round of Gossip. This module as and when it hears one
+ * of the three above mentioned messages updates the Failure Detector with the liveness information.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Gossiper implements IFailureDetectionEventListener, IEndPointStateChangePublisher
+{
+    private class GossipTimerTask extends TimerTask
+    {
+        public void run()
+        {
+            try
+            {
+                synchronized( Gossiper.instance() )
+                {
+                	/* Update the local heartbeat counter. */
+                    endPointStateMap_.get(localEndPoint_).getHeartBeatState().updateHeartBeat();
+                    List<GossipDigest> gDigests = new ArrayList<GossipDigest>();
+                    Gossiper.instance().makeRandomGossipDigest(gDigests);
+
+                    if ( gDigests.size() > 0 )
+                    {
+                        Message message = makeGossipDigestSynMessage(gDigests);
+                        /* Gossip to some random live member */
+                        boolean bVal = doGossipToLiveMember(message);
+
+                        /* Gossip to some unreachable member with some probability to check if he is back up */
+                        doGossipToUnreachableMember(message);
+
+                        /* Gossip to the seed. */
+                        if ( !bVal )
+                            doGossipToSeed(message);
+
+                        if (logger_.isTraceEnabled())
+                            logger_.trace("Performing status check ...");
+                        doStatusCheck();
+                    }
+                }
+            }
+            catch ( Throwable th )
+            {
+                logger_.info( LogUtil.throwableToString(th) );
+            }
+        }
+    }
+
+    final static int MAX_GOSSIP_PACKET_SIZE = 1428;
+    /* GS - abbreviation for GOSSIPER_STAGE */
+    final static String GOSSIP_STAGE = "GS";
+    /* GSV - abbreviation for GOSSIP-DIGEST-SYN-VERB */
+    final static String JOIN_VERB_HANDLER = "JVH";
+    /* GSV - abbreviation for GOSSIP-DIGEST-SYN-VERB */
+    final static String GOSSIP_DIGEST_SYN_VERB = "GSV";
+    /* GAV - abbreviation for GOSSIP-DIGEST-ACK-VERB */
+    final static String GOSSIP_DIGEST_ACK_VERB = "GAV";
+    /* GA2V - abbreviation for GOSSIP-DIGEST-ACK2-VERB */
+    final static String GOSSIP_DIGEST_ACK2_VERB = "GA2V";
+    final static int intervalInMillis_ = 1000;
+    private static Logger logger_ = Logger.getLogger(Gossiper.class);
+    static Gossiper gossiper_;
+
+    public synchronized static Gossiper instance()
+    {
+        if ( gossiper_ == null )
+        {
+            gossiper_ = new Gossiper();
+        }
+        return gossiper_;
+    }
+
+    private Timer gossipTimer_ = new Timer(false);
+    private EndPoint localEndPoint_;
+    private long aVeryLongTime_;
+    private Random random_ = new Random();
+    /* round robin index through live endpoint set */
+    private int rrIndex_ = 0;
+
+    /* subscribers for interest in EndPointState change */
+    private List<IEndPointStateChangeSubscriber> subscribers_ = new ArrayList<IEndPointStateChangeSubscriber>();
+
+    /* live member set */
+    private Set<EndPoint> liveEndpoints_ = new HashSet<EndPoint>();
+
+    /* unreachable member set */
+    private Set<EndPoint> unreachableEndpoints_ = new HashSet<EndPoint>();
+
+    /* initial seeds for joining the cluster */
+    private Set<EndPoint> seeds_ = new HashSet<EndPoint>();
+
+    /* map where key is the endpoint and value is the state associated with the endpoint */
+    Map<EndPoint, EndPointState> endPointStateMap_ = new Hashtable<EndPoint, EndPointState>();
+
+    /* private CTOR */
+    Gossiper()
+    {
+        aVeryLongTime_ = 259200 * 1000;
+        /* register with the Failure Detector for receiving Failure detector events */
+        FailureDetector.instance().registerFailureDetectionEventListener(this);
+        /* register the verbs */
+        MessagingService.getMessagingInstance().registerVerbHandlers(JOIN_VERB_HANDLER, new JoinVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(GOSSIP_DIGEST_SYN_VERB, new GossipDigestSynVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(GOSSIP_DIGEST_ACK_VERB, new GossipDigestAckVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(GOSSIP_DIGEST_ACK2_VERB, new GossipDigestAck2VerbHandler());
+        /* register the Gossip stage */
+        StageManager.registerStage( Gossiper.GOSSIP_STAGE, new SingleThreadedStage("GMFD") );
+    }
+
+    public void register(IEndPointStateChangeSubscriber subscriber)
+    {
+        subscribers_.add(subscriber);
+    }
+
+    public void unregister(IEndPointStateChangeSubscriber subscriber)
+    {
+        subscribers_.remove(subscriber);
+    }
+
+    public Set<EndPoint> getAllMembers()
+    {
+        Set<EndPoint> allMbrs = new HashSet<EndPoint>();
+        allMbrs.addAll(getLiveMembers());
+        allMbrs.addAll(getUnreachableMembers());
+        return allMbrs;
+    }
+
+    public Set<EndPoint> getLiveMembers()
+    {
+        Set<EndPoint> liveMbrs = new HashSet<EndPoint>(liveEndpoints_);
+        liveMbrs.add( new EndPoint( localEndPoint_.getHost(), localEndPoint_.getPort() ) );
+        return liveMbrs;
+    }
+
+    public Set<EndPoint> getUnreachableMembers()
+    {
+        return new HashSet<EndPoint>(unreachableEndpoints_);
+    }
+
+    /**
+     * This method is used to forcibly remove a node from the membership
+     * set. He is forgotten locally immediately.
+     *
+     * param@ ep the endpoint to be removed from membership.
+     */
+    public synchronized void removeFromMembership(EndPoint ep)
+    {
+        endPointStateMap_.remove(ep);
+        liveEndpoints_.remove(ep);
+        unreachableEndpoints_ .remove(ep);
+    }
+
+    /**
+     * This method is part of IFailureDetectionEventListener interface. This is invoked
+     * by the Failure Detector when it convicts an end point.
+     *
+     * param @ endpoint end point that is convicted.
+    */
+
+    public void convict(EndPoint endpoint)
+    {
+        EndPointState epState = endPointStateMap_.get(endpoint);
+        if ( epState != null )
+        {
+            if ( !epState.isAlive() && epState.isAGossiper() )
+            {
+                /*
+                 * just to be sure - is invoked just to make sure that
+                 * it was called at least once.
+                */
+                if ( liveEndpoints_.contains(endpoint) )
+                {
+                    logger_.info("EndPoint " + endpoint + " is now dead.");
+                    isAlive(endpoint, epState, false);
+
+                    /* Notify an endpoint is dead to interested parties. */
+                    EndPointState deltaState = new EndPointState(epState.getHeartBeatState());
+                    doNotifications(endpoint, deltaState);
+                }
+                epState.isAGossiper(false);
+            }
+        }
+    }
+
+    /**
+     * This method is part of IFailureDetectionEventListener interface. This is invoked
+     * by the Failure Detector when it suspects an end point.
+     *
+     * param @ endpoint end point that is suspected.
+    */
+    public void suspect(EndPoint endpoint)
+    {
+        EndPointState epState = endPointStateMap_.get(endpoint);
+        if ( epState.isAlive() )
+        {
+            logger_.info("EndPoint " + endpoint + " is now dead.");
+            isAlive(endpoint, epState, false);
+
+            /* Notify an endpoint is dead to interested parties. */
+            EndPointState deltaState = new EndPointState(epState.getHeartBeatState());
+            doNotifications(endpoint, deltaState);
+        }
+    }
+
+    int getMaxEndPointStateVersion(EndPointState epState)
+    {
+        List<Integer> versions = new ArrayList<Integer>();
+        versions.add( epState.getHeartBeatState().getHeartBeatVersion() );
+        Map<String, ApplicationState> appStateMap = epState.getApplicationState();
+
+        Set<String> keys = appStateMap.keySet();
+        for ( String key : keys )
+        {
+            int stateVersion = appStateMap.get(key).getStateVersion();
+            versions.add( stateVersion );
+        }
+
+        /* sort to get the max version to build GossipDigest for this endpoint */
+        Collections.sort(versions);
+        int maxVersion = versions.get(versions.size() - 1);
+        versions.clear();
+        return maxVersion;
+    }
+
+    /**
+     * Removes the endpoint from unreachable endpoint set
+     *
+     * @param endpoint endpoint to be removed from the current membership.
+    */
+    void evictFromMembership(EndPoint endpoint)
+    {
+        unreachableEndpoints_.remove(endpoint);
+    }
+
+    /* No locking required since it is called from a method that already has acquired a lock */
+    @Deprecated
+    void makeGossipDigest(List<GossipDigest> gDigests)
+    {
+        /* Add the local endpoint state */
+        EndPointState epState = endPointStateMap_.get(localEndPoint_);
+        int generation = epState.getHeartBeatState().getGeneration();
+        int maxVersion = getMaxEndPointStateVersion(epState);
+        gDigests.add( new GossipDigest(localEndPoint_, generation, maxVersion) );
+
+        for ( EndPoint liveEndPoint : liveEndpoints_ )
+        {
+            epState = endPointStateMap_.get(liveEndPoint);
+            if ( epState != null )
+            {
+                generation = epState.getHeartBeatState().getGeneration();
+                maxVersion = getMaxEndPointStateVersion(epState);
+                gDigests.add( new GossipDigest(liveEndPoint, generation, maxVersion) );
+            }
+            else
+            {
+            	gDigests.add( new GossipDigest(liveEndPoint, 0, 0) );
+            }
+        }
+    }
+
+    /**
+     * No locking required since it is called from a method that already
+     * has acquired a lock. The gossip digest is built based on randomization
+     * rather than just looping through the collection of live endpoints.
+     *
+     * @param gDigests list of Gossip Digests.
+    */
+    void makeRandomGossipDigest(List<GossipDigest> gDigests)
+    {
+        /* Add the local endpoint state */
+        EndPointState epState = endPointStateMap_.get(localEndPoint_);
+        int generation = epState.getHeartBeatState().getGeneration();
+        int maxVersion = getMaxEndPointStateVersion(epState);
+        gDigests.add( new GossipDigest(localEndPoint_, generation, maxVersion) );
+
+        List<EndPoint> endpoints = new ArrayList<EndPoint>( liveEndpoints_ );
+        Collections.shuffle(endpoints, random_);
+        for ( EndPoint liveEndPoint : endpoints )
+        {
+            epState = endPointStateMap_.get(liveEndPoint);
+            if ( epState != null )
+            {
+                generation = epState.getHeartBeatState().getGeneration();
+                maxVersion = getMaxEndPointStateVersion(epState);
+                gDigests.add( new GossipDigest(liveEndPoint, generation, maxVersion) );
+            }
+            else
+            {
+            	gDigests.add( new GossipDigest(liveEndPoint, 0, 0) );
+            }
+        }
+
+        /* FOR DEBUG ONLY - remove later */
+        StringBuilder sb = new StringBuilder();
+        for ( GossipDigest gDigest : gDigests )
+        {
+            sb.append(gDigest);
+            sb.append(" ");
+        }
+        if (logger_.isTraceEnabled())
+            logger_.trace("Gossip Digests are : " + sb.toString());
+    }
+
+    public int getCurrentGenerationNumber(EndPoint endpoint)
+    {
+    	return endPointStateMap_.get(endpoint).getHeartBeatState().getGeneration();
+    }
+
+    Message makeGossipDigestSynMessage(List<GossipDigest> gDigests) throws IOException
+    {
+        GossipDigestSynMessage gDigestMessage = new GossipDigestSynMessage(DatabaseDescriptor.getClusterName(), gDigests);
+        ByteArrayOutputStream bos = new ByteArrayOutputStream(Gossiper.MAX_GOSSIP_PACKET_SIZE);
+        DataOutputStream dos = new DataOutputStream( bos );
+        GossipDigestSynMessage.serializer().serialize(gDigestMessage, dos);
+        Message message = new Message(localEndPoint_, Gossiper.GOSSIP_STAGE, GOSSIP_DIGEST_SYN_VERB, bos.toByteArray());
+        return message;
+    }
+
+    Message makeGossipDigestAckMessage(GossipDigestAckMessage gDigestAckMessage) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream(Gossiper.MAX_GOSSIP_PACKET_SIZE);
+        DataOutputStream dos = new DataOutputStream(bos);
+        GossipDigestAckMessage.serializer().serialize(gDigestAckMessage, dos);
+        if (logger_.isTraceEnabled())
+            logger_.trace("@@@@ Size of GossipDigestAckMessage is " + bos.toByteArray().length);
+        Message message = new Message(localEndPoint_, Gossiper.GOSSIP_STAGE, GOSSIP_DIGEST_ACK_VERB, bos.toByteArray());
+        return message;
+    }
+
+    Message makeGossipDigestAck2Message(GossipDigestAck2Message gDigestAck2Message) throws IOException
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream(Gossiper.MAX_GOSSIP_PACKET_SIZE);
+        DataOutputStream dos = new DataOutputStream(bos);
+        GossipDigestAck2Message.serializer().serialize(gDigestAck2Message, dos);
+        Message message = new Message(localEndPoint_, Gossiper.GOSSIP_STAGE, GOSSIP_DIGEST_ACK2_VERB, bos.toByteArray());
+        return message;
+    }
+
+    boolean sendGossipToLiveNode(Message message)
+    {
+        int size = liveEndpoints_.size();
+        List<EndPoint> eps = new ArrayList<EndPoint>(liveEndpoints_);
+
+        if ( rrIndex_ >= size )
+        {
+            rrIndex_ = -1;
+        }
+
+        EndPoint to = eps.get(++rrIndex_);
+        if (logger_.isTraceEnabled())
+            logger_.trace("Sending a GossipDigestSynMessage to " + to + " ...");
+        MessagingService.getMessagingInstance().sendUdpOneWay(message, to);
+        return seeds_.contains(to);
+    }
+
+    /**
+     * Returns true if the chosen target was also a seed. False otherwise
+     *
+     *  @param message message to sent
+     *  @param epSet a set of endpoint from which a random endpoint is chosen.
+     *  @return true if the chosen endpoint is also a seed.
+     */
+    boolean sendGossip(Message message, Set<EndPoint> epSet)
+    {
+        int size = epSet.size();
+        /* Generate a random number from 0 -> size */
+        List<EndPoint> liveEndPoints = new ArrayList<EndPoint>(epSet);
+        int index = (size == 1) ? 0 : random_.nextInt(size);
+        EndPoint to = liveEndPoints.get(index);
+        if (logger_.isTraceEnabled())
+            logger_.trace("Sending a GossipDigestSynMessage to " + to + " ...");
+        MessagingService.getMessagingInstance().sendUdpOneWay(message, to);
+        return seeds_.contains(to);
+    }
+
+    /* Sends a Gossip message to a live member and returns a reference to the member */
+    boolean doGossipToLiveMember(Message message)
+    {
+        int size = liveEndpoints_.size();
+        if ( size == 0 )
+            return false;
+        // return sendGossipToLiveNode(message);
+        /* Use this for a cluster size >= 30 */
+        return sendGossip(message, liveEndpoints_);
+    }
+
+    /* Sends a Gossip message to an unreachable member */
+    void doGossipToUnreachableMember(Message message)
+    {
+        double liveEndPoints = liveEndpoints_.size();
+        double unreachableEndPoints = unreachableEndpoints_.size();
+        if ( unreachableEndPoints > 0 )
+        {
+            /* based on some probability */
+            double prob = unreachableEndPoints / (liveEndPoints + 1);
+            double randDbl = random_.nextDouble();
+            if ( randDbl < prob )
+                sendGossip(message, unreachableEndpoints_);
+        }
+    }
+
+    /* Gossip to a seed for facilitating partition healing */
+    void doGossipToSeed(Message message)
+    {
+        int size = seeds_.size();
+        if ( size > 0 )
+        {
+            if ( size == 1 && seeds_.contains(localEndPoint_) )
+            {
+                return;
+            }
+
+            if ( liveEndpoints_.size() == 0 )
+            {
+                sendGossip(message, seeds_);
+            }
+            else
+            {
+                /* Gossip with the seed with some probability. */
+                double probability = seeds_.size() / ( liveEndpoints_.size() + unreachableEndpoints_.size() );
+                double randDbl = random_.nextDouble();
+                if ( randDbl <= probability )
+                    sendGossip(message, seeds_);
+            }
+        }
+    }
+
+    void doStatusCheck()
+    {
+        Set<EndPoint> eps = endPointStateMap_.keySet();
+
+        for ( EndPoint endpoint : eps )
+        {
+            if ( endpoint.equals(localEndPoint_) )
+                continue;
+
+            FailureDetector.instance().interpret(endpoint);
+            EndPointState epState = endPointStateMap_.get(endpoint);
+            if ( epState != null )
+            {
+                long duration = System.currentTimeMillis() - epState.getUpdateTimestamp();
+                if ( !epState.isAlive() && (duration > aVeryLongTime_) )
+                {
+                    evictFromMembership(endpoint);
+                }
+            }
+        }
+    }
+
+    EndPointState getEndPointStateForEndPoint(EndPoint ep)
+    {
+        return endPointStateMap_.get(ep);
+    }
+
+    synchronized EndPointState getStateForVersionBiggerThan(EndPoint forEndpoint, int version)
+    {
+        EndPointState epState = endPointStateMap_.get(forEndpoint);
+        EndPointState reqdEndPointState = null;
+
+        if ( epState != null )
+        {
+            /*
+             * Here we try to include the Heart Beat state only if it is
+             * greater than the version passed in. It might happen that
+             * the heart beat version maybe lesser than the version passed
+             * in and some application state has a version that is greater
+             * than the version passed in. In this case we also send the old
+             * heart beat and throw it away on the receiver if it is redundant.
+            */
+            int localHbVersion = epState.getHeartBeatState().getHeartBeatVersion();
+            if ( localHbVersion > version )
+            {
+                reqdEndPointState = new EndPointState(epState.getHeartBeatState());
+            }
+            Map<String, ApplicationState> appStateMap = epState.getApplicationState();
+            /* Accumulate all application states whose versions are greater than "version" variable */
+            Set<String> keys = appStateMap.keySet();
+            for ( String key : keys )
+            {
+                ApplicationState appState = appStateMap.get(key);
+                if ( appState.getStateVersion() > version )
+                {
+                    if ( reqdEndPointState == null )
+                    {
+                        reqdEndPointState = new EndPointState(epState.getHeartBeatState());
+                    }
+                    reqdEndPointState.addApplicationState(key, appState);
+                }
+            }
+        }
+        return reqdEndPointState;
+    }
+
+    /*
+     * This method is called only from the JoinVerbHandler. This happens
+     * when a new node coming up multicasts the JoinMessage. Here we need
+     * to add the endPoint to the list of live endpoints.
+    */
+    synchronized void join(EndPoint from)
+    {
+        if ( !from.equals( localEndPoint_ ) )
+        {
+            /* Mark this endpoint as "live" */
+        	liveEndpoints_.add(from);
+            unreachableEndpoints_.remove(from);
+        }
+    }
+
+    void notifyFailureDetector(List<GossipDigest> gDigests)
+    {
+        IFailureDetector fd = FailureDetector.instance();
+        for ( GossipDigest gDigest : gDigests )
+        {
+            EndPointState localEndPointState = endPointStateMap_.get(gDigest.endPoint_);
+            /*
+             * If the local endpoint state exists then report to the FD only
+             * if the versions workout.
+            */
+            if ( localEndPointState != null )
+            {
+                int localGeneration = endPointStateMap_.get(gDigest.endPoint_).getHeartBeatState().generation_;
+                int remoteGeneration = gDigest.generation_;
+                if ( remoteGeneration > localGeneration )
+                {
+                    fd.report(gDigest.endPoint_);
+                    continue;
+                }
+
+                if ( remoteGeneration == localGeneration )
+                {
+                    int localVersion = getMaxEndPointStateVersion(localEndPointState);
+                    //int localVersion = endPointStateMap_.get(gDigest.endPoint_).getHeartBeatState().getHeartBeatVersion();
+                    int remoteVersion = gDigest.maxVersion_;
+                    if ( remoteVersion > localVersion )
+                    {
+                        fd.report(gDigest.endPoint_);
+                    }
+                }
+            }
+        }
+    }
+
+    void notifyFailureDetector(Map<EndPoint, EndPointState> remoteEpStateMap)
+    {
+        IFailureDetector fd = FailureDetector.instance();
+        Set<EndPoint> endpoints = remoteEpStateMap.keySet();
+        for ( EndPoint endpoint : endpoints )
+        {
+            EndPointState remoteEndPointState = remoteEpStateMap.get(endpoint);
+            EndPointState localEndPointState = endPointStateMap_.get(endpoint);
+            /*
+             * If the local endpoint state exists then report to the FD only
+             * if the versions workout.
+            */
+            if ( localEndPointState != null )
+            {
+                int localGeneration = localEndPointState.getHeartBeatState().generation_;
+                int remoteGeneration = remoteEndPointState.getHeartBeatState().generation_;
+                if ( remoteGeneration > localGeneration )
+                {
+                    fd.report(endpoint);
+                    continue;
+                }
+
+                if ( remoteGeneration == localGeneration )
+                {
+                    int localVersion = getMaxEndPointStateVersion(localEndPointState);
+                    //int localVersion = localEndPointState.getHeartBeatState().getHeartBeatVersion();
+                    int remoteVersion = remoteEndPointState.getHeartBeatState().getHeartBeatVersion();
+                    if ( remoteVersion > localVersion )
+                    {
+                        fd.report(endpoint);
+                    }
+                }
+            }
+        }
+    }
+
+    void markAlive(EndPoint addr, EndPointState localState)
+    {
+        if (logger_.isTraceEnabled())
+            logger_.trace("marking as alive " + addr);
+        if ( !localState.isAlive() )
+        {
+            isAlive(addr, localState, true);
+            logger_.info("EndPoint " + addr + " is now UP");
+        }
+    }
+
+    private void handleNewJoin(EndPoint ep, EndPointState epState)
+    {
+    	logger_.info("Node " + ep + " has now joined.");
+        /* Mark this endpoint as "live" */
+        endPointStateMap_.put(ep, epState);
+        isAlive(ep, epState, true);
+        /* Notify interested parties about endpoint state change */
+        doNotifications(ep, epState);
+    }
+
+    synchronized void applyStateLocally(Map<EndPoint, EndPointState> epStateMap)
+    {
+        Set<EndPoint> eps = epStateMap.keySet();
+        for( EndPoint ep : eps )
+        {
+            if ( ep.equals( localEndPoint_ ) )
+                continue;
+
+            EndPointState localEpStatePtr = endPointStateMap_.get(ep);
+            EndPointState remoteState = epStateMap.get(ep);
+            /*
+                If state does not exist just add it. If it does then add it only if the version
+                of the remote copy is greater than the local copy.
+            */
+            if ( localEpStatePtr != null )
+            {
+            	int localGeneration = localEpStatePtr.getHeartBeatState().getGeneration();
+            	int remoteGeneration = remoteState.getHeartBeatState().getGeneration();
+
+            	if (remoteGeneration > localGeneration)
+            	{
+            		handleNewJoin(ep, remoteState);
+            	}
+            	else if ( remoteGeneration == localGeneration )
+            	{
+	                /* manage the membership state */
+	                int localMaxVersion = getMaxEndPointStateVersion(localEpStatePtr);
+	                int remoteMaxVersion = getMaxEndPointStateVersion(remoteState);
+	                if ( remoteMaxVersion > localMaxVersion )
+	                {
+	                    markAlive(ep, localEpStatePtr);
+	                    applyHeartBeatStateLocally(ep, localEpStatePtr, remoteState);
+	                    /* apply ApplicationState */
+	                    applyApplicationStateLocally(ep, localEpStatePtr, remoteState);
+	                }
+            	}
+            }
+            else
+            {
+            	handleNewJoin(ep, remoteState);
+            }
+        }
+    }
+
+    void applyHeartBeatStateLocally(EndPoint addr, EndPointState localState, EndPointState remoteState)
+    {
+        HeartBeatState localHbState = localState.getHeartBeatState();
+        HeartBeatState remoteHbState = remoteState.getHeartBeatState();
+
+        if ( remoteHbState.getGeneration() > localHbState.getGeneration() )
+        {
+            markAlive(addr, localState);
+            localState.setHeartBeatState(remoteHbState);
+        }
+        if ( localHbState.getGeneration() == remoteHbState.getGeneration() )
+        {
+            if ( remoteHbState.getHeartBeatVersion() > localHbState.getHeartBeatVersion() )
+            {
+                int oldVersion = localHbState.getHeartBeatVersion();
+                localState.setHeartBeatState(remoteHbState);
+                if (logger_.isTraceEnabled())
+                    logger_.trace("Updating heartbeat state version to " + localState.getHeartBeatState().getHeartBeatVersion() + " from " + oldVersion + " for " + addr + " ...");
+            }
+        }
+    }
+
+    void applyApplicationStateLocally(EndPoint addr, EndPointState localStatePtr, EndPointState remoteStatePtr)
+    {
+        Map<String, ApplicationState> localAppStateMap = localStatePtr.getApplicationState();
+        Map<String, ApplicationState> remoteAppStateMap = remoteStatePtr.getApplicationState();
+
+        Set<String> remoteKeys = remoteAppStateMap.keySet();
+        for ( String remoteKey : remoteKeys )
+        {
+            ApplicationState remoteAppState = remoteAppStateMap.get(remoteKey);
+            ApplicationState localAppState = localAppStateMap.get(remoteKey);
+
+            /* If state doesn't exist locally for this key then just apply it */
+            if ( localAppState == null )
+            {
+                localStatePtr.addApplicationState(remoteKey, remoteAppState);
+                /* notify interested parties of endpoint state change */
+                EndPointState deltaState = new EndPointState(localStatePtr.getHeartBeatState());
+                deltaState.addApplicationState(remoteKey, remoteAppState);
+                doNotifications(addr, deltaState);
+                continue;
+            }
+
+            int remoteGeneration = remoteStatePtr.getHeartBeatState().getGeneration();
+            int localGeneration = localStatePtr.getHeartBeatState().getGeneration();
+
+            /* If the remoteGeneration is greater than localGeneration then apply state blindly */
+            if ( remoteGeneration > localGeneration )
+            {
+                localStatePtr.addApplicationState(remoteKey, remoteAppState);
+                /* notify interested parties of endpoint state change */
+                EndPointState deltaState = new EndPointState(localStatePtr.getHeartBeatState());
+                deltaState.addApplicationState(remoteKey, remoteAppState);
+                doNotifications(addr, deltaState);
+                continue;
+            }
+
+            /* If the generations are the same then apply state if the remote version is greater than local version. */
+            if ( remoteGeneration == localGeneration )
+            {
+                int remoteVersion = remoteAppState.getStateVersion();
+                int localVersion = localAppState.getStateVersion();
+
+                if ( remoteVersion > localVersion )
+                {
+                    localStatePtr.addApplicationState(remoteKey, remoteAppState);
+                    /* notify interested parties of endpoint state change */
+                    EndPointState deltaState = new EndPointState(localStatePtr.getHeartBeatState());
+                    deltaState.addApplicationState(remoteKey, remoteAppState);
+                    doNotifications(addr, deltaState);
+                }
+            }
+        }
+    }
+
+    void doNotifications(EndPoint addr, EndPointState epState)
+    {
+        for ( IEndPointStateChangeSubscriber subscriber : subscribers_ )
+        {
+            subscriber.onChange(addr, epState);
+        }
+    }
+
+    synchronized void isAlive(EndPoint addr, EndPointState epState, boolean value)
+    {
+        epState.isAlive(value);
+        if ( value )
+        {
+            liveEndpoints_.add(addr);
+            unreachableEndpoints_.remove(addr);
+        }
+        else
+        {
+            liveEndpoints_.remove(addr);
+            unreachableEndpoints_.add(addr);
+        }
+        if ( epState.isAGossiper() )
+            return;
+        epState.isAGossiper(true);
+    }
+
+    /* These are helper methods used from GossipDigestSynVerbHandler */
+    Map<EndPoint, GossipDigest> getEndPointGossipDigestMap(List<GossipDigest> gDigestList)
+    {
+        Map<EndPoint, GossipDigest> epMap = new HashMap<EndPoint, GossipDigest>();
+        for( GossipDigest gDigest : gDigestList )
+        {
+            epMap.put( gDigest.getEndPoint(), gDigest );
+        }
+        return epMap;
+    }
+
+    /* This is a helper method to get all EndPoints from a list of GossipDigests */
+    EndPoint[] getEndPointsFromGossipDigest(List<GossipDigest> gDigestList)
+    {
+        Set<EndPoint> set = new HashSet<EndPoint>();
+        for ( GossipDigest gDigest : gDigestList )
+        {
+            set.add( gDigest.getEndPoint() );
+        }
+        return set.toArray( new EndPoint[0] );
+    }
+
+    /* Request all the state for the endpoint in the gDigest */
+    void requestAll(GossipDigest gDigest, List<GossipDigest> deltaGossipDigestList, int remoteGeneration)
+    {
+        /* We are here since we have no data for this endpoint locally so request everthing. */
+        deltaGossipDigestList.add( new GossipDigest(gDigest.getEndPoint(), remoteGeneration, 0) );
+    }
+
+    /* Send all the data with version greater than maxRemoteVersion */
+    void sendAll(GossipDigest gDigest, Map<EndPoint, EndPointState> deltaEpStateMap, int maxRemoteVersion)
+    {
+        EndPointState localEpStatePtr = getStateForVersionBiggerThan(gDigest.getEndPoint(), maxRemoteVersion) ;
+        if ( localEpStatePtr != null )
+            deltaEpStateMap.put(gDigest.getEndPoint(), localEpStatePtr);
+    }
+
+    /*
+        This method is used to figure the state that the Gossiper has but Gossipee doesn't. The delta digests
+        and the delta state are built up.
+    */
+    synchronized void examineGossiper(List<GossipDigest> gDigestList, List<GossipDigest> deltaGossipDigestList, Map<EndPoint, EndPointState> deltaEpStateMap)
+    {
+        for ( GossipDigest gDigest : gDigestList )
+        {
+            int remoteGeneration = gDigest.getGeneration();
+            int maxRemoteVersion = gDigest.getMaxVersion();
+            /* Get state associated with the end point in digest */
+            EndPointState epStatePtr = endPointStateMap_.get(gDigest.getEndPoint());
+            /*
+                Here we need to fire a GossipDigestAckMessage. If we have some data associated with this endpoint locally
+                then we follow the "if" path of the logic. If we have absolutely nothing for this endpoint we need to
+                request all the data for this endpoint.
+            */
+            if ( epStatePtr != null )
+            {
+                int localGeneration = epStatePtr.getHeartBeatState().getGeneration();
+                /* get the max version of all keys in the state associated with this endpoint */
+                int maxLocalVersion = getMaxEndPointStateVersion(epStatePtr);
+                if ( remoteGeneration == localGeneration && maxRemoteVersion == maxLocalVersion )
+                    continue;
+
+                if ( remoteGeneration > localGeneration )
+                {
+                    /* we request everything from the gossiper */
+                    requestAll(gDigest, deltaGossipDigestList, remoteGeneration);
+                }
+                if ( remoteGeneration < localGeneration )
+                {
+                    /* send all data with generation = localgeneration and version > 0 */
+                    sendAll(gDigest, deltaEpStateMap, 0);
+                }
+                if ( remoteGeneration == localGeneration )
+                {
+                    /*
+                        If the max remote version is greater then we request the remote endpoint send us all the data
+                        for this endpoint with version greater than the max version number we have locally for this
+                        endpoint.
+                        If the max remote version is lesser, then we send all the data we have locally for this endpoint
+                        with version greater than the max remote version.
+                    */
+                    if ( maxRemoteVersion > maxLocalVersion )
+                    {
+                        deltaGossipDigestList.add( new GossipDigest(gDigest.getEndPoint(), remoteGeneration, maxLocalVersion) );
+                    }
+                    if ( maxRemoteVersion < maxLocalVersion )
+                    {
+                        /* send all data with generation = localgeneration and version > maxRemoteVersion */
+                        sendAll(gDigest, deltaEpStateMap, maxRemoteVersion);
+                    }
+                }
+            }
+            else
+            {
+                /* We are here since we have no data for this endpoint locally so request everything. */
+                requestAll(gDigest, deltaGossipDigestList, remoteGeneration);
+            }
+        }
+    }
+
+    public void start(EndPoint localEndPoint, int generationNbr) throws IOException
+    {
+        localEndPoint_ = localEndPoint;
+        /* Get the seeds from the config and initialize them. */
+        Set<String> seedHosts = DatabaseDescriptor.getSeeds();
+        for( String seedHost : seedHosts )
+        {
+            EndPoint seed = new EndPoint(InetAddress.getByName(seedHost).getHostAddress(),
+                                         DatabaseDescriptor.getControlPort());
+            if ( seed.equals(localEndPoint) )
+                continue;
+            seeds_.add(seed);
+        }
+
+        /* initialize the heartbeat state for this localEndPoint */
+        EndPointState localState = endPointStateMap_.get(localEndPoint_);
+        if ( localState == null )
+        {
+            HeartBeatState hbState = new HeartBeatState(generationNbr, 0);
+            localState = new EndPointState(hbState);
+            localState.isAlive(true);
+            localState.isAGossiper(true);
+            endPointStateMap_.put(localEndPoint_, localState);
+        }
+
+        /* starts a timer thread */
+        gossipTimer_.schedule( new GossipTimerTask(), Gossiper.intervalInMillis_, Gossiper.intervalInMillis_);
+    }
+
+    public synchronized void addApplicationState(String key, ApplicationState appState)
+    {
+        EndPointState epState = endPointStateMap_.get(localEndPoint_);
+        if ( epState != null )
+        {
+            epState.addApplicationState(key, appState);
+        }
+    }
+
+    public void stop()
+    {
+        gossipTimer_.cancel();
+    }
+}
+
+class JoinVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger( JoinVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+        EndPoint from = message.getFrom();
+        if (logger_.isDebugEnabled())
+          logger_.debug("Received a JoinMessage from " + from);
+
+        byte[] bytes = message.getMessageBody();
+        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
+
+        JoinMessage joinMessage = null;
+        try
+        {
+            joinMessage = JoinMessage.serializer().deserialize(dis);
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+        if ( joinMessage.clusterId_.equals( DatabaseDescriptor.getClusterName() ) )
+        {
+            Gossiper.instance().join(from);
+        }
+    }
+}
+
+class GossipDigestSynVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger( GossipDigestSynVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+        EndPoint from = message.getFrom();
+        if (logger_.isTraceEnabled())
+            logger_.trace("Received a GossipDigestSynMessage from " + from);
+
+        byte[] bytes = message.getMessageBody();
+        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
+
+        try
+        {
+            GossipDigestSynMessage gDigestMessage = GossipDigestSynMessage.serializer().deserialize(dis);
+            /* If the message is from a different cluster throw it away. */
+            if ( !gDigestMessage.clusterId_.equals(DatabaseDescriptor.getClusterName()) )
+                return;
+
+            List<GossipDigest> gDigestList = gDigestMessage.getGossipDigests();
+            /* Notify the Failure Detector */
+            Gossiper.instance().notifyFailureDetector(gDigestList);
+
+            doSort(gDigestList);
+
+            List<GossipDigest> deltaGossipDigestList = new ArrayList<GossipDigest>();
+            Map<EndPoint, EndPointState> deltaEpStateMap = new HashMap<EndPoint, EndPointState>();
+            Gossiper.instance().examineGossiper(gDigestList, deltaGossipDigestList, deltaEpStateMap);
+
+            GossipDigestAckMessage gDigestAck = new GossipDigestAckMessage(deltaGossipDigestList, deltaEpStateMap);
+            Message gDigestAckMessage = Gossiper.instance().makeGossipDigestAckMessage(gDigestAck);
+            if (logger_.isTraceEnabled())
+                logger_.trace("Sending a GossipDigestAckMessage to " + from);
+            MessagingService.getMessagingInstance().sendUdpOneWay(gDigestAckMessage, from);
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    /*
+     * First construct a map whose key is the endpoint in the GossipDigest and the value is the
+     * GossipDigest itself. Then build a list of version differences i.e difference between the
+     * version in the GossipDigest and the version in the local state for a given EndPoint.
+     * Sort this list. Now loop through the sorted list and retrieve the GossipDigest corresponding
+     * to the endpoint from the map that was initially constructed.
+    */
+    private void doSort(List<GossipDigest> gDigestList)
+    {
+        /* Construct a map of endpoint to GossipDigest. */
+        Map<EndPoint, GossipDigest> epToDigestMap = new HashMap<EndPoint, GossipDigest>();
+        for ( GossipDigest gDigest : gDigestList )
+        {
+            epToDigestMap.put(gDigest.getEndPoint(), gDigest);
+        }
+
+        /*
+         * These digests have their maxVersion set to the difference of the version
+         * of the local EndPointState and the version found in the GossipDigest.
+        */
+        List<GossipDigest> diffDigests = new ArrayList<GossipDigest>();
+        for ( GossipDigest gDigest : gDigestList )
+        {
+            EndPoint ep = gDigest.getEndPoint();
+            EndPointState epState = Gossiper.instance().getEndPointStateForEndPoint(ep);
+            int version = (epState != null) ? Gossiper.instance().getMaxEndPointStateVersion( epState ) : 0;
+            int diffVersion = Math.abs(version - gDigest.getMaxVersion() );
+            diffDigests.add( new GossipDigest(ep, gDigest.getGeneration(), diffVersion) );
+        }
+
+        gDigestList.clear();
+        Collections.sort(diffDigests);
+        int size = diffDigests.size();
+        /*
+         * Report the digests in descending order. This takes care of the endpoints
+         * that are far behind w.r.t this local endpoint
+        */
+        for ( int i = size - 1; i >= 0; --i )
+        {
+            gDigestList.add( epToDigestMap.get(diffDigests.get(i).getEndPoint()) );
+        }
+    }
+}
+
+class GossipDigestAckVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(GossipDigestAckVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+        EndPoint from = message.getFrom();
+        if (logger_.isTraceEnabled())
+            logger_.trace("Received a GossipDigestAckMessage from " + from);
+
+        byte[] bytes = message.getMessageBody();
+        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
+
+        try
+        {
+            GossipDigestAckMessage gDigestAckMessage = GossipDigestAckMessage.serializer().deserialize(dis);
+            List<GossipDigest> gDigestList = gDigestAckMessage.getGossipDigestList();
+            Map<EndPoint, EndPointState> epStateMap = gDigestAckMessage.getEndPointStateMap();
+
+            if ( epStateMap.size() > 0 )
+            {
+                /* Notify the Failure Detector */
+                Gossiper.instance().notifyFailureDetector(epStateMap);
+                Gossiper.instance().applyStateLocally(epStateMap);
+            }
+
+            /* Get the state required to send to this gossipee - construct GossipDigestAck2Message */
+            Map<EndPoint, EndPointState> deltaEpStateMap = new HashMap<EndPoint, EndPointState>();
+            for( GossipDigest gDigest : gDigestList )
+            {
+                EndPoint addr = gDigest.getEndPoint();
+                EndPointState localEpStatePtr = Gossiper.instance().getStateForVersionBiggerThan(addr, gDigest.getMaxVersion());
+                if ( localEpStatePtr != null )
+                    deltaEpStateMap.put(addr, localEpStatePtr);
+            }
+
+            GossipDigestAck2Message gDigestAck2 = new GossipDigestAck2Message(deltaEpStateMap);
+            Message gDigestAck2Message = Gossiper.instance().makeGossipDigestAck2Message(gDigestAck2);
+            if (logger_.isTraceEnabled())
+                logger_.trace("Sending a GossipDigestAck2Message to " + from);
+            MessagingService.getMessagingInstance().sendUdpOneWay(gDigestAck2Message, from);
+        }
+        catch ( IOException e )
+        {
+            throw new RuntimeException(e);
+        }
+    }
+}
+
+class GossipDigestAck2VerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(GossipDigestAck2VerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+        EndPoint from = message.getFrom();
+        if (logger_.isTraceEnabled())
+            logger_.trace("Received a GossipDigestAck2Message from " + from);
+
+        byte[] bytes = message.getMessageBody();
+        DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
+        GossipDigestAck2Message gDigestAck2Message = null;
+        try
+        {
+            gDigestAck2Message = GossipDigestAck2Message.serializer().deserialize(dis);
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+        Map<EndPoint, EndPointState> remoteEpStateMap = gDigestAck2Message.getEndPointStateMap();
+        /* Notify the Failure Detector */
+        Gossiper.instance().notifyFailureDetector(remoteEpStateMap);
+        Gossiper.instance().applyStateLocally(remoteEpStateMap);
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/gms/HeartBeatState.java b/src/java/org/apache/cassandra/gms/HeartBeatState.java
index 5e4e512f42..b99720d33f 100644
--- a/src/java/org/apache/cassandra/gms/HeartBeatState.java
+++ b/src/java/org/apache/cassandra/gms/HeartBeatState.java
@@ -1,109 +1,109 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.util.concurrent.atomic.AtomicInteger;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.cassandra.io.ICompactSerializer;
-
-
-/**
- * HeartBeat State associated with any given endpoint. 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class HeartBeatState
-{
-    private static ICompactSerializer<HeartBeatState> serializer_;
-    
-    static
-    {
-        serializer_ = new HeartBeatStateSerializer();
-    }
-    
-    int generation_;
-    AtomicInteger heartbeat_;
-    int version_;
-
-    HeartBeatState()
-    {
-    }
-    
-    HeartBeatState(int generation, int heartbeat)
-    {
-        this(generation, heartbeat, 0);
-    }
-    
-    HeartBeatState(int generation, int heartbeat, int version)
-    {
-        generation_ = generation;
-        heartbeat_ = new AtomicInteger(heartbeat);
-        version_ = version;
-    }
-
-    public static ICompactSerializer<HeartBeatState> serializer()
-    {
-        return serializer_;
-    }
-    
-    int getGeneration()
-    {
-        return generation_;
-    }
-    
-    void updateGeneration()
-    {
-        ++generation_;
-        version_ = VersionGenerator.getNextVersion();
-    }
-    
-    int getHeartBeat()
-    {
-        return heartbeat_.get();
-    }
-    
-    void updateHeartBeat()
-    {
-        heartbeat_.incrementAndGet();      
-        version_ = VersionGenerator.getNextVersion();
-    }
-    
-    int getHeartBeatVersion()
-    {
-        return version_;
-    }
-};
-
-class HeartBeatStateSerializer implements ICompactSerializer<HeartBeatState>
-{
-    public void serialize(HeartBeatState hbState, DataOutputStream dos) throws IOException
-    {
-        dos.writeInt(hbState.generation_);
-        dos.writeInt(hbState.heartbeat_.get());
-        dos.writeInt(hbState.version_);
-    }
-    
-    public HeartBeatState deserialize(DataInputStream dis) throws IOException
-    {
-        return new HeartBeatState(dis.readInt(), dis.readInt(), dis.readInt());
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.util.concurrent.atomic.AtomicInteger;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.io.ICompactSerializer;
+
+
+/**
+ * HeartBeat State associated with any given endpoint. 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class HeartBeatState
+{
+    private static ICompactSerializer<HeartBeatState> serializer_;
+    
+    static
+    {
+        serializer_ = new HeartBeatStateSerializer();
+    }
+    
+    int generation_;
+    AtomicInteger heartbeat_;
+    int version_;
+
+    HeartBeatState()
+    {
+    }
+    
+    HeartBeatState(int generation, int heartbeat)
+    {
+        this(generation, heartbeat, 0);
+    }
+    
+    HeartBeatState(int generation, int heartbeat, int version)
+    {
+        generation_ = generation;
+        heartbeat_ = new AtomicInteger(heartbeat);
+        version_ = version;
+    }
+
+    public static ICompactSerializer<HeartBeatState> serializer()
+    {
+        return serializer_;
+    }
+    
+    int getGeneration()
+    {
+        return generation_;
+    }
+    
+    void updateGeneration()
+    {
+        ++generation_;
+        version_ = VersionGenerator.getNextVersion();
+    }
+    
+    int getHeartBeat()
+    {
+        return heartbeat_.get();
+    }
+    
+    void updateHeartBeat()
+    {
+        heartbeat_.incrementAndGet();      
+        version_ = VersionGenerator.getNextVersion();
+    }
+    
+    int getHeartBeatVersion()
+    {
+        return version_;
+    }
+};
+
+class HeartBeatStateSerializer implements ICompactSerializer<HeartBeatState>
+{
+    public void serialize(HeartBeatState hbState, DataOutputStream dos) throws IOException
+    {
+        dos.writeInt(hbState.generation_);
+        dos.writeInt(hbState.heartbeat_.get());
+        dos.writeInt(hbState.version_);
+    }
+    
+    public HeartBeatState deserialize(DataInputStream dis) throws IOException
+    {
+        return new HeartBeatState(dis.readInt(), dis.readInt(), dis.readInt());
+    }
+}
diff --git a/src/java/org/apache/cassandra/gms/IEndPointStateChangePublisher.java b/src/java/org/apache/cassandra/gms/IEndPointStateChangePublisher.java
index 5e36c76937..d456300ee3 100644
--- a/src/java/org/apache/cassandra/gms/IEndPointStateChangePublisher.java
+++ b/src/java/org/apache/cassandra/gms/IEndPointStateChangePublisher.java
@@ -1,41 +1,41 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-/**
- * This is implemented by the Gossiper module to publish change events to interested parties.
- * Interested parties register/unregister interest by invoking the methods of this interface.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IEndPointStateChangePublisher
-{
-    /**
-     * Register for interesting state changes.
-     * @param subcriber module which implements the IEndPointStateChangeSubscriber
-     */
-    public void register(IEndPointStateChangeSubscriber subcriber);
-    
-    /**
-     * Unregister interest for state changes.
-     * @param subcriber module which implements the IEndPointStateChangeSubscriber
-     */
-    public void unregister(IEndPointStateChangeSubscriber subcriber);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+/**
+ * This is implemented by the Gossiper module to publish change events to interested parties.
+ * Interested parties register/unregister interest by invoking the methods of this interface.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IEndPointStateChangePublisher
+{
+    /**
+     * Register for interesting state changes.
+     * @param subcriber module which implements the IEndPointStateChangeSubscriber
+     */
+    public void register(IEndPointStateChangeSubscriber subcriber);
+    
+    /**
+     * Unregister interest for state changes.
+     * @param subcriber module which implements the IEndPointStateChangeSubscriber
+     */
+    public void unregister(IEndPointStateChangeSubscriber subcriber);
+}
diff --git a/src/java/org/apache/cassandra/gms/IEndPointStateChangeSubscriber.java b/src/java/org/apache/cassandra/gms/IEndPointStateChangeSubscriber.java
index 313dce31aa..2e0a12b210 100644
--- a/src/java/org/apache/cassandra/gms/IEndPointStateChangeSubscriber.java
+++ b/src/java/org/apache/cassandra/gms/IEndPointStateChangeSubscriber.java
@@ -1,44 +1,44 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import org.apache.cassandra.net.EndPoint;
-
-/**
- * This is called by an instance of the IEndPointStateChangePublisher to notify
- * interested parties about changes in the the state associated with any endpoint.
- * For instance if node A figures there is a changes in state for an endpoint B
- * it notifies all interested parties of this change. It is upto to the registered
- * instance to decide what he does with this change. Not all modules maybe interested 
- * in all state changes.
- *  
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IEndPointStateChangeSubscriber
-{
-    /**
-     * Use to inform interested parties about the change in the state
-     * for specified endpoint
-     * 
-     * @param endpoint endpoint for which the state change occurred.
-     * @param epState state that actually changed for the above endpoint.
-     */
-    public void onChange(EndPoint endpoint, EndPointState epState);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * This is called by an instance of the IEndPointStateChangePublisher to notify
+ * interested parties about changes in the the state associated with any endpoint.
+ * For instance if node A figures there is a changes in state for an endpoint B
+ * it notifies all interested parties of this change. It is upto to the registered
+ * instance to decide what he does with this change. Not all modules maybe interested 
+ * in all state changes.
+ *  
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IEndPointStateChangeSubscriber
+{
+    /**
+     * Use to inform interested parties about the change in the state
+     * for specified endpoint
+     * 
+     * @param endpoint endpoint for which the state change occurred.
+     * @param epState state that actually changed for the above endpoint.
+     */
+    public void onChange(EndPoint endpoint, EndPointState epState);
+}
diff --git a/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java b/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java
index 11b663532d..1cfed5adea 100644
--- a/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java
+++ b/src/java/org/apache/cassandra/gms/IFailureDetectionEventListener.java
@@ -1,44 +1,44 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import org.apache.cassandra.net.EndPoint;
-
-/**
- * Implemented by the Gossiper to either convict/suspect an endpoint
- * based on the PHI calculated by the Failure Detector on the inter-arrival
- * times of the heart beats.
- *  
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IFailureDetectionEventListener
-{  
-    /**
-     * Convict the specified endpoint.
-     * @param ep endpoint to be convicted
-     */
-    public void convict(EndPoint ep);
-    
-    /**
-     * Suspect the specified endpoint.
-     * @param ep endpoint to be suspected.
-     */
-    public void suspect(EndPoint ep);    
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * Implemented by the Gossiper to either convict/suspect an endpoint
+ * based on the PHI calculated by the Failure Detector on the inter-arrival
+ * times of the heart beats.
+ *  
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IFailureDetectionEventListener
+{  
+    /**
+     * Convict the specified endpoint.
+     * @param ep endpoint to be convicted
+     */
+    public void convict(EndPoint ep);
+    
+    /**
+     * Suspect the specified endpoint.
+     * @param ep endpoint to be suspected.
+     */
+    public void suspect(EndPoint ep);    
+}
diff --git a/src/java/org/apache/cassandra/gms/IFailureDetector.java b/src/java/org/apache/cassandra/gms/IFailureDetector.java
index 2d9d744fe8..2e88c63513 100644
--- a/src/java/org/apache/cassandra/gms/IFailureDetector.java
+++ b/src/java/org/apache/cassandra/gms/IFailureDetector.java
@@ -1,72 +1,72 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import org.apache.cassandra.net.EndPoint;
-
-/**
- * An interface that provides an application with the ability
- * to query liveness information of a node in the cluster. It 
- * also exposes methods which help an application register callbacks
- * for notifications of liveness information of nodes.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IFailureDetector
-{
-    /**
-     * Failure Detector's knowledge of whether a node is up or
-     * down.
-     * 
-     * @param ep endpoint in question.
-     * @return true if UP and false if DOWN.
-     */
-    public boolean isAlive(EndPoint ep);
-    
-    /**
-     * This method is invoked by any entity wanting to interrogate the status of an endpoint. 
-     * In our case it would be the Gossiper. The Failure Detector will then calculate Phi and
-     * deem an endpoint as suspicious or alive as explained in the Hayashibara paper. 
-     * 
-     * param ep endpoint for which we interpret the inter arrival times.
-    */
-    public void interpret(EndPoint ep);
-    
-    /**
-     * This method is invoked by the receiver of the heartbeat. In our case it would be
-     * the Gossiper. Gossiper inform the Failure Detector on receipt of a heartbeat. The
-     * FailureDetector will then sample the arrival time as explained in the paper.
-     * 
-     * param ep endpoint being reported.
-    */
-    public void report(EndPoint ep);
-    
-    /**
-     * Register interest for Failure Detector events. 
-     * @param listener implementation of an application provided IFailureDetectionEventListener 
-     */
-    public void registerFailureDetectionEventListener(IFailureDetectionEventListener listener);
-    
-    /**
-     * Un-register interest for Failure Detector events. 
-     * @param listener implementation of an application provided IFailureDetectionEventListener 
-     */
-    public void unregisterFailureDetectionEventListener(IFailureDetectionEventListener listener);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * An interface that provides an application with the ability
+ * to query liveness information of a node in the cluster. It 
+ * also exposes methods which help an application register callbacks
+ * for notifications of liveness information of nodes.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IFailureDetector
+{
+    /**
+     * Failure Detector's knowledge of whether a node is up or
+     * down.
+     * 
+     * @param ep endpoint in question.
+     * @return true if UP and false if DOWN.
+     */
+    public boolean isAlive(EndPoint ep);
+    
+    /**
+     * This method is invoked by any entity wanting to interrogate the status of an endpoint. 
+     * In our case it would be the Gossiper. The Failure Detector will then calculate Phi and
+     * deem an endpoint as suspicious or alive as explained in the Hayashibara paper. 
+     * 
+     * param ep endpoint for which we interpret the inter arrival times.
+    */
+    public void interpret(EndPoint ep);
+    
+    /**
+     * This method is invoked by the receiver of the heartbeat. In our case it would be
+     * the Gossiper. Gossiper inform the Failure Detector on receipt of a heartbeat. The
+     * FailureDetector will then sample the arrival time as explained in the paper.
+     * 
+     * param ep endpoint being reported.
+    */
+    public void report(EndPoint ep);
+    
+    /**
+     * Register interest for Failure Detector events. 
+     * @param listener implementation of an application provided IFailureDetectionEventListener 
+     */
+    public void registerFailureDetectionEventListener(IFailureDetectionEventListener listener);
+    
+    /**
+     * Un-register interest for Failure Detector events. 
+     * @param listener implementation of an application provided IFailureDetectionEventListener 
+     */
+    public void unregisterFailureDetectionEventListener(IFailureDetectionEventListener listener);
+}
diff --git a/src/java/org/apache/cassandra/gms/IFailureNotification.java b/src/java/org/apache/cassandra/gms/IFailureNotification.java
index 78caeb4340..a27edd36de 100644
--- a/src/java/org/apache/cassandra/gms/IFailureNotification.java
+++ b/src/java/org/apache/cassandra/gms/IFailureNotification.java
@@ -1,31 +1,31 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import org.apache.cassandra.net.EndPoint;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IFailureNotification
-{   
-    public void suspect(EndPoint ep);
-    public void revive(EndPoint ep);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IFailureNotification
+{   
+    public void suspect(EndPoint ep);
+    public void revive(EndPoint ep);
+}
diff --git a/src/java/org/apache/cassandra/gms/JoinMessage.java b/src/java/org/apache/cassandra/gms/JoinMessage.java
index b847c4ac35..c8f12048e4 100644
--- a/src/java/org/apache/cassandra/gms/JoinMessage.java
+++ b/src/java/org/apache/cassandra/gms/JoinMessage.java
@@ -1,66 +1,66 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.List;
-
-import org.apache.cassandra.io.ICompactSerializer;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class JoinMessage
-{
-    private static ICompactSerializer<JoinMessage> serializer_;
-    static
-    {
-        serializer_ = new JoinMessageSerializer();
-    }
-    
-    static ICompactSerializer<JoinMessage> serializer()
-    {
-        return serializer_;
-    }
-    
-    String clusterId_;
-    
-    JoinMessage(String clusterId)
-    {
-        clusterId_ = clusterId;
-    }
-}
-
-class JoinMessageSerializer implements ICompactSerializer<JoinMessage>
-{
-    public void serialize(JoinMessage joinMessage, DataOutputStream dos) throws IOException
-    {    
-        dos.writeUTF(joinMessage.clusterId_);         
-    }
-
-    public JoinMessage deserialize(DataInputStream dis) throws IOException
-    {
-        String clusterId = dis.readUTF();
-        return new JoinMessage(clusterId);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.List;
+
+import org.apache.cassandra.io.ICompactSerializer;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class JoinMessage
+{
+    private static ICompactSerializer<JoinMessage> serializer_;
+    static
+    {
+        serializer_ = new JoinMessageSerializer();
+    }
+    
+    static ICompactSerializer<JoinMessage> serializer()
+    {
+        return serializer_;
+    }
+    
+    String clusterId_;
+    
+    JoinMessage(String clusterId)
+    {
+        clusterId_ = clusterId;
+    }
+}
+
+class JoinMessageSerializer implements ICompactSerializer<JoinMessage>
+{
+    public void serialize(JoinMessage joinMessage, DataOutputStream dos) throws IOException
+    {    
+        dos.writeUTF(joinMessage.clusterId_);         
+    }
+
+    public JoinMessage deserialize(DataInputStream dis) throws IOException
+    {
+        String clusterId = dis.readUTF();
+        return new JoinMessage(clusterId);
+    }
+}
diff --git a/src/java/org/apache/cassandra/gms/PureRandom.java b/src/java/org/apache/cassandra/gms/PureRandom.java
index 5882efc837..a8fcf5b326 100644
--- a/src/java/org/apache/cassandra/gms/PureRandom.java
+++ b/src/java/org/apache/cassandra/gms/PureRandom.java
@@ -1,83 +1,83 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.util.Random;
-import java.util.BitSet;
-
-
-/**
- * Implementation of a PureRandomNumber generator. Use this class cautiously. Not
- * for general purpose use. Currently this is used by the Gossiper to choose a random
- * endpoint to Gossip to.
- *
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class PureRandom extends Random
-{
-    private BitSet bs_ = new BitSet();
-    private int lastUb_;
-
-    PureRandom()
-    {
-        super();
-    }
-
-    public int nextInt(int ub)
-    {
-    	if (ub <= 0)
-    		throw new IllegalArgumentException("ub must be positive");
-
-        if ( lastUb_ !=  ub )
-        {
-            bs_.clear();
-            lastUb_ = ub;
-        }
-        else if(bs_.cardinality() == ub)
-        {
-        	bs_.clear();
-        }
-
-        int value = super.nextInt(ub);
-        while ( bs_.get(value) )
-        {
-            value = super.nextInt(ub);
-        }
-        bs_.set(value);
-        return value;
-    }
-
-    public static void main(String[] args) throws Throwable
-    {
-    	Random pr = new PureRandom();
-        int ubs[] = new int[] { 2, 3, 1, 10, 5, 0};
-
-        for (int ub : ubs)
-        {
-            System.out.println("UB: " + String.valueOf(ub));
-            for (int j = 0; j < 10; j++)
-            {
-                int junk = pr.nextInt(ub);
-                // Do something with junk so JVM doesn't optimize away
-                System.out.println(junk);
-            }
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.util.Random;
+import java.util.BitSet;
+
+
+/**
+ * Implementation of a PureRandomNumber generator. Use this class cautiously. Not
+ * for general purpose use. Currently this is used by the Gossiper to choose a random
+ * endpoint to Gossip to.
+ *
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class PureRandom extends Random
+{
+    private BitSet bs_ = new BitSet();
+    private int lastUb_;
+
+    PureRandom()
+    {
+        super();
+    }
+
+    public int nextInt(int ub)
+    {
+    	if (ub <= 0)
+    		throw new IllegalArgumentException("ub must be positive");
+
+        if ( lastUb_ !=  ub )
+        {
+            bs_.clear();
+            lastUb_ = ub;
+        }
+        else if(bs_.cardinality() == ub)
+        {
+        	bs_.clear();
+        }
+
+        int value = super.nextInt(ub);
+        while ( bs_.get(value) )
+        {
+            value = super.nextInt(ub);
+        }
+        bs_.set(value);
+        return value;
+    }
+
+    public static void main(String[] args) throws Throwable
+    {
+    	Random pr = new PureRandom();
+        int ubs[] = new int[] { 2, 3, 1, 10, 5, 0};
+
+        for (int ub : ubs)
+        {
+            System.out.println("UB: " + String.valueOf(ub));
+            for (int j = 0; j < 10; j++)
+            {
+                int junk = pr.nextInt(ub);
+                // Do something with junk so JVM doesn't optimize away
+                System.out.println(junk);
+            }
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/gms/VersionGenerator.java b/src/java/org/apache/cassandra/gms/VersionGenerator.java
index 3c1d6756a3..d772da81c4 100644
--- a/src/java/org/apache/cassandra/gms/VersionGenerator.java
+++ b/src/java/org/apache/cassandra/gms/VersionGenerator.java
@@ -1,37 +1,37 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.gms;
-
-import java.util.concurrent.atomic.AtomicInteger;
-
-/**
- * A unique version number generator for any state that is generated by the 
- * local node.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class VersionGenerator
-{
-    private static AtomicInteger version_ = new AtomicInteger(0);
-    
-    public static int getNextVersion()
-    {
-        return version_.incrementAndGet();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.gms;
+
+import java.util.concurrent.atomic.AtomicInteger;
+
+/**
+ * A unique version number generator for any state that is generated by the 
+ * local node.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class VersionGenerator
+{
+    private static AtomicInteger version_ = new AtomicInteger(0);
+    
+    public static int getNextVersion()
+    {
+        return version_.incrementAndGet();
+    }
+}
diff --git a/src/java/org/apache/cassandra/io/BufferedRandomAccessFile.java b/src/java/org/apache/cassandra/io/BufferedRandomAccessFile.java
index 38d66273e4..43d08816c0 100644
--- a/src/java/org/apache/cassandra/io/BufferedRandomAccessFile.java
+++ b/src/java/org/apache/cassandra/io/BufferedRandomAccessFile.java
@@ -1,375 +1,375 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.File;
-import java.io.FileNotFoundException;
-import java.io.IOException;
-import java.io.RandomAccessFile;
-import java.util.Arrays;
-
-/**
- * A <code>BufferedRandomAccessFile</code> is like a
- * <code>RandomAccessFile</code>, but it uses a private buffer so that most
- * operations do not require a disk access.
- * <P>
- * 
- * Note: The operations on this class are unmonitored. Also, the correct
- * functioning of the <code>RandomAccessFile</code> methods that are not
- * overridden here relies on the implementation of those methods in the
- * superclass.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public final class BufferedRandomAccessFile extends RandomAccessFile
-{
-    static final int LogBuffSz_ = 16; // 64K buffer
-    public static final int BuffSz_ = (1 << LogBuffSz_);
-    static final long BuffMask_ = ~(((long) BuffSz_) - 1L);
-    
-    /*
-     * This implementation is based on the buffer implementation in Modula-3's
-     * "Rd", "Wr", "RdClass", and "WrClass" interfaces.
-     */
-    private boolean dirty_; // true iff unflushed bytes exist
-    private long curr_; // current position in file
-    private long lo_, hi_; // bounds on characters in "buff"
-    private byte[] buff_; // local buffer
-    private long maxHi_; // this.lo + this.buff.length
-    private boolean hitEOF_; // buffer contains last file block?
-    private long diskPos_; // disk position
-    
-    /*
-     * To describe the above fields, we introduce the following abstractions for
-     * the file "f":
-     * 
-     * len(f) the length of the file curr(f) the current position in the file
-     * c(f) the abstract contents of the file disk(f) the contents of f's
-     * backing disk file closed(f) true iff the file is closed
-     * 
-     * "curr(f)" is an index in the closed interval [0, len(f)]. "c(f)" is a
-     * character sequence of length "len(f)". "c(f)" and "disk(f)" may differ if
-     * "c(f)" contains unflushed writes not reflected in "disk(f)". The flush
-     * operation has the effect of making "disk(f)" identical to "c(f)".
-     * 
-     * A file is said to be *valid* if the following conditions hold:
-     * 
-     * V1. The "closed" and "curr" fields are correct:
-     * 
-     * f.closed == closed(f) f.curr == curr(f)
-     * 
-     * V2. The current position is either contained in the buffer, or just past
-     * the buffer:
-     * 
-     * f.lo <= f.curr <= f.hi
-     * 
-     * V3. Any (possibly) unflushed characters are stored in "f.buff":
-     * 
-     * (forall i in [f.lo, f.curr): c(f)[i] == f.buff[i - f.lo])
-     * 
-     * V4. For all characters not covered by V3, c(f) and disk(f) agree:
-     * 
-     * (forall i in [f.lo, len(f)): i not in [f.lo, f.curr) => c(f)[i] ==
-     * disk(f)[i])
-     * 
-     * V5. "f.dirty" is true iff the buffer contains bytes that should be
-     * flushed to the file; by V3 and V4, only part of the buffer can be dirty.
-     * 
-     * f.dirty == (exists i in [f.lo, f.curr): c(f)[i] != f.buff[i - f.lo])
-     * 
-     * V6. this.maxHi == this.lo + this.buff.length
-     * 
-     * Note that "f.buff" can be "null" in a valid file, since the range of
-     * characters in V3 is empty when "f.lo == f.curr".
-     * 
-     * A file is said to be *ready* if the buffer contains the current position,
-     * i.e., when:
-     * 
-     * R1. !f.closed && f.buff != null && f.lo <= f.curr && f.curr < f.hi
-     * 
-     * When a file is ready, reading or writing a single byte can be performed
-     * by reading or writing the in-memory buffer without performing a disk
-     * operation.
-     */
-    
-    /**
-     * Open a new <code>BufferedRandomAccessFile</code> on <code>file</code>
-     * in mode <code>mode</code>, which should be "r" for reading only, or
-     * "rw" for reading and writing.
-     */
-    public BufferedRandomAccessFile(File file, String mode) throws IOException
-    {
-        super(file, mode);
-        this.init(0);
-    }
-    
-    public BufferedRandomAccessFile(File file, String mode, int size) throws IOException
-    {
-        super(file, mode);
-        this.init(size);
-    }
-    
-    /**
-     * Open a new <code>BufferedRandomAccessFile</code> on the file named
-     * <code>name</code> in mode <code>mode</code>, which should be "r" for
-     * reading only, or "rw" for reading and writing.
-     */
-    public BufferedRandomAccessFile(String name, String mode) throws IOException
-    {
-        super(name, mode);
-        this.init(0);
-    }
-    
-    public BufferedRandomAccessFile(String name, String mode, int size) throws FileNotFoundException
-    {
-        super(name, mode);
-        this.init(size);
-    }
-    
-    private void init(int size)
-    {
-        this.dirty_ = false;
-        this.lo_ = this.curr_ = this.hi_ = 0;
-        this.buff_ = (size > BuffSz_) ? new byte[size] : new byte[BuffSz_];
-        this.maxHi_ = (long) BuffSz_;
-        this.hitEOF_ = false;
-        this.diskPos_ = 0L;
-    }
-    
-    public void close() throws IOException
-    {
-        this.flush();
-        this.buff_ = null;
-        super.close();
-    }
-    
-    /**
-     * Flush any bytes in the file's buffer that have not yet been written to
-     * disk. If the file was created read-only, this method is a no-op.
-     */
-    public void flush() throws IOException
-    {        
-        this.flushBuffer();
-    }
-    
-    /* Flush any dirty bytes in the buffer to disk. */
-    private void flushBuffer() throws IOException
-    {   
-        if (this.dirty_)
-        {
-            if (this.diskPos_ != this.lo_)
-                super.seek(this.lo_);
-            int len = (int) (this.curr_ - this.lo_);
-            super.write(this.buff_, 0, len);
-            this.diskPos_ = this.curr_;             
-            this.dirty_ = false;
-        }
-    }
-    
-    /*
-     * Read at most "this.buff.length" bytes into "this.buff", returning the
-     * number of bytes read. If the return result is less than
-     * "this.buff.length", then EOF was read.
-     */
-    private int fillBuffer() throws IOException
-    {
-        int cnt = 0;
-        int rem = this.buff_.length;
-        while (rem > 0)
-        {
-            int n = super.read(this.buff_, cnt, rem);
-            if (n < 0)
-                break;
-            cnt += n;
-            rem -= n;
-        }
-        if ( (cnt < 0) && (this.hitEOF_ = (cnt < this.buff_.length)) )
-        {
-            // make sure buffer that wasn't read is initialized with -1
-            Arrays.fill(this.buff_, cnt, this.buff_.length, (byte) 0xff);
-        }
-        this.diskPos_ += cnt;
-        return cnt;
-    }
-    
-    /*
-     * This method positions <code>this.curr</code> at position <code>pos</code>.
-     * If <code>pos</code> does not fall in the current buffer, it flushes the
-     * current buffer and loads the correct one.<p>
-     * 
-     * On exit from this routine <code>this.curr == this.hi</code> iff <code>pos</code>
-     * is at or past the end-of-file, which can only happen if the file was
-     * opened in read-only mode.
-     */
-    public void seek(long pos) throws IOException
-    {
-        if (pos >= this.hi_ || pos < this.lo_)
-        {
-            // seeking outside of current buffer -- flush and read             
-            this.flushBuffer();
-            this.lo_ = pos & BuffMask_; // start at BuffSz boundary
-            this.maxHi_ = this.lo_ + (long) this.buff_.length;
-            if (this.diskPos_ != this.lo_)
-            {
-                super.seek(this.lo_);
-                this.diskPos_ = this.lo_;
-            }
-            int n = this.fillBuffer();
-            this.hi_ = this.lo_ + (long) n;
-        }
-        else
-        {
-            // seeking inside current buffer -- no read required
-            if (pos < this.curr_)
-            {
-                // if seeking backwards, we must flush to maintain V4
-                this.flushBuffer();
-            }
-        }
-        this.curr_ = pos;
-    }
-    
-    public long getFilePointer()
-    {
-        return this.curr_;
-    }
-
-    public long length() throws IOException
-    {
-        // max accounts for the case where we have written past the old file length, but not yet flushed our buffer
-        return Math.max(this.curr_, super.length());
-    }
-
-    public int read() throws IOException
-    {
-        if (this.curr_ >= this.hi_)
-        {
-            // test for EOF
-            // if (this.hi < this.maxHi) return -1;
-            if (this.hitEOF_)
-                return -1;
-            
-            // slow path -- read another buffer
-            this.seek(this.curr_);
-            if (this.curr_ == this.hi_)
-                return -1;
-        }
-        byte res = this.buff_[(int) (this.curr_ - this.lo_)];
-        this.curr_++;
-        return ((int) res) & 0xFF; // convert byte -> int
-    }
-    
-    public int read(byte[] b) throws IOException
-    {
-        return this.read(b, 0, b.length);
-    }
-    
-    public int read(byte[] b, int off, int len) throws IOException
-    {
-        if (this.curr_ >= this.hi_)
-        {
-            // test for EOF
-            // if (this.hi < this.maxHi) return -1;
-            if (this.hitEOF_)
-                return -1;
-            
-            // slow path -- read another buffer
-            this.seek(this.curr_);
-            if (this.curr_ == this.hi_)
-                return -1;
-        }
-        len = Math.min(len, (int) (this.hi_ - this.curr_));
-        int buffOff = (int) (this.curr_ - this.lo_);
-        System.arraycopy(this.buff_, buffOff, b, off, len);
-        this.curr_ += len;
-        return len;
-    }
-    
-    public void write(int b) throws IOException
-    {
-        if (this.curr_ >= this.hi_)
-        {
-            if (this.hitEOF_ && this.hi_ < this.maxHi_)
-            {
-                // at EOF -- bump "hi"
-                this.hi_++;
-            }
-            else
-            {
-                // slow path -- write current buffer; read next one
-                this.seek(this.curr_);
-                if (this.curr_ == this.hi_)
-                {
-                    // appending to EOF -- bump "hi"
-                    this.hi_++;
-                }
-            }
-        }
-        this.buff_[(int) (this.curr_ - this.lo_)] = (byte) b;
-        this.curr_++;
-        this.dirty_ = true;
-    }
-    
-    public void write(byte[] b) throws IOException
-    {
-        this.write(b, 0, b.length);
-    }
-    
-    public void write(byte[] b, int off, int len) throws IOException
-    {        
-        while (len > 0)
-        {              
-            int n = this.writeAtMost(b, off, len);
-            off += n;
-            len -= n;
-            this.dirty_ = true;
-        }        
-    }
-    
-    /*
-     * Write at most "len" bytes to "b" starting at position "off", and return
-     * the number of bytes written.
-     */
-    private int writeAtMost(byte[] b, int off, int len) throws IOException
-    {        
-        if (this.curr_ >= this.hi_)
-        {
-            if (this.hitEOF_ && this.hi_ < this.maxHi_)
-            {
-                // at EOF -- bump "hi"
-                this.hi_ = this.maxHi_;
-            }
-            else
-            {                                
-                // slow path -- write current buffer; read next one                
-                this.seek(this.curr_);
-                if (this.curr_ == this.hi_)
-                {
-                    // appending to EOF -- bump "hi"
-                    this.hi_ = this.maxHi_;
-                }
-            }
-        }
-        len = Math.min(len, (int) (this.hi_ - this.curr_));
-        int buffOff = (int) (this.curr_ - this.lo_);
-        System.arraycopy(b, off, this.buff_, buffOff, len);
-        this.curr_ += len;
-        return len;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.File;
+import java.io.FileNotFoundException;
+import java.io.IOException;
+import java.io.RandomAccessFile;
+import java.util.Arrays;
+
+/**
+ * A <code>BufferedRandomAccessFile</code> is like a
+ * <code>RandomAccessFile</code>, but it uses a private buffer so that most
+ * operations do not require a disk access.
+ * <P>
+ * 
+ * Note: The operations on this class are unmonitored. Also, the correct
+ * functioning of the <code>RandomAccessFile</code> methods that are not
+ * overridden here relies on the implementation of those methods in the
+ * superclass.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class BufferedRandomAccessFile extends RandomAccessFile
+{
+    static final int LogBuffSz_ = 16; // 64K buffer
+    public static final int BuffSz_ = (1 << LogBuffSz_);
+    static final long BuffMask_ = ~(((long) BuffSz_) - 1L);
+    
+    /*
+     * This implementation is based on the buffer implementation in Modula-3's
+     * "Rd", "Wr", "RdClass", and "WrClass" interfaces.
+     */
+    private boolean dirty_; // true iff unflushed bytes exist
+    private long curr_; // current position in file
+    private long lo_, hi_; // bounds on characters in "buff"
+    private byte[] buff_; // local buffer
+    private long maxHi_; // this.lo + this.buff.length
+    private boolean hitEOF_; // buffer contains last file block?
+    private long diskPos_; // disk position
+    
+    /*
+     * To describe the above fields, we introduce the following abstractions for
+     * the file "f":
+     * 
+     * len(f) the length of the file curr(f) the current position in the file
+     * c(f) the abstract contents of the file disk(f) the contents of f's
+     * backing disk file closed(f) true iff the file is closed
+     * 
+     * "curr(f)" is an index in the closed interval [0, len(f)]. "c(f)" is a
+     * character sequence of length "len(f)". "c(f)" and "disk(f)" may differ if
+     * "c(f)" contains unflushed writes not reflected in "disk(f)". The flush
+     * operation has the effect of making "disk(f)" identical to "c(f)".
+     * 
+     * A file is said to be *valid* if the following conditions hold:
+     * 
+     * V1. The "closed" and "curr" fields are correct:
+     * 
+     * f.closed == closed(f) f.curr == curr(f)
+     * 
+     * V2. The current position is either contained in the buffer, or just past
+     * the buffer:
+     * 
+     * f.lo <= f.curr <= f.hi
+     * 
+     * V3. Any (possibly) unflushed characters are stored in "f.buff":
+     * 
+     * (forall i in [f.lo, f.curr): c(f)[i] == f.buff[i - f.lo])
+     * 
+     * V4. For all characters not covered by V3, c(f) and disk(f) agree:
+     * 
+     * (forall i in [f.lo, len(f)): i not in [f.lo, f.curr) => c(f)[i] ==
+     * disk(f)[i])
+     * 
+     * V5. "f.dirty" is true iff the buffer contains bytes that should be
+     * flushed to the file; by V3 and V4, only part of the buffer can be dirty.
+     * 
+     * f.dirty == (exists i in [f.lo, f.curr): c(f)[i] != f.buff[i - f.lo])
+     * 
+     * V6. this.maxHi == this.lo + this.buff.length
+     * 
+     * Note that "f.buff" can be "null" in a valid file, since the range of
+     * characters in V3 is empty when "f.lo == f.curr".
+     * 
+     * A file is said to be *ready* if the buffer contains the current position,
+     * i.e., when:
+     * 
+     * R1. !f.closed && f.buff != null && f.lo <= f.curr && f.curr < f.hi
+     * 
+     * When a file is ready, reading or writing a single byte can be performed
+     * by reading or writing the in-memory buffer without performing a disk
+     * operation.
+     */
+    
+    /**
+     * Open a new <code>BufferedRandomAccessFile</code> on <code>file</code>
+     * in mode <code>mode</code>, which should be "r" for reading only, or
+     * "rw" for reading and writing.
+     */
+    public BufferedRandomAccessFile(File file, String mode) throws IOException
+    {
+        super(file, mode);
+        this.init(0);
+    }
+    
+    public BufferedRandomAccessFile(File file, String mode, int size) throws IOException
+    {
+        super(file, mode);
+        this.init(size);
+    }
+    
+    /**
+     * Open a new <code>BufferedRandomAccessFile</code> on the file named
+     * <code>name</code> in mode <code>mode</code>, which should be "r" for
+     * reading only, or "rw" for reading and writing.
+     */
+    public BufferedRandomAccessFile(String name, String mode) throws IOException
+    {
+        super(name, mode);
+        this.init(0);
+    }
+    
+    public BufferedRandomAccessFile(String name, String mode, int size) throws FileNotFoundException
+    {
+        super(name, mode);
+        this.init(size);
+    }
+    
+    private void init(int size)
+    {
+        this.dirty_ = false;
+        this.lo_ = this.curr_ = this.hi_ = 0;
+        this.buff_ = (size > BuffSz_) ? new byte[size] : new byte[BuffSz_];
+        this.maxHi_ = (long) BuffSz_;
+        this.hitEOF_ = false;
+        this.diskPos_ = 0L;
+    }
+    
+    public void close() throws IOException
+    {
+        this.flush();
+        this.buff_ = null;
+        super.close();
+    }
+    
+    /**
+     * Flush any bytes in the file's buffer that have not yet been written to
+     * disk. If the file was created read-only, this method is a no-op.
+     */
+    public void flush() throws IOException
+    {        
+        this.flushBuffer();
+    }
+    
+    /* Flush any dirty bytes in the buffer to disk. */
+    private void flushBuffer() throws IOException
+    {   
+        if (this.dirty_)
+        {
+            if (this.diskPos_ != this.lo_)
+                super.seek(this.lo_);
+            int len = (int) (this.curr_ - this.lo_);
+            super.write(this.buff_, 0, len);
+            this.diskPos_ = this.curr_;             
+            this.dirty_ = false;
+        }
+    }
+    
+    /*
+     * Read at most "this.buff.length" bytes into "this.buff", returning the
+     * number of bytes read. If the return result is less than
+     * "this.buff.length", then EOF was read.
+     */
+    private int fillBuffer() throws IOException
+    {
+        int cnt = 0;
+        int rem = this.buff_.length;
+        while (rem > 0)
+        {
+            int n = super.read(this.buff_, cnt, rem);
+            if (n < 0)
+                break;
+            cnt += n;
+            rem -= n;
+        }
+        if ( (cnt < 0) && (this.hitEOF_ = (cnt < this.buff_.length)) )
+        {
+            // make sure buffer that wasn't read is initialized with -1
+            Arrays.fill(this.buff_, cnt, this.buff_.length, (byte) 0xff);
+        }
+        this.diskPos_ += cnt;
+        return cnt;
+    }
+    
+    /*
+     * This method positions <code>this.curr</code> at position <code>pos</code>.
+     * If <code>pos</code> does not fall in the current buffer, it flushes the
+     * current buffer and loads the correct one.<p>
+     * 
+     * On exit from this routine <code>this.curr == this.hi</code> iff <code>pos</code>
+     * is at or past the end-of-file, which can only happen if the file was
+     * opened in read-only mode.
+     */
+    public void seek(long pos) throws IOException
+    {
+        if (pos >= this.hi_ || pos < this.lo_)
+        {
+            // seeking outside of current buffer -- flush and read             
+            this.flushBuffer();
+            this.lo_ = pos & BuffMask_; // start at BuffSz boundary
+            this.maxHi_ = this.lo_ + (long) this.buff_.length;
+            if (this.diskPos_ != this.lo_)
+            {
+                super.seek(this.lo_);
+                this.diskPos_ = this.lo_;
+            }
+            int n = this.fillBuffer();
+            this.hi_ = this.lo_ + (long) n;
+        }
+        else
+        {
+            // seeking inside current buffer -- no read required
+            if (pos < this.curr_)
+            {
+                // if seeking backwards, we must flush to maintain V4
+                this.flushBuffer();
+            }
+        }
+        this.curr_ = pos;
+    }
+    
+    public long getFilePointer()
+    {
+        return this.curr_;
+    }
+
+    public long length() throws IOException
+    {
+        // max accounts for the case where we have written past the old file length, but not yet flushed our buffer
+        return Math.max(this.curr_, super.length());
+    }
+
+    public int read() throws IOException
+    {
+        if (this.curr_ >= this.hi_)
+        {
+            // test for EOF
+            // if (this.hi < this.maxHi) return -1;
+            if (this.hitEOF_)
+                return -1;
+            
+            // slow path -- read another buffer
+            this.seek(this.curr_);
+            if (this.curr_ == this.hi_)
+                return -1;
+        }
+        byte res = this.buff_[(int) (this.curr_ - this.lo_)];
+        this.curr_++;
+        return ((int) res) & 0xFF; // convert byte -> int
+    }
+    
+    public int read(byte[] b) throws IOException
+    {
+        return this.read(b, 0, b.length);
+    }
+    
+    public int read(byte[] b, int off, int len) throws IOException
+    {
+        if (this.curr_ >= this.hi_)
+        {
+            // test for EOF
+            // if (this.hi < this.maxHi) return -1;
+            if (this.hitEOF_)
+                return -1;
+            
+            // slow path -- read another buffer
+            this.seek(this.curr_);
+            if (this.curr_ == this.hi_)
+                return -1;
+        }
+        len = Math.min(len, (int) (this.hi_ - this.curr_));
+        int buffOff = (int) (this.curr_ - this.lo_);
+        System.arraycopy(this.buff_, buffOff, b, off, len);
+        this.curr_ += len;
+        return len;
+    }
+    
+    public void write(int b) throws IOException
+    {
+        if (this.curr_ >= this.hi_)
+        {
+            if (this.hitEOF_ && this.hi_ < this.maxHi_)
+            {
+                // at EOF -- bump "hi"
+                this.hi_++;
+            }
+            else
+            {
+                // slow path -- write current buffer; read next one
+                this.seek(this.curr_);
+                if (this.curr_ == this.hi_)
+                {
+                    // appending to EOF -- bump "hi"
+                    this.hi_++;
+                }
+            }
+        }
+        this.buff_[(int) (this.curr_ - this.lo_)] = (byte) b;
+        this.curr_++;
+        this.dirty_ = true;
+    }
+    
+    public void write(byte[] b) throws IOException
+    {
+        this.write(b, 0, b.length);
+    }
+    
+    public void write(byte[] b, int off, int len) throws IOException
+    {        
+        while (len > 0)
+        {              
+            int n = this.writeAtMost(b, off, len);
+            off += n;
+            len -= n;
+            this.dirty_ = true;
+        }        
+    }
+    
+    /*
+     * Write at most "len" bytes to "b" starting at position "off", and return
+     * the number of bytes written.
+     */
+    private int writeAtMost(byte[] b, int off, int len) throws IOException
+    {        
+        if (this.curr_ >= this.hi_)
+        {
+            if (this.hitEOF_ && this.hi_ < this.maxHi_)
+            {
+                // at EOF -- bump "hi"
+                this.hi_ = this.maxHi_;
+            }
+            else
+            {                                
+                // slow path -- write current buffer; read next one                
+                this.seek(this.curr_);
+                if (this.curr_ == this.hi_)
+                {
+                    // appending to EOF -- bump "hi"
+                    this.hi_ = this.maxHi_;
+                }
+            }
+        }
+        len = Math.min(len, (int) (this.hi_ - this.curr_));
+        int buffOff = (int) (this.curr_ - this.lo_);
+        System.arraycopy(b, off, this.buff_, buffOff, len);
+        this.curr_ += len;
+        return len;
+    }
+}
diff --git a/src/java/org/apache/cassandra/io/Coordinate.java b/src/java/org/apache/cassandra/io/Coordinate.java
index 23dff6e0eb..b40e1a2b79 100644
--- a/src/java/org/apache/cassandra/io/Coordinate.java
+++ b/src/java/org/apache/cassandra/io/Coordinate.java
@@ -1,42 +1,42 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.cassandra.io;
-
-/**
- * Section of a file that needs to be scanned
- * is represented by this class.
-*/
-public class Coordinate
-{
-    public final long start_;
-    public final long end_;
-    
-    Coordinate(long start, long end)
-    {
-        start_ = start;
-        end_ = end;
-    }
-
-    public String toString()
-    {
-        return "Coordinate(" +
-               "start_=" + start_ +
-               ", end_=" + end_ +
-               ')';
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io;
+
+/**
+ * Section of a file that needs to be scanned
+ * is represented by this class.
+*/
+public class Coordinate
+{
+    public final long start_;
+    public final long end_;
+    
+    Coordinate(long start, long end)
+    {
+        start_ = start;
+        end_ = end;
+    }
+
+    public String toString()
+    {
+        return "Coordinate(" +
+               "start_=" + start_ +
+               ", end_=" + end_ +
+               ')';
+    }
+}
diff --git a/src/java/org/apache/cassandra/io/DataInputBuffer.java b/src/java/org/apache/cassandra/io/DataInputBuffer.java
index c1cecadcc0..a30b572287 100644
--- a/src/java/org/apache/cassandra/io/DataInputBuffer.java
+++ b/src/java/org/apache/cassandra/io/DataInputBuffer.java
@@ -1,100 +1,100 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.*;
-
-
-/**
- * An implementation of the DataInputStream interface. This instance is completely thread 
- * unsafe.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public final class DataInputBuffer extends DataInputStream
-{
-    private static class Buffer extends ByteArrayInputStream
-    {        
-        public Buffer()
-        {
-            super(new byte[] {});
-        }
-
-        public void reset(byte[] input, int start, int length)
-        {
-            this.buf = input;
-            this.count = start + length;
-            this.mark = start;
-            this.pos = start;
-        }
-        
-        public int getPosition()
-        {
-            return pos;
-        }
-        
-        public void setPosition(int position)
-        {
-            pos = position;
-        }        
-
-        public int getLength()
-        {
-            return count;
-        }
-    }
-
-    private Buffer buffer_;
-
-    /** Constructs a new empty buffer. */
-    public DataInputBuffer()
-    {
-        this(new Buffer());
-    }
-
-    private DataInputBuffer(Buffer buffer)
-    {
-        super(buffer);
-        this.buffer_ = buffer;
-    }
-   
-    /** Resets the data that the buffer reads. */
-    public void reset(byte[] input, int length)
-    {
-        buffer_.reset(input, 0, length);
-    }
-
-    /** Resets the data that the buffer reads. */
-    public void reset(byte[] input, int start, int length)
-    {
-        buffer_.reset(input, start, length);
-    }
-
-    /** Returns the length of the input. */
-    public int getLength()
-    {
-        return buffer_.getLength();
-    }
-
-    public int getPosition()
-    {
-        return buffer_.getPosition();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+
+
+/**
+ * An implementation of the DataInputStream interface. This instance is completely thread 
+ * unsafe.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class DataInputBuffer extends DataInputStream
+{
+    private static class Buffer extends ByteArrayInputStream
+    {        
+        public Buffer()
+        {
+            super(new byte[] {});
+        }
+
+        public void reset(byte[] input, int start, int length)
+        {
+            this.buf = input;
+            this.count = start + length;
+            this.mark = start;
+            this.pos = start;
+        }
+        
+        public int getPosition()
+        {
+            return pos;
+        }
+        
+        public void setPosition(int position)
+        {
+            pos = position;
+        }        
+
+        public int getLength()
+        {
+            return count;
+        }
+    }
+
+    private Buffer buffer_;
+
+    /** Constructs a new empty buffer. */
+    public DataInputBuffer()
+    {
+        this(new Buffer());
+    }
+
+    private DataInputBuffer(Buffer buffer)
+    {
+        super(buffer);
+        this.buffer_ = buffer;
+    }
+   
+    /** Resets the data that the buffer reads. */
+    public void reset(byte[] input, int length)
+    {
+        buffer_.reset(input, 0, length);
+    }
+
+    /** Resets the data that the buffer reads. */
+    public void reset(byte[] input, int start, int length)
+    {
+        buffer_.reset(input, start, length);
+    }
+
+    /** Returns the length of the input. */
+    public int getLength()
+    {
+        return buffer_.getLength();
+    }
+
+    public int getPosition()
+    {
+        return buffer_.getPosition();
+    }
+}
diff --git a/src/java/org/apache/cassandra/io/DataOutputBuffer.java b/src/java/org/apache/cassandra/io/DataOutputBuffer.java
index d45839b663..e3cc792cd7 100644
--- a/src/java/org/apache/cassandra/io/DataOutputBuffer.java
+++ b/src/java/org/apache/cassandra/io/DataOutputBuffer.java
@@ -1,108 +1,108 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInput;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-
-/**
- * An implementation of the DataOutputStream interface. This class is completely thread
- * unsafe.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public class DataOutputBuffer extends DataOutputStream
-{
-    private static class Buffer extends ByteArrayOutputStream
-    {
-        public byte[] getData()
-        {
-            return buf;
-        }
-        
-        public int getLength()
-        {
-            return count;
-        }
-        
-        public void reset()
-        {
-            count = 0;
-        }
-        
-        public void write(DataInput in, int len) throws IOException
-        {
-            int newcount = count + len;
-            if (newcount > buf.length)
-            {
-                byte newbuf[] = new byte[Math.max(buf.length << 1, newcount)];
-                System.arraycopy(buf, 0, newbuf, 0, count);
-                buf = newbuf;
-            }
-            in.readFully(buf, count, len);
-            count = newcount;
-        }
-    }
-    
-    private Buffer buffer;
-    
-    /** Constructs a new empty buffer. */
-    public DataOutputBuffer()
-    {
-        this(new Buffer());
-    }
-    
-    private DataOutputBuffer(Buffer buffer)
-    {
-        super(buffer);
-        this.buffer = buffer;
-    }
-    
-    /**
-     * Returns the current contents of the buffer. Data is only valid to
-     * {@link #getLength()}.
-     */
-    public byte[] getData()
-    {
-        return buffer.getData();
-    }
-    
-    /** Returns the length of the valid data currently in the buffer. */
-    public int getLength()
-    {
-        return buffer.getLength();
-    }
-    
-    /** Resets the buffer to empty. */
-    public DataOutputBuffer reset()
-    {
-        this.written = 0;
-        buffer.reset();
-        return this;
-    }
-    
-    /** Writes bytes from a DataInput directly into the buffer. */
-    public void write(DataInput in, int length) throws IOException
-    {
-        buffer.write(in, length);
-    }   
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInput;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+
+/**
+ * An implementation of the DataOutputStream interface. This class is completely thread
+ * unsafe.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class DataOutputBuffer extends DataOutputStream
+{
+    private static class Buffer extends ByteArrayOutputStream
+    {
+        public byte[] getData()
+        {
+            return buf;
+        }
+        
+        public int getLength()
+        {
+            return count;
+        }
+        
+        public void reset()
+        {
+            count = 0;
+        }
+        
+        public void write(DataInput in, int len) throws IOException
+        {
+            int newcount = count + len;
+            if (newcount > buf.length)
+            {
+                byte newbuf[] = new byte[Math.max(buf.length << 1, newcount)];
+                System.arraycopy(buf, 0, newbuf, 0, count);
+                buf = newbuf;
+            }
+            in.readFully(buf, count, len);
+            count = newcount;
+        }
+    }
+    
+    private Buffer buffer;
+    
+    /** Constructs a new empty buffer. */
+    public DataOutputBuffer()
+    {
+        this(new Buffer());
+    }
+    
+    private DataOutputBuffer(Buffer buffer)
+    {
+        super(buffer);
+        this.buffer = buffer;
+    }
+    
+    /**
+     * Returns the current contents of the buffer. Data is only valid to
+     * {@link #getLength()}.
+     */
+    public byte[] getData()
+    {
+        return buffer.getData();
+    }
+    
+    /** Returns the length of the valid data currently in the buffer. */
+    public int getLength()
+    {
+        return buffer.getLength();
+    }
+    
+    /** Resets the buffer to empty. */
+    public DataOutputBuffer reset()
+    {
+        this.written = 0;
+        buffer.reset();
+        return this;
+    }
+    
+    /** Writes bytes from a DataInput directly into the buffer. */
+    public void write(DataInput in, int length) throws IOException
+    {
+        buffer.write(in, length);
+    }   
+}
diff --git a/src/java/org/apache/cassandra/io/FastBufferedInputStream.java b/src/java/org/apache/cassandra/io/FastBufferedInputStream.java
index 9ed153f78c..02b2bc4b84 100644
--- a/src/java/org/apache/cassandra/io/FastBufferedInputStream.java
+++ b/src/java/org/apache/cassandra/io/FastBufferedInputStream.java
@@ -1,486 +1,486 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.*;
-import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;
-
-/**
- * A <code>BufferedInputStream</code> adds functionality to another input
- * stream-namely, the ability to buffer the input and to support the
- * <code>mark</code> and <code>reset</code> methods. When the
- * <code>BufferedInputStream</code> is created, an internal buffer array is
- * created. As bytes from the stream are read or skipped, the internal buffer is
- * refilled as necessary from the contained input stream, many bytes at a time.
- * The <code>mark</code> operation remembers a point in the input stream and
- * the <code>reset</code> operation causes all the bytes read since the most
- * recent <code>mark</code> operation to be reread before new bytes are taken
- * from the contained input stream.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public class FastBufferedInputStream extends FilterInputStream
-{
-    
-    private static int defaultBufferSize = 8192;
-    
-    /**
-     * The internal buffer array where the data is stored. When necessary, it
-     * may be replaced by another array of a different size.
-     */
-    protected volatile byte buf[];
-    
-    /**
-     * Atomic updater to provide compareAndSet for buf. This is necessary
-     * because closes can be asynchronous. We use nullness of buf[] as primary
-     * indicator that this stream is closed. (The "in" field is also nulled out
-     * on close.)
-     */
-    private static final AtomicReferenceFieldUpdater<FastBufferedInputStream, byte[]> bufUpdater = AtomicReferenceFieldUpdater
-    .newUpdater(FastBufferedInputStream.class, byte[].class, "buf");
-    
-    /**
-     * The index one greater than the index of the last valid byte in the
-     * buffer. This value is always in the range <code>0</code> through
-     * <code>buf.length</code>; elements <code>buf[0]</code> through
-     * <code>buf[count-1]
-     * </code>contain buffered input data obtained from the
-     * underlying input stream.
-     */
-    protected int count;
-    
-    /**
-     * The current position in the buffer. This is the index of the next
-     * character to be read from the <code>buf</code> array.
-     * <p>
-     * This value is always in the range <code>0</code> through
-     * <code>count</code>. If it is less than <code>count</code>, then
-     * <code>buf[pos]</code> is the next byte to be supplied as input; if it
-     * is equal to <code>count</code>, then the next <code>read</code> or
-     * <code>skip</code> operation will require more bytes to be read from the
-     * contained input stream.
-     * 
-     * @see java.io.BufferedInputStream#buf
-     */
-    protected int pos;
-    
-    /**
-     * The value of the <code>pos</code> field at the time the last
-     * <code>mark</code> method was called.
-     * <p>
-     * This value is always in the range <code>-1</code> through
-     * <code>pos</code>. If there is no marked position in the input stream,
-     * this field is <code>-1</code>. If there is a marked position in the
-     * input stream, then <code>buf[markpos]</code> is the first byte to be
-     * supplied as input after a <code>reset</code> operation. If
-     * <code>markpos</code> is not <code>-1</code>, then all bytes from
-     * positions <code>buf[markpos]</code> through <code>buf[pos-1]</code>
-     * must remain in the buffer array (though they may be moved to another
-     * place in the buffer array, with suitable adjustments to the values of
-     * <code>count</code>, <code>pos</code>, and <code>markpos</code>);
-     * they may not be discarded unless and until the difference between
-     * <code>pos</code> and <code>markpos</code> exceeds
-     * <code>marklimit</code>.
-     * 
-     * @see java.io.BufferedInputStream#mark(int)
-     * @see java.io.BufferedInputStream#pos
-     */
-    protected int markpos = -1;
-    
-    /**
-     * The maximum read ahead allowed after a call to the <code>mark</code>
-     * method before subsequent calls to the <code>reset</code> method fail.
-     * Whenever the difference between <code>pos</code> and
-     * <code>markpos</code> exceeds <code>marklimit</code>, then the mark
-     * may be dropped by setting <code>markpos</code> to <code>-1</code>.
-     * 
-     * @see java.io.BufferedInputStream#mark(int)
-     * @see java.io.BufferedInputStream#reset()
-     */
-    protected int marklimit;
-    
-    /**
-     * Check to make sure that underlying input stream has not been nulled out
-     * due to close; if not return it;
-     */
-    private InputStream getInIfOpen() throws IOException
-    {
-        InputStream input = in;
-        if (input == null)
-            throw new IOException("Stream closed");
-        return input;
-    }
-    
-    /**
-     * Check to make sure that buffer has not been nulled out due to close; if
-     * not return it;
-     */
-    private byte[] getBufIfOpen() throws IOException
-    {
-        byte[] buffer = buf;
-        if (buffer == null)
-            throw new IOException("Stream closed");
-        return buffer;
-    }
-    
-    /**
-     * Creates a <code>BufferedInputStream</code> and saves its argument, the
-     * input stream <code>in</code>, for later use. An internal buffer array
-     * is created and stored in <code>buf</code>.
-     * 
-     * @param in
-     *            the underlying input stream.
-     */
-    public FastBufferedInputStream(InputStream in)
-    {
-        this(in, defaultBufferSize);
-    }
-    
-    /**
-     * Creates a <code>BufferedInputStream</code> with the specified buffer
-     * size, and saves its argument, the input stream <code>in</code>, for
-     * later use. An internal buffer array of length <code>size</code> is
-     * created and stored in <code>buf</code>.
-     * 
-     * @param in
-     *            the underlying input stream.
-     * @param size
-     *            the buffer size.
-     * @exception IllegalArgumentException
-     *                if size <= 0.
-     */
-    public FastBufferedInputStream(InputStream in, int size)
-    {
-        super(in);
-        if (size <= 0)
-        {
-            throw new IllegalArgumentException("Buffer size <= 0");
-        }
-        buf = new byte[size];
-    }
-    
-    /**
-     * Fills the buffer with more data, taking into account shuffling and other
-     * tricks for dealing with marks. Assumes that it is being called by a
-     * synchronized method. This method also assumes that all data has already
-     * been read in, hence pos > count.
-     */
-    private void fill() throws IOException
-    {
-        byte[] buffer = getBufIfOpen();
-        if (markpos < 0)
-            pos = 0; /* no mark: throw away the buffer */
-        else if (pos >= buffer.length) /* no room left in buffer */
-            if (markpos > 0)
-            { /* can throw away early part of the buffer */
-                int sz = pos - markpos;
-                System.arraycopy(buffer, markpos, buffer, 0, sz);
-                pos = sz;
-                markpos = 0;
-            }
-            else if (buffer.length >= marklimit)
-            {
-                markpos = -1; /* buffer got too big, invalidate mark */
-                pos = 0; /* drop buffer contents */
-            }
-            else
-            { /* grow buffer */
-                int nsz = pos * 2;
-                if (nsz > marklimit)
-                    nsz = marklimit;
-                byte nbuf[] = new byte[nsz];
-                System.arraycopy(buffer, 0, nbuf, 0, pos);
-                if (!bufUpdater.compareAndSet(this, buffer, nbuf))
-                {
-                    // Can't replace buf if there was an async close.
-                    // Note: This would need to be changed if fill()
-                    // is ever made accessible to multiple threads.
-                    // But for now, the only way CAS can fail is via close.
-                    // assert buf == null;
-                    throw new IOException("Stream closed");
-                }
-                buffer = nbuf;
-            }
-        count = pos;
-        int n = getInIfOpen().read(buffer, pos, buffer.length - pos);
-        if (n > 0)
-            count = n + pos;
-    }
-    
-    /**
-     * See the general contract of the <code>read</code> method of
-     * <code>InputStream</code>.
-     * 
-     * @return the next byte of data, or <code>-1</code> if the end of the
-     *         stream is reached.
-     * @exception IOException
-     *                if this input stream has been closed by invoking its
-     *                {@link #close()} method, or an I/O error occurs.
-     * @see java.io.FilterInputStream#in
-     */
-    public  int read() throws IOException
-    {
-        if (pos >= count)
-        {
-            fill();
-            if (pos >= count)
-                return -1;
-        }
-        return getBufIfOpen()[pos++] & 0xff;
-    }
-    
-    /**
-     * Read characters into a portion of an array, reading from the underlying
-     * stream at most once if necessary.
-     */
-    private int read1(byte[] b, int off, int len) throws IOException
-    {
-        int avail = count - pos;
-        if (avail <= 0)
-        {
-            /*
-             * If the requested length is at least as large as the buffer, and
-             * if there is no mark/reset activity, do not bother to copy the
-             * bytes into the local buffer. In this way buffered streams will
-             * cascade harmlessly.
-             */
-            if (len >= getBufIfOpen().length && markpos < 0)
-            {
-                return getInIfOpen().read(b, off, len);
-            }
-            fill();
-            avail = count - pos;
-            if (avail <= 0)
-                return -1;
-        }
-        int cnt = (avail < len) ? avail : len;
-        System.arraycopy(getBufIfOpen(), pos, b, off, cnt);
-        pos += cnt;
-        return cnt;
-    }
-    
-    /**
-     * Reads bytes from this byte-input stream into the specified byte array,
-     * starting at the given offset.
-     * 
-     * <p>
-     * This method implements the general contract of the corresponding
-     * <code>{@link InputStream#read(byte[], int, int) read}</code> method of
-     * the <code>{@link InputStream}</code> class. As an additional
-     * convenience, it attempts to read as many bytes as possible by repeatedly
-     * invoking the <code>read</code> method of the underlying stream. This
-     * iterated <code>read</code> continues until one of the following
-     * conditions becomes true:
-     * <ul>
-     * 
-     * <li> The specified number of bytes have been read,
-     * 
-     * <li> The <code>read</code> method of the underlying stream returns
-     * <code>-1</code>, indicating end-of-file, or
-     * 
-     * <li> The <code>available</code> method of the underlying stream returns
-     * zero, indicating that further input requests would block.
-     * 
-     * </ul>
-     * If the first <code>read</code> on the underlying stream returns
-     * <code>-1</code> to indicate end-of-file then this method returns
-     * <code>-1</code>. Otherwise this method returns the number of bytes
-     * actually read.
-     * 
-     * <p>
-     * Subclasses of this class are encouraged, but not required, to attempt to
-     * read as many bytes as possible in the same fashion.
-     * 
-     * @param b
-     *            destination buffer.
-     * @param off
-     *            offset at which to start storing bytes.
-     * @param len
-     *            maximum number of bytes to read.
-     * @return the number of bytes read, or <code>-1</code> if the end of the
-     *         stream has been reached.
-     * @exception IOException
-     *                if this input stream has been closed by invoking its
-     *                {@link #close()} method, or an I/O error occurs.
-     */
-    public  int read(byte b[], int off, int len) throws IOException
-    {
-        getBufIfOpen(); // Check for closed stream
-        if ((off | len | (off + len) | (b.length - (off + len))) < 0)
-        {
-            throw new IndexOutOfBoundsException();
-        }
-        else if (len == 0)
-        {
-            return 0;
-        }
-        
-        int n = 0;
-        for (;;)
-        {
-            int nread = read1(b, off + n, len - n);
-            if (nread <= 0)
-                return (n == 0) ? nread : n;
-            n += nread;
-            if (n >= len)
-                return n;
-            // if not closed but no bytes available, return
-            InputStream input = in;
-            if (input != null && input.available() <= 0)
-                return n;
-        }
-    }
-    
-    /**
-     * See the general contract of the <code>skip</code> method of
-     * <code>InputStream</code>.
-     * 
-     * @exception IOException
-     *                if the stream does not support seek, or if this input
-     *                stream has been closed by invoking its {@link #close()}
-     *                method, or an I/O error occurs.
-     */
-    public  long skip(long n) throws IOException
-    {
-        getBufIfOpen(); // Check for closed stream
-        if (n <= 0)
-        {
-            return 0;
-        }
-        long avail = count - pos;
-        
-        if (avail <= 0)
-        {
-            // If no mark position set then don't keep in buffer
-            if (markpos < 0)
-                return getInIfOpen().skip(n);
-            
-            // Fill in buffer to save bytes for reset
-            fill();
-            avail = count - pos;
-            if (avail <= 0)
-                return 0;
-        }
-        
-        long skipped = (avail < n) ? avail : n;
-        pos += skipped;
-        return skipped;
-    }
-    
-    /**
-     * Returns an estimate of the number of bytes that can be read (or skipped
-     * over) from this input stream without blocking by the next invocation of a
-     * method for this input stream. The next invocation might be the same
-     * thread or another thread. A single read or skip of this many bytes will
-     * not block, but may read or skip fewer bytes.
-     * <p>
-     * This method returns the sum of the number of bytes remaining to be read
-     * in the buffer (<code>count&nbsp;- pos</code>) and the result of
-     * calling the {@link java.io.FilterInputStream#in in}.available().
-     * 
-     * @return an estimate of the number of bytes that can be read (or skipped
-     *         over) from this input stream without blocking.
-     * @exception IOException
-     *                if this input stream has been closed by invoking its
-     *                {@link #close()} method, or an I/O error occurs.
-     */
-    public  int available() throws IOException
-    {
-        return getInIfOpen().available() + (count - pos);
-    }
-    
-    /**
-     * See the general contract of the <code>mark</code> method of
-     * <code>InputStream</code>.
-     * 
-     * @param readlimit
-     *            the maximum limit of bytes that can be read before the mark
-     *            position becomes invalid.
-     * @see java.io.BufferedInputStream#reset()
-     */
-    public  void mark(int readlimit)
-    {
-        marklimit = readlimit;
-        markpos = pos;
-    }
-    
-    /**
-     * See the general contract of the <code>reset</code> method of
-     * <code>InputStream</code>.
-     * <p>
-     * If <code>markpos</code> is <code>-1</code> (no mark has been set or
-     * the mark has been invalidated), an <code>IOException</code> is thrown.
-     * Otherwise, <code>pos</code> is set equal to <code>markpos</code>.
-     * 
-     * @exception IOException
-     *                if this stream has not been marked or, if the mark has
-     *                been invalidated, or the stream has been closed by
-     *                invoking its {@link #close()} method, or an I/O error
-     *                occurs.
-     * @see java.io.BufferedInputStream#mark(int)
-     */
-    public  void reset() throws IOException
-    {
-        getBufIfOpen(); // Cause exception if closed
-        if (markpos < 0)
-            throw new IOException("Resetting to invalid mark");
-        pos = markpos;
-    }
-    
-    /**
-     * Tests if this input stream supports the <code>mark</code> and
-     * <code>reset</code> methods. The <code>markSupported</code> method of
-     * <code>BufferedInputStream</code> returns <code>true</code>.
-     * 
-     * @return a <code>boolean</code> indicating if this stream type supports
-     *         the <code>mark</code> and <code>reset</code> methods.
-     * @see java.io.InputStream#mark(int)
-     * @see java.io.InputStream#reset()
-     */
-    public boolean markSupported()
-    {
-        return true;
-    }
-    
-    /**
-     * Closes this input stream and releases any system resources associated
-     * with the stream. Once the stream has been closed, further read(),
-     * available(), reset(), or skip() invocations will throw an IOException.
-     * Closing a previously closed stream has no effect.
-     * 
-     * @exception IOException
-     *                if an I/O error occurs.
-     */
-    public void close() throws IOException
-    {
-        byte[] buffer;
-        while ((buffer = buf) != null)
-        {
-            if (bufUpdater.compareAndSet(this, buffer, null))
-            {
-                InputStream input = in;
-                in = null;
-                if (input != null)
-                    input.close();
-                return;
-            }
-            // Else retry in case a new buf was CASed in fill()
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+import java.util.concurrent.atomic.AtomicReferenceFieldUpdater;
+
+/**
+ * A <code>BufferedInputStream</code> adds functionality to another input
+ * stream-namely, the ability to buffer the input and to support the
+ * <code>mark</code> and <code>reset</code> methods. When the
+ * <code>BufferedInputStream</code> is created, an internal buffer array is
+ * created. As bytes from the stream are read or skipped, the internal buffer is
+ * refilled as necessary from the contained input stream, many bytes at a time.
+ * The <code>mark</code> operation remembers a point in the input stream and
+ * the <code>reset</code> operation causes all the bytes read since the most
+ * recent <code>mark</code> operation to be reread before new bytes are taken
+ * from the contained input stream.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class FastBufferedInputStream extends FilterInputStream
+{
+    
+    private static int defaultBufferSize = 8192;
+    
+    /**
+     * The internal buffer array where the data is stored. When necessary, it
+     * may be replaced by another array of a different size.
+     */
+    protected volatile byte buf[];
+    
+    /**
+     * Atomic updater to provide compareAndSet for buf. This is necessary
+     * because closes can be asynchronous. We use nullness of buf[] as primary
+     * indicator that this stream is closed. (The "in" field is also nulled out
+     * on close.)
+     */
+    private static final AtomicReferenceFieldUpdater<FastBufferedInputStream, byte[]> bufUpdater = AtomicReferenceFieldUpdater
+    .newUpdater(FastBufferedInputStream.class, byte[].class, "buf");
+    
+    /**
+     * The index one greater than the index of the last valid byte in the
+     * buffer. This value is always in the range <code>0</code> through
+     * <code>buf.length</code>; elements <code>buf[0]</code> through
+     * <code>buf[count-1]
+     * </code>contain buffered input data obtained from the
+     * underlying input stream.
+     */
+    protected int count;
+    
+    /**
+     * The current position in the buffer. This is the index of the next
+     * character to be read from the <code>buf</code> array.
+     * <p>
+     * This value is always in the range <code>0</code> through
+     * <code>count</code>. If it is less than <code>count</code>, then
+     * <code>buf[pos]</code> is the next byte to be supplied as input; if it
+     * is equal to <code>count</code>, then the next <code>read</code> or
+     * <code>skip</code> operation will require more bytes to be read from the
+     * contained input stream.
+     * 
+     * @see java.io.BufferedInputStream#buf
+     */
+    protected int pos;
+    
+    /**
+     * The value of the <code>pos</code> field at the time the last
+     * <code>mark</code> method was called.
+     * <p>
+     * This value is always in the range <code>-1</code> through
+     * <code>pos</code>. If there is no marked position in the input stream,
+     * this field is <code>-1</code>. If there is a marked position in the
+     * input stream, then <code>buf[markpos]</code> is the first byte to be
+     * supplied as input after a <code>reset</code> operation. If
+     * <code>markpos</code> is not <code>-1</code>, then all bytes from
+     * positions <code>buf[markpos]</code> through <code>buf[pos-1]</code>
+     * must remain in the buffer array (though they may be moved to another
+     * place in the buffer array, with suitable adjustments to the values of
+     * <code>count</code>, <code>pos</code>, and <code>markpos</code>);
+     * they may not be discarded unless and until the difference between
+     * <code>pos</code> and <code>markpos</code> exceeds
+     * <code>marklimit</code>.
+     * 
+     * @see java.io.BufferedInputStream#mark(int)
+     * @see java.io.BufferedInputStream#pos
+     */
+    protected int markpos = -1;
+    
+    /**
+     * The maximum read ahead allowed after a call to the <code>mark</code>
+     * method before subsequent calls to the <code>reset</code> method fail.
+     * Whenever the difference between <code>pos</code> and
+     * <code>markpos</code> exceeds <code>marklimit</code>, then the mark
+     * may be dropped by setting <code>markpos</code> to <code>-1</code>.
+     * 
+     * @see java.io.BufferedInputStream#mark(int)
+     * @see java.io.BufferedInputStream#reset()
+     */
+    protected int marklimit;
+    
+    /**
+     * Check to make sure that underlying input stream has not been nulled out
+     * due to close; if not return it;
+     */
+    private InputStream getInIfOpen() throws IOException
+    {
+        InputStream input = in;
+        if (input == null)
+            throw new IOException("Stream closed");
+        return input;
+    }
+    
+    /**
+     * Check to make sure that buffer has not been nulled out due to close; if
+     * not return it;
+     */
+    private byte[] getBufIfOpen() throws IOException
+    {
+        byte[] buffer = buf;
+        if (buffer == null)
+            throw new IOException("Stream closed");
+        return buffer;
+    }
+    
+    /**
+     * Creates a <code>BufferedInputStream</code> and saves its argument, the
+     * input stream <code>in</code>, for later use. An internal buffer array
+     * is created and stored in <code>buf</code>.
+     * 
+     * @param in
+     *            the underlying input stream.
+     */
+    public FastBufferedInputStream(InputStream in)
+    {
+        this(in, defaultBufferSize);
+    }
+    
+    /**
+     * Creates a <code>BufferedInputStream</code> with the specified buffer
+     * size, and saves its argument, the input stream <code>in</code>, for
+     * later use. An internal buffer array of length <code>size</code> is
+     * created and stored in <code>buf</code>.
+     * 
+     * @param in
+     *            the underlying input stream.
+     * @param size
+     *            the buffer size.
+     * @exception IllegalArgumentException
+     *                if size <= 0.
+     */
+    public FastBufferedInputStream(InputStream in, int size)
+    {
+        super(in);
+        if (size <= 0)
+        {
+            throw new IllegalArgumentException("Buffer size <= 0");
+        }
+        buf = new byte[size];
+    }
+    
+    /**
+     * Fills the buffer with more data, taking into account shuffling and other
+     * tricks for dealing with marks. Assumes that it is being called by a
+     * synchronized method. This method also assumes that all data has already
+     * been read in, hence pos > count.
+     */
+    private void fill() throws IOException
+    {
+        byte[] buffer = getBufIfOpen();
+        if (markpos < 0)
+            pos = 0; /* no mark: throw away the buffer */
+        else if (pos >= buffer.length) /* no room left in buffer */
+            if (markpos > 0)
+            { /* can throw away early part of the buffer */
+                int sz = pos - markpos;
+                System.arraycopy(buffer, markpos, buffer, 0, sz);
+                pos = sz;
+                markpos = 0;
+            }
+            else if (buffer.length >= marklimit)
+            {
+                markpos = -1; /* buffer got too big, invalidate mark */
+                pos = 0; /* drop buffer contents */
+            }
+            else
+            { /* grow buffer */
+                int nsz = pos * 2;
+                if (nsz > marklimit)
+                    nsz = marklimit;
+                byte nbuf[] = new byte[nsz];
+                System.arraycopy(buffer, 0, nbuf, 0, pos);
+                if (!bufUpdater.compareAndSet(this, buffer, nbuf))
+                {
+                    // Can't replace buf if there was an async close.
+                    // Note: This would need to be changed if fill()
+                    // is ever made accessible to multiple threads.
+                    // But for now, the only way CAS can fail is via close.
+                    // assert buf == null;
+                    throw new IOException("Stream closed");
+                }
+                buffer = nbuf;
+            }
+        count = pos;
+        int n = getInIfOpen().read(buffer, pos, buffer.length - pos);
+        if (n > 0)
+            count = n + pos;
+    }
+    
+    /**
+     * See the general contract of the <code>read</code> method of
+     * <code>InputStream</code>.
+     * 
+     * @return the next byte of data, or <code>-1</code> if the end of the
+     *         stream is reached.
+     * @exception IOException
+     *                if this input stream has been closed by invoking its
+     *                {@link #close()} method, or an I/O error occurs.
+     * @see java.io.FilterInputStream#in
+     */
+    public  int read() throws IOException
+    {
+        if (pos >= count)
+        {
+            fill();
+            if (pos >= count)
+                return -1;
+        }
+        return getBufIfOpen()[pos++] & 0xff;
+    }
+    
+    /**
+     * Read characters into a portion of an array, reading from the underlying
+     * stream at most once if necessary.
+     */
+    private int read1(byte[] b, int off, int len) throws IOException
+    {
+        int avail = count - pos;
+        if (avail <= 0)
+        {
+            /*
+             * If the requested length is at least as large as the buffer, and
+             * if there is no mark/reset activity, do not bother to copy the
+             * bytes into the local buffer. In this way buffered streams will
+             * cascade harmlessly.
+             */
+            if (len >= getBufIfOpen().length && markpos < 0)
+            {
+                return getInIfOpen().read(b, off, len);
+            }
+            fill();
+            avail = count - pos;
+            if (avail <= 0)
+                return -1;
+        }
+        int cnt = (avail < len) ? avail : len;
+        System.arraycopy(getBufIfOpen(), pos, b, off, cnt);
+        pos += cnt;
+        return cnt;
+    }
+    
+    /**
+     * Reads bytes from this byte-input stream into the specified byte array,
+     * starting at the given offset.
+     * 
+     * <p>
+     * This method implements the general contract of the corresponding
+     * <code>{@link InputStream#read(byte[], int, int) read}</code> method of
+     * the <code>{@link InputStream}</code> class. As an additional
+     * convenience, it attempts to read as many bytes as possible by repeatedly
+     * invoking the <code>read</code> method of the underlying stream. This
+     * iterated <code>read</code> continues until one of the following
+     * conditions becomes true:
+     * <ul>
+     * 
+     * <li> The specified number of bytes have been read,
+     * 
+     * <li> The <code>read</code> method of the underlying stream returns
+     * <code>-1</code>, indicating end-of-file, or
+     * 
+     * <li> The <code>available</code> method of the underlying stream returns
+     * zero, indicating that further input requests would block.
+     * 
+     * </ul>
+     * If the first <code>read</code> on the underlying stream returns
+     * <code>-1</code> to indicate end-of-file then this method returns
+     * <code>-1</code>. Otherwise this method returns the number of bytes
+     * actually read.
+     * 
+     * <p>
+     * Subclasses of this class are encouraged, but not required, to attempt to
+     * read as many bytes as possible in the same fashion.
+     * 
+     * @param b
+     *            destination buffer.
+     * @param off
+     *            offset at which to start storing bytes.
+     * @param len
+     *            maximum number of bytes to read.
+     * @return the number of bytes read, or <code>-1</code> if the end of the
+     *         stream has been reached.
+     * @exception IOException
+     *                if this input stream has been closed by invoking its
+     *                {@link #close()} method, or an I/O error occurs.
+     */
+    public  int read(byte b[], int off, int len) throws IOException
+    {
+        getBufIfOpen(); // Check for closed stream
+        if ((off | len | (off + len) | (b.length - (off + len))) < 0)
+        {
+            throw new IndexOutOfBoundsException();
+        }
+        else if (len == 0)
+        {
+            return 0;
+        }
+        
+        int n = 0;
+        for (;;)
+        {
+            int nread = read1(b, off + n, len - n);
+            if (nread <= 0)
+                return (n == 0) ? nread : n;
+            n += nread;
+            if (n >= len)
+                return n;
+            // if not closed but no bytes available, return
+            InputStream input = in;
+            if (input != null && input.available() <= 0)
+                return n;
+        }
+    }
+    
+    /**
+     * See the general contract of the <code>skip</code> method of
+     * <code>InputStream</code>.
+     * 
+     * @exception IOException
+     *                if the stream does not support seek, or if this input
+     *                stream has been closed by invoking its {@link #close()}
+     *                method, or an I/O error occurs.
+     */
+    public  long skip(long n) throws IOException
+    {
+        getBufIfOpen(); // Check for closed stream
+        if (n <= 0)
+        {
+            return 0;
+        }
+        long avail = count - pos;
+        
+        if (avail <= 0)
+        {
+            // If no mark position set then don't keep in buffer
+            if (markpos < 0)
+                return getInIfOpen().skip(n);
+            
+            // Fill in buffer to save bytes for reset
+            fill();
+            avail = count - pos;
+            if (avail <= 0)
+                return 0;
+        }
+        
+        long skipped = (avail < n) ? avail : n;
+        pos += skipped;
+        return skipped;
+    }
+    
+    /**
+     * Returns an estimate of the number of bytes that can be read (or skipped
+     * over) from this input stream without blocking by the next invocation of a
+     * method for this input stream. The next invocation might be the same
+     * thread or another thread. A single read or skip of this many bytes will
+     * not block, but may read or skip fewer bytes.
+     * <p>
+     * This method returns the sum of the number of bytes remaining to be read
+     * in the buffer (<code>count&nbsp;- pos</code>) and the result of
+     * calling the {@link java.io.FilterInputStream#in in}.available().
+     * 
+     * @return an estimate of the number of bytes that can be read (or skipped
+     *         over) from this input stream without blocking.
+     * @exception IOException
+     *                if this input stream has been closed by invoking its
+     *                {@link #close()} method, or an I/O error occurs.
+     */
+    public  int available() throws IOException
+    {
+        return getInIfOpen().available() + (count - pos);
+    }
+    
+    /**
+     * See the general contract of the <code>mark</code> method of
+     * <code>InputStream</code>.
+     * 
+     * @param readlimit
+     *            the maximum limit of bytes that can be read before the mark
+     *            position becomes invalid.
+     * @see java.io.BufferedInputStream#reset()
+     */
+    public  void mark(int readlimit)
+    {
+        marklimit = readlimit;
+        markpos = pos;
+    }
+    
+    /**
+     * See the general contract of the <code>reset</code> method of
+     * <code>InputStream</code>.
+     * <p>
+     * If <code>markpos</code> is <code>-1</code> (no mark has been set or
+     * the mark has been invalidated), an <code>IOException</code> is thrown.
+     * Otherwise, <code>pos</code> is set equal to <code>markpos</code>.
+     * 
+     * @exception IOException
+     *                if this stream has not been marked or, if the mark has
+     *                been invalidated, or the stream has been closed by
+     *                invoking its {@link #close()} method, or an I/O error
+     *                occurs.
+     * @see java.io.BufferedInputStream#mark(int)
+     */
+    public  void reset() throws IOException
+    {
+        getBufIfOpen(); // Cause exception if closed
+        if (markpos < 0)
+            throw new IOException("Resetting to invalid mark");
+        pos = markpos;
+    }
+    
+    /**
+     * Tests if this input stream supports the <code>mark</code> and
+     * <code>reset</code> methods. The <code>markSupported</code> method of
+     * <code>BufferedInputStream</code> returns <code>true</code>.
+     * 
+     * @return a <code>boolean</code> indicating if this stream type supports
+     *         the <code>mark</code> and <code>reset</code> methods.
+     * @see java.io.InputStream#mark(int)
+     * @see java.io.InputStream#reset()
+     */
+    public boolean markSupported()
+    {
+        return true;
+    }
+    
+    /**
+     * Closes this input stream and releases any system resources associated
+     * with the stream. Once the stream has been closed, further read(),
+     * available(), reset(), or skip() invocations will throw an IOException.
+     * Closing a previously closed stream has no effect.
+     * 
+     * @exception IOException
+     *                if an I/O error occurs.
+     */
+    public void close() throws IOException
+    {
+        byte[] buffer;
+        while ((buffer = buf) != null)
+        {
+            if (bufUpdater.compareAndSet(this, buffer, null))
+            {
+                InputStream input = in;
+                in = null;
+                if (input != null)
+                    input.close();
+                return;
+            }
+            // Else retry in case a new buf was CASed in fill()
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/io/FastBufferedOutputStream.java b/src/java/org/apache/cassandra/io/FastBufferedOutputStream.java
index 133b425107..9e9c76176c 100644
--- a/src/java/org/apache/cassandra/io/FastBufferedOutputStream.java
+++ b/src/java/org/apache/cassandra/io/FastBufferedOutputStream.java
@@ -1,162 +1,162 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.*;
-
-/**
- * The class implements a buffered output stream. By setting up such an output
- * stream, an application can write bytes to the underlying output stream
- * without necessarily causing a call to the underlying system for each byte
- * written.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public class FastBufferedOutputStream extends FilterOutputStream
-{
-    /**
-     * The internal buffer where data is stored.
-     */
-    protected byte buf[];
-    
-    /**
-     * The number of valid bytes in the buffer. This value is always in the
-     * range <tt>0</tt> through <tt>buf.length</tt>; elements
-     * <tt>buf[0]</tt> through <tt>buf[count-1]</tt> contain valid byte
-     * data.
-     */
-    protected int count;
-    
-    /**
-     * Creates a new buffered output stream to write data to the specified
-     * underlying output stream.
-     * 
-     * @param out
-     *            the underlying output stream.
-     */
-    public FastBufferedOutputStream(OutputStream out)
-    {
-        this(out, 8192);
-    }
-    
-    /**
-     * Creates a new buffered output stream to write data to the specified
-     * underlying output stream with the specified buffer size.
-     * 
-     * @param out
-     *            the underlying output stream.
-     * @param size
-     *            the buffer size.
-     * @exception IllegalArgumentException
-     *                if size &lt;= 0.
-     */
-    public FastBufferedOutputStream(OutputStream out, int size)
-    {
-        super(out);
-        if (size <= 0)
-        {
-            throw new IllegalArgumentException("Buffer size <= 0");
-        }
-        buf = new byte[size];
-    }
-    
-    /** Flush the internal buffer */
-    private void flushBuffer() throws IOException
-    {
-        if (count > 0)
-        {
-            out.write(buf, 0, count);
-            count = 0;
-        }
-    }
-    
-    /**
-     * Writes the specified byte to this buffered output stream.
-     * 
-     * @param b
-     *            the byte to be written.
-     * @exception IOException
-     *                if an I/O error occurs.
-     */
-    public void write(int b) throws IOException
-    {
-        if (count >= buf.length)
-        {
-            flushBuffer();
-        }
-        buf[count++] = (byte) b;
-    }
-    
-    /**
-     * Writes <code>len</code> bytes from the specified byte array starting at
-     * offset <code>off</code> to this buffered output stream.
-     * 
-     * <p>
-     * Ordinarily this method stores bytes from the given array into this
-     * stream's buffer, flushing the buffer to the underlying output stream as
-     * needed. If the requested length is at least as large as this stream's
-     * buffer, however, then this method will flush the buffer and write the
-     * bytes directly to the underlying output stream. Thus redundant
-     * <code>BufferedOutputStream</code>s will not copy data unnecessarily.
-     * 
-     * @param b
-     *            the data.
-     * @param off
-     *            the start offset in the data.
-     * @param len
-     *            the number of bytes to write.
-     * @exception IOException
-     *                if an I/O error occurs.
-     */
-    public void write(byte b[], int off, int len)
-    throws IOException
-    {
-        if (len >= buf.length)
-        {
-            /*
-             * If the request length exceeds the size of the output buffer,
-             * flush the output buffer and then write the data directly. In this
-             * way buffered streams will cascade harmlessly.
-             */
-            flushBuffer();
-            out.write(b, off, len);
-            return;
-        }
-        if (len > buf.length - count)
-        {
-            flushBuffer();
-        }
-        System.arraycopy(b, off, buf, count, len);
-        count += len;
-    }
-    
-    /**
-     * Flushes this buffered output stream. This forces any buffered output
-     * bytes to be written out to the underlying output stream.
-     * 
-     * @exception IOException
-     *                if an I/O error occurs.
-     * @see java.io.FilterOutputStream#out
-     */
-    public void flush() throws IOException
-    {
-        flushBuffer();
-        out.flush();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+
+/**
+ * The class implements a buffered output stream. By setting up such an output
+ * stream, an application can write bytes to the underlying output stream
+ * without necessarily causing a call to the underlying system for each byte
+ * written.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class FastBufferedOutputStream extends FilterOutputStream
+{
+    /**
+     * The internal buffer where data is stored.
+     */
+    protected byte buf[];
+    
+    /**
+     * The number of valid bytes in the buffer. This value is always in the
+     * range <tt>0</tt> through <tt>buf.length</tt>; elements
+     * <tt>buf[0]</tt> through <tt>buf[count-1]</tt> contain valid byte
+     * data.
+     */
+    protected int count;
+    
+    /**
+     * Creates a new buffered output stream to write data to the specified
+     * underlying output stream.
+     * 
+     * @param out
+     *            the underlying output stream.
+     */
+    public FastBufferedOutputStream(OutputStream out)
+    {
+        this(out, 8192);
+    }
+    
+    /**
+     * Creates a new buffered output stream to write data to the specified
+     * underlying output stream with the specified buffer size.
+     * 
+     * @param out
+     *            the underlying output stream.
+     * @param size
+     *            the buffer size.
+     * @exception IllegalArgumentException
+     *                if size &lt;= 0.
+     */
+    public FastBufferedOutputStream(OutputStream out, int size)
+    {
+        super(out);
+        if (size <= 0)
+        {
+            throw new IllegalArgumentException("Buffer size <= 0");
+        }
+        buf = new byte[size];
+    }
+    
+    /** Flush the internal buffer */
+    private void flushBuffer() throws IOException
+    {
+        if (count > 0)
+        {
+            out.write(buf, 0, count);
+            count = 0;
+        }
+    }
+    
+    /**
+     * Writes the specified byte to this buffered output stream.
+     * 
+     * @param b
+     *            the byte to be written.
+     * @exception IOException
+     *                if an I/O error occurs.
+     */
+    public void write(int b) throws IOException
+    {
+        if (count >= buf.length)
+        {
+            flushBuffer();
+        }
+        buf[count++] = (byte) b;
+    }
+    
+    /**
+     * Writes <code>len</code> bytes from the specified byte array starting at
+     * offset <code>off</code> to this buffered output stream.
+     * 
+     * <p>
+     * Ordinarily this method stores bytes from the given array into this
+     * stream's buffer, flushing the buffer to the underlying output stream as
+     * needed. If the requested length is at least as large as this stream's
+     * buffer, however, then this method will flush the buffer and write the
+     * bytes directly to the underlying output stream. Thus redundant
+     * <code>BufferedOutputStream</code>s will not copy data unnecessarily.
+     * 
+     * @param b
+     *            the data.
+     * @param off
+     *            the start offset in the data.
+     * @param len
+     *            the number of bytes to write.
+     * @exception IOException
+     *                if an I/O error occurs.
+     */
+    public void write(byte b[], int off, int len)
+    throws IOException
+    {
+        if (len >= buf.length)
+        {
+            /*
+             * If the request length exceeds the size of the output buffer,
+             * flush the output buffer and then write the data directly. In this
+             * way buffered streams will cascade harmlessly.
+             */
+            flushBuffer();
+            out.write(b, off, len);
+            return;
+        }
+        if (len > buf.length - count)
+        {
+            flushBuffer();
+        }
+        System.arraycopy(b, off, buf, count, len);
+        count += len;
+    }
+    
+    /**
+     * Flushes this buffered output stream. This forces any buffered output
+     * bytes to be written out to the underlying output stream.
+     * 
+     * @exception IOException
+     *                if an I/O error occurs.
+     * @see java.io.FilterOutputStream#out
+     */
+    public void flush() throws IOException
+    {
+        flushBuffer();
+        out.flush();
+    }
+}
diff --git a/src/java/org/apache/cassandra/io/FileStruct.java b/src/java/org/apache/cassandra/io/FileStruct.java
index 40496edb9a..87b510801c 100644
--- a/src/java/org/apache/cassandra/io/FileStruct.java
+++ b/src/java/org/apache/cassandra/io/FileStruct.java
@@ -1,192 +1,192 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.IOException;
-import java.util.Iterator;
-
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.io.DataOutputBuffer;
-import org.apache.cassandra.io.IFileReader;
-import org.apache.cassandra.io.SSTableReader;
-
-import org.apache.log4j.Logger;
-import com.google.common.collect.AbstractIterator;
-
-
-public class FileStruct implements Comparable<FileStruct>, Iterator<String>
-{
-    private static Logger logger = Logger.getLogger(FileStruct.class);
-
-    private String key = null; // decorated!
-    private boolean exhausted = false;
-    private IFileReader reader;
-    private DataInputBuffer bufIn;
-    private DataOutputBuffer bufOut;
-    private SSTableReader sstable;
-    private FileStructIterator iterator;
-
-    FileStruct(SSTableReader sstable) throws IOException
-    {
-        this.reader = SequenceFile.bufferedReader(sstable.getFilename(), 1024 * 1024);
-        this.sstable = sstable;
-        bufIn = new DataInputBuffer();
-        bufOut = new DataOutputBuffer();
-    }
-
-    public String getFileName()
-    {
-        return reader.getFileName();
-    }
-
-    public void close() throws IOException
-    {
-        reader.close();
-    }
-
-    public boolean isExhausted()
-    {
-        return exhausted;
-    }
-
-    public DataInputBuffer getBufIn()
-    {
-        return bufIn;
-    }
-
-    public String getKey()
-    {
-        return key;
-    }
-
-    public int compareTo(FileStruct f)
-    {
-        return sstable.getPartitioner().getDecoratedKeyComparator().compare(key, f.key);
-    }    
-
-    public void seekTo(String seekKey)
-    {
-        try
-        {
-            long position = sstable.getNearestPosition(seekKey);
-            if (position < 0)
-            {
-                exhausted = true;
-                return;
-            }
-            reader.seek(position);
-            advance();
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException("corrupt sstable", e);
-        }
-    }
-
-    /*
-     * Read the next key from the data file, skipping block indexes.
-     * Caller must check isExhausted after each call to see if further
-     * reads are valid.
-     * Do not mix with calls to the iterator interface (next/hasnext).
-     * @deprecated -- prefer the iterator interface.
-     */
-    public void advance() throws IOException
-    {
-        if (exhausted)
-        {
-            throw new IndexOutOfBoundsException();
-        }
-
-        bufOut.reset();
-        if (reader.isEOF())
-        {
-            reader.close();
-            exhausted = true;
-            return;
-        }
-
-        long bytesread = reader.next(bufOut);
-        if (bytesread == -1)
-        {
-            reader.close();
-            exhausted = true;
-            return;
-        }
-
-        bufIn.reset(bufOut.getData(), bufOut.getLength());
-        key = bufIn.readUTF();
-    }
-
-    public boolean hasNext()
-    {
-        if (iterator == null)
-            iterator = new FileStructIterator();
-        return iterator.hasNext();
-    }
-
-    /** do not mix with manual calls to advance(). */
-    public String next()
-    {
-        if (iterator == null)
-            iterator = new FileStructIterator();
-        return iterator.next();
-    }
-
-    public void remove()
-    {
-        throw new UnsupportedOperationException();
-    }
-
-    private class FileStructIterator extends AbstractIterator<String>
-    {
-        public FileStructIterator()
-        {
-            if (key == null)
-            {
-                if (!isExhausted())
-                {
-                    forward();
-                }
-            }
-        }
-
-        private void forward()
-        {
-            try
-            {
-                advance();
-            }
-            catch (IOException e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-
-        protected String computeNext()
-        {
-            if (isExhausted())
-            {
-                return endOfData();
-            }
-            String oldKey = key;
-            forward();
-            return oldKey;
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.IOException;
+import java.util.Iterator;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.io.IFileReader;
+import org.apache.cassandra.io.SSTableReader;
+
+import org.apache.log4j.Logger;
+import com.google.common.collect.AbstractIterator;
+
+
+public class FileStruct implements Comparable<FileStruct>, Iterator<String>
+{
+    private static Logger logger = Logger.getLogger(FileStruct.class);
+
+    private String key = null; // decorated!
+    private boolean exhausted = false;
+    private IFileReader reader;
+    private DataInputBuffer bufIn;
+    private DataOutputBuffer bufOut;
+    private SSTableReader sstable;
+    private FileStructIterator iterator;
+
+    FileStruct(SSTableReader sstable) throws IOException
+    {
+        this.reader = SequenceFile.bufferedReader(sstable.getFilename(), 1024 * 1024);
+        this.sstable = sstable;
+        bufIn = new DataInputBuffer();
+        bufOut = new DataOutputBuffer();
+    }
+
+    public String getFileName()
+    {
+        return reader.getFileName();
+    }
+
+    public void close() throws IOException
+    {
+        reader.close();
+    }
+
+    public boolean isExhausted()
+    {
+        return exhausted;
+    }
+
+    public DataInputBuffer getBufIn()
+    {
+        return bufIn;
+    }
+
+    public String getKey()
+    {
+        return key;
+    }
+
+    public int compareTo(FileStruct f)
+    {
+        return sstable.getPartitioner().getDecoratedKeyComparator().compare(key, f.key);
+    }    
+
+    public void seekTo(String seekKey)
+    {
+        try
+        {
+            long position = sstable.getNearestPosition(seekKey);
+            if (position < 0)
+            {
+                exhausted = true;
+                return;
+            }
+            reader.seek(position);
+            advance();
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException("corrupt sstable", e);
+        }
+    }
+
+    /*
+     * Read the next key from the data file, skipping block indexes.
+     * Caller must check isExhausted after each call to see if further
+     * reads are valid.
+     * Do not mix with calls to the iterator interface (next/hasnext).
+     * @deprecated -- prefer the iterator interface.
+     */
+    public void advance() throws IOException
+    {
+        if (exhausted)
+        {
+            throw new IndexOutOfBoundsException();
+        }
+
+        bufOut.reset();
+        if (reader.isEOF())
+        {
+            reader.close();
+            exhausted = true;
+            return;
+        }
+
+        long bytesread = reader.next(bufOut);
+        if (bytesread == -1)
+        {
+            reader.close();
+            exhausted = true;
+            return;
+        }
+
+        bufIn.reset(bufOut.getData(), bufOut.getLength());
+        key = bufIn.readUTF();
+    }
+
+    public boolean hasNext()
+    {
+        if (iterator == null)
+            iterator = new FileStructIterator();
+        return iterator.hasNext();
+    }
+
+    /** do not mix with manual calls to advance(). */
+    public String next()
+    {
+        if (iterator == null)
+            iterator = new FileStructIterator();
+        return iterator.next();
+    }
+
+    public void remove()
+    {
+        throw new UnsupportedOperationException();
+    }
+
+    private class FileStructIterator extends AbstractIterator<String>
+    {
+        public FileStructIterator()
+        {
+            if (key == null)
+            {
+                if (!isExhausted())
+                {
+                    forward();
+                }
+            }
+        }
+
+        private void forward()
+        {
+            try
+            {
+                advance();
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+
+        protected String computeNext()
+        {
+            if (isExhausted())
+            {
+                return endOfData();
+            }
+            String oldKey = key;
+            forward();
+            return oldKey;
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/io/ICompactSerializer.java b/src/java/org/apache/cassandra/io/ICompactSerializer.java
index 3ed5fcbab5..19e442289d 100644
--- a/src/java/org/apache/cassandra/io/ICompactSerializer.java
+++ b/src/java/org/apache/cassandra/io/ICompactSerializer.java
@@ -1,48 +1,48 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.DataOutput;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.DataInputStream;
-
-/**
- * Allows for the controlled serialization/deserialization of a given type.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface ICompactSerializer<T>
-{
-	/**
-     * Serialize the specified type into the specified DataOutputStream instance.
-     * @param t type that needs to be serialized
-     * @param dos DataOutput into which serialization needs to happen.
-     * @throws IOException
-     */
-    public void serialize(T t, DataOutputStream dos) throws IOException;
-
-    /**
-     * Deserialize into the specified DataInputStream instance.
-     * @param dis DataInput from which deserialization needs to happen.
-     * @throws IOException
-     * @return the type that was deserialized
-     */
-    public T deserialize(DataInputStream dis) throws IOException;
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.DataOutput;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.DataInputStream;
+
+/**
+ * Allows for the controlled serialization/deserialization of a given type.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface ICompactSerializer<T>
+{
+	/**
+     * Serialize the specified type into the specified DataOutputStream instance.
+     * @param t type that needs to be serialized
+     * @param dos DataOutput into which serialization needs to happen.
+     * @throws IOException
+     */
+    public void serialize(T t, DataOutputStream dos) throws IOException;
+
+    /**
+     * Deserialize into the specified DataInputStream instance.
+     * @param dis DataInput from which deserialization needs to happen.
+     * @throws IOException
+     * @return the type that was deserialized
+     */
+    public T deserialize(DataInputStream dis) throws IOException;
+}
diff --git a/src/java/org/apache/cassandra/io/IFileReader.java b/src/java/org/apache/cassandra/io/IFileReader.java
index 4ea0ff5f9a..0710da6286 100644
--- a/src/java/org/apache/cassandra/io/IFileReader.java
+++ b/src/java/org/apache/cassandra/io/IFileReader.java
@@ -1,83 +1,83 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.SortedSet;
-
-/**
- * Interface to read from the SequenceFile abstraction.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IFileReader
-{
-    public String getFileName();
-    public long getEOF() throws IOException;
-    public long getCurrentPosition() throws IOException;
-    public boolean isHealthyFileDescriptor() throws IOException;
-    public void seek(long position) throws IOException;
-    public boolean isEOF() throws IOException;
-
-    /**
-     * Be extremely careful while using this API. This currently
-     * used to read the commit log header from the commit logs.
-     * Treat this as an internal API.
-     * 
-     * @param bytes read into this byte array.
-    */
-    public void readDirect(byte[] bytes) throws IOException;
-    
-    /**
-     * Read a long value from the underlying sub system.
-     * @return value read
-     * @throws IOException
-     */
-    public long readLong() throws IOException;
-        
-    /**
-     * This method dumps the next key/value into the DataOuputStream
-     * passed in.
-     *
-     * @param bufOut DataOutputStream that needs to be filled.
-     * @return number of bytes read.
-     * @throws IOException 
-    */
-    public long next(DataOutputBuffer bufOut) throws IOException;
-
-    /**
-     * This method dumps the next key/value into the DataOuputStream
-     * passed in. Always use this method to query for application
-     * specific data as it will have indexes.
-     *
-     * @param key - key we are interested in.
-     * @param bufOut - DataOutputStream that needs to be filled.
-     * @param columnFamilyName The name of the column family only without the ":"
-     * @param columnNames - The list of columns in the cfName column family
-     * 					     that we want to return
-    */
-    public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, SortedSet<byte[]> columnNames, long position) throws IOException;
-
-    /**
-     * Close the file after reading.
-     * @throws IOException
-     */
-    public void close() throws IOException;
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.SortedSet;
+
+/**
+ * Interface to read from the SequenceFile abstraction.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IFileReader
+{
+    public String getFileName();
+    public long getEOF() throws IOException;
+    public long getCurrentPosition() throws IOException;
+    public boolean isHealthyFileDescriptor() throws IOException;
+    public void seek(long position) throws IOException;
+    public boolean isEOF() throws IOException;
+
+    /**
+     * Be extremely careful while using this API. This currently
+     * used to read the commit log header from the commit logs.
+     * Treat this as an internal API.
+     * 
+     * @param bytes read into this byte array.
+    */
+    public void readDirect(byte[] bytes) throws IOException;
+    
+    /**
+     * Read a long value from the underlying sub system.
+     * @return value read
+     * @throws IOException
+     */
+    public long readLong() throws IOException;
+        
+    /**
+     * This method dumps the next key/value into the DataOuputStream
+     * passed in.
+     *
+     * @param bufOut DataOutputStream that needs to be filled.
+     * @return number of bytes read.
+     * @throws IOException 
+    */
+    public long next(DataOutputBuffer bufOut) throws IOException;
+
+    /**
+     * This method dumps the next key/value into the DataOuputStream
+     * passed in. Always use this method to query for application
+     * specific data as it will have indexes.
+     *
+     * @param key - key we are interested in.
+     * @param bufOut - DataOutputStream that needs to be filled.
+     * @param columnFamilyName The name of the column family only without the ":"
+     * @param columnNames - The list of columns in the cfName column family
+     * 					     that we want to return
+    */
+    public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, SortedSet<byte[]> columnNames, long position) throws IOException;
+
+    /**
+     * Close the file after reading.
+     * @throws IOException
+     */
+    public void close() throws IOException;
+}
diff --git a/src/java/org/apache/cassandra/io/IndexHelper.java b/src/java/org/apache/cassandra/io/IndexHelper.java
index ca2c71d22b..4c82fdf8f1 100644
--- a/src/java/org/apache/cassandra/io/IndexHelper.java
+++ b/src/java/org/apache/cassandra/io/IndexHelper.java
@@ -1,355 +1,355 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.DataInput;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.*;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.ColumnSerializer;
-import org.apache.cassandra.db.marshal.AbstractType;
-
-
-/**
- * Provides helper to serialize, deserialize and use column indexes.
- * Author : Karthik Ranganathan ( kranganathan@facebook.com )
- */
-
-public class IndexHelper
-{
-	/**
-	 * Serializes a column index to a data output stream
-	 * @param indexSizeInBytes Size of index to be written
-	 * @param columnIndexList List of column index entries as objects
-	 * @param dos the output stream into which the column index is to be written
-	 * @throws IOException
-	 */
-	public static void serialize(int indexSizeInBytes, List<ColumnIndexInfo> columnIndexList, DataOutputStream dos) throws IOException
-	{
-		/* if we have no data to index, the write that there is no index present */
-		if(indexSizeInBytes == 0 || columnIndexList == null || columnIndexList.size() == 0)
-		{
-			dos.writeBoolean(false);
-		}
-		else
-		{
-	        /* write if we are storing a column index */
-	    	dos.writeBoolean(true);
-	    	/* write the size of the index */
-	    	dos.writeInt(indexSizeInBytes);
-	        for( ColumnIndexInfo cIndexInfo : columnIndexList )
-	        {
-	        	cIndexInfo.serialize(dos);
-	        }
-		}
-	}
-    
-    /**
-     * Skip the bloom filter and the index and return the bytes read.
-     * @param in the data input from which the bloom filter and index 
-     *           should be skipped
-     * @return number of bytes read.
-     * @throws IOException
-     */
-    public static int skipBloomFilterAndIndex(DataInput in) throws IOException
-    {
-        int totalBytesRead = 0;
-        /* size of the bloom filter */
-        int size = in.readInt();
-        totalBytesRead += 4;
-        /* skip the serialized bloom filter */
-        in.skipBytes(size);
-        totalBytesRead += size;
-        /* skip the index on disk */
-        /* read if the file has column indexes */
-        boolean hasColumnIndexes = in.readBoolean();
-        totalBytesRead += 1;
-        if ( hasColumnIndexes )
-        {
-            totalBytesRead += skipIndex(in);
-        }
-        return totalBytesRead;
-    }
-    
-    /**
-     * Skip the bloom filter and return the bytes read.
-     * @param in the data input from which the bloom filter 
-     *           should be skipped
-     * @return number of bytes read.
-     * @throws IOException
-     */
-    public static int skipBloomFilter(DataInput in) throws IOException
-    {
-        int totalBytesRead = 0;
-        /* size of the bloom filter */
-        int size = in.readInt();
-        totalBytesRead += 4;
-        /* skip the serialized bloom filter */
-        in.skipBytes(size);
-        totalBytesRead += size;
-        return totalBytesRead;
-    }
-
-	/**
-	 * Skip the index and return the number of bytes read.
-	 * @param file the data input from which the index should be skipped
-	 * @return number of bytes read from the data input
-	 * @throws IOException
-	 */
-	public static int skipIndex(DataInput file) throws IOException
-	{
-        /* read only the column index list */
-        int columnIndexSize = file.readInt();
-        int totalBytesRead = 4;
-
-        /* skip the column index data */
-        file.skipBytes(columnIndexSize);
-        totalBytesRead += columnIndexSize;
-
-        return totalBytesRead;
-	}
-    
-    /**
-     * Deserialize the index into a structure and return the number of bytes read.
-     * @param tableName
-     *@param in Input from which the serialized form of the index is read
-     * @param columnIndexList the structure which is filled in with the deserialized index   @return number of bytes read from the input
-     * @throws IOException
-     */
-	static int deserializeIndex(String tableName, String cfName, DataInput in, List<ColumnIndexInfo> columnIndexList) throws IOException
-	{
-		/* read only the column index list */
-		int columnIndexSize = in.readInt();
-		int totalBytesRead = 4;
-
-		/* read the indexes into a separate buffer */
-		DataOutputBuffer indexOut = new DataOutputBuffer();
-        /* write the data into buffer */
-		indexOut.write(in, columnIndexSize);
-		totalBytesRead += columnIndexSize;
-
-		/* now deserialize the index list */
-        DataInputBuffer indexIn = new DataInputBuffer();
-        indexIn.reset(indexOut.getData(), indexOut.getLength());
-        
-        AbstractType comparator = DatabaseDescriptor.getComparator(tableName, cfName);
-
-        while (indexIn.available() > 0)
-        {
-            // TODO this is all kinds of messed up
-            ColumnIndexInfo cIndexInfo = new ColumnIndexInfo(comparator);
-            cIndexInfo = cIndexInfo.deserialize(indexIn);
-            columnIndexList.add(cIndexInfo);
-        }
-
-        return totalBytesRead;
-	}
-
-    /**
-     * Returns the range in which a given column falls in the index
-     * @param columnIndexList the in-memory representation of the column index
-     * @param dataSize the total size of the data
-     * @param totalNumCols total number of columns
-     * @return an object describing a subrange in which the column is serialized
-     */
-	static ColumnRange getColumnRangeFromNameIndex(IndexHelper.ColumnIndexInfo cIndexInfo, List<IndexHelper.ColumnIndexInfo> columnIndexList, int dataSize, int totalNumCols)
-	{
-		/* find the offset for the column */
-        int size = columnIndexList.size();
-        long start = 0;
-        long end = dataSize;
-        int numColumns = 0;      
-       
-        int index = Collections.binarySearch(columnIndexList, cIndexInfo);
-        if ( index < 0 )
-        {
-            /* We are here which means that the requested column is not an index. */
-            index = (++index)*(-1);
-        }
-        else
-        {
-        	++index;
-        }
-
-        /* calculate the starting offset from which we have to read */
-        start = (index == 0) ? 0 : columnIndexList.get(index - 1).position();
-
-        if( index < size )
-        {
-        	end = columnIndexList.get(index).position();
-            numColumns = columnIndexList.get(index).count();            
-        }
-        else
-        {
-        	end = dataSize;  
-            int totalColsIndexed = 0;
-            for( IndexHelper.ColumnIndexInfo colPosInfo : columnIndexList )
-            {
-                totalColsIndexed += colPosInfo.count();
-            }
-            numColumns = totalNumCols - totalColsIndexed;
-        }
-
-        return new ColumnRange(start, end, numColumns);
-	}
-
-	/**
-	 * Returns the sub-ranges that contain the list of columns in columnNames.
-	 * @param columnNames The list of columns whose subranges need to be found
-	 * @param columnIndexList the deserialized column indexes
-	 * @param dataSize the total size of data
-	 * @param totalNumCols the total number of columns
-	 * @return a list of subranges which contain all the columns in columnNames
-	 */
-	static List<ColumnRange> getMultiColumnRangesFromNameIndex(SortedSet<byte[]> columnNames, List<IndexHelper.ColumnIndexInfo> columnIndexList, int dataSize, int totalNumCols)
-	{
-		List<ColumnRange> columnRanges = new ArrayList<ColumnRange>();
-
-        if (columnIndexList.size() == 0)
-        {
-            columnRanges.add(new ColumnRange(0, dataSize, totalNumCols));
-        }
-        else
-        {
-            Map<Long, Boolean> offset = new HashMap<Long, Boolean>();
-            for (byte[] name : columnNames)
-            {
-                IndexHelper.ColumnIndexInfo cIndexInfo = new IndexHelper.ColumnIndexInfo(name, 0, 0, (AbstractType)columnNames.comparator());
-                ColumnRange columnRange = getColumnRangeFromNameIndex(cIndexInfo, columnIndexList, dataSize, totalNumCols);
-                if (offset.get(columnRange.coordinate().start_) == null)
-                {
-                    columnRanges.add(columnRange);
-                    offset.put(columnRange.coordinate().start_, true);
-                }
-            }
-        }
-
-        return columnRanges;
-	}
-
-
-    /**
-     * A column range containing the start and end
-     * offset of the appropriate column index chunk
-     * and the number of columns in that chunk.
-     * @author alakshman
-     *
-     */
-    public static class ColumnRange
-    {
-        private Coordinate coordinate_;
-        private int columnCount_;
-        
-        ColumnRange(long start, long end, int columnCount)
-        {
-            coordinate_ = new Coordinate(start, end);
-            columnCount_ = columnCount;
-        }
-        
-        Coordinate coordinate()
-        {
-            return coordinate_;
-        }
-        
-        int count()
-        {
-            return columnCount_;
-        }                
-    }
-
-	/**
-	 * A helper class to generate indexes while
-     * the columns are sorted by name on disk.
-	*/
-    public static class ColumnIndexInfo implements Comparable<ColumnIndexInfo>
-    {
-        private long position_;
-        private int columnCount_;
-        private byte[] name_;
-        private AbstractType comparator_;
-
-        public ColumnIndexInfo(AbstractType comparator_)
-        {
-            this.comparator_ = comparator_;
-        }
-
-        public ColumnIndexInfo(byte[] name, long position, int columnCount, AbstractType comparator)
-        {
-            this(comparator);
-            assert name.length == 0 || !"".equals(comparator.getString(name)); // Todo r/m length == 0 hack
-            name_ = name;
-            position_ = position;
-            columnCount_ = columnCount;
-        }
-                
-        public long position()
-        {
-            return position_;
-        }
-        
-        public void position(long position)
-        {
-            position_ = position;
-        }
-        
-        int count()
-        {
-            return columnCount_;
-        }
-        
-        public void count(int count)
-        {
-            columnCount_ = count;
-        }
-
-        public int compareTo(ColumnIndexInfo rhs)
-        {
-            return comparator_.compare(name_, rhs.name_);
-        }
-
-        public void serialize(DataOutputStream dos) throws IOException
-        {
-            dos.writeLong(position());
-            dos.writeInt(count());
-            ColumnSerializer.writeName(name_, dos);
-        }
-
-        public ColumnIndexInfo deserialize(DataInputStream dis) throws IOException
-        {
-            long position = dis.readLong();
-            int columnCount = dis.readInt();
-            byte[] name = ColumnSerializer.readName(dis);
-            return new ColumnIndexInfo(name, position, columnCount, comparator_);
-        }
-
-        public int size()
-        {
-            // serialized size -- CS.writeName includes a 2-byte length prefix
-            return 8 + 4 + 2 + name_.length;
-        }
-
-        public byte[] name()
-        {
-            return name_;
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.DataInput;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnSerializer;
+import org.apache.cassandra.db.marshal.AbstractType;
+
+
+/**
+ * Provides helper to serialize, deserialize and use column indexes.
+ * Author : Karthik Ranganathan ( kranganathan@facebook.com )
+ */
+
+public class IndexHelper
+{
+	/**
+	 * Serializes a column index to a data output stream
+	 * @param indexSizeInBytes Size of index to be written
+	 * @param columnIndexList List of column index entries as objects
+	 * @param dos the output stream into which the column index is to be written
+	 * @throws IOException
+	 */
+	public static void serialize(int indexSizeInBytes, List<ColumnIndexInfo> columnIndexList, DataOutputStream dos) throws IOException
+	{
+		/* if we have no data to index, the write that there is no index present */
+		if(indexSizeInBytes == 0 || columnIndexList == null || columnIndexList.size() == 0)
+		{
+			dos.writeBoolean(false);
+		}
+		else
+		{
+	        /* write if we are storing a column index */
+	    	dos.writeBoolean(true);
+	    	/* write the size of the index */
+	    	dos.writeInt(indexSizeInBytes);
+	        for( ColumnIndexInfo cIndexInfo : columnIndexList )
+	        {
+	        	cIndexInfo.serialize(dos);
+	        }
+		}
+	}
+    
+    /**
+     * Skip the bloom filter and the index and return the bytes read.
+     * @param in the data input from which the bloom filter and index 
+     *           should be skipped
+     * @return number of bytes read.
+     * @throws IOException
+     */
+    public static int skipBloomFilterAndIndex(DataInput in) throws IOException
+    {
+        int totalBytesRead = 0;
+        /* size of the bloom filter */
+        int size = in.readInt();
+        totalBytesRead += 4;
+        /* skip the serialized bloom filter */
+        in.skipBytes(size);
+        totalBytesRead += size;
+        /* skip the index on disk */
+        /* read if the file has column indexes */
+        boolean hasColumnIndexes = in.readBoolean();
+        totalBytesRead += 1;
+        if ( hasColumnIndexes )
+        {
+            totalBytesRead += skipIndex(in);
+        }
+        return totalBytesRead;
+    }
+    
+    /**
+     * Skip the bloom filter and return the bytes read.
+     * @param in the data input from which the bloom filter 
+     *           should be skipped
+     * @return number of bytes read.
+     * @throws IOException
+     */
+    public static int skipBloomFilter(DataInput in) throws IOException
+    {
+        int totalBytesRead = 0;
+        /* size of the bloom filter */
+        int size = in.readInt();
+        totalBytesRead += 4;
+        /* skip the serialized bloom filter */
+        in.skipBytes(size);
+        totalBytesRead += size;
+        return totalBytesRead;
+    }
+
+	/**
+	 * Skip the index and return the number of bytes read.
+	 * @param file the data input from which the index should be skipped
+	 * @return number of bytes read from the data input
+	 * @throws IOException
+	 */
+	public static int skipIndex(DataInput file) throws IOException
+	{
+        /* read only the column index list */
+        int columnIndexSize = file.readInt();
+        int totalBytesRead = 4;
+
+        /* skip the column index data */
+        file.skipBytes(columnIndexSize);
+        totalBytesRead += columnIndexSize;
+
+        return totalBytesRead;
+	}
+    
+    /**
+     * Deserialize the index into a structure and return the number of bytes read.
+     * @param tableName
+     *@param in Input from which the serialized form of the index is read
+     * @param columnIndexList the structure which is filled in with the deserialized index   @return number of bytes read from the input
+     * @throws IOException
+     */
+	static int deserializeIndex(String tableName, String cfName, DataInput in, List<ColumnIndexInfo> columnIndexList) throws IOException
+	{
+		/* read only the column index list */
+		int columnIndexSize = in.readInt();
+		int totalBytesRead = 4;
+
+		/* read the indexes into a separate buffer */
+		DataOutputBuffer indexOut = new DataOutputBuffer();
+        /* write the data into buffer */
+		indexOut.write(in, columnIndexSize);
+		totalBytesRead += columnIndexSize;
+
+		/* now deserialize the index list */
+        DataInputBuffer indexIn = new DataInputBuffer();
+        indexIn.reset(indexOut.getData(), indexOut.getLength());
+        
+        AbstractType comparator = DatabaseDescriptor.getComparator(tableName, cfName);
+
+        while (indexIn.available() > 0)
+        {
+            // TODO this is all kinds of messed up
+            ColumnIndexInfo cIndexInfo = new ColumnIndexInfo(comparator);
+            cIndexInfo = cIndexInfo.deserialize(indexIn);
+            columnIndexList.add(cIndexInfo);
+        }
+
+        return totalBytesRead;
+	}
+
+    /**
+     * Returns the range in which a given column falls in the index
+     * @param columnIndexList the in-memory representation of the column index
+     * @param dataSize the total size of the data
+     * @param totalNumCols total number of columns
+     * @return an object describing a subrange in which the column is serialized
+     */
+	static ColumnRange getColumnRangeFromNameIndex(IndexHelper.ColumnIndexInfo cIndexInfo, List<IndexHelper.ColumnIndexInfo> columnIndexList, int dataSize, int totalNumCols)
+	{
+		/* find the offset for the column */
+        int size = columnIndexList.size();
+        long start = 0;
+        long end = dataSize;
+        int numColumns = 0;      
+       
+        int index = Collections.binarySearch(columnIndexList, cIndexInfo);
+        if ( index < 0 )
+        {
+            /* We are here which means that the requested column is not an index. */
+            index = (++index)*(-1);
+        }
+        else
+        {
+        	++index;
+        }
+
+        /* calculate the starting offset from which we have to read */
+        start = (index == 0) ? 0 : columnIndexList.get(index - 1).position();
+
+        if( index < size )
+        {
+        	end = columnIndexList.get(index).position();
+            numColumns = columnIndexList.get(index).count();            
+        }
+        else
+        {
+        	end = dataSize;  
+            int totalColsIndexed = 0;
+            for( IndexHelper.ColumnIndexInfo colPosInfo : columnIndexList )
+            {
+                totalColsIndexed += colPosInfo.count();
+            }
+            numColumns = totalNumCols - totalColsIndexed;
+        }
+
+        return new ColumnRange(start, end, numColumns);
+	}
+
+	/**
+	 * Returns the sub-ranges that contain the list of columns in columnNames.
+	 * @param columnNames The list of columns whose subranges need to be found
+	 * @param columnIndexList the deserialized column indexes
+	 * @param dataSize the total size of data
+	 * @param totalNumCols the total number of columns
+	 * @return a list of subranges which contain all the columns in columnNames
+	 */
+	static List<ColumnRange> getMultiColumnRangesFromNameIndex(SortedSet<byte[]> columnNames, List<IndexHelper.ColumnIndexInfo> columnIndexList, int dataSize, int totalNumCols)
+	{
+		List<ColumnRange> columnRanges = new ArrayList<ColumnRange>();
+
+        if (columnIndexList.size() == 0)
+        {
+            columnRanges.add(new ColumnRange(0, dataSize, totalNumCols));
+        }
+        else
+        {
+            Map<Long, Boolean> offset = new HashMap<Long, Boolean>();
+            for (byte[] name : columnNames)
+            {
+                IndexHelper.ColumnIndexInfo cIndexInfo = new IndexHelper.ColumnIndexInfo(name, 0, 0, (AbstractType)columnNames.comparator());
+                ColumnRange columnRange = getColumnRangeFromNameIndex(cIndexInfo, columnIndexList, dataSize, totalNumCols);
+                if (offset.get(columnRange.coordinate().start_) == null)
+                {
+                    columnRanges.add(columnRange);
+                    offset.put(columnRange.coordinate().start_, true);
+                }
+            }
+        }
+
+        return columnRanges;
+	}
+
+
+    /**
+     * A column range containing the start and end
+     * offset of the appropriate column index chunk
+     * and the number of columns in that chunk.
+     * @author alakshman
+     *
+     */
+    public static class ColumnRange
+    {
+        private Coordinate coordinate_;
+        private int columnCount_;
+        
+        ColumnRange(long start, long end, int columnCount)
+        {
+            coordinate_ = new Coordinate(start, end);
+            columnCount_ = columnCount;
+        }
+        
+        Coordinate coordinate()
+        {
+            return coordinate_;
+        }
+        
+        int count()
+        {
+            return columnCount_;
+        }                
+    }
+
+	/**
+	 * A helper class to generate indexes while
+     * the columns are sorted by name on disk.
+	*/
+    public static class ColumnIndexInfo implements Comparable<ColumnIndexInfo>
+    {
+        private long position_;
+        private int columnCount_;
+        private byte[] name_;
+        private AbstractType comparator_;
+
+        public ColumnIndexInfo(AbstractType comparator_)
+        {
+            this.comparator_ = comparator_;
+        }
+
+        public ColumnIndexInfo(byte[] name, long position, int columnCount, AbstractType comparator)
+        {
+            this(comparator);
+            assert name.length == 0 || !"".equals(comparator.getString(name)); // Todo r/m length == 0 hack
+            name_ = name;
+            position_ = position;
+            columnCount_ = columnCount;
+        }
+                
+        public long position()
+        {
+            return position_;
+        }
+        
+        public void position(long position)
+        {
+            position_ = position;
+        }
+        
+        int count()
+        {
+            return columnCount_;
+        }
+        
+        public void count(int count)
+        {
+            columnCount_ = count;
+        }
+
+        public int compareTo(ColumnIndexInfo rhs)
+        {
+            return comparator_.compare(name_, rhs.name_);
+        }
+
+        public void serialize(DataOutputStream dos) throws IOException
+        {
+            dos.writeLong(position());
+            dos.writeInt(count());
+            ColumnSerializer.writeName(name_, dos);
+        }
+
+        public ColumnIndexInfo deserialize(DataInputStream dis) throws IOException
+        {
+            long position = dis.readLong();
+            int columnCount = dis.readInt();
+            byte[] name = ColumnSerializer.readName(dis);
+            return new ColumnIndexInfo(name, position, columnCount, comparator_);
+        }
+
+        public int size()
+        {
+            // serialized size -- CS.writeName includes a 2-byte length prefix
+            return 8 + 4 + 2 + name_.length;
+        }
+
+        public byte[] name()
+        {
+            return name_;
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/io/SSTable.java b/src/java/org/apache/cassandra/io/SSTable.java
index 6948a576c3..335a60e1ae 100644
--- a/src/java/org/apache/cassandra/io/SSTable.java
+++ b/src/java/org/apache/cassandra/io/SSTable.java
@@ -69,7 +69,7 @@ public abstract class SSTable
 
     static String parseTableName(String filename)
     {
-        return new File(filename).getParentFile().getName();        
+        return new File(filename).getParentFile().getName();        
     }
 
     /**
diff --git a/src/java/org/apache/cassandra/io/SSTableReader.java b/src/java/org/apache/cassandra/io/SSTableReader.java
index 029eee7ef0..fc804c5a35 100644
--- a/src/java/org/apache/cassandra/io/SSTableReader.java
+++ b/src/java/org/apache/cassandra/io/SSTableReader.java
@@ -1,434 +1,434 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.*;
-import java.util.*;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.io.SequenceFile.ColumnGroupReader;
-import org.apache.cassandra.utils.BloomFilter;
-import org.apache.cassandra.utils.FileUtils;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.cliffc.high_scale_lib.NonBlockingHashMap;
-import com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap;
-
-public class SSTableReader extends SSTable
-{
-    private static Logger logger = Logger.getLogger(SSTableReader.class);
-
-    private static FileSSTableMap openedFiles = new FileSSTableMap();
-
-    public static int indexInterval()
-    {
-        return INDEX_INTERVAL;
-    }
-
-    // todo can we refactor to take list of sstables?
-    public static int getApproximateKeyCount(List<String> dataFiles)
-    {
-        int count = 0;
-
-        for (String dataFileName : dataFiles)
-        {
-            SSTableReader sstable = openedFiles.get(dataFileName);
-            assert sstable != null;
-            int indexKeyCount = sstable.getIndexPositions().size();
-            count = count + (indexKeyCount + 1) * INDEX_INTERVAL;
-            if (logger.isDebugEnabled())
-                logger.debug("index size for bloom filter calc for file  : " + dataFileName + "   : " + count);
-        }
-
-        return count;
-    }
-
-    /**
-     * Get all indexed keys in the SSTable.
-     */
-    public static List<String> getIndexedKeys()
-    {
-        List<String> indexedKeys = new ArrayList<String>();
-
-        for (SSTableReader sstable : openedFiles.values())
-        {
-            for (KeyPosition kp : sstable.getIndexPositions())
-            {
-                indexedKeys.add(kp.key);
-            }
-        }
-        Collections.sort(indexedKeys);
-
-        return indexedKeys;
-    }
-
-    public static synchronized SSTableReader open(String dataFileName) throws IOException
-    {
-        return open(dataFileName, StorageService.getPartitioner(), DatabaseDescriptor.getKeysCachedFraction(parseTableName(dataFileName)));
-    }
-
-    public static synchronized SSTableReader open(String dataFileName, IPartitioner partitioner, double cacheFraction) throws IOException
-    {
-        SSTableReader sstable = openedFiles.get(dataFileName);
-        if (sstable == null)
-        {
-            assert partitioner != null;
-            sstable = new SSTableReader(dataFileName, partitioner);
-
-            long start = System.currentTimeMillis();
-            sstable.loadIndexFile();
-            sstable.loadBloomFilter();
-            if (cacheFraction > 0)
-            {
-                sstable.keyCache = createKeyCache((int)((sstable.getIndexPositions().size() + 1) * INDEX_INTERVAL * cacheFraction));
-            }
-            if (logger.isDebugEnabled())
-                logger.debug("INDEX LOAD TIME for "  + dataFileName + ": " + (System.currentTimeMillis() - start) + " ms.");
-
-            openedFiles.put(dataFileName, sstable);
-        }
-        return sstable;
-    }
-
-    public static synchronized SSTableReader get(String dataFileName) throws IOException
-    {
-        SSTableReader sstable = openedFiles.get(dataFileName);
-        assert sstable != null;
-        return sstable;
-    }
-
-    public static ConcurrentLinkedHashMap<String, Long> createKeyCache(int size)
-    {
-        return ConcurrentLinkedHashMap.create(ConcurrentLinkedHashMap.EvictionPolicy.SECOND_CHANCE, size);
-    }
-
-
-    private ConcurrentLinkedHashMap<String, Long> keyCache;
-
-    SSTableReader(String filename, IPartitioner partitioner, List<KeyPosition> indexPositions, BloomFilter bloomFilter, ConcurrentLinkedHashMap<String, Long> keyCache)
-    {
-        super(filename, partitioner);
-        this.indexPositions = indexPositions;
-        this.bf = bloomFilter;
-        this.keyCache = keyCache;
-        synchronized (SSTableReader.this)
-        {
-            openedFiles.put(filename, this);
-        }
-    }
-
-    private SSTableReader(String filename, IPartitioner partitioner)
-    {
-        super(filename, partitioner);
-    }
-
-    public List<KeyPosition> getIndexPositions()
-    {
-        return indexPositions;
-    }
-
-    private void loadBloomFilter() throws IOException
-    {
-        DataInputStream stream = new DataInputStream(new FileInputStream(filterFilename()));
-        bf = BloomFilter.serializer().deserialize(stream);
-    }
-
-    private void loadIndexFile() throws IOException
-    {
-        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(), "r");
-        indexPositions = new ArrayList<KeyPosition>();
-
-        int i = 0;
-        long indexSize = input.length();
-        while (true)
-        {
-            long indexPosition = input.getFilePointer();
-            if (indexPosition == indexSize)
-            {
-                break;
-            }
-            String decoratedKey = input.readUTF();
-            input.readLong();
-            if (i++ % INDEX_INTERVAL == 0)
-            {
-                indexPositions.add(new KeyPosition(decoratedKey, indexPosition));
-            }
-        }
-    }
-
-    /** get the position in the index file to start scanning to find the given key (at most indexInterval keys away) */
-    private long getIndexScanPosition(String decoratedKey, IPartitioner partitioner)
-    {
-        assert indexPositions != null && indexPositions.size() > 0;
-        int index = Collections.binarySearch(indexPositions, new KeyPosition(decoratedKey, -1));
-        if (index < 0)
-        {
-            // binary search gives us the first index _greater_ than the key searched for,
-            // i.e., its insertion position
-            int greaterThan = (index + 1) * -1;
-            if (greaterThan == 0)
-                return -1;
-            return indexPositions.get(greaterThan - 1).position;
-        }
-        else
-        {
-            return indexPositions.get(index).position;
-        }
-    }
-
-    /**
-     * returns the position in the data file to find the given key, or -1 if the key is not present
-     */
-    public long getPosition(String decoratedKey, IPartitioner partitioner) throws IOException
-    {
-        if (!bf.isPresent(decoratedKey))
-            return -1;
-        if (keyCache != null)
-        {
-            Long cachedPosition = keyCache.get(decoratedKey);
-            if (cachedPosition != null)
-            {
-                return cachedPosition;
-            }
-        }
-        long start = getIndexScanPosition(decoratedKey, partitioner);
-        if (start < 0)
-        {
-            return -1;
-        }
-
-        // TODO mmap the index file?
-        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(dataFile), "r");
-        input.seek(start);
-        int i = 0;
-        try
-        {
-            do
-            {
-                String indexDecoratedKey;
-                try
-                {
-                    indexDecoratedKey = input.readUTF();
-                }
-                catch (EOFException e)
-                {
-                    return -1;
-                }
-                long position = input.readLong();
-                int v = partitioner.getDecoratedKeyComparator().compare(indexDecoratedKey, decoratedKey);
-                if (v == 0)
-                {
-                    if (keyCache != null)
-                        keyCache.put(decoratedKey, position);
-                    return position;
-                }
-                if (v > 0)
-                    return -1;
-            } while  (++i < INDEX_INTERVAL);
-        }
-        finally
-        {
-            input.close();
-        }
-        return -1;
-    }
-
-    /** like getPosition, but if key is not found will return the location of the first key _greater_ than the desired one, or -1 if no such key exists. */
-    public long getNearestPosition(String decoratedKey) throws IOException
-    {
-        long start = getIndexScanPosition(decoratedKey, partitioner);
-        if (start < 0)
-        {
-            return 0;
-        }
-        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(dataFile), "r");
-        input.seek(start);
-        try
-        {
-            while (true)
-            {
-                String indexDecoratedKey;
-                try
-                {
-                    indexDecoratedKey = input.readUTF();
-                }
-                catch (EOFException e)
-                {
-                    return -1;
-                }
-                long position = input.readLong();
-                int v = partitioner.getDecoratedKeyComparator().compare(indexDecoratedKey, decoratedKey);
-                if (v >= 0)
-                    return position;
-            }
-        }
-        finally
-        {
-            input.close();
-        }
-    }
-
-    public DataInputBuffer next(final String clientKey, String cfName, SortedSet<byte[]> columnNames) throws IOException
-    {
-        IFileReader dataReader = null;
-        try
-        {
-            dataReader = SequenceFile.reader(dataFile);
-            String decoratedKey = partitioner.decorateKey(clientKey);
-            long position = getPosition(decoratedKey, partitioner);
-
-            DataOutputBuffer bufOut = new DataOutputBuffer();
-            DataInputBuffer bufIn = new DataInputBuffer();
-            long bytesRead = dataReader.next(decoratedKey, bufOut, cfName, columnNames, position);
-            if (bytesRead != -1L)
-            {
-                if (bufOut.getLength() > 0)
-                {
-                    bufIn.reset(bufOut.getData(), bufOut.getLength());
-                    /* read the key even though we do not use it */
-                    bufIn.readUTF();
-                    bufIn.readInt();
-                }
-            }
-            return bufIn;
-        }
-        finally
-        {
-            if (dataReader != null)
-            {
-                dataReader.close();
-            }
-        }
-    }
-
-    /**
-     * obtain a BlockReader for the getColumnSlice call.
-     */
-    public ColumnGroupReader getColumnGroupReader(String key, String cfName, byte[] startColumn, boolean isAscending) throws IOException
-    {
-        IFileReader dataReader = SequenceFile.reader(dataFile);
-
-        try
-        {
-            /* Morph key into actual key based on the partition type. */
-            String decoratedKey = partitioner.decorateKey(key);
-            long position = getPosition(decoratedKey, partitioner);
-            AbstractType comparator = DatabaseDescriptor.getComparator(getTableName(), cfName);
-            return new ColumnGroupReader(dataFile, decoratedKey, cfName, comparator, startColumn, isAscending, position);
-        }
-        finally
-        {
-            dataReader.close();
-        }
-    }
-
-    public void delete() throws IOException
-    {
-        FileUtils.deleteWithConfirm(new File(dataFile));
-        FileUtils.deleteWithConfirm(new File(indexFilename(dataFile)));
-        FileUtils.deleteWithConfirm(new File(filterFilename(dataFile)));
-        openedFiles.remove(dataFile);
-    }
-
-    /** obviously only for testing */
-    public void forceBloomFilterFailures()
-    {
-        bf = BloomFilter.alwaysMatchingBloomFilter();
-    }
-
-    static void reopenUnsafe() throws IOException // testing only
-    {
-        Collection<SSTableReader> sstables = new ArrayList<SSTableReader>(openedFiles.values());
-        openedFiles.clear();
-        for (SSTableReader sstable : sstables)
-        {
-            SSTableReader.open(sstable.dataFile, sstable.partitioner, 0.01);
-        }
-    }
-
-    IPartitioner getPartitioner()
-    {
-        return partitioner;
-    }
-
-    public FileStruct getFileStruct() throws IOException
-    {
-        return new FileStruct(this);
-    }
-
-    public String getTableName()
-    {
-        return parseTableName(dataFile);
-    }
-
-    public static void deleteAll() throws IOException
-    {
-        for (SSTableReader sstable : openedFiles.values())
-        {
-            sstable.delete();
-        }
-    }
-}
-
-class FileSSTableMap
-{
-    private final Map<String, SSTableReader> map = new NonBlockingHashMap<String, SSTableReader>();
-
-    public SSTableReader get(String filename)
-    {
-        try
-        {
-            return map.get(new File(filename).getCanonicalPath());
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    public SSTableReader put(String filename, SSTableReader value)
-    {
-        try
-        {
-            return map.put(new File(filename).getCanonicalPath(), value);
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    public Collection<SSTableReader> values()
-    {
-        return map.values();
-    }
-
-    public void clear()
-    {
-        map.clear();
-    }
-
-    public void remove(String filename) throws IOException
-    {
-        map.remove(new File(filename).getCanonicalPath());
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+import java.util.*;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.SequenceFile.ColumnGroupReader;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+import com.reardencommerce.kernel.collections.shared.evictable.ConcurrentLinkedHashMap;
+
+public class SSTableReader extends SSTable
+{
+    private static Logger logger = Logger.getLogger(SSTableReader.class);
+
+    private static FileSSTableMap openedFiles = new FileSSTableMap();
+
+    public static int indexInterval()
+    {
+        return INDEX_INTERVAL;
+    }
+
+    // todo can we refactor to take list of sstables?
+    public static int getApproximateKeyCount(List<String> dataFiles)
+    {
+        int count = 0;
+
+        for (String dataFileName : dataFiles)
+        {
+            SSTableReader sstable = openedFiles.get(dataFileName);
+            assert sstable != null;
+            int indexKeyCount = sstable.getIndexPositions().size();
+            count = count + (indexKeyCount + 1) * INDEX_INTERVAL;
+            if (logger.isDebugEnabled())
+                logger.debug("index size for bloom filter calc for file  : " + dataFileName + "   : " + count);
+        }
+
+        return count;
+    }
+
+    /**
+     * Get all indexed keys in the SSTable.
+     */
+    public static List<String> getIndexedKeys()
+    {
+        List<String> indexedKeys = new ArrayList<String>();
+
+        for (SSTableReader sstable : openedFiles.values())
+        {
+            for (KeyPosition kp : sstable.getIndexPositions())
+            {
+                indexedKeys.add(kp.key);
+            }
+        }
+        Collections.sort(indexedKeys);
+
+        return indexedKeys;
+    }
+
+    public static synchronized SSTableReader open(String dataFileName) throws IOException
+    {
+        return open(dataFileName, StorageService.getPartitioner(), DatabaseDescriptor.getKeysCachedFraction(parseTableName(dataFileName)));
+    }
+
+    public static synchronized SSTableReader open(String dataFileName, IPartitioner partitioner, double cacheFraction) throws IOException
+    {
+        SSTableReader sstable = openedFiles.get(dataFileName);
+        if (sstable == null)
+        {
+            assert partitioner != null;
+            sstable = new SSTableReader(dataFileName, partitioner);
+
+            long start = System.currentTimeMillis();
+            sstable.loadIndexFile();
+            sstable.loadBloomFilter();
+            if (cacheFraction > 0)
+            {
+                sstable.keyCache = createKeyCache((int)((sstable.getIndexPositions().size() + 1) * INDEX_INTERVAL * cacheFraction));
+            }
+            if (logger.isDebugEnabled())
+                logger.debug("INDEX LOAD TIME for "  + dataFileName + ": " + (System.currentTimeMillis() - start) + " ms.");
+
+            openedFiles.put(dataFileName, sstable);
+        }
+        return sstable;
+    }
+
+    public static synchronized SSTableReader get(String dataFileName) throws IOException
+    {
+        SSTableReader sstable = openedFiles.get(dataFileName);
+        assert sstable != null;
+        return sstable;
+    }
+
+    public static ConcurrentLinkedHashMap<String, Long> createKeyCache(int size)
+    {
+        return ConcurrentLinkedHashMap.create(ConcurrentLinkedHashMap.EvictionPolicy.SECOND_CHANCE, size);
+    }
+
+
+    private ConcurrentLinkedHashMap<String, Long> keyCache;
+
+    SSTableReader(String filename, IPartitioner partitioner, List<KeyPosition> indexPositions, BloomFilter bloomFilter, ConcurrentLinkedHashMap<String, Long> keyCache)
+    {
+        super(filename, partitioner);
+        this.indexPositions = indexPositions;
+        this.bf = bloomFilter;
+        this.keyCache = keyCache;
+        synchronized (SSTableReader.this)
+        {
+            openedFiles.put(filename, this);
+        }
+    }
+
+    private SSTableReader(String filename, IPartitioner partitioner)
+    {
+        super(filename, partitioner);
+    }
+
+    public List<KeyPosition> getIndexPositions()
+    {
+        return indexPositions;
+    }
+
+    private void loadBloomFilter() throws IOException
+    {
+        DataInputStream stream = new DataInputStream(new FileInputStream(filterFilename()));
+        bf = BloomFilter.serializer().deserialize(stream);
+    }
+
+    private void loadIndexFile() throws IOException
+    {
+        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(), "r");
+        indexPositions = new ArrayList<KeyPosition>();
+
+        int i = 0;
+        long indexSize = input.length();
+        while (true)
+        {
+            long indexPosition = input.getFilePointer();
+            if (indexPosition == indexSize)
+            {
+                break;
+            }
+            String decoratedKey = input.readUTF();
+            input.readLong();
+            if (i++ % INDEX_INTERVAL == 0)
+            {
+                indexPositions.add(new KeyPosition(decoratedKey, indexPosition));
+            }
+        }
+    }
+
+    /** get the position in the index file to start scanning to find the given key (at most indexInterval keys away) */
+    private long getIndexScanPosition(String decoratedKey, IPartitioner partitioner)
+    {
+        assert indexPositions != null && indexPositions.size() > 0;
+        int index = Collections.binarySearch(indexPositions, new KeyPosition(decoratedKey, -1));
+        if (index < 0)
+        {
+            // binary search gives us the first index _greater_ than the key searched for,
+            // i.e., its insertion position
+            int greaterThan = (index + 1) * -1;
+            if (greaterThan == 0)
+                return -1;
+            return indexPositions.get(greaterThan - 1).position;
+        }
+        else
+        {
+            return indexPositions.get(index).position;
+        }
+    }
+
+    /**
+     * returns the position in the data file to find the given key, or -1 if the key is not present
+     */
+    public long getPosition(String decoratedKey, IPartitioner partitioner) throws IOException
+    {
+        if (!bf.isPresent(decoratedKey))
+            return -1;
+        if (keyCache != null)
+        {
+            Long cachedPosition = keyCache.get(decoratedKey);
+            if (cachedPosition != null)
+            {
+                return cachedPosition;
+            }
+        }
+        long start = getIndexScanPosition(decoratedKey, partitioner);
+        if (start < 0)
+        {
+            return -1;
+        }
+
+        // TODO mmap the index file?
+        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(dataFile), "r");
+        input.seek(start);
+        int i = 0;
+        try
+        {
+            do
+            {
+                String indexDecoratedKey;
+                try
+                {
+                    indexDecoratedKey = input.readUTF();
+                }
+                catch (EOFException e)
+                {
+                    return -1;
+                }
+                long position = input.readLong();
+                int v = partitioner.getDecoratedKeyComparator().compare(indexDecoratedKey, decoratedKey);
+                if (v == 0)
+                {
+                    if (keyCache != null)
+                        keyCache.put(decoratedKey, position);
+                    return position;
+                }
+                if (v > 0)
+                    return -1;
+            } while  (++i < INDEX_INTERVAL);
+        }
+        finally
+        {
+            input.close();
+        }
+        return -1;
+    }
+
+    /** like getPosition, but if key is not found will return the location of the first key _greater_ than the desired one, or -1 if no such key exists. */
+    public long getNearestPosition(String decoratedKey) throws IOException
+    {
+        long start = getIndexScanPosition(decoratedKey, partitioner);
+        if (start < 0)
+        {
+            return 0;
+        }
+        BufferedRandomAccessFile input = new BufferedRandomAccessFile(indexFilename(dataFile), "r");
+        input.seek(start);
+        try
+        {
+            while (true)
+            {
+                String indexDecoratedKey;
+                try
+                {
+                    indexDecoratedKey = input.readUTF();
+                }
+                catch (EOFException e)
+                {
+                    return -1;
+                }
+                long position = input.readLong();
+                int v = partitioner.getDecoratedKeyComparator().compare(indexDecoratedKey, decoratedKey);
+                if (v >= 0)
+                    return position;
+            }
+        }
+        finally
+        {
+            input.close();
+        }
+    }
+
+    public DataInputBuffer next(final String clientKey, String cfName, SortedSet<byte[]> columnNames) throws IOException
+    {
+        IFileReader dataReader = null;
+        try
+        {
+            dataReader = SequenceFile.reader(dataFile);
+            String decoratedKey = partitioner.decorateKey(clientKey);
+            long position = getPosition(decoratedKey, partitioner);
+
+            DataOutputBuffer bufOut = new DataOutputBuffer();
+            DataInputBuffer bufIn = new DataInputBuffer();
+            long bytesRead = dataReader.next(decoratedKey, bufOut, cfName, columnNames, position);
+            if (bytesRead != -1L)
+            {
+                if (bufOut.getLength() > 0)
+                {
+                    bufIn.reset(bufOut.getData(), bufOut.getLength());
+                    /* read the key even though we do not use it */
+                    bufIn.readUTF();
+                    bufIn.readInt();
+                }
+            }
+            return bufIn;
+        }
+        finally
+        {
+            if (dataReader != null)
+            {
+                dataReader.close();
+            }
+        }
+    }
+
+    /**
+     * obtain a BlockReader for the getColumnSlice call.
+     */
+    public ColumnGroupReader getColumnGroupReader(String key, String cfName, byte[] startColumn, boolean isAscending) throws IOException
+    {
+        IFileReader dataReader = SequenceFile.reader(dataFile);
+
+        try
+        {
+            /* Morph key into actual key based on the partition type. */
+            String decoratedKey = partitioner.decorateKey(key);
+            long position = getPosition(decoratedKey, partitioner);
+            AbstractType comparator = DatabaseDescriptor.getComparator(getTableName(), cfName);
+            return new ColumnGroupReader(dataFile, decoratedKey, cfName, comparator, startColumn, isAscending, position);
+        }
+        finally
+        {
+            dataReader.close();
+        }
+    }
+
+    public void delete() throws IOException
+    {
+        FileUtils.deleteWithConfirm(new File(dataFile));
+        FileUtils.deleteWithConfirm(new File(indexFilename(dataFile)));
+        FileUtils.deleteWithConfirm(new File(filterFilename(dataFile)));
+        openedFiles.remove(dataFile);
+    }
+
+    /** obviously only for testing */
+    public void forceBloomFilterFailures()
+    {
+        bf = BloomFilter.alwaysMatchingBloomFilter();
+    }
+
+    static void reopenUnsafe() throws IOException // testing only
+    {
+        Collection<SSTableReader> sstables = new ArrayList<SSTableReader>(openedFiles.values());
+        openedFiles.clear();
+        for (SSTableReader sstable : sstables)
+        {
+            SSTableReader.open(sstable.dataFile, sstable.partitioner, 0.01);
+        }
+    }
+
+    IPartitioner getPartitioner()
+    {
+        return partitioner;
+    }
+
+    public FileStruct getFileStruct() throws IOException
+    {
+        return new FileStruct(this);
+    }
+
+    public String getTableName()
+    {
+        return parseTableName(dataFile);
+    }
+
+    public static void deleteAll() throws IOException
+    {
+        for (SSTableReader sstable : openedFiles.values())
+        {
+            sstable.delete();
+        }
+    }
+}
+
+class FileSSTableMap
+{
+    private final Map<String, SSTableReader> map = new NonBlockingHashMap<String, SSTableReader>();
+
+    public SSTableReader get(String filename)
+    {
+        try
+        {
+            return map.get(new File(filename).getCanonicalPath());
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    public SSTableReader put(String filename, SSTableReader value)
+    {
+        try
+        {
+            return map.put(new File(filename).getCanonicalPath(), value);
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    public Collection<SSTableReader> values()
+    {
+        return map.values();
+    }
+
+    public void clear()
+    {
+        map.clear();
+    }
+
+    public void remove(String filename) throws IOException
+    {
+        map.remove(new File(filename).getCanonicalPath());
+    }
+}
diff --git a/src/java/org/apache/cassandra/io/SequenceFile.java b/src/java/org/apache/cassandra/io/SequenceFile.java
index 4a19eebeec..863471536e 100644
--- a/src/java/org/apache/cassandra/io/SequenceFile.java
+++ b/src/java/org/apache/cassandra/io/SequenceFile.java
@@ -1,545 +1,545 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.*;
-import java.util.*;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.utils.BloomFilter;
-import org.apache.cassandra.db.marshal.AbstractType;
-
-import org.apache.log4j.Logger;
-import org.apache.commons.lang.ArrayUtils;
-
-/**
- * This class writes key/value pairs sequentially to disk. It is
- * also used to read sequentially from disk. However one could
- * jump to random positions to read data from the file. This class
- * also has many implementations of the IFileWriter and IFileReader
- * interfaces which are exposed through factory methods.
- */
-
-public class SequenceFile
-{
-    /**
-     *  This is a reader that finds the block for a starting column and returns
-     *  blocks before/after it for each next call. This function assumes that
-     *  the CF is sorted by name and exploits the name index.
-     */
-    public static class ColumnGroupReader extends BufferReader
-    {
-        private String key_;
-        private String cfName_;
-        private String cfType_;
-        private AbstractType comparator_;
-        private String subComparatorName_;
-        private boolean isAscending_;
-
-        private List<IndexHelper.ColumnIndexInfo> columnIndexList_;
-        private long columnStartPosition_;
-        private int curRangeIndex_;
-        private int allColumnsSize_;
-        private int localDeletionTime_;
-        private long markedForDeleteAt_;
-
-        ColumnGroupReader(String filename, String key, String cfName, AbstractType comparator, byte[] startColumn, boolean isAscending, long position) throws IOException
-        {
-            super(filename, 128 * 1024);
-            this.cfName_ = cfName;
-            this.comparator_ = comparator;
-            this.subComparatorName_ = DatabaseDescriptor.getSubComparator(SSTableReader.parseTableName(filename), cfName).getClass().getCanonicalName();
-            this.key_ = key;
-            this.isAscending_ = isAscending;
-            init(startColumn, position);
-        }
-
-        /**
-         *   Build a list of index entries ready for search.
-         */
-        private List<IndexHelper.ColumnIndexInfo> getFullColumnIndexList(List<IndexHelper.ColumnIndexInfo> columnIndexList, int totalNumCols)
-        {
-            if (columnIndexList.size() == 0)
-            {
-                /* if there is no column index, add an index entry that covers the full space. */
-                return Arrays.asList(new IndexHelper.ColumnIndexInfo(ArrayUtils.EMPTY_BYTE_ARRAY, 0, totalNumCols, comparator_));
-            }
-
-            List<IndexHelper.ColumnIndexInfo> fullColIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
-            int accumulatededCols = 0;
-            for (IndexHelper.ColumnIndexInfo colPosInfo : columnIndexList)
-                accumulatededCols += colPosInfo.count();
-            int remainingCols = totalNumCols - accumulatededCols;
-
-            fullColIndexList.add(new IndexHelper.ColumnIndexInfo(ArrayUtils.EMPTY_BYTE_ARRAY, 0, columnIndexList.get(0).count(), comparator_));
-            for (int i = 0; i < columnIndexList.size() - 1; i++)
-            {
-                IndexHelper.ColumnIndexInfo colPosInfo = columnIndexList.get(i);
-                fullColIndexList.add(new IndexHelper.ColumnIndexInfo(colPosInfo.name(),
-                                                                     colPosInfo.position(),
-                                                                     columnIndexList.get(i + 1).count(),
-                                                                     comparator_));
-            }
-            byte[] columnName = columnIndexList.get(columnIndexList.size() - 1).name();
-            fullColIndexList.add(new IndexHelper.ColumnIndexInfo(columnName,
-                                                                 columnIndexList.get(columnIndexList.size() - 1).position(),
-                                                                 remainingCols,
-                                                                 comparator_));
-            return fullColIndexList;
-        }
-
-        private void init(byte[] startColumn, long position) throws IOException
-        {
-            String keyInDisk = null;
-            if (seekTo(position) >= 0)
-                keyInDisk = file_.readUTF();
-
-            if ( keyInDisk != null && keyInDisk.equals(key_))
-            {
-                /* read off the size of this row */
-                int dataSize = file_.readInt();
-                /* skip the bloomfilter */
-                int totalBytesRead = IndexHelper.skipBloomFilter(file_);
-                /* read off the index flag, it has to be true */
-                boolean hasColumnIndexes = file_.readBoolean();
-                totalBytesRead += 1;
-
-                /* read the index */
-                List<IndexHelper.ColumnIndexInfo> colIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
-                if (hasColumnIndexes)
-                    totalBytesRead += IndexHelper.deserializeIndex(getTableName(), cfName_, file_, colIndexList);
-
-                /* need to do two things here.
-                 * 1. move the file pointer to the beginning of the list of stored columns
-                 * 2. calculate the size of all columns */
-                String cfName = file_.readUTF();
-                cfType_ = file_.readUTF();
-                String comparatorName = file_.readUTF();
-                assert comparatorName.equals(comparator_.getClass().getCanonicalName());
-                String subComparatorName = file_.readUTF(); // subcomparator
-                localDeletionTime_ = file_.readInt();
-                markedForDeleteAt_ = file_.readLong();
-                int totalNumCols = file_.readInt();
-                allColumnsSize_ = dataSize - (totalBytesRead + 4 * utfPrefix_ + cfName.length() + cfType_.length() + comparatorName.length() + subComparatorName.length() + 4 + 8 + 4);
-
-                columnStartPosition_ = file_.getFilePointer();
-                columnIndexList_ = getFullColumnIndexList(colIndexList, totalNumCols);
-
-                int index = Collections.binarySearch(columnIndexList_, new IndexHelper.ColumnIndexInfo(startColumn, 0, 0, comparator_));
-                curRangeIndex_ = index < 0 ? (++index) * (-1) - 1 : index;
-            }
-            else
-            {
-                /* no keys found in this file because of a false positive in BF */
-                curRangeIndex_ = -1;
-                columnIndexList_ = new ArrayList<IndexHelper.ColumnIndexInfo>();
-            }
-        }
-
-        private boolean getBlockFromCurIndex(DataOutputBuffer bufOut) throws IOException
-        {
-            if (curRangeIndex_ < 0 || curRangeIndex_ >= columnIndexList_.size())
-                return false;
-            IndexHelper.ColumnIndexInfo curColPostion = columnIndexList_.get(curRangeIndex_);
-            long start = curColPostion.position();
-            long end = curRangeIndex_ < columnIndexList_.size() - 1
-                       ? columnIndexList_.get(curRangeIndex_+1).position()
-                       : allColumnsSize_;
-
-            /* seek to the correct offset to the data, and calculate the data size */
-            file_.seek(columnStartPosition_ + start);
-            long dataSize = end - start;
-
-            bufOut.reset();
-            // write CF info
-            bufOut.writeUTF(cfName_);
-            bufOut.writeUTF(cfType_);
-            bufOut.writeUTF(comparator_.getClass().getCanonicalName());
-            bufOut.writeUTF(subComparatorName_);
-            bufOut.writeInt(localDeletionTime_);
-            bufOut.writeLong(markedForDeleteAt_);
-            // now write the columns
-            bufOut.writeInt(curColPostion.count());
-            bufOut.write(file_, (int)dataSize);
-            return true;
-        }
-
-        public boolean getNextBlock(DataOutputBuffer outBuf) throws IOException
-        {
-            boolean result = getBlockFromCurIndex(outBuf);
-            if (isAscending_)
-                curRangeIndex_++;
-            else
-                curRangeIndex_--;
-            return result;
-        }
-    }
-
-    public static abstract class AbstractReader implements IFileReader
-    {
-        private static final short utfPrefix_ = 2;
-        protected RandomAccessFile file_;
-        protected String filename_;
-
-        AbstractReader(String filename)
-        {
-            filename_ = filename;
-        }
-
-        String getTableName()
-        {
-            return SSTable.parseTableName(filename_);
-        }
-
-        public String getFileName()
-        {
-            return filename_;
-        }
-
-        long seekTo(long position) throws IOException
-        {
-            if (position >= 0)
-                seek(position);
-            return position;
-        }
-
-        /**
-         * Defreeze the bloom filter.
-         *
-         * @return bloom filter summarizing the column information
-         * @throws IOException
-         */
-        private BloomFilter defreezeBloomFilter() throws IOException
-        {
-            int size = file_.readInt();
-            byte[] bytes = new byte[size];
-            file_.readFully(bytes);
-            DataInputBuffer bufIn = new DataInputBuffer();
-            bufIn.reset(bytes, bytes.length);
-            BloomFilter bf = BloomFilter.serializer().deserialize(bufIn);
-            return bf;
-        }
-
-        /**
-         * Reads the column name indexes if present. If the
-         * indexes are based on time then skip over them.
-         *
-         * @param cfName
-         * @return
-         */
-        private int handleColumnNameIndexes(String cfName, List<IndexHelper.ColumnIndexInfo> columnIndexList) throws IOException
-        {
-            /* check if we have an index */
-            boolean hasColumnIndexes = file_.readBoolean();
-            int totalBytesRead = 1;
-            /* if we do then deserialize the index */
-            if (hasColumnIndexes)
-            {
-                String tableName = getTableName();
-                /* read the index */
-                totalBytesRead += IndexHelper.deserializeIndex(tableName, cfName, file_, columnIndexList);
-            }
-            return totalBytesRead;
-        }
-
-        /**
-         * This method dumps the next key/value into the DataOuputStream
-         * passed in. Always use this method to query for application
-         * specific data as it will have indexes.
-         *
-         * @param key       key we are interested in.
-         * @param bufOut    DataOutputStream that needs to be filled.
-         * @param columnFamilyName name of the columnFamily
-         * @param columnNames columnNames we are interested in
-         */
-        public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, SortedSet<byte[]> columnNames, long position) throws IOException
-        {
-            assert columnNames != null;
-
-            long bytesRead = -1L;
-            if (isEOF() || seekTo(position) < 0)
-                return bytesRead;
-
-            /* note the position where the key starts */
-            long startPosition = file_.getFilePointer();
-            String keyInDisk = file_.readUTF();
-            if (keyInDisk != null)
-            {
-                /*
-                 * If key on disk is greater than requested key
-                 * we can bail out since we exploit the property
-                 * of the SSTable format.
-                */
-                if (keyInDisk.compareTo(key) > 0)
-                    return bytesRead;
-
-                /*
-                 * If we found the key then we populate the buffer that
-                 * is passed in. If not then we skip over this key and
-                 * position ourselves to read the next one.
-                */
-                if (keyInDisk.equals(key))
-                {
-                    readColumns(key, bufOut, columnFamilyName, columnNames);
-                }
-                else
-                {
-                    /* skip over data portion */
-                    int dataSize = file_.readInt();
-                    file_.seek(dataSize + file_.getFilePointer());
-                }
-
-                long endPosition = file_.getFilePointer();
-                bytesRead = endPosition - startPosition;
-            }
-
-            return bytesRead;
-        }
-
-        private void readColumns(String key, DataOutputBuffer bufOut, String columnFamilyName, SortedSet<byte[]> cNames)
-        throws IOException
-        {
-            int dataSize = file_.readInt();
-
-            /* write the key into buffer */
-            bufOut.writeUTF(key);
-
-            /* if we need to read the all the columns do not read the column indexes */
-            if (cNames == null || cNames.size() == 0)
-            {
-                int bytesSkipped = IndexHelper.skipBloomFilterAndIndex(file_);
-                /*
-                       * read the correct number of bytes for the column family and
-                       * write data into buffer
-                      */
-                dataSize -= bytesSkipped;
-                /* write the data size */
-                bufOut.writeInt(dataSize);
-                /* write the data into buffer, except the boolean we have read */
-                bufOut.write(file_, dataSize);
-            }
-            else
-            {
-                /* Read the bloom filter summarizing the columns */
-                long preBfPos = file_.getFilePointer();
-                BloomFilter bf = defreezeBloomFilter();
-                long postBfPos = file_.getFilePointer();
-                dataSize -= (postBfPos - preBfPos);
-
-                List<IndexHelper.ColumnIndexInfo> columnIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
-                /* read the column name indexes if present */
-                int totalBytesRead = handleColumnNameIndexes(columnFamilyName, columnIndexList);
-                dataSize -= totalBytesRead;
-
-                /* read the column family name */
-                String cfName = file_.readUTF();
-                dataSize -= (utfPrefix_ + cfName.length());
-
-                String cfType = file_.readUTF();
-                dataSize -= (utfPrefix_ + cfType.length());
-
-                String comparatorName = file_.readUTF();
-                dataSize -= (utfPrefix_ + comparatorName.length());
-
-                String subComparatorName = file_.readUTF();
-                dataSize -= (utfPrefix_ + subComparatorName.length());
-
-                /* read local deletion time */
-                int localDeletionTime = file_.readInt();
-                dataSize -=4;
-
-                /* read if this cf is marked for delete */
-                long markedForDeleteAt = file_.readLong();
-                dataSize -= 8;
-
-                /* read the total number of columns */
-                int totalNumCols = file_.readInt();
-                dataSize -= 4;
-
-                // TODO: this is name sorted - but eventually this should be sorted by the same criteria as the col index
-                /* get the various column ranges we have to read */
-                List<IndexHelper.ColumnRange> columnRanges = IndexHelper.getMultiColumnRangesFromNameIndex(cNames, columnIndexList, dataSize, totalNumCols);
-
-                /* calculate the data size */
-                int numColsReturned = 0;
-                int dataSizeReturned = 0;
-                for (IndexHelper.ColumnRange columnRange : columnRanges)
-                {
-                    numColsReturned += columnRange.count();
-                    Coordinate coordinate = columnRange.coordinate();
-                    dataSizeReturned += coordinate.end_ - coordinate.start_;
-                }
-
-                // returned data size
-                bufOut.writeInt(dataSizeReturned + utfPrefix_ * 4 + cfName.length() + cfType.length() + comparatorName.length() + subComparatorName.length() + 4 + 4 + 8 + 4);
-                // echo back the CF data we read
-                bufOut.writeUTF(cfName);
-                bufOut.writeUTF(cfType);
-                bufOut.writeUTF(comparatorName);
-                bufOut.writeUTF(subComparatorName);
-                bufOut.writeInt(localDeletionTime);
-                bufOut.writeLong(markedForDeleteAt);
-                /* write number of columns */
-                bufOut.writeInt(numColsReturned);
-                int prevPosition = 0;
-                /* now write all the columns we are required to write */
-                for (IndexHelper.ColumnRange columnRange : columnRanges)
-                {
-                    /* seek to the correct offset to the data */
-                    Coordinate coordinate = columnRange.coordinate();
-                    file_.skipBytes((int) (coordinate.start_ - prevPosition));
-                    bufOut.write(file_, (int) (coordinate.end_ - coordinate.start_));
-                    prevPosition = (int) coordinate.end_;
-                }
-            }
-        }
-
-        /**
-         * This method dumps the next key/value into the DataOuputStream
-         * passed in.
-         *
-         * @param bufOut DataOutputStream that needs to be filled.
-         * @return total number of bytes read/considered
-         */
-        public long next(DataOutputBuffer bufOut) throws IOException
-        {
-            long bytesRead = -1L;
-            if (isEOF())
-                return bytesRead;
-
-            long startPosition = file_.getFilePointer();
-            String key = file_.readUTF();
-            if (key != null)
-            {
-                /* write the key into buffer */
-                bufOut.writeUTF(key);
-                int dataSize = file_.readInt();
-                /* write data size into buffer */
-                bufOut.writeInt(dataSize);
-                /* write the data into buffer */
-                bufOut.write(file_, dataSize);
-                long endPosition = file_.getFilePointer();
-                bytesRead = endPosition - startPosition;
-            }
-
-            return bytesRead;
-        }
-    }
-
-    public static class Reader extends AbstractReader
-    {
-        Reader(String filename) throws IOException
-        {
-            super(filename);
-            init(filename);
-        }
-
-        protected void init(String filename) throws IOException
-        {
-            file_ = new RandomAccessFile(filename, "r");
-        }
-
-        public long getEOF() throws IOException
-        {
-            return file_.length();
-        }
-
-        public long getCurrentPosition() throws IOException
-        {
-            return file_.getFilePointer();
-        }
-
-        public boolean isHealthyFileDescriptor() throws IOException
-        {
-            return file_.getFD().valid();
-        }
-
-        public void seek(long position) throws IOException
-        {
-            file_.seek(position);
-        }
-
-        public boolean isEOF() throws IOException
-        {
-            return (getCurrentPosition() == getEOF());
-        }
-
-        /**
-         * Be extremely careful while using this API. This currently
-         * used to read the commit log header from the commit logs.
-         * Treat this as an internal API.
-         *
-         * @param bytes read from the buffer into the this array
-         */
-        public void readDirect(byte[] bytes) throws IOException
-        {
-            file_.readFully(bytes);
-        }
-
-        public long readLong() throws IOException
-        {
-            return file_.readLong();
-        }
-
-        public void close() throws IOException
-        {
-            file_.close();
-        }
-    }
-
-    public static class BufferReader extends Reader
-    {
-        private int size_;
-
-        BufferReader(String filename, int size) throws IOException
-        {
-            super(filename);
-            size_ = size;
-        }
-
-        protected void init(String filename) throws IOException
-        {
-            file_ = new BufferedRandomAccessFile(filename, "r", size_);
-        }
-    }
-
-    private static Logger logger_ = Logger.getLogger(SequenceFile.class);
-    public static final short utfPrefix_ = 2;
-    public static final String marker_ = "Bloom-Filter";
-
-    public static AbstractWriter writer(String filename) throws IOException
-    {
-        return new AbstractWriter.Writer(filename);
-    }
-
-    public static AbstractWriter bufferedWriter(String filename, int size) throws IOException
-    {
-        return new AbstractWriter.BufferWriter(filename, size);
-    }
-
-    public static IFileReader reader(String filename) throws IOException
-    {
-        return new Reader(filename);
-    }
-
-    public static IFileReader bufferedReader(String filename, int size) throws IOException
-    {
-        return new BufferReader(filename, size);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.io;
+
+import java.io.*;
+import java.util.*;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.utils.BloomFilter;
+import org.apache.cassandra.db.marshal.AbstractType;
+
+import org.apache.log4j.Logger;
+import org.apache.commons.lang.ArrayUtils;
+
+/**
+ * This class writes key/value pairs sequentially to disk. It is
+ * also used to read sequentially from disk. However one could
+ * jump to random positions to read data from the file. This class
+ * also has many implementations of the IFileWriter and IFileReader
+ * interfaces which are exposed through factory methods.
+ */
+
+public class SequenceFile
+{
+    /**
+     *  This is a reader that finds the block for a starting column and returns
+     *  blocks before/after it for each next call. This function assumes that
+     *  the CF is sorted by name and exploits the name index.
+     */
+    public static class ColumnGroupReader extends BufferReader
+    {
+        private String key_;
+        private String cfName_;
+        private String cfType_;
+        private AbstractType comparator_;
+        private String subComparatorName_;
+        private boolean isAscending_;
+
+        private List<IndexHelper.ColumnIndexInfo> columnIndexList_;
+        private long columnStartPosition_;
+        private int curRangeIndex_;
+        private int allColumnsSize_;
+        private int localDeletionTime_;
+        private long markedForDeleteAt_;
+
+        ColumnGroupReader(String filename, String key, String cfName, AbstractType comparator, byte[] startColumn, boolean isAscending, long position) throws IOException
+        {
+            super(filename, 128 * 1024);
+            this.cfName_ = cfName;
+            this.comparator_ = comparator;
+            this.subComparatorName_ = DatabaseDescriptor.getSubComparator(SSTableReader.parseTableName(filename), cfName).getClass().getCanonicalName();
+            this.key_ = key;
+            this.isAscending_ = isAscending;
+            init(startColumn, position);
+        }
+
+        /**
+         *   Build a list of index entries ready for search.
+         */
+        private List<IndexHelper.ColumnIndexInfo> getFullColumnIndexList(List<IndexHelper.ColumnIndexInfo> columnIndexList, int totalNumCols)
+        {
+            if (columnIndexList.size() == 0)
+            {
+                /* if there is no column index, add an index entry that covers the full space. */
+                return Arrays.asList(new IndexHelper.ColumnIndexInfo(ArrayUtils.EMPTY_BYTE_ARRAY, 0, totalNumCols, comparator_));
+            }
+
+            List<IndexHelper.ColumnIndexInfo> fullColIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
+            int accumulatededCols = 0;
+            for (IndexHelper.ColumnIndexInfo colPosInfo : columnIndexList)
+                accumulatededCols += colPosInfo.count();
+            int remainingCols = totalNumCols - accumulatededCols;
+
+            fullColIndexList.add(new IndexHelper.ColumnIndexInfo(ArrayUtils.EMPTY_BYTE_ARRAY, 0, columnIndexList.get(0).count(), comparator_));
+            for (int i = 0; i < columnIndexList.size() - 1; i++)
+            {
+                IndexHelper.ColumnIndexInfo colPosInfo = columnIndexList.get(i);
+                fullColIndexList.add(new IndexHelper.ColumnIndexInfo(colPosInfo.name(),
+                                                                     colPosInfo.position(),
+                                                                     columnIndexList.get(i + 1).count(),
+                                                                     comparator_));
+            }
+            byte[] columnName = columnIndexList.get(columnIndexList.size() - 1).name();
+            fullColIndexList.add(new IndexHelper.ColumnIndexInfo(columnName,
+                                                                 columnIndexList.get(columnIndexList.size() - 1).position(),
+                                                                 remainingCols,
+                                                                 comparator_));
+            return fullColIndexList;
+        }
+
+        private void init(byte[] startColumn, long position) throws IOException
+        {
+            String keyInDisk = null;
+            if (seekTo(position) >= 0)
+                keyInDisk = file_.readUTF();
+
+            if ( keyInDisk != null && keyInDisk.equals(key_))
+            {
+                /* read off the size of this row */
+                int dataSize = file_.readInt();
+                /* skip the bloomfilter */
+                int totalBytesRead = IndexHelper.skipBloomFilter(file_);
+                /* read off the index flag, it has to be true */
+                boolean hasColumnIndexes = file_.readBoolean();
+                totalBytesRead += 1;
+
+                /* read the index */
+                List<IndexHelper.ColumnIndexInfo> colIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
+                if (hasColumnIndexes)
+                    totalBytesRead += IndexHelper.deserializeIndex(getTableName(), cfName_, file_, colIndexList);
+
+                /* need to do two things here.
+                 * 1. move the file pointer to the beginning of the list of stored columns
+                 * 2. calculate the size of all columns */
+                String cfName = file_.readUTF();
+                cfType_ = file_.readUTF();
+                String comparatorName = file_.readUTF();
+                assert comparatorName.equals(comparator_.getClass().getCanonicalName());
+                String subComparatorName = file_.readUTF(); // subcomparator
+                localDeletionTime_ = file_.readInt();
+                markedForDeleteAt_ = file_.readLong();
+                int totalNumCols = file_.readInt();
+                allColumnsSize_ = dataSize - (totalBytesRead + 4 * utfPrefix_ + cfName.length() + cfType_.length() + comparatorName.length() + subComparatorName.length() + 4 + 8 + 4);
+
+                columnStartPosition_ = file_.getFilePointer();
+                columnIndexList_ = getFullColumnIndexList(colIndexList, totalNumCols);
+
+                int index = Collections.binarySearch(columnIndexList_, new IndexHelper.ColumnIndexInfo(startColumn, 0, 0, comparator_));
+                curRangeIndex_ = index < 0 ? (++index) * (-1) - 1 : index;
+            }
+            else
+            {
+                /* no keys found in this file because of a false positive in BF */
+                curRangeIndex_ = -1;
+                columnIndexList_ = new ArrayList<IndexHelper.ColumnIndexInfo>();
+            }
+        }
+
+        private boolean getBlockFromCurIndex(DataOutputBuffer bufOut) throws IOException
+        {
+            if (curRangeIndex_ < 0 || curRangeIndex_ >= columnIndexList_.size())
+                return false;
+            IndexHelper.ColumnIndexInfo curColPostion = columnIndexList_.get(curRangeIndex_);
+            long start = curColPostion.position();
+            long end = curRangeIndex_ < columnIndexList_.size() - 1
+                       ? columnIndexList_.get(curRangeIndex_+1).position()
+                       : allColumnsSize_;
+
+            /* seek to the correct offset to the data, and calculate the data size */
+            file_.seek(columnStartPosition_ + start);
+            long dataSize = end - start;
+
+            bufOut.reset();
+            // write CF info
+            bufOut.writeUTF(cfName_);
+            bufOut.writeUTF(cfType_);
+            bufOut.writeUTF(comparator_.getClass().getCanonicalName());
+            bufOut.writeUTF(subComparatorName_);
+            bufOut.writeInt(localDeletionTime_);
+            bufOut.writeLong(markedForDeleteAt_);
+            // now write the columns
+            bufOut.writeInt(curColPostion.count());
+            bufOut.write(file_, (int)dataSize);
+            return true;
+        }
+
+        public boolean getNextBlock(DataOutputBuffer outBuf) throws IOException
+        {
+            boolean result = getBlockFromCurIndex(outBuf);
+            if (isAscending_)
+                curRangeIndex_++;
+            else
+                curRangeIndex_--;
+            return result;
+        }
+    }
+
+    public static abstract class AbstractReader implements IFileReader
+    {
+        private static final short utfPrefix_ = 2;
+        protected RandomAccessFile file_;
+        protected String filename_;
+
+        AbstractReader(String filename)
+        {
+            filename_ = filename;
+        }
+
+        String getTableName()
+        {
+            return SSTable.parseTableName(filename_);
+        }
+
+        public String getFileName()
+        {
+            return filename_;
+        }
+
+        long seekTo(long position) throws IOException
+        {
+            if (position >= 0)
+                seek(position);
+            return position;
+        }
+
+        /**
+         * Defreeze the bloom filter.
+         *
+         * @return bloom filter summarizing the column information
+         * @throws IOException
+         */
+        private BloomFilter defreezeBloomFilter() throws IOException
+        {
+            int size = file_.readInt();
+            byte[] bytes = new byte[size];
+            file_.readFully(bytes);
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(bytes, bytes.length);
+            BloomFilter bf = BloomFilter.serializer().deserialize(bufIn);
+            return bf;
+        }
+
+        /**
+         * Reads the column name indexes if present. If the
+         * indexes are based on time then skip over them.
+         *
+         * @param cfName
+         * @return
+         */
+        private int handleColumnNameIndexes(String cfName, List<IndexHelper.ColumnIndexInfo> columnIndexList) throws IOException
+        {
+            /* check if we have an index */
+            boolean hasColumnIndexes = file_.readBoolean();
+            int totalBytesRead = 1;
+            /* if we do then deserialize the index */
+            if (hasColumnIndexes)
+            {
+                String tableName = getTableName();
+                /* read the index */
+                totalBytesRead += IndexHelper.deserializeIndex(tableName, cfName, file_, columnIndexList);
+            }
+            return totalBytesRead;
+        }
+
+        /**
+         * This method dumps the next key/value into the DataOuputStream
+         * passed in. Always use this method to query for application
+         * specific data as it will have indexes.
+         *
+         * @param key       key we are interested in.
+         * @param bufOut    DataOutputStream that needs to be filled.
+         * @param columnFamilyName name of the columnFamily
+         * @param columnNames columnNames we are interested in
+         */
+        public long next(String key, DataOutputBuffer bufOut, String columnFamilyName, SortedSet<byte[]> columnNames, long position) throws IOException
+        {
+            assert columnNames != null;
+
+            long bytesRead = -1L;
+            if (isEOF() || seekTo(position) < 0)
+                return bytesRead;
+
+            /* note the position where the key starts */
+            long startPosition = file_.getFilePointer();
+            String keyInDisk = file_.readUTF();
+            if (keyInDisk != null)
+            {
+                /*
+                 * If key on disk is greater than requested key
+                 * we can bail out since we exploit the property
+                 * of the SSTable format.
+                */
+                if (keyInDisk.compareTo(key) > 0)
+                    return bytesRead;
+
+                /*
+                 * If we found the key then we populate the buffer that
+                 * is passed in. If not then we skip over this key and
+                 * position ourselves to read the next one.
+                */
+                if (keyInDisk.equals(key))
+                {
+                    readColumns(key, bufOut, columnFamilyName, columnNames);
+                }
+                else
+                {
+                    /* skip over data portion */
+                    int dataSize = file_.readInt();
+                    file_.seek(dataSize + file_.getFilePointer());
+                }
+
+                long endPosition = file_.getFilePointer();
+                bytesRead = endPosition - startPosition;
+            }
+
+            return bytesRead;
+        }
+
+        private void readColumns(String key, DataOutputBuffer bufOut, String columnFamilyName, SortedSet<byte[]> cNames)
+        throws IOException
+        {
+            int dataSize = file_.readInt();
+
+            /* write the key into buffer */
+            bufOut.writeUTF(key);
+
+            /* if we need to read the all the columns do not read the column indexes */
+            if (cNames == null || cNames.size() == 0)
+            {
+                int bytesSkipped = IndexHelper.skipBloomFilterAndIndex(file_);
+                /*
+                       * read the correct number of bytes for the column family and
+                       * write data into buffer
+                      */
+                dataSize -= bytesSkipped;
+                /* write the data size */
+                bufOut.writeInt(dataSize);
+                /* write the data into buffer, except the boolean we have read */
+                bufOut.write(file_, dataSize);
+            }
+            else
+            {
+                /* Read the bloom filter summarizing the columns */
+                long preBfPos = file_.getFilePointer();
+                BloomFilter bf = defreezeBloomFilter();
+                long postBfPos = file_.getFilePointer();
+                dataSize -= (postBfPos - preBfPos);
+
+                List<IndexHelper.ColumnIndexInfo> columnIndexList = new ArrayList<IndexHelper.ColumnIndexInfo>();
+                /* read the column name indexes if present */
+                int totalBytesRead = handleColumnNameIndexes(columnFamilyName, columnIndexList);
+                dataSize -= totalBytesRead;
+
+                /* read the column family name */
+                String cfName = file_.readUTF();
+                dataSize -= (utfPrefix_ + cfName.length());
+
+                String cfType = file_.readUTF();
+                dataSize -= (utfPrefix_ + cfType.length());
+
+                String comparatorName = file_.readUTF();
+                dataSize -= (utfPrefix_ + comparatorName.length());
+
+                String subComparatorName = file_.readUTF();
+                dataSize -= (utfPrefix_ + subComparatorName.length());
+
+                /* read local deletion time */
+                int localDeletionTime = file_.readInt();
+                dataSize -=4;
+
+                /* read if this cf is marked for delete */
+                long markedForDeleteAt = file_.readLong();
+                dataSize -= 8;
+
+                /* read the total number of columns */
+                int totalNumCols = file_.readInt();
+                dataSize -= 4;
+
+                // TODO: this is name sorted - but eventually this should be sorted by the same criteria as the col index
+                /* get the various column ranges we have to read */
+                List<IndexHelper.ColumnRange> columnRanges = IndexHelper.getMultiColumnRangesFromNameIndex(cNames, columnIndexList, dataSize, totalNumCols);
+
+                /* calculate the data size */
+                int numColsReturned = 0;
+                int dataSizeReturned = 0;
+                for (IndexHelper.ColumnRange columnRange : columnRanges)
+                {
+                    numColsReturned += columnRange.count();
+                    Coordinate coordinate = columnRange.coordinate();
+                    dataSizeReturned += coordinate.end_ - coordinate.start_;
+                }
+
+                // returned data size
+                bufOut.writeInt(dataSizeReturned + utfPrefix_ * 4 + cfName.length() + cfType.length() + comparatorName.length() + subComparatorName.length() + 4 + 4 + 8 + 4);
+                // echo back the CF data we read
+                bufOut.writeUTF(cfName);
+                bufOut.writeUTF(cfType);
+                bufOut.writeUTF(comparatorName);
+                bufOut.writeUTF(subComparatorName);
+                bufOut.writeInt(localDeletionTime);
+                bufOut.writeLong(markedForDeleteAt);
+                /* write number of columns */
+                bufOut.writeInt(numColsReturned);
+                int prevPosition = 0;
+                /* now write all the columns we are required to write */
+                for (IndexHelper.ColumnRange columnRange : columnRanges)
+                {
+                    /* seek to the correct offset to the data */
+                    Coordinate coordinate = columnRange.coordinate();
+                    file_.skipBytes((int) (coordinate.start_ - prevPosition));
+                    bufOut.write(file_, (int) (coordinate.end_ - coordinate.start_));
+                    prevPosition = (int) coordinate.end_;
+                }
+            }
+        }
+
+        /**
+         * This method dumps the next key/value into the DataOuputStream
+         * passed in.
+         *
+         * @param bufOut DataOutputStream that needs to be filled.
+         * @return total number of bytes read/considered
+         */
+        public long next(DataOutputBuffer bufOut) throws IOException
+        {
+            long bytesRead = -1L;
+            if (isEOF())
+                return bytesRead;
+
+            long startPosition = file_.getFilePointer();
+            String key = file_.readUTF();
+            if (key != null)
+            {
+                /* write the key into buffer */
+                bufOut.writeUTF(key);
+                int dataSize = file_.readInt();
+                /* write data size into buffer */
+                bufOut.writeInt(dataSize);
+                /* write the data into buffer */
+                bufOut.write(file_, dataSize);
+                long endPosition = file_.getFilePointer();
+                bytesRead = endPosition - startPosition;
+            }
+
+            return bytesRead;
+        }
+    }
+
+    public static class Reader extends AbstractReader
+    {
+        Reader(String filename) throws IOException
+        {
+            super(filename);
+            init(filename);
+        }
+
+        protected void init(String filename) throws IOException
+        {
+            file_ = new RandomAccessFile(filename, "r");
+        }
+
+        public long getEOF() throws IOException
+        {
+            return file_.length();
+        }
+
+        public long getCurrentPosition() throws IOException
+        {
+            return file_.getFilePointer();
+        }
+
+        public boolean isHealthyFileDescriptor() throws IOException
+        {
+            return file_.getFD().valid();
+        }
+
+        public void seek(long position) throws IOException
+        {
+            file_.seek(position);
+        }
+
+        public boolean isEOF() throws IOException
+        {
+            return (getCurrentPosition() == getEOF());
+        }
+
+        /**
+         * Be extremely careful while using this API. This currently
+         * used to read the commit log header from the commit logs.
+         * Treat this as an internal API.
+         *
+         * @param bytes read from the buffer into the this array
+         */
+        public void readDirect(byte[] bytes) throws IOException
+        {
+            file_.readFully(bytes);
+        }
+
+        public long readLong() throws IOException
+        {
+            return file_.readLong();
+        }
+
+        public void close() throws IOException
+        {
+            file_.close();
+        }
+    }
+
+    public static class BufferReader extends Reader
+    {
+        private int size_;
+
+        BufferReader(String filename, int size) throws IOException
+        {
+            super(filename);
+            size_ = size;
+        }
+
+        protected void init(String filename) throws IOException
+        {
+            file_ = new BufferedRandomAccessFile(filename, "r", size_);
+        }
+    }
+
+    private static Logger logger_ = Logger.getLogger(SequenceFile.class);
+    public static final short utfPrefix_ = 2;
+    public static final String marker_ = "Bloom-Filter";
+
+    public static AbstractWriter writer(String filename) throws IOException
+    {
+        return new AbstractWriter.Writer(filename);
+    }
+
+    public static AbstractWriter bufferedWriter(String filename, int size) throws IOException
+    {
+        return new AbstractWriter.BufferWriter(filename, size);
+    }
+
+    public static IFileReader reader(String filename) throws IOException
+    {
+        return new Reader(filename);
+    }
+
+    public static IFileReader bufferedReader(String filename, int size) throws IOException
+    {
+        return new BufferReader(filename, size);
+    }
+}
diff --git a/src/java/org/apache/cassandra/locator/AbstractStrategy.java b/src/java/org/apache/cassandra/locator/AbstractStrategy.java
index 93a6a91119..1d2979c17d 100644
--- a/src/java/org/apache/cassandra/locator/AbstractStrategy.java
+++ b/src/java/org/apache/cassandra/locator/AbstractStrategy.java
@@ -16,120 +16,120 @@
 * specific language governing permissions and limitations
 * under the License.
 */
-package org.apache.cassandra.locator;
-
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.gms.FailureDetector;
-import org.apache.cassandra.net.EndPoint;
-
-/**
- * This class contains a helper method that will be used by
- * all abstraction that implement the IReplicaPlacementStrategy
- * interface.
-*/
-public abstract class AbstractStrategy implements IReplicaPlacementStrategy
-{
-    protected static Logger logger_ = Logger.getLogger(AbstractStrategy.class);
-
-    protected TokenMetadata tokenMetadata_;
-    protected IPartitioner partitioner_;
-    protected int replicas_;
-    protected int storagePort_;
-
-    AbstractStrategy(TokenMetadata tokenMetadata, IPartitioner partitioner, int replicas, int storagePort)
-    {
-        tokenMetadata_ = tokenMetadata;
-        partitioner_ = partitioner;
-        replicas_ = replicas;
-        storagePort_ = storagePort;
-    }
-
-    /*
-     * This method changes the ports of the endpoints from
-     * the control port to the storage ports.
-    */
-    protected void retrofitPorts(List<EndPoint> eps)
-    {
-        for ( EndPoint ep : eps )
-        {
-            ep.setPort(storagePort_);
-        }
-    }
-
-    protected EndPoint getNextAvailableEndPoint(EndPoint startPoint, List<EndPoint> topN, List<EndPoint> liveNodes)
-    {
-        EndPoint endPoint = null;
-        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-        List tokens = new ArrayList(tokenToEndPointMap.keySet());
-        Collections.sort(tokens);
-        Token token = tokenMetadata_.getToken(startPoint);
-        int index = Collections.binarySearch(tokens, token);
-        if(index < 0)
-        {
-            index = (index + 1) * (-1);
-            if (index >= tokens.size())
-                index = 0;
-        }
-        int totalNodes = tokens.size();
-        int startIndex = (index+1)%totalNodes;
-        for (int i = startIndex, count = 1; count < totalNodes ; ++count, i = (i+1)%totalNodes)
-        {
-            EndPoint tmpEndPoint = tokenToEndPointMap.get(tokens.get(i));
-            if(FailureDetector.instance().isAlive(tmpEndPoint) && !topN.contains(tmpEndPoint) && !liveNodes.contains(tmpEndPoint))
-            {
-                endPoint = tmpEndPoint;
-                break;
-            }
-        }
-        return endPoint;
-    }
-
-    /*
-     * This method returns the hint map. The key is the endpoint
-     * on which the data is being placed and the value is the
-     * endpoint which is in the top N.
-     * Get the map of top N to the live nodes currently.
-     */
-    public Map<EndPoint, EndPoint> getHintedStorageEndPoints(Token token)
-    {
-        List<EndPoint> liveList = new ArrayList<EndPoint>();
-        Map<EndPoint, EndPoint> map = new HashMap<EndPoint, EndPoint>();
-        EndPoint[] topN = getStorageEndPoints( token );
-
-        for( int i = 0 ; i < topN.length ; i++)
-        {
-            if( FailureDetector.instance().isAlive(topN[i]))
-            {
-                map.put(topN[i], topN[i]);
-                liveList.add(topN[i]) ;
-            }
-            else
-            {
-                EndPoint endPoint = getNextAvailableEndPoint(topN[i], Arrays.asList(topN), liveList);
-                if(endPoint != null)
-                {
-                    map.put(endPoint, topN[i]);
-                    liveList.add(endPoint) ;
-                }
-                else
-                {
-                    // log a warning , maybe throw an exception
-                    logger_.warn("Unable to find a live Endpoint we might be out of live nodes , This is dangerous !!!!");
-                }
-            }
-        }
-        return map;
-    }
-
-    public abstract EndPoint[] getStorageEndPoints(Token token);
-}
+package org.apache.cassandra.locator;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.gms.FailureDetector;
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * This class contains a helper method that will be used by
+ * all abstraction that implement the IReplicaPlacementStrategy
+ * interface.
+*/
+public abstract class AbstractStrategy implements IReplicaPlacementStrategy
+{
+    protected static Logger logger_ = Logger.getLogger(AbstractStrategy.class);
+
+    protected TokenMetadata tokenMetadata_;
+    protected IPartitioner partitioner_;
+    protected int replicas_;
+    protected int storagePort_;
+
+    AbstractStrategy(TokenMetadata tokenMetadata, IPartitioner partitioner, int replicas, int storagePort)
+    {
+        tokenMetadata_ = tokenMetadata;
+        partitioner_ = partitioner;
+        replicas_ = replicas;
+        storagePort_ = storagePort;
+    }
+
+    /*
+     * This method changes the ports of the endpoints from
+     * the control port to the storage ports.
+    */
+    protected void retrofitPorts(List<EndPoint> eps)
+    {
+        for ( EndPoint ep : eps )
+        {
+            ep.setPort(storagePort_);
+        }
+    }
+
+    protected EndPoint getNextAvailableEndPoint(EndPoint startPoint, List<EndPoint> topN, List<EndPoint> liveNodes)
+    {
+        EndPoint endPoint = null;
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List tokens = new ArrayList(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        Token token = tokenMetadata_.getToken(startPoint);
+        int index = Collections.binarySearch(tokens, token);
+        if(index < 0)
+        {
+            index = (index + 1) * (-1);
+            if (index >= tokens.size())
+                index = 0;
+        }
+        int totalNodes = tokens.size();
+        int startIndex = (index+1)%totalNodes;
+        for (int i = startIndex, count = 1; count < totalNodes ; ++count, i = (i+1)%totalNodes)
+        {
+            EndPoint tmpEndPoint = tokenToEndPointMap.get(tokens.get(i));
+            if(FailureDetector.instance().isAlive(tmpEndPoint) && !topN.contains(tmpEndPoint) && !liveNodes.contains(tmpEndPoint))
+            {
+                endPoint = tmpEndPoint;
+                break;
+            }
+        }
+        return endPoint;
+    }
+
+    /*
+     * This method returns the hint map. The key is the endpoint
+     * on which the data is being placed and the value is the
+     * endpoint which is in the top N.
+     * Get the map of top N to the live nodes currently.
+     */
+    public Map<EndPoint, EndPoint> getHintedStorageEndPoints(Token token)
+    {
+        List<EndPoint> liveList = new ArrayList<EndPoint>();
+        Map<EndPoint, EndPoint> map = new HashMap<EndPoint, EndPoint>();
+        EndPoint[] topN = getStorageEndPoints( token );
+
+        for( int i = 0 ; i < topN.length ; i++)
+        {
+            if( FailureDetector.instance().isAlive(topN[i]))
+            {
+                map.put(topN[i], topN[i]);
+                liveList.add(topN[i]) ;
+            }
+            else
+            {
+                EndPoint endPoint = getNextAvailableEndPoint(topN[i], Arrays.asList(topN), liveList);
+                if(endPoint != null)
+                {
+                    map.put(endPoint, topN[i]);
+                    liveList.add(endPoint) ;
+                }
+                else
+                {
+                    // log a warning , maybe throw an exception
+                    logger_.warn("Unable to find a live Endpoint we might be out of live nodes , This is dangerous !!!!");
+                }
+            }
+        }
+        return map;
+    }
+
+    public abstract EndPoint[] getStorageEndPoints(Token token);
+}
diff --git a/src/java/org/apache/cassandra/locator/EndPointSnitch.java b/src/java/org/apache/cassandra/locator/EndPointSnitch.java
index fca7dd1307..72dfbc2db3 100644
--- a/src/java/org/apache/cassandra/locator/EndPointSnitch.java
+++ b/src/java/org/apache/cassandra/locator/EndPointSnitch.java
@@ -1,61 +1,61 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.locator;
-
-import java.net.*;
-
-import org.apache.cassandra.net.EndPoint;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class EndPointSnitch implements IEndPointSnitch
-{
-    public boolean isOnSameRack(EndPoint host, EndPoint host2) throws UnknownHostException
-    {
-        /*
-         * Look at the IP Address of the two hosts. Compare 
-         * the 3rd octet. If they are the same then the hosts
-         * are in the same rack else different racks. 
-        */        
-        byte[] ip = getIPAddress(host.getHost());
-        byte[] ip2 = getIPAddress(host2.getHost());
-        
-        return ( ip[2] == ip2[2] );
-    }
-    
-    public boolean isInSameDataCenter(EndPoint host, EndPoint host2) throws UnknownHostException
-    {
-        /*
-         * Look at the IP Address of the two hosts. Compare 
-         * the 2nd octet. If they are the same then the hosts
-         * are in the same datacenter else different datacenter. 
-        */
-        byte[] ip = getIPAddress(host.getHost());
-        byte[] ip2 = getIPAddress(host2.getHost());
-        
-        return ( ip[1] == ip2[1] );
-    }
-    
-    private byte[] getIPAddress(String host) throws UnknownHostException
-    {
-        InetAddress ia = InetAddress.getByName(host);         
-        return ia.getAddress();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.net.*;
+
+import org.apache.cassandra.net.EndPoint;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class EndPointSnitch implements IEndPointSnitch
+{
+    public boolean isOnSameRack(EndPoint host, EndPoint host2) throws UnknownHostException
+    {
+        /*
+         * Look at the IP Address of the two hosts. Compare 
+         * the 3rd octet. If they are the same then the hosts
+         * are in the same rack else different racks. 
+        */        
+        byte[] ip = getIPAddress(host.getHost());
+        byte[] ip2 = getIPAddress(host2.getHost());
+        
+        return ( ip[2] == ip2[2] );
+    }
+    
+    public boolean isInSameDataCenter(EndPoint host, EndPoint host2) throws UnknownHostException
+    {
+        /*
+         * Look at the IP Address of the two hosts. Compare 
+         * the 2nd octet. If they are the same then the hosts
+         * are in the same datacenter else different datacenter. 
+        */
+        byte[] ip = getIPAddress(host.getHost());
+        byte[] ip2 = getIPAddress(host2.getHost());
+        
+        return ( ip[1] == ip2[1] );
+    }
+    
+    private byte[] getIPAddress(String host) throws UnknownHostException
+    {
+        InetAddress ia = InetAddress.getByName(host);         
+        return ia.getAddress();
+    }
+}
diff --git a/src/java/org/apache/cassandra/locator/IEndPointSnitch.java b/src/java/org/apache/cassandra/locator/IEndPointSnitch.java
index 475c215802..3dd1f5a5de 100644
--- a/src/java/org/apache/cassandra/locator/IEndPointSnitch.java
+++ b/src/java/org/apache/cassandra/locator/IEndPointSnitch.java
@@ -1,52 +1,52 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.locator;
-
-import java.net.UnknownHostException;
-
-import org.apache.cassandra.net.EndPoint;
-
-
-/**
- * This interface helps determine location of node in the data center relative to another node.
- * Give a node A and another node B it can tell if A and B are on the same rack or in the same
- * data center.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IEndPointSnitch
-{
-    /**
-     * Helps determine if 2 nodes are in the same rack in the data center.
-     * @param host a specified endpoint
-     * @param host2 another specified endpoint
-     * @return true if on the same rack false otherwise
-     * @throws UnknownHostException
-     */
-    public boolean isOnSameRack(EndPoint host, EndPoint host2) throws UnknownHostException;
-    
-    /**
-     * Helps determine if 2 nodes are in the same data center.
-     * @param host a specified endpoint
-     * @param host2 another specified endpoint
-     * @return true if in the same data center false otherwise
-     * @throws UnknownHostException
-     */
-    public boolean isInSameDataCenter(EndPoint host, EndPoint host2) throws UnknownHostException;
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.net.UnknownHostException;
+
+import org.apache.cassandra.net.EndPoint;
+
+
+/**
+ * This interface helps determine location of node in the data center relative to another node.
+ * Give a node A and another node B it can tell if A and B are on the same rack or in the same
+ * data center.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IEndPointSnitch
+{
+    /**
+     * Helps determine if 2 nodes are in the same rack in the data center.
+     * @param host a specified endpoint
+     * @param host2 another specified endpoint
+     * @return true if on the same rack false otherwise
+     * @throws UnknownHostException
+     */
+    public boolean isOnSameRack(EndPoint host, EndPoint host2) throws UnknownHostException;
+    
+    /**
+     * Helps determine if 2 nodes are in the same data center.
+     * @param host a specified endpoint
+     * @param host2 another specified endpoint
+     * @return true if in the same data center false otherwise
+     * @throws UnknownHostException
+     */
+    public boolean isInSameDataCenter(EndPoint host, EndPoint host2) throws UnknownHostException;
+}
diff --git a/src/java/org/apache/cassandra/locator/IReplicaPlacementStrategy.java b/src/java/org/apache/cassandra/locator/IReplicaPlacementStrategy.java
index f550683cf2..3cd1bd50ba 100644
--- a/src/java/org/apache/cassandra/locator/IReplicaPlacementStrategy.java
+++ b/src/java/org/apache/cassandra/locator/IReplicaPlacementStrategy.java
@@ -1,40 +1,40 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.locator;
-
-import java.util.Map;
-import java.math.BigInteger;
-
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.net.EndPoint;
-
-
-/*
- * This interface has two implementations. One which
- * does not respect rack or datacenter awareness and
- * the other which does.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public interface IReplicaPlacementStrategy
-{
-	public EndPoint[] getStorageEndPoints(Token token);
-    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys);
-    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap);
-    public Map<EndPoint, EndPoint> getHintedStorageEndPoints(Token token);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.Map;
+import java.math.BigInteger;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.net.EndPoint;
+
+
+/*
+ * This interface has two implementations. One which
+ * does not respect rack or datacenter awareness and
+ * the other which does.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public interface IReplicaPlacementStrategy
+{
+	public EndPoint[] getStorageEndPoints(Token token);
+    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys);
+    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap);
+    public Map<EndPoint, EndPoint> getHintedStorageEndPoints(Token token);
+}
diff --git a/src/java/org/apache/cassandra/locator/RackAwareStrategy.java b/src/java/org/apache/cassandra/locator/RackAwareStrategy.java
index a9062fe650..19dd986613 100644
--- a/src/java/org/apache/cassandra/locator/RackAwareStrategy.java
+++ b/src/java/org/apache/cassandra/locator/RackAwareStrategy.java
@@ -1,142 +1,142 @@
-/*
-* Licensed to the Apache Software Foundation (ASF) under one
-* or more contributor license agreements.  See the NOTICE file
-* distributed with this work for additional information
-* regarding copyright ownership.  The ASF licenses this file
-* to you under the Apache License, Version 2.0 (the
-* "License"); you may not use this file except in compliance
-* with the License.  You may obtain a copy of the License at
-*
-*    http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing,
-* software distributed under the License is distributed on an
-* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-* KIND, either express or implied.  See the License for the
-* specific language governing permissions and limitations
-* under the License.
-*/
-package org.apache.cassandra.locator;
-
-import java.net.UnknownHostException;
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.LogUtil;
-
-/*
- * This class returns the nodes responsible for a given
- * key but does respects rack awareness. It makes a best
- * effort to get a node from a different data center and
- * a node in a different rack in the same datacenter as
- * the primary.
- */
-public class RackAwareStrategy extends AbstractStrategy
-{
-    public RackAwareStrategy(TokenMetadata tokenMetadata, IPartitioner partitioner, int replicas, int storagePort)
-    {
-        super(tokenMetadata, partitioner, replicas, storagePort);
-    }
-
-    public EndPoint[] getStorageEndPoints(Token token)
-    {
-        int startIndex;
-        List<EndPoint> list = new ArrayList<EndPoint>();
-        boolean bDataCenter = false;
-        boolean bOtherRack = false;
-        int foundCount = 0;
-        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-        List tokens = new ArrayList(tokenToEndPointMap.keySet());
-        Collections.sort(tokens);
-        int index = Collections.binarySearch(tokens, token);
-        if(index < 0)
-        {
-            index = (index + 1) * (-1);
-            if (index >= tokens.size())
-                index = 0;
-        }
-        int totalNodes = tokens.size();
-        // Add the node at the index by default
-        list.add(tokenToEndPointMap.get(tokens.get(index)));
-        foundCount++;
-        if( replicas_ == 1 )
-        {
-            return list.toArray(new EndPoint[list.size()]);
-        }
-        startIndex = (index + 1)%totalNodes;
-        IEndPointSnitch endPointSnitch = StorageService.instance().getEndPointSnitch();
-        
-        for (int i = startIndex, count = 1; count < totalNodes && foundCount < replicas_; ++count, i = (i+1)%totalNodes)
-        {
-            try
-            {
-                // First try to find one in a different data center
-                if(!endPointSnitch.isInSameDataCenter(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))))
-                {
-                    // If we have already found something in a diff datacenter no need to find another
-                    if( !bDataCenter )
-                    {
-                        list.add(tokenToEndPointMap.get(tokens.get(i)));
-                        bDataCenter = true;
-                        foundCount++;
-                    }
-                    continue;
-                }
-                // Now  try to find one on a different rack
-                if(!endPointSnitch.isOnSameRack(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))) &&
-                        endPointSnitch.isInSameDataCenter(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))))
-                {
-                    // If we have already found something in a diff rack no need to find another
-                    if( !bOtherRack )
-                    {
-                        list.add(tokenToEndPointMap.get(tokens.get(i)));
-                        bOtherRack = true;
-                        foundCount++;
-                    }
-                }
-            }
-            catch (UnknownHostException e)
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug(LogUtil.throwableToString(e));
-            }
-
-        }
-        // If we found N number of nodes we are good. This loop wil just exit. Otherwise just
-        // loop through the list and add until we have N nodes.
-        for (int i = startIndex, count = 1; count < totalNodes && foundCount < replicas_; ++count, i = (i+1)%totalNodes)
-        {
-            if( ! list.contains(tokenToEndPointMap.get(tokens.get(i))))
-            {
-                list.add(tokenToEndPointMap.get(tokens.get(i)));
-                foundCount++;
-            }
-        }
-        retrofitPorts(list);
-        return list.toArray(new EndPoint[list.size()]);
-    }
-    
-    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys)
-    {
-    	Map<String, EndPoint[]> results = new HashMap<String, EndPoint[]>();
-
-        for ( String key : keys )
-        {
-            results.put(key, getStorageEndPoints(partitioner_.getInitialToken(key)));
-        }
-
-        return results;
-    }
-
-    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap)
-    {
-        throw new UnsupportedOperationException("This operation is not currently supported");
-    }
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.locator;
+
+import java.net.UnknownHostException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+
+/*
+ * This class returns the nodes responsible for a given
+ * key but does respects rack awareness. It makes a best
+ * effort to get a node from a different data center and
+ * a node in a different rack in the same datacenter as
+ * the primary.
+ */
+public class RackAwareStrategy extends AbstractStrategy
+{
+    public RackAwareStrategy(TokenMetadata tokenMetadata, IPartitioner partitioner, int replicas, int storagePort)
+    {
+        super(tokenMetadata, partitioner, replicas, storagePort);
+    }
+
+    public EndPoint[] getStorageEndPoints(Token token)
+    {
+        int startIndex;
+        List<EndPoint> list = new ArrayList<EndPoint>();
+        boolean bDataCenter = false;
+        boolean bOtherRack = false;
+        int foundCount = 0;
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List tokens = new ArrayList(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        int index = Collections.binarySearch(tokens, token);
+        if(index < 0)
+        {
+            index = (index + 1) * (-1);
+            if (index >= tokens.size())
+                index = 0;
+        }
+        int totalNodes = tokens.size();
+        // Add the node at the index by default
+        list.add(tokenToEndPointMap.get(tokens.get(index)));
+        foundCount++;
+        if( replicas_ == 1 )
+        {
+            return list.toArray(new EndPoint[list.size()]);
+        }
+        startIndex = (index + 1)%totalNodes;
+        IEndPointSnitch endPointSnitch = StorageService.instance().getEndPointSnitch();
+        
+        for (int i = startIndex, count = 1; count < totalNodes && foundCount < replicas_; ++count, i = (i+1)%totalNodes)
+        {
+            try
+            {
+                // First try to find one in a different data center
+                if(!endPointSnitch.isInSameDataCenter(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))))
+                {
+                    // If we have already found something in a diff datacenter no need to find another
+                    if( !bDataCenter )
+                    {
+                        list.add(tokenToEndPointMap.get(tokens.get(i)));
+                        bDataCenter = true;
+                        foundCount++;
+                    }
+                    continue;
+                }
+                // Now  try to find one on a different rack
+                if(!endPointSnitch.isOnSameRack(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))) &&
+                        endPointSnitch.isInSameDataCenter(tokenToEndPointMap.get(tokens.get(index)), tokenToEndPointMap.get(tokens.get(i))))
+                {
+                    // If we have already found something in a diff rack no need to find another
+                    if( !bOtherRack )
+                    {
+                        list.add(tokenToEndPointMap.get(tokens.get(i)));
+                        bOtherRack = true;
+                        foundCount++;
+                    }
+                }
+            }
+            catch (UnknownHostException e)
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug(LogUtil.throwableToString(e));
+            }
+
+        }
+        // If we found N number of nodes we are good. This loop wil just exit. Otherwise just
+        // loop through the list and add until we have N nodes.
+        for (int i = startIndex, count = 1; count < totalNodes && foundCount < replicas_; ++count, i = (i+1)%totalNodes)
+        {
+            if( ! list.contains(tokenToEndPointMap.get(tokens.get(i))))
+            {
+                list.add(tokenToEndPointMap.get(tokens.get(i)));
+                foundCount++;
+            }
+        }
+        retrofitPorts(list);
+        return list.toArray(new EndPoint[list.size()]);
+    }
+    
+    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys)
+    {
+    	Map<String, EndPoint[]> results = new HashMap<String, EndPoint[]>();
+
+        for ( String key : keys )
+        {
+            results.put(key, getStorageEndPoints(partitioner_.getInitialToken(key)));
+        }
+
+        return results;
+    }
+
+    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap)
+    {
+        throw new UnsupportedOperationException("This operation is not currently supported");
+    }
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/locator/RackUnawareStrategy.java b/src/java/org/apache/cassandra/locator/RackUnawareStrategy.java
index f7feb69db6..cdcd3116aa 100644
--- a/src/java/org/apache/cassandra/locator/RackUnawareStrategy.java
+++ b/src/java/org/apache/cassandra/locator/RackUnawareStrategy.java
@@ -16,78 +16,78 @@
 * specific language governing permissions and limitations
 * under the License.
 */
-package org.apache.cassandra.locator;
-
-import java.util.ArrayList;
-import java.util.Collections;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.net.EndPoint;
-
-/**
- * This class returns the nodes responsible for a given
- * key but does not respect rack awareness. Basically
- * returns the 3 nodes that lie right next to each other
- * on the ring.
- */
-public class RackUnawareStrategy extends AbstractStrategy
-{
-    public RackUnawareStrategy(TokenMetadata tokenMetadata, IPartitioner partitioner, int replicas, int storagePort)
-    {
-        super(tokenMetadata, partitioner, replicas, storagePort);
-    }
-
-    public EndPoint[] getStorageEndPoints(Token token)
-    {
-        return getStorageEndPoints(token, tokenMetadata_.cloneTokenEndPointMap());            
-    }
-    
-    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap)
-    {
-        int startIndex;
-        List<EndPoint> list = new ArrayList<EndPoint>();
-        int foundCount = 0;
-        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
-        Collections.sort(tokens);
-        int index = Collections.binarySearch(tokens, token);
-        if(index < 0)
-        {
-            index = (index + 1) * (-1);
-            if (index >= tokens.size())
-                index = 0;
-        }
-        int totalNodes = tokens.size();
-        // Add the node at the index by default
-        list.add(tokenToEndPointMap.get(tokens.get(index)));
-        foundCount++;
-        startIndex = (index + 1)%totalNodes;
-        // If we found N number of nodes we are good. This loop will just exit. Otherwise just
-        // loop through the list and add until we have N nodes.
-        for (int i = startIndex, count = 1; count < totalNodes && foundCount < replicas_; ++count, i = (i+1)%totalNodes)
-        {
-            if( ! list.contains(tokenToEndPointMap.get(tokens.get(i))))
-            {
-                list.add(tokenToEndPointMap.get(tokens.get(i)));
-                foundCount++;
-            }
-        }
-        retrofitPorts(list);
-        return list.toArray(new EndPoint[list.size()]);
-    }
-            
-    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys)
-    {
-    	Map<String, EndPoint[]> results = new HashMap<String, EndPoint[]>();
-
-        for ( String key : keys )
-        {
-            results.put(key, getStorageEndPoints(partitioner_.getInitialToken(key)));
-        }
-
-        return results;
-    }
-}
+package org.apache.cassandra.locator;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * This class returns the nodes responsible for a given
+ * key but does not respect rack awareness. Basically
+ * returns the 3 nodes that lie right next to each other
+ * on the ring.
+ */
+public class RackUnawareStrategy extends AbstractStrategy
+{
+    public RackUnawareStrategy(TokenMetadata tokenMetadata, IPartitioner partitioner, int replicas, int storagePort)
+    {
+        super(tokenMetadata, partitioner, replicas, storagePort);
+    }
+
+    public EndPoint[] getStorageEndPoints(Token token)
+    {
+        return getStorageEndPoints(token, tokenMetadata_.cloneTokenEndPointMap());            
+    }
+    
+    public EndPoint[] getStorageEndPoints(Token token, Map<Token, EndPoint> tokenToEndPointMap)
+    {
+        int startIndex;
+        List<EndPoint> list = new ArrayList<EndPoint>();
+        int foundCount = 0;
+        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        int index = Collections.binarySearch(tokens, token);
+        if(index < 0)
+        {
+            index = (index + 1) * (-1);
+            if (index >= tokens.size())
+                index = 0;
+        }
+        int totalNodes = tokens.size();
+        // Add the node at the index by default
+        list.add(tokenToEndPointMap.get(tokens.get(index)));
+        foundCount++;
+        startIndex = (index + 1)%totalNodes;
+        // If we found N number of nodes we are good. This loop will just exit. Otherwise just
+        // loop through the list and add until we have N nodes.
+        for (int i = startIndex, count = 1; count < totalNodes && foundCount < replicas_; ++count, i = (i+1)%totalNodes)
+        {
+            if( ! list.contains(tokenToEndPointMap.get(tokens.get(i))))
+            {
+                list.add(tokenToEndPointMap.get(tokens.get(i)));
+                foundCount++;
+            }
+        }
+        retrofitPorts(list);
+        return list.toArray(new EndPoint[list.size()]);
+    }
+            
+    public Map<String, EndPoint[]> getStorageEndPoints(String[] keys)
+    {
+    	Map<String, EndPoint[]> results = new HashMap<String, EndPoint[]>();
+
+        for ( String key : keys )
+        {
+            results.put(key, getStorageEndPoints(partitioner_.getInitialToken(key)));
+        }
+
+        return results;
+    }
+}
diff --git a/src/java/org/apache/cassandra/locator/TokenMetadata.java b/src/java/org/apache/cassandra/locator/TokenMetadata.java
index 82129cfc61..17ef95aea0 100644
--- a/src/java/org/apache/cassandra/locator/TokenMetadata.java
+++ b/src/java/org/apache/cassandra/locator/TokenMetadata.java
@@ -1,173 +1,173 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.locator;
-
-import java.util.HashMap;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.locks.ReadWriteLock;
-import java.util.concurrent.locks.ReentrantReadWriteLock;
-
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.net.EndPoint;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class TokenMetadata
-{
-    /* Maintains token to endpoint map of every node in the cluster. */
-    private Map<Token, EndPoint> tokenToEndPointMap_ = new HashMap<Token, EndPoint>();
-    /* Maintains a reverse index of endpoint to token in the cluster. */
-    private Map<EndPoint, Token> endPointToTokenMap_ = new HashMap<EndPoint, Token>();
-    
-    /* Use this lock for manipulating the token map */
-    private final ReadWriteLock lock_ = new ReentrantReadWriteLock(true);
-
-    public TokenMetadata()
-    {
-    }
-
-    private TokenMetadata(Map<Token, EndPoint> tokenToEndPointMap, Map<EndPoint, Token> endPointToTokenMap)
-    {
-        tokenToEndPointMap_ = tokenToEndPointMap;
-        endPointToTokenMap_ = endPointToTokenMap;
-    }
-    
-    public TokenMetadata cloneMe()
-    {
-        return new TokenMetadata(cloneTokenEndPointMap(), cloneEndPointTokenMap());
-    }
-    
-    /**
-     * Update the two maps in an safe mode. 
-    */
-    public void update(Token token, EndPoint endpoint)
-    {
-        lock_.writeLock().lock();
-        try
-        {            
-            Token oldToken = endPointToTokenMap_.get(endpoint);
-            if ( oldToken != null )
-                tokenToEndPointMap_.remove(oldToken);
-            tokenToEndPointMap_.put(token, endpoint);
-            endPointToTokenMap_.put(endpoint, token);
-        }
-        finally
-        {
-            lock_.writeLock().unlock();
-        }
-    }
-    
-    /**
-     * Remove the entries in the two maps.
-     * @param endpoint
-     */
-    public void remove(EndPoint endpoint)
-    {
-        lock_.writeLock().lock();
-        try
-        {            
-            Token oldToken = endPointToTokenMap_.get(endpoint);
-            if ( oldToken != null )
-                tokenToEndPointMap_.remove(oldToken);            
-            endPointToTokenMap_.remove(endpoint);
-        }
-        finally
-        {
-            lock_.writeLock().unlock();
-        }
-    }
-    
-    public Token getToken(EndPoint endpoint)
-    {
-        lock_.readLock().lock();
-        try
-        {
-            return endPointToTokenMap_.get(endpoint);
-        }
-        finally
-        {
-            lock_.readLock().unlock();
-        }
-    }
-    
-    public boolean isKnownEndPoint(EndPoint ep)
-    {
-        lock_.readLock().lock();
-        try
-        {
-            return endPointToTokenMap_.containsKey(ep);
-        }
-        finally
-        {
-            lock_.readLock().unlock();
-        }
-    }
-    
-    /*
-     * Returns a safe clone of tokenToEndPointMap_.
-    */
-    public Map<Token, EndPoint> cloneTokenEndPointMap()
-    {
-        lock_.readLock().lock();
-        try
-        {            
-            return new HashMap<Token, EndPoint>( tokenToEndPointMap_ );
-        }
-        finally
-        {
-            lock_.readLock().unlock();
-        }
-    }
-    
-    /*
-     * Returns a safe clone of endPointTokenMap_.
-    */
-    public Map<EndPoint, Token> cloneEndPointTokenMap()
-    {
-        lock_.readLock().lock();
-        try
-        {            
-            return new HashMap<EndPoint, Token>( endPointToTokenMap_ );
-        }
-        finally
-        {
-            lock_.readLock().unlock();
-        }
-    }
-    
-    public String toString()
-    {
-        StringBuilder sb = new StringBuilder();
-        Set<EndPoint> eps = endPointToTokenMap_.keySet();
-        
-        for ( EndPoint ep : eps )
-        {
-            sb.append(ep);
-            sb.append(":");
-            sb.append(endPointToTokenMap_.get(ep));
-            sb.append(System.getProperty("line.separator"));
-        }
-        
-        return sb.toString();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.locator;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.locks.ReadWriteLock;
+import java.util.concurrent.locks.ReentrantReadWriteLock;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.net.EndPoint;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TokenMetadata
+{
+    /* Maintains token to endpoint map of every node in the cluster. */
+    private Map<Token, EndPoint> tokenToEndPointMap_ = new HashMap<Token, EndPoint>();
+    /* Maintains a reverse index of endpoint to token in the cluster. */
+    private Map<EndPoint, Token> endPointToTokenMap_ = new HashMap<EndPoint, Token>();
+    
+    /* Use this lock for manipulating the token map */
+    private final ReadWriteLock lock_ = new ReentrantReadWriteLock(true);
+
+    public TokenMetadata()
+    {
+    }
+
+    private TokenMetadata(Map<Token, EndPoint> tokenToEndPointMap, Map<EndPoint, Token> endPointToTokenMap)
+    {
+        tokenToEndPointMap_ = tokenToEndPointMap;
+        endPointToTokenMap_ = endPointToTokenMap;
+    }
+    
+    public TokenMetadata cloneMe()
+    {
+        return new TokenMetadata(cloneTokenEndPointMap(), cloneEndPointTokenMap());
+    }
+    
+    /**
+     * Update the two maps in an safe mode. 
+    */
+    public void update(Token token, EndPoint endpoint)
+    {
+        lock_.writeLock().lock();
+        try
+        {            
+            Token oldToken = endPointToTokenMap_.get(endpoint);
+            if ( oldToken != null )
+                tokenToEndPointMap_.remove(oldToken);
+            tokenToEndPointMap_.put(token, endpoint);
+            endPointToTokenMap_.put(endpoint, token);
+        }
+        finally
+        {
+            lock_.writeLock().unlock();
+        }
+    }
+    
+    /**
+     * Remove the entries in the two maps.
+     * @param endpoint
+     */
+    public void remove(EndPoint endpoint)
+    {
+        lock_.writeLock().lock();
+        try
+        {            
+            Token oldToken = endPointToTokenMap_.get(endpoint);
+            if ( oldToken != null )
+                tokenToEndPointMap_.remove(oldToken);            
+            endPointToTokenMap_.remove(endpoint);
+        }
+        finally
+        {
+            lock_.writeLock().unlock();
+        }
+    }
+    
+    public Token getToken(EndPoint endpoint)
+    {
+        lock_.readLock().lock();
+        try
+        {
+            return endPointToTokenMap_.get(endpoint);
+        }
+        finally
+        {
+            lock_.readLock().unlock();
+        }
+    }
+    
+    public boolean isKnownEndPoint(EndPoint ep)
+    {
+        lock_.readLock().lock();
+        try
+        {
+            return endPointToTokenMap_.containsKey(ep);
+        }
+        finally
+        {
+            lock_.readLock().unlock();
+        }
+    }
+    
+    /*
+     * Returns a safe clone of tokenToEndPointMap_.
+    */
+    public Map<Token, EndPoint> cloneTokenEndPointMap()
+    {
+        lock_.readLock().lock();
+        try
+        {            
+            return new HashMap<Token, EndPoint>( tokenToEndPointMap_ );
+        }
+        finally
+        {
+            lock_.readLock().unlock();
+        }
+    }
+    
+    /*
+     * Returns a safe clone of endPointTokenMap_.
+    */
+    public Map<EndPoint, Token> cloneEndPointTokenMap()
+    {
+        lock_.readLock().lock();
+        try
+        {            
+            return new HashMap<EndPoint, Token>( endPointToTokenMap_ );
+        }
+        finally
+        {
+            lock_.readLock().unlock();
+        }
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder();
+        Set<EndPoint> eps = endPointToTokenMap_.keySet();
+        
+        for ( EndPoint ep : eps )
+        {
+            sb.append(ep);
+            sb.append(":");
+            sb.append(endPointToTokenMap_.get(ep));
+            sb.append(System.getProperty("line.separator"));
+        }
+        
+        return sb.toString();
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/AsyncResult.java b/src/java/org/apache/cassandra/net/AsyncResult.java
index 684850b624..3c884bc0b1 100644
--- a/src/java/org/apache/cassandra/net/AsyncResult.java
+++ b/src/java/org/apache/cassandra/net/AsyncResult.java
@@ -1,132 +1,132 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.util.List;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.TimeoutException;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.locks.Condition;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class AsyncResult implements IAsyncResult
-{
-    private static Logger logger_ = Logger.getLogger( AsyncResult.class );
-    private byte[] result_;
-    private AtomicBoolean done_ = new AtomicBoolean(false);
-    private Lock lock_ = new ReentrantLock();
-    private Condition condition_;
-
-    public AsyncResult()
-    {        
-        condition_ = lock_.newCondition();
-    }    
-    
-    public byte[] get()
-    {
-        lock_.lock();
-        try
-        {
-            if ( !done_.get() )
-            {
-                condition_.await();                    
-            }
-        }
-        catch ( InterruptedException ex )
-        {
-            logger_.warn( LogUtil.throwableToString(ex) );
-        }
-        finally
-        {
-            lock_.unlock();            
-        }        
-        return result_;
-    }
-    
-    public boolean isDone()
-    {
-        return done_.get();
-    }
-    
-    public byte[] get(long timeout, TimeUnit tu) throws TimeoutException
-    {
-        lock_.lock();
-        try
-        {            
-            boolean bVal = true;
-            try
-            {
-                if ( !done_.get() )
-                {                    
-                    bVal = condition_.await(timeout, tu);
-                }
-            }
-            catch ( InterruptedException ex )
-            {
-                logger_.warn( LogUtil.throwableToString(ex) );
-            }
-            
-            if ( !bVal && !done_.get() )
-            {                                           
-                throw new TimeoutException("Operation timed out.");
-            }
-        }
-        finally
-        {
-            lock_.unlock();      
-        }
-        return result_;
-    }
-    
-    public List<byte[]> multiget()
-    {
-        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
-    }
-    
-    public List<byte[]> multiget(long timeout, TimeUnit tu) throws TimeoutException
-    {
-        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
-    }
-    
-    public void result(Message response)
-    {        
-        try
-        {
-            lock_.lock();
-            if ( !done_.get() )
-            {                
-                result_ = response.getMessageBody();
-                done_.set(true);
-                condition_.signal();
-            }
-        }
-        finally
-        {
-            lock_.unlock();
-        }        
-    }    
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class AsyncResult implements IAsyncResult
+{
+    private static Logger logger_ = Logger.getLogger( AsyncResult.class );
+    private byte[] result_;
+    private AtomicBoolean done_ = new AtomicBoolean(false);
+    private Lock lock_ = new ReentrantLock();
+    private Condition condition_;
+
+    public AsyncResult()
+    {        
+        condition_ = lock_.newCondition();
+    }    
+    
+    public byte[] get()
+    {
+        lock_.lock();
+        try
+        {
+            if ( !done_.get() )
+            {
+                condition_.await();                    
+            }
+        }
+        catch ( InterruptedException ex )
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+        finally
+        {
+            lock_.unlock();            
+        }        
+        return result_;
+    }
+    
+    public boolean isDone()
+    {
+        return done_.get();
+    }
+    
+    public byte[] get(long timeout, TimeUnit tu) throws TimeoutException
+    {
+        lock_.lock();
+        try
+        {            
+            boolean bVal = true;
+            try
+            {
+                if ( !done_.get() )
+                {                    
+                    bVal = condition_.await(timeout, tu);
+                }
+            }
+            catch ( InterruptedException ex )
+            {
+                logger_.warn( LogUtil.throwableToString(ex) );
+            }
+            
+            if ( !bVal && !done_.get() )
+            {                                           
+                throw new TimeoutException("Operation timed out.");
+            }
+        }
+        finally
+        {
+            lock_.unlock();      
+        }
+        return result_;
+    }
+    
+    public List<byte[]> multiget()
+    {
+        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
+    }
+    
+    public List<byte[]> multiget(long timeout, TimeUnit tu) throws TimeoutException
+    {
+        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
+    }
+    
+    public void result(Message response)
+    {        
+        try
+        {
+            lock_.lock();
+            if ( !done_.get() )
+            {                
+                result_ = response.getMessageBody();
+                done_.set(true);
+                condition_.signal();
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }        
+    }    
+}
diff --git a/src/java/org/apache/cassandra/net/CompactEndPointSerializationHelper.java b/src/java/org/apache/cassandra/net/CompactEndPointSerializationHelper.java
index 6ddf1a42d9..6461c0878b 100644
--- a/src/java/org/apache/cassandra/net/CompactEndPointSerializationHelper.java
+++ b/src/java/org/apache/cassandra/net/CompactEndPointSerializationHelper.java
@@ -1,50 +1,50 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.io.*;
-import java.net.InetAddress;
-import java.net.UnknownHostException;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class CompactEndPointSerializationHelper
-{
-    public static void serialize(EndPoint endPoint, DataOutputStream dos) throws IOException
-    {        
-        dos.write(EndPoint.toBytes(endPoint));
-    }
-    
-    public static EndPoint deserialize(DataInputStream dis) throws IOException
-    {     
-        byte[] bytes = new byte[6];
-        dis.readFully(bytes, 0, bytes.length);
-        return EndPoint.fromBytes(bytes);       
-    }
-    
-    public static void main(String[] args) throws Throwable
-    {
-        EndPoint ep = new EndPoint(7000);
-        byte[] bytes = EndPoint.toBytes(ep);
-        System.out.println(bytes.length);
-        EndPoint ep2 = EndPoint.fromBytes(bytes);
-        System.out.println(ep2);
-    }
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.*;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class CompactEndPointSerializationHelper
+{
+    public static void serialize(EndPoint endPoint, DataOutputStream dos) throws IOException
+    {        
+        dos.write(EndPoint.toBytes(endPoint));
+    }
+    
+    public static EndPoint deserialize(DataInputStream dis) throws IOException
+    {     
+        byte[] bytes = new byte[6];
+        dis.readFully(bytes, 0, bytes.length);
+        return EndPoint.fromBytes(bytes);       
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {
+        EndPoint ep = new EndPoint(7000);
+        byte[] bytes = EndPoint.toBytes(ep);
+        System.out.println(bytes.length);
+        EndPoint ep2 = EndPoint.fromBytes(bytes);
+        System.out.println(ep2);
+    }
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/net/ConnectionStatistics.java b/src/java/org/apache/cassandra/net/ConnectionStatistics.java
index 3b84361ff9..9ae6bbe3f5 100644
--- a/src/java/org/apache/cassandra/net/ConnectionStatistics.java
+++ b/src/java/org/apache/cassandra/net/ConnectionStatistics.java
@@ -1,78 +1,78 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class ConnectionStatistics
-{
-    private String localHost;
-    private int localPort;
-    private String remoteHost;
-    private int remotePort;
-    private int totalConnections;
-    private int connectionsInUse;
-
-    ConnectionStatistics(EndPoint localEp, EndPoint remoteEp, int tc, int ciu)
-    {
-        localHost = localEp.getHost();
-        localPort = localEp.getPort();
-        remoteHost = remoteEp.getHost();
-        remotePort = remoteEp.getPort();
-        totalConnections = tc;
-        connectionsInUse = ciu;
-    }
-    
-    public String getLocalHost()
-    {
-        return localHost;
-    }
-    
-    public int getLocalPort()
-    {
-        return localPort;
-    }
-    
-    public String getRemoteHost()
-    {
-        return remoteHost;
-    }
-    
-    public int getRemotePort()
-    {
-        return remotePort;
-    }
-    
-    public int getTotalConnections()
-    {
-        return totalConnections;
-    }
-    
-    public int getConnectionInUse()
-    {
-        return connectionsInUse;
-    }
-
-    public String toString()
-    {
-        return localHost + ":" + localPort + "->" + remoteHost + ":" + remotePort + " Total Connections open : " + totalConnections + " Connections in use : " + connectionsInUse;
-    }
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ConnectionStatistics
+{
+    private String localHost;
+    private int localPort;
+    private String remoteHost;
+    private int remotePort;
+    private int totalConnections;
+    private int connectionsInUse;
+
+    ConnectionStatistics(EndPoint localEp, EndPoint remoteEp, int tc, int ciu)
+    {
+        localHost = localEp.getHost();
+        localPort = localEp.getPort();
+        remoteHost = remoteEp.getHost();
+        remotePort = remoteEp.getPort();
+        totalConnections = tc;
+        connectionsInUse = ciu;
+    }
+    
+    public String getLocalHost()
+    {
+        return localHost;
+    }
+    
+    public int getLocalPort()
+    {
+        return localPort;
+    }
+    
+    public String getRemoteHost()
+    {
+        return remoteHost;
+    }
+    
+    public int getRemotePort()
+    {
+        return remotePort;
+    }
+    
+    public int getTotalConnections()
+    {
+        return totalConnections;
+    }
+    
+    public int getConnectionInUse()
+    {
+        return connectionsInUse;
+    }
+
+    public String toString()
+    {
+        return localHost + ":" + localPort + "->" + remoteHost + ":" + remotePort + " Total Connections open : " + totalConnections + " Connections in use : " + connectionsInUse;
+    }
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/net/EndPoint.java b/src/java/org/apache/cassandra/net/EndPoint.java
index dcbe9443e8..fe7905ff76 100644
--- a/src/java/org/apache/cassandra/net/EndPoint.java
+++ b/src/java/org/apache/cassandra/net/EndPoint.java
@@ -1,175 +1,175 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-
-import java.io.IOException;
-import java.io.Serializable;
-import java.net.*;
-import java.nio.ByteBuffer;
-import java.nio.CharBuffer;
-import java.util.HashMap;
-import java.util.Map;
-
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class EndPoint implements Serializable, Comparable<EndPoint>
-{
-    // logging and profiling.
-    private static Logger logger_ = Logger.getLogger(EndPoint.class);
-    private static final long serialVersionUID = -4962625949179835907L;
-    private static Map<CharBuffer, String> hostNames_ = new HashMap<CharBuffer, String>();
-
-    // use as a kind of magic number to send ourselves a message indicating listening state
-    protected static final int sentinelPort_ = 5555;
-    public static EndPoint sentinelLocalEndPoint_;
-    
-    static
-    {
-        try
-        {
-            sentinelLocalEndPoint_ = new EndPoint(FBUtilities.getHostAddress(), EndPoint.sentinelPort_);
-        }        
-        catch ( IOException ex )
-        {
-            logger_.warn(LogUtil.throwableToString(ex));
-        }
-    }
-
-    private String host_;
-    private int port_;
-
-    private transient InetSocketAddress ia_;
-
-    public EndPoint(String host, int port)
-    {
-        assert host.matches("\\d+\\.\\d+\\.\\d+\\.\\d+") : host;
-        host_ = host;
-        port_ = port;
-    }
-
-    // create a local endpoint id
-    public EndPoint(int port)
-    {
-        try
-        {
-            host_ = FBUtilities.getHostAddress();
-        }
-        catch (UnknownHostException e)
-        {
-            throw new RuntimeException(e);
-        }
-        port_ = port;
-    }
-
-    public String getHost()
-    {
-        return host_;
-    }
-
-    public int getPort()
-    {
-        return port_;
-    }
-
-    public void setPort(int port)
-    {
-        port_ = port;
-    }
-
-    public InetSocketAddress getInetAddress()
-    {
-        if (ia_ == null || ia_.isUnresolved())
-        {
-            ia_ = new InetSocketAddress(host_, port_);
-        }
-        return ia_;
-    }
-
-    public boolean equals(Object o)
-    {
-        if (!(o instanceof EndPoint))
-            return false;
-
-        EndPoint rhs = (EndPoint) o;
-        return (host_.equals(rhs.host_) && port_ == rhs.port_);
-    }
-
-    public int hashCode()
-    {
-        return (host_ + port_).hashCode();
-    }
-
-    public int compareTo(EndPoint rhs)
-    {
-        return host_.compareTo(rhs.host_);
-    }
-
-    public String toString()
-    {
-        return (host_ + ":" + port_);
-    }
-
-    public static EndPoint fromString(String str)
-    {
-        String[] values = str.split(":");
-        return new EndPoint(values[0], Integer.parseInt(values[1]));
-    }
-
-    public static byte[] toBytes(EndPoint ep)
-    {
-        ByteBuffer buffer = ByteBuffer.allocate(6);
-        byte[] iaBytes = ep.getInetAddress().getAddress().getAddress();
-        buffer.put(iaBytes);
-        buffer.put(MessagingService.toByteArray((short) ep.getPort()));
-        buffer.flip();
-        return buffer.array();
-    }
-
-    public static EndPoint fromBytes(byte[] bytes)
-    {
-        ByteBuffer buffer = ByteBuffer.allocate(4);
-        System.arraycopy(bytes, 0, buffer.array(), 0, 4);
-        byte[] portBytes = new byte[2];
-        System.arraycopy(bytes, 4, portBytes, 0, portBytes.length);
-        try
-        {
-            CharBuffer charBuffer = buffer.asCharBuffer();
-            String host = hostNames_.get(charBuffer);
-            if (host == null)
-            {               
-                host = InetAddress.getByAddress(buffer.array()).getHostAddress();              
-                hostNames_.put(charBuffer, host);
-            }
-            int port = (int) MessagingService.byteArrayToShort(portBytes);
-            return new EndPoint(host, port);
-        }
-        catch (UnknownHostException e)
-        {
-            throw new IllegalArgumentException(e);
-        }
-    }
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.net.*;
+import java.nio.ByteBuffer;
+import java.nio.CharBuffer;
+import java.util.HashMap;
+import java.util.Map;
+
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class EndPoint implements Serializable, Comparable<EndPoint>
+{
+    // logging and profiling.
+    private static Logger logger_ = Logger.getLogger(EndPoint.class);
+    private static final long serialVersionUID = -4962625949179835907L;
+    private static Map<CharBuffer, String> hostNames_ = new HashMap<CharBuffer, String>();
+
+    // use as a kind of magic number to send ourselves a message indicating listening state
+    protected static final int sentinelPort_ = 5555;
+    public static EndPoint sentinelLocalEndPoint_;
+    
+    static
+    {
+        try
+        {
+            sentinelLocalEndPoint_ = new EndPoint(FBUtilities.getHostAddress(), EndPoint.sentinelPort_);
+        }        
+        catch ( IOException ex )
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }
+    }
+
+    private String host_;
+    private int port_;
+
+    private transient InetSocketAddress ia_;
+
+    public EndPoint(String host, int port)
+    {
+        assert host.matches("\\d+\\.\\d+\\.\\d+\\.\\d+") : host;
+        host_ = host;
+        port_ = port;
+    }
+
+    // create a local endpoint id
+    public EndPoint(int port)
+    {
+        try
+        {
+            host_ = FBUtilities.getHostAddress();
+        }
+        catch (UnknownHostException e)
+        {
+            throw new RuntimeException(e);
+        }
+        port_ = port;
+    }
+
+    public String getHost()
+    {
+        return host_;
+    }
+
+    public int getPort()
+    {
+        return port_;
+    }
+
+    public void setPort(int port)
+    {
+        port_ = port;
+    }
+
+    public InetSocketAddress getInetAddress()
+    {
+        if (ia_ == null || ia_.isUnresolved())
+        {
+            ia_ = new InetSocketAddress(host_, port_);
+        }
+        return ia_;
+    }
+
+    public boolean equals(Object o)
+    {
+        if (!(o instanceof EndPoint))
+            return false;
+
+        EndPoint rhs = (EndPoint) o;
+        return (host_.equals(rhs.host_) && port_ == rhs.port_);
+    }
+
+    public int hashCode()
+    {
+        return (host_ + port_).hashCode();
+    }
+
+    public int compareTo(EndPoint rhs)
+    {
+        return host_.compareTo(rhs.host_);
+    }
+
+    public String toString()
+    {
+        return (host_ + ":" + port_);
+    }
+
+    public static EndPoint fromString(String str)
+    {
+        String[] values = str.split(":");
+        return new EndPoint(values[0], Integer.parseInt(values[1]));
+    }
+
+    public static byte[] toBytes(EndPoint ep)
+    {
+        ByteBuffer buffer = ByteBuffer.allocate(6);
+        byte[] iaBytes = ep.getInetAddress().getAddress().getAddress();
+        buffer.put(iaBytes);
+        buffer.put(MessagingService.toByteArray((short) ep.getPort()));
+        buffer.flip();
+        return buffer.array();
+    }
+
+    public static EndPoint fromBytes(byte[] bytes)
+    {
+        ByteBuffer buffer = ByteBuffer.allocate(4);
+        System.arraycopy(bytes, 0, buffer.array(), 0, 4);
+        byte[] portBytes = new byte[2];
+        System.arraycopy(bytes, 4, portBytes, 0, portBytes.length);
+        try
+        {
+            CharBuffer charBuffer = buffer.asCharBuffer();
+            String host = hostNames_.get(charBuffer);
+            if (host == null)
+            {               
+                host = InetAddress.getByAddress(buffer.array()).getHostAddress();              
+                hostNames_.put(charBuffer, host);
+            }
+            int port = (int) MessagingService.byteArrayToShort(portBytes);
+            return new EndPoint(host, port);
+        }
+        catch (UnknownHostException e)
+        {
+            throw new IllegalArgumentException(e);
+        }
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/net/FileStreamTask.java b/src/java/org/apache/cassandra/net/FileStreamTask.java
index 686ab1ed0c..8bdb605e17 100644
--- a/src/java/org/apache/cassandra/net/FileStreamTask.java
+++ b/src/java/org/apache/cassandra/net/FileStreamTask.java
@@ -1,85 +1,85 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.io.*;
-import java.net.SocketException;
-
-import org.apache.cassandra.net.sink.SinkManager;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class FileStreamTask implements Runnable
-{
-    private static Logger logger_ = Logger.getLogger( FileStreamTask.class );
-    
-    private String file_;
-    private long startPosition_;
-    private long total_;
-    private EndPoint from_;
-    private EndPoint to_;
-    
-    FileStreamTask(String file, long startPosition, long total, EndPoint from, EndPoint to)
-    {
-        file_ = file;
-        startPosition_ = startPosition;
-        total_ = total;
-        from_ = from;
-        to_ = to;
-    }
-    
-    public void run()
-    {
-        TcpConnection connection = null;
-        try
-        {                        
-            connection = new TcpConnection(from_, to_);
-            File file = new File(file_);             
-            connection.stream(file, startPosition_, total_);
-            MessagingService.setStreamingMode(false);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Done streaming " + file);
-        }            
-        catch ( SocketException se )
-        {                        
-            logger_.info(LogUtil.throwableToString(se));
-        }
-        catch ( IOException e )
-        {
-            logConnectAndIOException(e, connection);
-        }
-        catch (Throwable th)
-        {
-            logger_.warn(LogUtil.throwableToString(th));
-        }        
-    }
-    
-    private void logConnectAndIOException(IOException ex, TcpConnection connection)
-    {                    
-        if ( connection != null )
-        {
-            connection.errorClose();
-        }
-        logger_.info(LogUtil.throwableToString(ex));
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.*;
+import java.net.SocketException;
+
+import org.apache.cassandra.net.sink.SinkManager;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class FileStreamTask implements Runnable
+{
+    private static Logger logger_ = Logger.getLogger( FileStreamTask.class );
+    
+    private String file_;
+    private long startPosition_;
+    private long total_;
+    private EndPoint from_;
+    private EndPoint to_;
+    
+    FileStreamTask(String file, long startPosition, long total, EndPoint from, EndPoint to)
+    {
+        file_ = file;
+        startPosition_ = startPosition;
+        total_ = total;
+        from_ = from;
+        to_ = to;
+    }
+    
+    public void run()
+    {
+        TcpConnection connection = null;
+        try
+        {                        
+            connection = new TcpConnection(from_, to_);
+            File file = new File(file_);             
+            connection.stream(file, startPosition_, total_);
+            MessagingService.setStreamingMode(false);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Done streaming " + file);
+        }            
+        catch ( SocketException se )
+        {                        
+            logger_.info(LogUtil.throwableToString(se));
+        }
+        catch ( IOException e )
+        {
+            logConnectAndIOException(e, connection);
+        }
+        catch (Throwable th)
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }        
+    }
+    
+    private void logConnectAndIOException(IOException ex, TcpConnection connection)
+    {                    
+        if ( connection != null )
+        {
+            connection.errorClose();
+        }
+        logger_.info(LogUtil.throwableToString(ex));
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/Header.java b/src/java/org/apache/cassandra/net/Header.java
index 07ebeb7a8e..212724326b 100644
--- a/src/java/org/apache/cassandra/net/Header.java
+++ b/src/java/org/apache/cassandra/net/Header.java
@@ -1,182 +1,182 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.*;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.utils.GuidGenerator;
-
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class Header implements java.io.Serializable
-{
-    static final long serialVersionUID = -3194851946523170022L;
-    private static ICompactSerializer<Header> serializer_;
-    private static AtomicInteger idGen_ = new AtomicInteger(0);
-    
-    static
-    {
-        serializer_ = new HeaderSerializer();        
-    }
-    
-    static ICompactSerializer<Header> serializer()
-    {
-        return serializer_;
-    }
-
-    private EndPoint from_;
-    private String type_;
-    private String verb_;
-    private String messageId_;
-    protected Map<String, byte[]> details_ = new Hashtable<String, byte[]>();
-    
-    Header(String id, EndPoint from, String messageType, String verb)
-    {
-        messageId_ = id;
-        from_ = from;
-        type_ = messageType;
-        verb_ = verb;        
-    }
-    
-    Header(String id, EndPoint from, String messageType, String verb, Map<String, byte[]> details)
-    {
-        this(id, from, messageType, verb);
-        details_ = details;
-    }
-
-    Header(EndPoint from, String messageType, String verb)
-    {
-        messageId_ = Integer.toString(idGen_.incrementAndGet());
-        from_ = from;
-        type_ = messageType;
-        verb_ = verb;
-    }        
-
-    EndPoint getFrom()
-    {
-        return from_;
-    }
-
-    String getMessageType()
-    {
-        return type_;
-    }
-
-    String getVerb()
-    {
-        return verb_;
-    }
-
-    String getMessageId()
-    {
-        return messageId_;
-    }
-
-    void setMessageId(String id)
-    {
-        messageId_ = id;
-    }
-    
-    void setMessageType(String type)
-    {
-        type_ = type;
-    }
-    
-    void setMessageVerb(String verb)
-    {
-        verb_ = verb;
-    }
-    
-    byte[] getDetail(Object key)
-    {
-        return details_.get(key);
-    }
-    
-    void removeDetail(Object key)
-    {
-        details_.remove(key);
-    }
-    
-    void addDetail(String key, byte[] value)
-    {
-        details_.put(key, value);
-    }
-    
-    Map<String, byte[]> getDetails()
-    {
-        return details_;
-    }
-}
-
-class HeaderSerializer implements ICompactSerializer<Header>
-{
-    public void serialize(Header t, DataOutputStream dos) throws IOException
-    {           
-        dos.writeUTF(t.getMessageId());
-        CompactEndPointSerializationHelper.serialize(t.getFrom(), dos);
-        dos.writeUTF(t.getMessageType());
-        dos.writeUTF( t.getVerb() );
-        
-        /* Serialize the message header */
-        int size = t.details_.size();
-        dos.writeInt(size);
-        Set<String> keys = t.details_.keySet();
-        
-        for( String key : keys )
-        {
-            dos.writeUTF(key);
-            byte[] value = t.details_.get(key);
-            dos.writeInt(value.length);
-            dos.write(value);
-        }
-    }
-
-    public Header deserialize(DataInputStream dis) throws IOException
-    {
-        String id = dis.readUTF();
-        EndPoint from = CompactEndPointSerializationHelper.deserialize(dis);
-        String type = dis.readUTF();
-        String verb = dis.readUTF();
-        
-        /* Deserializing the message header */
-        int size = dis.readInt();
-        Map<String, byte[]> details = new Hashtable<String, byte[]>(size);
-        for ( int i = 0; i < size; ++i )
-        {
-            String key = dis.readUTF();
-            int length = dis.readInt();
-            byte[] bytes = new byte[length];
-            dis.readFully(bytes);
-            details.put(key, bytes);
-        }
-        
-        return new Header(id, from, type, verb, details);
-    }
-}
-
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.utils.GuidGenerator;
+
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Header implements java.io.Serializable
+{
+    static final long serialVersionUID = -3194851946523170022L;
+    private static ICompactSerializer<Header> serializer_;
+    private static AtomicInteger idGen_ = new AtomicInteger(0);
+    
+    static
+    {
+        serializer_ = new HeaderSerializer();        
+    }
+    
+    static ICompactSerializer<Header> serializer()
+    {
+        return serializer_;
+    }
+
+    private EndPoint from_;
+    private String type_;
+    private String verb_;
+    private String messageId_;
+    protected Map<String, byte[]> details_ = new Hashtable<String, byte[]>();
+    
+    Header(String id, EndPoint from, String messageType, String verb)
+    {
+        messageId_ = id;
+        from_ = from;
+        type_ = messageType;
+        verb_ = verb;        
+    }
+    
+    Header(String id, EndPoint from, String messageType, String verb, Map<String, byte[]> details)
+    {
+        this(id, from, messageType, verb);
+        details_ = details;
+    }
+
+    Header(EndPoint from, String messageType, String verb)
+    {
+        messageId_ = Integer.toString(idGen_.incrementAndGet());
+        from_ = from;
+        type_ = messageType;
+        verb_ = verb;
+    }        
+
+    EndPoint getFrom()
+    {
+        return from_;
+    }
+
+    String getMessageType()
+    {
+        return type_;
+    }
+
+    String getVerb()
+    {
+        return verb_;
+    }
+
+    String getMessageId()
+    {
+        return messageId_;
+    }
+
+    void setMessageId(String id)
+    {
+        messageId_ = id;
+    }
+    
+    void setMessageType(String type)
+    {
+        type_ = type;
+    }
+    
+    void setMessageVerb(String verb)
+    {
+        verb_ = verb;
+    }
+    
+    byte[] getDetail(Object key)
+    {
+        return details_.get(key);
+    }
+    
+    void removeDetail(Object key)
+    {
+        details_.remove(key);
+    }
+    
+    void addDetail(String key, byte[] value)
+    {
+        details_.put(key, value);
+    }
+    
+    Map<String, byte[]> getDetails()
+    {
+        return details_;
+    }
+}
+
+class HeaderSerializer implements ICompactSerializer<Header>
+{
+    public void serialize(Header t, DataOutputStream dos) throws IOException
+    {           
+        dos.writeUTF(t.getMessageId());
+        CompactEndPointSerializationHelper.serialize(t.getFrom(), dos);
+        dos.writeUTF(t.getMessageType());
+        dos.writeUTF( t.getVerb() );
+        
+        /* Serialize the message header */
+        int size = t.details_.size();
+        dos.writeInt(size);
+        Set<String> keys = t.details_.keySet();
+        
+        for( String key : keys )
+        {
+            dos.writeUTF(key);
+            byte[] value = t.details_.get(key);
+            dos.writeInt(value.length);
+            dos.write(value);
+        }
+    }
+
+    public Header deserialize(DataInputStream dis) throws IOException
+    {
+        String id = dis.readUTF();
+        EndPoint from = CompactEndPointSerializationHelper.deserialize(dis);
+        String type = dis.readUTF();
+        String verb = dis.readUTF();
+        
+        /* Deserializing the message header */
+        int size = dis.readInt();
+        Map<String, byte[]> details = new Hashtable<String, byte[]>(size);
+        for ( int i = 0; i < size; ++i )
+        {
+            String key = dis.readUTF();
+            int length = dis.readInt();
+            byte[] bytes = new byte[length];
+            dis.readFully(bytes);
+            details.put(key, bytes);
+        }
+        
+        return new Header(id, from, type, verb, details);
+    }
+}
+
+
diff --git a/src/java/org/apache/cassandra/net/HeaderTypes.java b/src/java/org/apache/cassandra/net/HeaderTypes.java
index 235196f53f..8de3cb3cfc 100644
--- a/src/java/org/apache/cassandra/net/HeaderTypes.java
+++ b/src/java/org/apache/cassandra/net/HeaderTypes.java
@@ -1,28 +1,28 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class HeaderTypes 
-{
-    public final static String TASK_PROFILE_CHAIN = "TASK_PROFILE_CHAIN";
-    public static String TASK_ID = "TASK_ID";
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class HeaderTypes 
+{
+    public final static String TASK_PROFILE_CHAIN = "TASK_PROFILE_CHAIN";
+    public static String TASK_ID = "TASK_ID";
+}
diff --git a/src/java/org/apache/cassandra/net/IAsyncCallback.java b/src/java/org/apache/cassandra/net/IAsyncCallback.java
index 5e7b9a961c..7fb8d9900c 100644
--- a/src/java/org/apache/cassandra/net/IAsyncCallback.java
+++ b/src/java/org/apache/cassandra/net/IAsyncCallback.java
@@ -1,38 +1,38 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IAsyncCallback 
-{
-	/**
-	 * @param msg responses to be returned
-	 */
-	public void response(Message msg);
-    
-    /**
-     * Attach some application specific context to the
-     * callback.
-     * @param o application specific context
-     */
-    public void attachContext(Object o);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IAsyncCallback 
+{
+	/**
+	 * @param msg responses to be returned
+	 */
+	public void response(Message msg);
+    
+    /**
+     * Attach some application specific context to the
+     * callback.
+     * @param o application specific context
+     */
+    public void attachContext(Object o);
+}
diff --git a/src/java/org/apache/cassandra/net/IAsyncResult.java b/src/java/org/apache/cassandra/net/IAsyncResult.java
index b0ed84cf55..2971c5c92d 100644
--- a/src/java/org/apache/cassandra/net/IAsyncResult.java
+++ b/src/java/org/apache/cassandra/net/IAsyncResult.java
@@ -1,73 +1,73 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.util.List;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.TimeoutException;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IAsyncResult
-{    
-    /**
-     * This is used to check if the task has been completed
-     * 
-     * @return true if the task has been completed and false otherwise.
-     */
-    public boolean isDone();
-    
-    /**
-     * Returns the result for the task that was submitted.
-     * @return the result wrapped in an Object[]
-    */
-    public byte[] get();
-    
-    /**
-     * Same operation as the above get() but allows the calling
-     * thread to specify a timeout.
-     * @param timeout the maximum time to wait
-     * @param tu the time unit of the timeout argument
-     * @return the result wrapped in an Object[]
-    */
-    public byte[] get(long timeout, TimeUnit tu) throws TimeoutException;
-    
-    /**
-     * Returns the result for all tasks that was submitted.
-     * @return the list of results wrapped in an Object[]
-    */
-    public List<byte[]> multiget();
-    
-    /**
-     * Same operation as the above get() but allows the calling
-     * thread to specify a timeout.
-     * @param timeout the maximum time to wait
-     * @param tu the time unit of the timeout argument
-     * @return the result wrapped in an Object[]
-    */
-    public List<byte[]> multiget(long timeout, TimeUnit tu) throws TimeoutException;
-    
-    /**
-     * Store the result obtained for the submitted task.
-     * @param result the response message
-     */
-    public void result(Message result);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IAsyncResult
+{    
+    /**
+     * This is used to check if the task has been completed
+     * 
+     * @return true if the task has been completed and false otherwise.
+     */
+    public boolean isDone();
+    
+    /**
+     * Returns the result for the task that was submitted.
+     * @return the result wrapped in an Object[]
+    */
+    public byte[] get();
+    
+    /**
+     * Same operation as the above get() but allows the calling
+     * thread to specify a timeout.
+     * @param timeout the maximum time to wait
+     * @param tu the time unit of the timeout argument
+     * @return the result wrapped in an Object[]
+    */
+    public byte[] get(long timeout, TimeUnit tu) throws TimeoutException;
+    
+    /**
+     * Returns the result for all tasks that was submitted.
+     * @return the list of results wrapped in an Object[]
+    */
+    public List<byte[]> multiget();
+    
+    /**
+     * Same operation as the above get() but allows the calling
+     * thread to specify a timeout.
+     * @param timeout the maximum time to wait
+     * @param tu the time unit of the timeout argument
+     * @return the result wrapped in an Object[]
+    */
+    public List<byte[]> multiget(long timeout, TimeUnit tu) throws TimeoutException;
+    
+    /**
+     * Store the result obtained for the submitted task.
+     * @param result the response message
+     */
+    public void result(Message result);
+}
diff --git a/src/java/org/apache/cassandra/net/IMessagingService.java b/src/java/org/apache/cassandra/net/IMessagingService.java
index 8ba6351b0f..8fd9b816bc 100644
--- a/src/java/org/apache/cassandra/net/IMessagingService.java
+++ b/src/java/org/apache/cassandra/net/IMessagingService.java
@@ -1,179 +1,179 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.io.IOException;
-
-import javax.xml.bind.JAXBException;
-
-import org.apache.cassandra.concurrent.IStage;
-
-
-/**
- * An IMessagingService provides the methods for sending messages to remote
- * endpoints. IMessagingService enables the sending of request-response style
- * messages and fire-forget style messages.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IMessagingService
-{   
-	/**
-     * Register a verb and the corresponding verb handler with the
-     * Messaging Service.
-     * @param type name of the verb.     
-     * @param verbHandler handler for the specified verb
-     */
-    public void registerVerbHandlers(String type, IVerbHandler verbHandler);
-    
-    /**
-     * Deregister all verbhandlers corresponding to localEndPoint.
-     * @param localEndPoint
-     */
-    public void deregisterAllVerbHandlers(EndPoint localEndPoint);
-    
-    /**
-     * Deregister a verbhandler corresponding to the verb from the
-     * Messaging Service.
-     * @param type name of the verb.      
-     */
-    public void deregisterVerbHandlers(String type);
-    
-    /**
-     * Listen on the specified port.
-     * @param ep EndPoint whose port to listen on.
-     * @param isHttp specify if the port is an Http port.     
-     */
-    public void listen(EndPoint ep) throws IOException;
-    
-    /**
-     * Listen on the specified port.
-     * @param ep EndPoint whose port to listen on.     
-     */
-    public void listenUDP(EndPoint ep);
-    
-    /**
-     * Send a message to a given endpoint. 
-     * @param message message to be sent.
-     * @param to endpoint to which the message needs to be sent
-     * @return an reference to an IAsyncResult which can be queried for the
-     * response
-     */
-    public IAsyncResult sendRR(Message message, EndPoint to);
-
-    /**
-     * Send a message to the given set of endpoints and informs the MessagingService
-     * to wait for at least <code>howManyResults</code> responses to determine success
-     * of failure.
-     * @param message message to be sent.
-     * @param to endpoints to which the message needs to be sent
-     * @param cb callback interface which is used to pass the responses
-     * @return an reference to message id used to match with the result
-     */
-    public String sendRR(Message message, EndPoint[] to, IAsyncCallback cb);
-    
-    /**
-     * Send a message to a given endpoint. This method specifies a callback
-     * which is invoked with the actual response.
-     * @param message message to be sent.
-     * @param to endpoint to which the message needs to be sent
-     * @param cb callback interface which is used to pass the responses or
-     *           suggest that a timeout occurred to the invoker of the send().
-     *           suggest that a timeout occurred to the invoker of the send().
-     * @return an reference to message id used to match with the result
-     */
-    public String sendRR(Message message, EndPoint to, IAsyncCallback cb);
-
-    /**
-     * Send a message to a given endpoint. The ith element in the <code>messages</code>
-     * array is sent to the ith element in the <code>to</code> array.This method assumes
-     * there is a one-one mapping between the <code>messages</code> array and
-     * the <code>to</code> array. Otherwise an  IllegalArgumentException will be thrown.
-     * This method also informs the MessagingService to wait for at least
-     * <code>howManyResults</code> responses to determine success of failure.
-     * @param messages messages to be sent.
-     * @param to endpoints to which the message needs to be sent
-     * @param cb callback interface which is used to pass the responses or
-     *           suggest that a timeout occured to the invoker of the send().
-     * @return an reference to message id used to match with the result
-     */
-    public String sendRR(Message[] messages, EndPoint[] to, IAsyncCallback cb);
-    
-    /**
-     * Send a message to a given endpoint. The ith element in the <code>messages</code>
-     * array is sent to the ith element in the <code>to</code> array.This method assumes
-     * there is a one-one mapping between the <code>messages</code> array and
-     * the <code>to</code> array. Otherwise an  IllegalArgumentException will be thrown.
-     * This method also informs the MessagingService to wait for at least
-     * <code>howManyResults</code> responses to determine success of failure.
-     * @param messages messages to be sent.
-     * @param to endpoints to which the message needs to be sent
-     * @return an reference to IAsyncResult
-     */
-    public IAsyncResult sendRR(Message[] messages, EndPoint[] to);
-    
-    /**
-     * Send a message to a given endpoint. The ith element in the <code>messages</code>
-     * array is sent to the ith element in the <code>to</code> array.This method assumes
-     * there is a one-one mapping between the <code>messages</code> array and
-     * the <code>to</code> array. Otherwise an  IllegalArgumentException will be thrown.
-     * The idea is that multi-groups of messages are grouped as one logical message
-     * whose results are harnessed via the <i>IAsyncResult</i>
-     * @param messages groups of grouped messages.
-     * @param to destination for the groups of messages
-     * @param cb the callback handler to be invoked for the responses
-     * @return the group id which is basically useless - it is only returned for API's
-     *         to look compatible.
-     */
-    public String sendRR(Message[][] messages, EndPoint[][] to, IAsyncCallback cb);
-
-    /**
-     * Send a message to a given endpoint. This method adheres to the fire and forget
-     * style messaging.
-     * @param message messages to be sent.
-     * @param to endpoint to which the message needs to be sent
-     */
-    public void sendOneWay(Message message, EndPoint to);
-        
-    /**
-     * Send a message to a given endpoint. This method adheres to the fire and forget
-     * style messaging.
-     * @param message messages to be sent.
-     * @param to endpoint to which the message needs to be sent
-     */
-    public void sendUdpOneWay(Message message, EndPoint to);
-    
-    /**
-     * Stream a file from source to destination. This is highly optimized
-     * to not hold any of the contents of the file in memory.
-     * @param file name of file to stream.
-     * @param startPosition position inside the file
-     * @param total number of bytes to stream
-     * @param to endpoint to which we need to stream the file.
-    */
-    public void stream(String file, long startPosition, long total, EndPoint from, EndPoint to);
-
-    /**
-     * This method returns the verb handler associated with the registered
-     * verb. If no handler has been registered then null is returned.
-     * @param verb for which the verb handler is sought
-     * @return a reference to IVerbHandler which is the handler for the specified verb
-     */
-    public IVerbHandler getVerbHandler(String verb);    
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+
+import javax.xml.bind.JAXBException;
+
+import org.apache.cassandra.concurrent.IStage;
+
+
+/**
+ * An IMessagingService provides the methods for sending messages to remote
+ * endpoints. IMessagingService enables the sending of request-response style
+ * messages and fire-forget style messages.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IMessagingService
+{   
+	/**
+     * Register a verb and the corresponding verb handler with the
+     * Messaging Service.
+     * @param type name of the verb.     
+     * @param verbHandler handler for the specified verb
+     */
+    public void registerVerbHandlers(String type, IVerbHandler verbHandler);
+    
+    /**
+     * Deregister all verbhandlers corresponding to localEndPoint.
+     * @param localEndPoint
+     */
+    public void deregisterAllVerbHandlers(EndPoint localEndPoint);
+    
+    /**
+     * Deregister a verbhandler corresponding to the verb from the
+     * Messaging Service.
+     * @param type name of the verb.      
+     */
+    public void deregisterVerbHandlers(String type);
+    
+    /**
+     * Listen on the specified port.
+     * @param ep EndPoint whose port to listen on.
+     * @param isHttp specify if the port is an Http port.     
+     */
+    public void listen(EndPoint ep) throws IOException;
+    
+    /**
+     * Listen on the specified port.
+     * @param ep EndPoint whose port to listen on.     
+     */
+    public void listenUDP(EndPoint ep);
+    
+    /**
+     * Send a message to a given endpoint. 
+     * @param message message to be sent.
+     * @param to endpoint to which the message needs to be sent
+     * @return an reference to an IAsyncResult which can be queried for the
+     * response
+     */
+    public IAsyncResult sendRR(Message message, EndPoint to);
+
+    /**
+     * Send a message to the given set of endpoints and informs the MessagingService
+     * to wait for at least <code>howManyResults</code> responses to determine success
+     * of failure.
+     * @param message message to be sent.
+     * @param to endpoints to which the message needs to be sent
+     * @param cb callback interface which is used to pass the responses
+     * @return an reference to message id used to match with the result
+     */
+    public String sendRR(Message message, EndPoint[] to, IAsyncCallback cb);
+    
+    /**
+     * Send a message to a given endpoint. This method specifies a callback
+     * which is invoked with the actual response.
+     * @param message message to be sent.
+     * @param to endpoint to which the message needs to be sent
+     * @param cb callback interface which is used to pass the responses or
+     *           suggest that a timeout occurred to the invoker of the send().
+     *           suggest that a timeout occurred to the invoker of the send().
+     * @return an reference to message id used to match with the result
+     */
+    public String sendRR(Message message, EndPoint to, IAsyncCallback cb);
+
+    /**
+     * Send a message to a given endpoint. The ith element in the <code>messages</code>
+     * array is sent to the ith element in the <code>to</code> array.This method assumes
+     * there is a one-one mapping between the <code>messages</code> array and
+     * the <code>to</code> array. Otherwise an  IllegalArgumentException will be thrown.
+     * This method also informs the MessagingService to wait for at least
+     * <code>howManyResults</code> responses to determine success of failure.
+     * @param messages messages to be sent.
+     * @param to endpoints to which the message needs to be sent
+     * @param cb callback interface which is used to pass the responses or
+     *           suggest that a timeout occured to the invoker of the send().
+     * @return an reference to message id used to match with the result
+     */
+    public String sendRR(Message[] messages, EndPoint[] to, IAsyncCallback cb);
+    
+    /**
+     * Send a message to a given endpoint. The ith element in the <code>messages</code>
+     * array is sent to the ith element in the <code>to</code> array.This method assumes
+     * there is a one-one mapping between the <code>messages</code> array and
+     * the <code>to</code> array. Otherwise an  IllegalArgumentException will be thrown.
+     * This method also informs the MessagingService to wait for at least
+     * <code>howManyResults</code> responses to determine success of failure.
+     * @param messages messages to be sent.
+     * @param to endpoints to which the message needs to be sent
+     * @return an reference to IAsyncResult
+     */
+    public IAsyncResult sendRR(Message[] messages, EndPoint[] to);
+    
+    /**
+     * Send a message to a given endpoint. The ith element in the <code>messages</code>
+     * array is sent to the ith element in the <code>to</code> array.This method assumes
+     * there is a one-one mapping between the <code>messages</code> array and
+     * the <code>to</code> array. Otherwise an  IllegalArgumentException will be thrown.
+     * The idea is that multi-groups of messages are grouped as one logical message
+     * whose results are harnessed via the <i>IAsyncResult</i>
+     * @param messages groups of grouped messages.
+     * @param to destination for the groups of messages
+     * @param cb the callback handler to be invoked for the responses
+     * @return the group id which is basically useless - it is only returned for API's
+     *         to look compatible.
+     */
+    public String sendRR(Message[][] messages, EndPoint[][] to, IAsyncCallback cb);
+
+    /**
+     * Send a message to a given endpoint. This method adheres to the fire and forget
+     * style messaging.
+     * @param message messages to be sent.
+     * @param to endpoint to which the message needs to be sent
+     */
+    public void sendOneWay(Message message, EndPoint to);
+        
+    /**
+     * Send a message to a given endpoint. This method adheres to the fire and forget
+     * style messaging.
+     * @param message messages to be sent.
+     * @param to endpoint to which the message needs to be sent
+     */
+    public void sendUdpOneWay(Message message, EndPoint to);
+    
+    /**
+     * Stream a file from source to destination. This is highly optimized
+     * to not hold any of the contents of the file in memory.
+     * @param file name of file to stream.
+     * @param startPosition position inside the file
+     * @param total number of bytes to stream
+     * @param to endpoint to which we need to stream the file.
+    */
+    public void stream(String file, long startPosition, long total, EndPoint from, EndPoint to);
+
+    /**
+     * This method returns the verb handler associated with the registered
+     * verb. If no handler has been registered then null is returned.
+     * @param verb for which the verb handler is sought
+     * @return a reference to IVerbHandler which is the handler for the specified verb
+     */
+    public IVerbHandler getVerbHandler(String verb);    
+}
diff --git a/src/java/org/apache/cassandra/net/IVerbHandler.java b/src/java/org/apache/cassandra/net/IVerbHandler.java
index 0b32037bde..bd8da0c22b 100644
--- a/src/java/org/apache/cassandra/net/IVerbHandler.java
+++ b/src/java/org/apache/cassandra/net/IVerbHandler.java
@@ -1,39 +1,39 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-/**
- * IVerbHandler provides the method that all verb handlers need to implement.
- * The concrete implementation of this interface would provide the functionality
- * for a given verb.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IVerbHandler
-{
-    /**
-     * This method delivers a message to the implementing class (if the implementing
-     * class was registered by a call to MessagingService.registerVerbHandlers).
-     * Note that the caller should not be holding any locks when calling this method
-     * because the implementation may be synchronized.
-     * 
-     * @param message - incoming message that needs handling.     
-     */
-    public void doVerb(Message message);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * IVerbHandler provides the method that all verb handlers need to implement.
+ * The concrete implementation of this interface would provide the functionality
+ * for a given verb.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IVerbHandler
+{
+    /**
+     * This method delivers a message to the implementing class (if the implementing
+     * class was registered by a call to MessagingService.registerVerbHandlers).
+     * Note that the caller should not be holding any locks when calling this method
+     * because the implementation may be synchronized.
+     * 
+     * @param message - incoming message that needs handling.     
+     */
+    public void doVerb(Message message);
+}
diff --git a/src/java/org/apache/cassandra/net/Message.java b/src/java/org/apache/cassandra/net/Message.java
index dbb95c466f..4e86d21972 100644
--- a/src/java/org/apache/cassandra/net/Message.java
+++ b/src/java/org/apache/cassandra/net/Message.java
@@ -1,182 +1,182 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.lang.reflect.Array;
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.ObjectInputStream;
-import java.io.ObjectOutputStream;
-import java.util.*;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.log4j.Logger;
-import org.apache.cassandra.utils.*;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class Message implements java.io.Serializable
-{    
-    static final long serialVersionUID = 6329198792470413221L;
-    private static ICompactSerializer<Message> serializer_;
-    
-    static
-    {
-        serializer_ = new MessageSerializer();        
-    }
-    
-    public static ICompactSerializer<Message> serializer()
-    {
-        return serializer_;
-    }
-    
-    Header header_;
-    private byte[] body_;
-    
-    protected Message(String id, EndPoint from, String messageType, String verb, byte[] body)
-    {
-        this(new Header(id, from, messageType, verb), body);
-    }
-    
-    protected Message(Header header, byte[] body)
-    {
-        header_ = header;
-        body_ = body;
-    }
-
-    public Message(EndPoint from, String messageType, String verb, byte[] body)
-    {
-        this(new Header(from, messageType, verb), body);
-    }    
-    
-    public byte[] getHeader(Object key)
-    {
-        return header_.getDetail(key);
-    }
-    
-    public void removeHeader(Object key)
-    {
-        header_.removeDetail(key);
-    }
-    
-    public void setMessageType(String type)
-    {
-        header_.setMessageType(type);
-    }
-
-    public void setMessageVerb(String verb)
-    {
-        header_.setMessageVerb(verb);
-    }
-
-    public void addHeader(String key, byte[] value)
-    {
-        header_.addDetail(key, value);
-    }
-    
-    public Map<String, byte[]> getHeaders()
-    {
-        return header_.getDetails();
-    }
-
-    public byte[] getMessageBody()
-    {
-        return body_;
-    }
-    
-    public void setMessageBody(byte[] body)
-    {
-        body_ = body;
-    }
-
-    public EndPoint getFrom()
-    {
-        return header_.getFrom();
-    }
-
-    public String getMessageType()
-    {
-        return header_.getMessageType();
-    }
-
-    public String getVerb()
-    {
-        return header_.getVerb();
-    }
-
-    public String getMessageId()
-    {
-        return header_.getMessageId();
-    }
-
-    void setMessageId(String id)
-    {
-        header_.setMessageId(id);
-    }    
-
-    public Message getReply(EndPoint from, byte[] args)
-    {        
-        Message response = new Message(getMessageId(),
-                                       from,
-                                       MessagingService.responseStage_,
-                                       MessagingService.responseVerbHandler_,
-                                       args);
-        return response;
-    }
-    
-    public String toString()
-    {
-        StringBuilder sbuf = new StringBuilder("");
-        String separator = System.getProperty("line.separator");
-        sbuf.append("ID:" + getMessageId())
-        	.append(separator)
-        	.append("FROM:" + getFrom())
-        	.append(separator)
-        	.append("TYPE:" + getMessageType())
-        	.append(separator)
-        	.append("VERB:" + getVerb())
-        	.append(separator);
-        return sbuf.toString();
-    }
-}
-
-class MessageSerializer implements ICompactSerializer<Message>
-{
-    public void serialize(Message t, DataOutputStream dos) throws IOException
-    {
-        Header.serializer().serialize( t.header_, dos);
-        byte[] bytes = t.getMessageBody();
-        dos.writeInt(bytes.length);
-        dos.write(bytes);
-    }
-
-    public Message deserialize(DataInputStream dis) throws IOException
-    {
-        Header header = Header.serializer().deserialize(dis);
-        int size = dis.readInt();
-        byte[] bytes = new byte[size];
-        dis.readFully(bytes);
-        // return new Message(header.getMessageId(), header.getFrom(), header.getMessageType(), header.getVerb(), new Object[]{bytes});
-        return new Message(header, bytes);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.lang.reflect.Array;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.util.*;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.utils.*;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Message implements java.io.Serializable
+{    
+    static final long serialVersionUID = 6329198792470413221L;
+    private static ICompactSerializer<Message> serializer_;
+    
+    static
+    {
+        serializer_ = new MessageSerializer();        
+    }
+    
+    public static ICompactSerializer<Message> serializer()
+    {
+        return serializer_;
+    }
+    
+    Header header_;
+    private byte[] body_;
+    
+    protected Message(String id, EndPoint from, String messageType, String verb, byte[] body)
+    {
+        this(new Header(id, from, messageType, verb), body);
+    }
+    
+    protected Message(Header header, byte[] body)
+    {
+        header_ = header;
+        body_ = body;
+    }
+
+    public Message(EndPoint from, String messageType, String verb, byte[] body)
+    {
+        this(new Header(from, messageType, verb), body);
+    }    
+    
+    public byte[] getHeader(Object key)
+    {
+        return header_.getDetail(key);
+    }
+    
+    public void removeHeader(Object key)
+    {
+        header_.removeDetail(key);
+    }
+    
+    public void setMessageType(String type)
+    {
+        header_.setMessageType(type);
+    }
+
+    public void setMessageVerb(String verb)
+    {
+        header_.setMessageVerb(verb);
+    }
+
+    public void addHeader(String key, byte[] value)
+    {
+        header_.addDetail(key, value);
+    }
+    
+    public Map<String, byte[]> getHeaders()
+    {
+        return header_.getDetails();
+    }
+
+    public byte[] getMessageBody()
+    {
+        return body_;
+    }
+    
+    public void setMessageBody(byte[] body)
+    {
+        body_ = body;
+    }
+
+    public EndPoint getFrom()
+    {
+        return header_.getFrom();
+    }
+
+    public String getMessageType()
+    {
+        return header_.getMessageType();
+    }
+
+    public String getVerb()
+    {
+        return header_.getVerb();
+    }
+
+    public String getMessageId()
+    {
+        return header_.getMessageId();
+    }
+
+    void setMessageId(String id)
+    {
+        header_.setMessageId(id);
+    }    
+
+    public Message getReply(EndPoint from, byte[] args)
+    {        
+        Message response = new Message(getMessageId(),
+                                       from,
+                                       MessagingService.responseStage_,
+                                       MessagingService.responseVerbHandler_,
+                                       args);
+        return response;
+    }
+    
+    public String toString()
+    {
+        StringBuilder sbuf = new StringBuilder("");
+        String separator = System.getProperty("line.separator");
+        sbuf.append("ID:" + getMessageId())
+        	.append(separator)
+        	.append("FROM:" + getFrom())
+        	.append(separator)
+        	.append("TYPE:" + getMessageType())
+        	.append(separator)
+        	.append("VERB:" + getVerb())
+        	.append(separator);
+        return sbuf.toString();
+    }
+}
+
+class MessageSerializer implements ICompactSerializer<Message>
+{
+    public void serialize(Message t, DataOutputStream dos) throws IOException
+    {
+        Header.serializer().serialize( t.header_, dos);
+        byte[] bytes = t.getMessageBody();
+        dos.writeInt(bytes.length);
+        dos.write(bytes);
+    }
+
+    public Message deserialize(DataInputStream dis) throws IOException
+    {
+        Header header = Header.serializer().deserialize(dis);
+        int size = dis.readInt();
+        byte[] bytes = new byte[size];
+        dis.readFully(bytes);
+        // return new Message(header.getMessageId(), header.getFrom(), header.getMessageType(), header.getVerb(), new Object[]{bytes});
+        return new Message(header, bytes);
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/MessageDeliveryTask.java b/src/java/org/apache/cassandra/net/MessageDeliveryTask.java
index b8d16be155..255a6bb4ec 100644
--- a/src/java/org/apache/cassandra/net/MessageDeliveryTask.java
+++ b/src/java/org/apache/cassandra/net/MessageDeliveryTask.java
@@ -1,47 +1,47 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import org.apache.log4j.Logger;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class MessageDeliveryTask implements Runnable
-{
-    private Message message_;
-    private static Logger logger_ = Logger.getLogger(MessageDeliveryTask.class);    
-    
-    public MessageDeliveryTask(Message message)
-    {
-        message_ = message;    
-    }
-    
-    public void run()
-    { 
-        String verb = message_.getVerb();
-        IVerbHandler verbHandler = MessagingService.getMessagingInstance().getVerbHandler(verb);
-        if ( verbHandler != null )
-        {
-            verbHandler.doVerb(message_);
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import org.apache.log4j.Logger;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MessageDeliveryTask implements Runnable
+{
+    private Message message_;
+    private static Logger logger_ = Logger.getLogger(MessageDeliveryTask.class);    
+    
+    public MessageDeliveryTask(Message message)
+    {
+        message_ = message;    
+    }
+    
+    public void run()
+    { 
+        String verb = message_.getVerb();
+        IVerbHandler verbHandler = MessagingService.getMessagingInstance().getVerbHandler(verb);
+        if ( verbHandler != null )
+        {
+            verbHandler.doVerb(message_);
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/MessageDeserializationTask.java b/src/java/org/apache/cassandra/net/MessageDeserializationTask.java
index 0cd6578151..e28460bc3d 100644
--- a/src/java/org/apache/cassandra/net/MessageDeserializationTask.java
+++ b/src/java/org/apache/cassandra/net/MessageDeserializationTask.java
@@ -1,68 +1,68 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.io.IOException;
-
-import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
-import org.apache.cassandra.concurrent.StageManager;
-import org.apache.cassandra.net.io.FastSerializer;
-import org.apache.cassandra.net.io.ISerializer;
-import org.apache.cassandra.net.sink.SinkManager;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class MessageDeserializationTask implements Runnable
-{
-    private static Logger logger_ = Logger.getLogger(MessageDeserializationTask.class); 
-    private static ISerializer serializer_ = new FastSerializer();
-    private int serializerType_;
-    private byte[] bytes_ = new byte[0];    
-    
-    MessageDeserializationTask(int serializerType, byte[] bytes)
-    {
-        serializerType_ = serializerType;
-        bytes_ = bytes;        
-    }
-    
-    public void run()
-    {
-        Message message = null;
-        try
-        {
-            message = serializer_.deserialize(bytes_);
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-
-        if ( message != null )
-        {
-            message = SinkManager.processServerMessageSink(message);
-            MessagingService.receive(message);
-        }
-    }
-
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.net.io.FastSerializer;
+import org.apache.cassandra.net.io.ISerializer;
+import org.apache.cassandra.net.sink.SinkManager;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class MessageDeserializationTask implements Runnable
+{
+    private static Logger logger_ = Logger.getLogger(MessageDeserializationTask.class); 
+    private static ISerializer serializer_ = new FastSerializer();
+    private int serializerType_;
+    private byte[] bytes_ = new byte[0];    
+    
+    MessageDeserializationTask(int serializerType, byte[] bytes)
+    {
+        serializerType_ = serializerType;
+        bytes_ = bytes;        
+    }
+    
+    public void run()
+    {
+        Message message = null;
+        try
+        {
+            message = serializer_.deserialize(bytes_);
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+
+        if ( message != null )
+        {
+            message = SinkManager.processServerMessageSink(message);
+            MessagingService.receive(message);
+        }
+    }
+
+}
diff --git a/src/java/org/apache/cassandra/net/MessageSerializationTask.java b/src/java/org/apache/cassandra/net/MessageSerializationTask.java
index 3078a9a356..48005be4a9 100644
--- a/src/java/org/apache/cassandra/net/MessageSerializationTask.java
+++ b/src/java/org/apache/cassandra/net/MessageSerializationTask.java
@@ -1,101 +1,101 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.io.IOException;
-import java.net.SocketException;
-
-import org.apache.cassandra.concurrent.Context;
-import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
-import org.apache.cassandra.concurrent.ThreadLocalContext;
-import org.apache.cassandra.net.sink.SinkManager;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class MessageSerializationTask implements Runnable
-{
-    private static Logger logger_ = Logger.getLogger(MessageSerializationTask.class);
-    private Message message_;
-    private EndPoint to_;    
-    
-    public MessageSerializationTask(Message message, EndPoint to)
-    {
-        message_ = message;
-        to_ = to;        
-    }
-    
-    public Message getMessage()
-    {
-        return message_;
-    }
-
-    public void run()
-    {        
-        /* Adding the message to be serialized in the TLS. For accessing in the afterExecute() */
-        Context ctx = new Context();
-        ctx.put(this.getClass().getName(), message_);
-        ThreadLocalContext.put(ctx);
-        
-        TcpConnection connection = null;
-        try
-        {
-            Message message = SinkManager.processClientMessageSink(message_);
-            if(null == message) 
-                return;
-            connection = MessagingService.getConnection(message_.getFrom(), to_);
-            connection.write(message);            
-        }            
-        catch ( SocketException se )
-        {            
-            // Shutting down the entire pool. May be too conservative an approach.
-            MessagingService.getConnectionPool(message_.getFrom(), to_).shutdown();
-            logger_.warn(LogUtil.throwableToString(se));
-        }
-        catch ( IOException e )
-        {
-            logConnectAndIOException(e, connection);
-        }
-        catch (Throwable th)
-        {
-            logger_.warn(LogUtil.throwableToString(th));
-        }
-        finally
-        {
-            if ( connection != null )
-            {
-                connection.close();
-            }            
-        }
-    }
-    
-    private void logConnectAndIOException(IOException ex, TcpConnection connection)
-    {                    
-        if ( connection != null )
-        {
-            connection.errorClose();
-        }
-        logger_.warn(LogUtil.throwableToString(ex));
-    }
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.net.SocketException;
+
+import org.apache.cassandra.concurrent.Context;
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadLocalContext;
+import org.apache.cassandra.net.sink.SinkManager;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class MessageSerializationTask implements Runnable
+{
+    private static Logger logger_ = Logger.getLogger(MessageSerializationTask.class);
+    private Message message_;
+    private EndPoint to_;    
+    
+    public MessageSerializationTask(Message message, EndPoint to)
+    {
+        message_ = message;
+        to_ = to;        
+    }
+    
+    public Message getMessage()
+    {
+        return message_;
+    }
+
+    public void run()
+    {        
+        /* Adding the message to be serialized in the TLS. For accessing in the afterExecute() */
+        Context ctx = new Context();
+        ctx.put(this.getClass().getName(), message_);
+        ThreadLocalContext.put(ctx);
+        
+        TcpConnection connection = null;
+        try
+        {
+            Message message = SinkManager.processClientMessageSink(message_);
+            if(null == message) 
+                return;
+            connection = MessagingService.getConnection(message_.getFrom(), to_);
+            connection.write(message);            
+        }            
+        catch ( SocketException se )
+        {            
+            // Shutting down the entire pool. May be too conservative an approach.
+            MessagingService.getConnectionPool(message_.getFrom(), to_).shutdown();
+            logger_.warn(LogUtil.throwableToString(se));
+        }
+        catch ( IOException e )
+        {
+            logConnectAndIOException(e, connection);
+        }
+        catch (Throwable th)
+        {
+            logger_.warn(LogUtil.throwableToString(th));
+        }
+        finally
+        {
+            if ( connection != null )
+            {
+                connection.close();
+            }            
+        }
+    }
+    
+    private void logConnectAndIOException(IOException ex, TcpConnection connection)
+    {                    
+        if ( connection != null )
+        {
+            connection.errorClose();
+        }
+        logger_.warn(LogUtil.throwableToString(ex));
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/net/MessagingConfig.java b/src/java/org/apache/cassandra/net/MessagingConfig.java
index 0cfc349f63..6da286b9a2 100644
--- a/src/java/org/apache/cassandra/net/MessagingConfig.java
+++ b/src/java/org/apache/cassandra/net/MessagingConfig.java
@@ -1,96 +1,96 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class MessagingConfig
-{
-    // The expected time for one message round trip.  It does not reflect message processing
-    // time at the receiver.
-    private static int expectedRoundTripTime_ = 400;
-    private static int numberOfPorts_ = 2;
-    private static int threadCount_ = 4;
-
-    public static int getMessagingThreadCount()
-    {
-        return threadCount_;
-    }
-
-    public static void setMessagingThreadCount(int threadCount)
-    {
-        threadCount_ = threadCount;
-    }
-
-    public static void setExpectedRoundTripTime(int roundTripTimeMillis) {
-    	if(roundTripTimeMillis > 0 )
-    		expectedRoundTripTime_ = roundTripTimeMillis;
-    }
-
-    public static int getExpectedRoundTripTime()
-    {
-        return expectedRoundTripTime_;
-    }
-
-    public static int getConnectionPoolInitialSize()
-    {
-        return ConnectionPoolConfiguration.initialSize_;
-    }
-
-    public static int getConnectionPoolGrowthFactor()
-    {
-        return ConnectionPoolConfiguration.growthFactor_;
-    }
-
-    public static int getConnectionPoolMaxSize()
-    {
-        return ConnectionPoolConfiguration.maxSize_;
-    }
-
-    public static int getConnectionPoolWaitTimeout()
-    {
-        return ConnectionPoolConfiguration.waitTimeout_;
-    }
-
-    public static int getConnectionPoolMonitorInterval()
-    {
-        return ConnectionPoolConfiguration.monitorInterval_;
-    }
-
-    public static void setNumberOfPorts(int n)
-    {
-        numberOfPorts_ = n;
-    }
-
-    public static int getNumberOfPorts()
-    {
-        return numberOfPorts_;
-    }
-}
-
-class ConnectionPoolConfiguration
-{
-    public static int initialSize_ = 1;
-    public static int growthFactor_ = 1;
-    public static int maxSize_ = 1;
-    public static int waitTimeout_ = 10;
-    public static int monitorInterval_ = 300;
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MessagingConfig
+{
+    // The expected time for one message round trip.  It does not reflect message processing
+    // time at the receiver.
+    private static int expectedRoundTripTime_ = 400;
+    private static int numberOfPorts_ = 2;
+    private static int threadCount_ = 4;
+
+    public static int getMessagingThreadCount()
+    {
+        return threadCount_;
+    }
+
+    public static void setMessagingThreadCount(int threadCount)
+    {
+        threadCount_ = threadCount;
+    }
+
+    public static void setExpectedRoundTripTime(int roundTripTimeMillis) {
+    	if(roundTripTimeMillis > 0 )
+    		expectedRoundTripTime_ = roundTripTimeMillis;
+    }
+
+    public static int getExpectedRoundTripTime()
+    {
+        return expectedRoundTripTime_;
+    }
+
+    public static int getConnectionPoolInitialSize()
+    {
+        return ConnectionPoolConfiguration.initialSize_;
+    }
+
+    public static int getConnectionPoolGrowthFactor()
+    {
+        return ConnectionPoolConfiguration.growthFactor_;
+    }
+
+    public static int getConnectionPoolMaxSize()
+    {
+        return ConnectionPoolConfiguration.maxSize_;
+    }
+
+    public static int getConnectionPoolWaitTimeout()
+    {
+        return ConnectionPoolConfiguration.waitTimeout_;
+    }
+
+    public static int getConnectionPoolMonitorInterval()
+    {
+        return ConnectionPoolConfiguration.monitorInterval_;
+    }
+
+    public static void setNumberOfPorts(int n)
+    {
+        numberOfPorts_ = n;
+    }
+
+    public static int getNumberOfPorts()
+    {
+        return numberOfPorts_;
+    }
+}
+
+class ConnectionPoolConfiguration
+{
+    public static int initialSize_ = 1;
+    public static int growthFactor_ = 1;
+    public static int maxSize_ = 1;
+    public static int waitTimeout_ = 10;
+    public static int monitorInterval_ = 300;
+}
diff --git a/src/java/org/apache/cassandra/net/MessagingService.java b/src/java/org/apache/cassandra/net/MessagingService.java
index 4f6611a5a4..96777777e1 100644
--- a/src/java/org/apache/cassandra/net/MessagingService.java
+++ b/src/java/org/apache/cassandra/net/MessagingService.java
@@ -1,759 +1,759 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import org.apache.cassandra.concurrent.*;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.net.io.SerializerType;
-import org.apache.cassandra.utils.*;
-import org.apache.log4j.Logger;
-
-import java.io.IOException;
-import java.net.MulticastSocket;
-import java.net.ServerSocket;
-import java.nio.ByteBuffer;
-import java.nio.channels.SelectionKey;
-import java.nio.channels.ServerSocketChannel;
-import java.security.MessageDigest;
-import java.util.*;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.locks.ReentrantLock;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class MessagingService implements IMessagingService
-{
-    private static boolean debugOn_ = false;   
-    
-    private static int version_ = 1;
-    //TODO: make this parameter dynamic somehow.  Not sure if config is appropriate.
-    private static SerializerType serializerType_ = SerializerType.BINARY;
-    
-    private static byte[] protocol_ = new byte[16];
-    /* Verb Handler for the Response */
-    public static final String responseVerbHandler_ = "RESPONSE";
-    /* Stage for responses. */
-    public static final String responseStage_ = "RESPONSE-STAGE";
-    private enum ReservedVerbs_ {RESPONSE};
-    
-    private static Map<String, String> reservedVerbs_ = new Hashtable<String, String>();
-    /* Indicate if we are currently streaming data to another node or receiving streaming data */
-    private static AtomicBoolean isStreaming_ = new AtomicBoolean(false);
-    
-    /* This records all the results mapped by message Id */
-    private static ICachetable<String, IAsyncCallback> callbackMap_;
-    private static ICachetable<String, IAsyncResult> taskCompletionMap_;
-    
-    /* Manages the table of endpoints it is listening on */
-    private static Set<EndPoint> endPoints_;
-    
-    /* List of sockets we are listening on */
-    private static Map<EndPoint, SelectionKey> listenSockets_ = new HashMap<EndPoint, SelectionKey>();
-    
-    /* Lookup table for registering message handlers based on the verb. */
-    private static Map<String, IVerbHandler> verbHandlers_;
-    
-    private static Map<String, MulticastSocket> mCastMembership_ = new HashMap<String, MulticastSocket>();
-    
-    /* Thread pool to handle messaging read activities of Socket and default stage */
-    private static ExecutorService messageDeserializationExecutor_;
-    
-    /* Thread pool to handle messaging write activities */
-    private static ExecutorService messageSerializerExecutor_;
-    
-    /* Thread pool to handle deserialization of messages read from the socket. */
-    private static ExecutorService messageDeserializerExecutor_;
-    
-    /* Thread pool to handle messaging write activities */
-    private static ExecutorService streamExecutor_;
-    
-    private final static ReentrantLock lock_ = new ReentrantLock();
-    private static Map<String, TcpConnectionManager> poolTable_ = new Hashtable<String, TcpConnectionManager>();
-    
-    private static boolean bShutdown_ = false;
-    
-    private static Logger logger_ = Logger.getLogger(MessagingService.class);
-    
-    private static IMessagingService messagingService_ = new MessagingService();
-    
-    public static boolean isDebugOn()
-    {
-        return debugOn_;
-    }
-    
-    public static void debugOn(boolean on)
-    {
-        debugOn_ = on;
-    }
-    
-    public static SerializerType getSerializerType()
-    {
-        return serializerType_;
-    }
-    
-    public synchronized static void serializerType(String type)
-    { 
-        if ( type.equalsIgnoreCase("binary") )
-        {
-            serializerType_ = SerializerType.BINARY;
-        }
-        else if ( type.equalsIgnoreCase("java") )
-        {
-            serializerType_ = SerializerType.JAVA;
-        }
-        else if ( type.equalsIgnoreCase("xml") )
-        {
-            serializerType_ = SerializerType.XML;
-        }
-    }
-    
-    public static int getVersion()
-    {
-        return version_;
-    }
-    
-    public static void setVersion(int version)
-    {
-        version_ = version;
-    }
-    
-    public static IMessagingService getMessagingInstance()
-    {   
-    	if ( bShutdown_ )
-    	{
-            lock_.lock();
-            try
-            {
-                if ( bShutdown_ )
-                {
-            		messagingService_ = new MessagingService();
-            		bShutdown_ = false;
-                }
-            }
-            finally
-            {
-                lock_.unlock();
-            }
-    	}
-        return messagingService_;
-    }
-    
-    public Object clone() throws CloneNotSupportedException
-    {
-        //Prevents the singleton from being cloned
-        throw new CloneNotSupportedException();
-    }
-
-    protected MessagingService()
-    {        
-        for ( ReservedVerbs_ verbs : ReservedVerbs_.values() )
-        {
-            reservedVerbs_.put(verbs.toString(), verbs.toString());
-        }
-        verbHandlers_ = new HashMap<String, IVerbHandler>();        
-        endPoints_ = new HashSet<EndPoint>();
-        /*
-         * Leave callbacks in the cachetable long enough that any related messages will arrive
-         * before the callback is evicted from the table. The concurrency level is set at 128
-         * which is the sum of the threads in the pool that adds shit into the table and the 
-         * pool that retrives the callback from here.
-        */ 
-        int maxSize = MessagingConfig.getMessagingThreadCount();
-        callbackMap_ = new Cachetable<String, IAsyncCallback>( 2 * DatabaseDescriptor.getRpcTimeout() );
-        taskCompletionMap_ = new Cachetable<String, IAsyncResult>( 2 * DatabaseDescriptor.getRpcTimeout() );        
-        
-        messageDeserializationExecutor_ = new DebuggableThreadPoolExecutor( maxSize,
-                maxSize,
-                Integer.MAX_VALUE,
-                TimeUnit.SECONDS,
-                new LinkedBlockingQueue<Runnable>(),
-                new ThreadFactoryImpl("MESSAGING-SERVICE-POOL")
-                );
-                
-        messageSerializerExecutor_ = new DebuggableThreadPoolExecutor( maxSize,
-                maxSize,
-                Integer.MAX_VALUE,
-                TimeUnit.SECONDS,
-                new LinkedBlockingQueue<Runnable>(),
-                new ThreadFactoryImpl("MESSAGE-SERIALIZER-POOL")
-                ); 
-        
-        messageDeserializerExecutor_ = new DebuggableThreadPoolExecutor( maxSize,
-                maxSize,
-                Integer.MAX_VALUE,
-                TimeUnit.SECONDS,
-                new LinkedBlockingQueue<Runnable>(),
-                new ThreadFactoryImpl("MESSAGE-DESERIALIZER-POOL")
-                ); 
-        
-        streamExecutor_ = new DebuggableThreadPoolExecutor("MESSAGE-STREAMING-POOL");
-                
-        protocol_ = hash(HashingSchemes.MD5, "FB-MESSAGING".getBytes());        
-        /* register the response verb handler */
-        registerVerbHandlers(MessagingService.responseVerbHandler_, new ResponseVerbHandler());
-        /* register stage for response */
-        StageManager.registerStage(MessagingService.responseStage_, new MultiThreadedStage("RESPONSE-STAGE", maxSize) );
-    }
-    
-    public byte[] hash(String type, byte data[])
-    {
-        byte result[] = null;
-        try
-        {
-            MessageDigest messageDigest = MessageDigest.getInstance(type);
-            result = messageDigest.digest(data);
-        }
-        catch(Exception e)
-        {
-            if (logger_.isDebugEnabled())
-                logger_.debug(LogUtil.throwableToString(e));
-        }
-        return result;
-    }
-    
-    public void listen(EndPoint localEp) throws IOException
-    {        
-        ServerSocketChannel serverChannel = ServerSocketChannel.open();
-        ServerSocket ss = serverChannel.socket();            
-        ss.bind(localEp.getInetAddress());
-        serverChannel.configureBlocking(false);
-        
-        SelectionKeyHandler handler = new TcpConnectionHandler(localEp);
-
-        SelectionKey key = SelectorManager.getSelectorManager().register(serverChannel, handler, SelectionKey.OP_ACCEPT);          
-        endPoints_.add(localEp);            
-        listenSockets_.put(localEp, key);             
-    }
-    
-    public void listenUDP(EndPoint localEp)
-    {
-        UdpConnection connection = new UdpConnection();
-        if (logger_.isDebugEnabled())
-          logger_.debug("Starting to listen on " + localEp);
-        try
-        {
-            connection.init(localEp.getPort());
-            endPoints_.add(localEp);     
-        }
-        catch ( IOException e )
-        {
-            logger_.warn(LogUtil.throwableToString(e));
-        }
-    }
-    
-    public static TcpConnectionManager getConnectionPool(EndPoint from, EndPoint to)
-    {
-        String key = from + ":" + to;
-        TcpConnectionManager cp = poolTable_.get(key);
-        if( cp == null )
-        {
-            lock_.lock();
-            try
-            {
-                cp = poolTable_.get(key);
-                if (cp == null )
-                {
-                    cp = new TcpConnectionManager(MessagingConfig.getConnectionPoolInitialSize(), 
-                            MessagingConfig.getConnectionPoolGrowthFactor(), 
-                            MessagingConfig.getConnectionPoolMaxSize(), from, to);
-                    poolTable_.put(key, cp);
-                }
-            }
-            finally
-            {
-                lock_.unlock();
-            }
-        }
-        return cp;
-    }
-
-    public static ConnectionStatistics[] getPoolStatistics()
-    {
-        Set<ConnectionStatistics> stats = new HashSet<ConnectionStatistics>();        
-        Iterator<TcpConnectionManager> it = poolTable_.values().iterator();
-        while ( it.hasNext() )
-        {
-            TcpConnectionManager cp = it.next();
-            ConnectionStatistics cs = new ConnectionStatistics(cp.getLocalEndPoint(), cp.getRemoteEndPoint(), cp.getPoolSize(), cp.getConnectionsInUse());
-            stats.add( cs );
-        }
-        return stats.toArray(new ConnectionStatistics[0]);
-    }
-    
-    public static TcpConnection getConnection(EndPoint from, EndPoint to) throws IOException
-    {
-        return getConnectionPool(from, to).getConnection();
-    }
-    
-    private void checkForReservedVerb(String type)
-    {
-    	if ( reservedVerbs_.get(type) != null && verbHandlers_.get(type) != null )
-    	{
-    		throw new IllegalArgumentException( type + " is a reserved verb handler. Scram!");
-    	}
-    }     
-    
-    public void registerVerbHandlers(String type, IVerbHandler verbHandler)
-    {
-    	checkForReservedVerb(type);
-    	verbHandlers_.put(type, verbHandler);
-    }
-    
-    public void deregisterAllVerbHandlers(EndPoint localEndPoint)
-    {
-        Iterator keys = verbHandlers_.keySet().iterator();
-        String key = null;
-        
-        /*
-         * endpoint specific verbhandlers can be distinguished because
-         * their key's contain the name of the endpoint. 
-         */
-        while(keys.hasNext())
-        {
-            key = (String)keys.next();
-            if (key.contains(localEndPoint.toString()))
-                keys.remove();
-        }
-    }
-    
-    public void deregisterVerbHandlers(String type)
-    {
-        verbHandlers_.remove(type);
-    }
-
-    public IVerbHandler getVerbHandler(String type)
-    {
-        IVerbHandler handler = (IVerbHandler)verbHandlers_.get(type);
-        return handler;
-    }
-
-    public String sendRR(Message message, EndPoint[] to, IAsyncCallback cb)
-    {
-        String messageId = message.getMessageId();                        
-        callbackMap_.put(messageId, cb);
-        for ( int i = 0; i < to.length; ++i )
-        {
-            sendOneWay(message, to[i]);
-        }
-        return messageId;
-    }
-    
-    public String sendRR(Message message, EndPoint to, IAsyncCallback cb)
-    {        
-        String messageId = message.getMessageId();
-        callbackMap_.put(messageId, cb);
-        sendOneWay(message, to);
-        return messageId;
-    }
-
-    public String sendRR(Message[] messages, EndPoint[] to, IAsyncCallback cb)
-    {
-        if ( messages.length != to.length )
-        {
-            throw new IllegalArgumentException("Number of messages and the number of endpoints need to be same.");
-        }
-        String groupId = GuidGenerator.guid();
-        callbackMap_.put(groupId, cb);
-        for ( int i = 0; i < messages.length; ++i )
-        {
-            messages[i].setMessageId(groupId);
-            sendOneWay(messages[i], to[i]);
-        }
-        return groupId;
-    } 
-    
-    public IAsyncResult sendRR(Message[] messages, EndPoint[] to)
-    {
-        if ( messages.length != to.length )
-        {
-            throw new IllegalArgumentException("Number of messages and the number of endpoints need to be same.");
-        }
-        
-        IAsyncResult iar = new MultiAsyncResult(messages.length);
-        String groupId = GuidGenerator.guid();
-        taskCompletionMap_.put(groupId, iar);
-        for ( int i = 0; i < messages.length; ++i )
-        {
-            messages[i].setMessageId(groupId);
-            sendOneWay(messages[i], to[i]);
-        }
-        
-        return iar;
-    }
-    
-    public String sendRR(Message[][] messages, EndPoint[][] to, IAsyncCallback cb)
-    {
-        if ( messages.length != to.length )
-        {
-            throw new IllegalArgumentException("Number of messages and the number of endpoints need to be same.");
-        }
-        
-        int length = messages.length;
-        String[] gids = new String[length];
-        /* Generate the requisite GUID's */
-        for ( int i = 0; i < length; ++i )
-        {
-            gids[i] = GuidGenerator.guid();
-        }
-        /* attach this context to the callback */
-        cb.attachContext(gids);
-        for ( int i = 0; i < length; ++i )
-        {
-            callbackMap_.put(gids[i], cb);
-            for ( int j = 0; j < messages[i].length; ++j )
-            {
-                messages[i][j].setMessageId(gids[i]);
-                sendOneWay(messages[i][j], to[i][j]);
-            }            
-        }      
-        return gids[0];
-    }
-
-    /*
-        Use this version for fire and forget style messaging.
-    */
-    public void sendOneWay(Message message, EndPoint to)
-    {        
-        // do local deliveries        
-        if ( message.getFrom().equals(to) )
-        {            
-            MessagingService.receive(message);
-            return;
-        }
-        
-        Runnable tcpWriteEvent = new MessageSerializationTask(message, to);
-        messageSerializerExecutor_.execute(tcpWriteEvent);    
-    }
-    
-    public IAsyncResult sendRR(Message message, EndPoint to)
-    {
-        IAsyncResult iar = new AsyncResult();
-        taskCompletionMap_.put(message.getMessageId(), iar);
-        sendOneWay(message, to);
-        return iar;
-    }
-    
-    public void sendUdpOneWay(Message message, EndPoint to)
-    {
-        EndPoint from = message.getFrom();              
-        if (message.getFrom().equals(to)) {
-            MessagingService.receive(message);
-            return;
-        }
-        
-        UdpConnection connection = null;
-        try
-        {
-            connection = new UdpConnection(); 
-            connection.init();            
-            connection.write(message, to);            
-        }            
-        catch ( IOException e )
-        {               
-            logger_.warn(LogUtil.throwableToString(e));
-        } 
-        finally
-        {
-            if ( connection != null )
-                connection.close();
-        }
-    }
-    
-    public void stream(String file, long startPosition, long total, EndPoint from, EndPoint to)
-    {
-        isStreaming_.set(true);
-        /* Streaming asynchronously on streamExector_ threads. */
-        Runnable streamingTask = new FileStreamTask(file, startPosition, total, from, to);
-        streamExecutor_.execute(streamingTask);
-    }
-    
-    /*
-     * Does the application determine if we are currently streaming data.
-     * This would imply either streaming to a receiver, receiving streamed
-     * data or both. 
-    */
-    public static boolean isStreaming()
-    {
-        return isStreaming_.get();
-    }
-    
-    public static void setStreamingMode(boolean bVal)
-    {
-        isStreaming_.set(bVal);
-    }
-    
-    public static void shutdown()
-    {
-        logger_.info("Shutting down ...");
-        synchronized ( MessagingService.class )
-        {          
-            /* Stop listening on any socket */            
-            for( SelectionKey skey : listenSockets_.values() )
-            {
-                skey.cancel();
-                try
-                {
-                    skey.channel().close();
-                }
-                catch (IOException e) {}
-            }
-            listenSockets_.clear();
-            
-            /* Shutdown the threads in the EventQueue's */            
-            messageDeserializationExecutor_.shutdownNow();            
-            messageSerializerExecutor_.shutdownNow();
-            messageDeserializerExecutor_.shutdownNow();
-            streamExecutor_.shutdownNow();
-            
-            /* shut down the cachetables */
-            taskCompletionMap_.shutdown();
-            callbackMap_.shutdown();                        
-                        
-            /* Interrupt the selector manager thread */
-            SelectorManager.getSelectorManager().interrupt();
-            
-            poolTable_.clear();            
-            verbHandlers_.clear();                                    
-            bShutdown_ = true;
-        }
-        if (logger_.isDebugEnabled())
-          logger_.debug("Shutdown invocation complete.");
-    }
-
-    public static void receive(Message message)
-    {        
-        enqueueRunnable(message.getMessageType(), new MessageDeliveryTask(message));
-    }
-    
-    public static boolean isLocalEndPoint(EndPoint ep)
-    {
-        return ( endPoints_.contains(ep) );
-    }
-        
-    private static void enqueueRunnable(String stageName, Runnable runnable){
-        
-        IStage stage = StageManager.getStage(stageName);   
-        
-        if ( stage != null )
-        {
-            stage.execute(runnable);
-        } 
-        else
-        {
-            logger_.info("Running on default stage - beware");
-            messageSerializerExecutor_.execute(runnable);
-        }
-    }    
-    
-    public static IAsyncCallback getRegisteredCallback(String key)
-    {
-        return callbackMap_.get(key);
-    }
-    
-    public static void removeRegisteredCallback(String key)
-    {
-        callbackMap_.remove(key);
-    }
-    
-    public static IAsyncResult getAsyncResult(String key)
-    {
-        return taskCompletionMap_.remove(key);
-    }
-    
-    public static void removeAsyncResult(String key)
-    {
-        taskCompletionMap_.remove(key);
-    }
-
-    public static byte[] getProtocol()
-    {
-        return protocol_;
-    }
-    
-    public static ExecutorService getReadExecutor()
-    {
-        return messageDeserializationExecutor_;
-    }
-    
-    public static ExecutorService getWriteExecutor()
-    {
-        return messageSerializerExecutor_;
-    }
-    
-    public static ExecutorService getDeserilizationExecutor()
-    {
-        return messageDeserializerExecutor_;
-    }
-
-    public static boolean isProtocolValid(byte[] protocol)
-    {
-        return isEqual(protocol_, protocol);
-    }
-    
-    public static boolean isEqual(byte digestA[], byte digestB[])
-    {
-        return MessageDigest.isEqual(digestA, digestB);
-    }
-
-    public static byte[] toByteArray(int i)
-    {
-        byte bytes[] = new byte[4];
-        bytes[0] = (byte)(i >>> 24 & 0xff);
-        bytes[1] = (byte)(i >>> 16 & 0xff);
-        bytes[2] = (byte)(i >>> 8 & 0xff);
-        bytes[3] = (byte)(i & 0xff);
-        return bytes;
-    }
-    
-    public static byte[] toByteArray(short s)
-    {
-        byte bytes[] = new byte[2];
-        bytes[0] = (byte)(s >>> 8 & 0xff);
-        bytes[1] = (byte)(s & 0xff);
-        return bytes;
-    }
-    
-    public static short byteArrayToShort(byte bytes[])
-    {
-        return byteArrayToShort(bytes, 0);
-    }
-    
-    public static short byteArrayToShort(byte bytes[], int offset)
-    {
-        if(bytes.length - offset < 2)
-            throw new IllegalArgumentException("A short must be 2 bytes in size.");
-        short n = 0;
-        for(int i = 0; i < 2; i++)
-        {
-            n <<= 8;
-            n |= bytes[offset + i] & 0xff;
-        }
-
-        return n;
-    }
-
-    public static int getBits(int x, int p, int n)
-    {
-        return x >>> (p + 1) - n & ~(-1 << n);
-    }
-    
-    public static int byteArrayToInt(byte bytes[])
-    {
-        return byteArrayToInt(bytes, 0);
-    }
-
-    public static int byteArrayToInt(byte bytes[], int offset)
-    {
-        if(bytes.length - offset < 4)
-            throw new IllegalArgumentException("An integer must be 4 bytes in size.");
-        int n = 0;
-        for(int i = 0; i < 4; i++)
-        {
-            n <<= 8;
-            n |= bytes[offset + i] & 0xff;
-        }
-
-        return n;
-    }
-    
-    public static ByteBuffer packIt(byte[] bytes, boolean compress, boolean stream, boolean listening)
-    {
-        byte[] size = toByteArray(bytes.length);
-        /* 
-             Setting up the protocol header. This is 4 bytes long
-             represented as an integer. The first 2 bits indicate
-             the serializer type. The 3rd bit indicates if compression
-             is turned on or off. It is turned off by default. The 4th
-             bit indicates if we are in streaming mode. It is turned off
-             by default. The 5th bit is used to indicate that the sender
-             is not listening on any well defined port. This implies the 
-             receiver needs to cache the connection using the port on the 
-             socket. The following 3 bits are reserved for future use. 
-             The next 8 bits indicate a version number. Remaining 15 bits 
-             are not used currently.            
-        */
-        int n = 0;
-        // Setting up the serializer bit
-        n |= serializerType_.ordinal();
-        // set compression bit.
-        if ( compress )
-            n |= 4;
-        
-        // set streaming bit
-        if ( stream )
-            n |= 8;
-        
-        // set listening 5th bit
-        if ( listening )
-            n |= 16;
-        
-        // Setting up the version bit 
-        n |= (version_ << 8);               
-        /* Finished the protocol header setup */
-               
-        byte[] header = toByteArray(n);
-        ByteBuffer buffer = ByteBuffer.allocate(16 + header.length + size.length + bytes.length);
-        buffer.put(protocol_);
-        buffer.put(header);
-        buffer.put(size);
-        buffer.put(bytes);
-        buffer.flip();
-        return buffer;
-    }
-        
-    public static ByteBuffer constructStreamHeader(boolean compress, boolean stream)
-    {
-        /* 
-        Setting up the protocol header. This is 4 bytes long
-        represented as an integer. The first 2 bits indicate
-        the serializer type. The 3rd bit indicates if compression
-        is turned on or off. It is turned off by default. The 4th
-        bit indicates if we are in streaming mode. It is turned off
-        by default. The following 4 bits are reserved for future use. 
-        The next 8 bits indicate a version number. Remaining 15 bits 
-        are not used currently.            
-        */
-        int n = 0;
-        // Setting up the serializer bit
-        n |= serializerType_.ordinal();
-        // set compression bit.
-        if ( compress )
-            n |= 4;
-       
-        // set streaming bit
-        if ( stream )
-            n |= 8;
-       
-        // Setting up the version bit 
-        n |= (version_ << 8);              
-        /* Finished the protocol header setup */
-              
-        byte[] header = toByteArray(n);
-        ByteBuffer buffer = ByteBuffer.allocate(16 + header.length);
-        buffer.put(protocol_);
-        buffer.put(header);
-        buffer.flip();
-        return buffer;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import org.apache.cassandra.concurrent.*;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.io.SerializerType;
+import org.apache.cassandra.utils.*;
+import org.apache.log4j.Logger;
+
+import java.io.IOException;
+import java.net.MulticastSocket;
+import java.net.ServerSocket;
+import java.nio.ByteBuffer;
+import java.nio.channels.SelectionKey;
+import java.nio.channels.ServerSocketChannel;
+import java.security.MessageDigest;
+import java.util.*;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.ReentrantLock;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MessagingService implements IMessagingService
+{
+    private static boolean debugOn_ = false;   
+    
+    private static int version_ = 1;
+    //TODO: make this parameter dynamic somehow.  Not sure if config is appropriate.
+    private static SerializerType serializerType_ = SerializerType.BINARY;
+    
+    private static byte[] protocol_ = new byte[16];
+    /* Verb Handler for the Response */
+    public static final String responseVerbHandler_ = "RESPONSE";
+    /* Stage for responses. */
+    public static final String responseStage_ = "RESPONSE-STAGE";
+    private enum ReservedVerbs_ {RESPONSE};
+    
+    private static Map<String, String> reservedVerbs_ = new Hashtable<String, String>();
+    /* Indicate if we are currently streaming data to another node or receiving streaming data */
+    private static AtomicBoolean isStreaming_ = new AtomicBoolean(false);
+    
+    /* This records all the results mapped by message Id */
+    private static ICachetable<String, IAsyncCallback> callbackMap_;
+    private static ICachetable<String, IAsyncResult> taskCompletionMap_;
+    
+    /* Manages the table of endpoints it is listening on */
+    private static Set<EndPoint> endPoints_;
+    
+    /* List of sockets we are listening on */
+    private static Map<EndPoint, SelectionKey> listenSockets_ = new HashMap<EndPoint, SelectionKey>();
+    
+    /* Lookup table for registering message handlers based on the verb. */
+    private static Map<String, IVerbHandler> verbHandlers_;
+    
+    private static Map<String, MulticastSocket> mCastMembership_ = new HashMap<String, MulticastSocket>();
+    
+    /* Thread pool to handle messaging read activities of Socket and default stage */
+    private static ExecutorService messageDeserializationExecutor_;
+    
+    /* Thread pool to handle messaging write activities */
+    private static ExecutorService messageSerializerExecutor_;
+    
+    /* Thread pool to handle deserialization of messages read from the socket. */
+    private static ExecutorService messageDeserializerExecutor_;
+    
+    /* Thread pool to handle messaging write activities */
+    private static ExecutorService streamExecutor_;
+    
+    private final static ReentrantLock lock_ = new ReentrantLock();
+    private static Map<String, TcpConnectionManager> poolTable_ = new Hashtable<String, TcpConnectionManager>();
+    
+    private static boolean bShutdown_ = false;
+    
+    private static Logger logger_ = Logger.getLogger(MessagingService.class);
+    
+    private static IMessagingService messagingService_ = new MessagingService();
+    
+    public static boolean isDebugOn()
+    {
+        return debugOn_;
+    }
+    
+    public static void debugOn(boolean on)
+    {
+        debugOn_ = on;
+    }
+    
+    public static SerializerType getSerializerType()
+    {
+        return serializerType_;
+    }
+    
+    public synchronized static void serializerType(String type)
+    { 
+        if ( type.equalsIgnoreCase("binary") )
+        {
+            serializerType_ = SerializerType.BINARY;
+        }
+        else if ( type.equalsIgnoreCase("java") )
+        {
+            serializerType_ = SerializerType.JAVA;
+        }
+        else if ( type.equalsIgnoreCase("xml") )
+        {
+            serializerType_ = SerializerType.XML;
+        }
+    }
+    
+    public static int getVersion()
+    {
+        return version_;
+    }
+    
+    public static void setVersion(int version)
+    {
+        version_ = version;
+    }
+    
+    public static IMessagingService getMessagingInstance()
+    {   
+    	if ( bShutdown_ )
+    	{
+            lock_.lock();
+            try
+            {
+                if ( bShutdown_ )
+                {
+            		messagingService_ = new MessagingService();
+            		bShutdown_ = false;
+                }
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+    	}
+        return messagingService_;
+    }
+    
+    public Object clone() throws CloneNotSupportedException
+    {
+        //Prevents the singleton from being cloned
+        throw new CloneNotSupportedException();
+    }
+
+    protected MessagingService()
+    {        
+        for ( ReservedVerbs_ verbs : ReservedVerbs_.values() )
+        {
+            reservedVerbs_.put(verbs.toString(), verbs.toString());
+        }
+        verbHandlers_ = new HashMap<String, IVerbHandler>();        
+        endPoints_ = new HashSet<EndPoint>();
+        /*
+         * Leave callbacks in the cachetable long enough that any related messages will arrive
+         * before the callback is evicted from the table. The concurrency level is set at 128
+         * which is the sum of the threads in the pool that adds shit into the table and the 
+         * pool that retrives the callback from here.
+        */ 
+        int maxSize = MessagingConfig.getMessagingThreadCount();
+        callbackMap_ = new Cachetable<String, IAsyncCallback>( 2 * DatabaseDescriptor.getRpcTimeout() );
+        taskCompletionMap_ = new Cachetable<String, IAsyncResult>( 2 * DatabaseDescriptor.getRpcTimeout() );        
+        
+        messageDeserializationExecutor_ = new DebuggableThreadPoolExecutor( maxSize,
+                maxSize,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl("MESSAGING-SERVICE-POOL")
+                );
+                
+        messageSerializerExecutor_ = new DebuggableThreadPoolExecutor( maxSize,
+                maxSize,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl("MESSAGE-SERIALIZER-POOL")
+                ); 
+        
+        messageDeserializerExecutor_ = new DebuggableThreadPoolExecutor( maxSize,
+                maxSize,
+                Integer.MAX_VALUE,
+                TimeUnit.SECONDS,
+                new LinkedBlockingQueue<Runnable>(),
+                new ThreadFactoryImpl("MESSAGE-DESERIALIZER-POOL")
+                ); 
+        
+        streamExecutor_ = new DebuggableThreadPoolExecutor("MESSAGE-STREAMING-POOL");
+                
+        protocol_ = hash(HashingSchemes.MD5, "FB-MESSAGING".getBytes());        
+        /* register the response verb handler */
+        registerVerbHandlers(MessagingService.responseVerbHandler_, new ResponseVerbHandler());
+        /* register stage for response */
+        StageManager.registerStage(MessagingService.responseStage_, new MultiThreadedStage("RESPONSE-STAGE", maxSize) );
+    }
+    
+    public byte[] hash(String type, byte data[])
+    {
+        byte result[] = null;
+        try
+        {
+            MessageDigest messageDigest = MessageDigest.getInstance(type);
+            result = messageDigest.digest(data);
+        }
+        catch(Exception e)
+        {
+            if (logger_.isDebugEnabled())
+                logger_.debug(LogUtil.throwableToString(e));
+        }
+        return result;
+    }
+    
+    public void listen(EndPoint localEp) throws IOException
+    {        
+        ServerSocketChannel serverChannel = ServerSocketChannel.open();
+        ServerSocket ss = serverChannel.socket();            
+        ss.bind(localEp.getInetAddress());
+        serverChannel.configureBlocking(false);
+        
+        SelectionKeyHandler handler = new TcpConnectionHandler(localEp);
+
+        SelectionKey key = SelectorManager.getSelectorManager().register(serverChannel, handler, SelectionKey.OP_ACCEPT);          
+        endPoints_.add(localEp);            
+        listenSockets_.put(localEp, key);             
+    }
+    
+    public void listenUDP(EndPoint localEp)
+    {
+        UdpConnection connection = new UdpConnection();
+        if (logger_.isDebugEnabled())
+          logger_.debug("Starting to listen on " + localEp);
+        try
+        {
+            connection.init(localEp.getPort());
+            endPoints_.add(localEp);     
+        }
+        catch ( IOException e )
+        {
+            logger_.warn(LogUtil.throwableToString(e));
+        }
+    }
+    
+    public static TcpConnectionManager getConnectionPool(EndPoint from, EndPoint to)
+    {
+        String key = from + ":" + to;
+        TcpConnectionManager cp = poolTable_.get(key);
+        if( cp == null )
+        {
+            lock_.lock();
+            try
+            {
+                cp = poolTable_.get(key);
+                if (cp == null )
+                {
+                    cp = new TcpConnectionManager(MessagingConfig.getConnectionPoolInitialSize(), 
+                            MessagingConfig.getConnectionPoolGrowthFactor(), 
+                            MessagingConfig.getConnectionPoolMaxSize(), from, to);
+                    poolTable_.put(key, cp);
+                }
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        return cp;
+    }
+
+    public static ConnectionStatistics[] getPoolStatistics()
+    {
+        Set<ConnectionStatistics> stats = new HashSet<ConnectionStatistics>();        
+        Iterator<TcpConnectionManager> it = poolTable_.values().iterator();
+        while ( it.hasNext() )
+        {
+            TcpConnectionManager cp = it.next();
+            ConnectionStatistics cs = new ConnectionStatistics(cp.getLocalEndPoint(), cp.getRemoteEndPoint(), cp.getPoolSize(), cp.getConnectionsInUse());
+            stats.add( cs );
+        }
+        return stats.toArray(new ConnectionStatistics[0]);
+    }
+    
+    public static TcpConnection getConnection(EndPoint from, EndPoint to) throws IOException
+    {
+        return getConnectionPool(from, to).getConnection();
+    }
+    
+    private void checkForReservedVerb(String type)
+    {
+    	if ( reservedVerbs_.get(type) != null && verbHandlers_.get(type) != null )
+    	{
+    		throw new IllegalArgumentException( type + " is a reserved verb handler. Scram!");
+    	}
+    }     
+    
+    public void registerVerbHandlers(String type, IVerbHandler verbHandler)
+    {
+    	checkForReservedVerb(type);
+    	verbHandlers_.put(type, verbHandler);
+    }
+    
+    public void deregisterAllVerbHandlers(EndPoint localEndPoint)
+    {
+        Iterator keys = verbHandlers_.keySet().iterator();
+        String key = null;
+        
+        /*
+         * endpoint specific verbhandlers can be distinguished because
+         * their key's contain the name of the endpoint. 
+         */
+        while(keys.hasNext())
+        {
+            key = (String)keys.next();
+            if (key.contains(localEndPoint.toString()))
+                keys.remove();
+        }
+    }
+    
+    public void deregisterVerbHandlers(String type)
+    {
+        verbHandlers_.remove(type);
+    }
+
+    public IVerbHandler getVerbHandler(String type)
+    {
+        IVerbHandler handler = (IVerbHandler)verbHandlers_.get(type);
+        return handler;
+    }
+
+    public String sendRR(Message message, EndPoint[] to, IAsyncCallback cb)
+    {
+        String messageId = message.getMessageId();                        
+        callbackMap_.put(messageId, cb);
+        for ( int i = 0; i < to.length; ++i )
+        {
+            sendOneWay(message, to[i]);
+        }
+        return messageId;
+    }
+    
+    public String sendRR(Message message, EndPoint to, IAsyncCallback cb)
+    {        
+        String messageId = message.getMessageId();
+        callbackMap_.put(messageId, cb);
+        sendOneWay(message, to);
+        return messageId;
+    }
+
+    public String sendRR(Message[] messages, EndPoint[] to, IAsyncCallback cb)
+    {
+        if ( messages.length != to.length )
+        {
+            throw new IllegalArgumentException("Number of messages and the number of endpoints need to be same.");
+        }
+        String groupId = GuidGenerator.guid();
+        callbackMap_.put(groupId, cb);
+        for ( int i = 0; i < messages.length; ++i )
+        {
+            messages[i].setMessageId(groupId);
+            sendOneWay(messages[i], to[i]);
+        }
+        return groupId;
+    } 
+    
+    public IAsyncResult sendRR(Message[] messages, EndPoint[] to)
+    {
+        if ( messages.length != to.length )
+        {
+            throw new IllegalArgumentException("Number of messages and the number of endpoints need to be same.");
+        }
+        
+        IAsyncResult iar = new MultiAsyncResult(messages.length);
+        String groupId = GuidGenerator.guid();
+        taskCompletionMap_.put(groupId, iar);
+        for ( int i = 0; i < messages.length; ++i )
+        {
+            messages[i].setMessageId(groupId);
+            sendOneWay(messages[i], to[i]);
+        }
+        
+        return iar;
+    }
+    
+    public String sendRR(Message[][] messages, EndPoint[][] to, IAsyncCallback cb)
+    {
+        if ( messages.length != to.length )
+        {
+            throw new IllegalArgumentException("Number of messages and the number of endpoints need to be same.");
+        }
+        
+        int length = messages.length;
+        String[] gids = new String[length];
+        /* Generate the requisite GUID's */
+        for ( int i = 0; i < length; ++i )
+        {
+            gids[i] = GuidGenerator.guid();
+        }
+        /* attach this context to the callback */
+        cb.attachContext(gids);
+        for ( int i = 0; i < length; ++i )
+        {
+            callbackMap_.put(gids[i], cb);
+            for ( int j = 0; j < messages[i].length; ++j )
+            {
+                messages[i][j].setMessageId(gids[i]);
+                sendOneWay(messages[i][j], to[i][j]);
+            }            
+        }      
+        return gids[0];
+    }
+
+    /*
+        Use this version for fire and forget style messaging.
+    */
+    public void sendOneWay(Message message, EndPoint to)
+    {        
+        // do local deliveries        
+        if ( message.getFrom().equals(to) )
+        {            
+            MessagingService.receive(message);
+            return;
+        }
+        
+        Runnable tcpWriteEvent = new MessageSerializationTask(message, to);
+        messageSerializerExecutor_.execute(tcpWriteEvent);    
+    }
+    
+    public IAsyncResult sendRR(Message message, EndPoint to)
+    {
+        IAsyncResult iar = new AsyncResult();
+        taskCompletionMap_.put(message.getMessageId(), iar);
+        sendOneWay(message, to);
+        return iar;
+    }
+    
+    public void sendUdpOneWay(Message message, EndPoint to)
+    {
+        EndPoint from = message.getFrom();              
+        if (message.getFrom().equals(to)) {
+            MessagingService.receive(message);
+            return;
+        }
+        
+        UdpConnection connection = null;
+        try
+        {
+            connection = new UdpConnection(); 
+            connection.init();            
+            connection.write(message, to);            
+        }            
+        catch ( IOException e )
+        {               
+            logger_.warn(LogUtil.throwableToString(e));
+        } 
+        finally
+        {
+            if ( connection != null )
+                connection.close();
+        }
+    }
+    
+    public void stream(String file, long startPosition, long total, EndPoint from, EndPoint to)
+    {
+        isStreaming_.set(true);
+        /* Streaming asynchronously on streamExector_ threads. */
+        Runnable streamingTask = new FileStreamTask(file, startPosition, total, from, to);
+        streamExecutor_.execute(streamingTask);
+    }
+    
+    /*
+     * Does the application determine if we are currently streaming data.
+     * This would imply either streaming to a receiver, receiving streamed
+     * data or both. 
+    */
+    public static boolean isStreaming()
+    {
+        return isStreaming_.get();
+    }
+    
+    public static void setStreamingMode(boolean bVal)
+    {
+        isStreaming_.set(bVal);
+    }
+    
+    public static void shutdown()
+    {
+        logger_.info("Shutting down ...");
+        synchronized ( MessagingService.class )
+        {          
+            /* Stop listening on any socket */            
+            for( SelectionKey skey : listenSockets_.values() )
+            {
+                skey.cancel();
+                try
+                {
+                    skey.channel().close();
+                }
+                catch (IOException e) {}
+            }
+            listenSockets_.clear();
+            
+            /* Shutdown the threads in the EventQueue's */            
+            messageDeserializationExecutor_.shutdownNow();            
+            messageSerializerExecutor_.shutdownNow();
+            messageDeserializerExecutor_.shutdownNow();
+            streamExecutor_.shutdownNow();
+            
+            /* shut down the cachetables */
+            taskCompletionMap_.shutdown();
+            callbackMap_.shutdown();                        
+                        
+            /* Interrupt the selector manager thread */
+            SelectorManager.getSelectorManager().interrupt();
+            
+            poolTable_.clear();            
+            verbHandlers_.clear();                                    
+            bShutdown_ = true;
+        }
+        if (logger_.isDebugEnabled())
+          logger_.debug("Shutdown invocation complete.");
+    }
+
+    public static void receive(Message message)
+    {        
+        enqueueRunnable(message.getMessageType(), new MessageDeliveryTask(message));
+    }
+    
+    public static boolean isLocalEndPoint(EndPoint ep)
+    {
+        return ( endPoints_.contains(ep) );
+    }
+        
+    private static void enqueueRunnable(String stageName, Runnable runnable){
+        
+        IStage stage = StageManager.getStage(stageName);   
+        
+        if ( stage != null )
+        {
+            stage.execute(runnable);
+        } 
+        else
+        {
+            logger_.info("Running on default stage - beware");
+            messageSerializerExecutor_.execute(runnable);
+        }
+    }    
+    
+    public static IAsyncCallback getRegisteredCallback(String key)
+    {
+        return callbackMap_.get(key);
+    }
+    
+    public static void removeRegisteredCallback(String key)
+    {
+        callbackMap_.remove(key);
+    }
+    
+    public static IAsyncResult getAsyncResult(String key)
+    {
+        return taskCompletionMap_.remove(key);
+    }
+    
+    public static void removeAsyncResult(String key)
+    {
+        taskCompletionMap_.remove(key);
+    }
+
+    public static byte[] getProtocol()
+    {
+        return protocol_;
+    }
+    
+    public static ExecutorService getReadExecutor()
+    {
+        return messageDeserializationExecutor_;
+    }
+    
+    public static ExecutorService getWriteExecutor()
+    {
+        return messageSerializerExecutor_;
+    }
+    
+    public static ExecutorService getDeserilizationExecutor()
+    {
+        return messageDeserializerExecutor_;
+    }
+
+    public static boolean isProtocolValid(byte[] protocol)
+    {
+        return isEqual(protocol_, protocol);
+    }
+    
+    public static boolean isEqual(byte digestA[], byte digestB[])
+    {
+        return MessageDigest.isEqual(digestA, digestB);
+    }
+
+    public static byte[] toByteArray(int i)
+    {
+        byte bytes[] = new byte[4];
+        bytes[0] = (byte)(i >>> 24 & 0xff);
+        bytes[1] = (byte)(i >>> 16 & 0xff);
+        bytes[2] = (byte)(i >>> 8 & 0xff);
+        bytes[3] = (byte)(i & 0xff);
+        return bytes;
+    }
+    
+    public static byte[] toByteArray(short s)
+    {
+        byte bytes[] = new byte[2];
+        bytes[0] = (byte)(s >>> 8 & 0xff);
+        bytes[1] = (byte)(s & 0xff);
+        return bytes;
+    }
+    
+    public static short byteArrayToShort(byte bytes[])
+    {
+        return byteArrayToShort(bytes, 0);
+    }
+    
+    public static short byteArrayToShort(byte bytes[], int offset)
+    {
+        if(bytes.length - offset < 2)
+            throw new IllegalArgumentException("A short must be 2 bytes in size.");
+        short n = 0;
+        for(int i = 0; i < 2; i++)
+        {
+            n <<= 8;
+            n |= bytes[offset + i] & 0xff;
+        }
+
+        return n;
+    }
+
+    public static int getBits(int x, int p, int n)
+    {
+        return x >>> (p + 1) - n & ~(-1 << n);
+    }
+    
+    public static int byteArrayToInt(byte bytes[])
+    {
+        return byteArrayToInt(bytes, 0);
+    }
+
+    public static int byteArrayToInt(byte bytes[], int offset)
+    {
+        if(bytes.length - offset < 4)
+            throw new IllegalArgumentException("An integer must be 4 bytes in size.");
+        int n = 0;
+        for(int i = 0; i < 4; i++)
+        {
+            n <<= 8;
+            n |= bytes[offset + i] & 0xff;
+        }
+
+        return n;
+    }
+    
+    public static ByteBuffer packIt(byte[] bytes, boolean compress, boolean stream, boolean listening)
+    {
+        byte[] size = toByteArray(bytes.length);
+        /* 
+             Setting up the protocol header. This is 4 bytes long
+             represented as an integer. The first 2 bits indicate
+             the serializer type. The 3rd bit indicates if compression
+             is turned on or off. It is turned off by default. The 4th
+             bit indicates if we are in streaming mode. It is turned off
+             by default. The 5th bit is used to indicate that the sender
+             is not listening on any well defined port. This implies the 
+             receiver needs to cache the connection using the port on the 
+             socket. The following 3 bits are reserved for future use. 
+             The next 8 bits indicate a version number. Remaining 15 bits 
+             are not used currently.            
+        */
+        int n = 0;
+        // Setting up the serializer bit
+        n |= serializerType_.ordinal();
+        // set compression bit.
+        if ( compress )
+            n |= 4;
+        
+        // set streaming bit
+        if ( stream )
+            n |= 8;
+        
+        // set listening 5th bit
+        if ( listening )
+            n |= 16;
+        
+        // Setting up the version bit 
+        n |= (version_ << 8);               
+        /* Finished the protocol header setup */
+               
+        byte[] header = toByteArray(n);
+        ByteBuffer buffer = ByteBuffer.allocate(16 + header.length + size.length + bytes.length);
+        buffer.put(protocol_);
+        buffer.put(header);
+        buffer.put(size);
+        buffer.put(bytes);
+        buffer.flip();
+        return buffer;
+    }
+        
+    public static ByteBuffer constructStreamHeader(boolean compress, boolean stream)
+    {
+        /* 
+        Setting up the protocol header. This is 4 bytes long
+        represented as an integer. The first 2 bits indicate
+        the serializer type. The 3rd bit indicates if compression
+        is turned on or off. It is turned off by default. The 4th
+        bit indicates if we are in streaming mode. It is turned off
+        by default. The following 4 bits are reserved for future use. 
+        The next 8 bits indicate a version number. Remaining 15 bits 
+        are not used currently.            
+        */
+        int n = 0;
+        // Setting up the serializer bit
+        n |= serializerType_.ordinal();
+        // set compression bit.
+        if ( compress )
+            n |= 4;
+       
+        // set streaming bit
+        if ( stream )
+            n |= 8;
+       
+        // Setting up the version bit 
+        n |= (version_ << 8);              
+        /* Finished the protocol header setup */
+              
+        byte[] header = toByteArray(n);
+        ByteBuffer buffer = ByteBuffer.allocate(16 + header.length);
+        buffer.put(protocol_);
+        buffer.put(header);
+        buffer.flip();
+        return buffer;
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/MultiAsyncResult.java b/src/java/org/apache/cassandra/net/MultiAsyncResult.java
index 95b0ca42e2..62ab74764c 100644
--- a/src/java/org/apache/cassandra/net/MultiAsyncResult.java
+++ b/src/java/org/apache/cassandra/net/MultiAsyncResult.java
@@ -1,133 +1,133 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.util.ArrayList;
-import java.util.List;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.TimeoutException;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.locks.Condition;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-public class MultiAsyncResult implements IAsyncResult
-{
-    private static Logger logger_ = Logger.getLogger( AsyncResult.class );
-    private int expectedResults_;
-    private List<byte[]> result_ = new ArrayList<byte[]>();
-    private AtomicBoolean done_ = new AtomicBoolean(false);
-    private Lock lock_ = new ReentrantLock();
-    private Condition condition_;
-    
-    MultiAsyncResult(int expectedResults)
-    {
-        expectedResults_ = expectedResults;
-        condition_ = lock_.newCondition();
-    }
-    
-    public byte[] get()
-    {
-        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
-    }
-    
-    public byte[] get(long timeout, TimeUnit tu) throws TimeoutException
-    {
-        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
-    }
-    
-    public List<byte[]> multiget()
-    {
-        lock_.lock();
-        try
-        {
-            if ( !done_.get() )
-            {
-                condition_.await();                    
-            }
-        }
-        catch ( InterruptedException ex )
-        {
-            logger_.warn( LogUtil.throwableToString(ex) );
-        }
-        finally
-        {
-            lock_.unlock();            
-        }        
-        return result_;
-    }
-    
-    public boolean isDone()
-    {
-        return done_.get();
-    }
-    
-    public List<byte[]> multiget(long timeout, TimeUnit tu) throws TimeoutException
-    {
-        lock_.lock();
-        try
-        {            
-            boolean bVal = true;
-            try
-            {
-                if ( !done_.get() )
-                {                    
-                    bVal = condition_.await(timeout, tu);
-                }
-            }
-            catch ( InterruptedException ex )
-            {
-                logger_.warn( LogUtil.throwableToString(ex) );
-            }
-            
-            if ( !bVal && !done_.get() )
-            {                                           
-                throw new TimeoutException("Operation timed out.");
-            }
-        }
-        finally
-        {
-            lock_.unlock();      
-        }
-        return result_;
-    }
-    
-    public void result(Message result)
-    {        
-        try
-        {
-            lock_.lock();
-            if ( !done_.get() )
-            {
-                result_.add(result.getMessageBody());
-                if ( result_.size() == expectedResults_ )
-                {
-                    done_.set(true);
-                    condition_.signal();
-                }
-            }
-        }
-        finally
-        {
-            lock_.unlock();
-        }        
-    }    
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+public class MultiAsyncResult implements IAsyncResult
+{
+    private static Logger logger_ = Logger.getLogger( AsyncResult.class );
+    private int expectedResults_;
+    private List<byte[]> result_ = new ArrayList<byte[]>();
+    private AtomicBoolean done_ = new AtomicBoolean(false);
+    private Lock lock_ = new ReentrantLock();
+    private Condition condition_;
+    
+    MultiAsyncResult(int expectedResults)
+    {
+        expectedResults_ = expectedResults;
+        condition_ = lock_.newCondition();
+    }
+    
+    public byte[] get()
+    {
+        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
+    }
+    
+    public byte[] get(long timeout, TimeUnit tu) throws TimeoutException
+    {
+        throw new UnsupportedOperationException("This operation is not supported in the AsyncResult abstraction.");
+    }
+    
+    public List<byte[]> multiget()
+    {
+        lock_.lock();
+        try
+        {
+            if ( !done_.get() )
+            {
+                condition_.await();                    
+            }
+        }
+        catch ( InterruptedException ex )
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+        finally
+        {
+            lock_.unlock();            
+        }        
+        return result_;
+    }
+    
+    public boolean isDone()
+    {
+        return done_.get();
+    }
+    
+    public List<byte[]> multiget(long timeout, TimeUnit tu) throws TimeoutException
+    {
+        lock_.lock();
+        try
+        {            
+            boolean bVal = true;
+            try
+            {
+                if ( !done_.get() )
+                {                    
+                    bVal = condition_.await(timeout, tu);
+                }
+            }
+            catch ( InterruptedException ex )
+            {
+                logger_.warn( LogUtil.throwableToString(ex) );
+            }
+            
+            if ( !bVal && !done_.get() )
+            {                                           
+                throw new TimeoutException("Operation timed out.");
+            }
+        }
+        finally
+        {
+            lock_.unlock();      
+        }
+        return result_;
+    }
+    
+    public void result(Message result)
+    {        
+        try
+        {
+            lock_.lock();
+            if ( !done_.get() )
+            {
+                result_.add(result.getMessageBody());
+                if ( result_.size() == expectedResults_ )
+                {
+                    done_.set(true);
+                    condition_.signal();
+                }
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }        
+    }    
+}
diff --git a/src/java/org/apache/cassandra/net/ProtocolHeader.java b/src/java/org/apache/cassandra/net/ProtocolHeader.java
index 5d49576109..621770abaa 100644
--- a/src/java/org/apache/cassandra/net/ProtocolHeader.java
+++ b/src/java/org/apache/cassandra/net/ProtocolHeader.java
@@ -1,36 +1,36 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public final class ProtocolHeader
-{
-    public static final String SERIALIZER = "SERIALIZER";
-    public static final String COMPRESSION = "COMPRESSION";
-    public static final String VERSION = "VERSION";
-    
-    public int serializerType_;
-    public boolean isCompressed_;
-    public boolean isStreamingMode_;
-    public boolean isListening_;
-    public int version_;
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public final class ProtocolHeader
+{
+    public static final String SERIALIZER = "SERIALIZER";
+    public static final String COMPRESSION = "COMPRESSION";
+    public static final String VERSION = "VERSION";
+    
+    public int serializerType_;
+    public boolean isCompressed_;
+    public boolean isStreamingMode_;
+    public boolean isListening_;
+    public int version_;
+}
diff --git a/src/java/org/apache/cassandra/net/ResponseVerbHandler.java b/src/java/org/apache/cassandra/net/ResponseVerbHandler.java
index 4c56d540ee..656988d9ed 100644
--- a/src/java/org/apache/cassandra/net/ResponseVerbHandler.java
+++ b/src/java/org/apache/cassandra/net/ResponseVerbHandler.java
@@ -1,52 +1,52 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class ResponseVerbHandler implements IVerbHandler
-{
-    private static final Logger logger_ = Logger.getLogger( ResponseVerbHandler.class );
-    
-    public void doVerb(Message message)
-    {     
-        String messageId = message.getMessageId();        
-        IAsyncCallback cb = MessagingService.getRegisteredCallback(messageId);
-        if ( cb != null )
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Processing response on a callback from " + message.getMessageId() + "@" + message.getFrom());
-            cb.response(message);
-        }
-        else
-        {            
-            IAsyncResult ar = MessagingService.getAsyncResult(messageId);
-            if ( ar != null )
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Processing response on an async result from " + message.getMessageId() + "@" + message.getFrom());
-                ar.result(message);
-            }
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class ResponseVerbHandler implements IVerbHandler
+{
+    private static final Logger logger_ = Logger.getLogger( ResponseVerbHandler.class );
+    
+    public void doVerb(Message message)
+    {     
+        String messageId = message.getMessageId();        
+        IAsyncCallback cb = MessagingService.getRegisteredCallback(messageId);
+        if ( cb != null )
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Processing response on a callback from " + message.getMessageId() + "@" + message.getFrom());
+            cb.response(message);
+        }
+        else
+        {            
+            IAsyncResult ar = MessagingService.getAsyncResult(messageId);
+            if ( ar != null )
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Processing response on an async result from " + message.getMessageId() + "@" + message.getFrom());
+                ar.result(message);
+            }
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/SelectionKeyHandler.java b/src/java/org/apache/cassandra/net/SelectionKeyHandler.java
index 5f9605ab24..33ed0c6128 100644
--- a/src/java/org/apache/cassandra/net/SelectionKeyHandler.java
+++ b/src/java/org/apache/cassandra/net/SelectionKeyHandler.java
@@ -1,84 +1,84 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.nio.channels.*;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class SelectionKeyHandler 
-{
-    /**
-     * Method which is called when the key becomes acceptable.
-     *
-     * @param key The key which is acceptable.
-     */
-    public void accept(SelectionKey key)
-    {
-         throw new UnsupportedOperationException("accept() cannot be called on " + getClass().getName() + "!");
-    }
-    
-    /**
-     * Method which is called when the key becomes connectable.
-     *
-     * @param key The key which is connectable.
-     */
-    public void connect(SelectionKey key)
-    {
-        throw new UnsupportedOperationException("connect() cannot be called on " + getClass().getName() + "!");
-    }
-    
-    /**
-     * Method which is called when the key becomes readable.
-     *
-     * @param key The key which is readable.
-     */
-    public void read(SelectionKey key)
-    {
-        throw new UnsupportedOperationException("read() cannot be called on " + getClass().getName() + "!");
-    }
-    
-    /**
-     * Method which is called when the key becomes writable.
-     *
-     * @param key The key which is writable.
-     */
-    public void write(SelectionKey key)
-    {
-        throw new UnsupportedOperationException("write() cannot be called on " + getClass().getName() + "!");
-    }
-    
-    protected static void turnOnInterestOps(SelectionKey key, int ops)
-    {
-        synchronized(key)
-        {
-            key.interestOps(key.interestOps() | ops);
-        }
-    }
-    
-    protected static void turnOffInterestOps(SelectionKey key, int ops)
-    {
-        synchronized(key)
-        {
-            key.interestOps(key.interestOps() & (~ops) );
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.nio.channels.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SelectionKeyHandler 
+{
+    /**
+     * Method which is called when the key becomes acceptable.
+     *
+     * @param key The key which is acceptable.
+     */
+    public void accept(SelectionKey key)
+    {
+         throw new UnsupportedOperationException("accept() cannot be called on " + getClass().getName() + "!");
+    }
+    
+    /**
+     * Method which is called when the key becomes connectable.
+     *
+     * @param key The key which is connectable.
+     */
+    public void connect(SelectionKey key)
+    {
+        throw new UnsupportedOperationException("connect() cannot be called on " + getClass().getName() + "!");
+    }
+    
+    /**
+     * Method which is called when the key becomes readable.
+     *
+     * @param key The key which is readable.
+     */
+    public void read(SelectionKey key)
+    {
+        throw new UnsupportedOperationException("read() cannot be called on " + getClass().getName() + "!");
+    }
+    
+    /**
+     * Method which is called when the key becomes writable.
+     *
+     * @param key The key which is writable.
+     */
+    public void write(SelectionKey key)
+    {
+        throw new UnsupportedOperationException("write() cannot be called on " + getClass().getName() + "!");
+    }
+    
+    protected static void turnOnInterestOps(SelectionKey key, int ops)
+    {
+        synchronized(key)
+        {
+            key.interestOps(key.interestOps() | ops);
+        }
+    }
+    
+    protected static void turnOffInterestOps(SelectionKey key, int ops)
+    {
+        synchronized(key)
+        {
+            key.interestOps(key.interestOps() & (~ops) );
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/SelectorManager.java b/src/java/org/apache/cassandra/net/SelectorManager.java
index ed3e9655c6..388d5a0eed 100644
--- a/src/java/org/apache/cassandra/net/SelectorManager.java
+++ b/src/java/org/apache/cassandra/net/SelectorManager.java
@@ -1,178 +1,178 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.io.IOException;
-import java.nio.channels.SelectableChannel;
-import java.nio.channels.SelectionKey;
-import java.nio.channels.Selector;
-
-import org.apache.log4j.Logger;
-
-public class SelectorManager extends Thread
-{
-    private static final Logger logger = Logger.getLogger(SelectorManager.class); 
-
-    // the underlying selector used
-    protected Selector selector;
-
-    // workaround JDK select/register bug
-    Object gate = new Object();
-
-    // The static selector manager which is used by all applications
-    private static SelectorManager manager;
-    
-    // The static UDP selector manager which is used by all applications
-    private static SelectorManager udpManager;
-
-    private SelectorManager(String name)
-    {
-        super(name);
-
-        try
-        {
-            selector = Selector.open();
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-
-        setDaemon(false);
-    }
-
-    /**
-     * Registers a new channel with the selector, and attaches the given
-     * SelectionKeyHandler as the handler for the newly created key. Operations
-     * which the handler is interested in will be called as available.
-     * 
-     * @param channel
-     *            The channel to register with the selector
-     * @param handler
-     *            The handler to use for the callbacks
-     * @param ops
-     *            The initial interest operations
-     * @return The SelectionKey which uniquely identifies this channel
-     * @exception IOException if the channel is closed
-     */
-    public SelectionKey register(SelectableChannel channel,
-            SelectionKeyHandler handler, int ops) throws IOException
-    {
-        assert channel != null;
-        assert handler != null;
-
-        synchronized(gate)
-        {
-            selector.wakeup();
-            return channel.register(selector, ops, handler);
-        }
-    }      
-
-    /**
-     * This method starts the socket manager listening for events. It is
-     * designed to be started when this thread's start() method is invoked.
-     */
-    public void run()
-    {
-        while (true)
-        {
-            try
-            {
-                selector.select(1);
-                doProcess();
-                synchronized(gate) {}
-            }
-            catch (IOException e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-    }
-
-    protected void doProcess() throws IOException
-    {
-        SelectionKey[] keys = selector.selectedKeys().toArray(new SelectionKey[0]);
-
-        for (SelectionKey key : keys)
-        {
-            selector.selectedKeys().remove(key);
-
-            synchronized (key)
-            {
-                SelectionKeyHandler skh = (SelectionKeyHandler) key.attachment();
-
-                if (skh != null)
-                {
-                    // accept
-                    if (key.isValid() && key.isAcceptable())
-                    {
-                        skh.accept(key);
-                    }
-
-                    // connect
-                    if (key.isValid() && key.isConnectable())
-                    {
-                        skh.connect(key);
-                    }
-
-                    // read
-                    if (key.isValid() && key.isReadable())
-                    {
-                        skh.read(key);
-                    }
-
-                    // write
-                    if (key.isValid() && key.isWritable())
-                    {
-                        skh.write(key);
-                    }
-                }
-            }
-        }
-    }
-
-    /**
-     * Returns the SelectorManager applications should use.
-     * 
-     * @return The SelectorManager which applications should use
-     */
-    public static SelectorManager getSelectorManager()
-    {
-        synchronized (SelectorManager.class)
-        {
-            if (manager == null)
-            {
-                manager = new SelectorManager("TCP Selector Manager");
-            }            
-        }
-        return manager;
-    }
-    
-    public static SelectorManager getUdpSelectorManager()
-    {
-        synchronized (SelectorManager.class)
-        {
-            if (udpManager == null)
-            {
-                udpManager = new SelectorManager("UDP Selector Manager");
-            }            
-        }
-        return udpManager;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.nio.channels.SelectableChannel;
+import java.nio.channels.SelectionKey;
+import java.nio.channels.Selector;
+
+import org.apache.log4j.Logger;
+
+public class SelectorManager extends Thread
+{
+    private static final Logger logger = Logger.getLogger(SelectorManager.class); 
+
+    // the underlying selector used
+    protected Selector selector;
+
+    // workaround JDK select/register bug
+    Object gate = new Object();
+
+    // The static selector manager which is used by all applications
+    private static SelectorManager manager;
+    
+    // The static UDP selector manager which is used by all applications
+    private static SelectorManager udpManager;
+
+    private SelectorManager(String name)
+    {
+        super(name);
+
+        try
+        {
+            selector = Selector.open();
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+
+        setDaemon(false);
+    }
+
+    /**
+     * Registers a new channel with the selector, and attaches the given
+     * SelectionKeyHandler as the handler for the newly created key. Operations
+     * which the handler is interested in will be called as available.
+     * 
+     * @param channel
+     *            The channel to register with the selector
+     * @param handler
+     *            The handler to use for the callbacks
+     * @param ops
+     *            The initial interest operations
+     * @return The SelectionKey which uniquely identifies this channel
+     * @exception IOException if the channel is closed
+     */
+    public SelectionKey register(SelectableChannel channel,
+            SelectionKeyHandler handler, int ops) throws IOException
+    {
+        assert channel != null;
+        assert handler != null;
+
+        synchronized(gate)
+        {
+            selector.wakeup();
+            return channel.register(selector, ops, handler);
+        }
+    }      
+
+    /**
+     * This method starts the socket manager listening for events. It is
+     * designed to be started when this thread's start() method is invoked.
+     */
+    public void run()
+    {
+        while (true)
+        {
+            try
+            {
+                selector.select(1);
+                doProcess();
+                synchronized(gate) {}
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+        }
+    }
+
+    protected void doProcess() throws IOException
+    {
+        SelectionKey[] keys = selector.selectedKeys().toArray(new SelectionKey[0]);
+
+        for (SelectionKey key : keys)
+        {
+            selector.selectedKeys().remove(key);
+
+            synchronized (key)
+            {
+                SelectionKeyHandler skh = (SelectionKeyHandler) key.attachment();
+
+                if (skh != null)
+                {
+                    // accept
+                    if (key.isValid() && key.isAcceptable())
+                    {
+                        skh.accept(key);
+                    }
+
+                    // connect
+                    if (key.isValid() && key.isConnectable())
+                    {
+                        skh.connect(key);
+                    }
+
+                    // read
+                    if (key.isValid() && key.isReadable())
+                    {
+                        skh.read(key);
+                    }
+
+                    // write
+                    if (key.isValid() && key.isWritable())
+                    {
+                        skh.write(key);
+                    }
+                }
+            }
+        }
+    }
+
+    /**
+     * Returns the SelectorManager applications should use.
+     * 
+     * @return The SelectorManager which applications should use
+     */
+    public static SelectorManager getSelectorManager()
+    {
+        synchronized (SelectorManager.class)
+        {
+            if (manager == null)
+            {
+                manager = new SelectorManager("TCP Selector Manager");
+            }            
+        }
+        return manager;
+    }
+    
+    public static SelectorManager getUdpSelectorManager()
+    {
+        synchronized (SelectorManager.class)
+        {
+            if (udpManager == null)
+            {
+                udpManager = new SelectorManager("UDP Selector Manager");
+            }            
+        }
+        return udpManager;
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/TcpConnection.java b/src/java/org/apache/cassandra/net/TcpConnection.java
index d49b25b591..3176b2a975 100644
--- a/src/java/org/apache/cassandra/net/TcpConnection.java
+++ b/src/java/org/apache/cassandra/net/TcpConnection.java
@@ -1,520 +1,520 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.io.*;
-import java.nio.ByteBuffer;
-import java.nio.channels.FileChannel;
-import java.nio.channels.SelectionKey;
-import java.nio.channels.SocketChannel;
-import java.util.*;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.ConcurrentLinkedQueue;
-import java.util.concurrent.locks.Condition;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.net.io.FastSerializer;
-import org.apache.cassandra.net.io.ISerializer;
-import org.apache.cassandra.net.io.ProtocolState;
-import org.apache.cassandra.net.io.StartState;
-import org.apache.cassandra.net.io.TcpReader;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class TcpConnection extends SelectionKeyHandler implements Comparable
-{
-    // logging and profiling.
-    private static Logger logger_ = Logger.getLogger(TcpConnection.class);  
-    private static ISerializer serializer_ = new FastSerializer();
-    private SocketChannel socketChannel_;
-    private SelectionKey key_;
-    private TcpConnectionManager pool_;
-    private boolean isIncoming_ = false;       
-    private TcpReader tcpReader_;    
-    private ReadWorkItem readWork_ = new ReadWorkItem(); 
-    private Queue<ByteBuffer> pendingWrites_ = new ConcurrentLinkedQueue<ByteBuffer>();
-    private EndPoint localEp_;
-    private EndPoint remoteEp_;
-    boolean inUse_ = false;
-         
-    /* 
-     * Added for streaming support. We need the boolean
-     * to indicate that this connection is used for 
-     * streaming. The Condition and the Lock are used
-     * to signal the stream() that it can continue
-     * streaming when the socket becomes writable.
-    */
-    private boolean bStream_ = false;
-    private Lock lock_;
-    private Condition condition_;
-    
-    // used from getConnection - outgoing
-    TcpConnection(TcpConnectionManager pool, EndPoint from, EndPoint to) throws IOException
-    {          
-        socketChannel_ = SocketChannel.open();            
-        socketChannel_.configureBlocking(false);        
-        pool_ = pool;
-        
-        localEp_ = from;
-        remoteEp_ = to;
-        
-        if ( !socketChannel_.connect( remoteEp_.getInetAddress() ) )
-        {
-            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_CONNECT);
-        }
-        else
-        {
-            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
-        }
-    }
-    
-    /*
-     * Used for streaming purposes has no pooling semantics.
-    */
-    TcpConnection(EndPoint from, EndPoint to) throws IOException
-    {
-        socketChannel_ = SocketChannel.open();               
-        socketChannel_.configureBlocking(false);       
-        
-        localEp_ = from;
-        remoteEp_ = to;
-        
-        if ( !socketChannel_.connect( remoteEp_.getInetAddress() ) )
-        {
-            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_CONNECT);
-        }
-        else
-        {
-            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
-        }
-        bStream_ = true;
-        lock_ = new ReentrantLock();
-        condition_ = lock_.newCondition();
-    }
-    
-    /*
-     * This method is invoked by the TcpConnectionHandler to accept incoming TCP connections.
-     * Accept the connection and then register interest for reads.
-    */
-    static void acceptConnection(SocketChannel socketChannel, EndPoint localEp, boolean isIncoming) throws IOException
-    {
-        TcpConnection tcpConnection = new TcpConnection(socketChannel, localEp, true);
-        tcpConnection.registerReadInterest();
-    }
-    
-    private void registerReadInterest() throws IOException
-    {
-        key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
-    }
-    
-    // used for incoming connections
-    TcpConnection(SocketChannel socketChannel, EndPoint localEp, boolean isIncoming) throws IOException
-    {       
-        socketChannel_ = socketChannel;
-        socketChannel_.configureBlocking(false);                           
-        isIncoming_ = isIncoming;
-        localEp_ = localEp;
-    }
-    
-    EndPoint getLocalEp()
-    {
-        return localEp_;
-    }
-    
-    public void setLocalEp(EndPoint localEp)
-    {
-        localEp_ = localEp;
-    }
-
-    public EndPoint getEndPoint() 
-    {
-        return remoteEp_;
-    }
-    
-    public boolean isIncoming()
-    {
-        return isIncoming_;
-    }    
-    
-    public SocketChannel getSocketChannel()
-    {
-        return socketChannel_;
-    }    
-    
-    public void write(Message message) throws IOException
-    {           
-        byte[] data = serializer_.serialize(message);        
-        if ( data.length > 0 )
-        {    
-            boolean listening = !message.getFrom().equals(EndPoint.sentinelLocalEndPoint_);
-            ByteBuffer buffer = MessagingService.packIt( data , false, false, listening);   
-            synchronized(this)
-            {
-                if (!pendingWrites_.isEmpty() || !socketChannel_.isConnected())
-                {                     
-                    pendingWrites_.add(buffer);                
-                    return;
-                }
-                
-                socketChannel_.write(buffer);                
-                
-                if (buffer.remaining() > 0) 
-                {                   
-                    pendingWrites_.add(buffer);
-                    turnOnInterestOps(key_, SelectionKey.OP_WRITE);
-                }
-            }
-        }
-    }
-    
-    public void stream(File file, long startPosition, long endPosition) throws IOException, InterruptedException
-    {
-        if ( !bStream_ )
-            throw new IllegalStateException("Cannot stream since we are not set up to stream data.");
-                
-        lock_.lock();        
-        try
-        {            
-            /* transfer 64MB in each attempt */
-            int limit = 64*1024*1024;  
-            long total = endPosition - startPosition;
-            /* keeps track of total number of bytes transferred */
-            long bytesWritten = 0L;                          
-            RandomAccessFile raf = new RandomAccessFile(file, "r");            
-            FileChannel fc = raf.getChannel();            
-            
-            /* 
-             * If the connection is not yet established then wait for
-             * the timeout period of 2 seconds. Attempt to reconnect 3 times and then 
-             * bail with an IOException.
-            */
-            long waitTime = 2;
-            int retry = 0;
-            while (!socketChannel_.isConnected())
-            {
-                if ( retry == 3 )
-                    throw new IOException("Unable to connect to " + remoteEp_ + " after " + retry + " attempts.");
-                condition_.await(waitTime, TimeUnit.SECONDS);
-                ++retry;
-            }
-            
-            while ( bytesWritten < total )
-            {                                
-                if ( startPosition == 0 )
-                {
-                    ByteBuffer buffer = MessagingService.constructStreamHeader(false, true);                      
-                    socketChannel_.write(buffer);
-                    if (buffer.remaining() > 0)
-                    {
-                        pendingWrites_.add(buffer);
-                        turnOnInterestOps(key_, SelectionKey.OP_WRITE);
-                        condition_.await();
-                    }
-                }
-                
-                /* returns the number of bytes transferred from file to the socket */
-                long bytesTransferred = fc.transferTo(startPosition, limit, socketChannel_);
-                if (logger_.isDebugEnabled())
-                    logger_.debug("Bytes transferred " + bytesTransferred);                
-                bytesWritten += bytesTransferred;
-                startPosition += bytesTransferred; 
-                /*
-                 * If the number of bytes transferred is less than intended 
-                 * then we need to wait till socket becomes writeable again. 
-                */
-                if ( bytesTransferred < limit && bytesWritten != total )
-                {                    
-                    turnOnInterestOps(key_, SelectionKey.OP_WRITE);
-                    condition_.await();
-                }
-            }
-        }
-        finally
-        {
-            lock_.unlock();
-        }        
-    }
-
-    private void resumeStreaming()
-    {
-        /* if not in streaming mode do nothing */
-        if ( !bStream_ )
-            return;
-        
-        lock_.lock();
-        try
-        {
-            condition_.signal();
-        }
-        finally
-        {
-            lock_.unlock();
-        }
-    }
-    
-    public void close()
-    {
-        inUse_ = false;
-        if ( pool_.contains(this) )
-            pool_.decUsed();               
-    }
-
-    public boolean isConnected()
-    {
-        return socketChannel_.isConnected();
-    }
-    
-    public boolean equals(Object o)
-    {
-        if ( !(o instanceof TcpConnection) )
-            return false;
-        
-        TcpConnection rhs = (TcpConnection)o;        
-        if ( localEp_.equals(rhs.localEp_) && remoteEp_.equals(rhs.remoteEp_) )
-            return true;
-        else
-            return false;
-    }
-    
-    public int hashCode()
-    {
-        return (localEp_ + ":" + remoteEp_).hashCode();
-    }
-
-    public String toString()
-    {        
-        return socketChannel_.toString();
-    }
-    
-    void closeSocket()
-    {
-        logger_.warn("Closing down connection " + socketChannel_ + " with " + pendingWrites_.size() + " writes remaining.");            
-        if ( pool_ != null )
-        {
-            pool_.removeConnection(this);
-        }
-        cancel(key_);
-        pendingWrites_.clear();
-    }
-    
-    void errorClose() 
-    {        
-        logger_.warn("Closing down connection " + socketChannel_);
-        pendingWrites_.clear();
-        cancel(key_);
-        pendingWrites_.clear();        
-        if ( pool_ != null )
-        {
-            pool_.removeConnection(this);            
-        }
-    }
-    
-    private void cancel(SelectionKey key)
-    {
-        if ( key != null )
-        {
-            key.cancel();
-            try
-            {
-                key.channel().close();
-            }
-            catch (IOException e) {}
-        }
-    }
-    
-    // called in the selector thread
-    public void connect(SelectionKey key)
-    {       
-        turnOffInterestOps(key, SelectionKey.OP_CONNECT);
-        try
-        {
-            if (socketChannel_.finishConnect())
-            {
-                turnOnInterestOps(key, SelectionKey.OP_READ);
-                
-                synchronized(this)
-                {
-                    // this will flush the pending                
-                    if (!pendingWrites_.isEmpty()) 
-                    {
-                        turnOnInterestOps(key_, SelectionKey.OP_WRITE);
-                    }
-                }
-                resumeStreaming();
-            } 
-            else 
-            {  
-                logger_.error("Closing connection because socket channel could not finishConnect.");;
-                errorClose();
-            }
-        } 
-        catch(IOException e) 
-        {               
-            logger_.error("Encountered IOException on connection: "  + socketChannel_, e);
-            errorClose();
-        }
-    }
-    
-    // called in the selector thread
-    public void write(SelectionKey key)
-    {   
-        turnOffInterestOps(key, SelectionKey.OP_WRITE);                
-        doPendingWrites();
-        /*
-         * This is executed only if we are in streaming mode.
-         * Idea is that we read a chunk of data from a source
-         * and wait to read the next from the source until we 
-         * are siganlled to do so from here. 
-        */
-         resumeStreaming();        
-    }
-    
-    void doPendingWrites()
-    {
-        synchronized(this)
-        {
-            try
-            {                     
-                while(!pendingWrites_.isEmpty()) 
-                {
-                    ByteBuffer buffer = pendingWrites_.peek();
-                    socketChannel_.write(buffer);                    
-                    if (buffer.remaining() > 0) 
-                    {   
-                        break;
-                    }               
-                    pendingWrites_.remove();
-                } 
-            
-            }
-            catch(IOException ex)
-            {
-                logger_.error(LogUtil.throwableToString(ex));
-                // This is to fix the wierd Linux bug with NIO.
-                errorClose();
-            }
-            finally
-            {    
-                if (!pendingWrites_.isEmpty())
-                {                    
-                    turnOnInterestOps(key_, SelectionKey.OP_WRITE);
-                }
-            }
-        }
-    }
-    
-    // called in the selector thread
-    public void read(SelectionKey key)
-    {
-        turnOffInterestOps(key, SelectionKey.OP_READ);
-        // publish this event onto to the TCPReadEvent Queue.
-        MessagingService.getReadExecutor().execute(readWork_);
-    }
-    
-    class ReadWorkItem implements Runnable
-    {                 
-        // called from the TCP READ thread pool
-        public void run()
-        {                         
-            if ( tcpReader_ == null )
-            {
-                tcpReader_ = new TcpReader(TcpConnection.this);    
-                StartState nextState = tcpReader_.getSocketState(TcpReader.TcpReaderState.PREAMBLE);
-                if ( nextState == null )
-                {
-                    nextState = new ProtocolState(tcpReader_);
-                    tcpReader_.putSocketState(TcpReader.TcpReaderState.PREAMBLE, nextState);
-                }
-                tcpReader_.morphState(nextState);
-            }
-            
-            try
-            {           
-                byte[] bytes = new byte[0];
-                while ( (bytes = tcpReader_.read()).length > 0 )
-                {                       
-                    ProtocolHeader pH = tcpReader_.getProtocolHeader();                    
-                    if ( !pH.isStreamingMode_ )
-                    {
-                        /* first message received */
-                        if (remoteEp_ == null)
-                        {             
-                            int port = ( pH.isListening_ ) ? DatabaseDescriptor.getStoragePort() : EndPoint.sentinelPort_;
-                            remoteEp_ = new EndPoint( socketChannel_.socket().getInetAddress().getHostAddress(), port );                            
-                            // put connection into pool if possible
-                            pool_ = MessagingService.getConnectionPool(localEp_, remoteEp_);                            
-                            pool_.addToPool(TcpConnection.this);                            
-                        }
-                        
-                        /* Deserialize and handle the message */
-                        MessagingService.getDeserilizationExecutor().submit( new MessageDeserializationTask(pH.serializerType_, bytes) );                                                  
-                        tcpReader_.resetState();
-                    }
-                    else
-                    {
-                        MessagingService.setStreamingMode(false);
-                        /* Close this socket connection  used for streaming */
-                        closeSocket();
-                    }                    
-                }
-            }
-            catch ( IOException ex )
-            {                   
-                handleException(ex);
-            }
-            catch ( Throwable th )
-            {
-                handleException(th);
-            }
-            finally
-            {
-                turnOnInterestOps(key_, SelectionKey.OP_READ);
-            }
-        }
-        
-        private void handleException(Throwable th)
-        {
-            logger_.warn("Problem reading from socket connected to : " + socketChannel_);
-            logger_.warn(LogUtil.throwableToString(th));
-            // This is to fix the weird Linux bug with NIO.
-            errorClose();
-        }
-    }
-    
-    public int pending()
-    {
-        return pendingWrites_.size();
-    }
-    
-    public int compareTo(Object o)
-    {
-        if (o instanceof TcpConnection) 
-        {
-            return pendingWrites_.size() - ((TcpConnection) o).pendingWrites_.size();            
-        }
-                    
-        throw new IllegalArgumentException();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.*;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.channels.SelectionKey;
+import java.nio.channels.SocketChannel;
+import java.util.*;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.ConcurrentLinkedQueue;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.io.FastSerializer;
+import org.apache.cassandra.net.io.ISerializer;
+import org.apache.cassandra.net.io.ProtocolState;
+import org.apache.cassandra.net.io.StartState;
+import org.apache.cassandra.net.io.TcpReader;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TcpConnection extends SelectionKeyHandler implements Comparable
+{
+    // logging and profiling.
+    private static Logger logger_ = Logger.getLogger(TcpConnection.class);  
+    private static ISerializer serializer_ = new FastSerializer();
+    private SocketChannel socketChannel_;
+    private SelectionKey key_;
+    private TcpConnectionManager pool_;
+    private boolean isIncoming_ = false;       
+    private TcpReader tcpReader_;    
+    private ReadWorkItem readWork_ = new ReadWorkItem(); 
+    private Queue<ByteBuffer> pendingWrites_ = new ConcurrentLinkedQueue<ByteBuffer>();
+    private EndPoint localEp_;
+    private EndPoint remoteEp_;
+    boolean inUse_ = false;
+         
+    /* 
+     * Added for streaming support. We need the boolean
+     * to indicate that this connection is used for 
+     * streaming. The Condition and the Lock are used
+     * to signal the stream() that it can continue
+     * streaming when the socket becomes writable.
+    */
+    private boolean bStream_ = false;
+    private Lock lock_;
+    private Condition condition_;
+    
+    // used from getConnection - outgoing
+    TcpConnection(TcpConnectionManager pool, EndPoint from, EndPoint to) throws IOException
+    {          
+        socketChannel_ = SocketChannel.open();            
+        socketChannel_.configureBlocking(false);        
+        pool_ = pool;
+        
+        localEp_ = from;
+        remoteEp_ = to;
+        
+        if ( !socketChannel_.connect( remoteEp_.getInetAddress() ) )
+        {
+            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_CONNECT);
+        }
+        else
+        {
+            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
+        }
+    }
+    
+    /*
+     * Used for streaming purposes has no pooling semantics.
+    */
+    TcpConnection(EndPoint from, EndPoint to) throws IOException
+    {
+        socketChannel_ = SocketChannel.open();               
+        socketChannel_.configureBlocking(false);       
+        
+        localEp_ = from;
+        remoteEp_ = to;
+        
+        if ( !socketChannel_.connect( remoteEp_.getInetAddress() ) )
+        {
+            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_CONNECT);
+        }
+        else
+        {
+            key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
+        }
+        bStream_ = true;
+        lock_ = new ReentrantLock();
+        condition_ = lock_.newCondition();
+    }
+    
+    /*
+     * This method is invoked by the TcpConnectionHandler to accept incoming TCP connections.
+     * Accept the connection and then register interest for reads.
+    */
+    static void acceptConnection(SocketChannel socketChannel, EndPoint localEp, boolean isIncoming) throws IOException
+    {
+        TcpConnection tcpConnection = new TcpConnection(socketChannel, localEp, true);
+        tcpConnection.registerReadInterest();
+    }
+    
+    private void registerReadInterest() throws IOException
+    {
+        key_ = SelectorManager.getSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
+    }
+    
+    // used for incoming connections
+    TcpConnection(SocketChannel socketChannel, EndPoint localEp, boolean isIncoming) throws IOException
+    {       
+        socketChannel_ = socketChannel;
+        socketChannel_.configureBlocking(false);                           
+        isIncoming_ = isIncoming;
+        localEp_ = localEp;
+    }
+    
+    EndPoint getLocalEp()
+    {
+        return localEp_;
+    }
+    
+    public void setLocalEp(EndPoint localEp)
+    {
+        localEp_ = localEp;
+    }
+
+    public EndPoint getEndPoint() 
+    {
+        return remoteEp_;
+    }
+    
+    public boolean isIncoming()
+    {
+        return isIncoming_;
+    }    
+    
+    public SocketChannel getSocketChannel()
+    {
+        return socketChannel_;
+    }    
+    
+    public void write(Message message) throws IOException
+    {           
+        byte[] data = serializer_.serialize(message);        
+        if ( data.length > 0 )
+        {    
+            boolean listening = !message.getFrom().equals(EndPoint.sentinelLocalEndPoint_);
+            ByteBuffer buffer = MessagingService.packIt( data , false, false, listening);   
+            synchronized(this)
+            {
+                if (!pendingWrites_.isEmpty() || !socketChannel_.isConnected())
+                {                     
+                    pendingWrites_.add(buffer);                
+                    return;
+                }
+                
+                socketChannel_.write(buffer);                
+                
+                if (buffer.remaining() > 0) 
+                {                   
+                    pendingWrites_.add(buffer);
+                    turnOnInterestOps(key_, SelectionKey.OP_WRITE);
+                }
+            }
+        }
+    }
+    
+    public void stream(File file, long startPosition, long endPosition) throws IOException, InterruptedException
+    {
+        if ( !bStream_ )
+            throw new IllegalStateException("Cannot stream since we are not set up to stream data.");
+                
+        lock_.lock();        
+        try
+        {            
+            /* transfer 64MB in each attempt */
+            int limit = 64*1024*1024;  
+            long total = endPosition - startPosition;
+            /* keeps track of total number of bytes transferred */
+            long bytesWritten = 0L;                          
+            RandomAccessFile raf = new RandomAccessFile(file, "r");            
+            FileChannel fc = raf.getChannel();            
+            
+            /* 
+             * If the connection is not yet established then wait for
+             * the timeout period of 2 seconds. Attempt to reconnect 3 times and then 
+             * bail with an IOException.
+            */
+            long waitTime = 2;
+            int retry = 0;
+            while (!socketChannel_.isConnected())
+            {
+                if ( retry == 3 )
+                    throw new IOException("Unable to connect to " + remoteEp_ + " after " + retry + " attempts.");
+                condition_.await(waitTime, TimeUnit.SECONDS);
+                ++retry;
+            }
+            
+            while ( bytesWritten < total )
+            {                                
+                if ( startPosition == 0 )
+                {
+                    ByteBuffer buffer = MessagingService.constructStreamHeader(false, true);                      
+                    socketChannel_.write(buffer);
+                    if (buffer.remaining() > 0)
+                    {
+                        pendingWrites_.add(buffer);
+                        turnOnInterestOps(key_, SelectionKey.OP_WRITE);
+                        condition_.await();
+                    }
+                }
+                
+                /* returns the number of bytes transferred from file to the socket */
+                long bytesTransferred = fc.transferTo(startPosition, limit, socketChannel_);
+                if (logger_.isDebugEnabled())
+                    logger_.debug("Bytes transferred " + bytesTransferred);                
+                bytesWritten += bytesTransferred;
+                startPosition += bytesTransferred; 
+                /*
+                 * If the number of bytes transferred is less than intended 
+                 * then we need to wait till socket becomes writeable again. 
+                */
+                if ( bytesTransferred < limit && bytesWritten != total )
+                {                    
+                    turnOnInterestOps(key_, SelectionKey.OP_WRITE);
+                    condition_.await();
+                }
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }        
+    }
+
+    private void resumeStreaming()
+    {
+        /* if not in streaming mode do nothing */
+        if ( !bStream_ )
+            return;
+        
+        lock_.lock();
+        try
+        {
+            condition_.signal();
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+    }
+    
+    public void close()
+    {
+        inUse_ = false;
+        if ( pool_.contains(this) )
+            pool_.decUsed();               
+    }
+
+    public boolean isConnected()
+    {
+        return socketChannel_.isConnected();
+    }
+    
+    public boolean equals(Object o)
+    {
+        if ( !(o instanceof TcpConnection) )
+            return false;
+        
+        TcpConnection rhs = (TcpConnection)o;        
+        if ( localEp_.equals(rhs.localEp_) && remoteEp_.equals(rhs.remoteEp_) )
+            return true;
+        else
+            return false;
+    }
+    
+    public int hashCode()
+    {
+        return (localEp_ + ":" + remoteEp_).hashCode();
+    }
+
+    public String toString()
+    {        
+        return socketChannel_.toString();
+    }
+    
+    void closeSocket()
+    {
+        logger_.warn("Closing down connection " + socketChannel_ + " with " + pendingWrites_.size() + " writes remaining.");            
+        if ( pool_ != null )
+        {
+            pool_.removeConnection(this);
+        }
+        cancel(key_);
+        pendingWrites_.clear();
+    }
+    
+    void errorClose() 
+    {        
+        logger_.warn("Closing down connection " + socketChannel_);
+        pendingWrites_.clear();
+        cancel(key_);
+        pendingWrites_.clear();        
+        if ( pool_ != null )
+        {
+            pool_.removeConnection(this);            
+        }
+    }
+    
+    private void cancel(SelectionKey key)
+    {
+        if ( key != null )
+        {
+            key.cancel();
+            try
+            {
+                key.channel().close();
+            }
+            catch (IOException e) {}
+        }
+    }
+    
+    // called in the selector thread
+    public void connect(SelectionKey key)
+    {       
+        turnOffInterestOps(key, SelectionKey.OP_CONNECT);
+        try
+        {
+            if (socketChannel_.finishConnect())
+            {
+                turnOnInterestOps(key, SelectionKey.OP_READ);
+                
+                synchronized(this)
+                {
+                    // this will flush the pending                
+                    if (!pendingWrites_.isEmpty()) 
+                    {
+                        turnOnInterestOps(key_, SelectionKey.OP_WRITE);
+                    }
+                }
+                resumeStreaming();
+            } 
+            else 
+            {  
+                logger_.error("Closing connection because socket channel could not finishConnect.");;
+                errorClose();
+            }
+        } 
+        catch(IOException e) 
+        {               
+            logger_.error("Encountered IOException on connection: "  + socketChannel_, e);
+            errorClose();
+        }
+    }
+    
+    // called in the selector thread
+    public void write(SelectionKey key)
+    {   
+        turnOffInterestOps(key, SelectionKey.OP_WRITE);                
+        doPendingWrites();
+        /*
+         * This is executed only if we are in streaming mode.
+         * Idea is that we read a chunk of data from a source
+         * and wait to read the next from the source until we 
+         * are siganlled to do so from here. 
+        */
+         resumeStreaming();        
+    }
+    
+    void doPendingWrites()
+    {
+        synchronized(this)
+        {
+            try
+            {                     
+                while(!pendingWrites_.isEmpty()) 
+                {
+                    ByteBuffer buffer = pendingWrites_.peek();
+                    socketChannel_.write(buffer);                    
+                    if (buffer.remaining() > 0) 
+                    {   
+                        break;
+                    }               
+                    pendingWrites_.remove();
+                } 
+            
+            }
+            catch(IOException ex)
+            {
+                logger_.error(LogUtil.throwableToString(ex));
+                // This is to fix the wierd Linux bug with NIO.
+                errorClose();
+            }
+            finally
+            {    
+                if (!pendingWrites_.isEmpty())
+                {                    
+                    turnOnInterestOps(key_, SelectionKey.OP_WRITE);
+                }
+            }
+        }
+    }
+    
+    // called in the selector thread
+    public void read(SelectionKey key)
+    {
+        turnOffInterestOps(key, SelectionKey.OP_READ);
+        // publish this event onto to the TCPReadEvent Queue.
+        MessagingService.getReadExecutor().execute(readWork_);
+    }
+    
+    class ReadWorkItem implements Runnable
+    {                 
+        // called from the TCP READ thread pool
+        public void run()
+        {                         
+            if ( tcpReader_ == null )
+            {
+                tcpReader_ = new TcpReader(TcpConnection.this);    
+                StartState nextState = tcpReader_.getSocketState(TcpReader.TcpReaderState.PREAMBLE);
+                if ( nextState == null )
+                {
+                    nextState = new ProtocolState(tcpReader_);
+                    tcpReader_.putSocketState(TcpReader.TcpReaderState.PREAMBLE, nextState);
+                }
+                tcpReader_.morphState(nextState);
+            }
+            
+            try
+            {           
+                byte[] bytes = new byte[0];
+                while ( (bytes = tcpReader_.read()).length > 0 )
+                {                       
+                    ProtocolHeader pH = tcpReader_.getProtocolHeader();                    
+                    if ( !pH.isStreamingMode_ )
+                    {
+                        /* first message received */
+                        if (remoteEp_ == null)
+                        {             
+                            int port = ( pH.isListening_ ) ? DatabaseDescriptor.getStoragePort() : EndPoint.sentinelPort_;
+                            remoteEp_ = new EndPoint( socketChannel_.socket().getInetAddress().getHostAddress(), port );                            
+                            // put connection into pool if possible
+                            pool_ = MessagingService.getConnectionPool(localEp_, remoteEp_);                            
+                            pool_.addToPool(TcpConnection.this);                            
+                        }
+                        
+                        /* Deserialize and handle the message */
+                        MessagingService.getDeserilizationExecutor().submit( new MessageDeserializationTask(pH.serializerType_, bytes) );                                                  
+                        tcpReader_.resetState();
+                    }
+                    else
+                    {
+                        MessagingService.setStreamingMode(false);
+                        /* Close this socket connection  used for streaming */
+                        closeSocket();
+                    }                    
+                }
+            }
+            catch ( IOException ex )
+            {                   
+                handleException(ex);
+            }
+            catch ( Throwable th )
+            {
+                handleException(th);
+            }
+            finally
+            {
+                turnOnInterestOps(key_, SelectionKey.OP_READ);
+            }
+        }
+        
+        private void handleException(Throwable th)
+        {
+            logger_.warn("Problem reading from socket connected to : " + socketChannel_);
+            logger_.warn(LogUtil.throwableToString(th));
+            // This is to fix the weird Linux bug with NIO.
+            errorClose();
+        }
+    }
+    
+    public int pending()
+    {
+        return pendingWrites_.size();
+    }
+    
+    public int compareTo(Object o)
+    {
+        if (o instanceof TcpConnection) 
+        {
+            return pendingWrites_.size() - ((TcpConnection) o).pendingWrites_.size();            
+        }
+                    
+        throw new IllegalArgumentException();
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/TcpConnectionHandler.java b/src/java/org/apache/cassandra/net/TcpConnectionHandler.java
index f7e22586e1..cd065eb663 100644
--- a/src/java/org/apache/cassandra/net/TcpConnectionHandler.java
+++ b/src/java/org/apache/cassandra/net/TcpConnectionHandler.java
@@ -1,60 +1,60 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.nio.channels.*;
-import java.io.IOException;
-import java.net.*;
-
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class TcpConnectionHandler extends SelectionKeyHandler
-{
-    private static Logger logger_ = Logger.getLogger(TcpConnectionHandler.class);
-    EndPoint localEp_;    
-    
-    public TcpConnectionHandler(EndPoint localEp) 
-    {
-        localEp_ = localEp;
-    }
-
-    public void accept(SelectionKey key)
-    {
-        try 
-        {            
-            ServerSocketChannel serverChannel = (ServerSocketChannel)key.channel();
-            SocketChannel client = serverChannel.accept();
-            
-            if ( client != null )
-            {                        
-                //new TcpConnection(client, localEp_, true);
-                TcpConnection.acceptConnection(client, localEp_, true);                
-            }            
-        } 
-        catch(IOException e) 
-        {
-            logger_.warn(LogUtil.throwableToString(e));
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.nio.channels.*;
+import java.io.IOException;
+import java.net.*;
+
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TcpConnectionHandler extends SelectionKeyHandler
+{
+    private static Logger logger_ = Logger.getLogger(TcpConnectionHandler.class);
+    EndPoint localEp_;    
+    
+    public TcpConnectionHandler(EndPoint localEp) 
+    {
+        localEp_ = localEp;
+    }
+
+    public void accept(SelectionKey key)
+    {
+        try 
+        {            
+            ServerSocketChannel serverChannel = (ServerSocketChannel)key.channel();
+            SocketChannel client = serverChannel.accept();
+            
+            if ( client != null )
+            {                        
+                //new TcpConnection(client, localEp_, true);
+                TcpConnection.acceptConnection(client, localEp_, true);                
+            }            
+        } 
+        catch(IOException e) 
+        {
+            logger_.warn(LogUtil.throwableToString(e));
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/TcpConnectionManager.java b/src/java/org/apache/cassandra/net/TcpConnectionManager.java
index 2ec2a9a8ce..8b5f5d7b95 100644
--- a/src/java/org/apache/cassandra/net/TcpConnectionManager.java
+++ b/src/java/org/apache/cassandra/net/TcpConnectionManager.java
@@ -1,217 +1,217 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.io.IOException;
-import java.util.*;
-import java.util.concurrent.*;
-import java.util.concurrent.locks.*;
-
-import org.apache.log4j.Logger;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class TcpConnectionManager
-{
-    private Lock lock_ = new ReentrantLock();
-    private List<TcpConnection> allConnections_;
-    private EndPoint localEp_;
-    private EndPoint remoteEp_;
-    private int initialSize_;
-    private int growthFactor_;
-    private int maxSize_;
-    private long lastTimeUsed_;
-    private boolean isShut_;
-    
-    private int inUse_;
-
-    TcpConnectionManager(int initialSize, int growthFactor, int maxSize, EndPoint localEp, EndPoint remoteEp)
-    {
-        initialSize_ = initialSize;
-        growthFactor_ = growthFactor;
-        maxSize_ = maxSize;
-        localEp_ = localEp;
-        remoteEp_ = remoteEp;     
-        isShut_ = false;                
-        lastTimeUsed_ = System.currentTimeMillis();        
-        allConnections_ = new Vector<TcpConnection>(); 
-    }
-    
-    TcpConnection getConnection() throws IOException
-    {
-        lock_.lock();
-        try
-        {
-            if (allConnections_.isEmpty()) 
-            {                
-                TcpConnection conn = new TcpConnection(this, localEp_, remoteEp_);
-                addToPool(conn);
-                conn.inUse_ = true;
-                incUsed();
-                return conn;
-            }
-            
-            TcpConnection least = getLeastLoaded();
-            
-            if ( (least != null && least.pending() == 0) || allConnections_.size() == maxSize_) {
-                least.inUse_ = true;
-                incUsed();
-                return least;
-            }
-                                    
-            TcpConnection connection = new TcpConnection(this, localEp_, remoteEp_);
-            if ( connection != null && !contains(connection) )
-            {
-                addToPool(connection);
-                connection.inUse_ = true;
-                incUsed();
-                return connection;
-            }
-            else
-            {
-                if ( connection != null )
-                {                
-                    connection.closeSocket();
-                }
-                return getLeastLoaded();
-            }
-        }
-        finally
-        {
-            lock_.unlock();
-        }
-    }
-    
-    protected TcpConnection getLeastLoaded() 
-    {  
-        TcpConnection connection = null;
-        lock_.lock();
-        try
-        {
-            Collections.sort(allConnections_);
-            connection = (allConnections_.size() > 0 ) ? allConnections_.get(0) : null;
-        }
-        finally
-        {
-            lock_.unlock();
-        }
-        return connection;
-    }
-    
-    void removeConnection(TcpConnection connection)
-    {
-        allConnections_.remove(connection);        
-    }
-    
-    void incUsed()
-    {
-        inUse_++;
-    }
-    
-    void decUsed()
-    {        
-        inUse_--;
-    }
-    
-    int getConnectionsInUse()
-    {
-        return inUse_;
-    }
-
-    void addToPool(TcpConnection connection)
-    { 
-        
-        if ( contains(connection) )
-            return;
-        
-        lock_.lock();
-        try
-        {
-            if ( allConnections_.size() < maxSize_ )
-            {                 
-                allConnections_.add(connection);                
-            }
-            else
-            {                
-                connection.closeSocket();
-            }
-        }
-        finally
-        {
-            lock_.unlock();
-        }
-    }
-    
-    void shutdown()
-    {    
-        lock_.lock();
-        try
-        {
-            while ( allConnections_.size() > 0 )
-            {
-                TcpConnection connection = allConnections_.remove(0);                        
-                connection.closeSocket();
-            }
-        }
-        finally
-        {
-            lock_.unlock();
-        }
-        isShut_ = true;
-    }
-
-    int getPoolSize()
-    {
-        return allConnections_.size();
-    }
-
-    EndPoint getLocalEndPoint()
-    {
-        return localEp_;
-    }
-    
-    EndPoint getRemoteEndPoint()
-    {
-        return remoteEp_;
-    }
-    
-    int getPendingWrites()
-    {
-        int total = 0;
-        lock_.lock();
-        try
-        {
-            for ( TcpConnection connection : allConnections_ )
-            {
-                total += connection.pending();
-            }
-        }
-        finally
-        {
-            lock_.unlock();
-        }
-        return total;
-    }
-    
-    boolean contains(TcpConnection connection)
-    {
-        return allConnections_.contains(connection);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.*;
+import java.util.concurrent.locks.*;
+
+import org.apache.log4j.Logger;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class TcpConnectionManager
+{
+    private Lock lock_ = new ReentrantLock();
+    private List<TcpConnection> allConnections_;
+    private EndPoint localEp_;
+    private EndPoint remoteEp_;
+    private int initialSize_;
+    private int growthFactor_;
+    private int maxSize_;
+    private long lastTimeUsed_;
+    private boolean isShut_;
+    
+    private int inUse_;
+
+    TcpConnectionManager(int initialSize, int growthFactor, int maxSize, EndPoint localEp, EndPoint remoteEp)
+    {
+        initialSize_ = initialSize;
+        growthFactor_ = growthFactor;
+        maxSize_ = maxSize;
+        localEp_ = localEp;
+        remoteEp_ = remoteEp;     
+        isShut_ = false;                
+        lastTimeUsed_ = System.currentTimeMillis();        
+        allConnections_ = new Vector<TcpConnection>(); 
+    }
+    
+    TcpConnection getConnection() throws IOException
+    {
+        lock_.lock();
+        try
+        {
+            if (allConnections_.isEmpty()) 
+            {                
+                TcpConnection conn = new TcpConnection(this, localEp_, remoteEp_);
+                addToPool(conn);
+                conn.inUse_ = true;
+                incUsed();
+                return conn;
+            }
+            
+            TcpConnection least = getLeastLoaded();
+            
+            if ( (least != null && least.pending() == 0) || allConnections_.size() == maxSize_) {
+                least.inUse_ = true;
+                incUsed();
+                return least;
+            }
+                                    
+            TcpConnection connection = new TcpConnection(this, localEp_, remoteEp_);
+            if ( connection != null && !contains(connection) )
+            {
+                addToPool(connection);
+                connection.inUse_ = true;
+                incUsed();
+                return connection;
+            }
+            else
+            {
+                if ( connection != null )
+                {                
+                    connection.closeSocket();
+                }
+                return getLeastLoaded();
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+    }
+    
+    protected TcpConnection getLeastLoaded() 
+    {  
+        TcpConnection connection = null;
+        lock_.lock();
+        try
+        {
+            Collections.sort(allConnections_);
+            connection = (allConnections_.size() > 0 ) ? allConnections_.get(0) : null;
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+        return connection;
+    }
+    
+    void removeConnection(TcpConnection connection)
+    {
+        allConnections_.remove(connection);        
+    }
+    
+    void incUsed()
+    {
+        inUse_++;
+    }
+    
+    void decUsed()
+    {        
+        inUse_--;
+    }
+    
+    int getConnectionsInUse()
+    {
+        return inUse_;
+    }
+
+    void addToPool(TcpConnection connection)
+    { 
+        
+        if ( contains(connection) )
+            return;
+        
+        lock_.lock();
+        try
+        {
+            if ( allConnections_.size() < maxSize_ )
+            {                 
+                allConnections_.add(connection);                
+            }
+            else
+            {                
+                connection.closeSocket();
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+    }
+    
+    void shutdown()
+    {    
+        lock_.lock();
+        try
+        {
+            while ( allConnections_.size() > 0 )
+            {
+                TcpConnection connection = allConnections_.remove(0);                        
+                connection.closeSocket();
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+        isShut_ = true;
+    }
+
+    int getPoolSize()
+    {
+        return allConnections_.size();
+    }
+
+    EndPoint getLocalEndPoint()
+    {
+        return localEp_;
+    }
+    
+    EndPoint getRemoteEndPoint()
+    {
+        return remoteEp_;
+    }
+    
+    int getPendingWrites()
+    {
+        int total = 0;
+        lock_.lock();
+        try
+        {
+            for ( TcpConnection connection : allConnections_ )
+            {
+                total += connection.pending();
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+        return total;
+    }
+    
+    boolean contains(TcpConnection connection)
+    {
+        return allConnections_.contains(connection);
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/UdpConnection.java b/src/java/org/apache/cassandra/net/UdpConnection.java
index 11a9ab4535..40cdc00252 100644
--- a/src/java/org/apache/cassandra/net/UdpConnection.java
+++ b/src/java/org/apache/cassandra/net/UdpConnection.java
@@ -1,168 +1,168 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net;
-
-import java.net.SocketAddress;
-import java.nio.*;
-import java.nio.channels.*;
-import java.util.*;
-import java.util.concurrent.*;
-import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-
-import org.apache.cassandra.net.io.ProtocolState;
-import org.apache.cassandra.net.sink.SinkManager;
-import org.apache.cassandra.utils.BasicUtilities;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-import org.apache.cassandra.concurrent.*;
-import org.apache.cassandra.utils.*;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class UdpConnection extends SelectionKeyHandler
-{
-    private static Logger logger_ = Logger.getLogger(UdpConnection.class);
-    private static final int BUFFER_SIZE = 4096;
-    private static final int protocol_ = 0xBADBEEF;
-    
-    private DatagramChannel socketChannel_;
-    private SelectionKey key_;    
-    private EndPoint localEndPoint_;
-    
-    public void init() throws IOException
-    {
-        socketChannel_ = DatagramChannel.open();
-        socketChannel_.socket().setReuseAddress(true);
-        socketChannel_.configureBlocking(false);        
-    }
-    
-    public void init(int port) throws IOException
-    {
-        // TODO: get TCP port from config and add one.
-        localEndPoint_ = new EndPoint(port);
-        socketChannel_ = DatagramChannel.open();
-        socketChannel_.socket().bind(localEndPoint_.getInetAddress());
-        socketChannel_.socket().setReuseAddress(true);
-        socketChannel_.configureBlocking(false);        
-        key_ = SelectorManager.getUdpSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
-    }
-    
-    public boolean write(Message message, EndPoint to) throws IOException
-    {
-        boolean bVal = true;                       
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream(bos);
-        Message.serializer().serialize(message, dos);        
-        byte[] data = bos.toByteArray();
-        if ( data.length > 0 )
-        {  
-            if (logger_.isTraceEnabled())
-                logger_.trace("Size of Gossip packet " + data.length);
-            byte[] protocol = BasicUtilities.intToByteArray(protocol_);
-            ByteBuffer buffer = ByteBuffer.allocate(data.length + protocol.length);
-            buffer.put( protocol );
-            buffer.put(data);
-            buffer.flip();
-            
-            int n  = socketChannel_.send(buffer, to.getInetAddress());
-            if ( n == 0 )
-            {
-                bVal = false;
-            }
-        }
-        return bVal;
-    }
-    
-    void close()
-    {
-        try
-        {
-            if ( socketChannel_ != null )
-                socketChannel_.close();
-        }
-        catch ( IOException ex )
-        {
-            logger_.error( LogUtil.throwableToString(ex) );
-        }
-    }
-    
-    public DatagramChannel getDatagramChannel()
-    {
-        return socketChannel_;
-    }
-    
-    private byte[] gobbleHeaderAndExtractBody(ByteBuffer buffer)
-    {
-        byte[] body = new byte[0];        
-        byte[] protocol = new byte[4];
-        buffer = buffer.get(protocol, 0, protocol.length);
-        int value = BasicUtilities.byteArrayToInt(protocol);
-        
-        if ( protocol_ != value )
-        {
-            logger_.info("Invalid protocol header in the incoming message " + value);
-            return body;
-        }
-        body = new byte[buffer.remaining()];
-        buffer.get(body, 0, body.length);       
-        return body;
-    }
-    
-    public void read(SelectionKey key)
-    {        
-        turnOffInterestOps(key, SelectionKey.OP_READ);
-        ByteBuffer buffer = ByteBuffer.allocate(BUFFER_SIZE);
-        try
-        {
-            SocketAddress sa = socketChannel_.receive(buffer);
-            if ( sa == null )
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug("*** No datagram packet was available to be read ***");
-                return;
-            }            
-            buffer.flip();
-            
-            byte[] bytes = gobbleHeaderAndExtractBody(buffer);
-            if ( bytes.length > 0 )
-            {
-                DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
-                Message message = Message.serializer().deserialize(dis);                
-                if ( message != null )
-                {                                        
-                    MessagingService.receive(message);
-                }
-            }
-        }
-        catch ( IOException ioe )
-        {
-            logger_.warn(LogUtil.throwableToString(ioe));
-        }
-        finally
-        {
-            turnOnInterestOps(key_, SelectionKey.OP_READ );
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net;
+
+import java.net.SocketAddress;
+import java.nio.*;
+import java.nio.channels.*;
+import java.util.*;
+import java.util.concurrent.*;
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.net.io.ProtocolState;
+import org.apache.cassandra.net.sink.SinkManager;
+import org.apache.cassandra.utils.BasicUtilities;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.cassandra.concurrent.*;
+import org.apache.cassandra.utils.*;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class UdpConnection extends SelectionKeyHandler
+{
+    private static Logger logger_ = Logger.getLogger(UdpConnection.class);
+    private static final int BUFFER_SIZE = 4096;
+    private static final int protocol_ = 0xBADBEEF;
+    
+    private DatagramChannel socketChannel_;
+    private SelectionKey key_;    
+    private EndPoint localEndPoint_;
+    
+    public void init() throws IOException
+    {
+        socketChannel_ = DatagramChannel.open();
+        socketChannel_.socket().setReuseAddress(true);
+        socketChannel_.configureBlocking(false);        
+    }
+    
+    public void init(int port) throws IOException
+    {
+        // TODO: get TCP port from config and add one.
+        localEndPoint_ = new EndPoint(port);
+        socketChannel_ = DatagramChannel.open();
+        socketChannel_.socket().bind(localEndPoint_.getInetAddress());
+        socketChannel_.socket().setReuseAddress(true);
+        socketChannel_.configureBlocking(false);        
+        key_ = SelectorManager.getUdpSelectorManager().register(socketChannel_, this, SelectionKey.OP_READ);
+    }
+    
+    public boolean write(Message message, EndPoint to) throws IOException
+    {
+        boolean bVal = true;                       
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        Message.serializer().serialize(message, dos);        
+        byte[] data = bos.toByteArray();
+        if ( data.length > 0 )
+        {  
+            if (logger_.isTraceEnabled())
+                logger_.trace("Size of Gossip packet " + data.length);
+            byte[] protocol = BasicUtilities.intToByteArray(protocol_);
+            ByteBuffer buffer = ByteBuffer.allocate(data.length + protocol.length);
+            buffer.put( protocol );
+            buffer.put(data);
+            buffer.flip();
+            
+            int n  = socketChannel_.send(buffer, to.getInetAddress());
+            if ( n == 0 )
+            {
+                bVal = false;
+            }
+        }
+        return bVal;
+    }
+    
+    void close()
+    {
+        try
+        {
+            if ( socketChannel_ != null )
+                socketChannel_.close();
+        }
+        catch ( IOException ex )
+        {
+            logger_.error( LogUtil.throwableToString(ex) );
+        }
+    }
+    
+    public DatagramChannel getDatagramChannel()
+    {
+        return socketChannel_;
+    }
+    
+    private byte[] gobbleHeaderAndExtractBody(ByteBuffer buffer)
+    {
+        byte[] body = new byte[0];        
+        byte[] protocol = new byte[4];
+        buffer = buffer.get(protocol, 0, protocol.length);
+        int value = BasicUtilities.byteArrayToInt(protocol);
+        
+        if ( protocol_ != value )
+        {
+            logger_.info("Invalid protocol header in the incoming message " + value);
+            return body;
+        }
+        body = new byte[buffer.remaining()];
+        buffer.get(body, 0, body.length);       
+        return body;
+    }
+    
+    public void read(SelectionKey key)
+    {        
+        turnOffInterestOps(key, SelectionKey.OP_READ);
+        ByteBuffer buffer = ByteBuffer.allocate(BUFFER_SIZE);
+        try
+        {
+            SocketAddress sa = socketChannel_.receive(buffer);
+            if ( sa == null )
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug("*** No datagram packet was available to be read ***");
+                return;
+            }            
+            buffer.flip();
+            
+            byte[] bytes = gobbleHeaderAndExtractBody(buffer);
+            if ( bytes.length > 0 )
+            {
+                DataInputStream dis = new DataInputStream( new ByteArrayInputStream(bytes) );
+                Message message = Message.serializer().deserialize(dis);                
+                if ( message != null )
+                {                                        
+                    MessagingService.receive(message);
+                }
+            }
+        }
+        catch ( IOException ioe )
+        {
+            logger_.warn(LogUtil.throwableToString(ioe));
+        }
+        finally
+        {
+            turnOnInterestOps(key_, SelectionKey.OP_READ );
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/io/ContentLengthState.java b/src/java/org/apache/cassandra/net/io/ContentLengthState.java
index df13174357..93b2f7868f 100644
--- a/src/java/org/apache/cassandra/net/io/ContentLengthState.java
+++ b/src/java/org/apache/cassandra/net/io/ContentLengthState.java
@@ -1,67 +1,67 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-
-import java.nio.ByteBuffer;
-import java.nio.channels.SocketChannel;
-import java.io.IOException;
-
-import org.apache.cassandra.utils.FBUtilities;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class ContentLengthState extends StartState
-{
-    private ByteBuffer buffer_;
-
-    ContentLengthState(TcpReader stream)
-    {
-        super(stream);
-        buffer_ = ByteBuffer.allocate(4);
-    }
-
-    public byte[] read() throws IOException, ReadNotCompleteException
-    {        
-        return doRead(buffer_);
-    }
-
-    public void morphState() throws IOException
-    {
-        int size = FBUtilities.byteArrayToInt(buffer_.array());        
-        StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.CONTENT);
-        if ( nextState == null )
-        {
-            nextState = new ContentState(stream_, size);
-            stream_.putSocketState( TcpReader.TcpReaderState.CONTENT, nextState );
-        }
-        else
-        {               
-            nextState.setContextData(size);
-        }
-        stream_.morphState( nextState );
-        buffer_.clear();
-    }
-    
-    public void setContextData(Object data)
-    {
-        throw new UnsupportedOperationException("This method is not supported in the ContentLengthState");
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+
+import java.nio.ByteBuffer;
+import java.nio.channels.SocketChannel;
+import java.io.IOException;
+
+import org.apache.cassandra.utils.FBUtilities;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class ContentLengthState extends StartState
+{
+    private ByteBuffer buffer_;
+
+    ContentLengthState(TcpReader stream)
+    {
+        super(stream);
+        buffer_ = ByteBuffer.allocate(4);
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {        
+        return doRead(buffer_);
+    }
+
+    public void morphState() throws IOException
+    {
+        int size = FBUtilities.byteArrayToInt(buffer_.array());        
+        StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.CONTENT);
+        if ( nextState == null )
+        {
+            nextState = new ContentState(stream_, size);
+            stream_.putSocketState( TcpReader.TcpReaderState.CONTENT, nextState );
+        }
+        else
+        {               
+            nextState.setContextData(size);
+        }
+        stream_.morphState( nextState );
+        buffer_.clear();
+    }
+    
+    public void setContextData(Object data)
+    {
+        throw new UnsupportedOperationException("This method is not supported in the ContentLengthState");
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/io/ContentState.java b/src/java/org/apache/cassandra/net/io/ContentState.java
index dd457479c5..7c479824f5 100644
--- a/src/java/org/apache/cassandra/net/io/ContentState.java
+++ b/src/java/org/apache/cassandra/net/io/ContentState.java
@@ -1,84 +1,84 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import java.nio.ByteBuffer;
-import java.nio.channels.SocketChannel;
-import java.io.IOException;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class ContentState extends StartState
-{
-    private ByteBuffer buffer_;   
-    private int length_;
-
-    ContentState(TcpReader stream, int length)
-    {
-        super(stream);
-        length_ = length; 
-        buffer_ = ByteBuffer.allocate(length_);
-    }
-
-    public byte[] read() throws IOException, ReadNotCompleteException
-    {          
-        return doRead(buffer_);
-    }
-
-    public void morphState() throws IOException
-    {        
-        StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.DONE);
-        if ( nextState == null )
-        {
-            nextState = new DoneState(stream_, toBytes());
-            stream_.putSocketState( TcpReader.TcpReaderState.DONE, nextState );
-        }
-        else
-        {            
-            nextState.setContextData(toBytes());
-        }
-        stream_.morphState( nextState );               
-    }
-    
-    private byte[] toBytes()
-    {
-        buffer_.position(0); 
-        /*
-        ByteBuffer slice = buffer_.slice();        
-        return slice.array();
-        */  
-        byte[] bytes = new byte[length_];
-        buffer_.get(bytes, 0, length_);
-        return bytes;
-    }
-    
-    public void setContextData(Object data)
-    {
-        Integer value = (Integer)data;
-        length_ = value;               
-        buffer_.clear();
-        if ( buffer_.capacity() < length_ )
-            buffer_ = ByteBuffer.allocate(length_);
-        else
-        {            
-            buffer_.limit(length_);
-        }        
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.nio.ByteBuffer;
+import java.nio.channels.SocketChannel;
+import java.io.IOException;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class ContentState extends StartState
+{
+    private ByteBuffer buffer_;   
+    private int length_;
+
+    ContentState(TcpReader stream, int length)
+    {
+        super(stream);
+        length_ = length; 
+        buffer_ = ByteBuffer.allocate(length_);
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {          
+        return doRead(buffer_);
+    }
+
+    public void morphState() throws IOException
+    {        
+        StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.DONE);
+        if ( nextState == null )
+        {
+            nextState = new DoneState(stream_, toBytes());
+            stream_.putSocketState( TcpReader.TcpReaderState.DONE, nextState );
+        }
+        else
+        {            
+            nextState.setContextData(toBytes());
+        }
+        stream_.morphState( nextState );               
+    }
+    
+    private byte[] toBytes()
+    {
+        buffer_.position(0); 
+        /*
+        ByteBuffer slice = buffer_.slice();        
+        return slice.array();
+        */  
+        byte[] bytes = new byte[length_];
+        buffer_.get(bytes, 0, length_);
+        return bytes;
+    }
+    
+    public void setContextData(Object data)
+    {
+        Integer value = (Integer)data;
+        length_ = value;               
+        buffer_.clear();
+        if ( buffer_.capacity() < length_ )
+            buffer_ = ByteBuffer.allocate(length_);
+        else
+        {            
+            buffer_.limit(length_);
+        }        
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/io/ContentStreamState.java b/src/java/org/apache/cassandra/net/io/ContentStreamState.java
index 65ff1a23a6..adce2bdd95 100644
--- a/src/java/org/apache/cassandra/net/io/ContentStreamState.java
+++ b/src/java/org/apache/cassandra/net/io/ContentStreamState.java
@@ -1,138 +1,138 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import java.net.InetSocketAddress;
-import java.nio.ByteBuffer;
-import java.nio.channels.FileChannel;
-import java.nio.channels.SocketChannel;
-import java.io.*;
-
-import org.apache.cassandra.db.Table;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-
-class ContentStreamState extends StartState
-{       
-    private static Logger logger_ = Logger.getLogger(ContentStreamState.class);
-    private static long count_ = 64*1024*1024;
-    /* Return this byte array to exit event loop */
-    private static byte[] bytes_ = new byte[1];
-    private long bytesRead_ = 0L;
-    private FileChannel fc_;
-    private StreamContextManager.StreamContext streamContext_;
-    private StreamContextManager.StreamStatus streamStatus_;
-    
-    ContentStreamState(TcpReader stream)
-    {
-        super(stream); 
-        SocketChannel socketChannel = stream.getStream();
-        InetSocketAddress remoteAddress = (InetSocketAddress)socketChannel.socket().getRemoteSocketAddress();
-        String remoteHost = remoteAddress.getHostName();        
-        streamContext_ = StreamContextManager.getStreamContext(remoteHost);   
-        streamStatus_ = StreamContextManager.getStreamStatus(remoteHost);
-    }
-    
-    private void createFileChannel() throws IOException
-    {
-        if ( fc_ == null )
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Creating file for " + streamContext_.getTargetFile());
-            FileOutputStream fos = new FileOutputStream( streamContext_.getTargetFile(), true );
-            fc_ = fos.getChannel();            
-        }
-    }
-
-    public byte[] read() throws IOException, ReadNotCompleteException
-    {        
-        SocketChannel socketChannel = stream_.getStream();
-        InetSocketAddress remoteAddress = (InetSocketAddress)socketChannel.socket().getRemoteSocketAddress();
-        String remoteHost = remoteAddress.getHostName();  
-        createFileChannel();
-        if ( streamContext_ != null )
-        {  
-            try
-            {
-                bytesRead_ += fc_.transferFrom(socketChannel, bytesRead_, ContentStreamState.count_);
-                if ( bytesRead_ != streamContext_.getExpectedBytes() )
-                    throw new ReadNotCompleteException("Specified number of bytes have not been read from the Socket Channel");
-            }
-            catch ( IOException ex )
-            {
-                /* Ask the source node to re-stream this file. */
-                streamStatus_.setAction(StreamContextManager.StreamCompletionAction.STREAM);                
-                handleStreamCompletion(remoteHost);
-                /* Delete the orphaned file. */
-                File file = new File(streamContext_.getTargetFile());
-                file.delete();
-                throw ex;
-            }
-            if ( bytesRead_ == streamContext_.getExpectedBytes() )
-            {       
-                if (logger_.isDebugEnabled())
-                    logger_.debug("Removing stream context " + streamContext_);                 
-                handleStreamCompletion(remoteHost);                              
-                bytesRead_ = 0L;
-                fc_.close();
-                morphState();
-            }                            
-        }
-        
-        return new byte[0];
-    }
-    
-    private void handleStreamCompletion(String remoteHost) throws IOException
-    {
-        /* 
-         * Streaming is complete. If all the data that has to be received inform the sender via 
-         * the stream completion callback so that the source may perform the requisite cleanup. 
-        */
-        IStreamComplete streamComplete = StreamContextManager.getStreamCompletionHandler(remoteHost);
-        if ( streamComplete != null )
-        {                    
-            streamComplete.onStreamCompletion(remoteHost, streamContext_, streamStatus_);                    
-        }
-    }
-
-    public void morphState() throws IOException
-    {        
-        /* We instantiate an array of size 1 so that we can exit the event loop of the read. */                
-        StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.DONE);
-        if ( nextState == null )
-        {
-            nextState = new DoneState(stream_, ContentStreamState.bytes_);
-            stream_.putSocketState( TcpReader.TcpReaderState.DONE, nextState );
-        }
-        else
-        {
-            nextState.setContextData(ContentStreamState.bytes_);
-        }
-        stream_.morphState( nextState );  
-    }
-    
-    public void setContextData(Object data)
-    {
-        throw new UnsupportedOperationException("This method is not supported in the ContentStreamState");
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.net.InetSocketAddress;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.channels.SocketChannel;
+import java.io.*;
+
+import org.apache.cassandra.db.Table;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+
+class ContentStreamState extends StartState
+{       
+    private static Logger logger_ = Logger.getLogger(ContentStreamState.class);
+    private static long count_ = 64*1024*1024;
+    /* Return this byte array to exit event loop */
+    private static byte[] bytes_ = new byte[1];
+    private long bytesRead_ = 0L;
+    private FileChannel fc_;
+    private StreamContextManager.StreamContext streamContext_;
+    private StreamContextManager.StreamStatus streamStatus_;
+    
+    ContentStreamState(TcpReader stream)
+    {
+        super(stream); 
+        SocketChannel socketChannel = stream.getStream();
+        InetSocketAddress remoteAddress = (InetSocketAddress)socketChannel.socket().getRemoteSocketAddress();
+        String remoteHost = remoteAddress.getHostName();        
+        streamContext_ = StreamContextManager.getStreamContext(remoteHost);   
+        streamStatus_ = StreamContextManager.getStreamStatus(remoteHost);
+    }
+    
+    private void createFileChannel() throws IOException
+    {
+        if ( fc_ == null )
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Creating file for " + streamContext_.getTargetFile());
+            FileOutputStream fos = new FileOutputStream( streamContext_.getTargetFile(), true );
+            fc_ = fos.getChannel();            
+        }
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {        
+        SocketChannel socketChannel = stream_.getStream();
+        InetSocketAddress remoteAddress = (InetSocketAddress)socketChannel.socket().getRemoteSocketAddress();
+        String remoteHost = remoteAddress.getHostName();  
+        createFileChannel();
+        if ( streamContext_ != null )
+        {  
+            try
+            {
+                bytesRead_ += fc_.transferFrom(socketChannel, bytesRead_, ContentStreamState.count_);
+                if ( bytesRead_ != streamContext_.getExpectedBytes() )
+                    throw new ReadNotCompleteException("Specified number of bytes have not been read from the Socket Channel");
+            }
+            catch ( IOException ex )
+            {
+                /* Ask the source node to re-stream this file. */
+                streamStatus_.setAction(StreamContextManager.StreamCompletionAction.STREAM);                
+                handleStreamCompletion(remoteHost);
+                /* Delete the orphaned file. */
+                File file = new File(streamContext_.getTargetFile());
+                file.delete();
+                throw ex;
+            }
+            if ( bytesRead_ == streamContext_.getExpectedBytes() )
+            {       
+                if (logger_.isDebugEnabled())
+                    logger_.debug("Removing stream context " + streamContext_);                 
+                handleStreamCompletion(remoteHost);                              
+                bytesRead_ = 0L;
+                fc_.close();
+                morphState();
+            }                            
+        }
+        
+        return new byte[0];
+    }
+    
+    private void handleStreamCompletion(String remoteHost) throws IOException
+    {
+        /* 
+         * Streaming is complete. If all the data that has to be received inform the sender via 
+         * the stream completion callback so that the source may perform the requisite cleanup. 
+        */
+        IStreamComplete streamComplete = StreamContextManager.getStreamCompletionHandler(remoteHost);
+        if ( streamComplete != null )
+        {                    
+            streamComplete.onStreamCompletion(remoteHost, streamContext_, streamStatus_);                    
+        }
+    }
+
+    public void morphState() throws IOException
+    {        
+        /* We instantiate an array of size 1 so that we can exit the event loop of the read. */                
+        StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.DONE);
+        if ( nextState == null )
+        {
+            nextState = new DoneState(stream_, ContentStreamState.bytes_);
+            stream_.putSocketState( TcpReader.TcpReaderState.DONE, nextState );
+        }
+        else
+        {
+            nextState.setContextData(ContentStreamState.bytes_);
+        }
+        stream_.morphState( nextState );  
+    }
+    
+    public void setContextData(Object data)
+    {
+        throw new UnsupportedOperationException("This method is not supported in the ContentStreamState");
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/io/DoneState.java b/src/java/org/apache/cassandra/net/io/DoneState.java
index 117b3d4735..c03587f8f8 100644
--- a/src/java/org/apache/cassandra/net/io/DoneState.java
+++ b/src/java/org/apache/cassandra/net/io/DoneState.java
@@ -1,52 +1,52 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import java.nio.channels.SocketChannel;
-import java.io.IOException;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-class DoneState extends StartState
-{
-    private byte[] bytes_ = new byte[0];
-
-    DoneState(TcpReader stream, byte[] bytes)
-    {
-        super(stream);
-        bytes_ = bytes;
-    }
-
-    public byte[] read() throws IOException, ReadNotCompleteException
-    {        
-        morphState();
-        return bytes_;
-    }
-
-    public void morphState() throws IOException
-    {                       
-        stream_.morphState(null);
-    }
-    
-    public void setContextData(Object data)
-    {                
-        bytes_ = (byte[])data;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.nio.channels.SocketChannel;
+import java.io.IOException;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+class DoneState extends StartState
+{
+    private byte[] bytes_ = new byte[0];
+
+    DoneState(TcpReader stream, byte[] bytes)
+    {
+        super(stream);
+        bytes_ = bytes;
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {        
+        morphState();
+        return bytes_;
+    }
+
+    public void morphState() throws IOException
+    {                       
+        stream_.morphState(null);
+    }
+    
+    public void setContextData(Object data)
+    {                
+        bytes_ = (byte[])data;
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/io/FastSerializer.java b/src/java/org/apache/cassandra/net/io/FastSerializer.java
index b4ccb206d1..43b14b56b5 100644
--- a/src/java/org/apache/cassandra/net/io/FastSerializer.java
+++ b/src/java/org/apache/cassandra/net/io/FastSerializer.java
@@ -1,46 +1,46 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import java.io.IOException;
-
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.io.DataOutputBuffer;
-import org.apache.cassandra.net.Message;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class FastSerializer implements ISerializer
-{ 
-    public byte[] serialize(Message message) throws IOException
-    {
-        DataOutputBuffer buffer = new DataOutputBuffer();
-        Message.serializer().serialize(message, buffer);
-        return buffer.getData();
-    }
-    
-    public Message deserialize(byte[] bytes) throws IOException
-    {
-        DataInputBuffer bufIn = new DataInputBuffer();
-        bufIn.reset(bytes, bytes.length);
-        return Message.serializer().deserialize(bufIn);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.io.IOException;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.net.Message;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class FastSerializer implements ISerializer
+{ 
+    public byte[] serialize(Message message) throws IOException
+    {
+        DataOutputBuffer buffer = new DataOutputBuffer();
+        Message.serializer().serialize(message, buffer);
+        return buffer.getData();
+    }
+    
+    public Message deserialize(byte[] bytes) throws IOException
+    {
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(bytes, bytes.length);
+        return Message.serializer().deserialize(bufIn);
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/io/ISerializer.java b/src/java/org/apache/cassandra/net/io/ISerializer.java
index 47320b0070..c25843a599 100644
--- a/src/java/org/apache/cassandra/net/io/ISerializer.java
+++ b/src/java/org/apache/cassandra/net/io/ISerializer.java
@@ -1,32 +1,32 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import java.io.IOException;
-
-import org.apache.cassandra.net.Message;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface ISerializer
-{
-    public byte[] serialize(Message message) throws IOException;
-    public Message deserialize(byte[] bytes) throws IOException;
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.io.IOException;
+
+import org.apache.cassandra.net.Message;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface ISerializer
+{
+    public byte[] serialize(Message message) throws IOException;
+    public Message deserialize(byte[] bytes) throws IOException;
+}
diff --git a/src/java/org/apache/cassandra/net/io/IStreamComplete.java b/src/java/org/apache/cassandra/net/io/IStreamComplete.java
index 103bbb714e..7fcbdd1677 100644
--- a/src/java/org/apache/cassandra/net/io/IStreamComplete.java
+++ b/src/java/org/apache/cassandra/net/io/IStreamComplete.java
@@ -1,36 +1,36 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import java.io.IOException;
-
-import org.apache.cassandra.net.EndPoint;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IStreamComplete
-{
-    /*
-     * This callback if registered with the StreamContextManager is 
-     * called when the stream from a host is completely handled. 
-    */
-    public void onStreamCompletion(String from, StreamContextManager.StreamContext streamContext, StreamContextManager.StreamStatus streamStatus) throws IOException;
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.io.IOException;
+
+import org.apache.cassandra.net.EndPoint;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IStreamComplete
+{
+    /*
+     * This callback if registered with the StreamContextManager is 
+     * called when the stream from a host is completely handled. 
+    */
+    public void onStreamCompletion(String from, StreamContextManager.StreamContext streamContext, StreamContextManager.StreamStatus streamStatus) throws IOException;
+}
diff --git a/src/java/org/apache/cassandra/net/io/ProtocolHeaderState.java b/src/java/org/apache/cassandra/net/io/ProtocolHeaderState.java
index 733134590a..87aafd6b81 100644
--- a/src/java/org/apache/cassandra/net/io/ProtocolHeaderState.java
+++ b/src/java/org/apache/cassandra/net/io/ProtocolHeaderState.java
@@ -1,103 +1,103 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import org.apache.cassandra.utils.*;
-import java.nio.ByteBuffer;
-import java.nio.channels.SocketChannel;
-import java.io.IOException;
-import org.apache.cassandra.net.MessagingService;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class ProtocolHeaderState extends StartState
-{
-    private ByteBuffer buffer_;
-
-    public ProtocolHeaderState(TcpReader stream)
-    {
-        super(stream);
-        buffer_ = ByteBuffer.allocate(4);
-    }
-
-    public byte[] read() throws IOException, ReadNotCompleteException
-    {        
-        return doRead(buffer_);
-    }
-
-    public void morphState() throws IOException
-    {
-        byte[] protocolHeader = buffer_.array();
-        int pH = MessagingService.byteArrayToInt(protocolHeader);
-        
-        int type = MessagingService.getBits(pH, 1, 2);
-        stream_.getProtocolHeader().serializerType_ = type;
-        
-        int stream = MessagingService.getBits(pH, 3, 1);
-        stream_.getProtocolHeader().isStreamingMode_ = (stream == 1) ? true : false;
-        
-        if ( stream_.getProtocolHeader().isStreamingMode_ )
-            MessagingService.setStreamingMode(true);
-        
-        int listening = MessagingService.getBits(pH, 4, 1);
-        stream_.getProtocolHeader().isListening_ = (listening == 1) ? true : false;
-        
-        int version = MessagingService.getBits(pH, 15, 8);
-        stream_.getProtocolHeader().version_ = version;
-        
-        if ( version <= MessagingService.getVersion() )
-        {
-            if ( stream_.getProtocolHeader().isStreamingMode_ )
-            { 
-                StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.CONTENT_STREAM);
-                if ( nextState == null )
-                {
-                    nextState = new ContentStreamState(stream_);
-                    stream_.putSocketState( TcpReader.TcpReaderState.CONTENT_STREAM, nextState );
-                }
-                stream_.morphState( nextState );
-                buffer_.clear();
-            }
-            else
-            {
-                StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.CONTENT_LENGTH);
-                if ( nextState == null )
-                {
-                    nextState = new ContentLengthState(stream_);
-                    stream_.putSocketState( TcpReader.TcpReaderState.CONTENT_LENGTH, nextState );
-                }                
-                stream_.morphState( nextState );   
-                buffer_.clear();
-            }            
-        }
-        else
-        {
-            throw new IOException("Invalid version in message. Scram.");
-        }
-    }
-    
-    public void setContextData(Object data)
-    {
-        throw new UnsupportedOperationException("This method is not supported in the ProtocolHeaderState");
-    }
-}
-
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import org.apache.cassandra.utils.*;
+import java.nio.ByteBuffer;
+import java.nio.channels.SocketChannel;
+import java.io.IOException;
+import org.apache.cassandra.net.MessagingService;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ProtocolHeaderState extends StartState
+{
+    private ByteBuffer buffer_;
+
+    public ProtocolHeaderState(TcpReader stream)
+    {
+        super(stream);
+        buffer_ = ByteBuffer.allocate(4);
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {        
+        return doRead(buffer_);
+    }
+
+    public void morphState() throws IOException
+    {
+        byte[] protocolHeader = buffer_.array();
+        int pH = MessagingService.byteArrayToInt(protocolHeader);
+        
+        int type = MessagingService.getBits(pH, 1, 2);
+        stream_.getProtocolHeader().serializerType_ = type;
+        
+        int stream = MessagingService.getBits(pH, 3, 1);
+        stream_.getProtocolHeader().isStreamingMode_ = (stream == 1) ? true : false;
+        
+        if ( stream_.getProtocolHeader().isStreamingMode_ )
+            MessagingService.setStreamingMode(true);
+        
+        int listening = MessagingService.getBits(pH, 4, 1);
+        stream_.getProtocolHeader().isListening_ = (listening == 1) ? true : false;
+        
+        int version = MessagingService.getBits(pH, 15, 8);
+        stream_.getProtocolHeader().version_ = version;
+        
+        if ( version <= MessagingService.getVersion() )
+        {
+            if ( stream_.getProtocolHeader().isStreamingMode_ )
+            { 
+                StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.CONTENT_STREAM);
+                if ( nextState == null )
+                {
+                    nextState = new ContentStreamState(stream_);
+                    stream_.putSocketState( TcpReader.TcpReaderState.CONTENT_STREAM, nextState );
+                }
+                stream_.morphState( nextState );
+                buffer_.clear();
+            }
+            else
+            {
+                StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.CONTENT_LENGTH);
+                if ( nextState == null )
+                {
+                    nextState = new ContentLengthState(stream_);
+                    stream_.putSocketState( TcpReader.TcpReaderState.CONTENT_LENGTH, nextState );
+                }                
+                stream_.morphState( nextState );   
+                buffer_.clear();
+            }            
+        }
+        else
+        {
+            throw new IOException("Invalid version in message. Scram.");
+        }
+    }
+    
+    public void setContextData(Object data)
+    {
+        throw new UnsupportedOperationException("This method is not supported in the ProtocolHeaderState");
+    }
+}
+
+
diff --git a/src/java/org/apache/cassandra/net/io/ProtocolState.java b/src/java/org/apache/cassandra/net/io/ProtocolState.java
index d47e4febb1..1e4ba09dfe 100644
--- a/src/java/org/apache/cassandra/net/io/ProtocolState.java
+++ b/src/java/org/apache/cassandra/net/io/ProtocolState.java
@@ -1,71 +1,71 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import org.apache.cassandra.utils.*;
-import java.nio.ByteBuffer;
-import java.nio.channels.SocketChannel;
-import java.io.IOException;
-import org.apache.cassandra.net.MessagingService;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class ProtocolState extends StartState
-{
-    private ByteBuffer buffer_;
-
-    public ProtocolState(TcpReader stream)
-    {
-        super(stream);
-        buffer_ = ByteBuffer.allocate(16);
-    }
-
-    public byte[] read() throws IOException, ReadNotCompleteException
-    {        
-        return doRead(buffer_);
-    }
-
-    public void morphState() throws IOException
-    {
-        byte[] protocol = buffer_.array();
-        if ( MessagingService.isProtocolValid(protocol) )
-        {            
-            StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.PROTOCOL);
-            if ( nextState == null )
-            {
-                nextState = new ProtocolHeaderState(stream_);
-                stream_.putSocketState( TcpReader.TcpReaderState.PROTOCOL, nextState );
-            }
-            stream_.morphState( nextState ); 
-            buffer_.clear();
-        }
-        else
-        {
-            throw new IOException("Invalid protocol header. The preamble seems to be messed up.");
-        }
-    }
-    
-    public void setContextData(Object data)
-    {
-        throw new UnsupportedOperationException("This method is not supported in the ProtocolState");
-    }
-}
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import org.apache.cassandra.utils.*;
+import java.nio.ByteBuffer;
+import java.nio.channels.SocketChannel;
+import java.io.IOException;
+import org.apache.cassandra.net.MessagingService;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class ProtocolState extends StartState
+{
+    private ByteBuffer buffer_;
+
+    public ProtocolState(TcpReader stream)
+    {
+        super(stream);
+        buffer_ = ByteBuffer.allocate(16);
+    }
+
+    public byte[] read() throws IOException, ReadNotCompleteException
+    {        
+        return doRead(buffer_);
+    }
+
+    public void morphState() throws IOException
+    {
+        byte[] protocol = buffer_.array();
+        if ( MessagingService.isProtocolValid(protocol) )
+        {            
+            StartState nextState = stream_.getSocketState(TcpReader.TcpReaderState.PROTOCOL);
+            if ( nextState == null )
+            {
+                nextState = new ProtocolHeaderState(stream_);
+                stream_.putSocketState( TcpReader.TcpReaderState.PROTOCOL, nextState );
+            }
+            stream_.morphState( nextState ); 
+            buffer_.clear();
+        }
+        else
+        {
+            throw new IOException("Invalid protocol header. The preamble seems to be messed up.");
+        }
+    }
+    
+    public void setContextData(Object data)
+    {
+        throw new UnsupportedOperationException("This method is not supported in the ProtocolState");
+    }
+}
+
diff --git a/src/java/org/apache/cassandra/net/io/ReadNotCompleteException.java b/src/java/org/apache/cassandra/net/io/ReadNotCompleteException.java
index e7fab63418..865a63611f 100644
--- a/src/java/org/apache/cassandra/net/io/ReadNotCompleteException.java
+++ b/src/java/org/apache/cassandra/net/io/ReadNotCompleteException.java
@@ -1,34 +1,34 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-/**
- * Created by IntelliJ IDEA.
- * User: lakshman
- * Date: Aug 22, 2005
- * Time: 11:37:31 AM
- * To change this template use File | Settings | File Templates.
- */
-public class ReadNotCompleteException extends Exception
-{
-    ReadNotCompleteException(String message)
-    {
-        super(message);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+/**
+ * Created by IntelliJ IDEA.
+ * User: lakshman
+ * Date: Aug 22, 2005
+ * Time: 11:37:31 AM
+ * To change this template use File | Settings | File Templates.
+ */
+public class ReadNotCompleteException extends Exception
+{
+    ReadNotCompleteException(String message)
+    {
+        super(message);
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/io/SerializerAttribute.java b/src/java/org/apache/cassandra/net/io/SerializerAttribute.java
index 0bdc78cb9f..cb6e869fb1 100644
--- a/src/java/org/apache/cassandra/net/io/SerializerAttribute.java
+++ b/src/java/org/apache/cassandra/net/io/SerializerAttribute.java
@@ -1,27 +1,27 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import java.lang.annotation.*;
-
-@Retention(RetentionPolicy.RUNTIME)
-public @interface SerializerAttribute
-{
-    SerializerType value();
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.lang.annotation.*;
+
+@Retention(RetentionPolicy.RUNTIME)
+public @interface SerializerAttribute
+{
+    SerializerType value();
+}
diff --git a/src/java/org/apache/cassandra/net/io/SerializerType.java b/src/java/org/apache/cassandra/net/io/SerializerType.java
index 6713908120..6bb328fcf9 100644
--- a/src/java/org/apache/cassandra/net/io/SerializerType.java
+++ b/src/java/org/apache/cassandra/net/io/SerializerType.java
@@ -1,27 +1,27 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-public enum SerializerType
-{
-    BINARY,
-    JAVA,
-    XML,
-    JSON
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+public enum SerializerType
+{
+    BINARY,
+    JAVA,
+    XML,
+    JSON
+}
diff --git a/src/java/org/apache/cassandra/net/io/StartState.java b/src/java/org/apache/cassandra/net/io/StartState.java
index 917d68de1c..7ed725803a 100644
--- a/src/java/org/apache/cassandra/net/io/StartState.java
+++ b/src/java/org/apache/cassandra/net/io/StartState.java
@@ -1,59 +1,59 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import java.nio.channels.SocketChannel;
-import java.nio.ByteBuffer;
-import java.io.IOException;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public abstract class StartState
-{
-    protected TcpReader stream_;
-
-    public StartState(TcpReader stream)
-    {
-        stream_ = stream;
-    }
-
-    public abstract byte[] read() throws IOException, ReadNotCompleteException;
-    public abstract void morphState() throws IOException;
-    public abstract void setContextData(Object data);
-
-    protected byte[] doRead(ByteBuffer buffer) throws IOException, ReadNotCompleteException
-    {        
-        SocketChannel socketChannel = stream_.getStream();
-        int bytesRead = socketChannel.read(buffer);     
-        if ( bytesRead == -1 && buffer.remaining() > 0 )
-        {            
-            throw new IOException("Reached an EOL or something bizzare occured. Reading from: " + socketChannel.socket().getInetAddress() + " BufferSizeRemaining: " + buffer.remaining());
-        }
-        if ( buffer.remaining() == 0 )
-        {
-            morphState();
-        }
-        else
-        {            
-            throw new ReadNotCompleteException("Specified number of bytes have not been read from the Socket Channel");
-        }
-        return new byte[0];
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.nio.channels.SocketChannel;
+import java.nio.ByteBuffer;
+import java.io.IOException;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public abstract class StartState
+{
+    protected TcpReader stream_;
+
+    public StartState(TcpReader stream)
+    {
+        stream_ = stream;
+    }
+
+    public abstract byte[] read() throws IOException, ReadNotCompleteException;
+    public abstract void morphState() throws IOException;
+    public abstract void setContextData(Object data);
+
+    protected byte[] doRead(ByteBuffer buffer) throws IOException, ReadNotCompleteException
+    {        
+        SocketChannel socketChannel = stream_.getStream();
+        int bytesRead = socketChannel.read(buffer);     
+        if ( bytesRead == -1 && buffer.remaining() > 0 )
+        {            
+            throw new IOException("Reached an EOL or something bizzare occured. Reading from: " + socketChannel.socket().getInetAddress() + " BufferSizeRemaining: " + buffer.remaining());
+        }
+        if ( buffer.remaining() == 0 )
+        {
+            morphState();
+        }
+        else
+        {            
+            throw new ReadNotCompleteException("Specified number of bytes have not been read from the Socket Channel");
+        }
+        return new byte[0];
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/io/StreamContextManager.java b/src/java/org/apache/cassandra/net/io/StreamContextManager.java
index 1482470c7b..13606cbf08 100644
--- a/src/java/org/apache/cassandra/net/io/StreamContextManager.java
+++ b/src/java/org/apache/cassandra/net/io/StreamContextManager.java
@@ -1,328 +1,328 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.io.Serializable;
-import java.util.*;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.service.StorageService;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class StreamContextManager
-{
-    private static Logger logger_ = Logger.getLogger(StreamContextManager.class);
-    
-    public static enum StreamCompletionAction
-    {
-        DELETE,
-        STREAM
-    }
-    
-    public static class StreamContext implements Serializable
-    {
-        private static Logger logger_ = Logger.getLogger(StreamContextManager.StreamContext.class);
-        private static ICompactSerializer<StreamContext> serializer_;
-        
-        static
-        {
-            serializer_ = new StreamContextSerializer();
-        }
-        
-        public static ICompactSerializer<StreamContext> serializer()
-        {
-            return serializer_;
-        }
-                
-        private String targetFile_;        
-        private long expectedBytes_;                     
-        private String table_;
-        
-        public StreamContext(String targetFile, long expectedBytes, String table)
-        {
-            targetFile_ = targetFile;
-            expectedBytes_ = expectedBytes;         
-            table_ = table;
-        }
-
-        public String getTable()
-        {
-            return table_;
-        }                
-                
-        public String getTargetFile()
-        {
-            return targetFile_;
-        }
-        
-        public void setTargetFile(String file)
-        {
-            targetFile_ = file;
-        }
-        
-        public long getExpectedBytes()
-        {
-            return expectedBytes_;
-        }
-                
-        public boolean equals(Object o)
-        {
-            if ( !(o instanceof StreamContext) )
-                return false;
-            
-            StreamContext rhs = (StreamContext)o;
-            return targetFile_.equals(rhs.targetFile_);
-        }
-        
-        public int hashCode()
-        {
-            return toString().hashCode();
-        }
-        
-        public String toString()
-        {
-            return targetFile_ + ":" + expectedBytes_;
-        }
-    }
-    
-    public static class StreamContextSerializer implements ICompactSerializer<StreamContext>
-    {
-        public void serialize(StreamContextManager.StreamContext sc, DataOutputStream dos) throws IOException
-        {
-            dos.writeUTF(sc.targetFile_);
-            dos.writeLong(sc.expectedBytes_);            
-            dos.writeUTF(sc.table_);
-        }
-        
-        public StreamContextManager.StreamContext deserialize(DataInputStream dis) throws IOException
-        {
-            String targetFile = dis.readUTF();
-            long expectedBytes = dis.readLong();           
-            String table = dis.readUTF();
-            return new StreamContext(targetFile, expectedBytes, table);
-        }
-    }
-    
-    public static class StreamStatus
-    {
-        private static ICompactSerializer<StreamStatus> serializer_;
-        
-        static 
-        {
-            serializer_ = new StreamStatusSerializer();
-        }
-        
-        public static ICompactSerializer<StreamStatus> serializer()
-        {
-            return serializer_;
-        }
-            
-        private String file_;               
-        private long expectedBytes_;                
-        private StreamCompletionAction action_;
-                
-        public StreamStatus(String file, long expectedBytes)
-        {
-            file_ = file;
-            expectedBytes_ = expectedBytes;
-            action_ = StreamContextManager.StreamCompletionAction.DELETE;
-        }
-        
-        public String getFile()
-        {
-            return file_;
-        }
-        
-        public long getExpectedBytes()
-        {
-            return expectedBytes_;
-        }
-        
-        void setAction(StreamContextManager.StreamCompletionAction action)
-        {
-            action_ = action;
-        }
-        
-        public StreamContextManager.StreamCompletionAction getAction()
-        {
-            return action_;
-        }
-    }
-    
-    public static class StreamStatusSerializer implements ICompactSerializer<StreamStatus>
-    {
-        public void serialize(StreamStatus streamStatus, DataOutputStream dos) throws IOException
-        {
-            dos.writeUTF(streamStatus.getFile());
-            dos.writeLong(streamStatus.getExpectedBytes());
-            dos.writeInt(streamStatus.getAction().ordinal());
-        }
-        
-        public StreamStatus deserialize(DataInputStream dis) throws IOException
-        {
-            String targetFile = dis.readUTF();
-            long expectedBytes = dis.readLong();
-            StreamStatus streamStatus = new StreamStatus(targetFile, expectedBytes);
-            
-            int ordinal = dis.readInt();                        
-            if ( ordinal == StreamCompletionAction.DELETE.ordinal() )
-            {
-                streamStatus.setAction(StreamCompletionAction.DELETE);
-            }
-            else if ( ordinal == StreamCompletionAction.STREAM.ordinal() )
-            {
-                streamStatus.setAction(StreamCompletionAction.STREAM);
-            }
-            
-            return streamStatus;
-        }
-    }
-    
-    public static class StreamStatusMessage implements Serializable
-    {
-        private static ICompactSerializer<StreamStatusMessage> serializer_;
-        
-        static 
-        {
-            serializer_ = new StreamStatusMessageSerializer();
-        }
-        
-        public static ICompactSerializer<StreamStatusMessage> serializer()
-        {
-            return serializer_;
-        }
-        
-        public static Message makeStreamStatusMessage(StreamStatusMessage streamStatusMessage) throws IOException
-        {
-            ByteArrayOutputStream bos = new ByteArrayOutputStream();
-            DataOutputStream dos = new DataOutputStream( bos );
-            StreamStatusMessage.serializer().serialize(streamStatusMessage, dos);
-            return new Message(StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapTerminateVerbHandler_, bos.toByteArray());
-        }
-        
-        protected StreamContextManager.StreamStatus streamStatus_;
-        
-        public StreamStatusMessage(StreamContextManager.StreamStatus streamStatus)
-        {
-            streamStatus_ = streamStatus;
-        }
-        
-        public StreamContextManager.StreamStatus getStreamStatus()
-        {
-            return streamStatus_;
-        }
-    }
-    
-    public static class StreamStatusMessageSerializer implements ICompactSerializer<StreamStatusMessage>
-    {
-        public void serialize(StreamStatusMessage streamStatusMessage, DataOutputStream dos) throws IOException
-        {
-            StreamStatus.serializer().serialize(streamStatusMessage.streamStatus_, dos);            
-        }
-        
-        public StreamStatusMessage deserialize(DataInputStream dis) throws IOException
-        {            
-            StreamContextManager.StreamStatus streamStatus = StreamStatus.serializer().deserialize(dis);         
-            return new StreamStatusMessage(streamStatus);
-        }
-    }
-        
-    /* Maintain a stream context per host that is the source of the stream */
-    public static Map<String, List<StreamContext>> ctxBag_ = new Hashtable<String, List<StreamContext>>();  
-    /* Maintain in this map the status of the streams that need to be sent back to the source */
-    public static Map<String, List<StreamStatus>> streamStatusBag_ = new Hashtable<String, List<StreamStatus>>();
-    /* Maintains a callback handler per endpoint to notify the app that a stream from a given endpoint has been handled */
-    public static Map<String, IStreamComplete> streamNotificationHandlers_ = new HashMap<String, IStreamComplete>();
-    
-    public synchronized static StreamContext getStreamContext(String key)
-    {        
-        List<StreamContext> context = ctxBag_.get(key);
-        if ( context == null )
-            throw new IllegalStateException("Streaming context has not been set.");
-        StreamContext streamContext = context.remove(0);        
-        if ( context.isEmpty() )
-            ctxBag_.remove(key);
-        return streamContext;
-    }
-    
-    public synchronized static StreamStatus getStreamStatus(String key)
-    {
-        List<StreamStatus> status = streamStatusBag_.get(key);
-        if ( status == null )
-            throw new IllegalStateException("Streaming status has not been set.");
-        StreamStatus streamStatus = status.remove(0);        
-        if ( status.isEmpty() )
-            streamStatusBag_.remove(key);
-        return streamStatus;
-    }
-    
-    /*
-     * This method helps determine if the StreamCompletionHandler needs
-     * to be invoked for the data being streamed from a source. 
-    */
-    public synchronized static boolean isDone(String key)
-    {
-        return (ctxBag_.get(key) == null);
-    }
-    
-    public synchronized static IStreamComplete getStreamCompletionHandler(String key)
-    {
-        return streamNotificationHandlers_.get(key);
-    }
-    
-    public synchronized static void removeStreamCompletionHandler(String key)
-    {
-        streamNotificationHandlers_.remove(key);
-    }
-    
-    public synchronized static void registerStreamCompletionHandler(String key, IStreamComplete streamComplete)
-    {
-        streamNotificationHandlers_.put(key, streamComplete);
-    }
-    
-    public synchronized static void addStreamContext(String key, StreamContext streamContext, StreamStatus streamStatus)
-    {
-        /* Record the stream context */
-        List<StreamContext> context = ctxBag_.get(key);        
-        if ( context == null )
-        {
-            context = new ArrayList<StreamContext>();
-            ctxBag_.put(key, context);
-        }
-        context.add(streamContext);
-        
-        /* Record the stream status for this stream context */
-        List<StreamStatus> status = streamStatusBag_.get(key);
-        if ( status == null )
-        {
-            status = new ArrayList<StreamStatus>();
-            streamStatusBag_.put(key, status);
-        }
-        status.add( streamStatus );
-    }        
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.*;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.service.StorageService;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class StreamContextManager
+{
+    private static Logger logger_ = Logger.getLogger(StreamContextManager.class);
+    
+    public static enum StreamCompletionAction
+    {
+        DELETE,
+        STREAM
+    }
+    
+    public static class StreamContext implements Serializable
+    {
+        private static Logger logger_ = Logger.getLogger(StreamContextManager.StreamContext.class);
+        private static ICompactSerializer<StreamContext> serializer_;
+        
+        static
+        {
+            serializer_ = new StreamContextSerializer();
+        }
+        
+        public static ICompactSerializer<StreamContext> serializer()
+        {
+            return serializer_;
+        }
+                
+        private String targetFile_;        
+        private long expectedBytes_;                     
+        private String table_;
+        
+        public StreamContext(String targetFile, long expectedBytes, String table)
+        {
+            targetFile_ = targetFile;
+            expectedBytes_ = expectedBytes;         
+            table_ = table;
+        }
+
+        public String getTable()
+        {
+            return table_;
+        }                
+                
+        public String getTargetFile()
+        {
+            return targetFile_;
+        }
+        
+        public void setTargetFile(String file)
+        {
+            targetFile_ = file;
+        }
+        
+        public long getExpectedBytes()
+        {
+            return expectedBytes_;
+        }
+                
+        public boolean equals(Object o)
+        {
+            if ( !(o instanceof StreamContext) )
+                return false;
+            
+            StreamContext rhs = (StreamContext)o;
+            return targetFile_.equals(rhs.targetFile_);
+        }
+        
+        public int hashCode()
+        {
+            return toString().hashCode();
+        }
+        
+        public String toString()
+        {
+            return targetFile_ + ":" + expectedBytes_;
+        }
+    }
+    
+    public static class StreamContextSerializer implements ICompactSerializer<StreamContext>
+    {
+        public void serialize(StreamContextManager.StreamContext sc, DataOutputStream dos) throws IOException
+        {
+            dos.writeUTF(sc.targetFile_);
+            dos.writeLong(sc.expectedBytes_);            
+            dos.writeUTF(sc.table_);
+        }
+        
+        public StreamContextManager.StreamContext deserialize(DataInputStream dis) throws IOException
+        {
+            String targetFile = dis.readUTF();
+            long expectedBytes = dis.readLong();           
+            String table = dis.readUTF();
+            return new StreamContext(targetFile, expectedBytes, table);
+        }
+    }
+    
+    public static class StreamStatus
+    {
+        private static ICompactSerializer<StreamStatus> serializer_;
+        
+        static 
+        {
+            serializer_ = new StreamStatusSerializer();
+        }
+        
+        public static ICompactSerializer<StreamStatus> serializer()
+        {
+            return serializer_;
+        }
+            
+        private String file_;               
+        private long expectedBytes_;                
+        private StreamCompletionAction action_;
+                
+        public StreamStatus(String file, long expectedBytes)
+        {
+            file_ = file;
+            expectedBytes_ = expectedBytes;
+            action_ = StreamContextManager.StreamCompletionAction.DELETE;
+        }
+        
+        public String getFile()
+        {
+            return file_;
+        }
+        
+        public long getExpectedBytes()
+        {
+            return expectedBytes_;
+        }
+        
+        void setAction(StreamContextManager.StreamCompletionAction action)
+        {
+            action_ = action;
+        }
+        
+        public StreamContextManager.StreamCompletionAction getAction()
+        {
+            return action_;
+        }
+    }
+    
+    public static class StreamStatusSerializer implements ICompactSerializer<StreamStatus>
+    {
+        public void serialize(StreamStatus streamStatus, DataOutputStream dos) throws IOException
+        {
+            dos.writeUTF(streamStatus.getFile());
+            dos.writeLong(streamStatus.getExpectedBytes());
+            dos.writeInt(streamStatus.getAction().ordinal());
+        }
+        
+        public StreamStatus deserialize(DataInputStream dis) throws IOException
+        {
+            String targetFile = dis.readUTF();
+            long expectedBytes = dis.readLong();
+            StreamStatus streamStatus = new StreamStatus(targetFile, expectedBytes);
+            
+            int ordinal = dis.readInt();                        
+            if ( ordinal == StreamCompletionAction.DELETE.ordinal() )
+            {
+                streamStatus.setAction(StreamCompletionAction.DELETE);
+            }
+            else if ( ordinal == StreamCompletionAction.STREAM.ordinal() )
+            {
+                streamStatus.setAction(StreamCompletionAction.STREAM);
+            }
+            
+            return streamStatus;
+        }
+    }
+    
+    public static class StreamStatusMessage implements Serializable
+    {
+        private static ICompactSerializer<StreamStatusMessage> serializer_;
+        
+        static 
+        {
+            serializer_ = new StreamStatusMessageSerializer();
+        }
+        
+        public static ICompactSerializer<StreamStatusMessage> serializer()
+        {
+            return serializer_;
+        }
+        
+        public static Message makeStreamStatusMessage(StreamStatusMessage streamStatusMessage) throws IOException
+        {
+            ByteArrayOutputStream bos = new ByteArrayOutputStream();
+            DataOutputStream dos = new DataOutputStream( bos );
+            StreamStatusMessage.serializer().serialize(streamStatusMessage, dos);
+            return new Message(StorageService.getLocalStorageEndPoint(), "", StorageService.bootStrapTerminateVerbHandler_, bos.toByteArray());
+        }
+        
+        protected StreamContextManager.StreamStatus streamStatus_;
+        
+        public StreamStatusMessage(StreamContextManager.StreamStatus streamStatus)
+        {
+            streamStatus_ = streamStatus;
+        }
+        
+        public StreamContextManager.StreamStatus getStreamStatus()
+        {
+            return streamStatus_;
+        }
+    }
+    
+    public static class StreamStatusMessageSerializer implements ICompactSerializer<StreamStatusMessage>
+    {
+        public void serialize(StreamStatusMessage streamStatusMessage, DataOutputStream dos) throws IOException
+        {
+            StreamStatus.serializer().serialize(streamStatusMessage.streamStatus_, dos);            
+        }
+        
+        public StreamStatusMessage deserialize(DataInputStream dis) throws IOException
+        {            
+            StreamContextManager.StreamStatus streamStatus = StreamStatus.serializer().deserialize(dis);         
+            return new StreamStatusMessage(streamStatus);
+        }
+    }
+        
+    /* Maintain a stream context per host that is the source of the stream */
+    public static Map<String, List<StreamContext>> ctxBag_ = new Hashtable<String, List<StreamContext>>();  
+    /* Maintain in this map the status of the streams that need to be sent back to the source */
+    public static Map<String, List<StreamStatus>> streamStatusBag_ = new Hashtable<String, List<StreamStatus>>();
+    /* Maintains a callback handler per endpoint to notify the app that a stream from a given endpoint has been handled */
+    public static Map<String, IStreamComplete> streamNotificationHandlers_ = new HashMap<String, IStreamComplete>();
+    
+    public synchronized static StreamContext getStreamContext(String key)
+    {        
+        List<StreamContext> context = ctxBag_.get(key);
+        if ( context == null )
+            throw new IllegalStateException("Streaming context has not been set.");
+        StreamContext streamContext = context.remove(0);        
+        if ( context.isEmpty() )
+            ctxBag_.remove(key);
+        return streamContext;
+    }
+    
+    public synchronized static StreamStatus getStreamStatus(String key)
+    {
+        List<StreamStatus> status = streamStatusBag_.get(key);
+        if ( status == null )
+            throw new IllegalStateException("Streaming status has not been set.");
+        StreamStatus streamStatus = status.remove(0);        
+        if ( status.isEmpty() )
+            streamStatusBag_.remove(key);
+        return streamStatus;
+    }
+    
+    /*
+     * This method helps determine if the StreamCompletionHandler needs
+     * to be invoked for the data being streamed from a source. 
+    */
+    public synchronized static boolean isDone(String key)
+    {
+        return (ctxBag_.get(key) == null);
+    }
+    
+    public synchronized static IStreamComplete getStreamCompletionHandler(String key)
+    {
+        return streamNotificationHandlers_.get(key);
+    }
+    
+    public synchronized static void removeStreamCompletionHandler(String key)
+    {
+        streamNotificationHandlers_.remove(key);
+    }
+    
+    public synchronized static void registerStreamCompletionHandler(String key, IStreamComplete streamComplete)
+    {
+        streamNotificationHandlers_.put(key, streamComplete);
+    }
+    
+    public synchronized static void addStreamContext(String key, StreamContext streamContext, StreamStatus streamStatus)
+    {
+        /* Record the stream context */
+        List<StreamContext> context = ctxBag_.get(key);        
+        if ( context == null )
+        {
+            context = new ArrayList<StreamContext>();
+            ctxBag_.put(key, context);
+        }
+        context.add(streamContext);
+        
+        /* Record the stream status for this stream context */
+        List<StreamStatus> status = streamStatusBag_.get(key);
+        if ( status == null )
+        {
+            status = new ArrayList<StreamStatus>();
+            streamStatusBag_.put(key, status);
+        }
+        status.add( streamStatus );
+    }        
+}
diff --git a/src/java/org/apache/cassandra/net/io/TcpReader.java b/src/java/org/apache/cassandra/net/io/TcpReader.java
index e9223a06c3..a979cc9498 100644
--- a/src/java/org/apache/cassandra/net/io/TcpReader.java
+++ b/src/java/org/apache/cassandra/net/io/TcpReader.java
@@ -1,122 +1,122 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.io;
-
-import java.io.IOException;
-import java.nio.channels.SocketChannel;
-import java.util.*;
-
-import org.apache.cassandra.net.ProtocolHeader;
-import org.apache.cassandra.net.TcpConnection;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class TcpReader
-{
-    public static enum TcpReaderState
-    {
-        START,
-        PREAMBLE,
-        PROTOCOL,
-        CONTENT_LENGTH,
-        CONTENT,
-        CONTENT_STREAM,
-        DONE
-    }
-    
-    private Map<TcpReaderState, StartState> stateMap_ = new HashMap<TcpReaderState, StartState>();
-    private TcpConnection connection_;
-    private StartState socketState_;
-    private ProtocolHeader protocolHeader_;
-    
-    public TcpReader(TcpConnection connection)
-    {
-        connection_ = connection;        
-    }
-    
-    public StartState getSocketState(TcpReaderState state)
-    {
-        return stateMap_.get(state);
-    }
-    
-    public void putSocketState(TcpReaderState state, StartState socketState)
-    {
-        stateMap_.put(state, socketState);
-    } 
-    
-    public void resetState()
-    {
-        StartState nextState = stateMap_.get(TcpReaderState.PREAMBLE);
-        if ( nextState == null )
-        {
-            nextState = new ProtocolState(this);
-            stateMap_.put(TcpReaderState.PREAMBLE, nextState);
-        }
-        socketState_ = nextState;
-    }
-    
-    public void morphState(StartState state)
-    {        
-        socketState_ = state;
-        if ( protocolHeader_ == null )
-            protocolHeader_ = new ProtocolHeader();
-    }
-    
-    public ProtocolHeader getProtocolHeader()
-    {
-        return protocolHeader_;
-    }
-    
-    public SocketChannel getStream()
-    {
-        return connection_.getSocketChannel();
-    }
-    
-    public byte[] read() throws IOException
-    {
-        byte[] bytes = new byte[0];      
-        while ( socketState_ != null )
-        {
-            try
-            {                                                                      
-                bytes = socketState_.read();
-            }
-            catch ( ReadNotCompleteException e )
-            {                
-                break;
-            }
-        }
-        return bytes;
-    }    
-    
-    public static void main(String[] args) throws Throwable
-    {
-        Map<TcpReaderState, StartState> stateMap = new HashMap<TcpReaderState, StartState>();
-        stateMap.put(TcpReaderState.CONTENT, new ContentState(null, 10));
-        stateMap.put(TcpReaderState.START, new ProtocolState(null));
-        stateMap.put(TcpReaderState.CONTENT_LENGTH, new ContentLengthState(null));
-        
-        StartState state = stateMap.get(TcpReaderState.CONTENT);
-        System.out.println( state.getClass().getName() );
-        state = stateMap.get(TcpReaderState.CONTENT_LENGTH);
-        System.out.println( state.getClass().getName() );
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.io;
+
+import java.io.IOException;
+import java.nio.channels.SocketChannel;
+import java.util.*;
+
+import org.apache.cassandra.net.ProtocolHeader;
+import org.apache.cassandra.net.TcpConnection;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TcpReader
+{
+    public static enum TcpReaderState
+    {
+        START,
+        PREAMBLE,
+        PROTOCOL,
+        CONTENT_LENGTH,
+        CONTENT,
+        CONTENT_STREAM,
+        DONE
+    }
+    
+    private Map<TcpReaderState, StartState> stateMap_ = new HashMap<TcpReaderState, StartState>();
+    private TcpConnection connection_;
+    private StartState socketState_;
+    private ProtocolHeader protocolHeader_;
+    
+    public TcpReader(TcpConnection connection)
+    {
+        connection_ = connection;        
+    }
+    
+    public StartState getSocketState(TcpReaderState state)
+    {
+        return stateMap_.get(state);
+    }
+    
+    public void putSocketState(TcpReaderState state, StartState socketState)
+    {
+        stateMap_.put(state, socketState);
+    } 
+    
+    public void resetState()
+    {
+        StartState nextState = stateMap_.get(TcpReaderState.PREAMBLE);
+        if ( nextState == null )
+        {
+            nextState = new ProtocolState(this);
+            stateMap_.put(TcpReaderState.PREAMBLE, nextState);
+        }
+        socketState_ = nextState;
+    }
+    
+    public void morphState(StartState state)
+    {        
+        socketState_ = state;
+        if ( protocolHeader_ == null )
+            protocolHeader_ = new ProtocolHeader();
+    }
+    
+    public ProtocolHeader getProtocolHeader()
+    {
+        return protocolHeader_;
+    }
+    
+    public SocketChannel getStream()
+    {
+        return connection_.getSocketChannel();
+    }
+    
+    public byte[] read() throws IOException
+    {
+        byte[] bytes = new byte[0];      
+        while ( socketState_ != null )
+        {
+            try
+            {                                                                      
+                bytes = socketState_.read();
+            }
+            catch ( ReadNotCompleteException e )
+            {                
+                break;
+            }
+        }
+        return bytes;
+    }    
+    
+    public static void main(String[] args) throws Throwable
+    {
+        Map<TcpReaderState, StartState> stateMap = new HashMap<TcpReaderState, StartState>();
+        stateMap.put(TcpReaderState.CONTENT, new ContentState(null, 10));
+        stateMap.put(TcpReaderState.START, new ProtocolState(null));
+        stateMap.put(TcpReaderState.CONTENT_LENGTH, new ContentLengthState(null));
+        
+        StartState state = stateMap.get(TcpReaderState.CONTENT);
+        System.out.println( state.getClass().getName() );
+        state = stateMap.get(TcpReaderState.CONTENT_LENGTH);
+        System.out.println( state.getClass().getName() );
+    }
+}
diff --git a/src/java/org/apache/cassandra/net/sink/IMessageSink.java b/src/java/org/apache/cassandra/net/sink/IMessageSink.java
index 8170235af5..ba7c83c043 100644
--- a/src/java/org/apache/cassandra/net/sink/IMessageSink.java
+++ b/src/java/org/apache/cassandra/net/sink/IMessageSink.java
@@ -1,30 +1,30 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.sink;
-
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.Message;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IMessageSink
-{
-    public Message handleMessage(Message message);    
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.sink;
+
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IMessageSink
+{
+    public Message handleMessage(Message message);    
+}
diff --git a/src/java/org/apache/cassandra/net/sink/SinkManager.java b/src/java/org/apache/cassandra/net/sink/SinkManager.java
index 503c019219..ab71bdc218 100644
--- a/src/java/org/apache/cassandra/net/sink/SinkManager.java
+++ b/src/java/org/apache/cassandra/net/sink/SinkManager.java
@@ -1,78 +1,78 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.net.sink;
-
-import java.util.*;
-import java.io.IOException;
-
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.Message;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class SinkManager
-{
-    private static LinkedList<IMessageSink> messageSinks_ = new LinkedList<IMessageSink>();
-
-    public static boolean isInitialized()
-    {
-        return ( messageSinks_.size() > 0 );
-    }
-
-    public static void addMessageSink(IMessageSink ms)
-    {
-        messageSinks_.addLast(ms);
-    }
-    
-    public static void clearSinks(){
-        messageSinks_.clear();
-    }
-
-    public static Message processClientMessageSink(Message message)
-    {
-        ListIterator<IMessageSink> li = messageSinks_.listIterator();
-        while ( li.hasNext() )
-        {
-            IMessageSink ms = li.next();
-            message = ms.handleMessage(message);
-            if ( message == null )
-            {
-                return null;
-            }
-        }
-        return message;
-    }
-
-    public static Message processServerMessageSink(Message message)
-    {
-        ListIterator<IMessageSink> li = messageSinks_.listIterator(messageSinks_.size());
-        while ( li.hasPrevious() )
-        {
-            IMessageSink ms = li.previous();
-            message = ms.handleMessage(message);
-            if ( message == null )
-            {
-                return null;
-            }
-        }
-        return message;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.net.sink;
+
+import java.util.*;
+import java.io.IOException;
+
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SinkManager
+{
+    private static LinkedList<IMessageSink> messageSinks_ = new LinkedList<IMessageSink>();
+
+    public static boolean isInitialized()
+    {
+        return ( messageSinks_.size() > 0 );
+    }
+
+    public static void addMessageSink(IMessageSink ms)
+    {
+        messageSinks_.addLast(ms);
+    }
+    
+    public static void clearSinks(){
+        messageSinks_.clear();
+    }
+
+    public static Message processClientMessageSink(Message message)
+    {
+        ListIterator<IMessageSink> li = messageSinks_.listIterator();
+        while ( li.hasNext() )
+        {
+            IMessageSink ms = li.next();
+            message = ms.handleMessage(message);
+            if ( message == null )
+            {
+                return null;
+            }
+        }
+        return message;
+    }
+
+    public static Message processServerMessageSink(Message message)
+    {
+        ListIterator<IMessageSink> li = messageSinks_.listIterator(messageSinks_.size());
+        while ( li.hasPrevious() )
+        {
+            IMessageSink ms = li.previous();
+            message = ms.handleMessage(message);
+            if ( message == null )
+            {
+                return null;
+            }
+        }
+        return message;
+    }
+}
diff --git a/src/java/org/apache/cassandra/procedures/GroovyScriptRunner.java b/src/java/org/apache/cassandra/procedures/GroovyScriptRunner.java
index a6ff3d158c..6df394d28d 100644
--- a/src/java/org/apache/cassandra/procedures/GroovyScriptRunner.java
+++ b/src/java/org/apache/cassandra/procedures/GroovyScriptRunner.java
@@ -16,16 +16,16 @@
 * specific language governing permissions and limitations
 * under the License.
 */
-package org.apache.cassandra.procedures;
-
-import groovy.lang.GroovyShell;
-
-public class GroovyScriptRunner
-{
-	private static GroovyShell groovyShell_ = new GroovyShell();
-
-	public static String evaluateString(String script)
-	{        
-		 return groovyShell_.evaluate(script).toString();
-	}
-}
+package org.apache.cassandra.procedures;
+
+import groovy.lang.GroovyShell;
+
+public class GroovyScriptRunner
+{
+	private static GroovyShell groovyShell_ = new GroovyShell();
+
+	public static String evaluateString(String script)
+	{        
+		 return groovyShell_.evaluate(script).toString();
+	}
+}
diff --git a/src/java/org/apache/cassandra/service/ConsistencyManager.java b/src/java/org/apache/cassandra/service/ConsistencyManager.java
index 75ba45dc77..94f9d3d034 100644
--- a/src/java/org/apache/cassandra/service/ConsistencyManager.java
+++ b/src/java/org/apache/cassandra/service/ConsistencyManager.java
@@ -1,183 +1,183 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.ReadCommand;
-import org.apache.cassandra.db.ReadResponse;
-import org.apache.cassandra.db.Row;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IAsyncCallback;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.utils.Cachetable;
-import org.apache.cassandra.utils.ICacheExpungeHook;
-import org.apache.cassandra.utils.ICachetable;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-import org.apache.commons.lang.StringUtils;
-
-
-class ConsistencyManager implements Runnable
-{
-	private static Logger logger_ = Logger.getLogger(ConsistencyManager.class);
-	
-	class DigestResponseHandler implements IAsyncCallback
-	{
-		List<Message> responses_ = new ArrayList<Message>();
-		
-		public void response(Message msg)
-		{
-			responses_.add(msg);
-			if ( responses_.size() == ConsistencyManager.this.replicas_.size() )
-				handleDigestResponses();
-		}
-        
-        public void attachContext(Object o)
-        {
-            throw new UnsupportedOperationException("This operation is not currently supported.");
-        }
-		
-		private void handleDigestResponses()
-		{
-			DataInputBuffer bufIn = new DataInputBuffer();
-			for( Message response : responses_ )
-			{
-				byte[] body = response.getMessageBody();            
-	            bufIn.reset(body, body.length);
-	            try
-	            {	               
-	                ReadResponse result = ReadResponse.serializer().deserialize(bufIn);
-	                byte[] digest = result.digest();
-	                if( !Arrays.equals(row_.digest(), digest) )
-					{
-	                	doReadRepair();
-	                	break;
-					}
-	            }
-	            catch( IOException ex )
-	            {
-	            	logger_.info(LogUtil.throwableToString(ex));
-	            }
-			}
-		}
-		
-		private void doReadRepair() throws IOException
-		{
-			IResponseResolver<Row> readResponseResolver = new ReadResponseResolver();
-            /* Add the local storage endpoint to the replicas_ list */
-            replicas_.add(StorageService.getLocalStorageEndPoint());
-			IAsyncCallback responseHandler = new DataRepairHandler(ConsistencyManager.this.replicas_.size(), readResponseResolver);	
-            ReadCommand readCommand = constructReadMessage(false);
-            Message message = readCommand.makeReadMessage();
-            if (logger_.isDebugEnabled())
-              logger_.debug("Performing read repair for " + readCommand_.key + " to " + message.getMessageId() + "@[" + StringUtils.join(replicas_, ", ") + "]");
-			MessagingService.getMessagingInstance().sendRR(message, replicas_.toArray(new EndPoint[replicas_.size()]), responseHandler);
-		}
-	}
-	
-	class DataRepairHandler implements IAsyncCallback, ICacheExpungeHook<String, String>
-	{
-		private List<Message> responses_ = new ArrayList<Message>();
-		private IResponseResolver<Row> readResponseResolver_;
-		private int majority_;
-		
-		DataRepairHandler(int responseCount, IResponseResolver<Row> readResponseResolver)
-		{
-			readResponseResolver_ = readResponseResolver;
-			majority_ = (responseCount >> 1) + 1;  
-		}
-		
-		public void response(Message message)
-		{
-			if (logger_.isDebugEnabled())
-			  logger_.debug("Received responses in DataRepairHandler : " + message.toString());
-			responses_.add(message);
-			if ( responses_.size() == majority_ )
-			{
-				String messageId = message.getMessageId();
-				readRepairTable_.put(messageId, messageId, this);				
-			}
-		}
-        
-        public void attachContext(Object o)
-        {
-            throw new UnsupportedOperationException("This operation is not currently supported.");
-        }
-		
-		public void callMe(String key, String value)
-		{
-			handleResponses();
-		}
-		
-		private void handleResponses()
-		{
-			try
-			{
-				readResponseResolver_.resolve(new ArrayList<Message>(responses_));
-			}
-			catch ( DigestMismatchException ex )
-			{
-				throw new RuntimeException(ex);
-			}
-		}
-	}
-
-	private static long scheduledTimeMillis_ = 600;
-	private static ICachetable<String, String> readRepairTable_ = new Cachetable<String, String>(scheduledTimeMillis_);
-	private final Row row_;
-	protected final List<EndPoint> replicas_;
-	private final ReadCommand readCommand_;
-
-    public ConsistencyManager(Row row, List<EndPoint> replicas, ReadCommand readCommand)
-    {
-        row_ = row;
-        replicas_ = replicas;
-        readCommand_ = readCommand;
-    }
-
-	public void run()
-	{
-        ReadCommand readCommandDigestOnly = constructReadMessage(true);
-		try
-		{
-			Message message = readCommandDigestOnly.makeReadMessage();
-            if (logger_.isDebugEnabled())
-              logger_.debug("Reading consistency digest for " + readCommand_.key + " from " + message.getMessageId() + "@[" + StringUtils.join(replicas_, ", ") + "]");
-            MessagingService.getMessagingInstance().sendRR(message, replicas_.toArray(new EndPoint[replicas_.size()]), new DigestResponseHandler());
-		}
-		catch (IOException ex)
-		{
-			throw new RuntimeException(ex);
-		}
-	}
-    
-    private ReadCommand constructReadMessage(boolean isDigestQuery)
-    {
-        ReadCommand readCommand = readCommand_.copy();
-        readCommand.setDigestQuery(isDigestQuery);
-        return readCommand;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.ReadResponse;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IAsyncCallback;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.Cachetable;
+import org.apache.cassandra.utils.ICacheExpungeHook;
+import org.apache.cassandra.utils.ICachetable;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+import org.apache.commons.lang.StringUtils;
+
+
+class ConsistencyManager implements Runnable
+{
+	private static Logger logger_ = Logger.getLogger(ConsistencyManager.class);
+	
+	class DigestResponseHandler implements IAsyncCallback
+	{
+		List<Message> responses_ = new ArrayList<Message>();
+		
+		public void response(Message msg)
+		{
+			responses_.add(msg);
+			if ( responses_.size() == ConsistencyManager.this.replicas_.size() )
+				handleDigestResponses();
+		}
+        
+        public void attachContext(Object o)
+        {
+            throw new UnsupportedOperationException("This operation is not currently supported.");
+        }
+		
+		private void handleDigestResponses()
+		{
+			DataInputBuffer bufIn = new DataInputBuffer();
+			for( Message response : responses_ )
+			{
+				byte[] body = response.getMessageBody();            
+	            bufIn.reset(body, body.length);
+	            try
+	            {	               
+	                ReadResponse result = ReadResponse.serializer().deserialize(bufIn);
+	                byte[] digest = result.digest();
+	                if( !Arrays.equals(row_.digest(), digest) )
+					{
+	                	doReadRepair();
+	                	break;
+					}
+	            }
+	            catch( IOException ex )
+	            {
+	            	logger_.info(LogUtil.throwableToString(ex));
+	            }
+			}
+		}
+		
+		private void doReadRepair() throws IOException
+		{
+			IResponseResolver<Row> readResponseResolver = new ReadResponseResolver();
+            /* Add the local storage endpoint to the replicas_ list */
+            replicas_.add(StorageService.getLocalStorageEndPoint());
+			IAsyncCallback responseHandler = new DataRepairHandler(ConsistencyManager.this.replicas_.size(), readResponseResolver);	
+            ReadCommand readCommand = constructReadMessage(false);
+            Message message = readCommand.makeReadMessage();
+            if (logger_.isDebugEnabled())
+              logger_.debug("Performing read repair for " + readCommand_.key + " to " + message.getMessageId() + "@[" + StringUtils.join(replicas_, ", ") + "]");
+			MessagingService.getMessagingInstance().sendRR(message, replicas_.toArray(new EndPoint[replicas_.size()]), responseHandler);
+		}
+	}
+	
+	class DataRepairHandler implements IAsyncCallback, ICacheExpungeHook<String, String>
+	{
+		private List<Message> responses_ = new ArrayList<Message>();
+		private IResponseResolver<Row> readResponseResolver_;
+		private int majority_;
+		
+		DataRepairHandler(int responseCount, IResponseResolver<Row> readResponseResolver)
+		{
+			readResponseResolver_ = readResponseResolver;
+			majority_ = (responseCount >> 1) + 1;  
+		}
+		
+		public void response(Message message)
+		{
+			if (logger_.isDebugEnabled())
+			  logger_.debug("Received responses in DataRepairHandler : " + message.toString());
+			responses_.add(message);
+			if ( responses_.size() == majority_ )
+			{
+				String messageId = message.getMessageId();
+				readRepairTable_.put(messageId, messageId, this);				
+			}
+		}
+        
+        public void attachContext(Object o)
+        {
+            throw new UnsupportedOperationException("This operation is not currently supported.");
+        }
+		
+		public void callMe(String key, String value)
+		{
+			handleResponses();
+		}
+		
+		private void handleResponses()
+		{
+			try
+			{
+				readResponseResolver_.resolve(new ArrayList<Message>(responses_));
+			}
+			catch ( DigestMismatchException ex )
+			{
+				throw new RuntimeException(ex);
+			}
+		}
+	}
+
+	private static long scheduledTimeMillis_ = 600;
+	private static ICachetable<String, String> readRepairTable_ = new Cachetable<String, String>(scheduledTimeMillis_);
+	private final Row row_;
+	protected final List<EndPoint> replicas_;
+	private final ReadCommand readCommand_;
+
+    public ConsistencyManager(Row row, List<EndPoint> replicas, ReadCommand readCommand)
+    {
+        row_ = row;
+        replicas_ = replicas;
+        readCommand_ = readCommand;
+    }
+
+	public void run()
+	{
+        ReadCommand readCommandDigestOnly = constructReadMessage(true);
+		try
+		{
+			Message message = readCommandDigestOnly.makeReadMessage();
+            if (logger_.isDebugEnabled())
+              logger_.debug("Reading consistency digest for " + readCommand_.key + " from " + message.getMessageId() + "@[" + StringUtils.join(replicas_, ", ") + "]");
+            MessagingService.getMessagingInstance().sendRR(message, replicas_.toArray(new EndPoint[replicas_.size()]), new DigestResponseHandler());
+		}
+		catch (IOException ex)
+		{
+			throw new RuntimeException(ex);
+		}
+	}
+    
+    private ReadCommand constructReadMessage(boolean isDigestQuery)
+    {
+        ReadCommand readCommand = readCommand_.copy();
+        readCommand.setDigestQuery(isDigestQuery);
+        return readCommand;
+    }
+}
diff --git a/src/java/org/apache/cassandra/service/DigestMismatchException.java b/src/java/org/apache/cassandra/service/DigestMismatchException.java
index 5ae5ffeb99..164658483e 100644
--- a/src/java/org/apache/cassandra/service/DigestMismatchException.java
+++ b/src/java/org/apache/cassandra/service/DigestMismatchException.java
@@ -1,30 +1,30 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class DigestMismatchException extends Exception
-{
-	public DigestMismatchException(String message)
-	{
-		super(message);
-	}
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class DigestMismatchException extends Exception
+{
+	public DigestMismatchException(String message)
+	{
+		super(message);
+	}
+}
diff --git a/src/java/org/apache/cassandra/service/IResponseResolver.java b/src/java/org/apache/cassandra/service/IResponseResolver.java
index 03a89719b7..47ed69dd3e 100644
--- a/src/java/org/apache/cassandra/service/IResponseResolver.java
+++ b/src/java/org/apache/cassandra/service/IResponseResolver.java
@@ -1,42 +1,42 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.util.List;
-
-import org.apache.cassandra.net.Message;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface IResponseResolver<T> {
-
-	/*
-	 * This Method resolves the responses that are passed in . for example : if
-	 * its write response then all we get is true or false return values which
-	 * implies if the writes were successful but for reads its more complicated
-	 * you need to look at the responses and then based on differences schedule
-	 * repairs . Hence you need to derive a response resolver based on your
-	 * needs from this interface.
-	 */
-	public T resolve(List<Message> responses) throws DigestMismatchException;
-	public boolean isDataPresent(List<Message> responses);
-
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.util.List;
+
+import org.apache.cassandra.net.Message;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface IResponseResolver<T> {
+
+	/*
+	 * This Method resolves the responses that are passed in . for example : if
+	 * its write response then all we get is true or false return values which
+	 * implies if the writes were successful but for reads its more complicated
+	 * you need to look at the responses and then based on differences schedule
+	 * repairs . Hence you need to derive a response resolver based on your
+	 * needs from this interface.
+	 */
+	public T resolve(List<Message> responses) throws DigestMismatchException;
+	public boolean isDataPresent(List<Message> responses);
+
+}
diff --git a/src/java/org/apache/cassandra/service/LoadDisseminator.java b/src/java/org/apache/cassandra/service/LoadDisseminator.java
index 852755c590..622385f98a 100644
--- a/src/java/org/apache/cassandra/service/LoadDisseminator.java
+++ b/src/java/org/apache/cassandra/service/LoadDisseminator.java
@@ -1,48 +1,48 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.cassandra.service;
-
-import java.util.TimerTask;
-
-import org.apache.cassandra.gms.ApplicationState;
-import org.apache.cassandra.gms.Gossiper;
-import org.apache.cassandra.utils.FileUtils;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-class LoadDisseminator extends TimerTask
-{
-    private final static Logger logger_ = Logger.getLogger(LoadDisseminator.class);
-    protected final static String loadInfo_= "LOAD-INFORMATION";
-    
-    public void run()
-    {
-        try
-        {
-            long diskSpace = FileUtils.getUsedDiskSpace();                
-            String diskUtilization = FileUtils.stringifyFileSize(diskSpace);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Disseminating load info ...");
-            Gossiper.instance().addApplicationState(LoadDisseminator.loadInfo_, new ApplicationState(diskUtilization));
-        }
-        catch ( Throwable ex )
-        {
-            logger_.warn( LogUtil.throwableToString(ex) );
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.service;
+
+import java.util.TimerTask;
+
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+class LoadDisseminator extends TimerTask
+{
+    private final static Logger logger_ = Logger.getLogger(LoadDisseminator.class);
+    protected final static String loadInfo_= "LOAD-INFORMATION";
+    
+    public void run()
+    {
+        try
+        {
+            long diskSpace = FileUtils.getUsedDiskSpace();                
+            String diskUtilization = FileUtils.stringifyFileSize(diskSpace);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Disseminating load info ...");
+            Gossiper.instance().addApplicationState(LoadDisseminator.loadInfo_, new ApplicationState(diskUtilization));
+        }
+        catch ( Throwable ex )
+        {
+            logger_.warn( LogUtil.throwableToString(ex) );
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/service/LoadInfo.java b/src/java/org/apache/cassandra/service/LoadInfo.java
index b1af54e2e0..a8d2385cdd 100644
--- a/src/java/org/apache/cassandra/service/LoadInfo.java
+++ b/src/java/org/apache/cassandra/service/LoadInfo.java
@@ -1,64 +1,64 @@
- /**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.util.Comparator;
-
-import org.apache.cassandra.utils.FileUtils;
-
-
-class LoadInfo
-{
-    protected static class DiskSpaceComparator implements Comparator<LoadInfo>
-    {
-        public int compare(LoadInfo li, LoadInfo li2)
-        {
-            if ( li == null || li2 == null )
-                throw new IllegalArgumentException("Cannot pass in values that are NULL.");
-            
-            double space = FileUtils.stringToFileSize(li.diskSpace_);
-            double space2 = FileUtils.stringToFileSize(li2.diskSpace_);
-            return (int)(space - space2);
-        }
-    }
-        
-    private String diskSpace_;
-    
-    LoadInfo(long diskSpace)
-    {       
-        diskSpace_ = FileUtils.stringifyFileSize(diskSpace);
-    }
-    
-    LoadInfo(String loadInfo)
-    {
-        diskSpace_ = loadInfo;
-    }
-    
-    String diskSpace()
-    {
-        return diskSpace_;
-    }
-    
-    public String toString()
-    {
-        StringBuilder sb = new StringBuilder("");       
-        sb.append(diskSpace_);
-        return sb.toString();
-    }
-}
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.util.Comparator;
+
+import org.apache.cassandra.utils.FileUtils;
+
+
+class LoadInfo
+{
+    protected static class DiskSpaceComparator implements Comparator<LoadInfo>
+    {
+        public int compare(LoadInfo li, LoadInfo li2)
+        {
+            if ( li == null || li2 == null )
+                throw new IllegalArgumentException("Cannot pass in values that are NULL.");
+            
+            double space = FileUtils.stringToFileSize(li.diskSpace_);
+            double space2 = FileUtils.stringToFileSize(li2.diskSpace_);
+            return (int)(space - space2);
+        }
+    }
+        
+    private String diskSpace_;
+    
+    LoadInfo(long diskSpace)
+    {       
+        diskSpace_ = FileUtils.stringifyFileSize(diskSpace);
+    }
+    
+    LoadInfo(String loadInfo)
+    {
+        diskSpace_ = loadInfo;
+    }
+    
+    String diskSpace()
+    {
+        return diskSpace_;
+    }
+    
+    public String toString()
+    {
+        StringBuilder sb = new StringBuilder("");       
+        sb.append(diskSpace_);
+        return sb.toString();
+    }
+}
diff --git a/src/java/org/apache/cassandra/service/MultiQuorumResponseHandler.java b/src/java/org/apache/cassandra/service/MultiQuorumResponseHandler.java
index 5020c3e979..f9ad796eb1 100644
--- a/src/java/org/apache/cassandra/service/MultiQuorumResponseHandler.java
+++ b/src/java/org/apache/cassandra/service/MultiQuorumResponseHandler.java
@@ -1,252 +1,252 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.io.IOException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.ArrayList;
-import java.util.Map;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.locks.*;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.TimeoutException;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.ReadCommand;
-import org.apache.cassandra.db.Row;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IAsyncCallback;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class MultiQuorumResponseHandler implements IAsyncCallback
-{ 
-    private static Logger logger_ = Logger.getLogger( QuorumResponseHandler.class );
-    private Lock lock_ = new ReentrantLock();
-    private Condition condition_;
-    /* This maps the keys to the original data read messages */
-    private Map<String, ReadCommand> readMessages_ = new HashMap<String, ReadCommand>();
-    /* This maps the key to its set of replicas */
-    private Map<String, EndPoint[]> endpoints_ = new HashMap<String, EndPoint[]>();
-    /* This maps the groupId to the individual callback for the set of messages */
-    private Map<String, SingleQuorumResponseHandler> handlers_ = new HashMap<String, SingleQuorumResponseHandler>();
-    /* This should hold all the responses for the keys */
-    private List<Row> responses_ = new ArrayList<Row>();
-    private AtomicBoolean done_ = new AtomicBoolean(false);
-    
-    /**
-     * This is used to handle the responses from the individual messages
-     * that are sent out to the replicas.
-     * @author alakshman
-     *
-    */
-    private class SingleQuorumResponseHandler implements IAsyncCallback
-    {
-        private Lock lock_ = new ReentrantLock();
-        private IResponseResolver<Row> responseResolver_;
-        private List<Message> responses_ = new ArrayList<Message>();
-        
-        SingleQuorumResponseHandler(IResponseResolver<Row> responseResolver)
-        {
-            responseResolver_ = responseResolver;
-        }
-        
-        public void attachContext(Object o)
-        {
-            throw new UnsupportedOperationException("This operation is not supported in this implementation");
-        }
-        
-        public void response(Message response)
-        {
-            lock_.lock();
-            try
-            {
-                responses_.add(response);
-                int majority = (DatabaseDescriptor.getReplicationFactor() >> 1) + 1;                            
-                if ( responses_.size() >= majority && responseResolver_.isDataPresent(responses_))
-                {
-                    onCompletion();               
-                }
-            }
-            catch ( IOException ex )
-            {
-                logger_.info( LogUtil.throwableToString(ex) );
-            }
-            finally
-            {
-                lock_.unlock();
-            }
-        }
-        
-        private void onCompletion() throws IOException
-        {
-            try
-            {
-                Row row = responseResolver_.resolve(responses_);
-                MultiQuorumResponseHandler.this.onCompleteResponse(row);
-            }
-            catch ( DigestMismatchException ex )
-            {
-                /* 
-                 * The DigestMismatchException has the key for which the mismatch
-                 * occurred bundled in it as context 
-                */
-                String key = ex.getMessage();
-                onDigestMismatch(key);
-            }
-        }
-        
-        /**
-         * This method is invoked on a digest match. We pass in the key
-         * in order to retrieve the appropriate data message that needs
-         * to be sent out to the replicas. 
-         * 
-         * @param key for which the mismatch occurred.
-        */
-        private void onDigestMismatch(String key) throws IOException
-        {
-            if ( DatabaseDescriptor.getConsistencyCheck())
-            {                                
-                ReadCommand readCommand = readMessages_.get(key);
-                readCommand.setDigestQuery(false);
-                Message messageRepair = readCommand.makeReadMessage();
-                EndPoint[] endpoints = MultiQuorumResponseHandler.this.endpoints_.get(readCommand.key);
-                Message[][] messages = new Message[][]{ {messageRepair, messageRepair, messageRepair} };
-                EndPoint[][] epList = new EndPoint[][]{ endpoints };
-                MessagingService.getMessagingInstance().sendRR(messages, epList, MultiQuorumResponseHandler.this);                
-            }
-        }
-    }
-    
-    public MultiQuorumResponseHandler(Map<String, ReadCommand> readMessages, Map<String, EndPoint[]> endpoints)
-    {        
-        condition_ = lock_.newCondition();
-        readMessages_ = readMessages;
-        endpoints_ = endpoints;
-    }
-    
-    public Row[] get() throws TimeoutException
-    {
-        long startTime = System.currentTimeMillis();
-        lock_.lock();
-        try
-        {            
-            boolean bVal = true;            
-            try
-            {
-                if ( !done_.get() )
-                {                   
-                    bVal = condition_.await(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
-                }
-            }
-            catch ( InterruptedException ex )
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug( LogUtil.throwableToString(ex) );
-            }
-            
-            if ( !bVal && !done_.get() )
-            {
-                StringBuilder sb = new StringBuilder("");
-                for ( Row row : responses_ )
-                {
-                    sb.append(row.key());
-                    sb.append(":");
-                }                
-                throw new TimeoutException("Operation timed out - received only " +  responses_.size() + " responses from " + sb.toString() + " .");
-            }
-        }
-        finally
-        {
-            lock_.unlock();
-        }
-        
-        logger_.info("MultiQuorumResponseHandler: " + (System.currentTimeMillis() - startTime) + " ms.");
-        return responses_.toArray( new Row[0] );
-    }
-    
-    /**
-     * Invoked when a complete response has been obtained
-     * for one of the sub-groups a.k.a keys for the query 
-     * has been performed.
-     * 
-     * @param row obtained as a result of the response.
-     */
-    void onCompleteResponse(Row row)
-    {        
-        if ( !done_.get() )
-        {
-            responses_.add(row);
-            if ( responses_.size() == readMessages_.size() )
-            {
-                done_.set(true);
-                condition_.signal();                
-            }
-        }
-    }
-    
-    /**
-     * The handler of the response message that has been
-     * sent by one of the replicas for one of the keys.
-     * 
-     * @param message the response message for one of the
-     *        message that we sent out.
-     */
-    public void response(Message message)
-    {
-        lock_.lock();
-        try
-        {
-            SingleQuorumResponseHandler handler = handlers_.get(message.getMessageId());
-            handler.response(message);
-        }
-        finally
-        {
-            lock_.unlock();
-        }
-    }
-    
-    /**
-     * The context that is passed in for the query of
-     * multiple keys in the system. For each message 
-     * id in the context register a callback handler 
-     * for the same. This is done so that all responses
-     * for a given key use the same callback handler.
-     * 
-     * @param o the context which is an array of strings
-     *        corresponding to the message id's for each
-     *        key.
-     */
-    public void attachContext(Object o)
-    {
-        String[] gids = (String[])o;
-        for ( String gid : gids )
-        {
-            IResponseResolver<Row> responseResolver = new ReadResponseResolver();
-            SingleQuorumResponseHandler handler = new SingleQuorumResponseHandler(responseResolver);
-            handlers_.put(gid, handler);
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.*;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ReadCommand;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IAsyncCallback;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MultiQuorumResponseHandler implements IAsyncCallback
+{ 
+    private static Logger logger_ = Logger.getLogger( QuorumResponseHandler.class );
+    private Lock lock_ = new ReentrantLock();
+    private Condition condition_;
+    /* This maps the keys to the original data read messages */
+    private Map<String, ReadCommand> readMessages_ = new HashMap<String, ReadCommand>();
+    /* This maps the key to its set of replicas */
+    private Map<String, EndPoint[]> endpoints_ = new HashMap<String, EndPoint[]>();
+    /* This maps the groupId to the individual callback for the set of messages */
+    private Map<String, SingleQuorumResponseHandler> handlers_ = new HashMap<String, SingleQuorumResponseHandler>();
+    /* This should hold all the responses for the keys */
+    private List<Row> responses_ = new ArrayList<Row>();
+    private AtomicBoolean done_ = new AtomicBoolean(false);
+    
+    /**
+     * This is used to handle the responses from the individual messages
+     * that are sent out to the replicas.
+     * @author alakshman
+     *
+    */
+    private class SingleQuorumResponseHandler implements IAsyncCallback
+    {
+        private Lock lock_ = new ReentrantLock();
+        private IResponseResolver<Row> responseResolver_;
+        private List<Message> responses_ = new ArrayList<Message>();
+        
+        SingleQuorumResponseHandler(IResponseResolver<Row> responseResolver)
+        {
+            responseResolver_ = responseResolver;
+        }
+        
+        public void attachContext(Object o)
+        {
+            throw new UnsupportedOperationException("This operation is not supported in this implementation");
+        }
+        
+        public void response(Message response)
+        {
+            lock_.lock();
+            try
+            {
+                responses_.add(response);
+                int majority = (DatabaseDescriptor.getReplicationFactor() >> 1) + 1;                            
+                if ( responses_.size() >= majority && responseResolver_.isDataPresent(responses_))
+                {
+                    onCompletion();               
+                }
+            }
+            catch ( IOException ex )
+            {
+                logger_.info( LogUtil.throwableToString(ex) );
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+        }
+        
+        private void onCompletion() throws IOException
+        {
+            try
+            {
+                Row row = responseResolver_.resolve(responses_);
+                MultiQuorumResponseHandler.this.onCompleteResponse(row);
+            }
+            catch ( DigestMismatchException ex )
+            {
+                /* 
+                 * The DigestMismatchException has the key for which the mismatch
+                 * occurred bundled in it as context 
+                */
+                String key = ex.getMessage();
+                onDigestMismatch(key);
+            }
+        }
+        
+        /**
+         * This method is invoked on a digest match. We pass in the key
+         * in order to retrieve the appropriate data message that needs
+         * to be sent out to the replicas. 
+         * 
+         * @param key for which the mismatch occurred.
+        */
+        private void onDigestMismatch(String key) throws IOException
+        {
+            if ( DatabaseDescriptor.getConsistencyCheck())
+            {                                
+                ReadCommand readCommand = readMessages_.get(key);
+                readCommand.setDigestQuery(false);
+                Message messageRepair = readCommand.makeReadMessage();
+                EndPoint[] endpoints = MultiQuorumResponseHandler.this.endpoints_.get(readCommand.key);
+                Message[][] messages = new Message[][]{ {messageRepair, messageRepair, messageRepair} };
+                EndPoint[][] epList = new EndPoint[][]{ endpoints };
+                MessagingService.getMessagingInstance().sendRR(messages, epList, MultiQuorumResponseHandler.this);                
+            }
+        }
+    }
+    
+    public MultiQuorumResponseHandler(Map<String, ReadCommand> readMessages, Map<String, EndPoint[]> endpoints)
+    {        
+        condition_ = lock_.newCondition();
+        readMessages_ = readMessages;
+        endpoints_ = endpoints;
+    }
+    
+    public Row[] get() throws TimeoutException
+    {
+        long startTime = System.currentTimeMillis();
+        lock_.lock();
+        try
+        {            
+            boolean bVal = true;            
+            try
+            {
+                if ( !done_.get() )
+                {                   
+                    bVal = condition_.await(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
+                }
+            }
+            catch ( InterruptedException ex )
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug( LogUtil.throwableToString(ex) );
+            }
+            
+            if ( !bVal && !done_.get() )
+            {
+                StringBuilder sb = new StringBuilder("");
+                for ( Row row : responses_ )
+                {
+                    sb.append(row.key());
+                    sb.append(":");
+                }                
+                throw new TimeoutException("Operation timed out - received only " +  responses_.size() + " responses from " + sb.toString() + " .");
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+        
+        logger_.info("MultiQuorumResponseHandler: " + (System.currentTimeMillis() - startTime) + " ms.");
+        return responses_.toArray( new Row[0] );
+    }
+    
+    /**
+     * Invoked when a complete response has been obtained
+     * for one of the sub-groups a.k.a keys for the query 
+     * has been performed.
+     * 
+     * @param row obtained as a result of the response.
+     */
+    void onCompleteResponse(Row row)
+    {        
+        if ( !done_.get() )
+        {
+            responses_.add(row);
+            if ( responses_.size() == readMessages_.size() )
+            {
+                done_.set(true);
+                condition_.signal();                
+            }
+        }
+    }
+    
+    /**
+     * The handler of the response message that has been
+     * sent by one of the replicas for one of the keys.
+     * 
+     * @param message the response message for one of the
+     *        message that we sent out.
+     */
+    public void response(Message message)
+    {
+        lock_.lock();
+        try
+        {
+            SingleQuorumResponseHandler handler = handlers_.get(message.getMessageId());
+            handler.response(message);
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+    }
+    
+    /**
+     * The context that is passed in for the query of
+     * multiple keys in the system. For each message 
+     * id in the context register a callback handler 
+     * for the same. This is done so that all responses
+     * for a given key use the same callback handler.
+     * 
+     * @param o the context which is an array of strings
+     *        corresponding to the message id's for each
+     *        key.
+     */
+    public void attachContext(Object o)
+    {
+        String[] gids = (String[])o;
+        for ( String gid : gids )
+        {
+            IResponseResolver<Row> responseResolver = new ReadResponseResolver();
+            SingleQuorumResponseHandler handler = new SingleQuorumResponseHandler(responseResolver);
+            handlers_.put(gid, handler);
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/service/QuorumResponseHandler.java b/src/java/org/apache/cassandra/service/QuorumResponseHandler.java
index c8dae08575..4566388067 100644
--- a/src/java/org/apache/cassandra/service/QuorumResponseHandler.java
+++ b/src/java/org/apache/cassandra/service/QuorumResponseHandler.java
@@ -1,126 +1,126 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.util.List;
-import java.util.ArrayList;
-import java.util.concurrent.atomic.AtomicBoolean;
-import java.util.concurrent.locks.*;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.TimeoutException;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.net.IAsyncCallback;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class QuorumResponseHandler<T> implements IAsyncCallback
-{
-    private static Logger logger_ = Logger.getLogger( QuorumResponseHandler.class );
-    private Lock lock_ = new ReentrantLock();
-    private Condition condition_;
-    private int responseCount_;
-    private List<Message> responses_ = new ArrayList<Message>();
-    private IResponseResolver<T> responseResolver_;
-    private AtomicBoolean done_ = new AtomicBoolean(false);
-
-    public QuorumResponseHandler(int responseCount, IResponseResolver<T> responseResolver) throws InvalidRequestException
-    {
-        if (responseCount > DatabaseDescriptor.getReplicationFactor())
-            throw new InvalidRequestException("Cannot block for more than the replication factor of " + DatabaseDescriptor.getReplicationFactor());
-        if (responseCount < 1)
-            throw new InvalidRequestException("Cannot block for less than one replica");
-        condition_ = lock_.newCondition();
-        responseCount_ = responseCount;
-        responseResolver_ =  responseResolver;
-    }
-    
-    public T get() throws TimeoutException, DigestMismatchException
-    {
-    	lock_.lock();
-        try
-        {            
-            boolean bVal = true;            
-            try
-            {
-            	if ( !done_.get() )
-                {            		
-            		bVal = condition_.await(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
-                }
-            }
-            catch ( InterruptedException ex )
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug( LogUtil.throwableToString(ex) );
-            }
-            
-            if ( !bVal && !done_.get() )
-            {
-                StringBuilder sb = new StringBuilder("");
-                for ( Message message : responses_ )
-                {
-                    sb.append(message.getFrom());                    
-                }                
-                throw new TimeoutException("Operation timed out - received only " +  responses_.size() + " responses from " + sb.toString() + " .");
-            }
-        }
-        finally
-        {
-            lock_.unlock();
-            for(Message response : responses_)
-            {
-            	MessagingService.removeRegisteredCallback( response.getMessageId() );
-            }
-        }
-
-    	return responseResolver_.resolve( responses_);
-    }
-    
-    public void response(Message message)
-    {
-        lock_.lock();
-        try
-        {            
-            if ( !done_.get() )
-            {
-            	responses_.add( message );
-            	if ( responses_.size() >= responseCount_ && responseResolver_.isDataPresent(responses_))
-            	{
-            		done_.set(true);
-            		condition_.signal();            	
-            	}
-            }
-        }
-        finally
-        {
-            lock_.unlock();
-        }
-    }
-    
-    public void attachContext(Object o)
-    {
-        throw new UnsupportedOperationException("This operation is not supported in this version of the callback handler");
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.util.List;
+import java.util.ArrayList;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.*;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.net.IAsyncCallback;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class QuorumResponseHandler<T> implements IAsyncCallback
+{
+    private static Logger logger_ = Logger.getLogger( QuorumResponseHandler.class );
+    private Lock lock_ = new ReentrantLock();
+    private Condition condition_;
+    private int responseCount_;
+    private List<Message> responses_ = new ArrayList<Message>();
+    private IResponseResolver<T> responseResolver_;
+    private AtomicBoolean done_ = new AtomicBoolean(false);
+
+    public QuorumResponseHandler(int responseCount, IResponseResolver<T> responseResolver) throws InvalidRequestException
+    {
+        if (responseCount > DatabaseDescriptor.getReplicationFactor())
+            throw new InvalidRequestException("Cannot block for more than the replication factor of " + DatabaseDescriptor.getReplicationFactor());
+        if (responseCount < 1)
+            throw new InvalidRequestException("Cannot block for less than one replica");
+        condition_ = lock_.newCondition();
+        responseCount_ = responseCount;
+        responseResolver_ =  responseResolver;
+    }
+    
+    public T get() throws TimeoutException, DigestMismatchException
+    {
+    	lock_.lock();
+        try
+        {            
+            boolean bVal = true;            
+            try
+            {
+            	if ( !done_.get() )
+                {            		
+            		bVal = condition_.await(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
+                }
+            }
+            catch ( InterruptedException ex )
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug( LogUtil.throwableToString(ex) );
+            }
+            
+            if ( !bVal && !done_.get() )
+            {
+                StringBuilder sb = new StringBuilder("");
+                for ( Message message : responses_ )
+                {
+                    sb.append(message.getFrom());                    
+                }                
+                throw new TimeoutException("Operation timed out - received only " +  responses_.size() + " responses from " + sb.toString() + " .");
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+            for(Message response : responses_)
+            {
+            	MessagingService.removeRegisteredCallback( response.getMessageId() );
+            }
+        }
+
+    	return responseResolver_.resolve( responses_);
+    }
+    
+    public void response(Message message)
+    {
+        lock_.lock();
+        try
+        {            
+            if ( !done_.get() )
+            {
+            	responses_.add( message );
+            	if ( responses_.size() >= responseCount_ && responseResolver_.isDataPresent(responses_))
+            	{
+            		done_.set(true);
+            		condition_.signal();            	
+            	}
+            }
+        }
+        finally
+        {
+            lock_.unlock();
+        }
+    }
+    
+    public void attachContext(Object o)
+    {
+        throw new UnsupportedOperationException("This operation is not supported in this version of the callback handler");
+    }
+}
diff --git a/src/java/org/apache/cassandra/service/ReadRepairManager.java b/src/java/org/apache/cassandra/service/ReadRepairManager.java
index 73a47dd991..8ad2cc23a8 100644
--- a/src/java/org/apache/cassandra/service/ReadRepairManager.java
+++ b/src/java/org/apache/cassandra/service/ReadRepairManager.java
@@ -1,125 +1,125 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.io.FileOutputStream;
-import java.io.IOException;
-import java.util.concurrent.locks.*;
-
-import org.apache.cassandra.db.Column;
-import org.apache.cassandra.db.ColumnFamily;
-import org.apache.cassandra.db.RowMutation;
-import org.apache.cassandra.db.RowMutationMessage;
-import org.apache.cassandra.db.SuperColumn;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.Header;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.utils.Cachetable;
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.utils.ICacheExpungeHook;
-import org.apache.cassandra.utils.ICachetable;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-
-
-/*
- * This class manages the read repairs . This is a singleton class
- * it basically uses the cache table construct to schedule writes that have to be 
- * made for read repairs. 
- * A cachetable is created which wakes up every n  milliseconds specified by 
- * expirationTimeInMillis and calls a global hook function on pending entries 
- * This function basically sends the message to the appropriate servers to update them
- * with the latest changes.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-class ReadRepairManager
-{
-    private static Logger logger_ = Logger.getLogger(ReadRepairManager.class);
-	private static final long expirationTimeInMillis = 2000;
-	private static Lock lock_ = new ReentrantLock();
-	private static ReadRepairManager self_ = null;
-
-	/*
-	 * This is the internal class which actually
-	 * implements the global hook function called by the read repair manager
-	 */
-	static class ReadRepairPerformer implements
-			ICacheExpungeHook<String, Message>
-	{
-		/*
-		 * The hook function which takes the end point and the row mutation that 
-		 * needs to be sent to the end point in order 
-		 * to perform read repair.
-		 */
-		public void callMe(String target,
-				Message message)
-		{
-			String[] pieces = FBUtilities.strip(target, ":");
-			EndPoint to = new EndPoint(pieces[0], Integer.parseInt(pieces[1]));
-			MessagingService.getMessagingInstance().sendOneWay(message, to);			
-		}
-
-	}
-
-	private ICachetable<String, Message> readRepairTable_ = new Cachetable<String, Message>(expirationTimeInMillis, new ReadRepairManager.ReadRepairPerformer());
-
-	protected ReadRepairManager()
-	{
-
-	}
-
-	public  static ReadRepairManager instance()
-	{
-		if (self_ == null)
-		{
-            lock_.lock();
-            try
-            {
-                if ( self_ == null )
-                    self_ = new ReadRepairManager();
-            }
-            finally
-            {
-                lock_.unlock();
-            }
-		}
-		return self_;
-	}
-
-	/*
-	 * Schedules a read repair.
-	 * @param target endpoint on which the read repair should happen
-	 * @param rowMutationMessage the row mutation message that has the repaired row.
-	 */
-	public void schedule(EndPoint target, RowMutationMessage rowMutationMessage)
-	{
-        try
-        {
-            Message message = rowMutationMessage.makeRowMutationMessage(StorageService.readRepairVerbHandler_);
-    		String key = target + ":" + message.getMessageId();
-    		readRepairTable_.put(key, message);
-        }
-        catch ( IOException ex )
-        {
-            logger_.error(LogUtil.throwableToString(ex));
-        }
-	}
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.FileOutputStream;
+import java.io.IOException;
+import java.util.concurrent.locks.*;
+
+import org.apache.cassandra.db.Column;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.RowMutationMessage;
+import org.apache.cassandra.db.SuperColumn;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Header;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.Cachetable;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.ICacheExpungeHook;
+import org.apache.cassandra.utils.ICachetable;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+
+/*
+ * This class manages the read repairs . This is a singleton class
+ * it basically uses the cache table construct to schedule writes that have to be 
+ * made for read repairs. 
+ * A cachetable is created which wakes up every n  milliseconds specified by 
+ * expirationTimeInMillis and calls a global hook function on pending entries 
+ * This function basically sends the message to the appropriate servers to update them
+ * with the latest changes.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+class ReadRepairManager
+{
+    private static Logger logger_ = Logger.getLogger(ReadRepairManager.class);
+	private static final long expirationTimeInMillis = 2000;
+	private static Lock lock_ = new ReentrantLock();
+	private static ReadRepairManager self_ = null;
+
+	/*
+	 * This is the internal class which actually
+	 * implements the global hook function called by the read repair manager
+	 */
+	static class ReadRepairPerformer implements
+			ICacheExpungeHook<String, Message>
+	{
+		/*
+		 * The hook function which takes the end point and the row mutation that 
+		 * needs to be sent to the end point in order 
+		 * to perform read repair.
+		 */
+		public void callMe(String target,
+				Message message)
+		{
+			String[] pieces = FBUtilities.strip(target, ":");
+			EndPoint to = new EndPoint(pieces[0], Integer.parseInt(pieces[1]));
+			MessagingService.getMessagingInstance().sendOneWay(message, to);			
+		}
+
+	}
+
+	private ICachetable<String, Message> readRepairTable_ = new Cachetable<String, Message>(expirationTimeInMillis, new ReadRepairManager.ReadRepairPerformer());
+
+	protected ReadRepairManager()
+	{
+
+	}
+
+	public  static ReadRepairManager instance()
+	{
+		if (self_ == null)
+		{
+            lock_.lock();
+            try
+            {
+                if ( self_ == null )
+                    self_ = new ReadRepairManager();
+            }
+            finally
+            {
+                lock_.unlock();
+            }
+		}
+		return self_;
+	}
+
+	/*
+	 * Schedules a read repair.
+	 * @param target endpoint on which the read repair should happen
+	 * @param rowMutationMessage the row mutation message that has the repaired row.
+	 */
+	public void schedule(EndPoint target, RowMutationMessage rowMutationMessage)
+	{
+        try
+        {
+            Message message = rowMutationMessage.makeRowMutationMessage(StorageService.readRepairVerbHandler_);
+    		String key = target + ":" + message.getMessageId();
+    		readRepairTable_.put(key, message);
+        }
+        catch ( IOException ex )
+        {
+            logger_.error(LogUtil.throwableToString(ex));
+        }
+	}
+}
diff --git a/src/java/org/apache/cassandra/service/ReadResponseResolver.java b/src/java/org/apache/cassandra/service/ReadResponseResolver.java
index 5129da989a..7d13af79eb 100644
--- a/src/java/org/apache/cassandra/service/ReadResponseResolver.java
+++ b/src/java/org/apache/cassandra/service/ReadResponseResolver.java
@@ -1,177 +1,177 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.cassandra.db.ColumnFamily;
-import org.apache.cassandra.db.ReadResponse;
-import org.apache.cassandra.db.Row;
-import org.apache.cassandra.db.RowMutation;
-import org.apache.cassandra.db.RowMutationMessage;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-
-/**
- * This class is used by all read functions and is called by the Quorum 
- * when at least a few of the servers (few is specified in Quorum)
- * have sent the response . The resolve function then schedules read repair 
- * and resolution of read data from the various servers.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public class ReadResponseResolver implements IResponseResolver<Row>
-{
-	private static Logger logger_ = Logger.getLogger(WriteResponseResolver.class);
-
-	/*
-	 * This method for resolving read data should look at the timestamps of each
-	 * of the columns that are read and should pick up columns with the latest
-	 * timestamp. For those columns where the timestamp is not the latest a
-	 * repair request should be scheduled.
-	 * 
-	 */
-	public Row resolve(List<Message> responses) throws DigestMismatchException
-	{
-        long startTime = System.currentTimeMillis();
-		Row retRow = null;
-		List<Row> rowList = new ArrayList<Row>();
-		List<EndPoint> endPoints = new ArrayList<EndPoint>();
-		String key = null;
-		String table = null;
-		byte[] digest = new byte[0];
-		boolean isDigestQuery = false;
-        
-        /*
-		 * Populate the list of rows from each of the messages
-		 * Check to see if there is a digest query. If a digest 
-         * query exists then we need to compare the digest with 
-         * the digest of the data that is received.
-        */
-        DataInputBuffer bufIn = new DataInputBuffer();
-		for (Message response : responses)
-		{					            
-            byte[] body = response.getMessageBody();
-            bufIn.reset(body, body.length);
-            try
-            {
-                long start = System.currentTimeMillis();
-                ReadResponse result = ReadResponse.serializer().deserialize(bufIn);
-                if (logger_.isDebugEnabled())
-                  logger_.debug( "Response deserialization time : " + (System.currentTimeMillis() - start) + " ms.");
-    			if(!result.isDigestQuery())
-    			{
-    				rowList.add(result.row());
-    				endPoints.add(response.getFrom());
-    				key = result.row().key();
-    				table = result.row().getTable();
-    			}
-    			else
-    			{
-    				digest = result.digest();
-    				isDigestQuery = true;
-    			}
-            }
-            catch( IOException ex )
-            {
-                logger_.info(LogUtil.throwableToString(ex));
-            }
-		}
-		// If there was a digest query compare it with all the data digests 
-		// If there is a mismatch then throw an exception so that read repair can happen.
-		if(isDigestQuery)
-		{
-			for(Row row: rowList)
-			{
-				if( !Arrays.equals(row.digest(), digest) )
-				{
-                    /* Wrap the key as the context in this exception */
-					throw new DigestMismatchException(row.key());
-				}
-			}
-		}
-		
-        /* If the rowList is empty then we had some exception above. */
-        if ( rowList.size() == 0 )
-        {
-            return retRow;
-        }
-        
-        /* Now calculate the resolved row */
-		retRow = new Row(table, key);
-		for (int i = 0 ; i < rowList.size(); i++)
-		{
-			retRow.repair(rowList.get(i));			
-		}
-
-        // At  this point  we have the return row .
-		// Now we need to calculate the difference 
-		// so that we can schedule read repairs 
-		for (int i = 0 ; i < rowList.size(); i++)
-		{
-			// since retRow is the resolved row it can be used as the super set
-			Row diffRow = rowList.get(i).diff(retRow);
-			if(diffRow == null) // no repair needs to happen
-				continue;
-			// create the row mutation message based on the diff and schedule a read repair 
-			RowMutation rowMutation = new RowMutation(table, key);            			
-	        for (ColumnFamily cf : diffRow.getColumnFamilies())
-	        {
-	            rowMutation.add(cf);
-	        }
-            RowMutationMessage rowMutationMessage = new RowMutationMessage(rowMutation);
-	        ReadRepairManager.instance().schedule(endPoints.get(i),rowMutationMessage);
-		}
-        logger_.info("resolve: " + (System.currentTimeMillis() - startTime) + " ms.");
-		return retRow;
-	}
-
-	public boolean isDataPresent(List<Message> responses)
-	{
-		boolean isDataPresent = false;
-		for (Message response : responses)
-		{
-            byte[] body = response.getMessageBody();
-			DataInputBuffer bufIn = new DataInputBuffer();
-            bufIn.reset(body, body.length);
-            try
-            {
-    			ReadResponse result = ReadResponse.serializer().deserialize(bufIn);
-    			if(!result.isDigestQuery())
-    			{
-    				isDataPresent = true;
-    			}
-                bufIn.close();
-            }
-            catch(IOException ex)
-            {
-                logger_.info(LogUtil.throwableToString(ex));
-            }                        
-		}
-		return isDataPresent;
-	}
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.ReadResponse;
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.RowMutation;
+import org.apache.cassandra.db.RowMutationMessage;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+
+/**
+ * This class is used by all read functions and is called by the Quorum 
+ * when at least a few of the servers (few is specified in Quorum)
+ * have sent the response . The resolve function then schedules read repair 
+ * and resolution of read data from the various servers.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class ReadResponseResolver implements IResponseResolver<Row>
+{
+	private static Logger logger_ = Logger.getLogger(WriteResponseResolver.class);
+
+	/*
+	 * This method for resolving read data should look at the timestamps of each
+	 * of the columns that are read and should pick up columns with the latest
+	 * timestamp. For those columns where the timestamp is not the latest a
+	 * repair request should be scheduled.
+	 * 
+	 */
+	public Row resolve(List<Message> responses) throws DigestMismatchException
+	{
+        long startTime = System.currentTimeMillis();
+		Row retRow = null;
+		List<Row> rowList = new ArrayList<Row>();
+		List<EndPoint> endPoints = new ArrayList<EndPoint>();
+		String key = null;
+		String table = null;
+		byte[] digest = new byte[0];
+		boolean isDigestQuery = false;
+        
+        /*
+		 * Populate the list of rows from each of the messages
+		 * Check to see if there is a digest query. If a digest 
+         * query exists then we need to compare the digest with 
+         * the digest of the data that is received.
+        */
+        DataInputBuffer bufIn = new DataInputBuffer();
+		for (Message response : responses)
+		{					            
+            byte[] body = response.getMessageBody();
+            bufIn.reset(body, body.length);
+            try
+            {
+                long start = System.currentTimeMillis();
+                ReadResponse result = ReadResponse.serializer().deserialize(bufIn);
+                if (logger_.isDebugEnabled())
+                  logger_.debug( "Response deserialization time : " + (System.currentTimeMillis() - start) + " ms.");
+    			if(!result.isDigestQuery())
+    			{
+    				rowList.add(result.row());
+    				endPoints.add(response.getFrom());
+    				key = result.row().key();
+    				table = result.row().getTable();
+    			}
+    			else
+    			{
+    				digest = result.digest();
+    				isDigestQuery = true;
+    			}
+            }
+            catch( IOException ex )
+            {
+                logger_.info(LogUtil.throwableToString(ex));
+            }
+		}
+		// If there was a digest query compare it with all the data digests 
+		// If there is a mismatch then throw an exception so that read repair can happen.
+		if(isDigestQuery)
+		{
+			for(Row row: rowList)
+			{
+				if( !Arrays.equals(row.digest(), digest) )
+				{
+                    /* Wrap the key as the context in this exception */
+					throw new DigestMismatchException(row.key());
+				}
+			}
+		}
+		
+        /* If the rowList is empty then we had some exception above. */
+        if ( rowList.size() == 0 )
+        {
+            return retRow;
+        }
+        
+        /* Now calculate the resolved row */
+		retRow = new Row(table, key);
+		for (int i = 0 ; i < rowList.size(); i++)
+		{
+			retRow.repair(rowList.get(i));			
+		}
+
+        // At  this point  we have the return row .
+		// Now we need to calculate the difference 
+		// so that we can schedule read repairs 
+		for (int i = 0 ; i < rowList.size(); i++)
+		{
+			// since retRow is the resolved row it can be used as the super set
+			Row diffRow = rowList.get(i).diff(retRow);
+			if(diffRow == null) // no repair needs to happen
+				continue;
+			// create the row mutation message based on the diff and schedule a read repair 
+			RowMutation rowMutation = new RowMutation(table, key);            			
+	        for (ColumnFamily cf : diffRow.getColumnFamilies())
+	        {
+	            rowMutation.add(cf);
+	        }
+            RowMutationMessage rowMutationMessage = new RowMutationMessage(rowMutation);
+	        ReadRepairManager.instance().schedule(endPoints.get(i),rowMutationMessage);
+		}
+        logger_.info("resolve: " + (System.currentTimeMillis() - startTime) + " ms.");
+		return retRow;
+	}
+
+	public boolean isDataPresent(List<Message> responses)
+	{
+		boolean isDataPresent = false;
+		for (Message response : responses)
+		{
+            byte[] body = response.getMessageBody();
+			DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length);
+            try
+            {
+    			ReadResponse result = ReadResponse.serializer().deserialize(bufIn);
+    			if(!result.isDigestQuery())
+    			{
+    				isDataPresent = true;
+    			}
+                bufIn.close();
+            }
+            catch(IOException ex)
+            {
+                logger_.info(LogUtil.throwableToString(ex));
+            }                        
+		}
+		return isDataPresent;
+	}
+}
diff --git a/src/java/org/apache/cassandra/service/StorageLoadBalancer.java b/src/java/org/apache/cassandra/service/StorageLoadBalancer.java
index 62e70119ea..a45b712878 100644
--- a/src/java/org/apache/cassandra/service/StorageLoadBalancer.java
+++ b/src/java/org/apache/cassandra/service/StorageLoadBalancer.java
@@ -1,403 +1,403 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.io.Serializable;
-import java.util.HashMap;
-import java.util.Map;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.ScheduledThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.atomic.AtomicBoolean;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
-import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
-import org.apache.cassandra.concurrent.SingleThreadedStage;
-import org.apache.cassandra.concurrent.StageManager;
-import org.apache.cassandra.concurrent.ThreadFactoryImpl;
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.gms.ApplicationState;
-import org.apache.cassandra.gms.EndPointState;
-import org.apache.cassandra.gms.Gossiper;
-import org.apache.cassandra.gms.IEndPointStateChangeSubscriber;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-
-/*
- * The load balancing algorithm here is an implementation of
- * the algorithm as described in the paper "Scalable range query
- * processing for large-scale distributed database applications".
- * This class keeps track of load information across the system.
- * It registers itself with the Gossiper for ApplicationState namely
- * load information i.e number of requests processed w.r.t distinct
- * keys at an Endpoint. Monitor load information for a 5 minute
- * interval and then do load balancing operations if necessary.
- * 
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-final class StorageLoadBalancer implements IEndPointStateChangeSubscriber
-{
-    class LoadBalancer implements Runnable
-    {
-        LoadBalancer()
-        {
-            /* Copy the entries in loadInfo_ into loadInfo2_ and use it for all calculations */
-            loadInfo2_.putAll(loadInfo_);
-        }
-
-        /**
-         * Obtain a node which is a potential target. Start with
-         * the neighbours i.e either successor or predecessor.
-         * Send the target a MoveMessage. If the node cannot be
-         * relocated on the ring then we pick another candidate for
-         * relocation.
-        */        
-        public void run()
-        {
-            /*
-            int threshold = (int)(StorageLoadBalancer.ratio_ * averageSystemLoad());
-            int myLoad = localLoad();            
-            EndPoint predecessor = storageService_.getPredecessor(StorageService.getLocalStorageEndPoint());
-            if (logger_.isDebugEnabled())
-              logger_.debug("Trying to relocate the predecessor " + predecessor);
-            boolean value = tryThisNode(myLoad, threshold, predecessor);
-            if ( !value )
-            {
-                loadInfo2_.remove(predecessor);
-                EndPoint successor = storageService_.getSuccessor(StorageService.getLocalStorageEndPoint());
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Trying to relocate the successor " + successor);
-                value = tryThisNode(myLoad, threshold, successor);
-                if ( !value )
-                {
-                    loadInfo2_.remove(successor);
-                    while ( !loadInfo2_.isEmpty() )
-                    {
-                        EndPoint target = findARandomLightNode();
-                        if ( target != null )
-                        {
-                            if (logger_.isDebugEnabled())
-                              logger_.debug("Trying to relocate the random node " + target);
-                            value = tryThisNode(myLoad, threshold, target);
-                            if ( !value )
-                            {
-                                loadInfo2_.remove(target);
-                            }
-                            else
-                            {
-                                break;
-                            }
-                        }
-                        else
-                        {
-                            // No light nodes available - this is NOT good.
-                            logger_.warn("Not even a single lightly loaded node is available ...");
-                            break;
-                        }
-                    }
-
-                    loadInfo2_.clear();                    
-                     // If we are here and no node was available to
-                     // perform load balance with we need to report and bail.                    
-                    if ( !value )
-                    {
-                        logger_.warn("Load Balancing operations weren't performed for this node");
-                    }
-                }                
-            }
-            */        
-        }
-
-        /*
-        private boolean tryThisNode(int myLoad, int threshold, EndPoint target)
-        {
-            boolean value = false;
-            LoadInfo li = loadInfo2_.get(target);
-            int pLoad = li.count();
-            if ( ((myLoad + pLoad) >> 1) <= threshold )
-            {
-                //calculate the number of keys to be transferred
-                int keyCount = ( (myLoad - pLoad) >> 1 );
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Number of keys we attempt to transfer to " + target + " " + keyCount);
-                // Determine the token that the target should join at.         
-                BigInteger targetToken = BootstrapAndLbHelper.getTokenBasedOnPrimaryCount(keyCount);
-                // Send a MoveMessage and see if this node is relocateable
-                MoveMessage moveMessage = new MoveMessage(targetToken);
-                Message message = new Message(StorageService.getLocalStorageEndPoint(), StorageLoadBalancer.lbStage_, StorageLoadBalancer.moveMessageVerbHandler_, new Object[]{moveMessage});
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Sending a move message to " + target);
-                IAsyncResult result = MessagingService.getMessagingInstance().sendRR(message, target);
-                value = (Boolean)result.get()[0];
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Response for query to relocate " + target + " is " + value);
-            }
-            return value;
-        }
-        */
-    }
-
-    class MoveMessageVerbHandler implements IVerbHandler
-    {
-        public void doVerb(Message message)
-        {
-            Message reply = message.getReply(StorageService.getLocalStorageEndPoint(), new byte[] {(byte)(isMoveable_.get() ? 1 : 0)});
-            MessagingService.getMessagingInstance().sendOneWay(reply, message.getFrom());
-            if ( isMoveable_.get() )
-            {
-                // MoveMessage moveMessage = (MoveMessage)message.getMessageBody()[0];
-                /* Start the leave operation and join the ring at the position specified */
-                isMoveable_.set(false);
-            }
-        }
-    }
-
-    private static final Logger logger_ = Logger.getLogger(StorageLoadBalancer.class);
-    private static final String lbStage_ = "LOAD-BALANCER-STAGE";
-    private static final String moveMessageVerbHandler_ = "MOVE-MESSAGE-VERB-HANDLER";
-    /* time to delay in minutes the actual load balance procedure if heavily loaded */
-    private static final int delay_ = 5;
-    /* Ratio of highest loaded node and the average load. */
-    private static final double ratio_ = 1.5;
-
-    private StorageService storageService_;
-    /* this indicates whether this node is already helping someone else */
-    private AtomicBoolean isMoveable_ = new AtomicBoolean(false);
-    private Map<EndPoint, LoadInfo> loadInfo_ = new HashMap<EndPoint, LoadInfo>();
-    /* This map is a clone of the one above and is used for various calculations during LB operation */
-    private Map<EndPoint, LoadInfo> loadInfo2_ = new HashMap<EndPoint, LoadInfo>();
-    /* This thread pool is used for initiating load balancing operations */
-    private ScheduledThreadPoolExecutor lb_ = new DebuggableScheduledThreadPoolExecutor(
-            1,
-            new ThreadFactoryImpl("LB-OPERATIONS")
-            );
-    /* This thread pool is used by target node to leave the ring. */
-    private ExecutorService lbOperations_ = new DebuggableThreadPoolExecutor("LB-TARGET");
-
-    StorageLoadBalancer(StorageService storageService)
-    {
-        storageService_ = storageService;
-        /* register the load balancer stage */
-        StageManager.registerStage(StorageLoadBalancer.lbStage_, new SingleThreadedStage(StorageLoadBalancer.lbStage_));
-        /* register the load balancer verb handler */
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageLoadBalancer.moveMessageVerbHandler_, new MoveMessageVerbHandler());
-    }
-
-    public void start()
-    {
-        /* Register with the Gossiper for EndPointState notifications */
-        Gossiper.instance().register(this);
-    }
-
-    public void onChange(EndPoint endpoint, EndPointState epState)
-    {
-        // load information for this specified endpoint for load balancing 
-        ApplicationState loadInfoState = epState.getApplicationState(LoadDisseminator.loadInfo_);
-        if ( loadInfoState != null )
-        {
-            String lInfoState = loadInfoState.getState();
-            LoadInfo lInfo = new LoadInfo(lInfoState);
-            loadInfo_.put(endpoint, lInfo);
-            
-            /*
-            int currentLoad = Integer.parseInt(loadInfoState.getState());
-            // update load information for this endpoint
-            loadInfo_.put(endpoint, currentLoad);
-
-            // clone load information to perform calculations
-            loadInfo2_.putAll(loadInfo_);
-            // Perform the analysis for load balance operations
-            if ( isHeavyNode() )
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug(StorageService.getLocalStorageEndPoint() + " is a heavy node with load " + localLoad());
-                // lb_.schedule( new LoadBalancer(), StorageLoadBalancer.delay_, TimeUnit.MINUTES );
-            }
-            */
-        }       
-    }
-
-    /*
-     * Load information associated with a given endpoint.
-    */
-    LoadInfo getLoad(EndPoint ep)
-    {
-        LoadInfo li = loadInfo_.get(ep);        
-        return li;        
-    }
-
-    /*
-    private boolean isMoveable()
-    {
-        if ( !isMoveable_.get() )
-            return false;
-        int myload = localLoad();
-        EndPoint successor = storageService_.getSuccessor(StorageService.getLocalStorageEndPoint());
-        LoadInfo li = loadInfo2_.get(successor);
-        // "load" is NULL means that the successor node has not
-        // yet gossiped its load information. We should return
-        // false in this case since we want to err on the side
-        // of caution.
-        if ( li == null )
-            return false;
-        else
-        {            
-            if ( ( myload + li.count() ) > StorageLoadBalancer.ratio_*averageSystemLoad() )
-                return false;
-            else
-                return true;
-        }
-    }
-    */
-
-    /*
-    private int localLoad()
-    {
-        LoadInfo value = loadInfo2_.get(StorageService.getLocalStorageEndPoint());
-        return (value == null) ? 0 : value.count();
-    }
-    */
-
-    /*
-    private int averageSystemLoad()
-    {
-        int nodeCount = loadInfo2_.size();
-        Set<EndPoint> nodes = loadInfo2_.keySet();
-
-        int systemLoad = 0;
-        for ( EndPoint node : nodes )
-        {
-            LoadInfo load = loadInfo2_.get(node);
-            if ( load != null )
-                systemLoad += load.count();
-        }
-        int averageLoad = (nodeCount > 0) ? (systemLoad / nodeCount) : 0;
-        if (logger_.isDebugEnabled())
-          logger_.debug("Average system load should be " + averageLoad);
-        return averageLoad;
-    }
-    */
-    
-    /*
-    private boolean isHeavyNode()
-    {
-        return ( localLoad() > ( StorageLoadBalancer.ratio_ * averageSystemLoad() ) );
-    }
-    */
-    
-    /*
-    private boolean isMoveable(EndPoint target)
-    {
-        int threshold = (int)(StorageLoadBalancer.ratio_ * averageSystemLoad());
-        if ( isANeighbour(target) )
-        {
-            // If the target is a neighbour then it is
-            // moveable if its
-            LoadInfo load = loadInfo2_.get(target);
-            if ( load == null )
-                return false;
-            else
-            {
-                int myload = localLoad();
-                int avgLoad = (load.count() + myload) >> 1;
-                if ( avgLoad <= threshold )
-                    return true;
-                else
-                    return false;
-            }
-        }
-        else
-        {
-            EndPoint successor = storageService_.getSuccessor(target);
-            LoadInfo sLoad = loadInfo2_.get(successor);
-            LoadInfo targetLoad = loadInfo2_.get(target);
-            if ( (sLoad.count() + targetLoad.count()) > threshold )
-                return false;
-            else
-                return true;
-        }
-    }
-    */
-
-    private boolean isANeighbour(EndPoint neighbour)
-    {
-        EndPoint predecessor = storageService_.getPredecessor(StorageService.getLocalStorageEndPoint());
-        if ( predecessor.equals(neighbour) )
-            return true;
-
-        EndPoint successor = storageService_.getSuccessor(StorageService.getLocalStorageEndPoint());
-        if ( successor.equals(neighbour) )
-            return true;
-
-        return false;
-    }
-
-    /*
-     * Determine the nodes that are lightly loaded. Choose at
-     * random one of the lightly loaded nodes and use them as
-     * a potential target for load balance.
-    */
-    /*
-    private EndPoint findARandomLightNode()
-    {
-        List<EndPoint> potentialCandidates = new ArrayList<EndPoint>();
-        Set<EndPoint> allTargets = loadInfo2_.keySet();
-        int avgLoad =  averageSystemLoad();
-
-        for( EndPoint target : allTargets )
-        {
-            LoadInfo load = loadInfo2_.get(target);
-            if ( load.count() < avgLoad )
-                potentialCandidates.add(target);
-        }
-
-        if ( potentialCandidates.size() > 0 )
-        {
-            Random random = new Random();
-            int index = random.nextInt(potentialCandidates.size());
-            return potentialCandidates.get(index);
-        }
-        return null;
-    }
-    */
-}
-
-class MoveMessage implements Serializable
-{
-    private Token targetToken_;
-
-    private MoveMessage()
-    {
-    }
-
-    MoveMessage(Token targetToken)
-    {
-        targetToken_ = targetToken;
-    }
-
-    Token getTargetToken()
-    {
-        return targetToken_;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.Serializable;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.ScheduledThreadPoolExecutor;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor;
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.SingleThreadedStage;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.gms.ApplicationState;
+import org.apache.cassandra.gms.EndPointState;
+import org.apache.cassandra.gms.Gossiper;
+import org.apache.cassandra.gms.IEndPointStateChangeSubscriber;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+
+/*
+ * The load balancing algorithm here is an implementation of
+ * the algorithm as described in the paper "Scalable range query
+ * processing for large-scale distributed database applications".
+ * This class keeps track of load information across the system.
+ * It registers itself with the Gossiper for ApplicationState namely
+ * load information i.e number of requests processed w.r.t distinct
+ * keys at an Endpoint. Monitor load information for a 5 minute
+ * interval and then do load balancing operations if necessary.
+ * 
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+final class StorageLoadBalancer implements IEndPointStateChangeSubscriber
+{
+    class LoadBalancer implements Runnable
+    {
+        LoadBalancer()
+        {
+            /* Copy the entries in loadInfo_ into loadInfo2_ and use it for all calculations */
+            loadInfo2_.putAll(loadInfo_);
+        }
+
+        /**
+         * Obtain a node which is a potential target. Start with
+         * the neighbours i.e either successor or predecessor.
+         * Send the target a MoveMessage. If the node cannot be
+         * relocated on the ring then we pick another candidate for
+         * relocation.
+        */        
+        public void run()
+        {
+            /*
+            int threshold = (int)(StorageLoadBalancer.ratio_ * averageSystemLoad());
+            int myLoad = localLoad();            
+            EndPoint predecessor = storageService_.getPredecessor(StorageService.getLocalStorageEndPoint());
+            if (logger_.isDebugEnabled())
+              logger_.debug("Trying to relocate the predecessor " + predecessor);
+            boolean value = tryThisNode(myLoad, threshold, predecessor);
+            if ( !value )
+            {
+                loadInfo2_.remove(predecessor);
+                EndPoint successor = storageService_.getSuccessor(StorageService.getLocalStorageEndPoint());
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Trying to relocate the successor " + successor);
+                value = tryThisNode(myLoad, threshold, successor);
+                if ( !value )
+                {
+                    loadInfo2_.remove(successor);
+                    while ( !loadInfo2_.isEmpty() )
+                    {
+                        EndPoint target = findARandomLightNode();
+                        if ( target != null )
+                        {
+                            if (logger_.isDebugEnabled())
+                              logger_.debug("Trying to relocate the random node " + target);
+                            value = tryThisNode(myLoad, threshold, target);
+                            if ( !value )
+                            {
+                                loadInfo2_.remove(target);
+                            }
+                            else
+                            {
+                                break;
+                            }
+                        }
+                        else
+                        {
+                            // No light nodes available - this is NOT good.
+                            logger_.warn("Not even a single lightly loaded node is available ...");
+                            break;
+                        }
+                    }
+
+                    loadInfo2_.clear();                    
+                     // If we are here and no node was available to
+                     // perform load balance with we need to report and bail.                    
+                    if ( !value )
+                    {
+                        logger_.warn("Load Balancing operations weren't performed for this node");
+                    }
+                }                
+            }
+            */        
+        }
+
+        /*
+        private boolean tryThisNode(int myLoad, int threshold, EndPoint target)
+        {
+            boolean value = false;
+            LoadInfo li = loadInfo2_.get(target);
+            int pLoad = li.count();
+            if ( ((myLoad + pLoad) >> 1) <= threshold )
+            {
+                //calculate the number of keys to be transferred
+                int keyCount = ( (myLoad - pLoad) >> 1 );
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Number of keys we attempt to transfer to " + target + " " + keyCount);
+                // Determine the token that the target should join at.         
+                BigInteger targetToken = BootstrapAndLbHelper.getTokenBasedOnPrimaryCount(keyCount);
+                // Send a MoveMessage and see if this node is relocateable
+                MoveMessage moveMessage = new MoveMessage(targetToken);
+                Message message = new Message(StorageService.getLocalStorageEndPoint(), StorageLoadBalancer.lbStage_, StorageLoadBalancer.moveMessageVerbHandler_, new Object[]{moveMessage});
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Sending a move message to " + target);
+                IAsyncResult result = MessagingService.getMessagingInstance().sendRR(message, target);
+                value = (Boolean)result.get()[0];
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Response for query to relocate " + target + " is " + value);
+            }
+            return value;
+        }
+        */
+    }
+
+    class MoveMessageVerbHandler implements IVerbHandler
+    {
+        public void doVerb(Message message)
+        {
+            Message reply = message.getReply(StorageService.getLocalStorageEndPoint(), new byte[] {(byte)(isMoveable_.get() ? 1 : 0)});
+            MessagingService.getMessagingInstance().sendOneWay(reply, message.getFrom());
+            if ( isMoveable_.get() )
+            {
+                // MoveMessage moveMessage = (MoveMessage)message.getMessageBody()[0];
+                /* Start the leave operation and join the ring at the position specified */
+                isMoveable_.set(false);
+            }
+        }
+    }
+
+    private static final Logger logger_ = Logger.getLogger(StorageLoadBalancer.class);
+    private static final String lbStage_ = "LOAD-BALANCER-STAGE";
+    private static final String moveMessageVerbHandler_ = "MOVE-MESSAGE-VERB-HANDLER";
+    /* time to delay in minutes the actual load balance procedure if heavily loaded */
+    private static final int delay_ = 5;
+    /* Ratio of highest loaded node and the average load. */
+    private static final double ratio_ = 1.5;
+
+    private StorageService storageService_;
+    /* this indicates whether this node is already helping someone else */
+    private AtomicBoolean isMoveable_ = new AtomicBoolean(false);
+    private Map<EndPoint, LoadInfo> loadInfo_ = new HashMap<EndPoint, LoadInfo>();
+    /* This map is a clone of the one above and is used for various calculations during LB operation */
+    private Map<EndPoint, LoadInfo> loadInfo2_ = new HashMap<EndPoint, LoadInfo>();
+    /* This thread pool is used for initiating load balancing operations */
+    private ScheduledThreadPoolExecutor lb_ = new DebuggableScheduledThreadPoolExecutor(
+            1,
+            new ThreadFactoryImpl("LB-OPERATIONS")
+            );
+    /* This thread pool is used by target node to leave the ring. */
+    private ExecutorService lbOperations_ = new DebuggableThreadPoolExecutor("LB-TARGET");
+
+    StorageLoadBalancer(StorageService storageService)
+    {
+        storageService_ = storageService;
+        /* register the load balancer stage */
+        StageManager.registerStage(StorageLoadBalancer.lbStage_, new SingleThreadedStage(StorageLoadBalancer.lbStage_));
+        /* register the load balancer verb handler */
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageLoadBalancer.moveMessageVerbHandler_, new MoveMessageVerbHandler());
+    }
+
+    public void start()
+    {
+        /* Register with the Gossiper for EndPointState notifications */
+        Gossiper.instance().register(this);
+    }
+
+    public void onChange(EndPoint endpoint, EndPointState epState)
+    {
+        // load information for this specified endpoint for load balancing 
+        ApplicationState loadInfoState = epState.getApplicationState(LoadDisseminator.loadInfo_);
+        if ( loadInfoState != null )
+        {
+            String lInfoState = loadInfoState.getState();
+            LoadInfo lInfo = new LoadInfo(lInfoState);
+            loadInfo_.put(endpoint, lInfo);
+            
+            /*
+            int currentLoad = Integer.parseInt(loadInfoState.getState());
+            // update load information for this endpoint
+            loadInfo_.put(endpoint, currentLoad);
+
+            // clone load information to perform calculations
+            loadInfo2_.putAll(loadInfo_);
+            // Perform the analysis for load balance operations
+            if ( isHeavyNode() )
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug(StorageService.getLocalStorageEndPoint() + " is a heavy node with load " + localLoad());
+                // lb_.schedule( new LoadBalancer(), StorageLoadBalancer.delay_, TimeUnit.MINUTES );
+            }
+            */
+        }       
+    }
+
+    /*
+     * Load information associated with a given endpoint.
+    */
+    LoadInfo getLoad(EndPoint ep)
+    {
+        LoadInfo li = loadInfo_.get(ep);        
+        return li;        
+    }
+
+    /*
+    private boolean isMoveable()
+    {
+        if ( !isMoveable_.get() )
+            return false;
+        int myload = localLoad();
+        EndPoint successor = storageService_.getSuccessor(StorageService.getLocalStorageEndPoint());
+        LoadInfo li = loadInfo2_.get(successor);
+        // "load" is NULL means that the successor node has not
+        // yet gossiped its load information. We should return
+        // false in this case since we want to err on the side
+        // of caution.
+        if ( li == null )
+            return false;
+        else
+        {            
+            if ( ( myload + li.count() ) > StorageLoadBalancer.ratio_*averageSystemLoad() )
+                return false;
+            else
+                return true;
+        }
+    }
+    */
+
+    /*
+    private int localLoad()
+    {
+        LoadInfo value = loadInfo2_.get(StorageService.getLocalStorageEndPoint());
+        return (value == null) ? 0 : value.count();
+    }
+    */
+
+    /*
+    private int averageSystemLoad()
+    {
+        int nodeCount = loadInfo2_.size();
+        Set<EndPoint> nodes = loadInfo2_.keySet();
+
+        int systemLoad = 0;
+        for ( EndPoint node : nodes )
+        {
+            LoadInfo load = loadInfo2_.get(node);
+            if ( load != null )
+                systemLoad += load.count();
+        }
+        int averageLoad = (nodeCount > 0) ? (systemLoad / nodeCount) : 0;
+        if (logger_.isDebugEnabled())
+          logger_.debug("Average system load should be " + averageLoad);
+        return averageLoad;
+    }
+    */
+    
+    /*
+    private boolean isHeavyNode()
+    {
+        return ( localLoad() > ( StorageLoadBalancer.ratio_ * averageSystemLoad() ) );
+    }
+    */
+    
+    /*
+    private boolean isMoveable(EndPoint target)
+    {
+        int threshold = (int)(StorageLoadBalancer.ratio_ * averageSystemLoad());
+        if ( isANeighbour(target) )
+        {
+            // If the target is a neighbour then it is
+            // moveable if its
+            LoadInfo load = loadInfo2_.get(target);
+            if ( load == null )
+                return false;
+            else
+            {
+                int myload = localLoad();
+                int avgLoad = (load.count() + myload) >> 1;
+                if ( avgLoad <= threshold )
+                    return true;
+                else
+                    return false;
+            }
+        }
+        else
+        {
+            EndPoint successor = storageService_.getSuccessor(target);
+            LoadInfo sLoad = loadInfo2_.get(successor);
+            LoadInfo targetLoad = loadInfo2_.get(target);
+            if ( (sLoad.count() + targetLoad.count()) > threshold )
+                return false;
+            else
+                return true;
+        }
+    }
+    */
+
+    private boolean isANeighbour(EndPoint neighbour)
+    {
+        EndPoint predecessor = storageService_.getPredecessor(StorageService.getLocalStorageEndPoint());
+        if ( predecessor.equals(neighbour) )
+            return true;
+
+        EndPoint successor = storageService_.getSuccessor(StorageService.getLocalStorageEndPoint());
+        if ( successor.equals(neighbour) )
+            return true;
+
+        return false;
+    }
+
+    /*
+     * Determine the nodes that are lightly loaded. Choose at
+     * random one of the lightly loaded nodes and use them as
+     * a potential target for load balance.
+    */
+    /*
+    private EndPoint findARandomLightNode()
+    {
+        List<EndPoint> potentialCandidates = new ArrayList<EndPoint>();
+        Set<EndPoint> allTargets = loadInfo2_.keySet();
+        int avgLoad =  averageSystemLoad();
+
+        for( EndPoint target : allTargets )
+        {
+            LoadInfo load = loadInfo2_.get(target);
+            if ( load.count() < avgLoad )
+                potentialCandidates.add(target);
+        }
+
+        if ( potentialCandidates.size() > 0 )
+        {
+            Random random = new Random();
+            int index = random.nextInt(potentialCandidates.size());
+            return potentialCandidates.get(index);
+        }
+        return null;
+    }
+    */
+}
+
+class MoveMessage implements Serializable
+{
+    private Token targetToken_;
+
+    private MoveMessage()
+    {
+    }
+
+    MoveMessage(Token targetToken)
+    {
+        targetToken_ = targetToken;
+    }
+
+    Token getTargetToken()
+    {
+        return targetToken_;
+    }
+}
diff --git a/src/java/org/apache/cassandra/service/StorageProxy.java b/src/java/org/apache/cassandra/service/StorageProxy.java
index c1cfc91f11..4663f45c95 100644
--- a/src/java/org/apache/cassandra/service/StorageProxy.java
+++ b/src/java/org/apache/cassandra/service/StorageProxy.java
@@ -1,686 +1,686 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-package org.apache.cassandra.service;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.Set;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.TimeoutException;
-import java.lang.management.ManagementFactory;
-
-import org.apache.commons.lang.StringUtils;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.*;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IAsyncResult;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.utils.TimedStatsDeque;
-import org.apache.log4j.Logger;
-
-import javax.management.MBeanServer;
-import javax.management.ObjectName;
-
-
-public class StorageProxy implements StorageProxyMBean
-{
-    private static Logger logger = Logger.getLogger(StorageProxy.class);
-
-    // mbean stuff
-    private static TimedStatsDeque readStats = new TimedStatsDeque(60000);
-    private static TimedStatsDeque rangeStats = new TimedStatsDeque(60000);
-    private static TimedStatsDeque writeStats = new TimedStatsDeque(60000);
-    private StorageProxy() {}
-    static
-    {
-        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
-        try
-        {
-            mbs.registerMBean(new StorageProxy(), new ObjectName("org.apache.cassandra.service:type=StorageProxy"));
-        }
-        catch (Exception e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    /**
-     * This method is responsible for creating Message to be
-     * sent over the wire to N replicas where some of the replicas
-     * may be hints.
-     */
-    private static Map<EndPoint, Message> createWriteMessages(RowMutation rm, Map<EndPoint, EndPoint> endpointMap) throws IOException
-    {
-		Map<EndPoint, Message> messageMap = new HashMap<EndPoint, Message>();
-		Message message = rm.makeRowMutationMessage();
-
-		for (Map.Entry<EndPoint, EndPoint> entry : endpointMap.entrySet())
-		{
-            EndPoint target = entry.getKey();
-            EndPoint hint = entry.getValue();
-            if ( !target.equals(hint) )
-			{
-				Message hintedMessage = rm.makeRowMutationMessage();
-				hintedMessage.addHeader(RowMutation.HINT, EndPoint.toBytes(hint) );
-				if (logger.isDebugEnabled())
-				    logger.debug("Sending the hint of " + hint.getHost() + " to " + target.getHost());
-				messageMap.put(target, hintedMessage);
-			}
-			else
-			{
-				messageMap.put(target, message);
-			}
-		}
-		return messageMap;
-    }
-    
-    /**
-     * Use this method to have this RowMutation applied
-     * across all replicas. This method will take care
-     * of the possibility of a replica being down and hint
-     * the data across to some other replica. 
-     * @param rm the mutation to be applied across the replicas
-    */
-    public static void insert(RowMutation rm)
-	{
-        /*
-         * Get the N nodes from storage service where the data needs to be
-         * replicated
-         * Construct a message for write
-         * Send them asynchronously to the replicas.
-        */
-
-        long startTime = System.currentTimeMillis();
-		try
-		{
-			Map<EndPoint, EndPoint> endpointMap = StorageService.instance().getNStorageEndPointMap(rm.key());
-			// TODO: throw a thrift exception if we do not have N nodes
-			Map<EndPoint, Message> messageMap = createWriteMessages(rm, endpointMap);
-			for (Map.Entry<EndPoint, Message> entry : messageMap.entrySet())
-			{
-                Message message = entry.getValue();
-                EndPoint endpoint = entry.getKey();
-                // Check if local and not hinted
-                byte[] hintedBytes = message.getHeader(RowMutation.HINT);
-                if (endpoint.equals(StorageService.getLocalStorageEndPoint())
-                        && !(hintedBytes!= null && hintedBytes.length>0))
-                {
-                    if (logger.isDebugEnabled())
-                        logger.debug("locally writing writing key " + rm.key()
-                                + " to " + endpoint);
-                    rm.apply();
-                } else
-                {
-                    if (logger.isDebugEnabled())
-                        logger.debug("insert writing key " + rm.key() + " to "
-                                + message.getMessageId() + "@" + endpoint);
-                	MessagingService.getMessagingInstance().sendOneWay(message, endpoint);
-                }
-			}
-		}
-        catch (IOException e)
-        {
-            throw new RuntimeException("error inserting key " + rm.key(), e);
-        }
-        finally
-        {
-            writeStats.add(System.currentTimeMillis() - startTime);
-        }
-    }
-    
-    public static void insertBlocking(RowMutation rm, int consistency_level) throws UnavailableException
-    {
-        long startTime = System.currentTimeMillis();
-        Message message = null;
-        try
-        {
-            message = rm.makeRowMutationMessage();
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException(e);
-        }
-        try
-        {
-            EndPoint[] endpoints = StorageService.instance().getNStorageEndPoint(rm.key());
-            if (endpoints.length < (DatabaseDescriptor.getReplicationFactor() / 2) + 1)
-            {
-                throw new UnavailableException();
-            }
-            int blockFor;
-            if (consistency_level == ConsistencyLevel.ONE)
-            {
-                blockFor = 1;
-            }
-            else if (consistency_level == ConsistencyLevel.QUORUM)
-            {
-                blockFor = (DatabaseDescriptor.getReplicationFactor() >> 1) + 1;
-            }
-            else if (consistency_level == ConsistencyLevel.ALL)
-            {
-                blockFor = DatabaseDescriptor.getReplicationFactor();
-            }
-            else
-            {
-                throw new UnsupportedOperationException("invalid consistency level " + consistency_level);
-            }
-            QuorumResponseHandler<Boolean> quorumResponseHandler = new QuorumResponseHandler<Boolean>(blockFor, new WriteResponseResolver());
-            if (logger.isDebugEnabled())
-                logger.debug("insertBlocking writing key " + rm.key() + " to " + message.getMessageId() + "@[" + StringUtils.join(endpoints, ", ") + "]");
-
-            MessagingService.getMessagingInstance().sendRR(message, endpoints, quorumResponseHandler);
-            if (!quorumResponseHandler.get())
-                throw new UnavailableException();
-        }
-        catch (Exception e)
-        {
-            logger.error("error writing key " + rm.key(), e);
-            throw new UnavailableException();
-        }
-        finally
-        {
-            writeStats.add(System.currentTimeMillis() - startTime);
-        }
-    }
-
-    public static void insertBlocking(RowMutation rm) throws UnavailableException
-    {
-        insertBlocking(rm, ConsistencyLevel.QUORUM);
-    }
-    
-    private static Map<String, Message> constructMessages(Map<String, ReadCommand> readMessages) throws IOException
-    {
-        Map<String, Message> messages = new HashMap<String, Message>();
-        Set<String> keys = readMessages.keySet();        
-        for ( String key : keys )
-        {
-            Message message = readMessages.get(key).makeReadMessage();
-            messages.put(key, message);
-        }        
-        return messages;
-    }
-    
-    private static IAsyncResult dispatchMessages(Map<String, EndPoint> endPoints, Map<String, Message> messages)
-    {
-        Set<String> keys = endPoints.keySet();
-        EndPoint[] eps = new EndPoint[keys.size()];
-        Message[] msgs  = new Message[keys.size()];
-        
-        int i = 0;
-        for ( String key : keys )
-        {
-            eps[i] = endPoints.get(key);
-            msgs[i] = messages.get(key);
-            ++i;
-        }
-        
-        IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(msgs, eps);
-        return iar;
-    }
-    
-    /**
-     * This is an implementation for the multiget version. 
-     * @param readMessages map of key --> ReadMessage to be sent
-     * @return map of key --> Row
-     * @throws IOException
-     * @throws TimeoutException
-     */
-    public static Map<String, Row> doReadProtocol(Map<String, ReadCommand> readMessages) throws IOException,TimeoutException
-    {
-        Map<String, Row> rows = new HashMap<String, Row>();
-        Set<String> keys = readMessages.keySet();
-        /* Find all the suitable endpoints for the keys */
-        Map<String, EndPoint> endPoints = StorageService.instance().findSuitableEndPoints(keys.toArray( new String[0] ));
-        /* Construct the messages to be sent out */
-        Map<String, Message> messages = constructMessages(readMessages);
-        /* Dispatch the messages to the respective endpoints */
-        IAsyncResult iar = dispatchMessages(endPoints, messages);        
-        List<byte[]> results = iar.multiget(2*DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
-        
-        for ( byte[] body : results )
-        {
-            DataInputBuffer bufIn = new DataInputBuffer();
-            bufIn.reset(body, body.length);
-            ReadResponse response = ReadResponse.serializer().deserialize(bufIn);
-            Row row = response.row();
-            rows.put(row.key(), row);
-        }        
-        return rows;
-    }
-
-    /**
-     * Read the data from one replica.  If there is no reply, read the data from another.  In the event we get
-     * the data we perform consistency checks and figure out if any repairs need to be done to the replicas.
-     * @param command the read to perform
-     * @return the row associated with command.key
-     * @throws Exception
-     */
-    private static Row weakReadRemote(ReadCommand command) throws IOException
-    {
-        EndPoint endPoint = StorageService.instance().findSuitableEndPoint(command.key);
-        assert endPoint != null;
-        Message message = command.makeReadMessage();
-        if (logger.isDebugEnabled())
-            logger.debug("weakreadremote reading " + command + " from " + message.getMessageId() + "@" + endPoint);
-        message.addHeader(ReadCommand.DO_REPAIR, ReadCommand.DO_REPAIR.getBytes());
-        IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(message, endPoint);
-        byte[] body;
-        try
-        {
-            body = iar.get(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
-        }
-        catch (TimeoutException e)
-        {
-            throw new RuntimeException("error reading key " + command.key, e);
-            // TODO retry to a different endpoint?
-        }
-        DataInputBuffer bufIn = new DataInputBuffer();
-        bufIn.reset(body, body.length);
-        ReadResponse response = ReadResponse.serializer().deserialize(bufIn);
-        return response.row();
-    }
-
-    /**
-     * Performs the actual reading of a row out of the StorageService, fetching
-     * a specific set of column names from a given column family.
-     */
-    public static Row readProtocol(ReadCommand command, int consistency_level)
-    throws IOException, TimeoutException, InvalidRequestException
-    {
-        long startTime = System.currentTimeMillis();
-
-        Row row;
-        EndPoint[] endpoints = StorageService.instance().getNStorageEndPoint(command.key);
-
-        if (consistency_level == ConsistencyLevel.ONE)
-        {
-            boolean foundLocal = Arrays.asList(endpoints).contains(StorageService.getLocalStorageEndPoint());
-            if (foundLocal)
-            {
-                row = weakReadLocal(command);
-            }
-            else
-            {
-                row = weakReadRemote(command);
-            }
-        }
-        else
-        {
-            assert consistency_level == ConsistencyLevel.QUORUM;
-            row = strongRead(command);
-        }
-
-        readStats.add(System.currentTimeMillis() - startTime);
-
-        return row;
-    }
-
-    public static Map<String, Row> readProtocol(String[] keys, ReadCommand readCommand, StorageService.ConsistencyLevel consistencyLevel) throws Exception
-    {
-        Map<String, Row> rows = new HashMap<String, Row>();        
-        switch ( consistencyLevel )
-        {
-            case WEAK:
-                rows = weakReadProtocol(keys, readCommand);
-                break;
-                
-            case STRONG:
-                rows = strongReadProtocol(keys, readCommand);
-                break;
-                
-            default:
-                rows = weakReadProtocol(keys, readCommand);
-                break;
-        }
-        return rows;
-    }
-
-    /**
-     * This is a multiget version of the above method.
-     * @param tablename
-     * @param keys
-     * @param columnFamily
-     * @param start
-     * @param count
-     * @return
-     * @throws IOException
-     * @throws TimeoutException
-     */
-    public static Map<String, Row> strongReadProtocol(String[] keys, ReadCommand readCommand) throws IOException, TimeoutException
-    {       
-        Map<String, Row> rows;
-        // TODO: throw a thrift exception if we do not have N nodes
-        Map<String, ReadCommand[]> readMessages = new HashMap<String, ReadCommand[]>();
-        for (String key : keys )
-        {
-            ReadCommand[] readParameters = new ReadCommand[2];
-            readParameters[0] = readCommand.copy();
-            readParameters[1] = readCommand.copy();
-            readParameters[1].setDigestQuery(true);
-            readMessages.put(key, readParameters);
-        }        
-        rows = doStrongReadProtocol(readMessages);         
-        return rows;
-    }
-
-    /*
-     * This function executes the read protocol.
-        // 1. Get the N nodes from storage service where the data needs to be
-        // replicated
-        // 2. Construct a message for read\write
-         * 3. Set one of the messages to get the data and the rest to get the digest
-        // 4. SendRR ( to all the nodes above )
-        // 5. Wait for a response from at least X nodes where X <= N and the data node
-         * 6. If the digest matches return the data.
-         * 7. else carry out read repair by getting data from all the nodes.
-        // 5. return success
-     */
-    private static Row strongRead(ReadCommand command) throws IOException, TimeoutException, InvalidRequestException
-    {
-        // TODO: throw a thrift exception if we do not have N nodes
-        assert !command.isDigestQuery();
-        ReadCommand readMessageDigestOnly = command.copy();
-        readMessageDigestOnly.setDigestQuery(true);
-
-        Row row = null;
-        Message message = command.makeReadMessage();
-        Message messageDigestOnly = readMessageDigestOnly.makeReadMessage();
-
-        IResponseResolver<Row> readResponseResolver = new ReadResponseResolver();
-        QuorumResponseHandler<Row> quorumResponseHandler = new QuorumResponseHandler<Row>(
-                DatabaseDescriptor.getQuorum(),
-                readResponseResolver);
-        EndPoint dataPoint = StorageService.instance().findSuitableEndPoint(command.key);
-        List<EndPoint> endpointList = new ArrayList<EndPoint>(Arrays.asList(StorageService.instance().getNStorageEndPoint(command.key)));
-        /* Remove the local storage endpoint from the list. */
-        endpointList.remove(dataPoint);
-        EndPoint[] endPoints = new EndPoint[endpointList.size() + 1];
-        Message messages[] = new Message[endpointList.size() + 1];
-
-        /*
-         * First message is sent to the node that will actually get
-         * the data for us. The other two replicas are only sent a
-         * digest query.
-        */
-        endPoints[0] = dataPoint;
-        messages[0] = message;
-        if (logger.isDebugEnabled())
-            logger.debug("strongread reading data for " + command + " from " + message.getMessageId() + "@" + dataPoint);
-        for (int i = 1; i < endPoints.length; i++)
-        {
-            EndPoint digestPoint = endpointList.get(i - 1);
-            endPoints[i] = digestPoint;
-            messages[i] = messageDigestOnly;
-            if (logger.isDebugEnabled())
-                logger.debug("strongread reading digest for " + command + " from " + messageDigestOnly.getMessageId() + "@" + digestPoint);
-        }
-
-        try
-        {
-            MessagingService.getMessagingInstance().sendRR(messages, endPoints, quorumResponseHandler);
-
-            long startTime2 = System.currentTimeMillis();
-            row = quorumResponseHandler.get();
-            if (logger.isDebugEnabled())
-                logger.debug("quorumResponseHandler: " + (System.currentTimeMillis() - startTime2) + " ms.");
-        }
-        catch (DigestMismatchException ex)
-        {
-            if ( DatabaseDescriptor.getConsistencyCheck())
-            {
-                IResponseResolver<Row> readResponseResolverRepair = new ReadResponseResolver();
-                QuorumResponseHandler<Row> quorumResponseHandlerRepair = new QuorumResponseHandler<Row>(
-                        DatabaseDescriptor.getQuorum(),
-                        readResponseResolverRepair);
-                logger.info("DigestMismatchException: " + command.key);
-                Message messageRepair = command.makeReadMessage();
-                MessagingService.getMessagingInstance().sendRR(messageRepair, endPoints,
-                                                               quorumResponseHandlerRepair);
-                try
-                {
-                    row = quorumResponseHandlerRepair.get();
-                }
-                catch (DigestMismatchException e)
-                {
-                    // TODO should this be a thrift exception?
-                    throw new RuntimeException("digest mismatch reading key " + command.key, e);
-                }
-            }
-        }
-
-        return row;
-    }
-
-    private static Map<String, Message[]> constructReplicaMessages(Map<String, ReadCommand[]> readMessages) throws IOException
-    {
-        Map<String, Message[]> messages = new HashMap<String, Message[]>();
-        Set<String> keys = readMessages.keySet();
-        
-        for ( String key : keys )
-        {
-            Message[] msg = new Message[DatabaseDescriptor.getReplicationFactor()];
-            ReadCommand[] readParameters = readMessages.get(key);
-            msg[0] = readParameters[0].makeReadMessage();
-            for ( int i = 1; i < msg.length; ++i )
-            {
-                msg[i] = readParameters[1].makeReadMessage();
-            }
-        }        
-        return messages;
-    }
-    
-    private static MultiQuorumResponseHandler dispatchMessages(Map<String, ReadCommand[]> readMessages, Map<String, Message[]> messages) throws IOException
-    {
-        Set<String> keys = messages.keySet();
-        /* This maps the keys to the original data read messages */
-        Map<String, ReadCommand> readMessage = new HashMap<String, ReadCommand>();
-        /* This maps the keys to their respective endpoints/replicas */
-        Map<String, EndPoint[]> endpoints = new HashMap<String, EndPoint[]>();
-        /* Groups the messages that need to be sent to the individual keys */
-        Message[][] msgList = new Message[messages.size()][DatabaseDescriptor.getReplicationFactor()];
-        /* Respects the above grouping and provides the endpoints for the above messages */
-        EndPoint[][] epList = new EndPoint[messages.size()][DatabaseDescriptor.getReplicationFactor()];
-        
-        int i = 0;
-        for ( String key : keys )
-        {
-            /* This is the primary */
-            EndPoint dataPoint = StorageService.instance().findSuitableEndPoint(key);
-            List<EndPoint> replicas = new ArrayList<EndPoint>( StorageService.instance().getNLiveStorageEndPoint(key) );
-            replicas.remove(dataPoint);
-            /* Get the messages to be sent index 0 is the data messages and index 1 is the digest message */
-            Message[] message = messages.get(key);           
-            msgList[i][0] = message[0];
-            int N = DatabaseDescriptor.getReplicationFactor();
-            for ( int j = 1; j < N; ++j )
-            {
-                msgList[i][j] = message[1];
-            }
-            /* Get the endpoints to which the above messages need to be sent */
-            epList[i][0] = dataPoint;
-            for ( int j = 1; i < N; ++i )
-            {                
-                epList[i][j] = replicas.get(j - 1);
-            } 
-            /* Data ReadMessage associated with this key */
-            readMessage.put( key, readMessages.get(key)[0] );
-            /* EndPoints for this specific key */
-            endpoints.put(key, epList[i]);
-            ++i;
-        }
-                
-        /* Handles the read semantics for this entire set of keys */
-        MultiQuorumResponseHandler quorumResponseHandlers = new MultiQuorumResponseHandler(readMessage, endpoints);
-        MessagingService.getMessagingInstance().sendRR(msgList, epList, quorumResponseHandlers);
-        return quorumResponseHandlers;
-    }
-    
-    /**
-    *  This method performs the read from the replicas for a bunch of keys.
-    *  @param readMessages map of key --> readMessage[] of two entries where 
-    *         the first entry is the readMessage for the data and the second
-    *         is the entry for the digest 
-    *  @return map containing key ---> Row
-    *  @throws IOException, TimeoutException
-   */
-    private static Map<String, Row> doStrongReadProtocol(Map<String, ReadCommand[]> readMessages) throws IOException
-    {        
-        Map<String, Row> rows = new HashMap<String, Row>();
-        /* Construct the messages to be sent to the replicas */
-        Map<String, Message[]> replicaMessages = constructReplicaMessages(readMessages);
-        /* Dispatch the messages to the different replicas */
-        MultiQuorumResponseHandler cb = dispatchMessages(readMessages, replicaMessages);
-        try
-        {
-            Row[] rows2 = cb.get();
-            for ( Row row : rows2 )
-            {
-                rows.put(row.key(), row);
-            }
-        }
-        catch (TimeoutException e)
-        {
-            throw new RuntimeException("timeout reading keys " + StringUtils.join(rows.keySet(), ", "), e);
-        }
-        return rows;
-    }
-
-    /**
-     * This version is used when results for multiple keys needs to be
-     * retrieved.
-     * 
-     * @param tablename name of the table that needs to be queried
-     * @param keys keys whose values we are interested in 
-     * @param columnFamily name of the "column" we are interested in
-     * @param columns the columns we are interested in
-     * @return a mapping of key --> Row
-     * @throws Exception
-     */
-    public static Map<String, Row> weakReadProtocol(String[] keys, ReadCommand readCommand) throws Exception
-    {
-        Row row = null;
-        Map<String, ReadCommand> readMessages = new HashMap<String, ReadCommand>();
-        for ( String key : keys )
-        {
-            ReadCommand readCmd = readCommand.copy();
-            readMessages.put(key, readCmd);
-        }
-        /* Performs the multiget in parallel */
-        Map<String, Row> rows = doReadProtocol(readMessages);
-        /*
-         * Do the consistency checks for the keys that are being queried
-         * in the background.
-        */
-        for ( String key : keys )
-        {
-            List<EndPoint> endpoints = StorageService.instance().getNLiveStorageEndPoint(key);
-            /* Remove the local storage endpoint from the list. */
-            endpoints.remove( StorageService.getLocalStorageEndPoint() );
-            if ( endpoints.size() > 0 && DatabaseDescriptor.getConsistencyCheck())
-                StorageService.instance().doConsistencyCheck(row, endpoints, readMessages.get(key));
-        }
-        return rows;
-    }
-
-    /*
-    * This function executes the read protocol locally and should be used only if consistency is not a concern.
-    * Read the data from the local disk and return if the row is NOT NULL. If the data is NULL do the read from
-    * one of the other replicas (in the same data center if possible) till we get the data. In the event we get
-    * the data we perform consistency checks and figure out if any repairs need to be done to the replicas.
-    */
-    private static Row weakReadLocal(ReadCommand command) throws IOException
-    {
-        if (logger.isDebugEnabled())
-            logger.debug("weakreadlocal reading " + command);
-        List<EndPoint> endpoints = StorageService.instance().getNLiveStorageEndPoint(command.key);
-        /* Remove the local storage endpoint from the list. */
-        endpoints.remove(StorageService.getLocalStorageEndPoint());
-        // TODO: throw a thrift exception if we do not have N nodes
-
-        Table table = Table.open(command.table);
-        Row row = command.getRow(table);
-
-        /*
-           * Do the consistency checks in the background and return the
-           * non NULL row.
-           */
-        if (endpoints.size() > 0 && DatabaseDescriptor.getConsistencyCheck())
-            StorageService.instance().doConsistencyCheck(row, endpoints, command);
-        return row;
-    }
-
-    static List<String> getKeyRange(RangeCommand command)
-    {
-        long startTime = System.currentTimeMillis();
-        try
-        {
-            EndPoint endPoint = StorageService.instance().findSuitableEndPoint(command.startWith);
-            IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(command.getMessage(), endPoint);
-
-            // read response
-            // TODO send more requests if we need to span multiple nodes
-            byte[] responseBody = iar.get(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
-            return RangeReply.read(responseBody).keys;
-        }
-        catch (Exception e)
-        {
-            throw new RuntimeException("error reading keyrange " + command, e);
-        }
-        finally
-        {
-            rangeStats.add(System.currentTimeMillis() - startTime);
-        }
-    }
-
-    public double getReadLatency()
-    {
-        return readStats.mean();
-    }
-
-    public double getRangeLatency()
-    {
-        return rangeStats.mean();
-    }
-
-    public double getWriteLatency()
-    {
-        return writeStats.mean();
-    }
-
-    public int getReadOperations()
-    {
-        return readStats.size();
-    }
-
-    public int getRangeOperations()
-    {
-        return rangeStats.size();
-    }
-
-    public int getWriteOperations()
-    {
-        return writeStats.size();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.Set;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.lang.management.ManagementFactory;
+
+import org.apache.commons.lang.StringUtils;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IAsyncResult;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.utils.TimedStatsDeque;
+import org.apache.log4j.Logger;
+
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+
+
+public class StorageProxy implements StorageProxyMBean
+{
+    private static Logger logger = Logger.getLogger(StorageProxy.class);
+
+    // mbean stuff
+    private static TimedStatsDeque readStats = new TimedStatsDeque(60000);
+    private static TimedStatsDeque rangeStats = new TimedStatsDeque(60000);
+    private static TimedStatsDeque writeStats = new TimedStatsDeque(60000);
+    private StorageProxy() {}
+    static
+    {
+        MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
+        try
+        {
+            mbs.registerMBean(new StorageProxy(), new ObjectName("org.apache.cassandra.service:type=StorageProxy"));
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    /**
+     * This method is responsible for creating Message to be
+     * sent over the wire to N replicas where some of the replicas
+     * may be hints.
+     */
+    private static Map<EndPoint, Message> createWriteMessages(RowMutation rm, Map<EndPoint, EndPoint> endpointMap) throws IOException
+    {
+		Map<EndPoint, Message> messageMap = new HashMap<EndPoint, Message>();
+		Message message = rm.makeRowMutationMessage();
+
+		for (Map.Entry<EndPoint, EndPoint> entry : endpointMap.entrySet())
+		{
+            EndPoint target = entry.getKey();
+            EndPoint hint = entry.getValue();
+            if ( !target.equals(hint) )
+			{
+				Message hintedMessage = rm.makeRowMutationMessage();
+				hintedMessage.addHeader(RowMutation.HINT, EndPoint.toBytes(hint) );
+				if (logger.isDebugEnabled())
+				    logger.debug("Sending the hint of " + hint.getHost() + " to " + target.getHost());
+				messageMap.put(target, hintedMessage);
+			}
+			else
+			{
+				messageMap.put(target, message);
+			}
+		}
+		return messageMap;
+    }
+    
+    /**
+     * Use this method to have this RowMutation applied
+     * across all replicas. This method will take care
+     * of the possibility of a replica being down and hint
+     * the data across to some other replica. 
+     * @param rm the mutation to be applied across the replicas
+    */
+    public static void insert(RowMutation rm)
+	{
+        /*
+         * Get the N nodes from storage service where the data needs to be
+         * replicated
+         * Construct a message for write
+         * Send them asynchronously to the replicas.
+        */
+
+        long startTime = System.currentTimeMillis();
+		try
+		{
+			Map<EndPoint, EndPoint> endpointMap = StorageService.instance().getNStorageEndPointMap(rm.key());
+			// TODO: throw a thrift exception if we do not have N nodes
+			Map<EndPoint, Message> messageMap = createWriteMessages(rm, endpointMap);
+			for (Map.Entry<EndPoint, Message> entry : messageMap.entrySet())
+			{
+                Message message = entry.getValue();
+                EndPoint endpoint = entry.getKey();
+                // Check if local and not hinted
+                byte[] hintedBytes = message.getHeader(RowMutation.HINT);
+                if (endpoint.equals(StorageService.getLocalStorageEndPoint())
+                        && !(hintedBytes!= null && hintedBytes.length>0))
+                {
+                    if (logger.isDebugEnabled())
+                        logger.debug("locally writing writing key " + rm.key()
+                                + " to " + endpoint);
+                    rm.apply();
+                } else
+                {
+                    if (logger.isDebugEnabled())
+                        logger.debug("insert writing key " + rm.key() + " to "
+                                + message.getMessageId() + "@" + endpoint);
+                	MessagingService.getMessagingInstance().sendOneWay(message, endpoint);
+                }
+			}
+		}
+        catch (IOException e)
+        {
+            throw new RuntimeException("error inserting key " + rm.key(), e);
+        }
+        finally
+        {
+            writeStats.add(System.currentTimeMillis() - startTime);
+        }
+    }
+    
+    public static void insertBlocking(RowMutation rm, int consistency_level) throws UnavailableException
+    {
+        long startTime = System.currentTimeMillis();
+        Message message = null;
+        try
+        {
+            message = rm.makeRowMutationMessage();
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+        try
+        {
+            EndPoint[] endpoints = StorageService.instance().getNStorageEndPoint(rm.key());
+            if (endpoints.length < (DatabaseDescriptor.getReplicationFactor() / 2) + 1)
+            {
+                throw new UnavailableException();
+            }
+            int blockFor;
+            if (consistency_level == ConsistencyLevel.ONE)
+            {
+                blockFor = 1;
+            }
+            else if (consistency_level == ConsistencyLevel.QUORUM)
+            {
+                blockFor = (DatabaseDescriptor.getReplicationFactor() >> 1) + 1;
+            }
+            else if (consistency_level == ConsistencyLevel.ALL)
+            {
+                blockFor = DatabaseDescriptor.getReplicationFactor();
+            }
+            else
+            {
+                throw new UnsupportedOperationException("invalid consistency level " + consistency_level);
+            }
+            QuorumResponseHandler<Boolean> quorumResponseHandler = new QuorumResponseHandler<Boolean>(blockFor, new WriteResponseResolver());
+            if (logger.isDebugEnabled())
+                logger.debug("insertBlocking writing key " + rm.key() + " to " + message.getMessageId() + "@[" + StringUtils.join(endpoints, ", ") + "]");
+
+            MessagingService.getMessagingInstance().sendRR(message, endpoints, quorumResponseHandler);
+            if (!quorumResponseHandler.get())
+                throw new UnavailableException();
+        }
+        catch (Exception e)
+        {
+            logger.error("error writing key " + rm.key(), e);
+            throw new UnavailableException();
+        }
+        finally
+        {
+            writeStats.add(System.currentTimeMillis() - startTime);
+        }
+    }
+
+    public static void insertBlocking(RowMutation rm) throws UnavailableException
+    {
+        insertBlocking(rm, ConsistencyLevel.QUORUM);
+    }
+    
+    private static Map<String, Message> constructMessages(Map<String, ReadCommand> readMessages) throws IOException
+    {
+        Map<String, Message> messages = new HashMap<String, Message>();
+        Set<String> keys = readMessages.keySet();        
+        for ( String key : keys )
+        {
+            Message message = readMessages.get(key).makeReadMessage();
+            messages.put(key, message);
+        }        
+        return messages;
+    }
+    
+    private static IAsyncResult dispatchMessages(Map<String, EndPoint> endPoints, Map<String, Message> messages)
+    {
+        Set<String> keys = endPoints.keySet();
+        EndPoint[] eps = new EndPoint[keys.size()];
+        Message[] msgs  = new Message[keys.size()];
+        
+        int i = 0;
+        for ( String key : keys )
+        {
+            eps[i] = endPoints.get(key);
+            msgs[i] = messages.get(key);
+            ++i;
+        }
+        
+        IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(msgs, eps);
+        return iar;
+    }
+    
+    /**
+     * This is an implementation for the multiget version. 
+     * @param readMessages map of key --> ReadMessage to be sent
+     * @return map of key --> Row
+     * @throws IOException
+     * @throws TimeoutException
+     */
+    public static Map<String, Row> doReadProtocol(Map<String, ReadCommand> readMessages) throws IOException,TimeoutException
+    {
+        Map<String, Row> rows = new HashMap<String, Row>();
+        Set<String> keys = readMessages.keySet();
+        /* Find all the suitable endpoints for the keys */
+        Map<String, EndPoint> endPoints = StorageService.instance().findSuitableEndPoints(keys.toArray( new String[0] ));
+        /* Construct the messages to be sent out */
+        Map<String, Message> messages = constructMessages(readMessages);
+        /* Dispatch the messages to the respective endpoints */
+        IAsyncResult iar = dispatchMessages(endPoints, messages);        
+        List<byte[]> results = iar.multiget(2*DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
+        
+        for ( byte[] body : results )
+        {
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length);
+            ReadResponse response = ReadResponse.serializer().deserialize(bufIn);
+            Row row = response.row();
+            rows.put(row.key(), row);
+        }        
+        return rows;
+    }
+
+    /**
+     * Read the data from one replica.  If there is no reply, read the data from another.  In the event we get
+     * the data we perform consistency checks and figure out if any repairs need to be done to the replicas.
+     * @param command the read to perform
+     * @return the row associated with command.key
+     * @throws Exception
+     */
+    private static Row weakReadRemote(ReadCommand command) throws IOException
+    {
+        EndPoint endPoint = StorageService.instance().findSuitableEndPoint(command.key);
+        assert endPoint != null;
+        Message message = command.makeReadMessage();
+        if (logger.isDebugEnabled())
+            logger.debug("weakreadremote reading " + command + " from " + message.getMessageId() + "@" + endPoint);
+        message.addHeader(ReadCommand.DO_REPAIR, ReadCommand.DO_REPAIR.getBytes());
+        IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(message, endPoint);
+        byte[] body;
+        try
+        {
+            body = iar.get(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
+        }
+        catch (TimeoutException e)
+        {
+            throw new RuntimeException("error reading key " + command.key, e);
+            // TODO retry to a different endpoint?
+        }
+        DataInputBuffer bufIn = new DataInputBuffer();
+        bufIn.reset(body, body.length);
+        ReadResponse response = ReadResponse.serializer().deserialize(bufIn);
+        return response.row();
+    }
+
+    /**
+     * Performs the actual reading of a row out of the StorageService, fetching
+     * a specific set of column names from a given column family.
+     */
+    public static Row readProtocol(ReadCommand command, int consistency_level)
+    throws IOException, TimeoutException, InvalidRequestException
+    {
+        long startTime = System.currentTimeMillis();
+
+        Row row;
+        EndPoint[] endpoints = StorageService.instance().getNStorageEndPoint(command.key);
+
+        if (consistency_level == ConsistencyLevel.ONE)
+        {
+            boolean foundLocal = Arrays.asList(endpoints).contains(StorageService.getLocalStorageEndPoint());
+            if (foundLocal)
+            {
+                row = weakReadLocal(command);
+            }
+            else
+            {
+                row = weakReadRemote(command);
+            }
+        }
+        else
+        {
+            assert consistency_level == ConsistencyLevel.QUORUM;
+            row = strongRead(command);
+        }
+
+        readStats.add(System.currentTimeMillis() - startTime);
+
+        return row;
+    }
+
+    public static Map<String, Row> readProtocol(String[] keys, ReadCommand readCommand, StorageService.ConsistencyLevel consistencyLevel) throws Exception
+    {
+        Map<String, Row> rows = new HashMap<String, Row>();        
+        switch ( consistencyLevel )
+        {
+            case WEAK:
+                rows = weakReadProtocol(keys, readCommand);
+                break;
+                
+            case STRONG:
+                rows = strongReadProtocol(keys, readCommand);
+                break;
+                
+            default:
+                rows = weakReadProtocol(keys, readCommand);
+                break;
+        }
+        return rows;
+    }
+
+    /**
+     * This is a multiget version of the above method.
+     * @param tablename
+     * @param keys
+     * @param columnFamily
+     * @param start
+     * @param count
+     * @return
+     * @throws IOException
+     * @throws TimeoutException
+     */
+    public static Map<String, Row> strongReadProtocol(String[] keys, ReadCommand readCommand) throws IOException, TimeoutException
+    {       
+        Map<String, Row> rows;
+        // TODO: throw a thrift exception if we do not have N nodes
+        Map<String, ReadCommand[]> readMessages = new HashMap<String, ReadCommand[]>();
+        for (String key : keys )
+        {
+            ReadCommand[] readParameters = new ReadCommand[2];
+            readParameters[0] = readCommand.copy();
+            readParameters[1] = readCommand.copy();
+            readParameters[1].setDigestQuery(true);
+            readMessages.put(key, readParameters);
+        }        
+        rows = doStrongReadProtocol(readMessages);         
+        return rows;
+    }
+
+    /*
+     * This function executes the read protocol.
+        // 1. Get the N nodes from storage service where the data needs to be
+        // replicated
+        // 2. Construct a message for read\write
+         * 3. Set one of the messages to get the data and the rest to get the digest
+        // 4. SendRR ( to all the nodes above )
+        // 5. Wait for a response from at least X nodes where X <= N and the data node
+         * 6. If the digest matches return the data.
+         * 7. else carry out read repair by getting data from all the nodes.
+        // 5. return success
+     */
+    private static Row strongRead(ReadCommand command) throws IOException, TimeoutException, InvalidRequestException
+    {
+        // TODO: throw a thrift exception if we do not have N nodes
+        assert !command.isDigestQuery();
+        ReadCommand readMessageDigestOnly = command.copy();
+        readMessageDigestOnly.setDigestQuery(true);
+
+        Row row = null;
+        Message message = command.makeReadMessage();
+        Message messageDigestOnly = readMessageDigestOnly.makeReadMessage();
+
+        IResponseResolver<Row> readResponseResolver = new ReadResponseResolver();
+        QuorumResponseHandler<Row> quorumResponseHandler = new QuorumResponseHandler<Row>(
+                DatabaseDescriptor.getQuorum(),
+                readResponseResolver);
+        EndPoint dataPoint = StorageService.instance().findSuitableEndPoint(command.key);
+        List<EndPoint> endpointList = new ArrayList<EndPoint>(Arrays.asList(StorageService.instance().getNStorageEndPoint(command.key)));
+        /* Remove the local storage endpoint from the list. */
+        endpointList.remove(dataPoint);
+        EndPoint[] endPoints = new EndPoint[endpointList.size() + 1];
+        Message messages[] = new Message[endpointList.size() + 1];
+
+        /*
+         * First message is sent to the node that will actually get
+         * the data for us. The other two replicas are only sent a
+         * digest query.
+        */
+        endPoints[0] = dataPoint;
+        messages[0] = message;
+        if (logger.isDebugEnabled())
+            logger.debug("strongread reading data for " + command + " from " + message.getMessageId() + "@" + dataPoint);
+        for (int i = 1; i < endPoints.length; i++)
+        {
+            EndPoint digestPoint = endpointList.get(i - 1);
+            endPoints[i] = digestPoint;
+            messages[i] = messageDigestOnly;
+            if (logger.isDebugEnabled())
+                logger.debug("strongread reading digest for " + command + " from " + messageDigestOnly.getMessageId() + "@" + digestPoint);
+        }
+
+        try
+        {
+            MessagingService.getMessagingInstance().sendRR(messages, endPoints, quorumResponseHandler);
+
+            long startTime2 = System.currentTimeMillis();
+            row = quorumResponseHandler.get();
+            if (logger.isDebugEnabled())
+                logger.debug("quorumResponseHandler: " + (System.currentTimeMillis() - startTime2) + " ms.");
+        }
+        catch (DigestMismatchException ex)
+        {
+            if ( DatabaseDescriptor.getConsistencyCheck())
+            {
+                IResponseResolver<Row> readResponseResolverRepair = new ReadResponseResolver();
+                QuorumResponseHandler<Row> quorumResponseHandlerRepair = new QuorumResponseHandler<Row>(
+                        DatabaseDescriptor.getQuorum(),
+                        readResponseResolverRepair);
+                logger.info("DigestMismatchException: " + command.key);
+                Message messageRepair = command.makeReadMessage();
+                MessagingService.getMessagingInstance().sendRR(messageRepair, endPoints,
+                                                               quorumResponseHandlerRepair);
+                try
+                {
+                    row = quorumResponseHandlerRepair.get();
+                }
+                catch (DigestMismatchException e)
+                {
+                    // TODO should this be a thrift exception?
+                    throw new RuntimeException("digest mismatch reading key " + command.key, e);
+                }
+            }
+        }
+
+        return row;
+    }
+
+    private static Map<String, Message[]> constructReplicaMessages(Map<String, ReadCommand[]> readMessages) throws IOException
+    {
+        Map<String, Message[]> messages = new HashMap<String, Message[]>();
+        Set<String> keys = readMessages.keySet();
+        
+        for ( String key : keys )
+        {
+            Message[] msg = new Message[DatabaseDescriptor.getReplicationFactor()];
+            ReadCommand[] readParameters = readMessages.get(key);
+            msg[0] = readParameters[0].makeReadMessage();
+            for ( int i = 1; i < msg.length; ++i )
+            {
+                msg[i] = readParameters[1].makeReadMessage();
+            }
+        }        
+        return messages;
+    }
+    
+    private static MultiQuorumResponseHandler dispatchMessages(Map<String, ReadCommand[]> readMessages, Map<String, Message[]> messages) throws IOException
+    {
+        Set<String> keys = messages.keySet();
+        /* This maps the keys to the original data read messages */
+        Map<String, ReadCommand> readMessage = new HashMap<String, ReadCommand>();
+        /* This maps the keys to their respective endpoints/replicas */
+        Map<String, EndPoint[]> endpoints = new HashMap<String, EndPoint[]>();
+        /* Groups the messages that need to be sent to the individual keys */
+        Message[][] msgList = new Message[messages.size()][DatabaseDescriptor.getReplicationFactor()];
+        /* Respects the above grouping and provides the endpoints for the above messages */
+        EndPoint[][] epList = new EndPoint[messages.size()][DatabaseDescriptor.getReplicationFactor()];
+        
+        int i = 0;
+        for ( String key : keys )
+        {
+            /* This is the primary */
+            EndPoint dataPoint = StorageService.instance().findSuitableEndPoint(key);
+            List<EndPoint> replicas = new ArrayList<EndPoint>( StorageService.instance().getNLiveStorageEndPoint(key) );
+            replicas.remove(dataPoint);
+            /* Get the messages to be sent index 0 is the data messages and index 1 is the digest message */
+            Message[] message = messages.get(key);           
+            msgList[i][0] = message[0];
+            int N = DatabaseDescriptor.getReplicationFactor();
+            for ( int j = 1; j < N; ++j )
+            {
+                msgList[i][j] = message[1];
+            }
+            /* Get the endpoints to which the above messages need to be sent */
+            epList[i][0] = dataPoint;
+            for ( int j = 1; i < N; ++i )
+            {                
+                epList[i][j] = replicas.get(j - 1);
+            } 
+            /* Data ReadMessage associated with this key */
+            readMessage.put( key, readMessages.get(key)[0] );
+            /* EndPoints for this specific key */
+            endpoints.put(key, epList[i]);
+            ++i;
+        }
+                
+        /* Handles the read semantics for this entire set of keys */
+        MultiQuorumResponseHandler quorumResponseHandlers = new MultiQuorumResponseHandler(readMessage, endpoints);
+        MessagingService.getMessagingInstance().sendRR(msgList, epList, quorumResponseHandlers);
+        return quorumResponseHandlers;
+    }
+    
+    /**
+    *  This method performs the read from the replicas for a bunch of keys.
+    *  @param readMessages map of key --> readMessage[] of two entries where 
+    *         the first entry is the readMessage for the data and the second
+    *         is the entry for the digest 
+    *  @return map containing key ---> Row
+    *  @throws IOException, TimeoutException
+   */
+    private static Map<String, Row> doStrongReadProtocol(Map<String, ReadCommand[]> readMessages) throws IOException
+    {        
+        Map<String, Row> rows = new HashMap<String, Row>();
+        /* Construct the messages to be sent to the replicas */
+        Map<String, Message[]> replicaMessages = constructReplicaMessages(readMessages);
+        /* Dispatch the messages to the different replicas */
+        MultiQuorumResponseHandler cb = dispatchMessages(readMessages, replicaMessages);
+        try
+        {
+            Row[] rows2 = cb.get();
+            for ( Row row : rows2 )
+            {
+                rows.put(row.key(), row);
+            }
+        }
+        catch (TimeoutException e)
+        {
+            throw new RuntimeException("timeout reading keys " + StringUtils.join(rows.keySet(), ", "), e);
+        }
+        return rows;
+    }
+
+    /**
+     * This version is used when results for multiple keys needs to be
+     * retrieved.
+     * 
+     * @param tablename name of the table that needs to be queried
+     * @param keys keys whose values we are interested in 
+     * @param columnFamily name of the "column" we are interested in
+     * @param columns the columns we are interested in
+     * @return a mapping of key --> Row
+     * @throws Exception
+     */
+    public static Map<String, Row> weakReadProtocol(String[] keys, ReadCommand readCommand) throws Exception
+    {
+        Row row = null;
+        Map<String, ReadCommand> readMessages = new HashMap<String, ReadCommand>();
+        for ( String key : keys )
+        {
+            ReadCommand readCmd = readCommand.copy();
+            readMessages.put(key, readCmd);
+        }
+        /* Performs the multiget in parallel */
+        Map<String, Row> rows = doReadProtocol(readMessages);
+        /*
+         * Do the consistency checks for the keys that are being queried
+         * in the background.
+        */
+        for ( String key : keys )
+        {
+            List<EndPoint> endpoints = StorageService.instance().getNLiveStorageEndPoint(key);
+            /* Remove the local storage endpoint from the list. */
+            endpoints.remove( StorageService.getLocalStorageEndPoint() );
+            if ( endpoints.size() > 0 && DatabaseDescriptor.getConsistencyCheck())
+                StorageService.instance().doConsistencyCheck(row, endpoints, readMessages.get(key));
+        }
+        return rows;
+    }
+
+    /*
+    * This function executes the read protocol locally and should be used only if consistency is not a concern.
+    * Read the data from the local disk and return if the row is NOT NULL. If the data is NULL do the read from
+    * one of the other replicas (in the same data center if possible) till we get the data. In the event we get
+    * the data we perform consistency checks and figure out if any repairs need to be done to the replicas.
+    */
+    private static Row weakReadLocal(ReadCommand command) throws IOException
+    {
+        if (logger.isDebugEnabled())
+            logger.debug("weakreadlocal reading " + command);
+        List<EndPoint> endpoints = StorageService.instance().getNLiveStorageEndPoint(command.key);
+        /* Remove the local storage endpoint from the list. */
+        endpoints.remove(StorageService.getLocalStorageEndPoint());
+        // TODO: throw a thrift exception if we do not have N nodes
+
+        Table table = Table.open(command.table);
+        Row row = command.getRow(table);
+
+        /*
+           * Do the consistency checks in the background and return the
+           * non NULL row.
+           */
+        if (endpoints.size() > 0 && DatabaseDescriptor.getConsistencyCheck())
+            StorageService.instance().doConsistencyCheck(row, endpoints, command);
+        return row;
+    }
+
+    static List<String> getKeyRange(RangeCommand command)
+    {
+        long startTime = System.currentTimeMillis();
+        try
+        {
+            EndPoint endPoint = StorageService.instance().findSuitableEndPoint(command.startWith);
+            IAsyncResult iar = MessagingService.getMessagingInstance().sendRR(command.getMessage(), endPoint);
+
+            // read response
+            // TODO send more requests if we need to span multiple nodes
+            byte[] responseBody = iar.get(DatabaseDescriptor.getRpcTimeout(), TimeUnit.MILLISECONDS);
+            return RangeReply.read(responseBody).keys;
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException("error reading keyrange " + command, e);
+        }
+        finally
+        {
+            rangeStats.add(System.currentTimeMillis() - startTime);
+        }
+    }
+
+    public double getReadLatency()
+    {
+        return readStats.mean();
+    }
+
+    public double getRangeLatency()
+    {
+        return rangeStats.mean();
+    }
+
+    public double getWriteLatency()
+    {
+        return writeStats.mean();
+    }
+
+    public int getReadOperations()
+    {
+        return readStats.size();
+    }
+
+    public int getRangeOperations()
+    {
+        return rangeStats.size();
+    }
+
+    public int getWriteOperations()
+    {
+        return writeStats.size();
+    }
+}
diff --git a/src/java/org/apache/cassandra/service/StorageService.java b/src/java/org/apache/cassandra/service/StorageService.java
index aba6f44e49..577c6c84ac 100644
--- a/src/java/org/apache/cassandra/service/StorageService.java
+++ b/src/java/org/apache/cassandra/service/StorageService.java
@@ -1,1095 +1,1095 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.io.File;
-import java.io.IOException;
-import java.lang.management.ManagementFactory;
-import java.util.*;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.locks.Lock;
-import java.util.concurrent.locks.ReentrantLock;
-import javax.management.MBeanServer;
-import javax.management.ObjectName;
-
-import org.apache.cassandra.concurrent.*;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.*;
-import org.apache.cassandra.dht.*;
-import org.apache.cassandra.gms.*;
-import org.apache.cassandra.locator.*;
-import org.apache.cassandra.net.*;
-import org.apache.cassandra.net.io.StreamContextManager;
-import org.apache.cassandra.tools.MembershipCleanerVerbHandler;
-import org.apache.cassandra.utils.FileUtils;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/*
- * This abstraction contains the token/identifier of this node
- * on the identifier space. This token gets gossiped around.
- * This class will also maintain histograms of the load information
- * of other nodes in the cluster.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public final class StorageService implements IEndPointStateChangeSubscriber, StorageServiceMBean
-{
-    private static Logger logger_ = Logger.getLogger(StorageService.class);     
-    private final static String nodeId_ = "NODE-IDENTIFIER";
-    private final static String loadAll_ = "LOAD-ALL";
-    /* Gossip load after every 5 mins. */
-    private static final long threshold_ = 5 * 60 * 1000L;
-    
-    /* All stage identifiers */
-    public final static String mutationStage_ = "ROW-MUTATION-STAGE";
-    public final static String readStage_ = "ROW-READ-STAGE";
-    
-    /* All verb handler identifiers */
-    public final static String mutationVerbHandler_ = "ROW-MUTATION-VERB-HANDLER";
-    public final static String tokenVerbHandler_ = "TOKEN-VERB-HANDLER";
-    public final static String loadVerbHandler_ = "LOAD-VERB-HANDLER";
-    public final static String binaryVerbHandler_ = "BINARY-VERB-HANDLER";
-    public final static String readRepairVerbHandler_ = "READ-REPAIR-VERB-HANDLER";
-    public final static String readVerbHandler_ = "ROW-READ-VERB-HANDLER";
-    public final static String bootStrapInitiateVerbHandler_ = "BOOTSTRAP-INITIATE-VERB-HANDLER";
-    public final static String bootStrapInitiateDoneVerbHandler_ = "BOOTSTRAP-INITIATE-DONE-VERB-HANDLER";
-    public final static String bootStrapTerminateVerbHandler_ = "BOOTSTRAP-TERMINATE-VERB-HANDLER";
-    public final static String dataFileVerbHandler_ = "DATA-FILE-VERB-HANDLER";
-    public final static String mbrshipCleanerVerbHandler_ = "MBRSHIP-CLEANER-VERB-HANDLER";
-    public final static String bsMetadataVerbHandler_ = "BS-METADATA-VERB-HANDLER";
-    public final static String calloutDeployVerbHandler_ = "CALLOUT-DEPLOY-VERB-HANDLER";
-    public static String rangeVerbHandler_ = "RANGE-VERB-HANDLER";
-
-    public static enum ConsistencyLevel
-    {
-    	WEAK,
-    	STRONG
-    }
-
-    private static StorageService instance_;
-    /* Used to lock the factory for creation of StorageService instance */
-    private static Lock createLock_ = new ReentrantLock();
-    private static EndPoint tcpAddr_;
-    private static EndPoint udpAddr_;
-    private static IPartitioner partitioner_;
-
-    public static EndPoint getLocalStorageEndPoint()
-    {
-        return tcpAddr_;
-    }
-
-    public static EndPoint getLocalControlEndPoint()
-    {
-        return udpAddr_;
-    }
-
-    public static IPartitioner getPartitioner() {
-        return partitioner_;
-    }
-    
-    public static enum BootstrapMode
-    {
-        HINT,
-        FULL
-    }
-
-    static
-    {
-        partitioner_ = DatabaseDescriptor.getPartitioner();
-    }
-
-
-    public static class BootstrapInitiateDoneVerbHandler implements IVerbHandler
-    {
-        private static Logger logger_ = Logger.getLogger( BootstrapInitiateDoneVerbHandler.class );
-
-        public void doVerb(Message message)
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Received a bootstrap initiate done message ...");
-            /* Let the Stream Manager do his thing. */
-            StreamManager.instance(message.getFrom()).start();            
-        }
-    }
-
-    /*
-     * Factory method that gets an instance of the StorageService
-     * class.
-    */
-    public static StorageService instance()
-    {
-        if ( instance_ == null )
-        {
-            StorageService.createLock_.lock();
-            try
-            {
-                if ( instance_ == null )
-                {
-                    try
-                    {
-                        instance_ = new StorageService();
-                    }
-                    catch ( Throwable th )
-                    {
-                        logger_.error(LogUtil.throwableToString(th));
-                        System.exit(1);
-                    }
-                }
-            }
-            finally
-            {
-                createLock_.unlock();
-            }
-        }
-        return instance_;
-    }
-
-    /*
-     * This is the endpoint snitch which depends on the network architecture. We
-     * need to keep this information for each endpoint so that we make decisions
-     * while doing things like replication etc.
-     *
-     */
-    private IEndPointSnitch endPointSnitch_;
-
-    /* This abstraction maintains the token/endpoint metadata information */
-    private TokenMetadata tokenMetadata_ = new TokenMetadata();
-    private SystemTable.StorageMetadata storageMetadata_;
-
-    /* Timer is used to disseminate load information */
-    private Timer loadTimer_ = new Timer(false);
-
-    /* This thread pool is used to do the bootstrap for a new node */
-    private ExecutorService bootStrapper_ = new DebuggableThreadPoolExecutor("BOOT-STRAPPER");
-    
-    /* This thread pool does consistency checks when the client doesn't care about consistency */
-    private ExecutorService consistencyManager_;
-
-    /* This is the entity that tracks load information of all nodes in the cluster */
-    private StorageLoadBalancer storageLoadBalancer_;
-    /* We use this interface to determine where replicas need to be placed */
-    private IReplicaPlacementStrategy nodePicker_;
-    
-    /*
-     * Registers with Management Server
-     */
-    private void init()
-    {
-        try
-        {
-            MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
-            mbs.registerMBean(this, new ObjectName(
-                    "org.apache.cassandra.service:type=StorageService"));
-        }
-        catch (Exception e)
-        {
-            logger_.error(LogUtil.throwableToString(e));
-        }
-    }
-
-    public StorageService()
-    {
-        init();
-        storageLoadBalancer_ = new StorageLoadBalancer(this);
-        endPointSnitch_ = DatabaseDescriptor.getEndPointSnitch();
-
-        /* register the verb handlers */
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.tokenVerbHandler_, new TokenUpdateVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.binaryVerbHandler_, new BinaryVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.loadVerbHandler_, new LoadVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.mutationVerbHandler_, new RowMutationVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.readRepairVerbHandler_, new ReadRepairVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.readVerbHandler_, new ReadVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bootStrapInitiateVerbHandler_, new Table.BootStrapInitiateVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bootStrapInitiateDoneVerbHandler_, new StorageService.BootstrapInitiateDoneVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bootStrapTerminateVerbHandler_, new StreamManager.BootstrapTerminateVerbHandler());
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.dataFileVerbHandler_, new DataFileVerbHandler() );
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.mbrshipCleanerVerbHandler_, new MembershipCleanerVerbHandler() );
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bsMetadataVerbHandler_, new BootstrapMetadataVerbHandler() );        
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.calloutDeployVerbHandler_, new CalloutDeployVerbHandler() );
-        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.rangeVerbHandler_, new RangeVerbHandler());
-        
-        /* register the stage for the mutations */
-        consistencyManager_ = new DebuggableThreadPoolExecutor(DatabaseDescriptor.getConsistencyThreads(),
-                                                               DatabaseDescriptor.getConsistencyThreads(),
-                                                               Integer.MAX_VALUE, TimeUnit.SECONDS,
-                                                               new LinkedBlockingQueue<Runnable>(), new ThreadFactoryImpl("CONSISTENCY-MANAGER"));
-        
-        StageManager.registerStage(StorageService.mutationStage_,
-                                   new MultiThreadedStage(StorageService.mutationStage_, DatabaseDescriptor.getConcurrentWriters()));
-        StageManager.registerStage(StorageService.readStage_,
-                                   new MultiThreadedStage(StorageService.readStage_, DatabaseDescriptor.getConcurrentReaders()));
-
-        Class cls = DatabaseDescriptor.getReplicaPlacementStrategyClass();
-        Class [] parameterTypes = new Class[] { TokenMetadata.class, IPartitioner.class, int.class, int.class};
-        try
-        {
-            nodePicker_ = (IReplicaPlacementStrategy) cls.getConstructor(parameterTypes).newInstance(tokenMetadata_, partitioner_, DatabaseDescriptor.getReplicationFactor(), DatabaseDescriptor.getStoragePort());
-        }
-        catch (Exception e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-
-    public void start() throws IOException
-    {
-        storageMetadata_ = SystemTable.initMetadata();
-        tcpAddr_ = new EndPoint(DatabaseDescriptor.getStoragePort());
-        udpAddr_ = new EndPoint(DatabaseDescriptor.getControlPort());
-        /* Listen for application messages */
-        MessagingService.getMessagingInstance().listen(tcpAddr_);
-        /* Listen for control messages */
-        MessagingService.getMessagingInstance().listenUDP(udpAddr_);
-
-        SelectorManager.getSelectorManager().start();
-        SelectorManager.getUdpSelectorManager().start();
-
-        /* starts a load timer thread */
-        loadTimer_.schedule( new LoadDisseminator(), StorageService.threshold_, StorageService.threshold_);
-        
-        /* Start the storage load balancer */
-        storageLoadBalancer_.start();
-        /* Register with the Gossiper for EndPointState notifications */
-        Gossiper.instance().register(this);
-        /*
-         * Start the gossiper with the generation # retrieved from the System
-         * table
-         */
-        Gossiper.instance().start(udpAddr_, storageMetadata_.getGeneration());
-        /* Make sure this token gets gossiped around. */
-        tokenMetadata_.update(storageMetadata_.getStorageId(), StorageService.tcpAddr_);
-        ApplicationState state = new ApplicationState(StorageService.getPartitioner().getTokenFactory().toString(storageMetadata_.getStorageId()));
-        Gossiper.instance().addApplicationState(StorageService.nodeId_, state);
-    }
-
-    public TokenMetadata getTokenMetadata()
-    {
-        return tokenMetadata_.cloneMe();
-    }
-
-    /* TODO: remove later */
-    public void updateTokenMetadata(Token token, EndPoint endpoint)
-    {
-        tokenMetadata_.update(token, endpoint);
-    }
-
-    public IEndPointSnitch getEndPointSnitch()
-    {
-    	return endPointSnitch_;
-    }
-    
-    /*
-     * Given an EndPoint this method will report if the
-     * endpoint is in the same data center as the local
-     * storage endpoint.
-    */
-    public boolean isInSameDataCenter(EndPoint endpoint) throws IOException
-    {
-        return endPointSnitch_.isInSameDataCenter(StorageService.tcpAddr_, endpoint);
-    }
-    
-    /*
-     * This method performs the requisite operations to make
-     * sure that the N replicas are in sync. We do this in the
-     * background when we do not care much about consistency.
-     */
-    public void doConsistencyCheck(Row row, List<EndPoint> endpoints, ReadCommand command)
-    {
-        Runnable consistencySentinel = new ConsistencyManager(row.cloneMe(), endpoints, command);
-        consistencyManager_.submit(consistencySentinel);
-    }
-
-    public Map<Range, List<EndPoint>> getRangeToEndPointMap()
-    {
-        /* Get the token to endpoint map. */
-        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-        /* All the ranges for the tokens */
-        Range[] ranges = getAllRanges(tokenToEndPointMap.keySet());
-        return constructRangeToEndPointMap(ranges);
-    }
-
-    /**
-     * Construct the range to endpoint mapping based on the true view 
-     * of the world. 
-     * @param ranges
-     * @return mapping of ranges to the replicas responsible for them.
-    */
-    public Map<Range, List<EndPoint>> constructRangeToEndPointMap(Range[] ranges)
-    {
-        if (logger_.isDebugEnabled())
-          logger_.debug("Constructing range to endpoint map ...");
-        Map<Range, List<EndPoint>> rangeToEndPointMap = new HashMap<Range, List<EndPoint>>();
-        for ( Range range : ranges )
-        {
-            EndPoint[] endpoints = getNStorageEndPoint(range.right());
-            rangeToEndPointMap.put(range, new ArrayList<EndPoint>( Arrays.asList(endpoints) ) );
-        }
-        if (logger_.isDebugEnabled())
-          logger_.debug("Done constructing range to endpoint map ...");
-        return rangeToEndPointMap;
-    }
-    
-    /**
-     * Construct the range to endpoint mapping based on the view as dictated
-     * by the mapping of token to endpoints passed in. 
-     * @param ranges
-     * @param tokenToEndPointMap mapping of token to endpoints.
-     * @return mapping of ranges to the replicas responsible for them.
-    */
-    public Map<Range, List<EndPoint>> constructRangeToEndPointMap(Range[] ranges, Map<Token, EndPoint> tokenToEndPointMap)
-    {
-        if (logger_.isDebugEnabled())
-          logger_.debug("Constructing range to endpoint map ...");
-        Map<Range, List<EndPoint>> rangeToEndPointMap = new HashMap<Range, List<EndPoint>>();
-        for ( Range range : ranges )
-        {
-            EndPoint[] endpoints = getNStorageEndPoint(range.right(), tokenToEndPointMap);
-            rangeToEndPointMap.put(range, new ArrayList<EndPoint>( Arrays.asList(endpoints) ) );
-        }
-        if (logger_.isDebugEnabled())
-          logger_.debug("Done constructing range to endpoint map ...");
-        return rangeToEndPointMap;
-    }
-    
-    /**
-     * Construct a mapping from endpoint to ranges that endpoint is
-     * responsible for.
-     * @return the mapping from endpoint to the ranges it is responsible
-     * for.
-     */
-    public Map<EndPoint, List<Range>> constructEndPointToRangesMap()
-    {
-        Map<EndPoint, List<Range>> endPointToRangesMap = new HashMap<EndPoint, List<Range>>();
-        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-        Collection<EndPoint> mbrs = tokenToEndPointMap.values();
-        for ( EndPoint mbr : mbrs )
-        {
-            endPointToRangesMap.put(mbr, getRangesForEndPoint(mbr));
-        }
-        return endPointToRangesMap;
-    }
-
-    /**
-     *  Called when there is a change in application state. In particular
-     *  we are interested in new tokens as a result of a new node or an
-     *  existing node moving to a new location on the ring.
-    */
-    public void onChange(EndPoint endpoint, EndPointState epState)
-    {
-        EndPoint ep = new EndPoint(endpoint.getHost(), DatabaseDescriptor.getStoragePort());
-        /* node identifier for this endpoint on the identifier space */
-        ApplicationState nodeIdState = epState.getApplicationState(StorageService.nodeId_);
-        if (nodeIdState != null)
-        {
-            Token newToken = getPartitioner().getTokenFactory().fromString(nodeIdState.getState());
-            if (logger_.isDebugEnabled())
-              logger_.debug("CHANGE IN STATE FOR " + endpoint + " - has token " + nodeIdState.getState());
-            Token oldToken = tokenMetadata_.getToken(ep);
-
-            if ( oldToken != null )
-            {
-                /*
-                 * If oldToken equals the newToken then the node had crashed
-                 * and is coming back up again. If oldToken is not equal to
-                 * the newToken this means that the node is being relocated
-                 * to another position in the ring.
-                */
-                if ( !oldToken.equals(newToken) )
-                {
-                    if (logger_.isDebugEnabled())
-                      logger_.debug("Relocation for endpoint " + ep);
-                    tokenMetadata_.update(newToken, ep);                    
-                }
-                else
-                {
-                    /*
-                     * This means the node crashed and is coming back up.
-                     * Deliver the hints that we have for this endpoint.
-                    */
-                    if (logger_.isDebugEnabled())
-                      logger_.debug("Sending hinted data to " + ep);
-                    doBootstrap(endpoint, BootstrapMode.HINT);
-                }
-            }
-            else
-            {
-                /*
-                 * This is a new node and we just update the token map.
-                */
-                tokenMetadata_.update(newToken, ep);
-            }
-        }
-        else
-        {
-            /*
-             * If we are here and if this node is UP and already has an entry
-             * in the token map. It means that the node was behind a network partition.
-            */
-            if ( epState.isAlive() && tokenMetadata_.isKnownEndPoint(endpoint) )
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug("EndPoint " + ep + " just recovered from a partition. Sending hinted data.");
-                doBootstrap(ep, BootstrapMode.HINT);
-            }
-        }
-
-        /* Check if a bootstrap is in order */
-        ApplicationState loadAllState = epState.getApplicationState(StorageService.loadAll_);
-        if ( loadAllState != null )
-        {
-            String nodes = loadAllState.getState();
-            if ( nodes != null )
-            {
-                doBootstrap(ep, BootstrapMode.FULL);
-            }
-        }
-    }
-
-    /**
-     * Get the count of primary keys from the sampler.
-    */
-    public String getLoadInfo()
-    {
-        long diskSpace = FileUtils.getUsedDiskSpace();
-    	return FileUtils.stringifyFileSize(diskSpace);
-    }
-
-    /**
-     * Get the primary count info for this endpoint.
-     * This is gossiped around and cached in the
-     * StorageLoadBalancer.
-    */
-    public String getLoadInfo(EndPoint ep)
-    {
-        LoadInfo li = storageLoadBalancer_.getLoad(ep);
-        return ( li == null ) ? "N/A" : li.toString();
-    }
-
-    /*
-     * This method updates the token on disk and modifies the cached
-     * StorageMetadata instance. This is only for the local endpoint.
-    */
-    public void updateToken(Token token) throws IOException
-    {
-        /* update the token on disk */
-        SystemTable.updateToken(token);
-        /* Update the storageMetadata cache */
-        storageMetadata_.setStorageId(token);
-        /* Update the token maps */
-        /* Get the old token. This needs to be removed. */
-        tokenMetadata_.update(token, StorageService.tcpAddr_);
-        /* Gossip this new token for the local storage instance */
-        ApplicationState state = new ApplicationState(StorageService.getPartitioner().getTokenFactory().toString(token));
-        Gossiper.instance().addApplicationState(StorageService.nodeId_, state);
-    }
-    
-    /*
-     * This method removes the state associated with this endpoint
-     * from the TokenMetadata instance.
-     * 
-     *  @param endpoint remove the token state associated with this 
-     *         endpoint.
-     */
-    public void removeTokenState(EndPoint endpoint) 
-    {
-        tokenMetadata_.remove(endpoint);
-        /* Remove the state from the Gossiper */
-        Gossiper.instance().removeFromMembership(endpoint);
-    }
-    
-    /*
-     * This method is invoked by the Loader process to force the
-     * node to move from its current position on the token ring, to
-     * a position to be determined based on the keys. This will help
-     * all nodes to start off perfectly load balanced. The array passed
-     * in is evaluated as follows by the loader process:
-     * If there are 10 keys in the system and a totality of 5 nodes
-     * then each node needs to have 2 keys i.e the array is made up
-     * of every 2nd key in the total list of keys.
-    */
-    public void relocate(String[] keys) throws IOException
-    {
-    	if ( keys.length > 0 )
-    	{
-            Token token = tokenMetadata_.getToken(StorageService.tcpAddr_);
-	        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-	        Token[] tokens = tokenToEndPointMap.keySet().toArray(new Token[tokenToEndPointMap.keySet().size()]);
-	        Arrays.sort(tokens);
-	        int index = Arrays.binarySearch(tokens, token) * (keys.length/tokens.length);
-            Token newToken = partitioner_.getInitialToken(keys[index]);
-	        /* update the token */
-	        updateToken(newToken);
-    	}
-    }
-    
-    /**
-     * This method takes a colon separated string of nodes that need
-     * to be bootstrapped. It is also used to filter some source of 
-     * data. Suppose the nodes to be bootstrapped are A, B and C. Then
-     * <i>allNodes</i> must be specified as A:B:C.
-     * 
-    */
-    private void doBootstrap(String nodes)
-    {
-        String[] allNodesAndFilter = nodes.split("-");
-        String nodesToLoad;
-        String filterSources = null;
-        
-        if ( allNodesAndFilter.length == 2 )
-        {
-            nodesToLoad = allNodesAndFilter[0];
-            filterSources = allNodesAndFilter[1];
-        }
-        else
-        {
-            nodesToLoad = allNodesAndFilter[0];
-        }        
-        String[] allNodes = nodesToLoad.split(":");
-        EndPoint[] endpoints = new EndPoint[allNodes.length];
-        Token[] tokens = new Token[allNodes.length];
-        
-        for ( int i = 0; i < allNodes.length; ++i )
-        {
-            endpoints[i] = new EndPoint( allNodes[i].trim(), DatabaseDescriptor.getStoragePort() );
-            tokens[i] = tokenMetadata_.getToken(endpoints[i]);
-        }
-        
-        /* Start the bootstrap algorithm */
-        if ( filterSources == null )
-        bootStrapper_.submit( new BootStrapper(endpoints, tokens) );
-        else
-        {
-            String[] allFilters = filterSources.split(":");
-            EndPoint[] filters = new EndPoint[allFilters.length];
-            for ( int i = 0; i < allFilters.length; ++i )
-            {
-                filters[i] = new EndPoint( allFilters[i].trim(), DatabaseDescriptor.getStoragePort() );
-            }
-            bootStrapper_.submit( new BootStrapper(endpoints, tokens, filters) );
-        }
-    }
-
-    /**
-     * Starts the bootstrap operations for the specified endpoint.
-     * The name of this method is however a misnomer since it does
-     * handoff of data to the specified node when it has crashed
-     * and come back up, marked as alive after a network partition
-     * and also when it joins the ring either as an old node being
-     * relocated or as a brand new node.
-    */
-    public final void doBootstrap(EndPoint endpoint, BootstrapMode mode)
-    {
-        switch ( mode )
-        {
-            case FULL:
-                Token token = tokenMetadata_.getToken(endpoint);
-                bootStrapper_.submit(new BootStrapper(new EndPoint[]{endpoint}, token));
-                break;
-
-            case HINT:
-                /* Deliver the hinted data to this endpoint. */
-                HintedHandOffManager.instance().deliverHints(endpoint);
-                break;
-
-            default:
-                break;
-        }
-    }
-
-    /* This methods belong to the MBean interface */
-    
-    public String getToken(EndPoint ep)
-    {
-        // render a String representation of the Token corresponding to this endpoint
-        // for a human-facing UI.  If there is no such Token then we use "" since
-        // it is not a valid value either for BigIntegerToken or StringToken.
-        EndPoint ep2 = new EndPoint(ep.getHost(), DatabaseDescriptor.getStoragePort());
-        Token token = tokenMetadata_.getToken(ep2);
-        // if there is no token for an endpoint, return an empty string to denote that
-        return ( token == null ) ? "" : token.toString();
-    }
-
-    public String getToken()
-    {
-        return tokenMetadata_.getToken(StorageService.tcpAddr_).toString();
-    }
-
-    public String getLiveNodes()
-    {
-        return stringify(Gossiper.instance().getLiveMembers());
-    }
-
-    public String getUnreachableNodes()
-    {
-        return stringify(Gossiper.instance().getUnreachableMembers());
-    }
-    
-    public int getCurrentGenerationNumber()
-    {
-        return Gossiper.instance().getCurrentGenerationNumber(udpAddr_);
-    }
-
-    /* Helper for the MBean interface */
-    private String stringify(Set<EndPoint> eps)
-    {
-        StringBuilder sb = new StringBuilder("");
-        for (EndPoint ep : eps)
-        {
-            sb.append(ep);
-            sb.append(" ");
-        }
-        return sb.toString();
-    }
-
-    public void loadAll(String nodes)
-    {        
-        doBootstrap(nodes);
-    }
-    
-    public void forceTableCleanup() throws IOException
-    {
-        List<String> tables = DatabaseDescriptor.getTables();
-        for ( String tName : tables )
-        {
-            Table table = Table.open(tName);
-            table.forceCleanup();
-        }
-    }
-    
-    /**
-     * Trigger the immediate compaction of all tables.
-     */
-    public void forceTableCompaction() throws IOException
-    {
-        List<String> tables = DatabaseDescriptor.getTables();
-        for ( String tName : tables )
-        {
-            Table table = Table.open(tName);
-            table.forceCompaction();
-        }        
-    }
-    
-    public void forceHandoff(List<String> dataDirectories, String host) throws IOException
-    {       
-        List<File> filesList = new ArrayList<File>();
-        List<StreamContextManager.StreamContext> streamContexts = new ArrayList<StreamContextManager.StreamContext>();
-        
-        for (String dataDir : dataDirectories)
-        {
-            File directory = new File(dataDir);
-            Collections.addAll(filesList, directory.listFiles());            
-        
-
-            for (File tableDir : directory.listFiles())
-            {
-                String tableName = tableDir.getName();
-
-                for (File file : tableDir.listFiles())
-                {
-                    streamContexts.add(new StreamContextManager.StreamContext(file.getAbsolutePath(), file.length(), tableName));
-                    if (logger_.isDebugEnabled())
-                      logger_.debug("Stream context metadata " + streamContexts);
-                }
-            }
-        }
-        
-        if ( streamContexts.size() > 0 )
-        {
-            EndPoint target = new EndPoint(host, DatabaseDescriptor.getStoragePort());
-            /* Set up the stream manager with the files that need to streamed */
-            StreamManager.instance(target).addFilesToStream((StreamContextManager.StreamContext[]) streamContexts.toArray());
-            /* Send the bootstrap initiate message */
-            BootstrapInitiateMessage biMessage = new BootstrapInitiateMessage((StreamContextManager.StreamContext[]) streamContexts.toArray());
-            Message message = BootstrapInitiateMessage.makeBootstrapInitiateMessage(biMessage);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Sending a bootstrap initiate message to " + target + " ...");
-            MessagingService.getMessagingInstance().sendOneWay(message, target);                
-            if (logger_.isDebugEnabled())
-              logger_.debug("Waiting for transfer to " + target + " to complete");
-            StreamManager.instance(target).waitForStreamCompletion();
-            if (logger_.isDebugEnabled())
-              logger_.debug("Done with transfer to " + target);  
-        }
-    }
-
-    /**
-     * Takes the snapshot for a given table.
-     * 
-     * @param tableName the name of the table.
-     * @param tag   the tag given to the snapshot (null is permissible)
-     */
-    public void takeSnapshot(String tableName, String tag) throws IOException
-    {
-    	if (DatabaseDescriptor.getTable(tableName) == null)
-        {
-            throw new IOException("Table " + tableName + "does not exist");
-    	}
-        Table tableInstance = Table.open(tableName);
-        tableInstance.snapshot(tag);
-    }
-    
-    /**
-     * Takes a snapshot for every table.
-     * 
-     * @param tag the tag given to the snapshot (null is permissible)
-     */
-    public void takeAllSnapshot(String tag) throws IOException
-    {
-    	for (String tableName: DatabaseDescriptor.getTables())
-        {
-            Table tableInstance = Table.open(tableName);
-            tableInstance.snapshot(tag);
-    	}
-    }
-
-    /**
-     * Remove all the existing snapshots.
-     */
-    public void clearSnapshot() throws IOException
-    {
-    	for (String tableName: DatabaseDescriptor.getTables())
-        {
-            Table tableInstance = Table.open(tableName);
-            tableInstance.clearSnapshot();
-    	}
-        if (logger_.isDebugEnabled())
-            logger_.debug("Cleared out all snapshot directories");
-    }
-
-    /* End of MBean interface methods */
-    
-    /**
-     * This method returns the predecessor of the endpoint ep on the identifier
-     * space.
-     */
-    EndPoint getPredecessor(EndPoint ep)
-    {
-        Token token = tokenMetadata_.getToken(ep);
-        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
-        Collections.sort(tokens);
-        int index = Collections.binarySearch(tokens, token);
-        return (index == 0) ? tokenToEndPointMap.get(tokens
-                .get(tokens.size() - 1)) : tokenToEndPointMap.get(tokens
-                .get(--index));
-    }
-
-    /*
-     * This method returns the successor of the endpoint ep on the identifier
-     * space.
-     */
-    public EndPoint getSuccessor(EndPoint ep)
-    {
-        Token token = tokenMetadata_.getToken(ep);
-        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
-        Collections.sort(tokens);
-        int index = Collections.binarySearch(tokens, token);
-        return (index == (tokens.size() - 1)) ? tokenToEndPointMap
-                .get(tokens.get(0))
-                : tokenToEndPointMap.get(tokens.get(++index));
-    }
-
-    /**
-     * Get the primary range for the specified endpoint.
-     * @param ep endpoint we are interested in.
-     * @return range for the specified endpoint.
-     */
-    public Range getPrimaryRangeForEndPoint(EndPoint ep)
-    {
-        Token right = tokenMetadata_.getToken(ep);
-        EndPoint predecessor = getPredecessor(ep);
-        Token left = tokenMetadata_.getToken(predecessor);
-        return new Range(left, right);
-    }
-    
-    /**
-     * Get all ranges an endpoint is responsible for.
-     * @param ep endpoint we are interested in.
-     * @return ranges for the specified endpoint.
-     */
-    List<Range> getRangesForEndPoint(EndPoint ep)
-    {
-        List<Range> ranges = new ArrayList<Range>();
-        ranges.add( getPrimaryRangeForEndPoint(ep) );
-        
-        EndPoint predecessor = ep;
-        int count = DatabaseDescriptor.getReplicationFactor() - 1;
-        for ( int i = 0; i < count; ++i )
-        {
-            predecessor = getPredecessor(predecessor);
-            ranges.add( getPrimaryRangeForEndPoint(predecessor) );
-        }
-        
-        return ranges;
-    }
-    
-    /**
-     * Get all ranges that span the ring as per
-     * current snapshot of the token distribution.
-     * @return all ranges in sorted order.
-     */
-    public Range[] getAllRanges()
-    {
-        return getAllRanges(tokenMetadata_.cloneTokenEndPointMap().keySet());
-    }
-    
-    /**
-     * Get all ranges that span the ring given a set
-     * of tokens. All ranges are in sorted order of 
-     * ranges.
-     * @return ranges in sorted order
-    */
-    public Range[] getAllRanges(Set<Token> tokens)
-    {
-        List<Range> ranges = new ArrayList<Range>();
-        List<Token> allTokens = new ArrayList<Token>(tokens);
-        Collections.sort(allTokens);
-        int size = allTokens.size();
-        for ( int i = 1; i < size; ++i )
-        {
-            Range range = new Range( allTokens.get(i - 1), allTokens.get(i) );
-            ranges.add(range);
-        }
-        Range range = new Range( allTokens.get(size - 1), allTokens.get(0) );
-        ranges.add(range);
-        return ranges.toArray( new Range[0] );
-    }
-
-    /**
-     * This method returns the endpoint that is responsible for storing the
-     * specified key.
-     *
-     * @param key - key for which we need to find the endpoint
-     * @return value - the endpoint responsible for this key
-     */
-    public EndPoint getPrimary(String key)
-    {
-        EndPoint endpoint = StorageService.tcpAddr_;
-        Token token = partitioner_.getInitialToken(key);
-        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
-        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
-        if (tokens.size() > 0)
-        {
-            Collections.sort(tokens);
-            int index = Collections.binarySearch(tokens, token);
-            if (index >= 0)
-            {
-                /*
-                 * retrieve the endpoint based on the token at this index in the
-                 * tokens list
-                 */
-                endpoint = tokenToEndPointMap.get(tokens.get(index));
-            }
-            else
-            {
-                index = (index + 1) * (-1);
-                if (index < tokens.size())
-                    endpoint = tokenToEndPointMap.get(tokens.get(index));
-                else
-                    endpoint = tokenToEndPointMap.get(tokens.get(0));
-            }
-        }
-        return endpoint;
-    }
-
-    /**
-     * This method determines whether the local endpoint is the
-     * primary for the given key.
-     * @param key
-     * @return true if the local endpoint is the primary replica.
-    */
-    public boolean isPrimary(String key)
-    {
-        EndPoint endpoint = getPrimary(key);
-        return StorageService.tcpAddr_.equals(endpoint);
-    }
-
-    /**
-     * This method returns the N endpoints that are responsible for storing the
-     * specified key i.e for replication.
-     *
-     * @param key - key for which we need to find the endpoint return value -
-     * the endpoint responsible for this key
-     */
-    public EndPoint[] getNStorageEndPoint(String key)
-    {
-        return nodePicker_.getStorageEndPoints(partitioner_.getInitialToken(key));
-    }
-    
-    private Map<String, EndPoint[]> getNStorageEndPoints(String[] keys)
-    {
-    	return nodePicker_.getStorageEndPoints(keys);
-    }
-    
-    
-    /**
-     * This method attempts to return N endpoints that are responsible for storing the
-     * specified key i.e for replication.
-     *
-     * @param key - key for which we need to find the endpoint return value -
-     * the endpoint responsible for this key
-     */
-    public List<EndPoint> getNLiveStorageEndPoint(String key)
-    {
-    	List<EndPoint> liveEps = new ArrayList<EndPoint>();
-    	EndPoint[] endpoints = getNStorageEndPoint(key);
-    	
-    	for ( EndPoint endpoint : endpoints )
-    	{
-    		if ( FailureDetector.instance().isAlive(endpoint) )
-    			liveEps.add(endpoint);
-    	}
-    	
-    	return liveEps;
-    }
-
-    /**
-     * This method returns the N endpoints that are responsible for storing the
-     * specified key i.e for replication.
-     *
-     * @param key - key for which we need to find the endpoint return value -
-     * the endpoint responsible for this key
-     */
-    public Map<EndPoint, EndPoint> getNStorageEndPointMap(String key)
-    {
-        return nodePicker_.getHintedStorageEndPoints(partitioner_.getInitialToken(key));
-    }
-
-    /**
-     * This method returns the N endpoints that are responsible for storing the
-     * specified token i.e for replication.
-     *
-     * @param token - position on the ring
-     */
-    public EndPoint[] getNStorageEndPoint(Token token)
-    {
-        return nodePicker_.getStorageEndPoints(token);
-    }
-    
-    /**
-     * This method returns the N endpoints that are responsible for storing the
-     * specified token i.e for replication and are based on the token to endpoint 
-     * mapping that is passed in.
-     *
-     * @param token - position on the ring
-     * @param tokens - w/o the following tokens in the token list
-     */
-    protected EndPoint[] getNStorageEndPoint(Token token, Map<Token, EndPoint> tokenToEndPointMap)
-    {
-        return nodePicker_.getStorageEndPoints(token, tokenToEndPointMap);
-    }
-
-    /**
-     * This function finds the most suitable endpoint given a key.
-     * It checks for locality and alive test.
-     */
-	public EndPoint findSuitableEndPoint(String key) throws IOException
-	{
-		EndPoint[] endpoints = getNStorageEndPoint(key);
-		for(EndPoint endPoint: endpoints)
-		{
-			if(endPoint.equals(StorageService.getLocalStorageEndPoint()))
-			{
-				return endPoint;
-			}
-		}
-		int j = 0;
-		for ( ; j < endpoints.length; ++j )
-		{
-			if ( StorageService.instance().isInSameDataCenter(endpoints[j]) && FailureDetector.instance().isAlive(endpoints[j]) )
-			{
-				return endpoints[j];
-			}
-		}
-		// We have tried to be really nice but looks like there are no servers 
-		// in the local data center that are alive and can service this request so 
-		// just send it to the first alive guy and see if we get anything.
-		j = 0;
-		for ( ; j < endpoints.length; ++j )
-		{
-			if ( FailureDetector.instance().isAlive(endpoints[j]) )
-			{
-				if (logger_.isDebugEnabled())
-				  logger_.debug("EndPoint " + endpoints[j] + " is alive so get data from it.");
-				return endpoints[j];
-			}
-		}
-		return null;
-	}
-	
-	public Map<String, EndPoint> findSuitableEndPoints(String[] keys) throws IOException
-	{
-		Map<String, EndPoint> suitableEndPoints = new HashMap<String, EndPoint>();
-		Map<String, EndPoint[]> results = getNStorageEndPoints(keys);
-		for ( String key : keys )
-		{
-			EndPoint[] endpoints = results.get(key);
-			/* indicates if we have to move on to the next key */
-			boolean moveOn = false;
-			for(EndPoint endPoint: endpoints)
-			{
-				if(endPoint.equals(StorageService.getLocalStorageEndPoint()))
-				{
-					suitableEndPoints.put(key, endPoint);
-					moveOn = true;
-					break;
-				}
-			}
-			
-			if ( moveOn )
-				continue;
-				
-			int j = 0;
-			for ( ; j < endpoints.length; ++j )
-			{
-				if ( StorageService.instance().isInSameDataCenter(endpoints[j]) && FailureDetector.instance().isAlive(endpoints[j]) )
-				{
-					if (logger_.isDebugEnabled())
-					  logger_.debug("EndPoint " + endpoints[j] + " is in the same data center as local storage endpoint.");
-					suitableEndPoints.put(key, endpoints[j]);
-					moveOn = true;
-					break;
-				}
-			}
-			
-			if ( moveOn )
-				continue;
-			
-			// We have tried to be really nice but looks like there are no servers 
-			// in the local data center that are alive and can service this request so 
-			// just send it to the first alive guy and see if we get anything.
-			j = 0;
-			for ( ; j < endpoints.length; ++j )
-			{
-				if ( FailureDetector.instance().isAlive(endpoints[j]) )
-				{
-					if (logger_.isDebugEnabled())
-					  logger_.debug("EndPoint " + endpoints[j] + " is alive so get data from it.");
-					suitableEndPoints.put(key, endpoints[j]);
-					break;
-				}
-			}
-		}
-		return suitableEndPoints;
-	}
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.File;
+import java.io.IOException;
+import java.lang.management.ManagementFactory;
+import java.util.*;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+import javax.management.MBeanServer;
+import javax.management.ObjectName;
+
+import org.apache.cassandra.concurrent.*;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.dht.*;
+import org.apache.cassandra.gms.*;
+import org.apache.cassandra.locator.*;
+import org.apache.cassandra.net.*;
+import org.apache.cassandra.net.io.StreamContextManager;
+import org.apache.cassandra.tools.MembershipCleanerVerbHandler;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/*
+ * This abstraction contains the token/identifier of this node
+ * on the identifier space. This token gets gossiped around.
+ * This class will also maintain histograms of the load information
+ * of other nodes in the cluster.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public final class StorageService implements IEndPointStateChangeSubscriber, StorageServiceMBean
+{
+    private static Logger logger_ = Logger.getLogger(StorageService.class);     
+    private final static String nodeId_ = "NODE-IDENTIFIER";
+    private final static String loadAll_ = "LOAD-ALL";
+    /* Gossip load after every 5 mins. */
+    private static final long threshold_ = 5 * 60 * 1000L;
+    
+    /* All stage identifiers */
+    public final static String mutationStage_ = "ROW-MUTATION-STAGE";
+    public final static String readStage_ = "ROW-READ-STAGE";
+    
+    /* All verb handler identifiers */
+    public final static String mutationVerbHandler_ = "ROW-MUTATION-VERB-HANDLER";
+    public final static String tokenVerbHandler_ = "TOKEN-VERB-HANDLER";
+    public final static String loadVerbHandler_ = "LOAD-VERB-HANDLER";
+    public final static String binaryVerbHandler_ = "BINARY-VERB-HANDLER";
+    public final static String readRepairVerbHandler_ = "READ-REPAIR-VERB-HANDLER";
+    public final static String readVerbHandler_ = "ROW-READ-VERB-HANDLER";
+    public final static String bootStrapInitiateVerbHandler_ = "BOOTSTRAP-INITIATE-VERB-HANDLER";
+    public final static String bootStrapInitiateDoneVerbHandler_ = "BOOTSTRAP-INITIATE-DONE-VERB-HANDLER";
+    public final static String bootStrapTerminateVerbHandler_ = "BOOTSTRAP-TERMINATE-VERB-HANDLER";
+    public final static String dataFileVerbHandler_ = "DATA-FILE-VERB-HANDLER";
+    public final static String mbrshipCleanerVerbHandler_ = "MBRSHIP-CLEANER-VERB-HANDLER";
+    public final static String bsMetadataVerbHandler_ = "BS-METADATA-VERB-HANDLER";
+    public final static String calloutDeployVerbHandler_ = "CALLOUT-DEPLOY-VERB-HANDLER";
+    public static String rangeVerbHandler_ = "RANGE-VERB-HANDLER";
+
+    public static enum ConsistencyLevel
+    {
+    	WEAK,
+    	STRONG
+    }
+
+    private static StorageService instance_;
+    /* Used to lock the factory for creation of StorageService instance */
+    private static Lock createLock_ = new ReentrantLock();
+    private static EndPoint tcpAddr_;
+    private static EndPoint udpAddr_;
+    private static IPartitioner partitioner_;
+
+    public static EndPoint getLocalStorageEndPoint()
+    {
+        return tcpAddr_;
+    }
+
+    public static EndPoint getLocalControlEndPoint()
+    {
+        return udpAddr_;
+    }
+
+    public static IPartitioner getPartitioner() {
+        return partitioner_;
+    }
+    
+    public static enum BootstrapMode
+    {
+        HINT,
+        FULL
+    }
+
+    static
+    {
+        partitioner_ = DatabaseDescriptor.getPartitioner();
+    }
+
+
+    public static class BootstrapInitiateDoneVerbHandler implements IVerbHandler
+    {
+        private static Logger logger_ = Logger.getLogger( BootstrapInitiateDoneVerbHandler.class );
+
+        public void doVerb(Message message)
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Received a bootstrap initiate done message ...");
+            /* Let the Stream Manager do his thing. */
+            StreamManager.instance(message.getFrom()).start();            
+        }
+    }
+
+    /*
+     * Factory method that gets an instance of the StorageService
+     * class.
+    */
+    public static StorageService instance()
+    {
+        if ( instance_ == null )
+        {
+            StorageService.createLock_.lock();
+            try
+            {
+                if ( instance_ == null )
+                {
+                    try
+                    {
+                        instance_ = new StorageService();
+                    }
+                    catch ( Throwable th )
+                    {
+                        logger_.error(LogUtil.throwableToString(th));
+                        System.exit(1);
+                    }
+                }
+            }
+            finally
+            {
+                createLock_.unlock();
+            }
+        }
+        return instance_;
+    }
+
+    /*
+     * This is the endpoint snitch which depends on the network architecture. We
+     * need to keep this information for each endpoint so that we make decisions
+     * while doing things like replication etc.
+     *
+     */
+    private IEndPointSnitch endPointSnitch_;
+
+    /* This abstraction maintains the token/endpoint metadata information */
+    private TokenMetadata tokenMetadata_ = new TokenMetadata();
+    private SystemTable.StorageMetadata storageMetadata_;
+
+    /* Timer is used to disseminate load information */
+    private Timer loadTimer_ = new Timer(false);
+
+    /* This thread pool is used to do the bootstrap for a new node */
+    private ExecutorService bootStrapper_ = new DebuggableThreadPoolExecutor("BOOT-STRAPPER");
+    
+    /* This thread pool does consistency checks when the client doesn't care about consistency */
+    private ExecutorService consistencyManager_;
+
+    /* This is the entity that tracks load information of all nodes in the cluster */
+    private StorageLoadBalancer storageLoadBalancer_;
+    /* We use this interface to determine where replicas need to be placed */
+    private IReplicaPlacementStrategy nodePicker_;
+    
+    /*
+     * Registers with Management Server
+     */
+    private void init()
+    {
+        try
+        {
+            MBeanServer mbs = ManagementFactory.getPlatformMBeanServer();
+            mbs.registerMBean(this, new ObjectName(
+                    "org.apache.cassandra.service:type=StorageService"));
+        }
+        catch (Exception e)
+        {
+            logger_.error(LogUtil.throwableToString(e));
+        }
+    }
+
+    public StorageService()
+    {
+        init();
+        storageLoadBalancer_ = new StorageLoadBalancer(this);
+        endPointSnitch_ = DatabaseDescriptor.getEndPointSnitch();
+
+        /* register the verb handlers */
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.tokenVerbHandler_, new TokenUpdateVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.binaryVerbHandler_, new BinaryVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.loadVerbHandler_, new LoadVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.mutationVerbHandler_, new RowMutationVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.readRepairVerbHandler_, new ReadRepairVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.readVerbHandler_, new ReadVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bootStrapInitiateVerbHandler_, new Table.BootStrapInitiateVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bootStrapInitiateDoneVerbHandler_, new StorageService.BootstrapInitiateDoneVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bootStrapTerminateVerbHandler_, new StreamManager.BootstrapTerminateVerbHandler());
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.dataFileVerbHandler_, new DataFileVerbHandler() );
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.mbrshipCleanerVerbHandler_, new MembershipCleanerVerbHandler() );
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.bsMetadataVerbHandler_, new BootstrapMetadataVerbHandler() );        
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.calloutDeployVerbHandler_, new CalloutDeployVerbHandler() );
+        MessagingService.getMessagingInstance().registerVerbHandlers(StorageService.rangeVerbHandler_, new RangeVerbHandler());
+        
+        /* register the stage for the mutations */
+        consistencyManager_ = new DebuggableThreadPoolExecutor(DatabaseDescriptor.getConsistencyThreads(),
+                                                               DatabaseDescriptor.getConsistencyThreads(),
+                                                               Integer.MAX_VALUE, TimeUnit.SECONDS,
+                                                               new LinkedBlockingQueue<Runnable>(), new ThreadFactoryImpl("CONSISTENCY-MANAGER"));
+        
+        StageManager.registerStage(StorageService.mutationStage_,
+                                   new MultiThreadedStage(StorageService.mutationStage_, DatabaseDescriptor.getConcurrentWriters()));
+        StageManager.registerStage(StorageService.readStage_,
+                                   new MultiThreadedStage(StorageService.readStage_, DatabaseDescriptor.getConcurrentReaders()));
+
+        Class cls = DatabaseDescriptor.getReplicaPlacementStrategyClass();
+        Class [] parameterTypes = new Class[] { TokenMetadata.class, IPartitioner.class, int.class, int.class};
+        try
+        {
+            nodePicker_ = (IReplicaPlacementStrategy) cls.getConstructor(parameterTypes).newInstance(tokenMetadata_, partitioner_, DatabaseDescriptor.getReplicationFactor(), DatabaseDescriptor.getStoragePort());
+        }
+        catch (Exception e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    public void start() throws IOException
+    {
+        storageMetadata_ = SystemTable.initMetadata();
+        tcpAddr_ = new EndPoint(DatabaseDescriptor.getStoragePort());
+        udpAddr_ = new EndPoint(DatabaseDescriptor.getControlPort());
+        /* Listen for application messages */
+        MessagingService.getMessagingInstance().listen(tcpAddr_);
+        /* Listen for control messages */
+        MessagingService.getMessagingInstance().listenUDP(udpAddr_);
+
+        SelectorManager.getSelectorManager().start();
+        SelectorManager.getUdpSelectorManager().start();
+
+        /* starts a load timer thread */
+        loadTimer_.schedule( new LoadDisseminator(), StorageService.threshold_, StorageService.threshold_);
+        
+        /* Start the storage load balancer */
+        storageLoadBalancer_.start();
+        /* Register with the Gossiper for EndPointState notifications */
+        Gossiper.instance().register(this);
+        /*
+         * Start the gossiper with the generation # retrieved from the System
+         * table
+         */
+        Gossiper.instance().start(udpAddr_, storageMetadata_.getGeneration());
+        /* Make sure this token gets gossiped around. */
+        tokenMetadata_.update(storageMetadata_.getStorageId(), StorageService.tcpAddr_);
+        ApplicationState state = new ApplicationState(StorageService.getPartitioner().getTokenFactory().toString(storageMetadata_.getStorageId()));
+        Gossiper.instance().addApplicationState(StorageService.nodeId_, state);
+    }
+
+    public TokenMetadata getTokenMetadata()
+    {
+        return tokenMetadata_.cloneMe();
+    }
+
+    /* TODO: remove later */
+    public void updateTokenMetadata(Token token, EndPoint endpoint)
+    {
+        tokenMetadata_.update(token, endpoint);
+    }
+
+    public IEndPointSnitch getEndPointSnitch()
+    {
+    	return endPointSnitch_;
+    }
+    
+    /*
+     * Given an EndPoint this method will report if the
+     * endpoint is in the same data center as the local
+     * storage endpoint.
+    */
+    public boolean isInSameDataCenter(EndPoint endpoint) throws IOException
+    {
+        return endPointSnitch_.isInSameDataCenter(StorageService.tcpAddr_, endpoint);
+    }
+    
+    /*
+     * This method performs the requisite operations to make
+     * sure that the N replicas are in sync. We do this in the
+     * background when we do not care much about consistency.
+     */
+    public void doConsistencyCheck(Row row, List<EndPoint> endpoints, ReadCommand command)
+    {
+        Runnable consistencySentinel = new ConsistencyManager(row.cloneMe(), endpoints, command);
+        consistencyManager_.submit(consistencySentinel);
+    }
+
+    public Map<Range, List<EndPoint>> getRangeToEndPointMap()
+    {
+        /* Get the token to endpoint map. */
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        /* All the ranges for the tokens */
+        Range[] ranges = getAllRanges(tokenToEndPointMap.keySet());
+        return constructRangeToEndPointMap(ranges);
+    }
+
+    /**
+     * Construct the range to endpoint mapping based on the true view 
+     * of the world. 
+     * @param ranges
+     * @return mapping of ranges to the replicas responsible for them.
+    */
+    public Map<Range, List<EndPoint>> constructRangeToEndPointMap(Range[] ranges)
+    {
+        if (logger_.isDebugEnabled())
+          logger_.debug("Constructing range to endpoint map ...");
+        Map<Range, List<EndPoint>> rangeToEndPointMap = new HashMap<Range, List<EndPoint>>();
+        for ( Range range : ranges )
+        {
+            EndPoint[] endpoints = getNStorageEndPoint(range.right());
+            rangeToEndPointMap.put(range, new ArrayList<EndPoint>( Arrays.asList(endpoints) ) );
+        }
+        if (logger_.isDebugEnabled())
+          logger_.debug("Done constructing range to endpoint map ...");
+        return rangeToEndPointMap;
+    }
+    
+    /**
+     * Construct the range to endpoint mapping based on the view as dictated
+     * by the mapping of token to endpoints passed in. 
+     * @param ranges
+     * @param tokenToEndPointMap mapping of token to endpoints.
+     * @return mapping of ranges to the replicas responsible for them.
+    */
+    public Map<Range, List<EndPoint>> constructRangeToEndPointMap(Range[] ranges, Map<Token, EndPoint> tokenToEndPointMap)
+    {
+        if (logger_.isDebugEnabled())
+          logger_.debug("Constructing range to endpoint map ...");
+        Map<Range, List<EndPoint>> rangeToEndPointMap = new HashMap<Range, List<EndPoint>>();
+        for ( Range range : ranges )
+        {
+            EndPoint[] endpoints = getNStorageEndPoint(range.right(), tokenToEndPointMap);
+            rangeToEndPointMap.put(range, new ArrayList<EndPoint>( Arrays.asList(endpoints) ) );
+        }
+        if (logger_.isDebugEnabled())
+          logger_.debug("Done constructing range to endpoint map ...");
+        return rangeToEndPointMap;
+    }
+    
+    /**
+     * Construct a mapping from endpoint to ranges that endpoint is
+     * responsible for.
+     * @return the mapping from endpoint to the ranges it is responsible
+     * for.
+     */
+    public Map<EndPoint, List<Range>> constructEndPointToRangesMap()
+    {
+        Map<EndPoint, List<Range>> endPointToRangesMap = new HashMap<EndPoint, List<Range>>();
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        Collection<EndPoint> mbrs = tokenToEndPointMap.values();
+        for ( EndPoint mbr : mbrs )
+        {
+            endPointToRangesMap.put(mbr, getRangesForEndPoint(mbr));
+        }
+        return endPointToRangesMap;
+    }
+
+    /**
+     *  Called when there is a change in application state. In particular
+     *  we are interested in new tokens as a result of a new node or an
+     *  existing node moving to a new location on the ring.
+    */
+    public void onChange(EndPoint endpoint, EndPointState epState)
+    {
+        EndPoint ep = new EndPoint(endpoint.getHost(), DatabaseDescriptor.getStoragePort());
+        /* node identifier for this endpoint on the identifier space */
+        ApplicationState nodeIdState = epState.getApplicationState(StorageService.nodeId_);
+        if (nodeIdState != null)
+        {
+            Token newToken = getPartitioner().getTokenFactory().fromString(nodeIdState.getState());
+            if (logger_.isDebugEnabled())
+              logger_.debug("CHANGE IN STATE FOR " + endpoint + " - has token " + nodeIdState.getState());
+            Token oldToken = tokenMetadata_.getToken(ep);
+
+            if ( oldToken != null )
+            {
+                /*
+                 * If oldToken equals the newToken then the node had crashed
+                 * and is coming back up again. If oldToken is not equal to
+                 * the newToken this means that the node is being relocated
+                 * to another position in the ring.
+                */
+                if ( !oldToken.equals(newToken) )
+                {
+                    if (logger_.isDebugEnabled())
+                      logger_.debug("Relocation for endpoint " + ep);
+                    tokenMetadata_.update(newToken, ep);                    
+                }
+                else
+                {
+                    /*
+                     * This means the node crashed and is coming back up.
+                     * Deliver the hints that we have for this endpoint.
+                    */
+                    if (logger_.isDebugEnabled())
+                      logger_.debug("Sending hinted data to " + ep);
+                    doBootstrap(endpoint, BootstrapMode.HINT);
+                }
+            }
+            else
+            {
+                /*
+                 * This is a new node and we just update the token map.
+                */
+                tokenMetadata_.update(newToken, ep);
+            }
+        }
+        else
+        {
+            /*
+             * If we are here and if this node is UP and already has an entry
+             * in the token map. It means that the node was behind a network partition.
+            */
+            if ( epState.isAlive() && tokenMetadata_.isKnownEndPoint(endpoint) )
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug("EndPoint " + ep + " just recovered from a partition. Sending hinted data.");
+                doBootstrap(ep, BootstrapMode.HINT);
+            }
+        }
+
+        /* Check if a bootstrap is in order */
+        ApplicationState loadAllState = epState.getApplicationState(StorageService.loadAll_);
+        if ( loadAllState != null )
+        {
+            String nodes = loadAllState.getState();
+            if ( nodes != null )
+            {
+                doBootstrap(ep, BootstrapMode.FULL);
+            }
+        }
+    }
+
+    /**
+     * Get the count of primary keys from the sampler.
+    */
+    public String getLoadInfo()
+    {
+        long diskSpace = FileUtils.getUsedDiskSpace();
+    	return FileUtils.stringifyFileSize(diskSpace);
+    }
+
+    /**
+     * Get the primary count info for this endpoint.
+     * This is gossiped around and cached in the
+     * StorageLoadBalancer.
+    */
+    public String getLoadInfo(EndPoint ep)
+    {
+        LoadInfo li = storageLoadBalancer_.getLoad(ep);
+        return ( li == null ) ? "N/A" : li.toString();
+    }
+
+    /*
+     * This method updates the token on disk and modifies the cached
+     * StorageMetadata instance. This is only for the local endpoint.
+    */
+    public void updateToken(Token token) throws IOException
+    {
+        /* update the token on disk */
+        SystemTable.updateToken(token);
+        /* Update the storageMetadata cache */
+        storageMetadata_.setStorageId(token);
+        /* Update the token maps */
+        /* Get the old token. This needs to be removed. */
+        tokenMetadata_.update(token, StorageService.tcpAddr_);
+        /* Gossip this new token for the local storage instance */
+        ApplicationState state = new ApplicationState(StorageService.getPartitioner().getTokenFactory().toString(token));
+        Gossiper.instance().addApplicationState(StorageService.nodeId_, state);
+    }
+    
+    /*
+     * This method removes the state associated with this endpoint
+     * from the TokenMetadata instance.
+     * 
+     *  @param endpoint remove the token state associated with this 
+     *         endpoint.
+     */
+    public void removeTokenState(EndPoint endpoint) 
+    {
+        tokenMetadata_.remove(endpoint);
+        /* Remove the state from the Gossiper */
+        Gossiper.instance().removeFromMembership(endpoint);
+    }
+    
+    /*
+     * This method is invoked by the Loader process to force the
+     * node to move from its current position on the token ring, to
+     * a position to be determined based on the keys. This will help
+     * all nodes to start off perfectly load balanced. The array passed
+     * in is evaluated as follows by the loader process:
+     * If there are 10 keys in the system and a totality of 5 nodes
+     * then each node needs to have 2 keys i.e the array is made up
+     * of every 2nd key in the total list of keys.
+    */
+    public void relocate(String[] keys) throws IOException
+    {
+    	if ( keys.length > 0 )
+    	{
+            Token token = tokenMetadata_.getToken(StorageService.tcpAddr_);
+	        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+	        Token[] tokens = tokenToEndPointMap.keySet().toArray(new Token[tokenToEndPointMap.keySet().size()]);
+	        Arrays.sort(tokens);
+	        int index = Arrays.binarySearch(tokens, token) * (keys.length/tokens.length);
+            Token newToken = partitioner_.getInitialToken(keys[index]);
+	        /* update the token */
+	        updateToken(newToken);
+    	}
+    }
+    
+    /**
+     * This method takes a colon separated string of nodes that need
+     * to be bootstrapped. It is also used to filter some source of 
+     * data. Suppose the nodes to be bootstrapped are A, B and C. Then
+     * <i>allNodes</i> must be specified as A:B:C.
+     * 
+    */
+    private void doBootstrap(String nodes)
+    {
+        String[] allNodesAndFilter = nodes.split("-");
+        String nodesToLoad;
+        String filterSources = null;
+        
+        if ( allNodesAndFilter.length == 2 )
+        {
+            nodesToLoad = allNodesAndFilter[0];
+            filterSources = allNodesAndFilter[1];
+        }
+        else
+        {
+            nodesToLoad = allNodesAndFilter[0];
+        }        
+        String[] allNodes = nodesToLoad.split(":");
+        EndPoint[] endpoints = new EndPoint[allNodes.length];
+        Token[] tokens = new Token[allNodes.length];
+        
+        for ( int i = 0; i < allNodes.length; ++i )
+        {
+            endpoints[i] = new EndPoint( allNodes[i].trim(), DatabaseDescriptor.getStoragePort() );
+            tokens[i] = tokenMetadata_.getToken(endpoints[i]);
+        }
+        
+        /* Start the bootstrap algorithm */
+        if ( filterSources == null )
+        bootStrapper_.submit( new BootStrapper(endpoints, tokens) );
+        else
+        {
+            String[] allFilters = filterSources.split(":");
+            EndPoint[] filters = new EndPoint[allFilters.length];
+            for ( int i = 0; i < allFilters.length; ++i )
+            {
+                filters[i] = new EndPoint( allFilters[i].trim(), DatabaseDescriptor.getStoragePort() );
+            }
+            bootStrapper_.submit( new BootStrapper(endpoints, tokens, filters) );
+        }
+    }
+
+    /**
+     * Starts the bootstrap operations for the specified endpoint.
+     * The name of this method is however a misnomer since it does
+     * handoff of data to the specified node when it has crashed
+     * and come back up, marked as alive after a network partition
+     * and also when it joins the ring either as an old node being
+     * relocated or as a brand new node.
+    */
+    public final void doBootstrap(EndPoint endpoint, BootstrapMode mode)
+    {
+        switch ( mode )
+        {
+            case FULL:
+                Token token = tokenMetadata_.getToken(endpoint);
+                bootStrapper_.submit(new BootStrapper(new EndPoint[]{endpoint}, token));
+                break;
+
+            case HINT:
+                /* Deliver the hinted data to this endpoint. */
+                HintedHandOffManager.instance().deliverHints(endpoint);
+                break;
+
+            default:
+                break;
+        }
+    }
+
+    /* This methods belong to the MBean interface */
+    
+    public String getToken(EndPoint ep)
+    {
+        // render a String representation of the Token corresponding to this endpoint
+        // for a human-facing UI.  If there is no such Token then we use "" since
+        // it is not a valid value either for BigIntegerToken or StringToken.
+        EndPoint ep2 = new EndPoint(ep.getHost(), DatabaseDescriptor.getStoragePort());
+        Token token = tokenMetadata_.getToken(ep2);
+        // if there is no token for an endpoint, return an empty string to denote that
+        return ( token == null ) ? "" : token.toString();
+    }
+
+    public String getToken()
+    {
+        return tokenMetadata_.getToken(StorageService.tcpAddr_).toString();
+    }
+
+    public String getLiveNodes()
+    {
+        return stringify(Gossiper.instance().getLiveMembers());
+    }
+
+    public String getUnreachableNodes()
+    {
+        return stringify(Gossiper.instance().getUnreachableMembers());
+    }
+    
+    public int getCurrentGenerationNumber()
+    {
+        return Gossiper.instance().getCurrentGenerationNumber(udpAddr_);
+    }
+
+    /* Helper for the MBean interface */
+    private String stringify(Set<EndPoint> eps)
+    {
+        StringBuilder sb = new StringBuilder("");
+        for (EndPoint ep : eps)
+        {
+            sb.append(ep);
+            sb.append(" ");
+        }
+        return sb.toString();
+    }
+
+    public void loadAll(String nodes)
+    {        
+        doBootstrap(nodes);
+    }
+    
+    public void forceTableCleanup() throws IOException
+    {
+        List<String> tables = DatabaseDescriptor.getTables();
+        for ( String tName : tables )
+        {
+            Table table = Table.open(tName);
+            table.forceCleanup();
+        }
+    }
+    
+    /**
+     * Trigger the immediate compaction of all tables.
+     */
+    public void forceTableCompaction() throws IOException
+    {
+        List<String> tables = DatabaseDescriptor.getTables();
+        for ( String tName : tables )
+        {
+            Table table = Table.open(tName);
+            table.forceCompaction();
+        }        
+    }
+    
+    public void forceHandoff(List<String> dataDirectories, String host) throws IOException
+    {       
+        List<File> filesList = new ArrayList<File>();
+        List<StreamContextManager.StreamContext> streamContexts = new ArrayList<StreamContextManager.StreamContext>();
+        
+        for (String dataDir : dataDirectories)
+        {
+            File directory = new File(dataDir);
+            Collections.addAll(filesList, directory.listFiles());            
+        
+
+            for (File tableDir : directory.listFiles())
+            {
+                String tableName = tableDir.getName();
+
+                for (File file : tableDir.listFiles())
+                {
+                    streamContexts.add(new StreamContextManager.StreamContext(file.getAbsolutePath(), file.length(), tableName));
+                    if (logger_.isDebugEnabled())
+                      logger_.debug("Stream context metadata " + streamContexts);
+                }
+            }
+        }
+        
+        if ( streamContexts.size() > 0 )
+        {
+            EndPoint target = new EndPoint(host, DatabaseDescriptor.getStoragePort());
+            /* Set up the stream manager with the files that need to streamed */
+            StreamManager.instance(target).addFilesToStream((StreamContextManager.StreamContext[]) streamContexts.toArray());
+            /* Send the bootstrap initiate message */
+            BootstrapInitiateMessage biMessage = new BootstrapInitiateMessage((StreamContextManager.StreamContext[]) streamContexts.toArray());
+            Message message = BootstrapInitiateMessage.makeBootstrapInitiateMessage(biMessage);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Sending a bootstrap initiate message to " + target + " ...");
+            MessagingService.getMessagingInstance().sendOneWay(message, target);                
+            if (logger_.isDebugEnabled())
+              logger_.debug("Waiting for transfer to " + target + " to complete");
+            StreamManager.instance(target).waitForStreamCompletion();
+            if (logger_.isDebugEnabled())
+              logger_.debug("Done with transfer to " + target);  
+        }
+    }
+
+    /**
+     * Takes the snapshot for a given table.
+     * 
+     * @param tableName the name of the table.
+     * @param tag   the tag given to the snapshot (null is permissible)
+     */
+    public void takeSnapshot(String tableName, String tag) throws IOException
+    {
+    	if (DatabaseDescriptor.getTable(tableName) == null)
+        {
+            throw new IOException("Table " + tableName + "does not exist");
+    	}
+        Table tableInstance = Table.open(tableName);
+        tableInstance.snapshot(tag);
+    }
+    
+    /**
+     * Takes a snapshot for every table.
+     * 
+     * @param tag the tag given to the snapshot (null is permissible)
+     */
+    public void takeAllSnapshot(String tag) throws IOException
+    {
+    	for (String tableName: DatabaseDescriptor.getTables())
+        {
+            Table tableInstance = Table.open(tableName);
+            tableInstance.snapshot(tag);
+    	}
+    }
+
+    /**
+     * Remove all the existing snapshots.
+     */
+    public void clearSnapshot() throws IOException
+    {
+    	for (String tableName: DatabaseDescriptor.getTables())
+        {
+            Table tableInstance = Table.open(tableName);
+            tableInstance.clearSnapshot();
+    	}
+        if (logger_.isDebugEnabled())
+            logger_.debug("Cleared out all snapshot directories");
+    }
+
+    /* End of MBean interface methods */
+    
+    /**
+     * This method returns the predecessor of the endpoint ep on the identifier
+     * space.
+     */
+    EndPoint getPredecessor(EndPoint ep)
+    {
+        Token token = tokenMetadata_.getToken(ep);
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        int index = Collections.binarySearch(tokens, token);
+        return (index == 0) ? tokenToEndPointMap.get(tokens
+                .get(tokens.size() - 1)) : tokenToEndPointMap.get(tokens
+                .get(--index));
+    }
+
+    /*
+     * This method returns the successor of the endpoint ep on the identifier
+     * space.
+     */
+    public EndPoint getSuccessor(EndPoint ep)
+    {
+        Token token = tokenMetadata_.getToken(ep);
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
+        Collections.sort(tokens);
+        int index = Collections.binarySearch(tokens, token);
+        return (index == (tokens.size() - 1)) ? tokenToEndPointMap
+                .get(tokens.get(0))
+                : tokenToEndPointMap.get(tokens.get(++index));
+    }
+
+    /**
+     * Get the primary range for the specified endpoint.
+     * @param ep endpoint we are interested in.
+     * @return range for the specified endpoint.
+     */
+    public Range getPrimaryRangeForEndPoint(EndPoint ep)
+    {
+        Token right = tokenMetadata_.getToken(ep);
+        EndPoint predecessor = getPredecessor(ep);
+        Token left = tokenMetadata_.getToken(predecessor);
+        return new Range(left, right);
+    }
+    
+    /**
+     * Get all ranges an endpoint is responsible for.
+     * @param ep endpoint we are interested in.
+     * @return ranges for the specified endpoint.
+     */
+    List<Range> getRangesForEndPoint(EndPoint ep)
+    {
+        List<Range> ranges = new ArrayList<Range>();
+        ranges.add( getPrimaryRangeForEndPoint(ep) );
+        
+        EndPoint predecessor = ep;
+        int count = DatabaseDescriptor.getReplicationFactor() - 1;
+        for ( int i = 0; i < count; ++i )
+        {
+            predecessor = getPredecessor(predecessor);
+            ranges.add( getPrimaryRangeForEndPoint(predecessor) );
+        }
+        
+        return ranges;
+    }
+    
+    /**
+     * Get all ranges that span the ring as per
+     * current snapshot of the token distribution.
+     * @return all ranges in sorted order.
+     */
+    public Range[] getAllRanges()
+    {
+        return getAllRanges(tokenMetadata_.cloneTokenEndPointMap().keySet());
+    }
+    
+    /**
+     * Get all ranges that span the ring given a set
+     * of tokens. All ranges are in sorted order of 
+     * ranges.
+     * @return ranges in sorted order
+    */
+    public Range[] getAllRanges(Set<Token> tokens)
+    {
+        List<Range> ranges = new ArrayList<Range>();
+        List<Token> allTokens = new ArrayList<Token>(tokens);
+        Collections.sort(allTokens);
+        int size = allTokens.size();
+        for ( int i = 1; i < size; ++i )
+        {
+            Range range = new Range( allTokens.get(i - 1), allTokens.get(i) );
+            ranges.add(range);
+        }
+        Range range = new Range( allTokens.get(size - 1), allTokens.get(0) );
+        ranges.add(range);
+        return ranges.toArray( new Range[0] );
+    }
+
+    /**
+     * This method returns the endpoint that is responsible for storing the
+     * specified key.
+     *
+     * @param key - key for which we need to find the endpoint
+     * @return value - the endpoint responsible for this key
+     */
+    public EndPoint getPrimary(String key)
+    {
+        EndPoint endpoint = StorageService.tcpAddr_;
+        Token token = partitioner_.getInitialToken(key);
+        Map<Token, EndPoint> tokenToEndPointMap = tokenMetadata_.cloneTokenEndPointMap();
+        List tokens = new ArrayList<Token>(tokenToEndPointMap.keySet());
+        if (tokens.size() > 0)
+        {
+            Collections.sort(tokens);
+            int index = Collections.binarySearch(tokens, token);
+            if (index >= 0)
+            {
+                /*
+                 * retrieve the endpoint based on the token at this index in the
+                 * tokens list
+                 */
+                endpoint = tokenToEndPointMap.get(tokens.get(index));
+            }
+            else
+            {
+                index = (index + 1) * (-1);
+                if (index < tokens.size())
+                    endpoint = tokenToEndPointMap.get(tokens.get(index));
+                else
+                    endpoint = tokenToEndPointMap.get(tokens.get(0));
+            }
+        }
+        return endpoint;
+    }
+
+    /**
+     * This method determines whether the local endpoint is the
+     * primary for the given key.
+     * @param key
+     * @return true if the local endpoint is the primary replica.
+    */
+    public boolean isPrimary(String key)
+    {
+        EndPoint endpoint = getPrimary(key);
+        return StorageService.tcpAddr_.equals(endpoint);
+    }
+
+    /**
+     * This method returns the N endpoints that are responsible for storing the
+     * specified key i.e for replication.
+     *
+     * @param key - key for which we need to find the endpoint return value -
+     * the endpoint responsible for this key
+     */
+    public EndPoint[] getNStorageEndPoint(String key)
+    {
+        return nodePicker_.getStorageEndPoints(partitioner_.getInitialToken(key));
+    }
+    
+    private Map<String, EndPoint[]> getNStorageEndPoints(String[] keys)
+    {
+    	return nodePicker_.getStorageEndPoints(keys);
+    }
+    
+    
+    /**
+     * This method attempts to return N endpoints that are responsible for storing the
+     * specified key i.e for replication.
+     *
+     * @param key - key for which we need to find the endpoint return value -
+     * the endpoint responsible for this key
+     */
+    public List<EndPoint> getNLiveStorageEndPoint(String key)
+    {
+    	List<EndPoint> liveEps = new ArrayList<EndPoint>();
+    	EndPoint[] endpoints = getNStorageEndPoint(key);
+    	
+    	for ( EndPoint endpoint : endpoints )
+    	{
+    		if ( FailureDetector.instance().isAlive(endpoint) )
+    			liveEps.add(endpoint);
+    	}
+    	
+    	return liveEps;
+    }
+
+    /**
+     * This method returns the N endpoints that are responsible for storing the
+     * specified key i.e for replication.
+     *
+     * @param key - key for which we need to find the endpoint return value -
+     * the endpoint responsible for this key
+     */
+    public Map<EndPoint, EndPoint> getNStorageEndPointMap(String key)
+    {
+        return nodePicker_.getHintedStorageEndPoints(partitioner_.getInitialToken(key));
+    }
+
+    /**
+     * This method returns the N endpoints that are responsible for storing the
+     * specified token i.e for replication.
+     *
+     * @param token - position on the ring
+     */
+    public EndPoint[] getNStorageEndPoint(Token token)
+    {
+        return nodePicker_.getStorageEndPoints(token);
+    }
+    
+    /**
+     * This method returns the N endpoints that are responsible for storing the
+     * specified token i.e for replication and are based on the token to endpoint 
+     * mapping that is passed in.
+     *
+     * @param token - position on the ring
+     * @param tokens - w/o the following tokens in the token list
+     */
+    protected EndPoint[] getNStorageEndPoint(Token token, Map<Token, EndPoint> tokenToEndPointMap)
+    {
+        return nodePicker_.getStorageEndPoints(token, tokenToEndPointMap);
+    }
+
+    /**
+     * This function finds the most suitable endpoint given a key.
+     * It checks for locality and alive test.
+     */
+	public EndPoint findSuitableEndPoint(String key) throws IOException
+	{
+		EndPoint[] endpoints = getNStorageEndPoint(key);
+		for(EndPoint endPoint: endpoints)
+		{
+			if(endPoint.equals(StorageService.getLocalStorageEndPoint()))
+			{
+				return endPoint;
+			}
+		}
+		int j = 0;
+		for ( ; j < endpoints.length; ++j )
+		{
+			if ( StorageService.instance().isInSameDataCenter(endpoints[j]) && FailureDetector.instance().isAlive(endpoints[j]) )
+			{
+				return endpoints[j];
+			}
+		}
+		// We have tried to be really nice but looks like there are no servers 
+		// in the local data center that are alive and can service this request so 
+		// just send it to the first alive guy and see if we get anything.
+		j = 0;
+		for ( ; j < endpoints.length; ++j )
+		{
+			if ( FailureDetector.instance().isAlive(endpoints[j]) )
+			{
+				if (logger_.isDebugEnabled())
+				  logger_.debug("EndPoint " + endpoints[j] + " is alive so get data from it.");
+				return endpoints[j];
+			}
+		}
+		return null;
+	}
+	
+	public Map<String, EndPoint> findSuitableEndPoints(String[] keys) throws IOException
+	{
+		Map<String, EndPoint> suitableEndPoints = new HashMap<String, EndPoint>();
+		Map<String, EndPoint[]> results = getNStorageEndPoints(keys);
+		for ( String key : keys )
+		{
+			EndPoint[] endpoints = results.get(key);
+			/* indicates if we have to move on to the next key */
+			boolean moveOn = false;
+			for(EndPoint endPoint: endpoints)
+			{
+				if(endPoint.equals(StorageService.getLocalStorageEndPoint()))
+				{
+					suitableEndPoints.put(key, endPoint);
+					moveOn = true;
+					break;
+				}
+			}
+			
+			if ( moveOn )
+				continue;
+				
+			int j = 0;
+			for ( ; j < endpoints.length; ++j )
+			{
+				if ( StorageService.instance().isInSameDataCenter(endpoints[j]) && FailureDetector.instance().isAlive(endpoints[j]) )
+				{
+					if (logger_.isDebugEnabled())
+					  logger_.debug("EndPoint " + endpoints[j] + " is in the same data center as local storage endpoint.");
+					suitableEndPoints.put(key, endpoints[j]);
+					moveOn = true;
+					break;
+				}
+			}
+			
+			if ( moveOn )
+				continue;
+			
+			// We have tried to be really nice but looks like there are no servers 
+			// in the local data center that are alive and can service this request so 
+			// just send it to the first alive guy and see if we get anything.
+			j = 0;
+			for ( ; j < endpoints.length; ++j )
+			{
+				if ( FailureDetector.instance().isAlive(endpoints[j]) )
+				{
+					if (logger_.isDebugEnabled())
+					  logger_.debug("EndPoint " + endpoints[j] + " is alive so get data from it.");
+					suitableEndPoints.put(key, endpoints[j]);
+					break;
+				}
+			}
+		}
+		return suitableEndPoints;
+	}
+}
diff --git a/src/java/org/apache/cassandra/service/StorageServiceMBean.java b/src/java/org/apache/cassandra/service/StorageServiceMBean.java
index d160bab892..2fa165652c 100644
--- a/src/java/org/apache/cassandra/service/StorageServiceMBean.java
+++ b/src/java/org/apache/cassandra/service/StorageServiceMBean.java
@@ -1,89 +1,89 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.io.IOException;
-import java.util.List;
-import java.util.Map;
-import org.apache.cassandra.dht.Range;
-import org.apache.cassandra.net.EndPoint;
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public interface StorageServiceMBean
-{    
-    public String getLiveNodes();
-    public String getUnreachableNodes();
-    public String getToken();
-    public Map<Range, List<EndPoint>> getRangeToEndPointMap();
-    public String getLoadInfo();
-    public int getCurrentGenerationNumber();
-    public void forceTableCompaction() throws IOException;
-    
-    /**
-     * This method will cause the local node initiate
-     * the bootstrap process for all the nodes specified
-     * in the string parameter passed in. This local node
-     * will calculate who gives what ranges to the nodes
-     * and then instructs the nodes to do so.
-     * 
-     * @param nodes colon delimited list of endpoints that need
-     *              to be bootstrapped
-    */
-    public void loadAll(String nodes);
-    
-    /**
-     * 
-     */
-    public void forceTableCleanup() throws IOException;
-
-    /**
-     * Stream the files in the bootstrap directory over to the
-     * node being bootstrapped. This is used in case of normal
-     * bootstrap failure. Use a tool to re-calculate the cardinality
-     * at a later point at the destination.
-     * @param directories colon separated list of directories from where 
-     *                files need to be picked up.
-     * @param target endpoint receiving data.
-    */
-    public void forceHandoff(List<String> directories, String target) throws IOException;
-
-    /**
-     * Takes the snapshot for a given table.
-     * 
-     * @param tableName the name of the table.
-     * @param tag       the tag given to the snapshot (null is permissible)
-     */
-    public void takeSnapshot(String tableName, String tag) throws IOException;
-
-    /**
-     * Takes a snapshot for every table.
-     * 
-     * @param tag the tag given to the snapshot (null is permissible)
-     */
-    public void takeAllSnapshot(String tag) throws IOException;
-
-    /**
-     * Remove all the existing snapshots.
-     */
-    public void clearSnapshot() throws IOException;
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.net.EndPoint;
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public interface StorageServiceMBean
+{    
+    public String getLiveNodes();
+    public String getUnreachableNodes();
+    public String getToken();
+    public Map<Range, List<EndPoint>> getRangeToEndPointMap();
+    public String getLoadInfo();
+    public int getCurrentGenerationNumber();
+    public void forceTableCompaction() throws IOException;
+    
+    /**
+     * This method will cause the local node initiate
+     * the bootstrap process for all the nodes specified
+     * in the string parameter passed in. This local node
+     * will calculate who gives what ranges to the nodes
+     * and then instructs the nodes to do so.
+     * 
+     * @param nodes colon delimited list of endpoints that need
+     *              to be bootstrapped
+    */
+    public void loadAll(String nodes);
+    
+    /**
+     * 
+     */
+    public void forceTableCleanup() throws IOException;
+
+    /**
+     * Stream the files in the bootstrap directory over to the
+     * node being bootstrapped. This is used in case of normal
+     * bootstrap failure. Use a tool to re-calculate the cardinality
+     * at a later point at the destination.
+     * @param directories colon separated list of directories from where 
+     *                files need to be picked up.
+     * @param target endpoint receiving data.
+    */
+    public void forceHandoff(List<String> directories, String target) throws IOException;
+
+    /**
+     * Takes the snapshot for a given table.
+     * 
+     * @param tableName the name of the table.
+     * @param tag       the tag given to the snapshot (null is permissible)
+     */
+    public void takeSnapshot(String tableName, String tag) throws IOException;
+
+    /**
+     * Takes a snapshot for every table.
+     * 
+     * @param tag the tag given to the snapshot (null is permissible)
+     */
+    public void takeAllSnapshot(String tag) throws IOException;
+
+    /**
+     * Remove all the existing snapshots.
+     */
+    public void clearSnapshot() throws IOException;
+}
diff --git a/src/java/org/apache/cassandra/service/StreamManager.java b/src/java/org/apache/cassandra/service/StreamManager.java
index 2575869dd2..fddbb94ca2 100644
--- a/src/java/org/apache/cassandra/service/StreamManager.java
+++ b/src/java/org/apache/cassandra/service/StreamManager.java
@@ -1,164 +1,164 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.io.File;
-import java.io.IOException;
-import java.util.*;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.net.io.StreamContextManager;
-import org.apache.cassandra.service.StorageService.BootstrapInitiateDoneVerbHandler;
-import org.apache.cassandra.utils.FileUtils;
-import org.apache.cassandra.utils.LogUtil;
-import org.apache.log4j.Logger;
-
-/*
- * This class manages the streaming of multiple files 
- * one after the other. 
-*/
-public final class StreamManager
-{   
-    private static Logger logger_ = Logger.getLogger( StreamManager.class );
-    
-    public static class BootstrapTerminateVerbHandler implements IVerbHandler
-    {
-        private static Logger logger_ = Logger.getLogger( BootstrapInitiateDoneVerbHandler.class );
-
-        public void doVerb(Message message)
-        {
-            byte[] body = message.getMessageBody();
-            DataInputBuffer bufIn = new DataInputBuffer();
-            bufIn.reset(body, body.length);
-
-            try
-            {
-                StreamContextManager.StreamStatusMessage streamStatusMessage = StreamContextManager.StreamStatusMessage.serializer().deserialize(bufIn);
-                StreamContextManager.StreamStatus streamStatus = streamStatusMessage.getStreamStatus();
-                                               
-                switch( streamStatus.getAction() )
-                {
-                    case DELETE:                              
-                        StreamManager.instance(message.getFrom()).finish(streamStatus.getFile());
-                        break;
-
-                    case STREAM:
-                        if (logger_.isDebugEnabled())
-                          logger_.debug("Need to re-stream file " + streamStatus.getFile());
-                        StreamManager.instance(message.getFrom()).repeat();
-                        break;
-
-                    default:
-                        break;
-                }
-            }
-            catch ( IOException ex )
-            {
-                logger_.info(LogUtil.throwableToString(ex));
-            }
-        }
-    }
-    
-    private static Map<EndPoint, StreamManager> streamManagers_ = new HashMap<EndPoint, StreamManager>();
-    
-    public static StreamManager instance(EndPoint to)
-    {
-        StreamManager streamManager = streamManagers_.get(to);
-        if ( streamManager == null )
-        {
-            streamManager = new StreamManager(to);
-            streamManagers_.put(to, streamManager);
-        }
-        return streamManager;
-    }
-    
-    private List<File> filesToStream_ = new ArrayList<File>();
-    private EndPoint to_;
-    private long totalBytesToStream_ = 0L;
-    
-    private StreamManager(EndPoint to)
-    {
-        to_ = to;
-    }
-    
-    public void addFilesToStream(StreamContextManager.StreamContext[] streamContexts)
-    {
-        for ( StreamContextManager.StreamContext streamContext : streamContexts )
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug("Adding file " + streamContext.getTargetFile() + " to be streamed.");
-            filesToStream_.add( new File( streamContext.getTargetFile() ) );
-            totalBytesToStream_ += streamContext.getExpectedBytes();
-        }
-    }
-    
-    void start()
-    {
-        if ( filesToStream_.size() > 0 )
-        {
-            File file = filesToStream_.get(0);
-            if (logger_.isDebugEnabled())
-              logger_.debug("Streaming file " + file + " ...");
-            MessagingService.getMessagingInstance().stream(file.getAbsolutePath(), 0L, file.length(), StorageService.getLocalStorageEndPoint(), to_);
-        }
-    }
-    
-    void repeat()
-    {
-        if ( filesToStream_.size() > 0 )
-            start();
-    }
-    
-    void finish(String file) throws IOException
-    {
-        File f = new File(file);
-        if (logger_.isDebugEnabled())
-          logger_.debug("Deleting file " + file + " after streaming " + f.length() + "/" + totalBytesToStream_ + " bytes.");
-        FileUtils.delete(file);
-        filesToStream_.remove(0);
-        if ( filesToStream_.size() > 0 )
-            start();
-        else
-        {
-            synchronized(this)
-            {
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Signalling that streaming is done for " + to_);
-                notifyAll();
-            }
-        }
-    }
-    
-    public synchronized void waitForStreamCompletion()
-    {
-        try
-        {
-            wait();
-        }
-        catch(InterruptedException ex)
-        {
-            logger_.warn(LogUtil.throwableToString(ex));
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.*;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.io.StreamContextManager;
+import org.apache.cassandra.service.StorageService.BootstrapInitiateDoneVerbHandler;
+import org.apache.cassandra.utils.FileUtils;
+import org.apache.cassandra.utils.LogUtil;
+import org.apache.log4j.Logger;
+
+/*
+ * This class manages the streaming of multiple files 
+ * one after the other. 
+*/
+public final class StreamManager
+{   
+    private static Logger logger_ = Logger.getLogger( StreamManager.class );
+    
+    public static class BootstrapTerminateVerbHandler implements IVerbHandler
+    {
+        private static Logger logger_ = Logger.getLogger( BootstrapInitiateDoneVerbHandler.class );
+
+        public void doVerb(Message message)
+        {
+            byte[] body = message.getMessageBody();
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length);
+
+            try
+            {
+                StreamContextManager.StreamStatusMessage streamStatusMessage = StreamContextManager.StreamStatusMessage.serializer().deserialize(bufIn);
+                StreamContextManager.StreamStatus streamStatus = streamStatusMessage.getStreamStatus();
+                                               
+                switch( streamStatus.getAction() )
+                {
+                    case DELETE:                              
+                        StreamManager.instance(message.getFrom()).finish(streamStatus.getFile());
+                        break;
+
+                    case STREAM:
+                        if (logger_.isDebugEnabled())
+                          logger_.debug("Need to re-stream file " + streamStatus.getFile());
+                        StreamManager.instance(message.getFrom()).repeat();
+                        break;
+
+                    default:
+                        break;
+                }
+            }
+            catch ( IOException ex )
+            {
+                logger_.info(LogUtil.throwableToString(ex));
+            }
+        }
+    }
+    
+    private static Map<EndPoint, StreamManager> streamManagers_ = new HashMap<EndPoint, StreamManager>();
+    
+    public static StreamManager instance(EndPoint to)
+    {
+        StreamManager streamManager = streamManagers_.get(to);
+        if ( streamManager == null )
+        {
+            streamManager = new StreamManager(to);
+            streamManagers_.put(to, streamManager);
+        }
+        return streamManager;
+    }
+    
+    private List<File> filesToStream_ = new ArrayList<File>();
+    private EndPoint to_;
+    private long totalBytesToStream_ = 0L;
+    
+    private StreamManager(EndPoint to)
+    {
+        to_ = to;
+    }
+    
+    public void addFilesToStream(StreamContextManager.StreamContext[] streamContexts)
+    {
+        for ( StreamContextManager.StreamContext streamContext : streamContexts )
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug("Adding file " + streamContext.getTargetFile() + " to be streamed.");
+            filesToStream_.add( new File( streamContext.getTargetFile() ) );
+            totalBytesToStream_ += streamContext.getExpectedBytes();
+        }
+    }
+    
+    void start()
+    {
+        if ( filesToStream_.size() > 0 )
+        {
+            File file = filesToStream_.get(0);
+            if (logger_.isDebugEnabled())
+              logger_.debug("Streaming file " + file + " ...");
+            MessagingService.getMessagingInstance().stream(file.getAbsolutePath(), 0L, file.length(), StorageService.getLocalStorageEndPoint(), to_);
+        }
+    }
+    
+    void repeat()
+    {
+        if ( filesToStream_.size() > 0 )
+            start();
+    }
+    
+    void finish(String file) throws IOException
+    {
+        File f = new File(file);
+        if (logger_.isDebugEnabled())
+          logger_.debug("Deleting file " + file + " after streaming " + f.length() + "/" + totalBytesToStream_ + " bytes.");
+        FileUtils.delete(file);
+        filesToStream_.remove(0);
+        if ( filesToStream_.size() > 0 )
+            start();
+        else
+        {
+            synchronized(this)
+            {
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Signalling that streaming is done for " + to_);
+                notifyAll();
+            }
+        }
+    }
+    
+    public synchronized void waitForStreamCompletion()
+    {
+        try
+        {
+            wait();
+        }
+        catch(InterruptedException ex)
+        {
+            logger_.warn(LogUtil.throwableToString(ex));
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/service/TokenUpdateVerbHandler.java b/src/java/org/apache/cassandra/service/TokenUpdateVerbHandler.java
index 574144a4d8..ecc0617762 100644
--- a/src/java/org/apache/cassandra/service/TokenUpdateVerbHandler.java
+++ b/src/java/org/apache/cassandra/service/TokenUpdateVerbHandler.java
@@ -1,54 +1,54 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.io.IOException;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.utils.LogUtil;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class TokenUpdateVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger(TokenUpdateVerbHandler.class);
-
-    public void doVerb(Message message)
-    {
-    	byte[] body = message.getMessageBody();
-        Token token = StorageService.getPartitioner().getTokenFactory().fromByteArray(body);
-        try
-        {
-        	logger_.info("Updating the token to [" + token + "]");
-        	StorageService.instance().updateToken(token);
-        }
-    	catch( IOException ex )
-    	{
-    		if (logger_.isDebugEnabled())
-    		  logger_.debug(LogUtil.throwableToString(ex));
-    	}
-    }
-
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.io.IOException;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.utils.LogUtil;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TokenUpdateVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(TokenUpdateVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+    	byte[] body = message.getMessageBody();
+        Token token = StorageService.getPartitioner().getTokenFactory().fromByteArray(body);
+        try
+        {
+        	logger_.info("Updating the token to [" + token + "]");
+        	StorageService.instance().updateToken(token);
+        }
+    	catch( IOException ex )
+    	{
+    		if (logger_.isDebugEnabled())
+    		  logger_.debug(LogUtil.throwableToString(ex));
+    	}
+    }
+
+}
diff --git a/src/java/org/apache/cassandra/service/WriteResponseResolver.java b/src/java/org/apache/cassandra/service/WriteResponseResolver.java
index 08b83b02f2..5f5e34faff 100644
--- a/src/java/org/apache/cassandra/service/WriteResponseResolver.java
+++ b/src/java/org/apache/cassandra/service/WriteResponseResolver.java
@@ -1,77 +1,77 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service;
-
-import java.util.List;
-import java.io.DataInputStream;
-import java.io.ByteArrayInputStream;
-import java.io.IOException;
-
-import org.apache.cassandra.db.WriteResponse;
-import org.apache.cassandra.net.Message;
-import org.apache.log4j.Logger;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class WriteResponseResolver implements IResponseResolver<Boolean> {
-
-	private static Logger logger_ = Logger.getLogger(WriteResponseResolver.class);
-
-	/*
-	 * The resolve function for the Write looks at all the responses if all the
-	 * responses returned are false then we have a problem since that means the
-	 * key was not written to any of the servers we want to notify the client of
-	 * this so in that case we should return a false saying that the write
-	 * failed.
-	 * 
-	 */
-	public Boolean resolve(List<Message> responses) throws DigestMismatchException 
-	{
-		// TODO: We need to log error responses here for example
-		// if a write fails for a key log that the key could not be replicated
-		boolean returnValue = false;
-		for (Message response : responses) {
-            WriteResponse writeResponseMessage = null;
-            try
-            {
-                writeResponseMessage = WriteResponse.serializer().deserialize(new DataInputStream(new ByteArrayInputStream(response.getMessageBody())));
-            }
-            catch (IOException e)
-            {
-                throw new RuntimeException(e);
-            }
-            boolean result = writeResponseMessage.isSuccess();
-            if (!result) {
-				if (logger_.isDebugEnabled())
-                    logger_.debug("Write at " + response.getFrom()
-						+ " may have failed for the key " + writeResponseMessage.key());
-			}
-			returnValue |= result;
-		}
-		return returnValue;
-	}
-
-	public boolean isDataPresent(List<Message> responses)
-	{
-		return true;
-	}
-	
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service;
+
+import java.util.List;
+import java.io.DataInputStream;
+import java.io.ByteArrayInputStream;
+import java.io.IOException;
+
+import org.apache.cassandra.db.WriteResponse;
+import org.apache.cassandra.net.Message;
+import org.apache.log4j.Logger;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class WriteResponseResolver implements IResponseResolver<Boolean> {
+
+	private static Logger logger_ = Logger.getLogger(WriteResponseResolver.class);
+
+	/*
+	 * The resolve function for the Write looks at all the responses if all the
+	 * responses returned are false then we have a problem since that means the
+	 * key was not written to any of the servers we want to notify the client of
+	 * this so in that case we should return a false saying that the write
+	 * failed.
+	 * 
+	 */
+	public Boolean resolve(List<Message> responses) throws DigestMismatchException 
+	{
+		// TODO: We need to log error responses here for example
+		// if a write fails for a key log that the key could not be replicated
+		boolean returnValue = false;
+		for (Message response : responses) {
+            WriteResponse writeResponseMessage = null;
+            try
+            {
+                writeResponseMessage = WriteResponse.serializer().deserialize(new DataInputStream(new ByteArrayInputStream(response.getMessageBody())));
+            }
+            catch (IOException e)
+            {
+                throw new RuntimeException(e);
+            }
+            boolean result = writeResponseMessage.isSuccess();
+            if (!result) {
+				if (logger_.isDebugEnabled())
+                    logger_.debug("Write at " + response.getFrom()
+						+ " may have failed for the key " + writeResponseMessage.key());
+			}
+			returnValue |= result;
+		}
+		return returnValue;
+	}
+
+	public boolean isDataPresent(List<Message> responses)
+	{
+		return true;
+	}
+	
+}
diff --git a/src/java/org/apache/cassandra/tools/KeyChecker.java b/src/java/org/apache/cassandra/tools/KeyChecker.java
index fff2f187cd..d844561f74 100644
--- a/src/java/org/apache/cassandra/tools/KeyChecker.java
+++ b/src/java/org/apache/cassandra/tools/KeyChecker.java
@@ -1,99 +1,99 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.tools;
-
-import java.io.BufferedReader;
-import java.io.FileInputStream;
-import java.io.InputStreamReader;
-import java.io.RandomAccessFile;
-
-import org.apache.cassandra.db.Row;
-import org.apache.cassandra.db.Table;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.utils.LogUtil;
-
-
-public class KeyChecker
-{
-    private static final int bufSize_ = 128*1024*1024;
-    /*
-     * This function checks if the local storage endpoint 
-     * is responsible for storing this key .
-     */
-    private static boolean checkIfProcessKey(String key)
-    {
-        EndPoint[] endPoints = StorageService.instance().getNStorageEndPoint(key);
-        EndPoint localEndPoint = StorageService.getLocalStorageEndPoint();
-        for(EndPoint endPoint : endPoints)
-        {
-            if(endPoint.equals(localEndPoint))
-                return true;
-        }
-        return false;
-    }
-    
-    public static void main(String[] args) throws Throwable
-    {
-        if ( args.length != 1 )
-        {
-            System.out.println("Usage : java com.facebook.infrastructure.tools.KeyChecker <file containing all keys>");
-            System.exit(1);
-        }
-        
-        LogUtil.init();
-        StorageService s = StorageService.instance();
-        s.start();
-        
-        /* Sleep for proper discovery */
-        Thread.sleep(240000);
-        /* Create the file for the missing keys */
-        RandomAccessFile raf = new RandomAccessFile( "Missing-" + FBUtilities.getHostAddress() + ".dat", "rw");
-        
-        /* Start reading the file that contains the keys */
-        BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(args[0]) ), KeyChecker.bufSize_ );
-        String key = null;
-        boolean bStarted = false;
-        
-        while ( ( key = bufReader.readLine() ) != null )
-        {            
-            if ( !bStarted )
-            {
-                bStarted = true;
-                System.out.println("Started the processing of the file ...");
-            }
-            
-            key = key.trim();
-            if ( StorageService.instance().isPrimary(key) )
-            {
-                System.out.println("Processing key " + key);
-                Row row = Table.open("Mailbox").getRow(key, "MailboxMailList0");
-                if ( row.isEmpty() )
-                {
-                    System.out.println("MISSING KEY : " + key);
-                    raf.write(key.getBytes());
-                    raf.write(System.getProperty("line.separator").getBytes());
-                }
-            }
-        }
-        System.out.println("DONE checking keys ...");
-        raf.close();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.BufferedReader;
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+import java.io.RandomAccessFile;
+
+import org.apache.cassandra.db.Row;
+import org.apache.cassandra.db.Table;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.LogUtil;
+
+
+public class KeyChecker
+{
+    private static final int bufSize_ = 128*1024*1024;
+    /*
+     * This function checks if the local storage endpoint 
+     * is responsible for storing this key .
+     */
+    private static boolean checkIfProcessKey(String key)
+    {
+        EndPoint[] endPoints = StorageService.instance().getNStorageEndPoint(key);
+        EndPoint localEndPoint = StorageService.getLocalStorageEndPoint();
+        for(EndPoint endPoint : endPoints)
+        {
+            if(endPoint.equals(localEndPoint))
+                return true;
+        }
+        return false;
+    }
+    
+    public static void main(String[] args) throws Throwable
+    {
+        if ( args.length != 1 )
+        {
+            System.out.println("Usage : java com.facebook.infrastructure.tools.KeyChecker <file containing all keys>");
+            System.exit(1);
+        }
+        
+        LogUtil.init();
+        StorageService s = StorageService.instance();
+        s.start();
+        
+        /* Sleep for proper discovery */
+        Thread.sleep(240000);
+        /* Create the file for the missing keys */
+        RandomAccessFile raf = new RandomAccessFile( "Missing-" + FBUtilities.getHostAddress() + ".dat", "rw");
+        
+        /* Start reading the file that contains the keys */
+        BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(args[0]) ), KeyChecker.bufSize_ );
+        String key = null;
+        boolean bStarted = false;
+        
+        while ( ( key = bufReader.readLine() ) != null )
+        {            
+            if ( !bStarted )
+            {
+                bStarted = true;
+                System.out.println("Started the processing of the file ...");
+            }
+            
+            key = key.trim();
+            if ( StorageService.instance().isPrimary(key) )
+            {
+                System.out.println("Processing key " + key);
+                Row row = Table.open("Mailbox").getRow(key, "MailboxMailList0");
+                if ( row.isEmpty() )
+                {
+                    System.out.println("MISSING KEY : " + key);
+                    raf.write(key.getBytes());
+                    raf.write(System.getProperty("line.separator").getBytes());
+                }
+            }
+        }
+        System.out.println("DONE checking keys ...");
+        raf.close();
+    }
+}
diff --git a/src/java/org/apache/cassandra/tools/MembershipCleaner.java b/src/java/org/apache/cassandra/tools/MembershipCleaner.java
index bdf7e5ff1f..28775f85bc 100644
--- a/src/java/org/apache/cassandra/tools/MembershipCleaner.java
+++ b/src/java/org/apache/cassandra/tools/MembershipCleaner.java
@@ -1,122 +1,122 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.tools;
-
-import java.io.BufferedReader;
-import java.io.ByteArrayOutputStream;
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.FileInputStream;
-import java.io.IOException;
-import java.io.InputStreamReader;
-import java.io.Serializable;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.cassandra.io.ICompactSerializer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.FBUtilities;
-
-public class MembershipCleaner
-{
-    private static final int port_ = 7000;
-    private static final long waitTime_ = 10000;
-    
-    public static void main(String[] args) throws Throwable
-    {
-        if ( args.length != 3 )
-        {
-            System.out.println("Usage : java com.facebook.infrastructure.tools.MembershipCleaner " +
-                    "<ip:port to send the message> " +
-                    "<node which needs to be removed> " +
-                    "<file containing all nodes in the cluster>");
-            System.exit(1);
-        }
-        
-        String ipPort = args[0];
-        String node = args[1];
-        String file = args[2];
-        
-        String[] ipPortPair = ipPort.split(":");
-        EndPoint target = new EndPoint(ipPortPair[0], Integer.valueOf(ipPortPair[1]));
-        MembershipCleanerMessage mcMessage = new MembershipCleanerMessage(node);
-        
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream(bos);
-        MembershipCleanerMessage.serializer().serialize(mcMessage, dos);
-        /* Construct the token update message to be sent */
-        Message mbrshipCleanerMessage = new Message( new EndPoint(FBUtilities.getHostAddress(), port_), "", StorageService.mbrshipCleanerVerbHandler_, bos.toByteArray() );
-        
-        BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ) );
-        String line = null;
-       
-        while ( ( line = bufReader.readLine() ) != null )
-        {            
-            mbrshipCleanerMessage.addHeader(line, line.getBytes());
-        }
-        
-        System.out.println("Sending a membership clean message to " + target);
-        MessagingService.getMessagingInstance().sendOneWay(mbrshipCleanerMessage, target);
-        Thread.sleep(MembershipCleaner.waitTime_);
-        System.out.println("Done sending the update message");
-    }
-    
-    public static class MembershipCleanerMessage implements Serializable
-    {
-        private static ICompactSerializer<MembershipCleanerMessage> serializer_;
-        private static AtomicInteger idGen_ = new AtomicInteger(0);
-        
-        static
-        {
-            serializer_ = new MembershipCleanerMessageSerializer();            
-        }
-        
-        static ICompactSerializer<MembershipCleanerMessage> serializer()
-        {
-            return serializer_;
-        }
-
-        private String target_;
-        
-        MembershipCleanerMessage(String target)
-        {
-            target_ = target;        
-        }
-        
-        String getTarget()
-        {
-            return target_;
-        }
-    }
-    
-    public static class MembershipCleanerMessageSerializer implements ICompactSerializer<MembershipCleanerMessage>
-    {
-        public void serialize(MembershipCleanerMessage mcMessage, DataOutputStream dos) throws IOException
-        {            
-            dos.writeUTF(mcMessage.getTarget() );                      
-        }
-        
-        public MembershipCleanerMessage deserialize(DataInputStream dis) throws IOException
-        {            
-            return new MembershipCleanerMessage(dis.readUTF());
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.BufferedReader;
+import java.io.ByteArrayOutputStream;
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.FileInputStream;
+import java.io.IOException;
+import java.io.InputStreamReader;
+import java.io.Serializable;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.apache.cassandra.io.ICompactSerializer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class MembershipCleaner
+{
+    private static final int port_ = 7000;
+    private static final long waitTime_ = 10000;
+    
+    public static void main(String[] args) throws Throwable
+    {
+        if ( args.length != 3 )
+        {
+            System.out.println("Usage : java com.facebook.infrastructure.tools.MembershipCleaner " +
+                    "<ip:port to send the message> " +
+                    "<node which needs to be removed> " +
+                    "<file containing all nodes in the cluster>");
+            System.exit(1);
+        }
+        
+        String ipPort = args[0];
+        String node = args[1];
+        String file = args[2];
+        
+        String[] ipPortPair = ipPort.split(":");
+        EndPoint target = new EndPoint(ipPortPair[0], Integer.valueOf(ipPortPair[1]));
+        MembershipCleanerMessage mcMessage = new MembershipCleanerMessage(node);
+        
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        MembershipCleanerMessage.serializer().serialize(mcMessage, dos);
+        /* Construct the token update message to be sent */
+        Message mbrshipCleanerMessage = new Message( new EndPoint(FBUtilities.getHostAddress(), port_), "", StorageService.mbrshipCleanerVerbHandler_, bos.toByteArray() );
+        
+        BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ) );
+        String line = null;
+       
+        while ( ( line = bufReader.readLine() ) != null )
+        {            
+            mbrshipCleanerMessage.addHeader(line, line.getBytes());
+        }
+        
+        System.out.println("Sending a membership clean message to " + target);
+        MessagingService.getMessagingInstance().sendOneWay(mbrshipCleanerMessage, target);
+        Thread.sleep(MembershipCleaner.waitTime_);
+        System.out.println("Done sending the update message");
+    }
+    
+    public static class MembershipCleanerMessage implements Serializable
+    {
+        private static ICompactSerializer<MembershipCleanerMessage> serializer_;
+        private static AtomicInteger idGen_ = new AtomicInteger(0);
+        
+        static
+        {
+            serializer_ = new MembershipCleanerMessageSerializer();            
+        }
+        
+        static ICompactSerializer<MembershipCleanerMessage> serializer()
+        {
+            return serializer_;
+        }
+
+        private String target_;
+        
+        MembershipCleanerMessage(String target)
+        {
+            target_ = target;        
+        }
+        
+        String getTarget()
+        {
+            return target_;
+        }
+    }
+    
+    public static class MembershipCleanerMessageSerializer implements ICompactSerializer<MembershipCleanerMessage>
+    {
+        public void serialize(MembershipCleanerMessage mcMessage, DataOutputStream dos) throws IOException
+        {            
+            dos.writeUTF(mcMessage.getTarget() );                      
+        }
+        
+        public MembershipCleanerMessage deserialize(DataInputStream dis) throws IOException
+        {            
+            return new MembershipCleanerMessage(dis.readUTF());
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java b/src/java/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java
index e98d1263e0..b34db74d7f 100644
--- a/src/java/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java
+++ b/src/java/org/apache/cassandra/tools/MembershipCleanerVerbHandler.java
@@ -1,89 +1,89 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.tools;
-
-import java.io.IOException;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.LogUtil;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class MembershipCleanerVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger(MembershipCleanerVerbHandler.class);
-
-    public void doVerb(Message message)
-    {
-        byte[] body = message.getMessageBody();
-        
-        try
-        {
-            DataInputBuffer bufIn = new DataInputBuffer();
-            bufIn.reset(body, body.length);
-            /* Deserialize to get the token for this endpoint. */
-            MembershipCleaner.MembershipCleanerMessage mcMessage = MembershipCleaner.MembershipCleanerMessage.serializer().deserialize(bufIn);
-            
-            String target = mcMessage.getTarget();
-            logger_.info("Removing the node [" + target + "] from membership");
-            EndPoint targetEndPoint = new EndPoint(target, DatabaseDescriptor.getControlPort());
-            /* Remove the token related information for this endpoint */
-            StorageService.instance().removeTokenState(targetEndPoint);
-            
-            /* Get the headers for this message */
-            Map<String, byte[]> headers = message.getHeaders();
-            headers.remove( StorageService.getLocalStorageEndPoint().getHost() );
-            if (logger_.isDebugEnabled())
-              logger_.debug("Number of nodes in the header " + headers.size());
-            Set<String> nodes = headers.keySet();
-            
-            for ( String node : nodes )
-            {            
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Processing node " + node);
-                byte[] bytes = headers.remove(node);
-                /* Send a message to this node to alter its membership state. */
-                EndPoint targetNode = new EndPoint(node, DatabaseDescriptor.getStoragePort());                
-                
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Sending a membership clean message to " + targetNode);
-                MessagingService.getMessagingInstance().sendOneWay(message, targetNode);
-                break;
-            }                        
-        }
-        catch( IOException ex )
-        {
-            if (logger_.isDebugEnabled())
-              logger_.debug(LogUtil.throwableToString(ex));
-        }
-    }
-
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class MembershipCleanerVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(MembershipCleanerVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+        byte[] body = message.getMessageBody();
+        
+        try
+        {
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length);
+            /* Deserialize to get the token for this endpoint. */
+            MembershipCleaner.MembershipCleanerMessage mcMessage = MembershipCleaner.MembershipCleanerMessage.serializer().deserialize(bufIn);
+            
+            String target = mcMessage.getTarget();
+            logger_.info("Removing the node [" + target + "] from membership");
+            EndPoint targetEndPoint = new EndPoint(target, DatabaseDescriptor.getControlPort());
+            /* Remove the token related information for this endpoint */
+            StorageService.instance().removeTokenState(targetEndPoint);
+            
+            /* Get the headers for this message */
+            Map<String, byte[]> headers = message.getHeaders();
+            headers.remove( StorageService.getLocalStorageEndPoint().getHost() );
+            if (logger_.isDebugEnabled())
+              logger_.debug("Number of nodes in the header " + headers.size());
+            Set<String> nodes = headers.keySet();
+            
+            for ( String node : nodes )
+            {            
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Processing node " + node);
+                byte[] bytes = headers.remove(node);
+                /* Send a message to this node to alter its membership state. */
+                EndPoint targetNode = new EndPoint(node, DatabaseDescriptor.getStoragePort());                
+                
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Sending a membership clean message to " + targetNode);
+                MessagingService.getMessagingInstance().sendOneWay(message, targetNode);
+                break;
+            }                        
+        }
+        catch( IOException ex )
+        {
+            if (logger_.isDebugEnabled())
+              logger_.debug(LogUtil.throwableToString(ex));
+        }
+    }
+
+}
diff --git a/src/java/org/apache/cassandra/tools/ThreadListBuilder.java b/src/java/org/apache/cassandra/tools/ThreadListBuilder.java
index e10692516c..940c133201 100644
--- a/src/java/org/apache/cassandra/tools/ThreadListBuilder.java
+++ b/src/java/org/apache/cassandra/tools/ThreadListBuilder.java
@@ -1,94 +1,94 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.tools;
-
-import java.io.BufferedReader;
-import java.io.File;
-import java.io.FileInputStream;
-import java.io.InputStreamReader;
-import java.io.RandomAccessFile;
-import java.util.ArrayList;
-import java.util.List;
-
-import org.apache.cassandra.io.DataOutputBuffer;
-import org.apache.cassandra.utils.BloomFilter;
-
-
-public class ThreadListBuilder
-{
-    private final static int bufSize_ = 64*1024*1024;
-    private final static int count_ = 128*1024*1024;
-    
-    public static void main(String[] args) throws Throwable
-    {
-        if ( args.length != 2 )
-        {
-            System.out.println("Usage : java org.apache.cassandra.tools.ThreadListBuilder <directory containing files to be processed> <directory to dump the bloom filter in.>");
-            System.exit(1);
-        }
-        
-        File directory = new File(args[0]);
-        File[] files = directory.listFiles();
-        List<DataOutputBuffer> buffers = new ArrayList<DataOutputBuffer>();    
-        BloomFilter bf = new BloomFilter(count_, 8);        
-        int keyCount = 0;
-        
-        /* Process the list of files. */
-        for ( File file : files )
-        {
-            System.out.println("Processing file " + file);
-            BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ), ThreadListBuilder.bufSize_ );
-            String line = null;
-            
-            while ( (line = bufReader.readLine()) != null )
-            {
-                /* After accumulating count_ keys reset the bloom filter. */
-                if ( keyCount > 0 && keyCount % count_ == 0 )
-                {                       
-                    DataOutputBuffer bufOut = new DataOutputBuffer();
-                    BloomFilter.serializer().serialize(bf, bufOut);
-                    System.out.println("Finished serializing the bloom filter");
-                    buffers.add(bufOut);
-                    bf = new BloomFilter(count_, 8);
-                }
-                line = line.trim();                
-                bf.add(line);
-                ++keyCount;
-            }
-        }
-        
-        /* Add the bloom filter assuming the last one was left out */
-        DataOutputBuffer bufOut = new DataOutputBuffer();
-        BloomFilter.serializer().serialize(bf, bufOut);
-        buffers.add(bufOut);
-        
-        
-        int size = buffers.size();
-        for ( int i = 0; i < size; ++i )
-        {
-            DataOutputBuffer buffer = buffers.get(i);
-            String file = args[1] + File.separator + "Bloom-Filter-" + i + ".dat";
-            RandomAccessFile raf = new RandomAccessFile(file, "rw");
-            raf.write(buffer.getData(), 0, buffer.getLength());
-            raf.close();
-            buffer.close();
-        }
-        System.out.println("Done writing the bloom filter to disk");
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.BufferedReader;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+import java.io.RandomAccessFile;
+import java.util.ArrayList;
+import java.util.List;
+
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.utils.BloomFilter;
+
+
+public class ThreadListBuilder
+{
+    private final static int bufSize_ = 64*1024*1024;
+    private final static int count_ = 128*1024*1024;
+    
+    public static void main(String[] args) throws Throwable
+    {
+        if ( args.length != 2 )
+        {
+            System.out.println("Usage : java org.apache.cassandra.tools.ThreadListBuilder <directory containing files to be processed> <directory to dump the bloom filter in.>");
+            System.exit(1);
+        }
+        
+        File directory = new File(args[0]);
+        File[] files = directory.listFiles();
+        List<DataOutputBuffer> buffers = new ArrayList<DataOutputBuffer>();    
+        BloomFilter bf = new BloomFilter(count_, 8);        
+        int keyCount = 0;
+        
+        /* Process the list of files. */
+        for ( File file : files )
+        {
+            System.out.println("Processing file " + file);
+            BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ), ThreadListBuilder.bufSize_ );
+            String line = null;
+            
+            while ( (line = bufReader.readLine()) != null )
+            {
+                /* After accumulating count_ keys reset the bloom filter. */
+                if ( keyCount > 0 && keyCount % count_ == 0 )
+                {                       
+                    DataOutputBuffer bufOut = new DataOutputBuffer();
+                    BloomFilter.serializer().serialize(bf, bufOut);
+                    System.out.println("Finished serializing the bloom filter");
+                    buffers.add(bufOut);
+                    bf = new BloomFilter(count_, 8);
+                }
+                line = line.trim();                
+                bf.add(line);
+                ++keyCount;
+            }
+        }
+        
+        /* Add the bloom filter assuming the last one was left out */
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        BloomFilter.serializer().serialize(bf, bufOut);
+        buffers.add(bufOut);
+        
+        
+        int size = buffers.size();
+        for ( int i = 0; i < size; ++i )
+        {
+            DataOutputBuffer buffer = buffers.get(i);
+            String file = args[1] + File.separator + "Bloom-Filter-" + i + ".dat";
+            RandomAccessFile raf = new RandomAccessFile(file, "rw");
+            raf.write(buffer.getData(), 0, buffer.getLength());
+            raf.close();
+            buffer.close();
+        }
+        System.out.println("Done writing the bloom filter to disk");
+    }
+}
diff --git a/src/java/org/apache/cassandra/tools/TokenUpdateVerbHandler.java b/src/java/org/apache/cassandra/tools/TokenUpdateVerbHandler.java
index 7d16438696..c91058a5ec 100644
--- a/src/java/org/apache/cassandra/tools/TokenUpdateVerbHandler.java
+++ b/src/java/org/apache/cassandra/tools/TokenUpdateVerbHandler.java
@@ -1,98 +1,98 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.tools;
-
-import java.io.ByteArrayOutputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.Map;
-import java.util.Set;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.LogUtil;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class TokenUpdateVerbHandler implements IVerbHandler
-{
-    private static Logger logger_ = Logger.getLogger(TokenUpdateVerbHandler.class);
-
-    public void doVerb(Message message)
-    {
-    	byte[] body = message.getMessageBody();
-        
-        try
-        {
-            DataInputBuffer bufIn = new DataInputBuffer();
-            bufIn.reset(body, body.length);
-            /* Deserialize to get the token for this endpoint. */
-            Token token = Token.serializer().deserialize(bufIn);
-
-            logger_.info("Updating the token to [" + token + "]");
-            StorageService.instance().updateToken(token);
-            
-            /* Get the headers for this message */
-            Map<String, byte[]> headers = message.getHeaders();
-            headers.remove( StorageService.getLocalStorageEndPoint().getHost() );
-            if (logger_.isDebugEnabled())
-              logger_.debug("Number of nodes in the header " + headers.size());
-            Set<String> nodes = headers.keySet();
-            
-            IPartitioner p = StorageService.getPartitioner();
-            for ( String node : nodes )
-            {            
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Processing node " + node);
-                byte[] bytes = headers.remove(node);
-                /* Send a message to this node to update its token to the one retrieved. */
-                EndPoint target = new EndPoint(node, DatabaseDescriptor.getStoragePort());
-                token = p.getTokenFactory().fromByteArray(bytes);
-                
-                /* Reset the new Message */
-                ByteArrayOutputStream bos = new ByteArrayOutputStream();
-                DataOutputStream dos = new DataOutputStream(bos);
-                Token.serializer().serialize(token, dos);
-                message.setMessageBody(bos.toByteArray());
-                
-                if (logger_.isDebugEnabled())
-                  logger_.debug("Sending a token update message to " + target + " to update it to " + token);
-                MessagingService.getMessagingInstance().sendOneWay(message, target);
-                break;
-            }                        
-        }
-    	catch( IOException ex )
-    	{
-    		if (logger_.isDebugEnabled())
-    		  logger_.debug(LogUtil.throwableToString(ex));
-    	}
-    }
-
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.Map;
+import java.util.Set;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.LogUtil;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class TokenUpdateVerbHandler implements IVerbHandler
+{
+    private static Logger logger_ = Logger.getLogger(TokenUpdateVerbHandler.class);
+
+    public void doVerb(Message message)
+    {
+    	byte[] body = message.getMessageBody();
+        
+        try
+        {
+            DataInputBuffer bufIn = new DataInputBuffer();
+            bufIn.reset(body, body.length);
+            /* Deserialize to get the token for this endpoint. */
+            Token token = Token.serializer().deserialize(bufIn);
+
+            logger_.info("Updating the token to [" + token + "]");
+            StorageService.instance().updateToken(token);
+            
+            /* Get the headers for this message */
+            Map<String, byte[]> headers = message.getHeaders();
+            headers.remove( StorageService.getLocalStorageEndPoint().getHost() );
+            if (logger_.isDebugEnabled())
+              logger_.debug("Number of nodes in the header " + headers.size());
+            Set<String> nodes = headers.keySet();
+            
+            IPartitioner p = StorageService.getPartitioner();
+            for ( String node : nodes )
+            {            
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Processing node " + node);
+                byte[] bytes = headers.remove(node);
+                /* Send a message to this node to update its token to the one retrieved. */
+                EndPoint target = new EndPoint(node, DatabaseDescriptor.getStoragePort());
+                token = p.getTokenFactory().fromByteArray(bytes);
+                
+                /* Reset the new Message */
+                ByteArrayOutputStream bos = new ByteArrayOutputStream();
+                DataOutputStream dos = new DataOutputStream(bos);
+                Token.serializer().serialize(token, dos);
+                message.setMessageBody(bos.toByteArray());
+                
+                if (logger_.isDebugEnabled())
+                  logger_.debug("Sending a token update message to " + target + " to update it to " + token);
+                MessagingService.getMessagingInstance().sendOneWay(message, target);
+                break;
+            }                        
+        }
+    	catch( IOException ex )
+    	{
+    		if (logger_.isDebugEnabled())
+    		  logger_.debug(LogUtil.throwableToString(ex));
+    	}
+    }
+
+}
diff --git a/src/java/org/apache/cassandra/tools/TokenUpdater.java b/src/java/org/apache/cassandra/tools/TokenUpdater.java
index 01e1f0091f..378e2645aa 100644
--- a/src/java/org/apache/cassandra/tools/TokenUpdater.java
+++ b/src/java/org/apache/cassandra/tools/TokenUpdater.java
@@ -1,80 +1,80 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.tools;
-
-import java.io.BufferedReader;
-import java.io.ByteArrayOutputStream;
-import java.io.DataOutputStream;
-import java.io.FileInputStream;
-import java.io.InputStreamReader;
-
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.net.EndPoint;
-import org.apache.cassandra.net.Message;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.utils.FBUtilities;
-
-public class TokenUpdater
-{
-    private static final int port_ = 7000;
-    private static final long waitTime_ = 10000;
-    
-    public static void main(String[] args) throws Throwable
-    {
-        if ( args.length != 3 )
-        {
-            System.out.println("Usage : java org.apache.cassandra.tools.TokenUpdater <ip:port> <token> <file containing node token info>");
-            System.exit(1);
-        }
-        
-        String ipPort = args[0];
-        IPartitioner p = StorageService.getPartitioner();
-        Token token = p.getTokenFactory().fromString(args[1]);
-        String file = args[2];
-        
-        String[] ipPortPair = ipPort.split(":");
-        EndPoint target = new EndPoint(ipPortPair[0], Integer.valueOf(ipPortPair[1]));
-
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        DataOutputStream dos = new DataOutputStream(bos);
-        Token.serializer().serialize(token, dos);
-
-        /* Construct the token update message to be sent */
-        Message tokenUpdateMessage = new Message( new EndPoint(FBUtilities.getHostAddress(), port_), "", StorageService.tokenVerbHandler_, bos.toByteArray() );
-        
-        BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ) );
-        String line = null;
-       
-        while ( ( line = bufReader.readLine() ) != null )
-        {
-            String[] nodeTokenPair = line.split(" ");
-            /* Add the node and the token pair into the header of this message. */
-            Token nodeToken = p.getTokenFactory().fromString(nodeTokenPair[1]);
-            tokenUpdateMessage.addHeader(nodeTokenPair[0], p.getTokenFactory().toByteArray(nodeToken));
-        }
-        
-        System.out.println("Sending a token update message to " + target);
-        MessagingService.getMessagingInstance().sendOneWay(tokenUpdateMessage, target);
-        Thread.sleep(TokenUpdater.waitTime_);
-        System.out.println("Done sending the update message");
-    }
-
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.tools;
+
+import java.io.BufferedReader;
+import java.io.ByteArrayOutputStream;
+import java.io.DataOutputStream;
+import java.io.FileInputStream;
+import java.io.InputStreamReader;
+
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.net.EndPoint;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class TokenUpdater
+{
+    private static final int port_ = 7000;
+    private static final long waitTime_ = 10000;
+    
+    public static void main(String[] args) throws Throwable
+    {
+        if ( args.length != 3 )
+        {
+            System.out.println("Usage : java org.apache.cassandra.tools.TokenUpdater <ip:port> <token> <file containing node token info>");
+            System.exit(1);
+        }
+        
+        String ipPort = args[0];
+        IPartitioner p = StorageService.getPartitioner();
+        Token token = p.getTokenFactory().fromString(args[1]);
+        String file = args[2];
+        
+        String[] ipPortPair = ipPort.split(":");
+        EndPoint target = new EndPoint(ipPortPair[0], Integer.valueOf(ipPortPair[1]));
+
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        DataOutputStream dos = new DataOutputStream(bos);
+        Token.serializer().serialize(token, dos);
+
+        /* Construct the token update message to be sent */
+        Message tokenUpdateMessage = new Message( new EndPoint(FBUtilities.getHostAddress(), port_), "", StorageService.tokenVerbHandler_, bos.toByteArray() );
+        
+        BufferedReader bufReader = new BufferedReader( new InputStreamReader( new FileInputStream(file) ) );
+        String line = null;
+       
+        while ( ( line = bufReader.readLine() ) != null )
+        {
+            String[] nodeTokenPair = line.split(" ");
+            /* Add the node and the token pair into the header of this message. */
+            Token nodeToken = p.getTokenFactory().fromString(nodeTokenPair[1]);
+            tokenUpdateMessage.addHeader(nodeTokenPair[0], p.getTokenFactory().toByteArray(nodeToken));
+        }
+        
+        System.out.println("Sending a token update message to " + target);
+        MessagingService.getMessagingInstance().sendOneWay(tokenUpdateMessage, target);
+        Thread.sleep(TokenUpdater.waitTime_);
+        System.out.println("Done sending the update message");
+    }
+
+}
diff --git a/src/java/org/apache/cassandra/utils/BasicUtilities.java b/src/java/org/apache/cassandra/utils/BasicUtilities.java
index d08cfbc9c1..68c6138749 100644
--- a/src/java/org/apache/cassandra/utils/BasicUtilities.java
+++ b/src/java/org/apache/cassandra/utils/BasicUtilities.java
@@ -1,66 +1,66 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-import java.nio.ByteBuffer;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-
-public class BasicUtilities
-{        
-	public static byte[] longToByteArray(long arg)
-	{      
-        byte[] retVal = new byte[8];
-        ByteBuffer.wrap(retVal).putLong(arg);
-        return retVal; 
-	 }
-	
-	public static long byteArrayToLong(byte[] arg)
-	{
-        return ByteBuffer.wrap(arg).getLong();
-	}
-	
-	public static byte[] intToByteArray(int arg)
-	{      
-        byte[] retVal = new byte[4];
-        ByteBuffer.wrap(retVal).putInt(arg);
-        return retVal; 
-	 }
-	
-	public static int byteArrayToInt(byte[] arg)
-	{
-        return ByteBuffer.wrap(arg).getInt();
-	}
-	
-	public static byte[] shortToByteArray(short arg)
-	{      
-        byte[] retVal = new byte[2];
-        ByteBuffer bb= ByteBuffer.wrap(retVal);
-        bb.putShort(arg);
-        return retVal; 
-	 }
-	
-	public static short byteArrayToShort(byte[] arg)
-	{
-        return ByteBuffer.wrap(arg).getShort();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.nio.ByteBuffer;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+
+public class BasicUtilities
+{        
+	public static byte[] longToByteArray(long arg)
+	{      
+        byte[] retVal = new byte[8];
+        ByteBuffer.wrap(retVal).putLong(arg);
+        return retVal; 
+	 }
+	
+	public static long byteArrayToLong(byte[] arg)
+	{
+        return ByteBuffer.wrap(arg).getLong();
+	}
+	
+	public static byte[] intToByteArray(int arg)
+	{      
+        byte[] retVal = new byte[4];
+        ByteBuffer.wrap(retVal).putInt(arg);
+        return retVal; 
+	 }
+	
+	public static int byteArrayToInt(byte[] arg)
+	{
+        return ByteBuffer.wrap(arg).getInt();
+	}
+	
+	public static byte[] shortToByteArray(short arg)
+	{      
+        byte[] retVal = new byte[2];
+        ByteBuffer bb= ByteBuffer.wrap(retVal);
+        bb.putShort(arg);
+        return retVal; 
+	 }
+	
+	public static short byteArrayToShort(byte[] arg)
+	{
+        return ByteBuffer.wrap(arg).getShort();
+    }
+}
diff --git a/src/java/org/apache/cassandra/utils/BloomCalculations.java b/src/java/org/apache/cassandra/utils/BloomCalculations.java
index 0510dd1d46..6fdb1ea9f6 100644
--- a/src/java/org/apache/cassandra/utils/BloomCalculations.java
+++ b/src/java/org/apache/cassandra/utils/BloomCalculations.java
@@ -1,132 +1,132 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-/**
- * The following calculations are taken from:
- * http://www.cs.wisc.edu/~cao/papers/summary-cache/node8.html
- * "Bloom Filters - the math"
- *
- * This class's static methods are meant to facilitate the use of the Bloom
- * Filter class by helping to choose correct values of 'bits per element' and
- * 'number of hash functions, k'.
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-public class BloomCalculations {
-
-    private static final int maxBuckets = 15;
-    private static final int minBuckets = 2;
-    private static final int minK = 1;
-    private static final int maxK = 8;
-    private static final int[] optKPerBuckets =
-            new int[]{1, // dummy K for 0 buckets per element
-                      1, // dummy K for 1 buckets per element
-                      1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 8, 8, 8, 8};
-
-    /**
-     * In the following table, the row 'i' shows false positive rates if i buckets
-     * per element are used.  Column 'j' shows false positive rates if j hash
-     * functions are used.  The first row is 'i=0', the first column is 'j=0'.
-     * Each cell (i,j) the false positive rate determined by using i buckets per
-     * element and j hash functions.
-     */
-    static final double[][] probs = new double[][]{
-        {1.0}, // dummy row representing 0 buckets per element
-        {1.0, 1.0}, // dummy row representing 1 buckets per element
-        {1.0, 0.393,  0.400},
-        {1.0, 0.283,  0.237,  0.253},
-        {1.0, 0.221,  0.155,  0.147,   0.160},
-        {1.0, 0.181,  0.109,  0.092,   0.092,   0.101}, // 5
-        {1.0, 0.154,  0.0804, 0.0609,  0.0561,  0.0578,  0.0638},
-        {1.0, 0.133,  0.0618, 0.0423,  0.0359,  0.0347,  0.0364},
-        {1.0, 0.118,  0.0489, 0.0306,  0.024,   0.0217,  0.0216,  0.0229},
-        {1.0, 0.105,  0.0397, 0.0228,  0.0166,  0.0141,  0.0133,  0.0135,  0.0145}, // 9
-        {1.0, 0.0952, 0.0329, 0.0174,  0.0118,  0.00943, 0.00844, 0.00819, 0.00846},
-        {1.0, 0.0869, 0.0276, 0.0136,  0.00864, 0.0065,  0.00552, 0.00513, 0.00509},
-        {1.0, 0.08,   0.0236, 0.0108,  0.00646, 0.00459, 0.00371, 0.00329, 0.00314},
-        {1.0, 0.074,  0.0203, 0.00875, 0.00492, 0.00332, 0.00255, 0.00217, 0.00199},
-        {1.0, 0.0689, 0.0177, 0.00718, 0.00381, 0.00244, 0.00179, 0.00146, 0.00129},
-        {1.0, 0.0645, 0.0156, 0.00596, 0.003,   0.00183, 0.00128, 0.001,   0.000852} // 15
-    };  // the first column is a dummy column representing K=0.
-
-    /**
-     * Given the number of buckets that can be used per element, return the optimal
-     * number of hash functions in order to minimize the false positive rate.
-     *
-     * @param bucketsPerElement
-     * @return The number of hash functions that minimize the false positive rate.
-     */
-    public static int computeBestK(int bucketsPerElement){
-        assert bucketsPerElement >= 0;
-        if(bucketsPerElement >= optKPerBuckets.length)
-            return optKPerBuckets[optKPerBuckets.length-1];
-        return optKPerBuckets[bucketsPerElement];
-    }
-
-    /**
-     * A wrapper class that holds two key parameters for a Bloom Filter: the
-     * number of hash functions used, and the number of buckets per element used.
-     */
-    public static final class BloomSpecification {
-        final int K; // number of hash functions.
-        final int bucketsPerElement;
-
-        public BloomSpecification(int k, int bucketsPerElement) {
-            K = k;
-            this.bucketsPerElement = bucketsPerElement;
-        }
-    }
-
-    /**
-     * Given a maximum tolerable false positive probability, compute a Bloom
-     * specification which will give less than the specified false positive rate,
-     * but minimize the number of buckets per element and the number of hash
-     * functions used.  Because bandwidth (and therefore total bitvector size)
-     * is considered more expensive than computing power, preference is given
-     * to minimizing buckets per element rather than number of hash functions.
-     *
-     * @param maxFalsePosProb The maximum tolerable false positive rate.
-     * @return A Bloom Specification which would result in a false positive rate
-     * less than specified by the function call.
-     */
-    public static BloomSpecification computeBucketsAndK(double maxFalsePosProb){
-        // Handle the trivial cases
-        if(maxFalsePosProb >= probs[minBuckets][minK]) {
-            return new BloomSpecification(2, optKPerBuckets[2]);
-        }
-        if(maxFalsePosProb < probs[maxBuckets][maxK]) {
-            return new BloomSpecification(maxK, maxBuckets);
-        }
-
-        // First find the minimal required number of buckets:
-        int bucketsPerElement = 2;
-        int K = optKPerBuckets[2];
-        while(probs[bucketsPerElement][K] > maxFalsePosProb){
-            bucketsPerElement++;
-            K = optKPerBuckets[bucketsPerElement];
-        }
-        // Now that the number of buckets is sufficient, see if we can relax K
-        // without losing too much precision.
-        while(probs[bucketsPerElement][K - 1] <= maxFalsePosProb){
-            K--;
-        }
-
-        return new BloomSpecification(K, bucketsPerElement);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+/**
+ * The following calculations are taken from:
+ * http://www.cs.wisc.edu/~cao/papers/summary-cache/node8.html
+ * "Bloom Filters - the math"
+ *
+ * This class's static methods are meant to facilitate the use of the Bloom
+ * Filter class by helping to choose correct values of 'bits per element' and
+ * 'number of hash functions, k'.
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+public class BloomCalculations {
+
+    private static final int maxBuckets = 15;
+    private static final int minBuckets = 2;
+    private static final int minK = 1;
+    private static final int maxK = 8;
+    private static final int[] optKPerBuckets =
+            new int[]{1, // dummy K for 0 buckets per element
+                      1, // dummy K for 1 buckets per element
+                      1, 2, 3, 3, 4, 5, 5, 6, 7, 8, 8, 8, 8, 8};
+
+    /**
+     * In the following table, the row 'i' shows false positive rates if i buckets
+     * per element are used.  Column 'j' shows false positive rates if j hash
+     * functions are used.  The first row is 'i=0', the first column is 'j=0'.
+     * Each cell (i,j) the false positive rate determined by using i buckets per
+     * element and j hash functions.
+     */
+    static final double[][] probs = new double[][]{
+        {1.0}, // dummy row representing 0 buckets per element
+        {1.0, 1.0}, // dummy row representing 1 buckets per element
+        {1.0, 0.393,  0.400},
+        {1.0, 0.283,  0.237,  0.253},
+        {1.0, 0.221,  0.155,  0.147,   0.160},
+        {1.0, 0.181,  0.109,  0.092,   0.092,   0.101}, // 5
+        {1.0, 0.154,  0.0804, 0.0609,  0.0561,  0.0578,  0.0638},
+        {1.0, 0.133,  0.0618, 0.0423,  0.0359,  0.0347,  0.0364},
+        {1.0, 0.118,  0.0489, 0.0306,  0.024,   0.0217,  0.0216,  0.0229},
+        {1.0, 0.105,  0.0397, 0.0228,  0.0166,  0.0141,  0.0133,  0.0135,  0.0145}, // 9
+        {1.0, 0.0952, 0.0329, 0.0174,  0.0118,  0.00943, 0.00844, 0.00819, 0.00846},
+        {1.0, 0.0869, 0.0276, 0.0136,  0.00864, 0.0065,  0.00552, 0.00513, 0.00509},
+        {1.0, 0.08,   0.0236, 0.0108,  0.00646, 0.00459, 0.00371, 0.00329, 0.00314},
+        {1.0, 0.074,  0.0203, 0.00875, 0.00492, 0.00332, 0.00255, 0.00217, 0.00199},
+        {1.0, 0.0689, 0.0177, 0.00718, 0.00381, 0.00244, 0.00179, 0.00146, 0.00129},
+        {1.0, 0.0645, 0.0156, 0.00596, 0.003,   0.00183, 0.00128, 0.001,   0.000852} // 15
+    };  // the first column is a dummy column representing K=0.
+
+    /**
+     * Given the number of buckets that can be used per element, return the optimal
+     * number of hash functions in order to minimize the false positive rate.
+     *
+     * @param bucketsPerElement
+     * @return The number of hash functions that minimize the false positive rate.
+     */
+    public static int computeBestK(int bucketsPerElement){
+        assert bucketsPerElement >= 0;
+        if(bucketsPerElement >= optKPerBuckets.length)
+            return optKPerBuckets[optKPerBuckets.length-1];
+        return optKPerBuckets[bucketsPerElement];
+    }
+
+    /**
+     * A wrapper class that holds two key parameters for a Bloom Filter: the
+     * number of hash functions used, and the number of buckets per element used.
+     */
+    public static final class BloomSpecification {
+        final int K; // number of hash functions.
+        final int bucketsPerElement;
+
+        public BloomSpecification(int k, int bucketsPerElement) {
+            K = k;
+            this.bucketsPerElement = bucketsPerElement;
+        }
+    }
+
+    /**
+     * Given a maximum tolerable false positive probability, compute a Bloom
+     * specification which will give less than the specified false positive rate,
+     * but minimize the number of buckets per element and the number of hash
+     * functions used.  Because bandwidth (and therefore total bitvector size)
+     * is considered more expensive than computing power, preference is given
+     * to minimizing buckets per element rather than number of hash functions.
+     *
+     * @param maxFalsePosProb The maximum tolerable false positive rate.
+     * @return A Bloom Specification which would result in a false positive rate
+     * less than specified by the function call.
+     */
+    public static BloomSpecification computeBucketsAndK(double maxFalsePosProb){
+        // Handle the trivial cases
+        if(maxFalsePosProb >= probs[minBuckets][minK]) {
+            return new BloomSpecification(2, optKPerBuckets[2]);
+        }
+        if(maxFalsePosProb < probs[maxBuckets][maxK]) {
+            return new BloomSpecification(maxK, maxBuckets);
+        }
+
+        // First find the minimal required number of buckets:
+        int bucketsPerElement = 2;
+        int K = optKPerBuckets[2];
+        while(probs[bucketsPerElement][K] > maxFalsePosProb){
+            bucketsPerElement++;
+            K = optKPerBuckets[bucketsPerElement];
+        }
+        // Now that the number of buckets is sufficient, see if we can relax K
+        // without losing too much precision.
+        while(probs[bucketsPerElement][K - 1] <= maxFalsePosProb){
+            K--;
+        }
+
+        return new BloomSpecification(K, bucketsPerElement);
+    }
+}
diff --git a/src/java/org/apache/cassandra/utils/BloomFilter.java b/src/java/org/apache/cassandra/utils/BloomFilter.java
index dca375976d..e78961783f 100644
--- a/src/java/org/apache/cassandra/utils/BloomFilter.java
+++ b/src/java/org/apache/cassandra/utils/BloomFilter.java
@@ -1,156 +1,156 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-import java.io.DataInputStream;
-import java.io.DataOutputStream;
-import java.io.IOException;
-import java.util.BitSet;
-
-import org.apache.cassandra.io.ICompactSerializer;
-
-public class BloomFilter extends Filter
-{
-    static ICompactSerializer<BloomFilter> serializer_ = new BloomFilterSerializer();
-
-    public static ICompactSerializer<BloomFilter> serializer()
-    {
-        return serializer_;
-    }
-
-    private BitSet filter_;
-
-    public BloomFilter(int numElements, int bucketsPerElement)
-    {
-        this(BloomCalculations.computeBestK(bucketsPerElement), new BitSet(numElements * bucketsPerElement + 20));
-    }
-
-    public BloomFilter(int numElements, double maxFalsePosProbability)
-    {
-        BloomCalculations.BloomSpecification spec = BloomCalculations
-                .computeBucketsAndK(maxFalsePosProbability);
-        filter_ = new BitSet(numElements * spec.bucketsPerElement + 20);
-        hashCount = spec.K;
-    }
-
-    /*
-     * This version is only used by the deserializer.
-     */
-    BloomFilter(int hashes, BitSet filter)
-    {
-        hashCount = hashes;
-        filter_ = filter;
-    }
-
-    public void clear()
-    {
-        filter_.clear();
-    }
-
-    int buckets()
-    {
-        return filter_.size();
-    }
-
-    BitSet filter()
-    {
-        return filter_;
-    }
-
-    public boolean isPresent(String key)
-    {
-        for (int bucketIndex : getHashBuckets(key))
-        {
-            if (!filter_.get(bucketIndex))
-            {
-                return false;
-            }
-        }
-        return true;
-    }
-
-    /*
-     @param key -- value whose hash is used to fill
-     the filter_.
-     This is a general purpose API.
-     */
-    public void add(String key)
-    {
-        for (int bucketIndex : getHashBuckets(key))
-        {
-            filter_.set(bucketIndex);
-        }
-    }
-
-    public void add(byte[] key)
-    {
-        for (int bucketIndex : getHashBuckets(key))
-        {
-            filter_.set(bucketIndex);
-        }
-    }
-
-    public String toString()
-    {
-        return filter_.toString();
-    }
-
-    ICompactSerializer tserializer()
-    {
-        return serializer_;
-    }
-
-    int emptyBuckets()
-    {
-        int n = 0;
-        for (int i = 0; i < buckets(); i++)
-        {
-            if (!filter_.get(i))
-            {
-                n++;
-            }
-        }
-        return n;
-    }
-
-    /** @return a BloomFilter that always returns a positive match, for testing */
-    public static BloomFilter alwaysMatchingBloomFilter()
-    {
-        BitSet set = new BitSet(64);
-        set.set(0, 64);
-        return new BloomFilter(1, set);
-    }
-}
-
-class BloomFilterSerializer implements ICompactSerializer<BloomFilter>
-{
-    public void serialize(BloomFilter bf, DataOutputStream dos)
-            throws IOException
-    {
-        dos.writeInt(bf.getHashCount());
-        BitSetSerializer.serialize(bf.filter(), dos);
-    }
-
-    public BloomFilter deserialize(DataInputStream dis) throws IOException
-    {
-        int hashes = dis.readInt();
-        BitSet bs = BitSetSerializer.deserialize(dis);
-        return new BloomFilter(hashes, bs);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.DataInputStream;
+import java.io.DataOutputStream;
+import java.io.IOException;
+import java.util.BitSet;
+
+import org.apache.cassandra.io.ICompactSerializer;
+
+public class BloomFilter extends Filter
+{
+    static ICompactSerializer<BloomFilter> serializer_ = new BloomFilterSerializer();
+
+    public static ICompactSerializer<BloomFilter> serializer()
+    {
+        return serializer_;
+    }
+
+    private BitSet filter_;
+
+    public BloomFilter(int numElements, int bucketsPerElement)
+    {
+        this(BloomCalculations.computeBestK(bucketsPerElement), new BitSet(numElements * bucketsPerElement + 20));
+    }
+
+    public BloomFilter(int numElements, double maxFalsePosProbability)
+    {
+        BloomCalculations.BloomSpecification spec = BloomCalculations
+                .computeBucketsAndK(maxFalsePosProbability);
+        filter_ = new BitSet(numElements * spec.bucketsPerElement + 20);
+        hashCount = spec.K;
+    }
+
+    /*
+     * This version is only used by the deserializer.
+     */
+    BloomFilter(int hashes, BitSet filter)
+    {
+        hashCount = hashes;
+        filter_ = filter;
+    }
+
+    public void clear()
+    {
+        filter_.clear();
+    }
+
+    int buckets()
+    {
+        return filter_.size();
+    }
+
+    BitSet filter()
+    {
+        return filter_;
+    }
+
+    public boolean isPresent(String key)
+    {
+        for (int bucketIndex : getHashBuckets(key))
+        {
+            if (!filter_.get(bucketIndex))
+            {
+                return false;
+            }
+        }
+        return true;
+    }
+
+    /*
+     @param key -- value whose hash is used to fill
+     the filter_.
+     This is a general purpose API.
+     */
+    public void add(String key)
+    {
+        for (int bucketIndex : getHashBuckets(key))
+        {
+            filter_.set(bucketIndex);
+        }
+    }
+
+    public void add(byte[] key)
+    {
+        for (int bucketIndex : getHashBuckets(key))
+        {
+            filter_.set(bucketIndex);
+        }
+    }
+
+    public String toString()
+    {
+        return filter_.toString();
+    }
+
+    ICompactSerializer tserializer()
+    {
+        return serializer_;
+    }
+
+    int emptyBuckets()
+    {
+        int n = 0;
+        for (int i = 0; i < buckets(); i++)
+        {
+            if (!filter_.get(i))
+            {
+                n++;
+            }
+        }
+        return n;
+    }
+
+    /** @return a BloomFilter that always returns a positive match, for testing */
+    public static BloomFilter alwaysMatchingBloomFilter()
+    {
+        BitSet set = new BitSet(64);
+        set.set(0, 64);
+        return new BloomFilter(1, set);
+    }
+}
+
+class BloomFilterSerializer implements ICompactSerializer<BloomFilter>
+{
+    public void serialize(BloomFilter bf, DataOutputStream dos)
+            throws IOException
+    {
+        dos.writeInt(bf.getHashCount());
+        BitSetSerializer.serialize(bf.filter(), dos);
+    }
+
+    public BloomFilter deserialize(DataInputStream dis) throws IOException
+    {
+        int hashes = dis.readInt();
+        BitSet bs = BitSetSerializer.deserialize(dis);
+        return new BloomFilter(hashes, bs);
+    }
+}
diff --git a/src/java/org/apache/cassandra/utils/Cachetable.java b/src/java/org/apache/cassandra/utils/Cachetable.java
index 445aa79e29..fce1e8e38c 100644
--- a/src/java/org/apache/cassandra/utils/Cachetable.java
+++ b/src/java/org/apache/cassandra/utils/Cachetable.java
@@ -1,218 +1,218 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-import java.util.*;
-
-import org.apache.log4j.Logger;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class Cachetable<K,V> implements ICachetable<K,V>
-{
-    private class CacheableObject<V>
-    {
-        private V value_;
-        private long age_;
-        
-        CacheableObject(V o)
-        {
-            value_ = o;
-            age_ = System.currentTimeMillis();
-        }
-
-        public boolean equals(Object o)
-        {
-            return value_.equals(o);
-        }
-
-        public int hashCode()
-        {
-            return value_.hashCode();
-        }
-
-        V getValue()
-        {
-            return value_;
-        }           
-        
-        boolean isReadyToDie(long expiration)
-        {
-            return ( (System.currentTimeMillis() - age_) > expiration );
-        }
-    }
-    
-    private class CacheMonitor extends TimerTask
-    {
-        private long expiration_;
-        
-        CacheMonitor(long expiration)
-        {
-            expiration_ = expiration;
-        }
-        
-        public void run()
-        {  
-            Map<K,V> expungedValues = new HashMap<K,V>();            
-            synchronized(cache_)
-            {
-                Enumeration<K> e = cache_.keys();
-                while( e.hasMoreElements() )
-                {        
-                    K key = e.nextElement();
-                    CacheableObject<V> co = cache_.get(key);
-                    if ( co != null && co.isReadyToDie(expiration_) )
-                    {
-                        V v = co.getValue();
-                        if(null != v)
-                            expungedValues.put(key, v);
-                        cache_.remove(key);                                       
-                    }
-                }
-            }
-            
-            /* Calling the hooks on the keys that have been expunged */
-            Set<K> keys = expungedValues.keySet();                                               
-            for ( K key : keys )
-            {                                
-                V value = expungedValues.get(key);
-                ICacheExpungeHook<K,V> hook = hooks_.remove(key);
-                try 
-                {
-                    if ( hook != null )
-                    {
-                        hook.callMe(key, value);                    
-                    }
-                    else if ( globalHook_ != null )
-                    {
-                        globalHook_.callMe(key, value);
-                    }
-                }
-                catch(Throwable e)
-                {
-                    logger_.info(LogUtil.throwableToString(e));
-                }
-            }
-            expungedValues.clear();
-        }
-    }   
-
-    private ICacheExpungeHook<K,V> globalHook_;
-    private Hashtable<K, CacheableObject<V>> cache_;
-    private Map<K, ICacheExpungeHook<K,V>> hooks_;
-    private Timer timer_;
-    private static int counter_ = 0;
-    private static Logger logger_ = Logger.getLogger(Cachetable.class);
-
-    private void init(long expiration)
-    {
-        if ( expiration <= 0 )
-            throw new IllegalArgumentException("Argument specified must be a positive number");
-
-        cache_ = new Hashtable<K, CacheableObject<V>>();
-        hooks_ = new Hashtable<K, ICacheExpungeHook<K,V>>();
-        timer_ = new Timer("CACHETABLE-TIMER-" + (++counter_), true);        
-        timer_.schedule(new CacheMonitor(expiration), expiration, expiration);
-    }
-    
-    /*
-     * Specify the TTL for objects in the cache
-     * in milliseconds.
-     */
-    public Cachetable(long expiration)
-    {
-        init(expiration);   
-    }    
-    
-    /*
-     * Specify the TTL for objects in the cache
-     * in milliseconds and a global expunge hook. If
-     * a key has a key-specific hook installed invoke that
-     * instead.
-     */
-    public Cachetable(long expiration, ICacheExpungeHook<K,V> global)
-    {
-        init(expiration);
-        globalHook_ = global;        
-    }
-    
-    public void shutdown()
-    {
-        timer_.cancel();
-    }
-    
-    public void put(K key, V value)
-    {        
-        cache_.put(key, new CacheableObject<V>(value));
-    }
-
-    public void put(K key, V value, ICacheExpungeHook<K,V> hook)
-    {
-        put(key, value);
-        hooks_.put(key, hook);
-    }
-
-    public V get(K key)
-    {
-        V result = null;
-        CacheableObject<V> co = cache_.get(key);
-        if ( co != null )
-        {
-            result = co.getValue();
-        }
-        return result;
-    }
-
-    public V remove(K key)
-    {
-        CacheableObject<V> co = cache_.remove(key);
-        V result = null;
-        if ( co != null )
-        {
-            result = co.getValue();
-        }
-        return result;
-    }
-
-    public int size()
-    {
-        return cache_.size();
-    }
-
-    public boolean containsKey(K key)
-    {
-        return cache_.containsKey(key);
-    }
-
-    public boolean containsValue(V value)
-    {
-        return cache_.containsValue( new CacheableObject<V>(value) );
-    }
-
-    public boolean isEmpty()
-    {
-        return cache_.isEmpty();
-    }
-
-    public Set<K> keySet()
-    {
-        return cache_.keySet();
-    }    
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.util.*;
+
+import org.apache.log4j.Logger;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class Cachetable<K,V> implements ICachetable<K,V>
+{
+    private class CacheableObject<V>
+    {
+        private V value_;
+        private long age_;
+        
+        CacheableObject(V o)
+        {
+            value_ = o;
+            age_ = System.currentTimeMillis();
+        }
+
+        public boolean equals(Object o)
+        {
+            return value_.equals(o);
+        }
+
+        public int hashCode()
+        {
+            return value_.hashCode();
+        }
+
+        V getValue()
+        {
+            return value_;
+        }           
+        
+        boolean isReadyToDie(long expiration)
+        {
+            return ( (System.currentTimeMillis() - age_) > expiration );
+        }
+    }
+    
+    private class CacheMonitor extends TimerTask
+    {
+        private long expiration_;
+        
+        CacheMonitor(long expiration)
+        {
+            expiration_ = expiration;
+        }
+        
+        public void run()
+        {  
+            Map<K,V> expungedValues = new HashMap<K,V>();            
+            synchronized(cache_)
+            {
+                Enumeration<K> e = cache_.keys();
+                while( e.hasMoreElements() )
+                {        
+                    K key = e.nextElement();
+                    CacheableObject<V> co = cache_.get(key);
+                    if ( co != null && co.isReadyToDie(expiration_) )
+                    {
+                        V v = co.getValue();
+                        if(null != v)
+                            expungedValues.put(key, v);
+                        cache_.remove(key);                                       
+                    }
+                }
+            }
+            
+            /* Calling the hooks on the keys that have been expunged */
+            Set<K> keys = expungedValues.keySet();                                               
+            for ( K key : keys )
+            {                                
+                V value = expungedValues.get(key);
+                ICacheExpungeHook<K,V> hook = hooks_.remove(key);
+                try 
+                {
+                    if ( hook != null )
+                    {
+                        hook.callMe(key, value);                    
+                    }
+                    else if ( globalHook_ != null )
+                    {
+                        globalHook_.callMe(key, value);
+                    }
+                }
+                catch(Throwable e)
+                {
+                    logger_.info(LogUtil.throwableToString(e));
+                }
+            }
+            expungedValues.clear();
+        }
+    }   
+
+    private ICacheExpungeHook<K,V> globalHook_;
+    private Hashtable<K, CacheableObject<V>> cache_;
+    private Map<K, ICacheExpungeHook<K,V>> hooks_;
+    private Timer timer_;
+    private static int counter_ = 0;
+    private static Logger logger_ = Logger.getLogger(Cachetable.class);
+
+    private void init(long expiration)
+    {
+        if ( expiration <= 0 )
+            throw new IllegalArgumentException("Argument specified must be a positive number");
+
+        cache_ = new Hashtable<K, CacheableObject<V>>();
+        hooks_ = new Hashtable<K, ICacheExpungeHook<K,V>>();
+        timer_ = new Timer("CACHETABLE-TIMER-" + (++counter_), true);        
+        timer_.schedule(new CacheMonitor(expiration), expiration, expiration);
+    }
+    
+    /*
+     * Specify the TTL for objects in the cache
+     * in milliseconds.
+     */
+    public Cachetable(long expiration)
+    {
+        init(expiration);   
+    }    
+    
+    /*
+     * Specify the TTL for objects in the cache
+     * in milliseconds and a global expunge hook. If
+     * a key has a key-specific hook installed invoke that
+     * instead.
+     */
+    public Cachetable(long expiration, ICacheExpungeHook<K,V> global)
+    {
+        init(expiration);
+        globalHook_ = global;        
+    }
+    
+    public void shutdown()
+    {
+        timer_.cancel();
+    }
+    
+    public void put(K key, V value)
+    {        
+        cache_.put(key, new CacheableObject<V>(value));
+    }
+
+    public void put(K key, V value, ICacheExpungeHook<K,V> hook)
+    {
+        put(key, value);
+        hooks_.put(key, hook);
+    }
+
+    public V get(K key)
+    {
+        V result = null;
+        CacheableObject<V> co = cache_.get(key);
+        if ( co != null )
+        {
+            result = co.getValue();
+        }
+        return result;
+    }
+
+    public V remove(K key)
+    {
+        CacheableObject<V> co = cache_.remove(key);
+        V result = null;
+        if ( co != null )
+        {
+            result = co.getValue();
+        }
+        return result;
+    }
+
+    public int size()
+    {
+        return cache_.size();
+    }
+
+    public boolean containsKey(K key)
+    {
+        return cache_.containsKey(key);
+    }
+
+    public boolean containsValue(V value)
+    {
+        return cache_.containsValue( new CacheableObject<V>(value) );
+    }
+
+    public boolean isEmpty()
+    {
+        return cache_.isEmpty();
+    }
+
+    public Set<K> keySet()
+    {
+        return cache_.keySet();
+    }    
+}
diff --git a/src/java/org/apache/cassandra/utils/FBUtilities.java b/src/java/org/apache/cassandra/utils/FBUtilities.java
index 0ab147110c..b70b8b1b49 100644
--- a/src/java/org/apache/cassandra/utils/FBUtilities.java
+++ b/src/java/org/apache/cassandra/utils/FBUtilities.java
@@ -1,391 +1,391 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-import java.io.*;
-import java.math.BigInteger;
-import java.net.InetAddress;
-import java.net.UnknownHostException;
-import java.security.MessageDigest;
-import java.text.DateFormat;
-import java.text.SimpleDateFormat;
-import java.util.ArrayList;
-import java.util.Date;
-import java.util.List;
-import java.util.StringTokenizer;
-import java.util.zip.DataFormatException;
-import java.util.zip.Deflater;
-import java.util.zip.Inflater;
-
-import org.apache.log4j.Logger;
-
-import org.apache.cassandra.config.DatabaseDescriptor;
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class FBUtilities
-{
-    private static Logger logger_ = Logger.getLogger(FBUtilities.class);
-
-    private static InetAddress localInetAddress_;
-
-    public static String getTimestamp()
-    {
-        Date date = new Date();
-        DateFormat df  = new SimpleDateFormat("MM/dd/yyyy HH:mm:ss");
-        return df.format(date);
-    }
-    
-    public static String getTimestamp(long value)
-    {
-        Date date = new Date(value);
-        DateFormat df  = new SimpleDateFormat("MM/dd/yyyy HH:mm:ss");
-        return df.format(date);
-    }
-
-    public static int getBits(int x, int p, int n)
-    {
-         return ( x >>> (p + 1 - n)) & ~(~0 << n);
-    }
-
-    public static String getCurrentThreadStackTrace()
-    {
-        Throwable throwable = new Throwable();
-        StackTraceElement[] ste = throwable.getStackTrace();
-        StringBuilder sbuf = new StringBuilder();
-
-        for ( int i = ste.length - 1; i > 0; --i )
-        {
-            sbuf.append(ste[i].getClassName())
-            	.append(".")
-            	.append(ste[i].getMethodName())
-            	.append("/");
-        }
-        sbuf.deleteCharAt(sbuf.length() - 1);
-        return sbuf.toString();
-    }
-
-    public static String[] strip(String string, String token)
-    {
-        StringTokenizer st = new StringTokenizer(string, token);
-        List<String> result = new ArrayList<String>();
-        while ( st.hasMoreTokens() )
-        {
-            result.add( (String)st.nextElement() );
-        }
-        return result.toArray( new String[0] );
-    }
-
-    public static byte[] serializeToStream(Object o)
-    {
-        ByteArrayOutputStream bos = new ByteArrayOutputStream();
-        byte[] bytes = new byte[0];
-        try
-        {
-    		ObjectOutputStream oos = new ObjectOutputStream(bos);
-            oos.writeObject(o);
-            oos.flush();
-    		bytes = bos.toByteArray();
-    		oos.close();
-    		bos.close();
-        }
-        catch ( IOException e )
-        {
-            LogUtil.getLogger(FBUtilities.class.getName()).info( LogUtil.throwableToString(e) );
-        }
-		return bytes;
-    }
-
-    public static Object deserializeFromStream(byte[] bytes)
-    {
-        Object o = null;
-		ByteArrayInputStream bis = new ByteArrayInputStream(bytes);
-        try
-        {
-    		ObjectInputStream ois = new ObjectInputStream(bis);
-            try
-            {
-    		    o = ois.readObject();
-            }
-            catch ( ClassNotFoundException e )
-            {
-            }
-    		ois.close();
-    		bis.close();
-        }
-        catch ( IOException e )
-        {
-            LogUtil.getLogger(FBUtilities.class.getName()).info( LogUtil.throwableToString(e) );
-        }
-		return o;
-    }
-
-    public static InetAddress getLocalAddress() throws UnknownHostException
-    {
-	if ( localInetAddress_ == null )
-		localInetAddress_ = InetAddress.getLocalHost();
-        return localInetAddress_;
-    }
-
-    public static String getHostAddress() throws UnknownHostException
-    {
-        InetAddress inetAddr = getLocalAddress();
-        if (DatabaseDescriptor.getListenAddress() != null)
-        {
-            inetAddr = InetAddress.getByName(DatabaseDescriptor.getListenAddress());
-        }
-        return inetAddr.getHostAddress();
-    }
-
-    public static boolean isHostLocalHost(InetAddress host)
-    {
-        try {
-            return getLocalAddress().equals(host);
-        }
-        catch ( UnknownHostException e )
-        {
-            return false;
-        }
-    }
-
-    public static byte[] toByteArray(int i)
-    {
-        byte[] bytes = new byte[4];
-        bytes[0] = (byte)( ( i >>> 24 ) & 0xFF);
-        bytes[1] = (byte)( ( i >>> 16 ) & 0xFF);
-        bytes[2] = (byte)( ( i >>> 8 ) & 0xFF);
-        bytes[3] = (byte)( i & 0xFF );
-        return bytes;
-    }
-
-    public static int byteArrayToInt(byte[] bytes)
-    {
-    	return byteArrayToInt(bytes, 0);
-    }
-
-    public static int byteArrayToInt(byte[] bytes, int offset)
-    {
-        if ( bytes.length - offset < 4 )
-        {
-            throw new IllegalArgumentException("An integer must be 4 bytes in size.");
-        }
-        int n = 0;
-        for ( int i = 0; i < 4; ++i )
-        {
-            n <<= 8;
-            n |= bytes[offset + i] & 0xFF;
-        }
-        return n;
-    }
-
-    public static boolean isEqualBits(byte[] bytes1, byte[] bytes2)
-    {
-        return 0 == compareByteArrays(bytes1, bytes2);
-    }
-
-    public static int compareByteArrays(byte[] bytes1, byte[] bytes2){
-        if(null == bytes1){
-            if(null == bytes2) return 0;
-            else return -1;
-        }
-        if(null == bytes2) return 1;
-
-        for(int i = 0; i < bytes1.length && i < bytes2.length; i++){
-            int cmp = compareBytes(bytes1[i], bytes2[i]);
-            if(0 != cmp) return cmp;
-        }
-        if(bytes1.length == bytes2.length) return 0;
-        else return (bytes1.length < bytes2.length)? -1 : 1;
-    }
-
-    public static int compareBytes(byte b1, byte b2){
-        return compareBytes((int)b1, (int)b2);
-    }
-
-    public static int compareBytes(int b1, int b2){
-        int i1 = b1 & 0xFF;
-        int i2 = b2 & 0xFF;
-        if(i1 < i2) return -1;
-        else if(i1 == i2) return 0;
-        else return 1;
-    }
-
-    public static String stackTrace(Throwable e)
-    {
-        StringWriter sw = new StringWriter();
-        PrintWriter pw = new PrintWriter( sw );
-        e.printStackTrace(pw);
-        return sw.toString();
-    }
-
-    public static BigInteger hash(String data)
-    {
-        byte[] result = hash(HashingSchemes.MD5, data.getBytes());
-        BigInteger hash = new BigInteger(result);
-        return hash.abs();        
-    }
-
-    public static byte[] hash(String type, byte[] data)
-    {
-    	byte[] result = null;
-    	try
-        {
-    		MessageDigest messageDigest = MessageDigest.getInstance(type);
-    		result = messageDigest.digest(data);
-    	}
-    	catch (Exception e)
-        {
-    		if (logger_.isDebugEnabled())
-                logger_.debug(LogUtil.throwableToString(e));
-    	}
-    	return result;
-	}
-
-    public static boolean isEqual(byte[] digestA, byte[] digestB)
-    {
-        return MessageDigest.isEqual(digestA, digestB);
-    }
-
-    // The given byte array is compressed onto the specified stream.
-    // The method does not close the stream. The caller will have to do it.
-    public static void compressToStream(byte[] input, ByteArrayOutputStream bos) throws IOException
-    {
-    	 // Create the compressor with highest level of compression
-        Deflater compressor = new Deflater();
-        compressor.setLevel(Deflater.BEST_COMPRESSION);
-
-        // Give the compressor the data to compress
-        compressor.setInput(input);
-        compressor.finish();
-
-        // Write the compressed data to the stream
-        byte[] buf = new byte[1024];
-        while (!compressor.finished())
-        {
-            int count = compressor.deflate(buf);
-            bos.write(buf, 0, count);
-        }
-    }
-
-
-    public static byte[] compress(byte[] input) throws IOException
-    {
-        // Create an expandable byte array to hold the compressed data.
-        // You cannot use an array that's the same size as the orginal because
-        // there is no guarantee that the compressed data will be smaller than
-        // the uncompressed data.
-        ByteArrayOutputStream bos = new ByteArrayOutputStream(input.length);
-        compressToStream(input,bos);
-        bos.close();
-
-        // Get the compressed data
-        return bos.toByteArray();
-    }
-
-
-    public static byte[] decompress(byte[] compressedData, int off, int len) throws IOException, DataFormatException
-    {
-    	 // Create the decompressor and give it the data to compress
-        Inflater decompressor = new Inflater();
-        decompressor.setInput(compressedData, off, len);
-
-        // Create an expandable byte array to hold the decompressed data
-        ByteArrayOutputStream bos = new ByteArrayOutputStream(compressedData.length);
-
-        // Decompress the data
-        byte[] buf = new byte[1024];
-        while (!decompressor.finished())
-        {
-            int count = decompressor.inflate(buf);
-            bos.write(buf, 0, count);
-        }
-        bos.close();
-
-        // Get the decompressed data
-        return bos.toByteArray();
-    }
-
-    public static byte[] decompress(byte[] compressedData) throws IOException, DataFormatException
-    {
-    	return decompress(compressedData, 0, compressedData.length);
-    }
-
-     public static byte[] xor(byte[] b1, byte[] b2)
-     {
-         assert b1 != null;
-         assert b2 != null;
-    	 byte[] bLess;
-    	 byte[] bMore;
-
-    	 if(b1.length > b2.length)
-    	 {
-    		 bLess = b2;
-    		 bMore = b1;
-    	 }
-    	 else
-    	 {
-    		 bLess = b1;
-    		 bMore = b2;
-    	 }
-
-    	 for(int i = 0 ; i< bLess.length; i++ )
-    	 {
-    		 bMore[i] = (byte)(bMore[i] ^ bLess[i]);
-    	 }
-
-    	 return bMore;
-     }
-
-     public static int getUTF8Length(String string)
-     {
-     	/*
-     	 * We store the string as UTF-8 encoded, so when we calculate the length, it
-     	 * should be converted to UTF-8.
-     	 */
-     	String utfName  = string;
-     	int length = utfName.length();
-     	try
-     	{
-     		//utfName  = new String(string.getBytes("UTF-8"));
-     		length = string.getBytes("UTF-8").length;
-     	}
-     	catch (UnsupportedEncodingException e)
-     	{
-     		LogUtil.getLogger(FBUtilities.class.getName()).info(LogUtil.throwableToString(e));
-     	}
-
-     	return length;
-     }
-
-    public static void writeByteArray(byte[] bytes, DataOutput out) throws IOException
-    {
-        out.writeInt(bytes.length);
-        out.write(bytes);
-    }
-
-    public static byte[] readByteArray(DataInput in) throws IOException
-    {
-        int length = in.readInt();
-        byte[] bytes = new byte[length];
-        in.readFully(bytes);
-        return bytes;
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.*;
+import java.math.BigInteger;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.security.MessageDigest;
+import java.text.DateFormat;
+import java.text.SimpleDateFormat;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.List;
+import java.util.StringTokenizer;
+import java.util.zip.DataFormatException;
+import java.util.zip.Deflater;
+import java.util.zip.Inflater;
+
+import org.apache.log4j.Logger;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class FBUtilities
+{
+    private static Logger logger_ = Logger.getLogger(FBUtilities.class);
+
+    private static InetAddress localInetAddress_;
+
+    public static String getTimestamp()
+    {
+        Date date = new Date();
+        DateFormat df  = new SimpleDateFormat("MM/dd/yyyy HH:mm:ss");
+        return df.format(date);
+    }
+    
+    public static String getTimestamp(long value)
+    {
+        Date date = new Date(value);
+        DateFormat df  = new SimpleDateFormat("MM/dd/yyyy HH:mm:ss");
+        return df.format(date);
+    }
+
+    public static int getBits(int x, int p, int n)
+    {
+         return ( x >>> (p + 1 - n)) & ~(~0 << n);
+    }
+
+    public static String getCurrentThreadStackTrace()
+    {
+        Throwable throwable = new Throwable();
+        StackTraceElement[] ste = throwable.getStackTrace();
+        StringBuilder sbuf = new StringBuilder();
+
+        for ( int i = ste.length - 1; i > 0; --i )
+        {
+            sbuf.append(ste[i].getClassName())
+            	.append(".")
+            	.append(ste[i].getMethodName())
+            	.append("/");
+        }
+        sbuf.deleteCharAt(sbuf.length() - 1);
+        return sbuf.toString();
+    }
+
+    public static String[] strip(String string, String token)
+    {
+        StringTokenizer st = new StringTokenizer(string, token);
+        List<String> result = new ArrayList<String>();
+        while ( st.hasMoreTokens() )
+        {
+            result.add( (String)st.nextElement() );
+        }
+        return result.toArray( new String[0] );
+    }
+
+    public static byte[] serializeToStream(Object o)
+    {
+        ByteArrayOutputStream bos = new ByteArrayOutputStream();
+        byte[] bytes = new byte[0];
+        try
+        {
+    		ObjectOutputStream oos = new ObjectOutputStream(bos);
+            oos.writeObject(o);
+            oos.flush();
+    		bytes = bos.toByteArray();
+    		oos.close();
+    		bos.close();
+        }
+        catch ( IOException e )
+        {
+            LogUtil.getLogger(FBUtilities.class.getName()).info( LogUtil.throwableToString(e) );
+        }
+		return bytes;
+    }
+
+    public static Object deserializeFromStream(byte[] bytes)
+    {
+        Object o = null;
+		ByteArrayInputStream bis = new ByteArrayInputStream(bytes);
+        try
+        {
+    		ObjectInputStream ois = new ObjectInputStream(bis);
+            try
+            {
+    		    o = ois.readObject();
+            }
+            catch ( ClassNotFoundException e )
+            {
+            }
+    		ois.close();
+    		bis.close();
+        }
+        catch ( IOException e )
+        {
+            LogUtil.getLogger(FBUtilities.class.getName()).info( LogUtil.throwableToString(e) );
+        }
+		return o;
+    }
+
+    public static InetAddress getLocalAddress() throws UnknownHostException
+    {
+	if ( localInetAddress_ == null )
+		localInetAddress_ = InetAddress.getLocalHost();
+        return localInetAddress_;
+    }
+
+    public static String getHostAddress() throws UnknownHostException
+    {
+        InetAddress inetAddr = getLocalAddress();
+        if (DatabaseDescriptor.getListenAddress() != null)
+        {
+            inetAddr = InetAddress.getByName(DatabaseDescriptor.getListenAddress());
+        }
+        return inetAddr.getHostAddress();
+    }
+
+    public static boolean isHostLocalHost(InetAddress host)
+    {
+        try {
+            return getLocalAddress().equals(host);
+        }
+        catch ( UnknownHostException e )
+        {
+            return false;
+        }
+    }
+
+    public static byte[] toByteArray(int i)
+    {
+        byte[] bytes = new byte[4];
+        bytes[0] = (byte)( ( i >>> 24 ) & 0xFF);
+        bytes[1] = (byte)( ( i >>> 16 ) & 0xFF);
+        bytes[2] = (byte)( ( i >>> 8 ) & 0xFF);
+        bytes[3] = (byte)( i & 0xFF );
+        return bytes;
+    }
+
+    public static int byteArrayToInt(byte[] bytes)
+    {
+    	return byteArrayToInt(bytes, 0);
+    }
+
+    public static int byteArrayToInt(byte[] bytes, int offset)
+    {
+        if ( bytes.length - offset < 4 )
+        {
+            throw new IllegalArgumentException("An integer must be 4 bytes in size.");
+        }
+        int n = 0;
+        for ( int i = 0; i < 4; ++i )
+        {
+            n <<= 8;
+            n |= bytes[offset + i] & 0xFF;
+        }
+        return n;
+    }
+
+    public static boolean isEqualBits(byte[] bytes1, byte[] bytes2)
+    {
+        return 0 == compareByteArrays(bytes1, bytes2);
+    }
+
+    public static int compareByteArrays(byte[] bytes1, byte[] bytes2){
+        if(null == bytes1){
+            if(null == bytes2) return 0;
+            else return -1;
+        }
+        if(null == bytes2) return 1;
+
+        for(int i = 0; i < bytes1.length && i < bytes2.length; i++){
+            int cmp = compareBytes(bytes1[i], bytes2[i]);
+            if(0 != cmp) return cmp;
+        }
+        if(bytes1.length == bytes2.length) return 0;
+        else return (bytes1.length < bytes2.length)? -1 : 1;
+    }
+
+    public static int compareBytes(byte b1, byte b2){
+        return compareBytes((int)b1, (int)b2);
+    }
+
+    public static int compareBytes(int b1, int b2){
+        int i1 = b1 & 0xFF;
+        int i2 = b2 & 0xFF;
+        if(i1 < i2) return -1;
+        else if(i1 == i2) return 0;
+        else return 1;
+    }
+
+    public static String stackTrace(Throwable e)
+    {
+        StringWriter sw = new StringWriter();
+        PrintWriter pw = new PrintWriter( sw );
+        e.printStackTrace(pw);
+        return sw.toString();
+    }
+
+    public static BigInteger hash(String data)
+    {
+        byte[] result = hash(HashingSchemes.MD5, data.getBytes());
+        BigInteger hash = new BigInteger(result);
+        return hash.abs();        
+    }
+
+    public static byte[] hash(String type, byte[] data)
+    {
+    	byte[] result = null;
+    	try
+        {
+    		MessageDigest messageDigest = MessageDigest.getInstance(type);
+    		result = messageDigest.digest(data);
+    	}
+    	catch (Exception e)
+        {
+    		if (logger_.isDebugEnabled())
+                logger_.debug(LogUtil.throwableToString(e));
+    	}
+    	return result;
+	}
+
+    public static boolean isEqual(byte[] digestA, byte[] digestB)
+    {
+        return MessageDigest.isEqual(digestA, digestB);
+    }
+
+    // The given byte array is compressed onto the specified stream.
+    // The method does not close the stream. The caller will have to do it.
+    public static void compressToStream(byte[] input, ByteArrayOutputStream bos) throws IOException
+    {
+    	 // Create the compressor with highest level of compression
+        Deflater compressor = new Deflater();
+        compressor.setLevel(Deflater.BEST_COMPRESSION);
+
+        // Give the compressor the data to compress
+        compressor.setInput(input);
+        compressor.finish();
+
+        // Write the compressed data to the stream
+        byte[] buf = new byte[1024];
+        while (!compressor.finished())
+        {
+            int count = compressor.deflate(buf);
+            bos.write(buf, 0, count);
+        }
+    }
+
+
+    public static byte[] compress(byte[] input) throws IOException
+    {
+        // Create an expandable byte array to hold the compressed data.
+        // You cannot use an array that's the same size as the orginal because
+        // there is no guarantee that the compressed data will be smaller than
+        // the uncompressed data.
+        ByteArrayOutputStream bos = new ByteArrayOutputStream(input.length);
+        compressToStream(input,bos);
+        bos.close();
+
+        // Get the compressed data
+        return bos.toByteArray();
+    }
+
+
+    public static byte[] decompress(byte[] compressedData, int off, int len) throws IOException, DataFormatException
+    {
+    	 // Create the decompressor and give it the data to compress
+        Inflater decompressor = new Inflater();
+        decompressor.setInput(compressedData, off, len);
+
+        // Create an expandable byte array to hold the decompressed data
+        ByteArrayOutputStream bos = new ByteArrayOutputStream(compressedData.length);
+
+        // Decompress the data
+        byte[] buf = new byte[1024];
+        while (!decompressor.finished())
+        {
+            int count = decompressor.inflate(buf);
+            bos.write(buf, 0, count);
+        }
+        bos.close();
+
+        // Get the decompressed data
+        return bos.toByteArray();
+    }
+
+    public static byte[] decompress(byte[] compressedData) throws IOException, DataFormatException
+    {
+    	return decompress(compressedData, 0, compressedData.length);
+    }
+
+     public static byte[] xor(byte[] b1, byte[] b2)
+     {
+         assert b1 != null;
+         assert b2 != null;
+    	 byte[] bLess;
+    	 byte[] bMore;
+
+    	 if(b1.length > b2.length)
+    	 {
+    		 bLess = b2;
+    		 bMore = b1;
+    	 }
+    	 else
+    	 {
+    		 bLess = b1;
+    		 bMore = b2;
+    	 }
+
+    	 for(int i = 0 ; i< bLess.length; i++ )
+    	 {
+    		 bMore[i] = (byte)(bMore[i] ^ bLess[i]);
+    	 }
+
+    	 return bMore;
+     }
+
+     public static int getUTF8Length(String string)
+     {
+     	/*
+     	 * We store the string as UTF-8 encoded, so when we calculate the length, it
+     	 * should be converted to UTF-8.
+     	 */
+     	String utfName  = string;
+     	int length = utfName.length();
+     	try
+     	{
+     		//utfName  = new String(string.getBytes("UTF-8"));
+     		length = string.getBytes("UTF-8").length;
+     	}
+     	catch (UnsupportedEncodingException e)
+     	{
+     		LogUtil.getLogger(FBUtilities.class.getName()).info(LogUtil.throwableToString(e));
+     	}
+
+     	return length;
+     }
+
+    public static void writeByteArray(byte[] bytes, DataOutput out) throws IOException
+    {
+        out.writeInt(bytes.length);
+        out.write(bytes);
+    }
+
+    public static byte[] readByteArray(DataInput in) throws IOException
+    {
+        int length = in.readInt();
+        byte[] bytes = new byte[length];
+        in.readFully(bytes);
+        return bytes;
+    }
+}
diff --git a/src/java/org/apache/cassandra/utils/FastLinkedHashMap.java b/src/java/org/apache/cassandra/utils/FastLinkedHashMap.java
index 8af3612e75..1b11081f91 100644
--- a/src/java/org/apache/cassandra/utils/FastLinkedHashMap.java
+++ b/src/java/org/apache/cassandra/utils/FastLinkedHashMap.java
@@ -1,24 +1,24 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-public class FastLinkedHashMap<K,V> extends FastHashMap<K,V>
-{
-    
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+public class FastLinkedHashMap<K,V> extends FastHashMap<K,V>
+{
+    
+}
diff --git a/src/java/org/apache/cassandra/utils/FileUtils.java b/src/java/org/apache/cassandra/utils/FileUtils.java
index 41568d195b..a29f2a85ec 100644
--- a/src/java/org/apache/cassandra/utils/FileUtils.java
+++ b/src/java/org/apache/cassandra/utils/FileUtils.java
@@ -1,306 +1,306 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-import java.io.*;
-import java.text.DecimalFormat;
-import java.util.*;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.TimeUnit;
-
-import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
-import org.apache.cassandra.concurrent.ThreadFactoryImpl;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.log4j.Logger;
-
-
-
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class FileUtils
-{
-    private static Logger logger_ = Logger.getLogger(FileUtils.class);
-    private static final DecimalFormat df_ = new DecimalFormat("#.##");
-    private static final double kb_ = 1024d;
-    private static final double mb_ = 1024*1024d;
-    private static final double gb_ = 1024*1024*1024d;
-    private static final double tb_ = 1024*1024*1024*1024d;
-
-    private static ExecutorService deleter_ = new DebuggableThreadPoolExecutor("FILEUTILS-DELETE-POOL");
-
-    public static void shutdown()
-    {
-    	deleter_.shutdownNow();
-    }
-
-    public static void deleteWithConfirm(File file) throws IOException
-    {
-        assert file.exists() : "attempted to delete non-existing file " + file.getName();
-        if (!file.delete())
-        {
-            throw new IOException("Failed to delete " + file.getName());
-        }
-    }
-
-    public static class Deleter implements Runnable
-    {
-    	File file_ = null;
-
-    	public Deleter(File f)
-        {
-    		file_ = f;
-        }
-
-        public void run()
-        {
-        	if(file_ == null)
-        		return;
-        	logger_.info("*** Deleting " + file_.getName() + " ***");
-        	if(!file_.delete())
-        	{
-            	logger_.warn("Warning : Unable to delete file " + file_.getAbsolutePath());
-        	}
-        }
-    }
-
-    public static class FileComparator implements Comparator<File>
-    {
-        public int compare(File f, File f2)
-        {
-            return (int)(f.lastModified() - f2.lastModified());
-        }
-
-        public boolean equals(Object o)
-        {
-            if ( !(o instanceof FileComparator) )
-                return false;
-            return true;
-        }
-    }
-
-    public static void createDirectory(String directory) throws IOException
-    {
-        File file = new File(directory);
-        if (!file.exists())
-        {
-            if (!file.mkdirs())
-            {
-                throw new IOException("unable to mkdirs " + directory);
-            }
-        }
-    }
-
-    public static void createFile(String directory) throws IOException
-    {
-        File file = new File(directory);
-        if ( !file.exists() )
-            file.createNewFile();
-    }
-
-    public static boolean isExists(String filename) throws IOException
-    {
-        File file = new File(filename);
-        return file.exists();
-    }
-
-    public static boolean delete(String file)
-    {
-        File f = new File(file);
-        return f.delete();
-    }
-
-    public static void deleteAsync(String file) throws IOException
-    {
-        File f = new File(file);
-    	Runnable deleter = new Deleter(f);
-        deleter_.submit(deleter);
-    }
-
-    public static boolean delete(List<String> files) throws IOException
-    {
-        boolean bVal = true;
-        for ( int i = 0; i < files.size(); ++i )
-        {
-            String file = files.get(i);
-            bVal = delete(file);
-            if (bVal)
-            {
-            	if (logger_.isDebugEnabled())
-            	  logger_.debug("Deleted file " + file);
-                files.remove(i);
-            }
-        }
-        return bVal;
-    }
-
-    public static void delete(File[] files) throws IOException
-    {
-        for ( File file : files )
-        {
-            file.delete();
-        }
-    }
-
-    public static String stringifyFileSize(double value)
-    {
-        double d = 0d;
-        if ( value >= tb_ )
-        {
-            d = value / tb_;
-            String val = df_.format(d);
-            return val + " TB";
-        }
-        else if ( value >= gb_ )
-        {
-            d = value / gb_;
-            String val = df_.format(d);
-            return val + " GB";
-        }
-        else if ( value >= mb_ )
-        {
-            d = value / mb_;
-            String val = df_.format(d);
-            return val + " MB";
-        }
-        else if ( value >= kb_ )
-        {
-            d = value / kb_;
-            String val = df_.format(d);
-            return val + " KB";
-        }
-        else
-        {       
-            String val = df_.format(value);
-            return val + " bytes.";
-        }        
-    }
-    
-    public static double stringToFileSize(String value)
-    {        
-        String[] peices = value.split(" ");
-        double d = Double.valueOf(peices[0]);
-        if ( peices[1].equals("TB") )
-        {
-            d *= tb_;
-        }
-        else if ( peices[1].equals("GB") )
-        {
-            d *= gb_;
-        }
-        else if ( peices[1].equals("MB") )
-        {
-            d *= mb_;
-        }
-        else if ( peices[1].equals("KB") )
-        {
-            d *= kb_;
-        }
-        else
-        {
-            d *= 1;
-        }
-        return d;
-    }
-    
-    public static long getUsedDiskSpace()
-    {
-        long diskSpace = 0L;
-        String[] directories = DatabaseDescriptor.getAllDataFileLocations();        
-        for ( String directory : directories )
-        {
-            File f = new File(directory);
-            File[] files = f.listFiles();
-            for ( File file : files )
-            {
-                diskSpace += file.length();
-            }
-        }
-
-        String value = df_.format(diskSpace);
-        return Long.parseLong(value);
-    }    
-    
-    
-	
-    /**
-     * Deletes all files and subdirectories under "dir".
-     * @param dir Directory to be deleted
-     * @return boolean Returns "true" if all deletions were successful.
-     *                 If a deletion fails, the method stops attempting to
-     *                 delete and returns "false".
-     */
-    public static boolean deleteDir(File dir) {
-
-        if (dir.isDirectory()) {
-            String[] children = dir.list();
-            for (int i=0; i<children.length; i++) {
-                boolean success = deleteDir(new File(dir, children[i]));
-                if (!success) {
-                    return false;
-                }
-            }
-        }
-
-        // The directory is now empty so now it can be smoked
-        return dir.delete();
-    }
-
-    /**
-     * Create a hard link for a given file.
-     * 
-     * @param sourceFile      The name of the source file.
-     * @param destinationFile The name of the destination file.
-     * 
-     * @throws IOException if an error has occurred while creating the link.
-     */
-    public static void createHardLink(File sourceFile, File destinationFile) throws IOException
-    {
-        String osname = System.getProperty("os.name");
-        ProcessBuilder pb;
-        if (osname.startsWith("Windows"))
-        {
-            float osversion = Float.parseFloat(System.getProperty("os.version"));
-            if (osversion >= 6.0f)
-            {
-                pb = new ProcessBuilder("cmd", "/c", "mklink", "/H", destinationFile.getAbsolutePath(), sourceFile.getAbsolutePath());
-            }
-            else
-            {
-                pb = new ProcessBuilder("fsutil", "hardlink", "create", destinationFile.getAbsolutePath(), sourceFile.getAbsolutePath());
-            }
-        }
-        else
-        {
-            pb = new ProcessBuilder("ln", sourceFile.getAbsolutePath(), destinationFile.getAbsolutePath());
-            pb.redirectErrorStream(true);
-        }
-        Process p = pb.start();
-        try
-        {
-            p.waitFor();
-        }
-        catch (InterruptedException e)
-        {
-            throw new RuntimeException(e);
-        }
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.*;
+import java.text.DecimalFormat;
+import java.util.*;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.LinkedBlockingQueue;
+import java.util.concurrent.TimeUnit;
+
+import org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor;
+import org.apache.cassandra.concurrent.ThreadFactoryImpl;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.log4j.Logger;
+
+
+
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class FileUtils
+{
+    private static Logger logger_ = Logger.getLogger(FileUtils.class);
+    private static final DecimalFormat df_ = new DecimalFormat("#.##");
+    private static final double kb_ = 1024d;
+    private static final double mb_ = 1024*1024d;
+    private static final double gb_ = 1024*1024*1024d;
+    private static final double tb_ = 1024*1024*1024*1024d;
+
+    private static ExecutorService deleter_ = new DebuggableThreadPoolExecutor("FILEUTILS-DELETE-POOL");
+
+    public static void shutdown()
+    {
+    	deleter_.shutdownNow();
+    }
+
+    public static void deleteWithConfirm(File file) throws IOException
+    {
+        assert file.exists() : "attempted to delete non-existing file " + file.getName();
+        if (!file.delete())
+        {
+            throw new IOException("Failed to delete " + file.getName());
+        }
+    }
+
+    public static class Deleter implements Runnable
+    {
+    	File file_ = null;
+
+    	public Deleter(File f)
+        {
+    		file_ = f;
+        }
+
+        public void run()
+        {
+        	if(file_ == null)
+        		return;
+        	logger_.info("*** Deleting " + file_.getName() + " ***");
+        	if(!file_.delete())
+        	{
+            	logger_.warn("Warning : Unable to delete file " + file_.getAbsolutePath());
+        	}
+        }
+    }
+
+    public static class FileComparator implements Comparator<File>
+    {
+        public int compare(File f, File f2)
+        {
+            return (int)(f.lastModified() - f2.lastModified());
+        }
+
+        public boolean equals(Object o)
+        {
+            if ( !(o instanceof FileComparator) )
+                return false;
+            return true;
+        }
+    }
+
+    public static void createDirectory(String directory) throws IOException
+    {
+        File file = new File(directory);
+        if (!file.exists())
+        {
+            if (!file.mkdirs())
+            {
+                throw new IOException("unable to mkdirs " + directory);
+            }
+        }
+    }
+
+    public static void createFile(String directory) throws IOException
+    {
+        File file = new File(directory);
+        if ( !file.exists() )
+            file.createNewFile();
+    }
+
+    public static boolean isExists(String filename) throws IOException
+    {
+        File file = new File(filename);
+        return file.exists();
+    }
+
+    public static boolean delete(String file)
+    {
+        File f = new File(file);
+        return f.delete();
+    }
+
+    public static void deleteAsync(String file) throws IOException
+    {
+        File f = new File(file);
+    	Runnable deleter = new Deleter(f);
+        deleter_.submit(deleter);
+    }
+
+    public static boolean delete(List<String> files) throws IOException
+    {
+        boolean bVal = true;
+        for ( int i = 0; i < files.size(); ++i )
+        {
+            String file = files.get(i);
+            bVal = delete(file);
+            if (bVal)
+            {
+            	if (logger_.isDebugEnabled())
+            	  logger_.debug("Deleted file " + file);
+                files.remove(i);
+            }
+        }
+        return bVal;
+    }
+
+    public static void delete(File[] files) throws IOException
+    {
+        for ( File file : files )
+        {
+            file.delete();
+        }
+    }
+
+    public static String stringifyFileSize(double value)
+    {
+        double d = 0d;
+        if ( value >= tb_ )
+        {
+            d = value / tb_;
+            String val = df_.format(d);
+            return val + " TB";
+        }
+        else if ( value >= gb_ )
+        {
+            d = value / gb_;
+            String val = df_.format(d);
+            return val + " GB";
+        }
+        else if ( value >= mb_ )
+        {
+            d = value / mb_;
+            String val = df_.format(d);
+            return val + " MB";
+        }
+        else if ( value >= kb_ )
+        {
+            d = value / kb_;
+            String val = df_.format(d);
+            return val + " KB";
+        }
+        else
+        {       
+            String val = df_.format(value);
+            return val + " bytes.";
+        }        
+    }
+    
+    public static double stringToFileSize(String value)
+    {        
+        String[] peices = value.split(" ");
+        double d = Double.valueOf(peices[0]);
+        if ( peices[1].equals("TB") )
+        {
+            d *= tb_;
+        }
+        else if ( peices[1].equals("GB") )
+        {
+            d *= gb_;
+        }
+        else if ( peices[1].equals("MB") )
+        {
+            d *= mb_;
+        }
+        else if ( peices[1].equals("KB") )
+        {
+            d *= kb_;
+        }
+        else
+        {
+            d *= 1;
+        }
+        return d;
+    }
+    
+    public static long getUsedDiskSpace()
+    {
+        long diskSpace = 0L;
+        String[] directories = DatabaseDescriptor.getAllDataFileLocations();        
+        for ( String directory : directories )
+        {
+            File f = new File(directory);
+            File[] files = f.listFiles();
+            for ( File file : files )
+            {
+                diskSpace += file.length();
+            }
+        }
+
+        String value = df_.format(diskSpace);
+        return Long.parseLong(value);
+    }    
+    
+    
+	
+    /**
+     * Deletes all files and subdirectories under "dir".
+     * @param dir Directory to be deleted
+     * @return boolean Returns "true" if all deletions were successful.
+     *                 If a deletion fails, the method stops attempting to
+     *                 delete and returns "false".
+     */
+    public static boolean deleteDir(File dir) {
+
+        if (dir.isDirectory()) {
+            String[] children = dir.list();
+            for (int i=0; i<children.length; i++) {
+                boolean success = deleteDir(new File(dir, children[i]));
+                if (!success) {
+                    return false;
+                }
+            }
+        }
+
+        // The directory is now empty so now it can be smoked
+        return dir.delete();
+    }
+
+    /**
+     * Create a hard link for a given file.
+     * 
+     * @param sourceFile      The name of the source file.
+     * @param destinationFile The name of the destination file.
+     * 
+     * @throws IOException if an error has occurred while creating the link.
+     */
+    public static void createHardLink(File sourceFile, File destinationFile) throws IOException
+    {
+        String osname = System.getProperty("os.name");
+        ProcessBuilder pb;
+        if (osname.startsWith("Windows"))
+        {
+            float osversion = Float.parseFloat(System.getProperty("os.version"));
+            if (osversion >= 6.0f)
+            {
+                pb = new ProcessBuilder("cmd", "/c", "mklink", "/H", destinationFile.getAbsolutePath(), sourceFile.getAbsolutePath());
+            }
+            else
+            {
+                pb = new ProcessBuilder("fsutil", "hardlink", "create", destinationFile.getAbsolutePath(), sourceFile.getAbsolutePath());
+            }
+        }
+        else
+        {
+            pb = new ProcessBuilder("ln", sourceFile.getAbsolutePath(), destinationFile.getAbsolutePath());
+            pb.redirectErrorStream(true);
+        }
+        Process p = pb.start();
+        try
+        {
+            p.waitFor();
+        }
+        catch (InterruptedException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/utils/GuidGenerator.java b/src/java/org/apache/cassandra/utils/GuidGenerator.java
index a243821f4b..18251b4545 100644
--- a/src/java/org/apache/cassandra/utils/GuidGenerator.java
+++ b/src/java/org/apache/cassandra/utils/GuidGenerator.java
@@ -1,129 +1,129 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-import org.apache.log4j.Logger;
-
-import java.util.*;
-import java.net.*;
-import java.security.*;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class GuidGenerator {
-    private static Logger logger_ = Logger.getLogger(GuidGenerator.class);
-    private static Random myRand;
-    private static SecureRandom mySecureRand;
-    private static String s_id;
-    private static SafeMessageDigest md5 = null;
-
-    static {
-        if (System.getProperty("java.security.egd") == null) {
-            System.setProperty("java.security.egd", "file:/dev/urandom");
-        }
-        mySecureRand = new SecureRandom();
-        long secureInitializer = mySecureRand.nextLong();
-        myRand = new Random(secureInitializer);
-        try {
-            s_id = InetAddress.getLocalHost().toString();
-        }
-        catch (UnknownHostException e) {
-            if (logger_.isDebugEnabled())
-                logger_.debug(LogUtil.throwableToString(e));
-        }
-
-        try {
-            MessageDigest myMd5 = MessageDigest.getInstance("MD5");
-            md5 = new SafeMessageDigest(myMd5);
-        }
-        catch (NoSuchAlgorithmException e) {
-            if (logger_.isDebugEnabled())
-                logger_.debug(LogUtil.throwableToString(e));
-        }
-    }
-
-
-    public static String guid() {
-        byte[] array = guidAsBytes();
-        
-        StringBuilder sb = new StringBuilder();
-        for (int j = 0; j < array.length; ++j) {
-            int b = array[j] & 0xFF;
-            if (b < 0x10) sb.append('0');
-            sb.append(Integer.toHexString(b));
-        }
-
-        return convertToStandardFormat( sb.toString() );
-    }
-    
-    public static String guidToString(byte[] bytes)
-    {
-        StringBuilder sb = new StringBuilder();
-        for (int j = 0; j < bytes.length; ++j) {
-            int b = bytes[j] & 0xFF;
-            if (b < 0x10) sb.append('0');
-            sb.append(Integer.toHexString(b));
-        }
-
-        return convertToStandardFormat( sb.toString() );
-    }
-    
-    public static byte[] guidAsBytes()
-    {
-        StringBuilder sbValueBeforeMD5 = new StringBuilder();
-        long time = System.currentTimeMillis();
-        long rand = 0;
-        rand = myRand.nextLong();
-        sbValueBeforeMD5.append(s_id)
-        				.append(":")
-        				.append(Long.toString(time))
-        				.append(":")
-        				.append(Long.toString(rand));
-
-        String valueBeforeMD5 = sbValueBeforeMD5.toString();
-        return md5.digest(valueBeforeMD5.getBytes());
-    }
-
-    /*
-        * Convert to the standard format for GUID
-        * Example: C2FEEEAC-CFCD-11D1-8B05-00600806D9B6
-    */
-
-    private static String convertToStandardFormat(String valueAfterMD5) {
-        String raw = valueAfterMD5.toUpperCase();
-        StringBuilder sb = new StringBuilder();
-        sb.append(raw.substring(0, 8))
-          .append("-")
-          .append(raw.substring(8, 12))
-          .append("-")
-          .append(raw.substring(12, 16))
-          .append("-")
-          .append(raw.substring(16, 20))
-          .append("-")
-          .append(raw.substring(20));
-        return sb.toString();
-    }
-}
-
-
-
-
-
-
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import org.apache.log4j.Logger;
+
+import java.util.*;
+import java.net.*;
+import java.security.*;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class GuidGenerator {
+    private static Logger logger_ = Logger.getLogger(GuidGenerator.class);
+    private static Random myRand;
+    private static SecureRandom mySecureRand;
+    private static String s_id;
+    private static SafeMessageDigest md5 = null;
+
+    static {
+        if (System.getProperty("java.security.egd") == null) {
+            System.setProperty("java.security.egd", "file:/dev/urandom");
+        }
+        mySecureRand = new SecureRandom();
+        long secureInitializer = mySecureRand.nextLong();
+        myRand = new Random(secureInitializer);
+        try {
+            s_id = InetAddress.getLocalHost().toString();
+        }
+        catch (UnknownHostException e) {
+            if (logger_.isDebugEnabled())
+                logger_.debug(LogUtil.throwableToString(e));
+        }
+
+        try {
+            MessageDigest myMd5 = MessageDigest.getInstance("MD5");
+            md5 = new SafeMessageDigest(myMd5);
+        }
+        catch (NoSuchAlgorithmException e) {
+            if (logger_.isDebugEnabled())
+                logger_.debug(LogUtil.throwableToString(e));
+        }
+    }
+
+
+    public static String guid() {
+        byte[] array = guidAsBytes();
+        
+        StringBuilder sb = new StringBuilder();
+        for (int j = 0; j < array.length; ++j) {
+            int b = array[j] & 0xFF;
+            if (b < 0x10) sb.append('0');
+            sb.append(Integer.toHexString(b));
+        }
+
+        return convertToStandardFormat( sb.toString() );
+    }
+    
+    public static String guidToString(byte[] bytes)
+    {
+        StringBuilder sb = new StringBuilder();
+        for (int j = 0; j < bytes.length; ++j) {
+            int b = bytes[j] & 0xFF;
+            if (b < 0x10) sb.append('0');
+            sb.append(Integer.toHexString(b));
+        }
+
+        return convertToStandardFormat( sb.toString() );
+    }
+    
+    public static byte[] guidAsBytes()
+    {
+        StringBuilder sbValueBeforeMD5 = new StringBuilder();
+        long time = System.currentTimeMillis();
+        long rand = 0;
+        rand = myRand.nextLong();
+        sbValueBeforeMD5.append(s_id)
+        				.append(":")
+        				.append(Long.toString(time))
+        				.append(":")
+        				.append(Long.toString(rand));
+
+        String valueBeforeMD5 = sbValueBeforeMD5.toString();
+        return md5.digest(valueBeforeMD5.getBytes());
+    }
+
+    /*
+        * Convert to the standard format for GUID
+        * Example: C2FEEEAC-CFCD-11D1-8B05-00600806D9B6
+    */
+
+    private static String convertToStandardFormat(String valueAfterMD5) {
+        String raw = valueAfterMD5.toUpperCase();
+        StringBuilder sb = new StringBuilder();
+        sb.append(raw.substring(0, 8))
+          .append("-")
+          .append(raw.substring(8, 12))
+          .append("-")
+          .append(raw.substring(12, 16))
+          .append("-")
+          .append(raw.substring(16, 20))
+          .append("-")
+          .append(raw.substring(20));
+        return sb.toString();
+    }
+}
+
+
+
+
+
+
diff --git a/src/java/org/apache/cassandra/utils/HashingSchemes.java b/src/java/org/apache/cassandra/utils/HashingSchemes.java
index a8e782a1da..0b8370ccb2 100644
--- a/src/java/org/apache/cassandra/utils/HashingSchemes.java
+++ b/src/java/org/apache/cassandra/utils/HashingSchemes.java
@@ -1,34 +1,34 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-/**
- * Created by IntelliJ IDEA.
- * User: lakshman
- * Date: Aug 17, 2005
- * Time: 3:32:42 PM
- * To change this template use File | Settings | File Templates.
- */
-
-public final class HashingSchemes
-{
-    public static final String SHA_1 = "SHA-1";
-    public static final String SHA1 = "SHA1";
-    public static final String MD5 = "MD5";
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+/**
+ * Created by IntelliJ IDEA.
+ * User: lakshman
+ * Date: Aug 17, 2005
+ * Time: 3:32:42 PM
+ * To change this template use File | Settings | File Templates.
+ */
+
+public final class HashingSchemes
+{
+    public static final String SHA_1 = "SHA-1";
+    public static final String SHA1 = "SHA1";
+    public static final String MD5 = "MD5";
+}
diff --git a/src/java/org/apache/cassandra/utils/ICacheExpungeHook.java b/src/java/org/apache/cassandra/utils/ICacheExpungeHook.java
index 3ba8edbfeb..12483553b5 100644
--- a/src/java/org/apache/cassandra/utils/ICacheExpungeHook.java
+++ b/src/java/org/apache/cassandra/utils/ICacheExpungeHook.java
@@ -1,31 +1,31 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-/**
- * Created by IntelliJ IDEA.
- * User: lakshman
- * Date: Aug 16, 2005
- * Time: 1:08:58 PM
- * To change this template use File | Settings | File Templates.
- */
-public interface ICacheExpungeHook<K,V>
-{
-    public void callMe(K key , V value);
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+/**
+ * Created by IntelliJ IDEA.
+ * User: lakshman
+ * Date: Aug 16, 2005
+ * Time: 1:08:58 PM
+ * To change this template use File | Settings | File Templates.
+ */
+public interface ICacheExpungeHook<K,V>
+{
+    public void callMe(K key , V value);
+}
diff --git a/src/java/org/apache/cassandra/utils/ICachetable.java b/src/java/org/apache/cassandra/utils/ICachetable.java
index c5057a643a..f3a6ed115a 100644
--- a/src/java/org/apache/cassandra/utils/ICachetable.java
+++ b/src/java/org/apache/cassandra/utils/ICachetable.java
@@ -1,36 +1,36 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-import java.util.Enumeration;
-import java.util.Set;
-
-public interface ICachetable<K,V>
-{
-    public void put(K key, V value);
-    public void put(K key, V value, ICacheExpungeHook<K,V> hook);
-	public V get(K key);
-    public V remove(K key);
-    public int size();
-    public boolean containsKey(K key);
-    public boolean containsValue(V value);
-    public boolean isEmpty();    
-    public Set<K> keySet();
-    public void shutdown();
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.util.Enumeration;
+import java.util.Set;
+
+public interface ICachetable<K,V>
+{
+    public void put(K key, V value);
+    public void put(K key, V value, ICacheExpungeHook<K,V> hook);
+	public V get(K key);
+    public V remove(K key);
+    public int size();
+    public boolean containsKey(K key);
+    public boolean containsValue(V value);
+    public boolean isEmpty();    
+    public Set<K> keySet();
+    public void shutdown();
+}
diff --git a/src/java/org/apache/cassandra/utils/Log4jLogger.java b/src/java/org/apache/cassandra/utils/Log4jLogger.java
index a8a3a0e6fe..ca52498c92 100644
--- a/src/java/org/apache/cassandra/utils/Log4jLogger.java
+++ b/src/java/org/apache/cassandra/utils/Log4jLogger.java
@@ -1,52 +1,52 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-/**
- * Log4j configurations may change while the application is running, 
- * potentially invalidating a logger's appender(s).  This is a convenience
- * class to wrap logger calls so that a logger is always explicitly 
- * invoked.
- */
-
-
-public class Log4jLogger {
-    
-    private String name_ = null;
-    
-    public Log4jLogger(String name){
-        name_ = name;
-    }
-    
-    public void debug(Object arg){ 
-        LogUtil.getLogger(name_).debug(LogUtil.getTimestamp() + " - " + arg);
-    }    
-    public void info(Object arg){
-        LogUtil.getLogger(name_).info(LogUtil.getTimestamp() + " - " + arg);
-    }
-    public void warn(Object arg){
-        LogUtil.getLogger(name_).warn(LogUtil.getTimestamp() + " - " + arg);
-    }
-    public void error(Object arg){
-        LogUtil.getLogger(name_).error(LogUtil.getTimestamp() + " - " + arg);
-    }
-    public void fatal(Object arg){
-        LogUtil.getLogger(name_).fatal(LogUtil.getTimestamp() + " - " + arg);
-    } 
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+/**
+ * Log4j configurations may change while the application is running, 
+ * potentially invalidating a logger's appender(s).  This is a convenience
+ * class to wrap logger calls so that a logger is always explicitly 
+ * invoked.
+ */
+
+
+public class Log4jLogger {
+    
+    private String name_ = null;
+    
+    public Log4jLogger(String name){
+        name_ = name;
+    }
+    
+    public void debug(Object arg){ 
+        LogUtil.getLogger(name_).debug(LogUtil.getTimestamp() + " - " + arg);
+    }    
+    public void info(Object arg){
+        LogUtil.getLogger(name_).info(LogUtil.getTimestamp() + " - " + arg);
+    }
+    public void warn(Object arg){
+        LogUtil.getLogger(name_).warn(LogUtil.getTimestamp() + " - " + arg);
+    }
+    public void error(Object arg){
+        LogUtil.getLogger(name_).error(LogUtil.getTimestamp() + " - " + arg);
+    }
+    public void fatal(Object arg){
+        LogUtil.getLogger(name_).fatal(LogUtil.getTimestamp() + " - " + arg);
+    } 
+}
diff --git a/src/java/org/apache/cassandra/utils/LogUtil.java b/src/java/org/apache/cassandra/utils/LogUtil.java
index 51d4842918..d98af43af9 100644
--- a/src/java/org/apache/cassandra/utils/LogUtil.java
+++ b/src/java/org/apache/cassandra/utils/LogUtil.java
@@ -1,119 +1,119 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-import java.io.*;
-import java.text.*;
-import java.util.*;
-import org.apache.log4j.*;
-import org.apache.log4j.spi.LoggerFactory;
-import org.apache.log4j.xml.DOMConfigurator;
-
-public class LogUtil
-{
-
-    public LogUtil()
-    {
-    }
-
-    public static void init()
-    {
-        //BasicConfigurator.configure();
-        String file = System.getProperty("storage-config");
-        file += File.separator + "log4j.properties";
-        PropertyConfigurator.configure(file);
-    }
-
-    public static Logger getLogger(String name)
-    {
-        return Logger.getLogger(name);
-    }
-    
-    public static String stackTrace(Throwable e)
-    {
-        StringWriter sw = new StringWriter();
-        PrintWriter pw = new PrintWriter(sw);
-        e.printStackTrace(pw);
-        return sw.toString();
-    }
-
-    public static String getTimestamp()
-    {
-        Date date = new Date();
-        DateFormat df = new SimpleDateFormat("MM/dd/yyyy HH:mm:ss");
-        return df.format(date);
-    }
-    
-    public static String throwableToString(Throwable e)
-    {
-        StringBuilder sbuf = new StringBuilder("");
-        String trace = stackTrace(e);
-        sbuf.append((new StringBuilder())
-        	.append("Exception was generated at : ")
-        	.append(getTimestamp())
-        	.append(" on thread ")
-        	.append(Thread.currentThread().getName())
-        	.toString());
-        sbuf.append(System.getProperty("line.separator"));
-        String message = e.getMessage();
-        if(message != null)
-            sbuf.append(message);
-        sbuf.append(System.getProperty("line.separator"))
-        	.append(trace);
-        return sbuf.toString();
-    }
-
-    public static String getLogMessage(String message)
-    {
-        StringBuilder sbuf = new StringBuilder((new StringBuilder())
-        		.append("Log started at : ")
-        		.append(getTimestamp())
-        		.toString());
-        sbuf.append(File.separator)
-        	.append(message);
-        return sbuf.toString();
-    }
-
-    public static void setLogLevel(String logger, String level)
-    {        
-        Logger loggerObj = LogManager.getLogger(logger);
-        if(null == loggerObj)
-            return;
-        level = level.toUpperCase();
-        if(level.equals("DEBUG"))
-            loggerObj.setLevel(Level.DEBUG);
-        else
-        if(level.equals("ERROR"))
-            loggerObj.setLevel(Level.ERROR);
-        else
-        if(level.equals("FATAL"))
-            loggerObj.setLevel(Level.FATAL);
-        else
-        if(level.equals("INFO"))
-            loggerObj.setLevel(Level.INFO);
-        else
-        if(level.equals("OFF"))
-            loggerObj.setLevel(Level.OFF);
-        else
-        if(level.equals("WARN"))
-            loggerObj.setLevel(Level.WARN);
-        else
-            loggerObj.setLevel(Level.ALL);
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.io.*;
+import java.text.*;
+import java.util.*;
+import org.apache.log4j.*;
+import org.apache.log4j.spi.LoggerFactory;
+import org.apache.log4j.xml.DOMConfigurator;
+
+public class LogUtil
+{
+
+    public LogUtil()
+    {
+    }
+
+    public static void init()
+    {
+        //BasicConfigurator.configure();
+        String file = System.getProperty("storage-config");
+        file += File.separator + "log4j.properties";
+        PropertyConfigurator.configure(file);
+    }
+
+    public static Logger getLogger(String name)
+    {
+        return Logger.getLogger(name);
+    }
+    
+    public static String stackTrace(Throwable e)
+    {
+        StringWriter sw = new StringWriter();
+        PrintWriter pw = new PrintWriter(sw);
+        e.printStackTrace(pw);
+        return sw.toString();
+    }
+
+    public static String getTimestamp()
+    {
+        Date date = new Date();
+        DateFormat df = new SimpleDateFormat("MM/dd/yyyy HH:mm:ss");
+        return df.format(date);
+    }
+    
+    public static String throwableToString(Throwable e)
+    {
+        StringBuilder sbuf = new StringBuilder("");
+        String trace = stackTrace(e);
+        sbuf.append((new StringBuilder())
+        	.append("Exception was generated at : ")
+        	.append(getTimestamp())
+        	.append(" on thread ")
+        	.append(Thread.currentThread().getName())
+        	.toString());
+        sbuf.append(System.getProperty("line.separator"));
+        String message = e.getMessage();
+        if(message != null)
+            sbuf.append(message);
+        sbuf.append(System.getProperty("line.separator"))
+        	.append(trace);
+        return sbuf.toString();
+    }
+
+    public static String getLogMessage(String message)
+    {
+        StringBuilder sbuf = new StringBuilder((new StringBuilder())
+        		.append("Log started at : ")
+        		.append(getTimestamp())
+        		.toString());
+        sbuf.append(File.separator)
+        	.append(message);
+        return sbuf.toString();
+    }
+
+    public static void setLogLevel(String logger, String level)
+    {        
+        Logger loggerObj = LogManager.getLogger(logger);
+        if(null == loggerObj)
+            return;
+        level = level.toUpperCase();
+        if(level.equals("DEBUG"))
+            loggerObj.setLevel(Level.DEBUG);
+        else
+        if(level.equals("ERROR"))
+            loggerObj.setLevel(Level.ERROR);
+        else
+        if(level.equals("FATAL"))
+            loggerObj.setLevel(Level.FATAL);
+        else
+        if(level.equals("INFO"))
+            loggerObj.setLevel(Level.INFO);
+        else
+        if(level.equals("OFF"))
+            loggerObj.setLevel(Level.OFF);
+        else
+        if(level.equals("WARN"))
+            loggerObj.setLevel(Level.WARN);
+        else
+            loggerObj.setLevel(Level.ALL);
+    }
+}
diff --git a/src/java/org/apache/cassandra/utils/SafeMessageDigest.java b/src/java/org/apache/cassandra/utils/SafeMessageDigest.java
index 2d39027d15..1d1656fe7b 100644
--- a/src/java/org/apache/cassandra/utils/SafeMessageDigest.java
+++ b/src/java/org/apache/cassandra/utils/SafeMessageDigest.java
@@ -1,85 +1,85 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.utils;
-
-import java.security.MessageDigest;
-import java.security.NoSuchAlgorithmException;
-/**
- * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
- */
-
-public class SafeMessageDigest
-{
-    private MessageDigest md_ = null;
-
-    public static SafeMessageDigest digest_;
-    static
-    {
-        try
-        {
-            digest_ = new SafeMessageDigest(MessageDigest.getInstance("SHA-1"));
-        }
-        catch (NoSuchAlgorithmException e)
-        {
-            assert (false);
-        }
-    }
-
-    public SafeMessageDigest(MessageDigest md)
-    {
-        md_ = md;
-    }
-
-    public synchronized void update(byte[] theBytes)
-    {
-        md_.update(theBytes);
-    }
-
-    //NOTE: This should be used instead of seperate update() and then digest()
-    public synchronized byte[] digest(byte[] theBytes)
-    {
-        //this does an implicit update()
-        return md_.digest(theBytes);
-    }
-
-    public synchronized byte[] digest()
-    {
-        return md_.digest();
-    }
-
-    public byte[] unprotectedDigest()
-    {
-        return md_.digest();
-    }
-
-    public void unprotectedUpdate(byte[] theBytes)
-    {
-        md_.update(theBytes);
-    }
-
-    public byte[] unprotectedDigest(byte[] theBytes)
-    {
-        return md_.digest(theBytes);
-    }
-
-    public int getDigestLength()
-    {
-        return md_.getDigestLength();
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.utils;
+
+import java.security.MessageDigest;
+import java.security.NoSuchAlgorithmException;
+/**
+ * Author : Avinash Lakshman ( alakshman@facebook.com) & Prashant Malik ( pmalik@facebook.com )
+ */
+
+public class SafeMessageDigest
+{
+    private MessageDigest md_ = null;
+
+    public static SafeMessageDigest digest_;
+    static
+    {
+        try
+        {
+            digest_ = new SafeMessageDigest(MessageDigest.getInstance("SHA-1"));
+        }
+        catch (NoSuchAlgorithmException e)
+        {
+            assert (false);
+        }
+    }
+
+    public SafeMessageDigest(MessageDigest md)
+    {
+        md_ = md;
+    }
+
+    public synchronized void update(byte[] theBytes)
+    {
+        md_.update(theBytes);
+    }
+
+    //NOTE: This should be used instead of seperate update() and then digest()
+    public synchronized byte[] digest(byte[] theBytes)
+    {
+        //this does an implicit update()
+        return md_.digest(theBytes);
+    }
+
+    public synchronized byte[] digest()
+    {
+        return md_.digest();
+    }
+
+    public byte[] unprotectedDigest()
+    {
+        return md_.digest();
+    }
+
+    public void unprotectedUpdate(byte[] theBytes)
+    {
+        md_.update(theBytes);
+    }
+
+    public byte[] unprotectedDigest(byte[] theBytes)
+    {
+        return md_.digest(theBytes);
+    }
+
+    public int getDigestLength()
+    {
+        return md_.getDigestLength();
+    }
+}
diff --git a/test/unit/org/apache/cassandra/db/TableTest.java b/test/unit/org/apache/cassandra/db/TableTest.java
index 9cc53539b9..284c35d4a4 100644
--- a/test/unit/org/apache/cassandra/db/TableTest.java
+++ b/test/unit/org/apache/cassandra/db/TableTest.java
@@ -1,360 +1,360 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.db;
-
-import java.util.*;
-import java.io.IOException;
-
-import org.apache.commons.lang.StringUtils;
-import org.apache.commons.lang.ArrayUtils;
-import org.junit.Test;
-
-import static junit.framework.Assert.*;
-import org.apache.cassandra.CleanupHelper;
-import static org.apache.cassandra.Util.column;
-import static org.apache.cassandra.Util.getBytes;
-import org.apache.cassandra.db.filter.NamesQueryFilter;
-import org.apache.cassandra.db.filter.QueryPath;
-import org.apache.cassandra.db.marshal.LongType;
-import org.apache.cassandra.io.SSTableReader;
-
-public class TableTest extends CleanupHelper
-{
-    private static final String KEY2 = "key2";
-    private static final String TEST_KEY = "key1";
-
-    interface Runner
-    {
-        public void run() throws Exception;
-    }
-
-    private void reTest(Runner setup, ColumnFamilyStore cfs, Runner verify) throws Exception
-    {
-        setup.run();
-        verify.run();
-        cfs.forceBlockingFlush();
-        verify.run();
-    }
-
-    @Test
-    public void testOpen() throws Throwable {
-        Table table = Table.open("Mailbox");
-        Row row = table.get("35300190:1");
-        assertNotNull(row);
-    }
-    
-    @Test
-    public void testGetRowSingleColumn() throws Throwable
-    {
-        final Table table = Table.open("Keyspace1");
-        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
-
-        Runner setup = new Runner()
-        {
-            public void run() throws Exception
-            {
-                RowMutation rm = makeSimpleRowMutation();
-                rm.apply();
-            }
-        };
-        Runner verify = new Runner()
-        {
-            public void run() throws Exception
-            {
-                ColumnFamily cf;
-
-                cf = cfStore.getColumnFamily(new NamesQueryFilter(TEST_KEY, new QueryPath("Standard1"), "col1".getBytes()));
-                assertColumns(cf, "col1");
-
-                cf = cfStore.getColumnFamily(new NamesQueryFilter(TEST_KEY, new QueryPath("Standard1"), "col3".getBytes()));
-                assertColumns(cf, "col3");
-            }
-        };
-        reTest(setup, table.getColumnFamilyStore("Standard1"), verify);
-    }
-            
-    @Test
-    public void testGetRowSliceByRange() throws Throwable
-    {
-    	String key = TEST_KEY+"slicerow";
-    	Table table = Table.open("Keyspace1");
-        ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
-    	RowMutation rm = new RowMutation("Keyspace1", key);
-        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
-        // First write "a", "b", "c"
-        cf.addColumn(column("a", "val1", 1L));
-        cf.addColumn(column("b", "val2", 1L));
-        cf.addColumn(column("c", "val3", 1L));
-        rm.add(cf);
-        rm.apply();
-        
-        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), "b".getBytes(), "c".getBytes(), true, 100);
-        assertEquals(2, cf.getColumnCount());
-        
-        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), "b".getBytes(), "b".getBytes(), true, 100);
-        assertEquals(1, cf.getColumnCount());
-        
-        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), "b".getBytes(), "c".getBytes(), true, 1);
-        assertEquals(1, cf.getColumnCount());
-        
-        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), "c".getBytes(), "b".getBytes(), true, 1);
-        assertNull(cf);
-    }
-
-    private RowMutation makeSimpleRowMutation()
-    {
-        RowMutation rm = new RowMutation("Keyspace1",TEST_KEY);
-        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
-        cf.addColumn(column("col1","val1", 1L));
-        cf.addColumn(column("col2","val2", 1L));
-        cf.addColumn(column("col3","val3", 1L));
-        rm.add(cf);
-        return rm;
-    }
-
-    @Test
-    public void testGetSliceNoMatch() throws Throwable
-    {
-        Table table = Table.open("Keyspace1");
-        RowMutation rm = new RowMutation("Keyspace1", "row1000");
-        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard2");
-        cf.addColumn(column("col1", "val1", 1));
-        rm.add(cf);
-        rm.apply();
-
-        validateGetSliceNoMatch(table);
-        table.getColumnFamilyStore("Standard2").forceBlockingFlush();
-        validateGetSliceNoMatch(table);
-
-        Collection<SSTableReader> ssTables = table.getColumnFamilyStore("Standard2").getSSTables();
-        assertEquals(1, ssTables.size());
-        ssTables.iterator().next().forceBloomFilterFailures();
-        validateGetSliceNoMatch(table);
-    }
-
-    private void validateGetSliceNoMatch(Table table) throws IOException
-    {
-        ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard2");
-        ColumnFamily cf;
-
-        // key before the rows that exists
-        cf = cfStore.getColumnFamily("a", new QueryPath("Standard2"), ArrayUtils.EMPTY_BYTE_ARRAY, ArrayUtils.EMPTY_BYTE_ARRAY, true, 1);
-        assertColumns(cf);
-
-        // key after the rows that exist
-        cf = cfStore.getColumnFamily("z", new QueryPath("Standard2"), ArrayUtils.EMPTY_BYTE_ARRAY, ArrayUtils.EMPTY_BYTE_ARRAY, true, 1);
-        assertColumns(cf);
-    }
-
-    @Test
-    public void testGetSliceFromBasic() throws Throwable
-    {
-        // tests slicing against data from one row in a memtable and then flushed to an sstable
-        final Table table = Table.open("Keyspace1");
-        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
-        final String ROW = "row1";
-        Runner setup = new Runner()
-        {
-            public void run() throws Exception
-            {
-                RowMutation rm = new RowMutation("Keyspace1", ROW);
-                ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
-                cf.addColumn(column("col1", "val1", 1L));
-                cf.addColumn(column("col3", "val3", 1L));
-                cf.addColumn(column("col4", "val4", 1L));
-                cf.addColumn(column("col5", "val5", 1L));
-                cf.addColumn(column("col7", "val7", 1L));
-                cf.addColumn(column("col9", "val9", 1L));
-                rm.add(cf);
-                rm.apply();
-
-                rm = new RowMutation("Keyspace1", ROW);
-                rm.delete(new QueryPath("Standard1", null, "col4".getBytes()), 2L);
-                rm.apply();
-            }
-        };
-
-        Runner verify = new Runner()
-        {
-            public void run() throws Exception
-            {
-                Row result;
-                ColumnFamily cf;
-
-                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col5".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 2);
-                assertColumns(cf, "col5", "col7");
-
-                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col4".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 2);
-                assertColumns(cf, "col4", "col5", "col7");
-                assertColumns(ColumnFamilyStore.removeDeleted(cf, Integer.MAX_VALUE), "col5", "col7");
-
-                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col5".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, false, 2);
-                assertColumns(cf, "col3", "col4", "col5");
-
-                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col6".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, false, 2);
-                assertColumns(cf, "col3", "col4", "col5");
-
-                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col95".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 2);
-                assertColumns(cf);
-
-                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col0".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, false, 2);
-                assertColumns(cf);
-            }
-        };
-
-        reTest(setup, table.getColumnFamilyStore("Standard1"), verify);
-    }
-
-    @Test
-    public void testGetSliceFromAdvanced() throws Throwable
-    {
-        // tests slicing against data from one row spread across two sstables
-        final Table table = Table.open("Keyspace1");
-        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
-        final String ROW = "row2";
-        Runner setup = new Runner()
-        {
-            public void run() throws Exception
-            {
-                RowMutation rm = new RowMutation("Keyspace1", ROW);
-                ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
-                cf.addColumn(column("col1", "val1", 1L));
-                cf.addColumn(column("col2", "val2", 1L));
-                cf.addColumn(column("col3", "val3", 1L));
-                cf.addColumn(column("col4", "val4", 1L));
-                cf.addColumn(column("col5", "val5", 1L));
-                cf.addColumn(column("col6", "val6", 1L));
-                rm.add(cf);
-                rm.apply();
-                cfStore.forceBlockingFlush();
-
-                rm = new RowMutation("Keyspace1", ROW);
-                cf = ColumnFamily.create("Keyspace1", "Standard1");
-                cf.addColumn(column("col1", "valx", 2L));
-                cf.addColumn(column("col2", "valx", 2L));
-                cf.addColumn(column("col3", "valx", 2L));
-                rm.add(cf);
-                rm.apply();
-            }
-        };
-
-        Runner verify = new Runner()
-        {
-            public void run() throws Exception
-            {
-                ColumnFamily cf;
-
-                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col2".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 3);
-                assertColumns(cf, "col2", "col3", "col4");
-                assertEquals(new String(cf.getColumn("col2".getBytes()).value()), "valx");
-                assertEquals(new String(cf.getColumn("col3".getBytes()).value()), "valx");
-                assertEquals(new String(cf.getColumn("col4".getBytes()).value()), "val4");
-            }
-        };
-
-        reTest(setup, table.getColumnFamilyStore("Standard1"), verify);
-    }
-
-    @Test
-    public void testGetSliceFromLarge() throws Throwable
-    {
-        // tests slicing against 1000 columns in an sstable
-        Table table = Table.open("Keyspace1");
-        ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
-        String ROW = "row3";
-        RowMutation rm = new RowMutation("Keyspace1", ROW);
-        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
-        for (int i = 1000; i < 2000; i++)
-            cf.addColumn(column("col" + i, ("vvvvvvvvvvvvvvvv" + i), 1L));
-        rm.add(cf);
-        rm.apply();
-        cfStore.forceBlockingFlush();
-
-        cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col1000".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 3);
-        assertColumns(cf, "col1000", "col1001", "col1002");
-        assertEquals(new String(cf.getColumn("col1000".getBytes()).value()), "vvvvvvvvvvvvvvvv1000");
-        assertEquals(new String(cf.getColumn("col1001".getBytes()).value()), "vvvvvvvvvvvvvvvv1001");
-        assertEquals(new String(cf.getColumn("col1002".getBytes()).value()), "vvvvvvvvvvvvvvvv1002");
-
-        cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col1195".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 3);
-        assertColumns(cf, "col1195", "col1196", "col1197");
-        assertEquals(new String(cf.getColumn("col1195".getBytes()).value()), "vvvvvvvvvvvvvvvv1195");
-        assertEquals(new String(cf.getColumn("col1196".getBytes()).value()), "vvvvvvvvvvvvvvvv1196");
-        assertEquals(new String(cf.getColumn("col1197".getBytes()).value()), "vvvvvvvvvvvvvvvv1197");
-
-        cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col1196".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, false, 3);
-        assertColumns(cf, "col1194", "col1195", "col1196");
-        assertEquals(new String(cf.getColumn("col1194".getBytes()).value()), "vvvvvvvvvvvvvvvv1194");
-        assertEquals(new String(cf.getColumn("col1195".getBytes()).value()), "vvvvvvvvvvvvvvvv1195");
-        assertEquals(new String(cf.getColumn("col1196".getBytes()).value()), "vvvvvvvvvvvvvvvv1196");
-
-        cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col1990".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 3);
-        assertColumns(cf, "col1990", "col1991", "col1992");
-        assertEquals(new String(cf.getColumn("col1990".getBytes()).value()), "vvvvvvvvvvvvvvvv1990");
-        assertEquals(new String(cf.getColumn("col1991".getBytes()).value()), "vvvvvvvvvvvvvvvv1991");
-        assertEquals(new String(cf.getColumn("col1992".getBytes()).value()), "vvvvvvvvvvvvvvvv1992");
-    }
-
-    @Test
-    public void testGetSliceFromSuperBasic() throws Throwable
-    {
-        // tests slicing against data from one row spread across two sstables
-        final Table table = Table.open("Keyspace1");
-        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Super1");
-        final String ROW = "row2";
-        Runner setup = new Runner()
-        {
-            public void run() throws Exception
-            {
-                RowMutation rm = new RowMutation("Keyspace1", ROW);
-                ColumnFamily cf = ColumnFamily.create("Keyspace1", "Super1");
-                SuperColumn sc = new SuperColumn("sc1".getBytes(), new LongType());
-                sc.addColumn(new Column(getBytes(1), "val1".getBytes(), 1L));
-                cf.addColumn(sc);
-                rm.add(cf);
-                rm.apply();
-            }
-        };
-
-        Runner verify = new Runner()
-        {
-            public void run() throws Exception
-            {
-                ColumnFamily cf = cfStore.getColumnFamily(ROW, new QueryPath("Super1"), ArrayUtils.EMPTY_BYTE_ARRAY, ArrayUtils.EMPTY_BYTE_ARRAY, true, 10);
-                assertColumns(cf, "sc1");
-                assertEquals(new String(cf.getColumn("sc1".getBytes()).getSubColumn(getBytes(1)).value()), "val1");
-            }
-        };
-
-        reTest(setup, table.getColumnFamilyStore("Standard1"), verify);
-    }
-
-    public static void assertColumns(ColumnFamily cf, String... columnNames)
-    {
-        Collection<IColumn> columns = cf == null ? new TreeSet<IColumn>() : cf.getSortedColumns();
-        List<String> L = new ArrayList<String>();
-        for (IColumn column : columns)
-        {
-            L.add(new String(column.name()));
-        }
-        assert Arrays.equals(L.toArray(new String[columns.size()]), columnNames)
-                : "Columns [" + ((cf == null) ? "" : cf.getComparator().getColumnsString(columns)) + "]"
-                  + " is not expected [" + StringUtils.join(columnNames, ",") + "]";
-    }
-}
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.util.*;
+import java.io.IOException;
+
+import org.apache.commons.lang.StringUtils;
+import org.apache.commons.lang.ArrayUtils;
+import org.junit.Test;
+
+import static junit.framework.Assert.*;
+import org.apache.cassandra.CleanupHelper;
+import static org.apache.cassandra.Util.column;
+import static org.apache.cassandra.Util.getBytes;
+import org.apache.cassandra.db.filter.NamesQueryFilter;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.io.SSTableReader;
+
+public class TableTest extends CleanupHelper
+{
+    private static final String KEY2 = "key2";
+    private static final String TEST_KEY = "key1";
+
+    interface Runner
+    {
+        public void run() throws Exception;
+    }
+
+    private void reTest(Runner setup, ColumnFamilyStore cfs, Runner verify) throws Exception
+    {
+        setup.run();
+        verify.run();
+        cfs.forceBlockingFlush();
+        verify.run();
+    }
+
+    @Test
+    public void testOpen() throws Throwable {
+        Table table = Table.open("Mailbox");
+        Row row = table.get("35300190:1");
+        assertNotNull(row);
+    }
+    
+    @Test
+    public void testGetRowSingleColumn() throws Throwable
+    {
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+
+        Runner setup = new Runner()
+        {
+            public void run() throws Exception
+            {
+                RowMutation rm = makeSimpleRowMutation();
+                rm.apply();
+            }
+        };
+        Runner verify = new Runner()
+        {
+            public void run() throws Exception
+            {
+                ColumnFamily cf;
+
+                cf = cfStore.getColumnFamily(new NamesQueryFilter(TEST_KEY, new QueryPath("Standard1"), "col1".getBytes()));
+                assertColumns(cf, "col1");
+
+                cf = cfStore.getColumnFamily(new NamesQueryFilter(TEST_KEY, new QueryPath("Standard1"), "col3".getBytes()));
+                assertColumns(cf, "col3");
+            }
+        };
+        reTest(setup, table.getColumnFamilyStore("Standard1"), verify);
+    }
+            
+    @Test
+    public void testGetRowSliceByRange() throws Throwable
+    {
+    	String key = TEST_KEY+"slicerow";
+    	Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+    	RowMutation rm = new RowMutation("Keyspace1", key);
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+        // First write "a", "b", "c"
+        cf.addColumn(column("a", "val1", 1L));
+        cf.addColumn(column("b", "val2", 1L));
+        cf.addColumn(column("c", "val3", 1L));
+        rm.add(cf);
+        rm.apply();
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), "b".getBytes(), "c".getBytes(), true, 100);
+        assertEquals(2, cf.getColumnCount());
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), "b".getBytes(), "b".getBytes(), true, 100);
+        assertEquals(1, cf.getColumnCount());
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), "b".getBytes(), "c".getBytes(), true, 1);
+        assertEquals(1, cf.getColumnCount());
+        
+        cf = cfStore.getColumnFamily(key, new QueryPath("Standard1"), "c".getBytes(), "b".getBytes(), true, 1);
+        assertNull(cf);
+    }
+
+    private RowMutation makeSimpleRowMutation()
+    {
+        RowMutation rm = new RowMutation("Keyspace1",TEST_KEY);
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+        cf.addColumn(column("col1","val1", 1L));
+        cf.addColumn(column("col2","val2", 1L));
+        cf.addColumn(column("col3","val3", 1L));
+        rm.add(cf);
+        return rm;
+    }
+
+    @Test
+    public void testGetSliceNoMatch() throws Throwable
+    {
+        Table table = Table.open("Keyspace1");
+        RowMutation rm = new RowMutation("Keyspace1", "row1000");
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard2");
+        cf.addColumn(column("col1", "val1", 1));
+        rm.add(cf);
+        rm.apply();
+
+        validateGetSliceNoMatch(table);
+        table.getColumnFamilyStore("Standard2").forceBlockingFlush();
+        validateGetSliceNoMatch(table);
+
+        Collection<SSTableReader> ssTables = table.getColumnFamilyStore("Standard2").getSSTables();
+        assertEquals(1, ssTables.size());
+        ssTables.iterator().next().forceBloomFilterFailures();
+        validateGetSliceNoMatch(table);
+    }
+
+    private void validateGetSliceNoMatch(Table table) throws IOException
+    {
+        ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard2");
+        ColumnFamily cf;
+
+        // key before the rows that exists
+        cf = cfStore.getColumnFamily("a", new QueryPath("Standard2"), ArrayUtils.EMPTY_BYTE_ARRAY, ArrayUtils.EMPTY_BYTE_ARRAY, true, 1);
+        assertColumns(cf);
+
+        // key after the rows that exist
+        cf = cfStore.getColumnFamily("z", new QueryPath("Standard2"), ArrayUtils.EMPTY_BYTE_ARRAY, ArrayUtils.EMPTY_BYTE_ARRAY, true, 1);
+        assertColumns(cf);
+    }
+
+    @Test
+    public void testGetSliceFromBasic() throws Throwable
+    {
+        // tests slicing against data from one row in a memtable and then flushed to an sstable
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+        final String ROW = "row1";
+        Runner setup = new Runner()
+        {
+            public void run() throws Exception
+            {
+                RowMutation rm = new RowMutation("Keyspace1", ROW);
+                ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+                cf.addColumn(column("col1", "val1", 1L));
+                cf.addColumn(column("col3", "val3", 1L));
+                cf.addColumn(column("col4", "val4", 1L));
+                cf.addColumn(column("col5", "val5", 1L));
+                cf.addColumn(column("col7", "val7", 1L));
+                cf.addColumn(column("col9", "val9", 1L));
+                rm.add(cf);
+                rm.apply();
+
+                rm = new RowMutation("Keyspace1", ROW);
+                rm.delete(new QueryPath("Standard1", null, "col4".getBytes()), 2L);
+                rm.apply();
+            }
+        };
+
+        Runner verify = new Runner()
+        {
+            public void run() throws Exception
+            {
+                Row result;
+                ColumnFamily cf;
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col5".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 2);
+                assertColumns(cf, "col5", "col7");
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col4".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 2);
+                assertColumns(cf, "col4", "col5", "col7");
+                assertColumns(ColumnFamilyStore.removeDeleted(cf, Integer.MAX_VALUE), "col5", "col7");
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col5".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, false, 2);
+                assertColumns(cf, "col3", "col4", "col5");
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col6".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, false, 2);
+                assertColumns(cf, "col3", "col4", "col5");
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col95".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 2);
+                assertColumns(cf);
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col0".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, false, 2);
+                assertColumns(cf);
+            }
+        };
+
+        reTest(setup, table.getColumnFamilyStore("Standard1"), verify);
+    }
+
+    @Test
+    public void testGetSliceFromAdvanced() throws Throwable
+    {
+        // tests slicing against data from one row spread across two sstables
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+        final String ROW = "row2";
+        Runner setup = new Runner()
+        {
+            public void run() throws Exception
+            {
+                RowMutation rm = new RowMutation("Keyspace1", ROW);
+                ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+                cf.addColumn(column("col1", "val1", 1L));
+                cf.addColumn(column("col2", "val2", 1L));
+                cf.addColumn(column("col3", "val3", 1L));
+                cf.addColumn(column("col4", "val4", 1L));
+                cf.addColumn(column("col5", "val5", 1L));
+                cf.addColumn(column("col6", "val6", 1L));
+                rm.add(cf);
+                rm.apply();
+                cfStore.forceBlockingFlush();
+
+                rm = new RowMutation("Keyspace1", ROW);
+                cf = ColumnFamily.create("Keyspace1", "Standard1");
+                cf.addColumn(column("col1", "valx", 2L));
+                cf.addColumn(column("col2", "valx", 2L));
+                cf.addColumn(column("col3", "valx", 2L));
+                rm.add(cf);
+                rm.apply();
+            }
+        };
+
+        Runner verify = new Runner()
+        {
+            public void run() throws Exception
+            {
+                ColumnFamily cf;
+
+                cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col2".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 3);
+                assertColumns(cf, "col2", "col3", "col4");
+                assertEquals(new String(cf.getColumn("col2".getBytes()).value()), "valx");
+                assertEquals(new String(cf.getColumn("col3".getBytes()).value()), "valx");
+                assertEquals(new String(cf.getColumn("col4".getBytes()).value()), "val4");
+            }
+        };
+
+        reTest(setup, table.getColumnFamilyStore("Standard1"), verify);
+    }
+
+    @Test
+    public void testGetSliceFromLarge() throws Throwable
+    {
+        // tests slicing against 1000 columns in an sstable
+        Table table = Table.open("Keyspace1");
+        ColumnFamilyStore cfStore = table.getColumnFamilyStore("Standard1");
+        String ROW = "row3";
+        RowMutation rm = new RowMutation("Keyspace1", ROW);
+        ColumnFamily cf = ColumnFamily.create("Keyspace1", "Standard1");
+        for (int i = 1000; i < 2000; i++)
+            cf.addColumn(column("col" + i, ("vvvvvvvvvvvvvvvv" + i), 1L));
+        rm.add(cf);
+        rm.apply();
+        cfStore.forceBlockingFlush();
+
+        cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col1000".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 3);
+        assertColumns(cf, "col1000", "col1001", "col1002");
+        assertEquals(new String(cf.getColumn("col1000".getBytes()).value()), "vvvvvvvvvvvvvvvv1000");
+        assertEquals(new String(cf.getColumn("col1001".getBytes()).value()), "vvvvvvvvvvvvvvvv1001");
+        assertEquals(new String(cf.getColumn("col1002".getBytes()).value()), "vvvvvvvvvvvvvvvv1002");
+
+        cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col1195".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 3);
+        assertColumns(cf, "col1195", "col1196", "col1197");
+        assertEquals(new String(cf.getColumn("col1195".getBytes()).value()), "vvvvvvvvvvvvvvvv1195");
+        assertEquals(new String(cf.getColumn("col1196".getBytes()).value()), "vvvvvvvvvvvvvvvv1196");
+        assertEquals(new String(cf.getColumn("col1197".getBytes()).value()), "vvvvvvvvvvvvvvvv1197");
+
+        cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col1196".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, false, 3);
+        assertColumns(cf, "col1194", "col1195", "col1196");
+        assertEquals(new String(cf.getColumn("col1194".getBytes()).value()), "vvvvvvvvvvvvvvvv1194");
+        assertEquals(new String(cf.getColumn("col1195".getBytes()).value()), "vvvvvvvvvvvvvvvv1195");
+        assertEquals(new String(cf.getColumn("col1196".getBytes()).value()), "vvvvvvvvvvvvvvvv1196");
+
+        cf = cfStore.getColumnFamily(ROW, new QueryPath("Standard1"), "col1990".getBytes(), ArrayUtils.EMPTY_BYTE_ARRAY, true, 3);
+        assertColumns(cf, "col1990", "col1991", "col1992");
+        assertEquals(new String(cf.getColumn("col1990".getBytes()).value()), "vvvvvvvvvvvvvvvv1990");
+        assertEquals(new String(cf.getColumn("col1991".getBytes()).value()), "vvvvvvvvvvvvvvvv1991");
+        assertEquals(new String(cf.getColumn("col1992".getBytes()).value()), "vvvvvvvvvvvvvvvv1992");
+    }
+
+    @Test
+    public void testGetSliceFromSuperBasic() throws Throwable
+    {
+        // tests slicing against data from one row spread across two sstables
+        final Table table = Table.open("Keyspace1");
+        final ColumnFamilyStore cfStore = table.getColumnFamilyStore("Super1");
+        final String ROW = "row2";
+        Runner setup = new Runner()
+        {
+            public void run() throws Exception
+            {
+                RowMutation rm = new RowMutation("Keyspace1", ROW);
+                ColumnFamily cf = ColumnFamily.create("Keyspace1", "Super1");
+                SuperColumn sc = new SuperColumn("sc1".getBytes(), new LongType());
+                sc.addColumn(new Column(getBytes(1), "val1".getBytes(), 1L));
+                cf.addColumn(sc);
+                rm.add(cf);
+                rm.apply();
+            }
+        };
+
+        Runner verify = new Runner()
+        {
+            public void run() throws Exception
+            {
+                ColumnFamily cf = cfStore.getColumnFamily(ROW, new QueryPath("Super1"), ArrayUtils.EMPTY_BYTE_ARRAY, ArrayUtils.EMPTY_BYTE_ARRAY, true, 10);
+                assertColumns(cf, "sc1");
+                assertEquals(new String(cf.getColumn("sc1".getBytes()).getSubColumn(getBytes(1)).value()), "val1");
+            }
+        };
+
+        reTest(setup, table.getColumnFamilyStore("Standard1"), verify);
+    }
+
+    public static void assertColumns(ColumnFamily cf, String... columnNames)
+    {
+        Collection<IColumn> columns = cf == null ? new TreeSet<IColumn>() : cf.getSortedColumns();
+        List<String> L = new ArrayList<String>();
+        for (IColumn column : columns)
+        {
+            L.add(new String(column.name()));
+        }
+        assert Arrays.equals(L.toArray(new String[columns.size()]), columnNames)
+                : "Columns [" + ((cf == null) ? "" : cf.getComparator().getColumnsString(columns)) + "]"
+                  + " is not expected [" + StringUtils.join(columnNames, ",") + "]";
+    }
+}
diff --git a/test/unit/org/apache/cassandra/gms/GossipDigestTest.java b/test/unit/org/apache/cassandra/gms/GossipDigestTest.java
index a58f74a025..32cfd2cb4f 100644
--- a/test/unit/org/apache/cassandra/gms/GossipDigestTest.java
+++ b/test/unit/org/apache/cassandra/gms/GossipDigestTest.java
@@ -1,37 +1,37 @@
-package org.apache.cassandra.gms;
-
-import static org.junit.Assert.*;
-
-import java.io.IOException;
-
-import org.apache.cassandra.io.DataInputBuffer;
-import org.apache.cassandra.io.DataOutputBuffer;
-import org.apache.cassandra.net.EndPoint;
-import org.junit.Test;
-
-public class GossipDigestTest
-{
-
-    @Test
-    public void test() throws IOException
-    {
-        EndPoint endPoint = new EndPoint("127.0.0.1", 3333);
-        int generation = 0;
-        int maxVersion = 123;
-        GossipDigest expected = new GossipDigest(endPoint, generation, maxVersion);
-        //make sure we get the same values out
-        assertEquals(endPoint, expected.getEndPoint());
-        assertEquals(generation, expected.getGeneration());
-        assertEquals(maxVersion, expected.getMaxVersion());
-        
-        //test the serialization and equals
-        DataOutputBuffer output = new DataOutputBuffer();
-        GossipDigest.serializer().serialize(expected, output);
-        
-        DataInputBuffer input = new DataInputBuffer();
-        input.reset(output.getData(), output.getLength());
-        GossipDigest actual = GossipDigest.serializer().deserialize(input);
-        assertEquals(0, expected.compareTo(actual));
-    }
-
-}
+package org.apache.cassandra.gms;
+
+import static org.junit.Assert.*;
+
+import java.io.IOException;
+
+import org.apache.cassandra.io.DataInputBuffer;
+import org.apache.cassandra.io.DataOutputBuffer;
+import org.apache.cassandra.net.EndPoint;
+import org.junit.Test;
+
+public class GossipDigestTest
+{
+
+    @Test
+    public void test() throws IOException
+    {
+        EndPoint endPoint = new EndPoint("127.0.0.1", 3333);
+        int generation = 0;
+        int maxVersion = 123;
+        GossipDigest expected = new GossipDigest(endPoint, generation, maxVersion);
+        //make sure we get the same values out
+        assertEquals(endPoint, expected.getEndPoint());
+        assertEquals(generation, expected.getGeneration());
+        assertEquals(maxVersion, expected.getMaxVersion());
+        
+        //test the serialization and equals
+        DataOutputBuffer output = new DataOutputBuffer();
+        GossipDigest.serializer().serialize(expected, output);
+        
+        DataInputBuffer input = new DataInputBuffer();
+        input.reset(output.getData(), output.getLength());
+        GossipDigest actual = GossipDigest.serializer().deserialize(input);
+        assertEquals(0, expected.compareTo(actual));
+    }
+
+}
