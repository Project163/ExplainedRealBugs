diff --git a/CHANGES.txt b/CHANGES.txt
index 248139fccd..b323f18c7e 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -1,4 +1,5 @@
 2.1.4
+ * Safer Resource Management++ (CASSANDRA-8707)
  * Write partition size estimates into a system table (CASSANDRA-7688)
  * cqlsh: Fix keys() and full() collection indexes in DESCRIBE output
    (CASSANDRA-8154)
diff --git a/src/java/org/apache/cassandra/db/DataTracker.java b/src/java/org/apache/cassandra/db/DataTracker.java
index 5ec06bce1a..acf9f9254a 100644
--- a/src/java/org/apache/cassandra/db/DataTracker.java
+++ b/src/java/org/apache/cassandra/db/DataTracker.java
@@ -367,7 +367,7 @@ public class DataTracker
         while (!view.compareAndSet(currentView, newView));
         for (SSTableReader sstable : currentView.sstables)
             if (!remaining.contains(sstable))
-                sstable.sharedRef().release();
+                sstable.selfRef().release();
         notifySSTablesChanged(remaining, Collections.<SSTableReader>emptySet(), OperationType.UNKNOWN);
     }
 
@@ -406,7 +406,7 @@ public class DataTracker
             sstable.setTrackedBy(this);
 
         for (SSTableReader sstable : oldSSTables)
-            sstable.sharedRef().release();
+            sstable.selfRef().release();
     }
 
     private void removeSSTablesFromTracker(Collection<SSTableReader> oldSSTables)
@@ -467,7 +467,7 @@ public class DataTracker
         {
             boolean firstToCompact = sstable.markObsolete();
             assert tolerateCompacted || firstToCompact : sstable + " was already marked compacted";
-            sstable.sharedRef().release();
+            sstable.selfRef().release();
         }
     }
 
diff --git a/src/java/org/apache/cassandra/db/compaction/CompactionTask.java b/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
index b6c215e21d..4d9b463487 100644
--- a/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
+++ b/src/java/org/apache/cassandra/db/compaction/CompactionTask.java
@@ -18,10 +18,8 @@
 package org.apache.cassandra.db.compaction;
 
 import java.io.File;
-import java.io.IOException;
 import java.util.Arrays;
 import java.util.Collection;
-import java.util.Collections;
 import java.util.HashMap;
 import java.util.Iterator;
 import java.util.List;
@@ -31,7 +29,6 @@ import java.util.UUID;
 import java.util.concurrent.TimeUnit;
 
 import com.google.common.base.Predicate;
-import com.google.common.collect.ImmutableList;
 import com.google.common.collect.Iterables;
 import com.google.common.collect.Sets;
 import org.apache.commons.lang3.StringUtils;
@@ -40,7 +37,6 @@ import org.slf4j.LoggerFactory;
 
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.db.ColumnFamilyStore;
-import org.apache.cassandra.db.RowIndexEntry;
 import org.apache.cassandra.db.SystemKeyspace;
 import org.apache.cassandra.db.compaction.CompactionManager.CompactionExecutorStatsCollector;
 import org.apache.cassandra.io.sstable.SSTableReader;
diff --git a/src/java/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategy.java b/src/java/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategy.java
index fbd715c9ff..8b1610e203 100644
--- a/src/java/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategy.java
+++ b/src/java/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategy.java
@@ -144,8 +144,8 @@ public class SizeTieredCompactionStrategy extends AbstractCompactionStrategy
         // calculate the total reads/sec across all sstables
         double totalReads = 0.0;
         for (SSTableReader sstr : sstables)
-            if (sstr.readMeter != null)
-                totalReads += sstr.readMeter.twoHourRate();
+            if (sstr.getReadMeter() != null)
+                totalReads += sstr.getReadMeter().twoHourRate();
 
         // if this is a system table with no read meters or we don't have any read rates yet, just return them all
         if (totalReads == 0.0)
@@ -159,11 +159,11 @@ public class SizeTieredCompactionStrategy extends AbstractCompactionStrategy
         while (cutoffIndex < sstables.size())
         {
             SSTableReader sstable = sstables.get(cutoffIndex);
-            if (sstable.readMeter == null)
+            if (sstable.getReadMeter() == null)
             {
                 throw new AssertionError("If you're seeing this exception, please attach your logs to CASSANDRA-8238 to help us debug. "+sstable);
             }
-            double reads = sstable.readMeter.twoHourRate();
+            double reads = sstable.getReadMeter().twoHourRate();
             if (totalColdReads + reads > maxColdReads)
                 break;
 
@@ -307,7 +307,7 @@ public class SizeTieredCompactionStrategy extends AbstractCompactionStrategy
     private static double hotness(SSTableReader sstr)
     {
         // system tables don't have read meters, just use 0.0 for the hotness
-        return sstr.readMeter == null ? 0.0 : sstr.readMeter.twoHourRate() / sstr.estimatedKeys();
+        return sstr.getReadMeter() == null ? 0.0 : sstr.getReadMeter().twoHourRate() / sstr.estimatedKeys();
     }
 
     public synchronized AbstractCompactionTask getNextBackgroundTask(int gcBefore)
diff --git a/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java b/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java
index a40048a840..aaf165655e 100644
--- a/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java
+++ b/src/java/org/apache/cassandra/io/compress/CompressionMetadata.java
@@ -325,18 +325,15 @@ public class CompressionMetadata
         public CompressionMetadata open(long dataLength, long compressedLength, SSTableWriter.FinishType finishType)
         {
             RefCountedMemory offsets;
-            switch (finishType)
+            if (finishType.isFinal)
             {
-                case EARLY:
-                    offsets = this.offsets;
-                    break;
-                case NORMAL:
-                case FINISH_EARLY:
-                    offsets = this.offsets.copy(count * 8L);
-                    this.offsets.unreference();
-                    break;
-                default:
-                    throw new AssertionError();
+                // we now know how many offsets we have and can resize the offsets properly
+                offsets = this.offsets.copy(count * 8L);
+                this.offsets.unreference();
+            }
+            else
+            {
+                offsets = this.offsets;
             }
             return new CompressionMetadata(filePath, parameters, offsets, count * 8L, dataLength, compressedLength, Descriptor.Version.CURRENT.hasPostCompressionAdlerChecksums);
         }
diff --git a/src/java/org/apache/cassandra/io/sstable/IndexSummary.java b/src/java/org/apache/cassandra/io/sstable/IndexSummary.java
index f53a7e444c..0cde12456f 100644
--- a/src/java/org/apache/cassandra/io/sstable/IndexSummary.java
+++ b/src/java/org/apache/cassandra/io/sstable/IndexSummary.java
@@ -17,21 +17,19 @@
  */
 package org.apache.cassandra.io.sstable;
 
-import java.io.Closeable;
 import java.io.DataInputStream;
 import java.io.IOException;
 import java.nio.ByteBuffer;
 
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
 import org.apache.cassandra.cache.RefCountedMemory;
 import org.apache.cassandra.db.DecoratedKey;
 import org.apache.cassandra.db.RowPosition;
 import org.apache.cassandra.dht.IPartitioner;
 import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.io.util.Memory;
 import org.apache.cassandra.io.util.MemoryOutputStream;
 import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.concurrent.WrappedSharedCloseable;
 
 import static org.apache.cassandra.io.sstable.Downsampling.BASE_SAMPLING_LEVEL;
 
@@ -45,10 +43,8 @@ import static org.apache.cassandra.io.sstable.Downsampling.BASE_SAMPLING_LEVEL;
  *     (This is necessary because keys can have different lengths.)
  *  2.  A sequence of (DecoratedKey, position) pairs, where position is the offset into the actual index file.
  */
-public class IndexSummary implements Closeable
+public class IndexSummary extends WrappedSharedCloseable
 {
-    private static final Logger logger = LoggerFactory.getLogger(IndexSummary.class);
-
     public static final IndexSummarySerializer serializer = new IndexSummarySerializer();
 
     /**
@@ -60,7 +56,7 @@ public class IndexSummary implements Closeable
     private final IPartitioner partitioner;
     private final int summarySize;
     private final int sizeAtFullSampling;
-    private final RefCountedMemory bytes;
+    private final Memory bytes;
 
     /**
      * A value between 1 and BASE_SAMPLING_LEVEL that represents how many of the original
@@ -70,17 +66,29 @@ public class IndexSummary implements Closeable
      */
     private final int samplingLevel;
 
-    public IndexSummary(IPartitioner partitioner, RefCountedMemory memory, int summarySize, int sizeAtFullSampling,
+    public IndexSummary(IPartitioner partitioner, Memory bytes, int summarySize, int sizeAtFullSampling,
                         int minIndexInterval, int samplingLevel)
     {
+        super(bytes);
         this.partitioner = partitioner;
         this.minIndexInterval = minIndexInterval;
         this.summarySize = summarySize;
         this.sizeAtFullSampling = sizeAtFullSampling;
-        this.bytes = memory;
+        this.bytes = bytes;
         this.samplingLevel = samplingLevel;
     }
 
+    private IndexSummary(IndexSummary copy)
+    {
+        super(copy);
+        this.partitioner = copy.partitioner;
+        this.minIndexInterval = copy.minIndexInterval;
+        this.summarySize = copy.summarySize;
+        this.sizeAtFullSampling = copy.sizeAtFullSampling;
+        this.bytes = copy.bytes;
+        this.samplingLevel = copy.samplingLevel;
+    }
+
     // binary search is notoriously more difficult to get right than it looks; this is lifted from
     // Harmony's Collections implementation
     public int binarySearch(RowPosition key)
@@ -137,7 +145,7 @@ public class IndexSummary implements Closeable
         long start = getPositionInSummary(index);
         long end = calculateEnd(index);
         byte[] entry = new byte[(int)(end - start)];
-        bytes.getBytes(start, entry, 0, (int)(end - start));
+        bytes.getBytes(start, entry, 0, (int) (end - start));
         return entry;
     }
 
@@ -206,6 +214,11 @@ public class IndexSummary implements Closeable
         return Downsampling.getEffectiveIndexIntervalAfterIndex(index, samplingLevel, minIndexInterval);
     }
 
+    public IndexSummary sharedCopy()
+    {
+        return new IndexSummary(this);
+    }
+
     public static class IndexSummarySerializer
     {
         public void serialize(IndexSummary t, DataOutputPlus out, boolean withSamplingLevel) throws IOException
@@ -256,16 +269,4 @@ public class IndexSummary implements Closeable
             return new IndexSummary(partitioner, memory, summarySize, fullSamplingSummarySize, minIndexInterval, samplingLevel);
         }
     }
-
-    @Override
-    public void close()
-    {
-        bytes.unreference();
-    }
-
-    public IndexSummary readOnlyClone()
-    {
-        bytes.reference();
-        return this;
-    }
 }
diff --git a/src/java/org/apache/cassandra/io/sstable/IndexSummaryBuilder.java b/src/java/org/apache/cassandra/io/sstable/IndexSummaryBuilder.java
index 8e9cc308b4..df326d7cde 100644
--- a/src/java/org/apache/cassandra/io/sstable/IndexSummaryBuilder.java
+++ b/src/java/org/apache/cassandra/io/sstable/IndexSummaryBuilder.java
@@ -28,6 +28,7 @@ import org.apache.cassandra.cache.RefCountedMemory;
 import org.apache.cassandra.db.DecoratedKey;
 import org.apache.cassandra.db.TypeSizes;
 import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.io.util.Memory;
 
 import static org.apache.cassandra.io.sstable.Downsampling.BASE_SAMPLING_LEVEL;
 import static org.apache.cassandra.io.sstable.SSTable.getMinimalKey;
@@ -148,7 +149,7 @@ public class IndexSummaryBuilder
 
         // first we write out the position in the *summary* for each key in the summary,
         // then we write out (key, actual index position) pairs
-        RefCountedMemory memory = new RefCountedMemory(offheapSize + (length * 4));
+        Memory memory = Memory.allocate(offheapSize + (length * 4));
         int idxPosition = 0;
         int keyPosition = length * 4;
         for (int i = 0; i < length; i++)
diff --git a/src/java/org/apache/cassandra/io/sstable/IndexSummaryManager.java b/src/java/org/apache/cassandra/io/sstable/IndexSummaryManager.java
index 65b25a42a9..4144c32099 100644
--- a/src/java/org/apache/cassandra/io/sstable/IndexSummaryManager.java
+++ b/src/java/org/apache/cassandra/io/sstable/IndexSummaryManager.java
@@ -266,9 +266,9 @@ public class IndexSummaryManager implements IndexSummaryManagerMBean
         double totalReadsPerSec = 0.0;
         for (SSTableReader sstable : nonCompacting)
         {
-            if (sstable.readMeter != null)
+            if (sstable.getReadMeter() != null)
             {
-                Double readRate = sstable.readMeter.fifteenMinuteRate();
+                Double readRate = sstable.getReadMeter().fifteenMinuteRate();
                 totalReadsPerSec += readRate;
                 readRates.put(sstable, readRate);
             }
@@ -314,7 +314,7 @@ public class IndexSummaryManager implements IndexSummaryManagerMBean
             int minIndexInterval = sstable.metadata.getMinIndexInterval();
             int maxIndexInterval = sstable.metadata.getMaxIndexInterval();
 
-            double readsPerSec = sstable.readMeter == null ? 0.0 : sstable.readMeter.fifteenMinuteRate();
+            double readsPerSec = sstable.getReadMeter() == null ? 0.0 : sstable.getReadMeter().fifteenMinuteRate();
             long idealSpace = Math.round(remainingSpace * (readsPerSec / totalReadsPerSec));
 
             // figure out how many entries our idealSpace would buy us, and pick a new sampling level based on that
diff --git a/src/java/org/apache/cassandra/io/sstable/SSTableDeletingTask.java b/src/java/org/apache/cassandra/io/sstable/SSTableDeletingTask.java
index fb1cbb3c45..3da6906350 100644
--- a/src/java/org/apache/cassandra/io/sstable/SSTableDeletingTask.java
+++ b/src/java/org/apache/cassandra/io/sstable/SSTableDeletingTask.java
@@ -40,31 +40,42 @@ public class SSTableDeletingTask implements Runnable
     // and delete will fail (on Windows) until it is (we only force the unmapping on SUN VMs).
     // Additionally, we need to make sure to delete the data file first, so on restart the others
     // will be recognized as GCable.
-    private static final Set<SSTableDeletingTask> failedTasks = new CopyOnWriteArraySet<SSTableDeletingTask>();
+    private static final Set<SSTableDeletingTask> failedTasks = new CopyOnWriteArraySet<>();
 
     private final SSTableReader referent;
     private final Descriptor desc;
     private final Set<Component> components;
     private DataTracker tracker;
 
-    public SSTableDeletingTask(SSTableReader referent)
+    /**
+     * realDescriptor is the actual descriptor for the sstable, the descriptor inside
+     * referent can be 'faked' as FINAL for early opened files. We need the real one
+     * to be able to remove the files.
+     */
+    public SSTableDeletingTask(Descriptor realDescriptor, SSTableReader referent)
     {
         this.referent = referent;
-        if (referent.openReason == SSTableReader.OpenReason.EARLY)
+        this.desc = realDescriptor;
+        switch (desc.type)
         {
-            this.desc = referent.descriptor.asType(Descriptor.Type.TEMPLINK);
-            this.components = Sets.newHashSet(Component.DATA, Component.PRIMARY_INDEX);
-        }
-        else
-        {
-            this.desc = referent.descriptor;
-            this.components = referent.components;
+            case FINAL:
+                this.components = referent.components;
+                break;
+            case TEMPLINK:
+                this.components = Sets.newHashSet(Component.DATA, Component.PRIMARY_INDEX);
+                break;
+            default:
+                throw new IllegalStateException();
         }
     }
 
     public void setTracker(DataTracker tracker)
     {
-        this.tracker = tracker;
+        // the tracker is used only to notify listeners of deletion of the sstable;
+        // since deletion of a non-final file is not really deletion of the sstable,
+        // we don't want to notify the listeners in this event
+        if (desc.type == Descriptor.Type.FINAL)
+            this.tracker = tracker;
     }
 
     public void schedule()
@@ -79,9 +90,6 @@ public class SSTableDeletingTask implements Runnable
         if (tracker != null)
             tracker.notifyDeleting(referent);
 
-        if (referent.readMeter != null)
-            SystemKeyspace.clearSSTableReadMeter(referent.getKeyspaceName(), referent.getColumnFamilyName(), referent.descriptor.generation);
-
         // If we can't successfully delete the DATA component, set the task to be retried later: see above
         File datafile = new File(desc.filenameFor(Component.DATA));
         if (!datafile.delete())
diff --git a/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java b/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
index 06f71d802e..cd23ae2e31 100644
--- a/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
+++ b/src/java/org/apache/cassandra/io/sstable/SSTableLoader.java
@@ -209,8 +209,8 @@ public class SSTableLoader implements StreamEventHandler
     {
         for (SSTableReader sstable : sstables)
         {
-            sstable.sharedRef().release();
-            assert sstable.sharedRef().globalCount() == 0;
+            sstable.selfRef().release();
+            assert sstable.selfRef().globalCount() == 0;
         }
     }
 
diff --git a/src/java/org/apache/cassandra/io/sstable/SSTableReader.java b/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
index f34939a6f9..a28eb44e81 100644
--- a/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
+++ b/src/java/org/apache/cassandra/io/sstable/SSTableReader.java
@@ -36,11 +36,7 @@ import java.util.Iterator;
 import java.util.List;
 import java.util.Map;
 import java.util.Set;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.LinkedBlockingQueue;
-import java.util.concurrent.ScheduledFuture;
-import java.util.concurrent.ScheduledThreadPoolExecutor;
-import java.util.concurrent.TimeUnit;
+import java.util.concurrent.*;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.atomic.AtomicLong;
 
@@ -118,10 +114,62 @@ import org.apache.cassandra.utils.concurrent.RefCounted;
 import static org.apache.cassandra.db.Directories.SECONDARY_INDEX_NAME_SEPARATOR;
 
 /**
- * SSTableReaders are open()ed by Keyspace.onStart; after that they are created by SSTableWriter.renameAndOpen.
- * Do not re-call open() on existing SSTable files; use the references kept by ColumnFamilyStore post-start instead.
+ * An SSTableReader can be constructed in a number of places, but typically is either
+ * read from disk at startup, or constructed from a flushed memtable, or after compaction
+ * to replace some existing sstables. However once created, an sstablereader may also be modified.
+ *
+ * A reader's OpenReason describes its current stage in its lifecycle, as follows:
+ *
+ * NORMAL
+ * From:       None        => Reader has been read from disk, either at startup or from a flushed memtable
+ *             EARLY       => Reader is the final result of a compaction
+ *             MOVED_START => Reader WAS being compacted, but this failed and it has been restored to NORMAL status
+ *
+ * EARLY
+ * From:       None        => Reader is a compaction replacement that is either incomplete and has been opened
+ *                            to represent its partial result status, or has been finished but the compaction
+ *                            it is a part of has not yet completed fully
+ *             EARLY       => Same as from None, only it is not the first time it has been
+ *
+ * MOVED_START
+ * From:       NORMAL      => Reader is being compacted. This compaction has not finished, but the compaction result
+ *                            is either partially or fully opened, to either partially or fully replace this reader.
+ *                            This reader's start key has been updated to represent this, so that reads only hit
+ *                            one or the other reader.
+ *
+ * METADATA_CHANGE
+ * From:       NORMAL      => Reader has seen low traffic and the amount of memory available for index summaries is
+ *                            constrained, so its index summary has been downsampled.
+ *         METADATA_CHANGE => Same
+ *
+ * Note that in parallel to this, there are two different Descriptor types; TMPLINK and FINAL; the latter corresponds
+ * to NORMAL state readers and all readers that replace a NORMAL one. TMPLINK is used for EARLY state readers and
+ * no others.
+ *
+ * When a reader is being compacted, if the result is large its replacement may be opened as EARLY before compaction
+ * completes in order to present the result to consumers earlier. In this case the reader will itself be changed to
+ * a MOVED_START state, where its start no longer represents its on-disk minimum key. This is to permit reads to be
+ * directed to only one reader when the two represent the same data. The EARLY file can represent a compaction result
+ * that is either partially complete and still in-progress, or a complete and immutable sstable that is part of a larger
+ * macro compaction action that has not yet fully completed.
+ *
+ * Currently ALL compaction results at least briefly go through an EARLY open state prior to completion, regardless
+ * of if early opening is enabled.
+ *
+ * Since a reader can be created multiple times over the same shared underlying resources, and the exact resources
+ * it shares between each instance differ subtly, we track the lifetime of any underlying resource with its own
+ * reference count, which each instance takes a Ref to. Each instance then tracks references to itself, and once these
+ * all expire it releases its Refs to these underlying resources.
+ *
+ * There is some shared cleanup behaviour needed only once all sstablereaders in a certain stage of their lifecycle
+ * (i.e. EARLY or NORMAL opening), and some that must only occur once all readers of any kind over a single logical
+ * sstable have expired. These are managed by the TypeTidy and GlobalTidy classes at the bottom, and are effectively
+ * managed as another resource each instance tracks its own Ref instance to, to ensure all of these resources are
+ * cleaned up safely and can be debugged otherwise.
+ *
+ * TODO: fill in details about DataTracker and lifecycle interactions for tools, and for compaction strategies
  */
-public class SSTableReader extends SSTable implements RefCounted
+public class SSTableReader extends SSTable implements RefCounted<SSTableReader>
 {
     private static final Logger logger = LoggerFactory.getLogger(SSTableReader.class);
 
@@ -166,7 +214,8 @@ public class SSTableReader extends SSTable implements RefCounted
     {
         NORMAL,
         EARLY,
-        METADATA_CHANGE
+        METADATA_CHANGE,
+        MOVED_START
     }
 
     public final OpenReason openReason;
@@ -174,7 +223,6 @@ public class SSTableReader extends SSTable implements RefCounted
     // indexfile and datafile: might be null before a call to load()
     private SegmentedFile ifile;
     private SegmentedFile dfile;
-
     private IndexSummary indexSummary;
     private IFilter bf;
 
@@ -184,8 +232,7 @@ public class SSTableReader extends SSTable implements RefCounted
 
     // technically isCompacted is not necessary since it should never be unreferenced unless it is also compacted,
     // but it seems like a good extra layer of protection against reference counting bugs to not delete data based on that alone
-    private final AtomicBoolean isCompacted = new AtomicBoolean(false);
-    private final AtomicBoolean isSuspect = new AtomicBoolean(false);
+    private AtomicBoolean isSuspect = new AtomicBoolean(false);
 
     // not final since we need to be able to change level on a file.
     private volatile StatsMetadata sstableMetadata;
@@ -193,12 +240,10 @@ public class SSTableReader extends SSTable implements RefCounted
     private final AtomicLong keyCacheHit = new AtomicLong(0);
     private final AtomicLong keyCacheRequest = new AtomicLong(0);
 
-    private final Tidier tidy = new Tidier();
-    private final RefCounted refCounted = RefCounted.Impl.get(tidy);
+    private final InstanceTidier tidy = new InstanceTidier(descriptor, metadata);
+    private final Ref<SSTableReader> selfRef = new Ref<>(this, tidy);
 
-    @VisibleForTesting
-    public RestorableMeter readMeter;
-    private ScheduledFuture readMeterSyncFuture;
+    private RestorableMeter readMeter;
 
     /**
      * Calculate approximate key count.
@@ -399,7 +444,7 @@ public class SSTableReader extends SSTable implements RefCounted
         sstable.ifile = ibuilder.complete(sstable.descriptor.filenameFor(Component.PRIMARY_INDEX));
         sstable.dfile = dbuilder.complete(sstable.descriptor.filenameFor(Component.DATA));
         sstable.bf = FilterFactory.AlwaysPresent;
-        sstable.tidy.setup(sstable);
+        sstable.setup();
         return sstable;
     }
 
@@ -443,13 +488,13 @@ public class SSTableReader extends SSTable implements RefCounted
         sstable.load(validationMetadata);
         logger.debug("INDEX LOAD TIME for {}: {} ms.", descriptor, TimeUnit.NANOSECONDS.toMillis(System.nanoTime() - start));
 
+        sstable.setup();
         if (validate)
             sstable.validate();
 
         if (sstable.getKeyCache() != null)
             logger.debug("key cache contains {}/{} keys", sstable.getKeyCache().size(), sstable.getKeyCache().getCapacity());
 
-        sstable.tidy.setup(sstable);
         return sstable;
     }
 
@@ -545,32 +590,6 @@ public class SSTableReader extends SSTable implements RefCounted
         this.sstableMetadata = sstableMetadata;
         this.maxDataAge = maxDataAge;
         this.openReason = openReason;
-
-        tidy.deletingTask = new SSTableDeletingTask(this);
-
-        // Don't track read rates for tables in the system keyspace and don't bother trying to load or persist
-        // the read meter when in client mode.  Also don't track reads for special operations (like early open)
-        // this is to avoid overflowing the executor queue (see CASSANDRA-8066)
-        if (Keyspace.SYSTEM_KS.equals(desc.ksname) || Config.isClientMode() || openReason != OpenReason.NORMAL)
-        {
-            readMeter = null;
-            readMeterSyncFuture = null;
-            return;
-        }
-
-        readMeter = SystemKeyspace.getSSTableReadMeter(desc.ksname, desc.cfname, desc.generation);
-        // sync the average read rate to system.sstable_activity every five minutes, starting one minute from now
-        readMeterSyncFuture = syncExecutor.scheduleAtFixedRate(new Runnable()
-        {
-            public void run()
-            {
-                if (!isCompacted.get())
-                {
-                    meterSyncThrottle.acquire();
-                    SystemKeyspace.persistSSTableReadMeter(desc.ksname, desc.cfname, desc.generation, readMeter);
-                }
-            }
-        }, 1, 5, TimeUnit.MINUTES);
     }
 
     private SSTableReader(Descriptor desc,
@@ -586,12 +605,11 @@ public class SSTableReader extends SSTable implements RefCounted
                           OpenReason openReason)
     {
         this(desc, components, metadata, partitioner, maxDataAge, sstableMetadata, openReason);
-
         this.ifile = ifile;
         this.dfile = dfile;
         this.indexSummary = indexSummary;
         this.bf = bloomFilter;
-        tidy.setup(this);
+        this.setup();
     }
 
     public static long getTotalBytes(Iterable<SSTableReader> sstables)
@@ -626,7 +644,7 @@ public class SSTableReader extends SSTable implements RefCounted
 
     public void setTrackedBy(DataTracker tracker)
     {
-        tidy.deletingTask.setTracker(tracker);
+        tidy.type.deletingTask.setTracker(tracker);
         // under normal operation we can do this at any time, but SSTR is also used outside C* proper,
         // e.g. by BulkLoader, which does not initialize the cache.  As a kludge, we set up the cache
         // here when we know we're being wired into the rest of the server infrastructure.
@@ -698,7 +716,6 @@ public class SSTableReader extends SSTable implements RefCounted
         dfile = dbuilder.complete(descriptor.filenameFor(Component.DATA));
         if (saveSummaryIfCreated && (recreateBloomFilter || !summaryLoaded)) // save summary information to disk
             saveSummary(ibuilder, dbuilder);
-        tidy.setup(this);
     }
 
     /**
@@ -792,6 +809,8 @@ public class SSTableReader extends SSTable implements RefCounted
         }
         catch (IOException e)
         {
+            if (indexSummary != null)
+                indexSummary.close();
             logger.debug("Cannot deserialize SSTable Summary File {}: {}", summariesFile.getPath(), e.getMessage());
             // corrupted; delete it and fall back to creating a new summary
             FileUtils.closeQuietly(iStream);
@@ -850,20 +869,21 @@ public class SSTableReader extends SSTable implements RefCounted
 
     public void setReplacedBy(SSTableReader replacement)
     {
-        synchronized (tidy.replaceLock)
+        synchronized (tidy.global)
         {
-            assert tidy.replacedBy == null;
-            tidy.replacedBy = replacement;
-            replacement.tidy.replaces = this;
-            replacement.tidy.replaceLock = tidy.replaceLock;
+            assert replacement != null;
+            assert !tidy.isReplaced;
+            assert tidy.global.live == this;
+            tidy.isReplaced = true;
+            tidy.global.live = replacement;
         }
     }
 
     public SSTableReader cloneWithNewStart(DecoratedKey newStart, final Runnable runOnClose)
     {
-        synchronized (tidy.replaceLock)
+        synchronized (tidy.global)
         {
-            assert tidy.replacedBy == null;
+            assert openReason != OpenReason.EARLY;
 
             if (newStart.compareTo(this.first) > 0)
             {
@@ -895,10 +915,9 @@ public class SSTableReader extends SSTable implements RefCounted
                 }
             }
 
-            SSTableReader replacement = new SSTableReader(descriptor, components, metadata, partitioner, ifile, dfile, indexSummary.readOnlyClone(), bf, maxDataAge, sstableMetadata,
-                    openReason == OpenReason.EARLY ? openReason : OpenReason.METADATA_CHANGE);
-            replacement.readMeterSyncFuture = this.readMeterSyncFuture;
-            replacement.readMeter = this.readMeter;
+            SSTableReader replacement = new SSTableReader(descriptor, components, metadata, partitioner, ifile.sharedCopy(),
+                                                          dfile.sharedCopy(), indexSummary.sharedCopy(), bf.sharedCopy(),
+                                                          maxDataAge, sstableMetadata, OpenReason.MOVED_START);
             replacement.first = this.last.compareTo(newStart) > 0 ? newStart : this.last;
             replacement.last = this.last;
             setReplacedBy(replacement);
@@ -916,9 +935,9 @@ public class SSTableReader extends SSTable implements RefCounted
      */
     public SSTableReader cloneWithNewSummarySamplingLevel(ColumnFamilyStore parent, int samplingLevel) throws IOException
     {
-        synchronized (tidy.replaceLock)
+        synchronized (tidy.global)
         {
-            assert tidy.replacedBy == null;
+            assert openReason != OpenReason.EARLY;
 
             int minIndexInterval = metadata.getMinIndexInterval();
             int maxIndexInterval = metadata.getMaxIndexInterval();
@@ -957,10 +976,9 @@ public class SSTableReader extends SSTable implements RefCounted
             StorageMetrics.load.inc(newSize - oldSize);
             parent.metric.liveDiskSpaceUsed.inc(newSize - oldSize);
 
-            SSTableReader replacement = new SSTableReader(descriptor, components, metadata, partitioner, ifile, dfile, newSummary, bf, maxDataAge, sstableMetadata,
-                    openReason == OpenReason.EARLY ? openReason : OpenReason.METADATA_CHANGE);
-            replacement.readMeterSyncFuture = this.readMeterSyncFuture;
-            replacement.readMeter = this.readMeter;
+            SSTableReader replacement = new SSTableReader(descriptor, components, metadata, partitioner, ifile.sharedCopy(),
+                                                          dfile.sharedCopy(), newSummary, bf.sharedCopy(), maxDataAge,
+                                                          sstableMetadata, OpenReason.METADATA_CHANGE);
             replacement.first = this.first;
             replacement.last = this.last;
             setReplacedBy(replacement);
@@ -992,6 +1010,11 @@ public class SSTableReader extends SSTable implements RefCounted
         }
     }
 
+    public RestorableMeter getReadMeter()
+    {
+        return readMeter;
+    }
+
     public int getIndexSummarySamplingLevel()
     {
         return indexSummary.getSamplingLevel();
@@ -1014,14 +1037,17 @@ public class SSTableReader extends SSTable implements RefCounted
 
     public void releaseSummary() throws IOException
     {
-        indexSummary.close();
+        tidy.releaseSummary();
         indexSummary = null;
     }
 
     private void validate()
     {
         if (this.first.compareTo(this.last) > 0)
+        {
+            selfRef().release();
             throw new IllegalStateException(String.format("SSTable first key %s > last key %s", this.first, this.last));
+        }
     }
 
     /**
@@ -1567,16 +1593,16 @@ public class SSTableReader extends SSTable implements RefCounted
         if (logger.isDebugEnabled())
             logger.debug("Marking {} compacted", getFilename());
 
-        synchronized (tidy.replaceLock)
+        synchronized (tidy.global)
         {
-            assert tidy.replacedBy == null : getFilename();
+            assert !tidy.isReplaced;
         }
-        return !isCompacted.getAndSet(true);
+        return !tidy.global.isCompacted.getAndSet(true);
     }
 
     public boolean isMarkedCompacted()
     {
-        return isCompacted.get();
+        return tidy.global.isCompacted.get();
     }
 
     public void markSuspect()
@@ -1673,16 +1699,7 @@ public class SSTableReader extends SSTable implements RefCounted
 
     public SSTableReader getCurrentReplacement()
     {
-        synchronized (tidy.replaceLock)
-        {
-            SSTableReader cur = this, next = tidy.replacedBy;
-            while (next != null)
-            {
-                cur = next;
-                next = next.tidy.replacedBy;
-            }
-            return cur;
-        }
+        return tidy.global.live;
     }
 
     /**
@@ -1882,199 +1899,314 @@ public class SSTableReader extends SSTable implements RefCounted
         }
     }
 
-    public Ref tryRef()
+    public Ref<SSTableReader> tryRef()
     {
-        return refCounted.tryRef();
+        return selfRef.tryRef();
     }
 
-    public Ref sharedRef()
+    public Ref<SSTableReader> selfRef()
     {
-        return refCounted.sharedRef();
+        return selfRef;
     }
 
-    private static final class Tidier implements Tidy
+    public Ref<SSTableReader> ref()
     {
-        private String name;
-        private CFMetaData metadata;
-        // indexfile and datafile: might be null before a call to load()
-        private SegmentedFile ifile;
-        private SegmentedFile dfile;
+        return selfRef.ref();
+    }
 
-        private IndexSummary indexSummary;
-        private IFilter bf;
+    void setup()
+    {
+        tidy.setup(this);
+        this.readMeter = tidy.global.readMeter;
+    }
+
+    @VisibleForTesting
+    public void overrideReadMeter(RestorableMeter readMeter)
+    {
+        this.readMeter = tidy.global.readMeter = readMeter;
+    }
 
-        private AtomicBoolean isCompacted;
+    /**
+     * One instance per SSTableReader we create. This references the type-shared tidy, which in turn references
+     * the globally shared tidy, i.e.
+     *
+     * InstanceTidier => DescriptorTypeTitdy => GlobalTidy
+     *
+     * We can create many InstanceTidiers (one for every time we reopen an sstable with MOVED_START for example), but there can only be
+     * two DescriptorTypeTidy (FINAL and TEMPLINK) and only one GlobalTidy for one single logical sstable.
+     *
+     * When the InstanceTidier cleansup, it releases its reference to its DescriptorTypeTidy; when all InstanceTidiers
+     * for that type have run, the DescriptorTypeTidy cleansup. DescriptorTypeTidy behaves in the same way towards GlobalTidy.
+     *
+     * For ease, we stash a direct reference to both our type-shared and global tidier
+     */
+    private static final class InstanceTidier implements Tidy
+    {
+        private final Descriptor descriptor;
+        private final CFMetaData metadata;
+        private IFilter bf;
+        private IndexSummary summary;
 
-        /**
-         * To support replacing this sstablereader with another object that represents that same underlying sstable, but with different associated resources,
-         * we build a linked-list chain of replacement, which we synchronise using a shared object to make maintenance of the list across multiple threads simple.
-         * On close we check if any of the closeable resources differ between any chains either side of us; any that are in neither of the adjacent links (if any) are closed.
-         * Once we've made this decision we remove ourselves from the linked list, so that anybody behind/ahead will compare against only other still opened resources.
-         */
-        private Object replaceLock = new Object();
-        private SSTableReader replacedBy;
-        private SSTableReader replaces;
-        private SSTableDeletingTask deletingTask;
+        private SegmentedFile dfile;
+        private SegmentedFile ifile;
         private Runnable runOnClose;
+        private boolean isReplaced = false;
+
+        // a reference to our shared per-Descriptor.Type tidy instance, that
+        // we will release when we are ourselves released
+        private Ref<DescriptorTypeTidy> typeRef;
+
+        // a convenience stashing of the shared per-descriptor-type tidy instance itself
+        // and the per-logical-sstable globally shared state that it is linked to
+        private DescriptorTypeTidy type;
+        private GlobalTidy global;
+
+        private boolean setup;
+
+        void setup(SSTableReader reader)
+        {
+            this.setup = true;
+            this.bf = reader.bf;
+            this.summary = reader.indexSummary;
+            this.dfile = reader.dfile;
+            this.ifile = reader.ifile;
+            // get a new reference to the shared descriptor-type tidy
+            this.typeRef = DescriptorTypeTidy.get(reader);
+            this.type = typeRef.get();
+            this.global = type.globalRef.get();
+        }
 
-        @VisibleForTesting
-        public RestorableMeter readMeter;
-        private volatile ScheduledFuture readMeterSyncFuture;
+        InstanceTidier(Descriptor descriptor, CFMetaData metadata)
+        {
+            this.descriptor = descriptor;
+            this.metadata = metadata;
+        }
 
-        private void setup(SSTableReader reader)
+        public void tidy()
         {
-            name = reader.toString();
-            metadata = reader.metadata;
-            ifile = reader.ifile;
-            dfile = reader.dfile;
-            indexSummary = reader.indexSummary;
-            bf = reader.bf;
-            isCompacted = reader.isCompacted;
-            readMeterSyncFuture = reader.readMeterSyncFuture;
+            // don't try to cleanup if the sstablereader was never fully constructed
+            if (!setup)
+                return;
+
+            final ColumnFamilyStore cfs = Schema.instance.getColumnFamilyStoreInstance(metadata.cfId);
+            final OpOrder.Barrier barrier;
+            if (cfs != null)
+            {
+                barrier = cfs.readOrdering.newBarrier();
+                barrier.issue();
+            }
+            else
+                barrier = null;
+
+            ScheduledExecutors.nonPeriodicTasks.execute(new Runnable()
+            {
+                public void run()
+                {
+                    if (barrier != null)
+                        barrier.await();
+                    bf.close();
+                    dfile.close();
+                    ifile.close();
+                    if (summary != null)
+                        summary.close();
+                    if (runOnClose != null)
+                        runOnClose.run();
+                    typeRef.release();
+                }
+            });
         }
 
         public String name()
         {
-            return name;
+            return descriptor.toString();
         }
 
-        private void dropPageCache()
+        void releaseSummary()
         {
-            dropPageCache(dfile.path);
-            dropPageCache(ifile.path);
+            summary.close();
+            assert summary.isCleanedUp();
+            summary = null;
         }
+    }
+
+    /**
+     * One shared between all instances of a given Descriptor.Type.
+     * Performs only two things: the deletion of the sstables for the type,
+     * if necessary; and the shared reference to the globally shared state.
+     *
+     * All InstanceTidiers, on setup(), ask the static get() method for their shared state,
+     * and stash a reference to it to be released when they are. Once all such references are
+     * released, the shared tidy will be performed.
+     */
+    static final class DescriptorTypeTidy implements Tidy
+    {
+        // keyed by REAL descriptor (TMPLINK/FINAL), mapping to the shared DescriptorTypeTidy for that descriptor
+        static final ConcurrentMap<Descriptor, Ref<DescriptorTypeTidy>> lookup = new ConcurrentHashMap<>();
 
-        private void dropPageCache(String filePath)
+        private final Descriptor desc;
+        private final Ref<GlobalTidy> globalRef;
+        private final SSTableDeletingTask deletingTask;
+
+        DescriptorTypeTidy(Descriptor desc, SSTableReader sstable)
         {
-            RandomAccessFile file = null;
+            this.desc = desc;
+            this.deletingTask = new SSTableDeletingTask(desc, sstable);
+            // get a new reference to the shared global tidy
+            this.globalRef = GlobalTidy.get(sstable);
+        }
 
-            try
+        public void tidy()
+        {
+            lookup.remove(desc);
+            boolean isCompacted = globalRef.get().isCompacted.get();
+            globalRef.release();
+            switch (desc.type)
             {
-                file = new RandomAccessFile(filePath, "r");
+                case FINAL:
+                    if (isCompacted)
+                        deletingTask.run();
+                    break;
+                case TEMPLINK:
+                    deletingTask.run();
+                    break;
+                default:
+                    throw new IllegalStateException();
+            }
+        }
 
-                int fd = CLibrary.getfd(file.getFD());
+        public String name()
+        {
+            return desc.toString();
+        }
 
-                if (fd > 0)
-                {
-                    if (logger.isDebugEnabled())
-                        logger.debug(String.format("Dropping page cache of file %s.", filePath));
+        // get a new reference to the shared DescriptorTypeTidy for this sstable
+        public static Ref<DescriptorTypeTidy> get(SSTableReader sstable)
+        {
+            Descriptor desc = sstable.descriptor;
+            if (sstable.openReason == OpenReason.EARLY)
+                desc = desc.asType(Descriptor.Type.TEMPLINK);
+            Ref<DescriptorTypeTidy> refc = lookup.get(desc);
+            if (refc != null)
+                return refc.ref();
+            final DescriptorTypeTidy tidy = new DescriptorTypeTidy(desc, sstable);
+            refc = new Ref<>(tidy, tidy);
+            Ref<?> ex = lookup.putIfAbsent(desc, refc);
+            assert ex == null;
+            return refc;
+        }
+    }
 
-                    CLibrary.trySkipCache(fd, 0, 0);
-                }
-            }
-            catch (IOException e)
+    /**
+     * One instance per logical sstable. This both tracks shared cleanup and some shared state related
+     * to the sstable's lifecycle. All DescriptorTypeTidy instances, on construction, obtain a reference to us
+     * via our static get(). There should only ever be at most two such references extant at any one time,
+     * since only TMPLINK and FINAL type descriptors should be open as readers. When all files of both
+     * kinds have been released, this shared tidy will be performed.
+     */
+    static final class GlobalTidy implements Tidy
+    {
+        // keyed by FINAL descriptor, mapping to the shared GlobalTidy for that descriptor
+        static final ConcurrentMap<Descriptor, Ref<GlobalTidy>> lookup = new ConcurrentHashMap<>();
+
+        private final Descriptor desc;
+        // a single convenience property for getting the most recent version of an sstable, not related to tidying
+        private SSTableReader live;
+        // the readMeter that is shared between all instances of the sstable, and can be overridden in all of them
+        // at once also, for testing purposes
+        private RestorableMeter readMeter;
+        // the scheduled persistence of the readMeter, that we will cancel once all instances of this logical
+        // sstable have been released
+        private final ScheduledFuture readMeterSyncFuture;
+        // shared state managing if the logical sstable has been compacted; this is used in cleanup both here
+        // and in the FINAL type tidier
+        private final AtomicBoolean isCompacted;
+
+        GlobalTidy(final SSTableReader reader)
+        {
+            this.desc = reader.descriptor;
+            this.isCompacted = new AtomicBoolean();
+            this.live = reader;
+            // Don't track read rates for tables in the system keyspace and don't bother trying to load or persist
+            // the read meter when in client mode.
+            if (Keyspace.SYSTEM_KS.equals(desc.ksname) || Config.isClientMode())
             {
-                // we don't care if cache cleanup fails
+                readMeter = null;
+                readMeterSyncFuture = null;
+                return;
             }
-            finally
+
+            readMeter = SystemKeyspace.getSSTableReadMeter(desc.ksname, desc.cfname, desc.generation);
+            // sync the average read rate to system.sstable_activity every five minutes, starting one minute from now
+            readMeterSyncFuture = syncExecutor.scheduleAtFixedRate(new Runnable()
             {
-                FileUtils.closeQuietly(file);
-            }
+                public void run()
+                {
+                    if (!isCompacted.get())
+                    {
+                        meterSyncThrottle.acquire();
+                        SystemKeyspace.persistSSTableReadMeter(desc.ksname, desc.cfname, desc.generation, readMeter);
+                    }
+                }
+            }, 1, 5, TimeUnit.MINUTES);
         }
 
         public void tidy()
         {
+            lookup.remove(desc);
             if (readMeterSyncFuture != null)
-                readMeterSyncFuture.cancel(false);
+                readMeterSyncFuture.cancel(true);
+            if (isCompacted.get())
+                SystemKeyspace.clearSSTableReadMeter(desc.ksname, desc.cfname, desc.generation);
+            // don't ideally want to dropPageCache for the file until all instances have been released
+            dropPageCache(desc.filenameFor(Component.DATA));
+            dropPageCache(desc.filenameFor(Component.PRIMARY_INDEX));
+        }
 
-            synchronized (replaceLock)
-            {
-                boolean closeBf = true, closeSummary = true, closeFiles = true, deleteFiles = isCompacted.get();
+        public String name()
+        {
+            return desc.toString();
+        }
 
-                if (replacedBy != null)
-                {
-                    closeBf = replacedBy.bf != bf;
-                    closeSummary = replacedBy.indexSummary != indexSummary;
-                    closeFiles = replacedBy.dfile != dfile;
-                    // if the replacement sstablereader uses a different path, clean up our paths
-                    deleteFiles = !dfile.path.equals(replacedBy.dfile.path);
-                }
+        // get a new reference to the shared GlobalTidy for this sstable
+        public static Ref<GlobalTidy> get(SSTableReader sstable)
+        {
+            Descriptor descriptor = sstable.descriptor;
+            Ref<GlobalTidy> refc = lookup.get(descriptor);
+            if (refc != null)
+                return refc.ref();
+            final GlobalTidy tidy = new GlobalTidy(sstable);
+            refc = new Ref<>(tidy, tidy);
+            Ref<?> ex = lookup.putIfAbsent(descriptor, refc);
+            assert ex == null;
+            return refc;
+        }
+    }
 
-                if (replaces != null)
-                {
-                    closeBf &= replaces.bf != bf;
-                    closeSummary &= replaces.indexSummary != indexSummary;
-                    closeFiles &= replaces.dfile != dfile;
-                    deleteFiles &= !dfile.path.equals(replaces.dfile.path);
-                }
+    private static void dropPageCache(String filePath)
+    {
+        RandomAccessFile file = null;
 
-                boolean deleteAll = false;
-                if (isCompacted.get())
-                {
-                    assert replacedBy == null;
-                    if (replaces != null && !deleteFiles)
-                    {
-                        replaces.tidy.replacedBy = null;
-                        replaces.tidy.deletingTask = deletingTask;
-                        replaces.markObsolete();
-                    }
-                    else
-                    {
-                        deleteAll = true;
-                    }
-                }
-                else
-                {
-                    closeSummary &= indexSummary != null;
-                    if (replaces != null)
-                        replaces.tidy.replacedBy = replacedBy;
-                    if (replacedBy != null)
-                        replacedBy.tidy.replaces = replaces;
-                }
+        try
+        {
+            file = new RandomAccessFile(filePath, "r");
 
-                scheduleTidy(closeBf, closeSummary, closeFiles, deleteFiles, deleteAll);
-            }
-        }
+            int fd = CLibrary.getfd(file.getFD());
 
-        private void scheduleTidy(final boolean closeBf, final boolean closeSummary, final boolean closeFiles, final boolean deleteFiles, final boolean deleteAll)
-        {
-            final ColumnFamilyStore cfs = Schema.instance.getColumnFamilyStoreInstance(metadata.cfId);
-            final OpOrder.Barrier barrier;
-            if (cfs != null)
+            if (fd > 0)
             {
-                barrier = cfs.readOrdering.newBarrier();
-                barrier.issue();
-            }
-            else
-                barrier = null;
+                if (logger.isDebugEnabled())
+                    logger.debug(String.format("Dropping page cache of file %s.", filePath));
 
-            ScheduledExecutors.nonPeriodicTasks.execute(new Runnable()
-            {
-                public void run()
-                {
-                    if (barrier != null)
-                        barrier.await();
-                    if (closeBf)
-                        bf.close();
-                    if (closeSummary)
-                        indexSummary.close();
-                    if (closeFiles)
-                    {
-                        ifile.cleanup();
-                        dfile.cleanup();
-                    }
-                    if (runOnClose != null)
-                        runOnClose.run();
-                    if (deleteAll)
-                    {
-                        /**
-                         * Do the OS a favour and suggest (using fadvice call) that we
-                         * don't want to see pages of this SSTable in memory anymore.
-                         *
-                         * NOTE: We can't use madvice in java because it requires the address of
-                         * the mapping, so instead we always open a file and run fadvice(fd, 0, 0) on it
-                         */
-                        dropPageCache();
-                        deletingTask.run();
-                    }
-                    else if (deleteFiles)
-                    {
-                        FileUtils.deleteWithConfirm(new File(dfile.path));
-                        FileUtils.deleteWithConfirm(new File(ifile.path));
-                    }
-                }
-            });
+                CLibrary.trySkipCache(fd, 0, 0);
+            }
+        }
+        catch (IOException e)
+        {
+            // we don't care if cache cleanup fails
+        }
+        finally
+        {
+            FileUtils.closeQuietly(file);
         }
     }
-
 }
diff --git a/src/java/org/apache/cassandra/io/sstable/SSTableRewriter.java b/src/java/org/apache/cassandra/io/sstable/SSTableRewriter.java
index 7784b18873..6356d4d253 100644
--- a/src/java/org/apache/cassandra/io/sstable/SSTableRewriter.java
+++ b/src/java/org/apache/cassandra/io/sstable/SSTableRewriter.java
@@ -183,14 +183,14 @@ public class SSTableRewriter
         for (SSTableReader sstable : finished)
         {
             sstable.markObsolete();
-            sstable.sharedRef().release();
+            sstable.selfRef().release();
         }
 
         // abort the writers
         for (Finished finished : finishedEarly)
         {
             boolean opened = finished.reader != null;
-            finished.writer.abort(!opened);
+            finished.writer.abort();
             if (opened)
             {
                 // if we've already been opened, add ourselves to the discard pile
@@ -361,7 +361,7 @@ public class SSTableRewriter
             }
             else
             {
-                f.writer.abort(true);
+                f.writer.abort();
                 assert f.reader == null;
             }
         }
@@ -380,9 +380,9 @@ public class SSTableRewriter
         {
             for (SSTableReader reader : discard)
             {
-                if (reader.getCurrentReplacement() == null)
+                if (reader.getCurrentReplacement() == reader)
                     reader.markObsolete();
-                reader.sharedRef().release();
+                reader.selfRef().release();
             }
         }
         else
diff --git a/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java b/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
index cc60594720..d4303143e1 100644
--- a/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
+++ b/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
@@ -17,7 +17,6 @@
  */
 package org.apache.cassandra.io.sstable;
 
-import java.io.Closeable;
 import java.io.DataInput;
 import java.io.File;
 import java.io.FileOutputStream;
@@ -338,17 +337,13 @@ public class SSTableWriter extends SSTable
      * After failure, attempt to close the index writer and data file before deleting all temp components for the sstable
      */
     public void abort()
-    {
-        abort(true);
-    }
-    public void abort(boolean closeBf)
     {
         assert descriptor.type.isTemporary;
         if (iwriter == null && dataFile == null)
             return;
 
         if (iwriter != null)
-            iwriter.abort(closeBf);
+            iwriter.abort();
 
         if (dataFile!= null)
             dataFile.abort();
@@ -407,7 +402,7 @@ public class SSTableWriter extends SSTable
                                                            components, metadata,
                                                            partitioner, ifile,
                                                            dfile, iwriter.summary.build(partitioner, exclusiveUpperBoundOfReadableIndex),
-                                                           iwriter.bf, maxDataAge, sstableMetadata, SSTableReader.OpenReason.EARLY);
+                                                           iwriter.bf.sharedCopy(), maxDataAge, sstableMetadata, SSTableReader.OpenReason.EARLY);
 
         // now it's open, find the ACTUAL last readable key (i.e. for which the data file has also been flushed)
         sstable.first = getMinimalKey(first);
@@ -416,7 +411,7 @@ public class SSTableWriter extends SSTable
         if (inclusiveUpperBoundOfReadableData == null)
         {
             // Prevent leaving tmplink files on disk
-            sstable.sharedRef().release();
+            sstable.selfRef().release();
             return null;
         }
         int offset = 2;
@@ -428,7 +423,7 @@ public class SSTableWriter extends SSTable
             inclusiveUpperBoundOfReadableData = iwriter.getMaxReadableKey(offset++);
             if (inclusiveUpperBoundOfReadableData == null)
             {
-                sstable.sharedRef().release();
+                sstable.selfRef().release();
                 return null;
             }
         }
@@ -438,14 +433,17 @@ public class SSTableWriter extends SSTable
 
     public static enum FinishType
     {
-        NORMAL(SSTableReader.OpenReason.NORMAL),
-        EARLY(SSTableReader.OpenReason.EARLY), // no renaming
-        FINISH_EARLY(SSTableReader.OpenReason.NORMAL); // tidy up an EARLY finish
+        CLOSE(null, true),
+        NORMAL(SSTableReader.OpenReason.NORMAL, true),
+        EARLY(SSTableReader.OpenReason.EARLY, false), // no renaming
+        FINISH_EARLY(SSTableReader.OpenReason.NORMAL, true); // tidy up an EARLY finish
         final SSTableReader.OpenReason openReason;
 
-        FinishType(SSTableReader.OpenReason openReason)
+        public final boolean isFinal;
+        FinishType(SSTableReader.OpenReason openReason, boolean isFinal)
         {
             this.openReason = openReason;
+            this.isFinal = isFinal;
         }
     }
 
@@ -461,6 +459,7 @@ public class SSTableWriter extends SSTable
 
     public SSTableReader finish(FinishType finishType, long maxDataAge, long repairedAt)
     {
+        assert finishType != FinishType.CLOSE;
         Pair<Descriptor, StatsMetadata> p;
 
         p = close(finishType, repairedAt < 0 ? this.repairedAt : repairedAt);
@@ -480,16 +479,16 @@ public class SSTableWriter extends SSTable
                                                            ifile,
                                                            dfile,
                                                            iwriter.summary.build(partitioner),
-                                                           iwriter.bf,
+                                                           iwriter.bf.sharedCopy(),
                                                            maxDataAge,
                                                            metadata,
                                                            finishType.openReason);
         sstable.first = getMinimalKey(first);
         sstable.last = getMinimalKey(last);
 
-        switch (finishType)
+        if (finishType.isFinal)
         {
-            case NORMAL: case FINISH_EARLY:
+            iwriter.bf.close();
             // try to save the summaries to disk
             sstable.saveSummary(iwriter.builder, dbuilder);
             iwriter = null;
@@ -501,16 +500,18 @@ public class SSTableWriter extends SSTable
     // Close the writer and return the descriptor to the new sstable and it's metadata
     public Pair<Descriptor, StatsMetadata> close()
     {
-        return close(FinishType.NORMAL, this.repairedAt);
+        return close(FinishType.CLOSE, this.repairedAt);
     }
 
     private Pair<Descriptor, StatsMetadata> close(FinishType type, long repairedAt)
     {
         switch (type)
         {
-            case EARLY: case NORMAL:
+            case EARLY: case CLOSE: case NORMAL:
             iwriter.close();
             dataFile.close();
+            if (type == FinishType.CLOSE)
+                iwriter.bf.close();
         }
 
         // write sstable statistics
@@ -521,9 +522,8 @@ public class SSTableWriter extends SSTable
 
         // remove the 'tmp' marker from all components
         Descriptor descriptor = this.descriptor;
-        switch (type)
+        if (type.isFinal)
         {
-            case NORMAL: case FINISH_EARLY:
             dataFile.writeFullChecksum(descriptor);
             writeMetadata(descriptor, metadataComponents);
             // save the table of components
@@ -629,11 +629,10 @@ public class SSTableWriter extends SSTable
             builder.addPotentialBoundary(indexPosition);
         }
 
-        public void abort(boolean closeBf)
+        public void abort()
         {
             indexFile.abort();
-            if (closeBf)
-                bf.close();
+            bf.close();
         }
 
         /**
diff --git a/src/java/org/apache/cassandra/io/util/BufferedPoolingSegmentedFile.java b/src/java/org/apache/cassandra/io/util/BufferedPoolingSegmentedFile.java
index 57f465f4dd..8334965305 100644
--- a/src/java/org/apache/cassandra/io/util/BufferedPoolingSegmentedFile.java
+++ b/src/java/org/apache/cassandra/io/util/BufferedPoolingSegmentedFile.java
@@ -25,7 +25,17 @@ public class BufferedPoolingSegmentedFile extends PoolingSegmentedFile
 {
     public BufferedPoolingSegmentedFile(String path, long length)
     {
-        super(path, length);
+        super(new Cleanup(path), path, length);
+    }
+
+    private BufferedPoolingSegmentedFile(BufferedPoolingSegmentedFile copy)
+    {
+        super(copy);
+    }
+
+    public BufferedPoolingSegmentedFile sharedCopy()
+    {
+        return new BufferedPoolingSegmentedFile(this);
     }
 
     public static class Builder extends SegmentedFile.Builder
diff --git a/src/java/org/apache/cassandra/io/util/BufferedSegmentedFile.java b/src/java/org/apache/cassandra/io/util/BufferedSegmentedFile.java
index 2f715daeef..c29bbf3128 100644
--- a/src/java/org/apache/cassandra/io/util/BufferedSegmentedFile.java
+++ b/src/java/org/apache/cassandra/io/util/BufferedSegmentedFile.java
@@ -20,12 +20,30 @@ package org.apache.cassandra.io.util;
 import java.io.File;
 
 import org.apache.cassandra.io.sstable.SSTableWriter;
+import org.apache.cassandra.utils.concurrent.SharedCloseable;
 
 public class BufferedSegmentedFile extends SegmentedFile
 {
     public BufferedSegmentedFile(String path, long length)
     {
-        super(path, length);
+        super(new Cleanup(path), path, length);
+    }
+
+    private BufferedSegmentedFile(BufferedSegmentedFile copy)
+    {
+        super(copy);
+    }
+
+    private static class Cleanup extends SegmentedFile.Cleanup
+    {
+        protected Cleanup(String path)
+        {
+            super(path);
+        }
+        public void tidy() throws Exception
+        {
+
+        }
     }
 
     public static class Builder extends SegmentedFile.Builder
@@ -49,7 +67,8 @@ public class BufferedSegmentedFile extends SegmentedFile
         return reader;
     }
 
-    public void cleanup()
+    public BufferedSegmentedFile sharedCopy()
     {
+        return new BufferedSegmentedFile(this);
     }
 }
diff --git a/src/java/org/apache/cassandra/io/util/CompressedPoolingSegmentedFile.java b/src/java/org/apache/cassandra/io/util/CompressedPoolingSegmentedFile.java
index 11d091a2f3..94d23bf191 100644
--- a/src/java/org/apache/cassandra/io/util/CompressedPoolingSegmentedFile.java
+++ b/src/java/org/apache/cassandra/io/util/CompressedPoolingSegmentedFile.java
@@ -28,10 +28,31 @@ public class CompressedPoolingSegmentedFile extends PoolingSegmentedFile impleme
 
     public CompressedPoolingSegmentedFile(String path, CompressionMetadata metadata)
     {
-        super(path, metadata.dataLength, metadata.compressedFileLength);
+        super(new Cleanup(path, metadata), path, metadata.dataLength, metadata.compressedFileLength);
         this.metadata = metadata;
     }
 
+    private CompressedPoolingSegmentedFile(CompressedPoolingSegmentedFile copy)
+    {
+        super(copy);
+        this.metadata = copy.metadata;
+    }
+
+    protected static final class Cleanup extends PoolingSegmentedFile.Cleanup
+    {
+        final CompressionMetadata metadata;
+        protected Cleanup(String path, CompressionMetadata metadata)
+        {
+            super(path);
+            this.metadata = metadata;
+        }
+        public void tidy() throws Exception
+        {
+            super.tidy();
+            metadata.close();
+        }
+    }
+
     public static class Builder extends CompressedSegmentedFile.Builder
     {
         public Builder(CompressedSequentialWriter writer)
@@ -59,10 +80,8 @@ public class CompressedPoolingSegmentedFile extends PoolingSegmentedFile impleme
         return metadata;
     }
 
-    @Override
-    public void cleanup()
+    public CompressedPoolingSegmentedFile sharedCopy()
     {
-        super.cleanup();
-        metadata.close();
+        return new CompressedPoolingSegmentedFile(this);
     }
 }
diff --git a/src/java/org/apache/cassandra/io/util/CompressedSegmentedFile.java b/src/java/org/apache/cassandra/io/util/CompressedSegmentedFile.java
index b788715f95..0c20bb917e 100644
--- a/src/java/org/apache/cassandra/io/util/CompressedSegmentedFile.java
+++ b/src/java/org/apache/cassandra/io/util/CompressedSegmentedFile.java
@@ -28,10 +28,35 @@ public class CompressedSegmentedFile extends SegmentedFile implements ICompresse
 
     public CompressedSegmentedFile(String path, CompressionMetadata metadata)
     {
-        super(path, metadata.dataLength, metadata.compressedFileLength);
+        super(new Cleanup(path, metadata), path, metadata.dataLength, metadata.compressedFileLength);
         this.metadata = metadata;
     }
 
+    private CompressedSegmentedFile(CompressedSegmentedFile copy)
+    {
+        super(copy);
+        this.metadata = copy.metadata;
+    }
+
+    private static final class Cleanup extends SegmentedFile.Cleanup
+    {
+        final CompressionMetadata metadata;
+        protected Cleanup(String path, CompressionMetadata metadata)
+        {
+            super(path);
+            this.metadata = metadata;
+        }
+        public void tidy() throws Exception
+        {
+            metadata.close();
+        }
+    }
+
+    public CompressedSegmentedFile sharedCopy()
+    {
+        return new CompressedSegmentedFile(this);
+    }
+
     public static class Builder extends SegmentedFile.Builder
     {
         protected final CompressedSequentialWriter writer;
@@ -70,9 +95,4 @@ public class CompressedSegmentedFile extends SegmentedFile implements ICompresse
     {
         return metadata;
     }
-
-    public void cleanup()
-    {
-        metadata.close();
-    }
 }
diff --git a/src/java/org/apache/cassandra/io/util/Memory.java b/src/java/org/apache/cassandra/io/util/Memory.java
index 53064335a9..3874669a4d 100644
--- a/src/java/org/apache/cassandra/io/util/Memory.java
+++ b/src/java/org/apache/cassandra/io/util/Memory.java
@@ -17,6 +17,7 @@
  */
 package org.apache.cassandra.io.util;
 
+import java.io.Closeable;
 import java.nio.ByteBuffer;
 import java.nio.ByteOrder;
 
@@ -30,7 +31,7 @@ import sun.nio.ch.DirectBuffer;
 /**
  * An off-heap region of memory that must be manually free'd when no longer needed.
  */
-public class Memory
+public class Memory implements AutoCloseable
 {
     private static final Unsafe unsafe = NativeAllocator.unsafe;
     private static final IAllocator allocator = DatabaseDescriptor.getoffHeapMemoryAllocator();
@@ -302,6 +303,11 @@ public class Memory
         peer = 0;
     }
 
+    public void close() throws Exception
+    {
+        free();
+    }
+
     public long size()
     {
         assert peer != 0;
diff --git a/src/java/org/apache/cassandra/io/util/MmappedSegmentedFile.java b/src/java/org/apache/cassandra/io/util/MmappedSegmentedFile.java
index 3b2cc9886e..8b4ae9d94f 100644
--- a/src/java/org/apache/cassandra/io/util/MmappedSegmentedFile.java
+++ b/src/java/org/apache/cassandra/io/util/MmappedSegmentedFile.java
@@ -47,10 +47,21 @@ public class MmappedSegmentedFile extends SegmentedFile
 
     public MmappedSegmentedFile(String path, long length, Segment[] segments)
     {
-        super(path, length);
+        super(new Cleanup(path, segments), path, length);
         this.segments = segments;
     }
 
+    private MmappedSegmentedFile(MmappedSegmentedFile copy)
+    {
+        super(copy);
+        this.segments = copy.segments;
+    }
+
+    public MmappedSegmentedFile sharedCopy()
+    {
+        return new MmappedSegmentedFile(this);
+    }
+
     /**
      * @return The segment entry for the given position.
      */
@@ -85,31 +96,41 @@ public class MmappedSegmentedFile extends SegmentedFile
         return file;
     }
 
-    public void cleanup()
+    private static final class Cleanup extends SegmentedFile.Cleanup
     {
-        if (!FileUtils.isCleanerAvailable())
-            return;
+        final Segment[] segments;
+        protected Cleanup(String path, Segment[] segments)
+        {
+            super(path);
+            this.segments = segments;
+        }
+
+        public void tidy()
+        {
+            if (!FileUtils.isCleanerAvailable())
+                return;
 
         /*
          * Try forcing the unmapping of segments using undocumented unsafe sun APIs.
          * If this fails (non Sun JVM), we'll have to wait for the GC to finalize the mapping.
          * If this works and a thread tries to access any segment, hell will unleash on earth.
          */
-        try
-        {
-            for (Segment segment : segments)
+            try
             {
-                if (segment.right == null)
-                    continue;
-                FileUtils.clean(segment.right);
+                for (Segment segment : segments)
+                {
+                    if (segment.right == null)
+                        continue;
+                    FileUtils.clean(segment.right);
+                }
+                logger.debug("All segments have been unmapped successfully");
+            }
+            catch (Exception e)
+            {
+                JVMStabilityInspector.inspectThrowable(e);
+                // This is not supposed to happen
+                logger.error("Error while unmapping segments", e);
             }
-            logger.debug("All segments have been unmapped successfully");
-        }
-        catch (Exception e)
-        {
-            JVMStabilityInspector.inspectThrowable(e);
-            // This is not supposed to happen
-            logger.error("Error while unmapping segments", e);
         }
     }
 
diff --git a/src/java/org/apache/cassandra/io/util/PoolingSegmentedFile.java b/src/java/org/apache/cassandra/io/util/PoolingSegmentedFile.java
index 01f4e31de1..daca22f23a 100644
--- a/src/java/org/apache/cassandra/io/util/PoolingSegmentedFile.java
+++ b/src/java/org/apache/cassandra/io/util/PoolingSegmentedFile.java
@@ -21,15 +21,35 @@ import org.apache.cassandra.service.FileCacheService;
 
 public abstract class PoolingSegmentedFile extends SegmentedFile
 {
-    final FileCacheService.CacheKey cacheKey = new FileCacheService.CacheKey();
-    protected PoolingSegmentedFile(String path, long length)
+    final FileCacheService.CacheKey cacheKey;
+    protected PoolingSegmentedFile(Cleanup cleanup, String path, long length)
     {
-        super(path, length);
+        this(cleanup, path, length, length);
     }
 
-    protected PoolingSegmentedFile(String path, long length, long onDiskLength)
+    protected PoolingSegmentedFile(Cleanup cleanup, String path, long length, long onDiskLength)
     {
-        super(path, length, onDiskLength);
+        super(cleanup, path, length, onDiskLength);
+        cacheKey = cleanup.cacheKey;
+    }
+
+    public PoolingSegmentedFile(PoolingSegmentedFile copy)
+    {
+        super(copy);
+        cacheKey = copy.cacheKey;
+    }
+
+    protected static class Cleanup extends SegmentedFile.Cleanup
+    {
+        final FileCacheService.CacheKey cacheKey = new FileCacheService.CacheKey();
+        protected Cleanup(String path)
+        {
+            super(path);
+        }
+        public void tidy() throws Exception
+        {
+            FileCacheService.instance.invalidate(cacheKey, path);
+        }
     }
 
     public FileDataInput getSegment(long position)
@@ -49,9 +69,4 @@ public abstract class PoolingSegmentedFile extends SegmentedFile
     {
         FileCacheService.instance.put(cacheKey, reader);
     }
-
-    public void cleanup()
-    {
-        FileCacheService.instance.invalidate(cacheKey, path);
-    }
 }
diff --git a/src/java/org/apache/cassandra/io/util/SegmentedFile.java b/src/java/org/apache/cassandra/io/util/SegmentedFile.java
index badae560e9..d557b726d0 100644
--- a/src/java/org/apache/cassandra/io/util/SegmentedFile.java
+++ b/src/java/org/apache/cassandra/io/util/SegmentedFile.java
@@ -31,6 +31,8 @@ import org.apache.cassandra.io.FSReadError;
 import org.apache.cassandra.io.compress.CompressedSequentialWriter;
 import org.apache.cassandra.io.sstable.SSTableWriter;
 import org.apache.cassandra.utils.Pair;
+import org.apache.cassandra.utils.concurrent.RefCounted;
+import org.apache.cassandra.utils.concurrent.SharedCloseableImpl;
 
 /**
  * Abstracts a read-only file that has been split into segments, each of which can be represented by an independent
@@ -41,7 +43,7 @@ import org.apache.cassandra.utils.Pair;
  * would need to be longer than 2GB, that segment will not be mmap'd, and a new RandomAccessFile will be created for
  * each access to that segment.
  */
-public abstract class SegmentedFile
+public abstract class SegmentedFile extends SharedCloseableImpl
 {
     public final String path;
     public final long length;
@@ -53,18 +55,43 @@ public abstract class SegmentedFile
     /**
      * Use getBuilder to get a Builder to construct a SegmentedFile.
      */
-    SegmentedFile(String path, long length)
+    SegmentedFile(Cleanup cleanup, String path, long length)
     {
-        this(path, length, length);
+        this(cleanup, path, length, length);
     }
 
-    protected SegmentedFile(String path, long length, long onDiskLength)
+    protected SegmentedFile(Cleanup cleanup, String path, long length, long onDiskLength)
     {
+        super(cleanup);
         this.path = new File(path).getAbsolutePath();
         this.length = length;
         this.onDiskLength = onDiskLength;
     }
 
+    public SegmentedFile(SegmentedFile copy)
+    {
+        super(copy);
+        path = copy.path;
+        length = copy.length;
+        onDiskLength = copy.onDiskLength;
+    }
+
+    protected static abstract class Cleanup implements RefCounted.Tidy
+    {
+        final String path;
+        protected Cleanup(String path)
+        {
+            this.path = path;
+        }
+
+        public String name()
+        {
+            return path;
+        }
+    }
+
+    public abstract SegmentedFile sharedCopy();
+
     /**
      * @return A SegmentedFile.Builder.
      */
@@ -95,11 +122,6 @@ public abstract class SegmentedFile
         return new SegmentIterator(position);
     }
 
-    /**
-     * Do whatever action is needed to reclaim ressources used by this SegmentedFile.
-     */
-    public abstract void cleanup();
-
     /**
      * Collects potential segmentation points in an underlying file, and builds a SegmentedFile to represent it.
      */
diff --git a/src/java/org/apache/cassandra/service/ActiveRepairService.java b/src/java/org/apache/cassandra/service/ActiveRepairService.java
index 15e76414bc..bf1cdd6360 100644
--- a/src/java/org/apache/cassandra/service/ActiveRepairService.java
+++ b/src/java/org/apache/cassandra/service/ActiveRepairService.java
@@ -432,7 +432,7 @@ public class ActiveRepairService
         {
             Set<SSTableReader> sstables = sstableMap.get(cfId);
             Iterator<SSTableReader> sstableIterator = sstables.iterator();
-            ImmutableMap.Builder<SSTableReader, Ref> references = ImmutableMap.builder();
+            ImmutableMap.Builder<SSTableReader, Ref<SSTableReader>> references = ImmutableMap.builder();
             while (sstableIterator.hasNext())
             {
                 SSTableReader sstable = sstableIterator.next();
@@ -442,7 +442,7 @@ public class ActiveRepairService
                 }
                 else
                 {
-                    Ref ref = sstable.tryRef();
+                    Ref<SSTableReader> ref = sstable.tryRef();
                     if (ref == null)
                         sstableIterator.remove();
                     else
diff --git a/src/java/org/apache/cassandra/streaming/StreamSession.java b/src/java/org/apache/cassandra/streaming/StreamSession.java
index 6108dea77e..a9f507574e 100644
--- a/src/java/org/apache/cassandra/streaming/StreamSession.java
+++ b/src/java/org/apache/cassandra/streaming/StreamSession.java
@@ -353,7 +353,7 @@ public class StreamSession implements IEndpointStateChangeSubscriber
     public static class SSTableStreamingSections
     {
         public final SSTableReader sstable;
-        public final Ref ref;
+        public final Ref<SSTableReader> ref;
         public final List<Pair<Long, Long>> sections;
         public final long estimatedKeys;
         public final long repairedAt;
diff --git a/src/java/org/apache/cassandra/streaming/messages/OutgoingFileMessage.java b/src/java/org/apache/cassandra/streaming/messages/OutgoingFileMessage.java
index 5ebf289ba3..069e97f318 100644
--- a/src/java/org/apache/cassandra/streaming/messages/OutgoingFileMessage.java
+++ b/src/java/org/apache/cassandra/streaming/messages/OutgoingFileMessage.java
@@ -62,7 +62,7 @@ public class OutgoingFileMessage extends StreamMessage
 
     public final FileMessageHeader header;
     public final SSTableReader sstable;
-    public final Ref ref;
+    public final Ref<SSTableReader> ref;
 
     public OutgoingFileMessage(SSTableReader sstable, Ref ref, int sequenceNumber, long estimatedKeys, List<Pair<Long, Long>> sections, long repairedAt)
     {
diff --git a/src/java/org/apache/cassandra/tools/StandaloneScrubber.java b/src/java/org/apache/cassandra/tools/StandaloneScrubber.java
index 1bc2674235..d4202186da 100644
--- a/src/java/org/apache/cassandra/tools/StandaloneScrubber.java
+++ b/src/java/org/apache/cassandra/tools/StandaloneScrubber.java
@@ -121,7 +121,7 @@ public class StandaloneScrubber
 
                         // Remove the sstable (it's been copied by scrub and snapshotted)
                         sstable.markObsolete();
-                        sstable.sharedRef().release();
+                        sstable.selfRef().release();
                     }
                     catch (Exception e)
                     {
diff --git a/src/java/org/apache/cassandra/utils/AlwaysPresentFilter.java b/src/java/org/apache/cassandra/utils/AlwaysPresentFilter.java
index cc162d4204..1a029e5506 100644
--- a/src/java/org/apache/cassandra/utils/AlwaysPresentFilter.java
+++ b/src/java/org/apache/cassandra/utils/AlwaysPresentFilter.java
@@ -32,6 +32,11 @@ public class AlwaysPresentFilter implements IFilter
 
     public void close() { }
 
+    public IFilter sharedCopy()
+    {
+        return this;
+    }
+
     public long serializedSize() { return 0; }
 
     @Override
diff --git a/src/java/org/apache/cassandra/utils/BloomFilter.java b/src/java/org/apache/cassandra/utils/BloomFilter.java
index ceba89b0a3..77b2d44480 100644
--- a/src/java/org/apache/cassandra/utils/BloomFilter.java
+++ b/src/java/org/apache/cassandra/utils/BloomFilter.java
@@ -21,9 +21,10 @@ import java.nio.ByteBuffer;
 
 import com.google.common.annotations.VisibleForTesting;
 
+import org.apache.cassandra.utils.concurrent.WrappedSharedCloseable;
 import org.apache.cassandra.utils.obs.IBitSet;
 
-public abstract class BloomFilter implements IFilter
+public abstract class BloomFilter extends WrappedSharedCloseable implements IFilter
 {
     private static final ThreadLocal<long[]> reusableIndexes = new ThreadLocal<long[]>()
     {
@@ -36,12 +37,20 @@ public abstract class BloomFilter implements IFilter
     public final IBitSet bitset;
     public final int hashCount;
 
-    BloomFilter(int hashes, IBitSet bitset)
+    BloomFilter(int hashCount, IBitSet bitset)
     {
-        this.hashCount = hashes;
+        super(bitset);
+        this.hashCount = hashCount;
         this.bitset = bitset;
     }
 
+    BloomFilter(BloomFilter copy)
+    {
+        super(copy);
+        this.hashCount = copy.hashCount;
+        this.bitset = copy.bitset;
+    }
+
     // Murmur is faster than an SHA-based approach and provides as-good collision
     // resistance.  The combinatorial generation approach described in
     // http://www.eecs.harvard.edu/~kirsch/pubs/bbbf/esa06.pdf
@@ -110,9 +119,4 @@ public abstract class BloomFilter implements IFilter
     {
         bitset.clear();
     }
-
-    public void close()
-    {
-        bitset.close();
-    }
 }
diff --git a/src/java/org/apache/cassandra/utils/IFilter.java b/src/java/org/apache/cassandra/utils/IFilter.java
index 60c0590abc..bde63336af 100644
--- a/src/java/org/apache/cassandra/utils/IFilter.java
+++ b/src/java/org/apache/cassandra/utils/IFilter.java
@@ -17,10 +17,11 @@
  */
 package org.apache.cassandra.utils;
 
-import java.io.Closeable;
 import java.nio.ByteBuffer;
 
-public interface IFilter extends Closeable
+import org.apache.cassandra.utils.concurrent.SharedCloseable;
+
+public interface IFilter extends SharedCloseable
 {
     void add(ByteBuffer key);
 
@@ -32,6 +33,8 @@ public interface IFilter extends Closeable
 
     void close();
 
+    IFilter sharedCopy();
+
     /**
      * Returns the amount of memory in bytes used off heap.
      * @return the amount of memory in bytes used off heap
diff --git a/src/java/org/apache/cassandra/utils/Murmur3BloomFilter.java b/src/java/org/apache/cassandra/utils/Murmur3BloomFilter.java
index f7c7632809..431ca5bbdb 100644
--- a/src/java/org/apache/cassandra/utils/Murmur3BloomFilter.java
+++ b/src/java/org/apache/cassandra/utils/Murmur3BloomFilter.java
@@ -26,9 +26,14 @@ public class Murmur3BloomFilter extends BloomFilter
 {
     public static final Murmur3BloomFilterSerializer serializer = new Murmur3BloomFilterSerializer();
 
-    public Murmur3BloomFilter(int hashes, IBitSet bs)
+    public Murmur3BloomFilter(int hashCount, IBitSet bs)
     {
-        super(hashes, bs);
+        super(hashCount, bs);
+    }
+
+    protected Murmur3BloomFilter(Murmur3BloomFilter copy)
+    {
+        super(copy);
     }
 
     public long serializedSize()
@@ -36,6 +41,11 @@ public class Murmur3BloomFilter extends BloomFilter
         return serializer.serializedSize(this, TypeSizes.NATIVE);
     }
 
+    public IFilter sharedCopy()
+    {
+        return new Murmur3BloomFilter(this);
+    }
+
     @Override
     public long offHeapSize()
     {
diff --git a/src/java/org/apache/cassandra/utils/concurrent/Ref.java b/src/java/org/apache/cassandra/utils/concurrent/Ref.java
index 4afceb0be4..ad1293b314 100644
--- a/src/java/org/apache/cassandra/utils/concurrent/Ref.java
+++ b/src/java/org/apache/cassandra/utils/concurrent/Ref.java
@@ -2,24 +2,69 @@ package org.apache.cassandra.utils.concurrent;
 
 import java.lang.ref.PhantomReference;
 import java.lang.ref.ReferenceQueue;
+import java.util.Collections;
+import java.util.Set;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.ConcurrentLinkedQueue;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.atomic.AtomicInteger;
 import java.util.concurrent.atomic.AtomicIntegerFieldUpdater;
 
+import com.google.common.base.Throwables;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import org.apache.cassandra.concurrent.NamedThreadFactory;
+
 /**
- * A single managed reference to a RefCounted object
+ * An object that needs ref counting does the two following:
+ *   - defines a Tidy object that will cleanup once it's gone,
+ *     (this must retain no references to the object we're tracking (only its resources and how to clean up))
+ * Then, one of two options:
+ * 1) Construct a Ref directly pointing to it, and always use this Ref; or
+ * 2)
+ *   - implements RefCounted
+ *   - encapsulates a Ref, we'll call selfRef, to which it proxies all calls to RefCounted behaviours
+ *   - users must ensure no references to the selfRef leak, or are retained outside of a method scope.
+ *     (to ensure the selfRef is collected with the object, so that leaks may be detected and corrected)
+ *
+ * This class' functionality is achieved by what may look at first glance like a complex web of references,
+ * but boils down to:
+ *
+ * Target --> selfRef --> [Ref.State] <--> Ref.GlobalState --> Tidy
+ *                                             ^
+ *                                             |
+ * Ref ----------------------------------------
+ *                                             |
+ * Global -------------------------------------
+ *
+ * So that, if Target is collected, Impl is collected and, hence, so is selfRef.
+ *
+ * Once ref or selfRef are collected, the paired Ref.State's release method is called, which if it had
+ * not already been called will update Ref.GlobalState and log an error.
+ *
+ * Once the Ref.GlobalState has been completely released, the Tidy method is called and it removes the global reference
+ * to itself so it may also be collected.
  */
-public final class Ref
+public final class Ref<T> implements RefCounted<T>, AutoCloseable
 {
     static final Logger logger = LoggerFactory.getLogger(Ref.class);
     static final boolean DEBUG_ENABLED = System.getProperty("cassandra.debugrefcount", "false").equalsIgnoreCase("true");
 
     final State state;
+    final T referent;
 
-    Ref(RefCountedImpl.GlobalState state, boolean isSharedRef)
+    public Ref(T referent, Tidy tidy)
     {
-        this.state = new State(state, this, RefCountedImpl.referenceQueue, isSharedRef);
+        this.state = new State(new GlobalState(tidy), this, referenceQueue);
+        this.referent = referent;
+    }
+
+    Ref(T referent, GlobalState state)
+    {
+        this.state = new State(state, this, referenceQueue);
+        this.referent = referent;
     }
 
     /**
@@ -32,6 +77,36 @@ public final class Ref
         state.release(false);
     }
 
+    public void ensureReleased()
+    {
+        state.ensureReleased();
+    }
+
+    public void close()
+    {
+        state.ensureReleased();
+    }
+
+    public T get()
+    {
+        state.assertNotReleased();
+        return referent;
+    }
+
+    public Ref<T> tryRef()
+    {
+        return state.globalState.ref() ? new Ref<>(referent, state.globalState) : null;
+    }
+
+    public Ref<T> ref()
+    {
+        Ref<T> ref = tryRef();
+        // TODO: print the last release as well as the release here
+        if (ref == null)
+            state.assertNotReleased();
+        return ref;
+    }
+
     /**
      * A convenience method for reporting:
      * @return the number of currently extant references globally, including the shared reference
@@ -41,25 +116,36 @@ public final class Ref
         return state.globalState.count();
     }
 
-    // similar to RefCountedState, but tracks only the management of each unique ref created to the managed object
+    // similar to Ref.GlobalState, but tracks only the management of each unique ref created to the managed object
     // ensures it is only released once, and that it is always released
     static final class State extends PhantomReference<Ref>
     {
         final Debug debug = DEBUG_ENABLED ? new Debug() : null;
-        final boolean isSharedRef;
-        final RefCountedImpl.GlobalState globalState;
+        final GlobalState globalState;
         private volatile int released;
 
         private static final AtomicIntegerFieldUpdater<State> releasedUpdater = AtomicIntegerFieldUpdater.newUpdater(State.class, "released");
 
-        public State(final RefCountedImpl.GlobalState globalState, Ref reference, ReferenceQueue<? super Ref> q, boolean isSharedRef)
+        public State(final GlobalState globalState, Ref reference, ReferenceQueue<? super Ref> q)
         {
             super(reference, q);
             this.globalState = globalState;
-            this.isSharedRef = isSharedRef;
             globalState.register(this);
         }
 
+        void assertNotReleased()
+        {
+            if (DEBUG_ENABLED && released == 1)
+                debug.log(toString());
+            assert released == 0;
+        }
+
+        void ensureReleased()
+        {
+            if (releasedUpdater.getAndSet(this, 1) == 0)
+                globalState.release(this);
+        }
+
         void release(boolean leak)
         {
             if (!releasedUpdater.compareAndSet(this, 0, 1))
@@ -67,7 +153,7 @@ public final class Ref
                 if (!leak)
                 {
                     String id = this.toString();
-                    logger.error("BAD RELEASE: attempted to release a{} reference ({}) that has already been released", isSharedRef ? " shared" : "", id);
+                    logger.error("BAD RELEASE: attempted to release a reference ({}) that has already been released", id);
                     if (DEBUG_ENABLED)
                         debug.log(id);
                     throw new IllegalStateException("Attempted to release a reference that has already been released");
@@ -78,10 +164,7 @@ public final class Ref
             if (leak)
             {
                 String id = this.toString();
-                if (isSharedRef)
-                    logger.error("LEAK DETECTED: the shared reference ({}) to {} was not released before the object was garbage collected", id, globalState);
-                else
-                    logger.error("LEAK DETECTED: a reference ({}) to {} was not released before the reference was garbage collected", id, globalState);
+                logger.error("LEAK DETECTED: a reference ({}) to {} was not released before the reference was garbage collected", id, globalState);
                 if (DEBUG_ENABLED)
                     debug.log(id);
             }
@@ -129,6 +212,101 @@ public final class Ref
         }
     }
 
-}
+    // the object that manages the actual cleaning up; this does not reference the target object
+    // so that we can detect when references are lost to the resource itself, and still cleanup afterwards
+    // the Tidy object MUST not contain any references to the object we are managing
+    static final class GlobalState
+    {
+        // we need to retain a reference to each of the PhantomReference instances
+        // we are using to track individual refs
+        private final ConcurrentLinkedQueue<State> locallyExtant = new ConcurrentLinkedQueue<>();
+        // the number of live refs
+        private final AtomicInteger counts = new AtomicInteger();
+        // the object to call to cleanup when our refs are all finished with
+        private final Tidy tidy;
+
+        GlobalState(Tidy tidy)
+        {
+            this.tidy = tidy;
+            globallyExtant.add(this);
+        }
 
+        void register(Ref.State ref)
+        {
+            locallyExtant.add(ref);
+        }
 
+        // increment ref count if not already tidied, and return success/failure
+        boolean ref()
+        {
+            while (true)
+            {
+                int cur = counts.get();
+                if (cur < 0)
+                    return false;
+                if (counts.compareAndSet(cur, cur + 1))
+                    return true;
+            }
+        }
+
+        // release a single reference, and cleanup if no more are extant
+        void release(Ref.State ref)
+        {
+            locallyExtant.remove(ref);
+            if (-1 == counts.decrementAndGet())
+            {
+                globallyExtant.remove(this);
+                try
+                {
+                    tidy.tidy();
+                }
+                catch (Throwable t)
+                {
+                    logger.error("Error when closing {}", this, t);
+                    Throwables.propagate(t);
+                }
+            }
+        }
+
+        int count()
+        {
+            return 1 + counts.get();
+        }
+
+        public String toString()
+        {
+            return tidy.getClass() + "@" + System.identityHashCode(tidy) + ":" + tidy.name();
+        }
+    }
+
+    private static final Set<GlobalState> globallyExtant = Collections.newSetFromMap(new ConcurrentHashMap<GlobalState, Boolean>());
+    static final ReferenceQueue<Object> referenceQueue = new ReferenceQueue<>();
+    private static final ExecutorService EXEC = Executors.newFixedThreadPool(1, new NamedThreadFactory("Reference-Reaper"));
+    static
+    {
+        EXEC.execute(new Runnable()
+        {
+            public void run()
+            {
+                try
+                {
+                    while (true)
+                    {
+                        Object obj = referenceQueue.remove();
+                        if (obj instanceof Ref.State)
+                        {
+                            ((Ref.State) obj).release(true);
+                        }
+                    }
+                }
+                catch (InterruptedException e)
+                {
+                }
+                finally
+                {
+                    EXEC.execute(this);
+                }
+            }
+        });
+    }
+}
diff --git a/src/java/org/apache/cassandra/utils/concurrent/RefCounted.java b/src/java/org/apache/cassandra/utils/concurrent/RefCounted.java
index 7ad51ad7f3..e68c7bdc1d 100644
--- a/src/java/org/apache/cassandra/utils/concurrent/RefCounted.java
+++ b/src/java/org/apache/cassandra/utils/concurrent/RefCounted.java
@@ -36,59 +36,29 @@ import org.slf4j.LoggerFactory;
 import org.apache.cassandra.concurrent.NamedThreadFactory;
 
 /**
- * An object that needs ref counting does the following:
+ * An object that needs ref counting does the two following:
  *   - defines a Tidy object that will cleanup once it's gone,
  *     (this must retain no references to the object we're tracking (only its resources and how to clean up))
+ * Then, one of two options:
+ * 1) Construct a Ref directly pointing to it, and always use this Ref; or
+ * 2)
  *   - implements RefCounted
- *   - encapsulates a RefCounted.Impl, to which it proxies all calls to RefCounted behaviours
- *   - ensures no external access to the encapsulated Impl, and permits no references to it to leak
- *   - users must ensure no references to the sharedRef leak, or are retained outside of a method scope either.
- *     (to ensure the sharedRef is collected with the object, so that leaks may be detected and corrected)
- *
- * This class' functionality is achieved by what may look at first glance like a complex web of references,
- * but boils down to:
- *
- * Target --> Impl --> sharedRef --> [RefState] <--> RefCountedState --> Tidy
- *                                        ^                ^
- *                                        |                |
- * Ref -----------------------------------                 |
- *                                                         |
- * Global -------------------------------------------------
- *
- * So that, if Target is collected, Impl is collected and, hence, so is sharedRef.
- *
- * Once ref or sharedRef are collected, the paired RefState's release method is called, which if it had
- * not already been called will update RefCountedState and log an error.
- *
- * Once the RefCountedState has been completely released, the Tidy method is called and it removes the global reference
- * to itself so it may also be collected.
+ *   - encapsulates a Ref, we'll call selfRef, to which it proxies all calls to RefCounted behaviours
+ *   - users must ensure no references to the selfRef leak, or are retained outside of a method scope.
+ *     (to ensure the selfRef is collected with the object, so that leaks may be detected and corrected)
  */
-public interface RefCounted
+public interface RefCounted<T>
 {
-
     /**
      * @return the a new Ref() to the managed object, incrementing its refcount, or null if it is already released
      */
-    public Ref tryRef();
+    public Ref<T> tryRef();
 
-    /**
-     * @return the shared Ref that is created at instantiation of the RefCounted instance.
-     * Once released, if no other refs are extant the object will be tidied; references to
-     * this object should never be retained outside of a method's scope
-     */
-    public Ref sharedRef();
+    public Ref<T> ref();
 
     public static interface Tidy
     {
-        void tidy();
+        void tidy() throws Exception;
         String name();
     }
-
-    public static class Impl
-    {
-        public static RefCounted get(Tidy tidy)
-        {
-            return new RefCountedImpl(tidy);
-        }
-    }
 }
diff --git a/src/java/org/apache/cassandra/utils/concurrent/RefCountedImpl.java b/src/java/org/apache/cassandra/utils/concurrent/RefCountedImpl.java
deleted file mode 100644
index 0de6f40a15..0000000000
--- a/src/java/org/apache/cassandra/utils/concurrent/RefCountedImpl.java
+++ /dev/null
@@ -1,132 +0,0 @@
-package org.apache.cassandra.utils.concurrent;
-
-import java.lang.ref.ReferenceQueue;
-import java.util.Collections;
-import java.util.Set;
-import java.util.concurrent.ConcurrentHashMap;
-import java.util.concurrent.ConcurrentLinkedQueue;
-import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
-import java.util.concurrent.atomic.AtomicInteger;
-
-import org.apache.cassandra.concurrent.NamedThreadFactory;
-
-// default implementation; can be hidden and proxied (like we do for SSTableReader)
-final class RefCountedImpl implements RefCounted
-{
-    private final Ref sharedRef;
-    private final GlobalState state;
-
-    public RefCountedImpl(Tidy tidy)
-    {
-        this.state = new GlobalState(tidy);
-        sharedRef = new Ref(this.state, true);
-        globallyExtant.add(this.state);
-    }
-
-    /**
-     * see {@link RefCounted#tryRef()}
-     */
-    public Ref tryRef()
-    {
-        return state.ref() ? new Ref(state, false) : null;
-    }
-
-    /**
-     * see {@link RefCounted#sharedRef()}
-     */
-    public Ref sharedRef()
-    {
-        return sharedRef;
-    }
-
-    // the object that manages the actual cleaning up; this does not reference the RefCounted.Impl
-    // so that we can detect when references are lost to the resource itself, and still cleanup afterwards
-    // the Tidy object MUST not contain any references to the object we are managing
-    static final class GlobalState
-    {
-        // we need to retain a reference to each of the PhantomReference instances
-        // we are using to track individual refs
-        private final ConcurrentLinkedQueue<Ref.State> locallyExtant = new ConcurrentLinkedQueue<>();
-        // the number of live refs
-        private final AtomicInteger counts = new AtomicInteger();
-        // the object to call to cleanup when our refs are all finished with
-        private final Tidy tidy;
-
-        GlobalState(Tidy tidy)
-        {
-            this.tidy = tidy;
-        }
-
-        void register(Ref.State ref)
-        {
-            locallyExtant.add(ref);
-        }
-
-        // increment ref count if not already tidied, and return success/failure
-        boolean ref()
-        {
-            while (true)
-            {
-                int cur = counts.get();
-                if (cur < 0)
-                    return false;
-                if (counts.compareAndSet(cur, cur + 1))
-                    return true;
-            }
-        }
-
-        // release a single reference, and cleanup if no more are extant
-        void release(Ref.State ref)
-        {
-            locallyExtant.remove(ref);
-            if (-1 == counts.decrementAndGet())
-            {
-                globallyExtant.remove(this);
-                tidy.tidy();
-            }
-        }
-
-        int count()
-        {
-            return 1 + counts.get();
-        }
-
-        public String toString()
-        {
-            return tidy.name();
-        }
-    }
-
-    private static final Set<GlobalState> globallyExtant = Collections.newSetFromMap(new ConcurrentHashMap<GlobalState, Boolean>());
-    static final ReferenceQueue<Object> referenceQueue = new ReferenceQueue<>();
-    private static final ExecutorService EXEC = Executors.newFixedThreadPool(1, new NamedThreadFactory("Reference-Reaper"));
-    static
-    {
-        EXEC.execute(new Runnable()
-        {
-            public void run()
-            {
-                try
-                {
-                    while (true)
-                    {
-                        Object obj = referenceQueue.remove();
-                        if (obj instanceof Ref.State)
-                        {
-                            ((Ref.State) obj).release(true);
-                        }
-                    }
-                }
-                catch (InterruptedException e)
-                {
-                }
-                finally
-                {
-                    EXEC.execute(this);
-                }
-            }
-        });
-    }
-}
-
diff --git a/src/java/org/apache/cassandra/utils/concurrent/Refs.java b/src/java/org/apache/cassandra/utils/concurrent/Refs.java
index ed5fcfa3d9..3a930d2d9d 100644
--- a/src/java/org/apache/cassandra/utils/concurrent/Refs.java
+++ b/src/java/org/apache/cassandra/utils/concurrent/Refs.java
@@ -12,16 +12,16 @@ import com.google.common.collect.Iterators;
  *
  * All of the java.util.Collection operations that modify the collection are unsupported.
  */
-public final class Refs<T extends RefCounted> extends AbstractCollection<T> implements AutoCloseable
+public final class Refs<T extends RefCounted<T>> extends AbstractCollection<T> implements AutoCloseable
 {
-    private final Map<T, Ref> references;
+    private final Map<T, Ref<T>> references;
 
     public Refs()
     {
         this.references = new HashMap<>();
     }
 
-    public Refs(Map<T, Ref> references)
+    public Refs(Map<T, Ref<T>> references)
     {
         this.references = new HashMap<>(references);
     }
@@ -88,11 +88,11 @@ public final class Refs<T extends RefCounted> extends AbstractCollection<T> impl
      */
     public void release(Collection<T> release)
     {
-        List<Ref> refs = new ArrayList<>();
+        List<Ref<T>> refs = new ArrayList<>();
         List<T> notPresent = null;
         for (T obj : release)
         {
-            Ref ref = references.remove(obj);
+            Ref<T> ref = references.remove(obj);
             if (ref == null)
             {
                 if (notPresent == null)
@@ -132,7 +132,7 @@ public final class Refs<T extends RefCounted> extends AbstractCollection<T> impl
      */
     public boolean tryRef(T t)
     {
-        Ref ref = t.tryRef();
+        Ref<T> ref = t.tryRef();
         if (ref == null)
             return false;
         ref = references.put(t, ref);
@@ -156,8 +156,8 @@ public final class Refs<T extends RefCounted> extends AbstractCollection<T> impl
      */
     public Refs<T> addAll(Refs<T> add)
     {
-        List<Ref> overlap = new ArrayList<>();
-        for (Map.Entry<T, Ref> e : add.references.entrySet())
+        List<Ref<T>> overlap = new ArrayList<>();
+        for (Map.Entry<T, Ref<T>> e : add.references.entrySet())
         {
             if (this.references.containsKey(e.getKey()))
                 overlap.add(e.getValue());
@@ -172,12 +172,12 @@ public final class Refs<T extends RefCounted> extends AbstractCollection<T> impl
     /**
      * Acquire a reference to all of the provided objects, or none
      */
-    public static <T extends RefCounted> Refs<T> tryRef(Iterable<T> reference)
+    public static <T extends RefCounted<T>> Refs<T> tryRef(Iterable<T> reference)
     {
-        HashMap<T, Ref> refs = new HashMap<>();
+        HashMap<T, Ref<T>> refs = new HashMap<>();
         for (T rc : reference)
         {
-            Ref ref = rc.tryRef();
+            Ref<T> ref = rc.tryRef();
             if (ref == null)
             {
                 release(refs.values());
@@ -188,7 +188,7 @@ public final class Refs<T extends RefCounted> extends AbstractCollection<T> impl
         return new Refs<T>(refs);
     }
 
-    public static <T extends RefCounted> Refs<T> ref(Iterable<T> reference)
+    public static <T extends RefCounted<T>> Refs<T> ref(Iterable<T> reference)
     {
         Refs<T> refs = tryRef(reference);
         if (refs != null)
@@ -196,7 +196,7 @@ public final class Refs<T extends RefCounted> extends AbstractCollection<T> impl
         throw new IllegalStateException();
     }
 
-    private static void release(Iterable<Ref> refs)
+    private static void release(Iterable<? extends Ref<?>> refs)
     {
         Throwable fail = null;
         for (Ref ref : refs)
diff --git a/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java b/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java
index b23f1c627d..02efa65758 100644
--- a/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java
+++ b/src/java/org/apache/cassandra/utils/obs/OffHeapBitSet.java
@@ -172,4 +172,9 @@ public class OffHeapBitSet implements IBitSet
         }
         return (int) ((h >> 32) ^ h) + 0x98761234;
     }
+
+    public String toString()
+    {
+        return "[OffHeapBitSet]";
+    }
 }
diff --git a/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java b/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
index 2732be57d6..ec8ad8a8be 100644
--- a/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
+++ b/test/unit/org/apache/cassandra/db/ColumnFamilyStoreTest.java
@@ -1753,9 +1753,10 @@ public class ColumnFamilyStoreTest extends SchemaLoader
         sstables = dir.sstableLister().list();
         assertEquals(2, sstables.size());
 
+        SSTableReader sstable2 = SSTableReader.open(sstable1.descriptor);
         UUID compactionTaskID = SystemKeyspace.startCompaction(
                 Keyspace.open(ks).getColumnFamilyStore(cf),
-                Collections.singleton(SSTableReader.open(sstable1.descriptor)));
+                Collections.singleton(sstable2));
 
         Map<Integer, UUID> unfinishedCompaction = new HashMap<>();
         unfinishedCompaction.put(sstable1.descriptor.generation, compactionTaskID);
@@ -1768,6 +1769,8 @@ public class ColumnFamilyStoreTest extends SchemaLoader
 
         Map<Pair<String, String>, Map<Integer, UUID>> unfinished = SystemKeyspace.getUnfinishedCompactions();
         assertTrue(unfinished.isEmpty());
+        sstable1.selfRef().release();
+        sstable2.selfRef().release();
     }
 
     /**
diff --git a/test/unit/org/apache/cassandra/db/compaction/AntiCompactionTest.java b/test/unit/org/apache/cassandra/db/compaction/AntiCompactionTest.java
index d9442c7d0e..7756abee1c 100644
--- a/test/unit/org/apache/cassandra/db/compaction/AntiCompactionTest.java
+++ b/test/unit/org/apache/cassandra/db/compaction/AntiCompactionTest.java
@@ -96,7 +96,7 @@ public class AntiCompactionTest extends SchemaLoader
         for (SSTableReader sstable : store.getSSTables())
         {
             assertFalse(sstable.isMarkedCompacted());
-            assertEquals(1, sstable.sharedRef().globalCount());
+            assertEquals(1, sstable.selfRef().globalCount());
         }
         assertEquals(0, store.getDataTracker().getCompacting().size());
         assertEquals(repairedKeys, 4);
@@ -157,7 +157,7 @@ public class AntiCompactionTest extends SchemaLoader
         CompactionManager.instance.performAnticompaction(store, ranges, refs, 1);
         assertThat(store.getSSTables().size(), is(1));
         assertThat(Iterables.get(store.getSSTables(), 0).isRepaired(), is(false));
-        assertThat(Iterables.get(store.getSSTables(), 0).sharedRef().globalCount(), is(1));
+        assertThat(Iterables.get(store.getSSTables(), 0).selfRef().globalCount(), is(1));
         assertThat(store.getDataTracker().getCompacting().size(), is(0));
     }
 
@@ -174,7 +174,7 @@ public class AntiCompactionTest extends SchemaLoader
 
         assertThat(store.getSSTables().size(), is(1));
         assertThat(Iterables.get(store.getSSTables(), 0).isRepaired(), is(true));
-        assertThat(Iterables.get(store.getSSTables(), 0).sharedRef().globalCount(), is(1));
+        assertThat(Iterables.get(store.getSSTables(), 0).selfRef().globalCount(), is(1));
         assertThat(store.getDataTracker().getCompacting().size(), is(0));
     }
 
diff --git a/test/unit/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategyTest.java b/test/unit/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategyTest.java
index d9bf01788a..87b284e577 100644
--- a/test/unit/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategyTest.java
+++ b/test/unit/org/apache/cassandra/db/compaction/SizeTieredCompactionStrategyTest.java
@@ -173,9 +173,9 @@ public class SizeTieredCompactionStrategyTest extends SchemaLoader
         List<SSTableReader> interestingBucket = mostInterestingBucket(Collections.singletonList(sstrs.subList(0, 2)), 4, 32);
         assertTrue("nothing should be returned when all buckets are below the min threshold", interestingBucket.isEmpty());
 
-        sstrs.get(0).readMeter = new RestorableMeter(100.0, 100.0);
-        sstrs.get(1).readMeter = new RestorableMeter(200.0, 200.0);
-        sstrs.get(2).readMeter = new RestorableMeter(300.0, 300.0);
+        sstrs.get(0).overrideReadMeter(new RestorableMeter(100.0, 100.0));
+        sstrs.get(1).overrideReadMeter(new RestorableMeter(200.0, 200.0));
+        sstrs.get(2).overrideReadMeter(new RestorableMeter(300.0, 300.0));
 
         long estimatedKeys = sstrs.get(0).estimatedKeys();
 
@@ -215,43 +215,43 @@ public class SizeTieredCompactionStrategyTest extends SchemaLoader
         List<SSTableReader> sstrs = new ArrayList<>(cfs.getSSTables());
 
         for (SSTableReader sstr : sstrs)
-            sstr.readMeter = null;
+            sstr.overrideReadMeter(null);
         filtered = filterColdSSTables(sstrs, 0.05, 0);
         assertEquals("when there are no read meters, no sstables should be filtered", sstrs.size(), filtered.size());
 
         for (SSTableReader sstr : sstrs)
-            sstr.readMeter = new RestorableMeter(0.0, 0.0);
+            sstr.overrideReadMeter(new RestorableMeter(0.0, 0.0));
         filtered = filterColdSSTables(sstrs, 0.05, 0);
         assertEquals("when all read meters are zero, no sstables should be filtered", sstrs.size(), filtered.size());
 
         // leave all read rates at 0 besides one
-        sstrs.get(0).readMeter = new RestorableMeter(1000.0, 1000.0);
+        sstrs.get(0).overrideReadMeter(new RestorableMeter(1000.0, 1000.0));
         filtered = filterColdSSTables(sstrs, 0.05, 0);
         assertEquals("there should only be one hot sstable", 1, filtered.size());
-        assertEquals(1000.0, filtered.get(0).readMeter.twoHourRate(), 0.5);
+        assertEquals(1000.0, filtered.get(0).getReadMeter().twoHourRate(), 0.5);
 
         // the total read rate is 100, and we'll set a threshold of 2.5%, so two of the sstables with read
         // rate 1.0 should be ignored, but not the third
         for (SSTableReader sstr : sstrs)
-            sstr.readMeter = new RestorableMeter(0.0, 0.0);
-        sstrs.get(0).readMeter = new RestorableMeter(97.0, 97.0);
-        sstrs.get(1).readMeter = new RestorableMeter(1.0, 1.0);
-        sstrs.get(2).readMeter = new RestorableMeter(1.0, 1.0);
-        sstrs.get(3).readMeter = new RestorableMeter(1.0, 1.0);
+            sstr.overrideReadMeter(new RestorableMeter(0.0, 0.0));
+        sstrs.get(0).overrideReadMeter(new RestorableMeter(97.0, 97.0));
+        sstrs.get(1).overrideReadMeter(new RestorableMeter(1.0, 1.0));
+        sstrs.get(2).overrideReadMeter(new RestorableMeter(1.0, 1.0));
+        sstrs.get(3).overrideReadMeter(new RestorableMeter(1.0, 1.0));
 
         filtered = filterColdSSTables(sstrs, 0.025, 0);
         assertEquals(2, filtered.size());
-        assertEquals(98.0, filtered.get(0).readMeter.twoHourRate() + filtered.get(1).readMeter.twoHourRate(), 0.5);
+        assertEquals(98.0, filtered.get(0).getReadMeter().twoHourRate() + filtered.get(1).getReadMeter().twoHourRate(), 0.5);
 
         // make sure a threshold of 0.0 doesn't result in any sstables being filtered
         for (SSTableReader sstr : sstrs)
-            sstr.readMeter = new RestorableMeter(1.0, 1.0);
+            sstr.overrideReadMeter(new RestorableMeter(1.0, 1.0));
         filtered = filterColdSSTables(sstrs, 0.0, 0);
         assertEquals(sstrs.size(), filtered.size());
 
         // just for fun, set a threshold where all sstables are considered cold
         for (SSTableReader sstr : sstrs)
-            sstr.readMeter = new RestorableMeter(1.0, 1.0);
+            sstr.overrideReadMeter(new RestorableMeter(1.0, 1.0));
         filtered = filterColdSSTables(sstrs, 1.0, 0);
         assertTrue(filtered.isEmpty());
     }
diff --git a/test/unit/org/apache/cassandra/io/sstable/IndexSummaryManagerTest.java b/test/unit/org/apache/cassandra/io/sstable/IndexSummaryManagerTest.java
index 0a2b5a6ec6..0bb9d5fc2c 100644
--- a/test/unit/org/apache/cassandra/io/sstable/IndexSummaryManagerTest.java
+++ b/test/unit/org/apache/cassandra/io/sstable/IndexSummaryManagerTest.java
@@ -90,7 +90,7 @@ public class IndexSummaryManagerTest extends SchemaLoader
     private static List<SSTableReader> resetSummaries(List<SSTableReader> sstables, long originalOffHeapSize) throws IOException
     {
         for (SSTableReader sstable : sstables)
-            sstable.readMeter = new RestorableMeter(100.0, 100.0);
+            sstable.overrideReadMeter(new RestorableMeter(100.0, 100.0));
 
         sstables = redistributeSummaries(Collections.EMPTY_LIST, sstables, originalOffHeapSize * sstables.size());
         for (SSTableReader sstable : sstables)
@@ -117,7 +117,7 @@ public class IndexSummaryManagerTest extends SchemaLoader
     {
         public int compare(SSTableReader o1, SSTableReader o2)
         {
-            return Double.compare(o1.readMeter.fifteenMinuteRate(), o2.readMeter.fifteenMinuteRate());
+            return Double.compare(o1.getReadMeter().fifteenMinuteRate(), o2.getReadMeter().fifteenMinuteRate());
         }
     };
 
@@ -172,7 +172,7 @@ public class IndexSummaryManagerTest extends SchemaLoader
 
         List<SSTableReader> sstables = new ArrayList<>(cfs.getSSTables());
         for (SSTableReader sstable : sstables)
-            sstable.readMeter = new RestorableMeter(100.0, 100.0);
+            sstable.overrideReadMeter(new RestorableMeter(100.0, 100.0));
 
         for (SSTableReader sstable : sstables)
             assertEquals(cfs.metadata.getMinIndexInterval(), sstable.getEffectiveIndexInterval(), 0.001);
@@ -244,7 +244,7 @@ public class IndexSummaryManagerTest extends SchemaLoader
 
         List<SSTableReader> sstables = new ArrayList<>(cfs.getSSTables());
         for (SSTableReader sstable : sstables)
-            sstable.readMeter = new RestorableMeter(100.0, 100.0);
+            sstable.overrideReadMeter(new RestorableMeter(100.0, 100.0));
 
         IndexSummaryManager.redistributeSummaries(Collections.EMPTY_LIST, sstables, 1);
         sstables = new ArrayList<>(cfs.getSSTables());
@@ -286,7 +286,7 @@ public class IndexSummaryManagerTest extends SchemaLoader
 
         List<SSTableReader> sstables = new ArrayList<>(cfs.getSSTables());
         for (SSTableReader sstable : sstables)
-            sstable.readMeter = new RestorableMeter(100.0, 100.0);
+            sstable.overrideReadMeter(new RestorableMeter(100.0, 100.0));
 
         long singleSummaryOffHeapSpace = sstables.get(0).getIndexSummaryOffHeapSize();
 
@@ -325,8 +325,8 @@ public class IndexSummaryManagerTest extends SchemaLoader
 
         // make two of the four sstables cold, only leave enough space for three full index summaries,
         // so the two cold sstables should get downsampled to be half of their original size
-        sstables.get(0).readMeter = new RestorableMeter(50.0, 50.0);
-        sstables.get(1).readMeter = new RestorableMeter(50.0, 50.0);
+        sstables.get(0).overrideReadMeter(new RestorableMeter(50.0, 50.0));
+        sstables.get(1).overrideReadMeter(new RestorableMeter(50.0, 50.0));
         sstables = redistributeSummaries(Collections.EMPTY_LIST, sstables, (singleSummaryOffHeapSpace * 3));
         Collections.sort(sstables, hotnessComparator);
         assertEquals(BASE_SAMPLING_LEVEL / 2, sstables.get(0).getIndexSummarySamplingLevel());
@@ -338,8 +338,8 @@ public class IndexSummaryManagerTest extends SchemaLoader
         // small increases or decreases in the read rate don't result in downsampling or upsampling
         double lowerRate = 50.0 * (DOWNSAMPLE_THESHOLD + (DOWNSAMPLE_THESHOLD * 0.10));
         double higherRate = 50.0 * (UPSAMPLE_THRESHOLD - (UPSAMPLE_THRESHOLD * 0.10));
-        sstables.get(0).readMeter = new RestorableMeter(lowerRate, lowerRate);
-        sstables.get(1).readMeter = new RestorableMeter(higherRate, higherRate);
+        sstables.get(0).overrideReadMeter(new RestorableMeter(lowerRate, lowerRate));
+        sstables.get(1).overrideReadMeter(new RestorableMeter(higherRate, higherRate));
         sstables = redistributeSummaries(Collections.EMPTY_LIST, sstables, (singleSummaryOffHeapSpace * 3));
         Collections.sort(sstables, hotnessComparator);
         assertEquals(BASE_SAMPLING_LEVEL / 2, sstables.get(0).getIndexSummarySamplingLevel());
@@ -350,10 +350,10 @@ public class IndexSummaryManagerTest extends SchemaLoader
 
         // reset, and then this time, leave enough space for one of the cold sstables to not get downsampled
         sstables = resetSummaries(sstables, singleSummaryOffHeapSpace);
-        sstables.get(0).readMeter = new RestorableMeter(1.0, 1.0);
-        sstables.get(1).readMeter = new RestorableMeter(2.0, 2.0);
-        sstables.get(2).readMeter = new RestorableMeter(1000.0, 1000.0);
-        sstables.get(3).readMeter = new RestorableMeter(1000.0, 1000.0);
+        sstables.get(0).overrideReadMeter(new RestorableMeter(1.0, 1.0));
+        sstables.get(1).overrideReadMeter(new RestorableMeter(2.0, 2.0));
+        sstables.get(2).overrideReadMeter(new RestorableMeter(1000.0, 1000.0));
+        sstables.get(3).overrideReadMeter(new RestorableMeter(1000.0, 1000.0));
 
         sstables = redistributeSummaries(Collections.EMPTY_LIST, sstables, (singleSummaryOffHeapSpace * 3) + 50);
         Collections.sort(sstables, hotnessComparator);
@@ -372,10 +372,10 @@ public class IndexSummaryManagerTest extends SchemaLoader
         // coldest sstables will get downsampled to 4/128 of their size, leaving us with 1 and 92/128th index
         // summaries worth of space.  The hottest sstable should get a full index summary, and the one in the middle
         // should get the remainder.
-        sstables.get(0).readMeter = new RestorableMeter(0.0, 0.0);
-        sstables.get(1).readMeter = new RestorableMeter(0.0, 0.0);
-        sstables.get(2).readMeter = new RestorableMeter(92, 92);
-        sstables.get(3).readMeter = new RestorableMeter(128.0, 128.0);
+        sstables.get(0).overrideReadMeter(new RestorableMeter(0.0, 0.0));
+        sstables.get(1).overrideReadMeter(new RestorableMeter(0.0, 0.0));
+        sstables.get(2).overrideReadMeter(new RestorableMeter(92, 92));
+        sstables.get(3).overrideReadMeter(new RestorableMeter(128.0, 128.0));
         sstables = redistributeSummaries(Collections.EMPTY_LIST, sstables, (long) (singleSummaryOffHeapSpace + (singleSummaryOffHeapSpace * (92.0 / BASE_SAMPLING_LEVEL))));
         Collections.sort(sstables, hotnessComparator);
         assertEquals(1, sstables.get(0).getIndexSummarySize());  // at the min sampling level
@@ -421,10 +421,13 @@ public class IndexSummaryManagerTest extends SchemaLoader
         SSTableReader sstable = original;
         for (int samplingLevel = 1; samplingLevel < BASE_SAMPLING_LEVEL; samplingLevel++)
         {
+            SSTableReader prev = sstable;
             sstable = sstable.cloneWithNewSummarySamplingLevel(cfs, samplingLevel);
             assertEquals(samplingLevel, sstable.getIndexSummarySamplingLevel());
             int expectedSize = (numRows * samplingLevel) / (sstable.metadata.getMinIndexInterval() * BASE_SAMPLING_LEVEL);
             assertEquals(expectedSize, sstable.getIndexSummarySize(), 1);
+            if (prev != original)
+                prev.selfRef().release();
         }
 
         // don't leave replaced SSTRs around to break other tests
diff --git a/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java b/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java
index 8bef669d38..19ba274c88 100644
--- a/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java
+++ b/test/unit/org/apache/cassandra/io/sstable/LegacySSTableTest.java
@@ -118,7 +118,7 @@ public class LegacySSTableTest extends SchemaLoader
         ranges.add(new Range<>(p.getMinimumToken(), p.getToken(ByteBufferUtil.bytes("100"))));
         ranges.add(new Range<>(p.getToken(ByteBufferUtil.bytes("100")), p.getMinimumToken()));
         ArrayList<StreamSession.SSTableStreamingSections> details = new ArrayList<>();
-        details.add(new StreamSession.SSTableStreamingSections(sstable, sstable.tryRef(),
+        details.add(new StreamSession.SSTableStreamingSections(sstable, sstable.ref(),
                                                                sstable.getPositionsForRanges(ranges),
                                                                sstable.estimatedKeysForRanges(ranges), sstable.getSSTableMetadata().repairedAt));
         new StreamPlan("LegacyStreamingTest").transferFiles(FBUtilities.getBroadcastAddress(), details)
@@ -138,6 +138,7 @@ public class LegacySSTableTest extends SchemaLoader
             assert cf.deletionInfo().equals(DeletionInfo.live());
             assert iter.next().name().toByteBuffer().equals(key);
         }
+        sstable.selfRef().release();
     }
 
     @Test
diff --git a/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java b/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java
index 51588f29bf..9607673def 100644
--- a/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java
+++ b/test/unit/org/apache/cassandra/io/sstable/SSTableReaderTest.java
@@ -203,15 +203,15 @@ public class SSTableReaderTest extends SchemaLoader
         store.forceBlockingFlush();
 
         SSTableReader sstable = store.getSSTables().iterator().next();
-        assertEquals(0, sstable.readMeter.count());
+        assertEquals(0, sstable.getReadMeter().count());
 
         DecoratedKey key = sstable.partitioner.decorateKey(ByteBufferUtil.bytes("4"));
         store.getColumnFamily(key, Composites.EMPTY, Composites.EMPTY, false, 100, 100);
-        assertEquals(1, sstable.readMeter.count());
+        assertEquals(1, sstable.getReadMeter().count());
         store.getColumnFamily(key, cellname("0"), cellname("0"), false, 100, 100);
-        assertEquals(2, sstable.readMeter.count());
+        assertEquals(2, sstable.getReadMeter().count());
         store.getColumnFamily(Util.namesQueryFilter(store, key, cellname("0")));
-        assertEquals(3, sstable.readMeter.count());
+        assertEquals(3, sstable.getReadMeter().count());
     }
 
     @Test
@@ -334,6 +334,7 @@ public class SSTableReaderTest extends SchemaLoader
         Assert.assertArrayEquals(ByteBufferUtil.getArray(firstKey.getKey()), target.getIndexSummaryKey(0));
         assert target.first.equals(firstKey);
         assert target.last.equals(lastKey);
+        target.selfRef().release();
     }
 
     @Test
@@ -360,6 +361,7 @@ public class SSTableReaderTest extends SchemaLoader
 
         SSTableReader reopened = SSTableReader.open(sstable.descriptor);
         assert reopened.first.getToken() instanceof LocalToken;
+        reopened.selfRef().release();
     }
 
     /** see CASSANDRA-5407 */
@@ -417,6 +419,7 @@ public class SSTableReaderTest extends SchemaLoader
         SSTableReader bulkLoaded = SSTableReader.openForBatch(sstable.descriptor, components, store.metadata, sstable.partitioner);
         sections = bulkLoaded.getPositionsForRanges(ranges);
         assert sections.size() == 1 : "Expected to find range in sstable opened for bulk loading";
+        bulkLoaded.selfRef().release();
     }
 
     @Test
diff --git a/test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java b/test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java
index acf8c90845..2e11624cbf 100644
--- a/test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java
+++ b/test/unit/org/apache/cassandra/io/sstable/SSTableRewriterTest.java
@@ -19,11 +19,7 @@ package org.apache.cassandra.io.sstable;
 
 import java.io.File;
 import java.nio.ByteBuffer;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.HashSet;
-import java.util.List;
-import java.util.Set;
+import java.util.*;
 
 import com.google.common.collect.Sets;
 import org.junit.Test;
@@ -200,10 +196,11 @@ public class SSTableRewriterTest extends SchemaLoader
         assertTrue(s != s2);
         assertFileCounts(dir.list(), 2, 3);
         s.markObsolete();
-        s.sharedRef().release();
+        s.selfRef().release();
+        s2.selfRef().release();
         Thread.sleep(1000);
         assertFileCounts(dir.list(), 0, 3);
-        writer.abort(false);
+        writer.abort();
         Thread.sleep(1000);
         int datafiles = assertFileCounts(dir.list(), 0, 0);
         assertEquals(datafiles, 0);
@@ -705,7 +702,7 @@ public class SSTableRewriterTest extends SchemaLoader
         for (SSTableReader sstable : cfs.getSSTables())
         {
             assertFalse(sstable.isMarkedCompacted());
-            assertEquals(1, sstable.sharedRef().globalCount());
+            assertEquals(1, sstable.selfRef().globalCount());
             liveDescriptors.add(sstable.descriptor.generation);
         }
         for (File dir : cfs.directories.getCFDirectories())
diff --git a/test/unit/org/apache/cassandra/streaming/StreamTransferTaskTest.java b/test/unit/org/apache/cassandra/streaming/StreamTransferTaskTest.java
index f84ae11684..1447b29600 100644
--- a/test/unit/org/apache/cassandra/streaming/StreamTransferTaskTest.java
+++ b/test/unit/org/apache/cassandra/streaming/StreamTransferTaskTest.java
@@ -64,7 +64,7 @@ public class StreamTransferTaskTest extends SchemaLoader
         {
             List<Range<Token>> ranges = new ArrayList<>();
             ranges.add(new Range<>(sstable.first.getToken(), sstable.last.getToken()));
-            task.addTransferFile(sstable, sstable.sharedRef(), 1, sstable.getPositionsForRanges(ranges), 0);
+            task.addTransferFile(sstable, sstable.selfRef(), 1, sstable.getPositionsForRanges(ranges), 0);
         }
         assertEquals(2, task.getTotalNumberOfFiles());
 
diff --git a/test/unit/org/apache/cassandra/tools/SSTableExportTest.java b/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
index c3f3419467..c918d6ae7b 100644
--- a/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
+++ b/test/unit/org/apache/cassandra/tools/SSTableExportTest.java
@@ -189,6 +189,7 @@ public class SSTableExportTest extends SchemaLoader
         qf = Util.namesQueryFilter(cfs, Util.dk("rowExclude"), "name");
         cf = qf.getSSTableColumnIterator(reader).getColumnFamily();
         assert cf == null;
+        reader.selfRef().release();
     }
 
     @Test
diff --git a/test/unit/org/apache/cassandra/tools/SSTableImportTest.java b/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
index b0f4c0a809..a5f05f8ec5 100644
--- a/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
+++ b/test/unit/org/apache/cassandra/tools/SSTableImportTest.java
@@ -71,6 +71,7 @@ public class SSTableImportTest extends SchemaLoader
         assert expCol.value().equals(hexToBytes("76616c4143"));
         assert expCol instanceof ExpiringCell;
         assert ((ExpiringCell)expCol).getTimeToLive() == 42 && expCol.getLocalDeletionTime() == 2000000000;
+        reader.selfRef().release();
     }
 
     private ColumnFamily cloneForAdditions(OnDiskAtomIterator iter)
@@ -105,6 +106,7 @@ public class SSTableImportTest extends SchemaLoader
         assert expCol.value().equals(hexToBytes("76616c4143"));
         assert expCol instanceof ExpiringCell;
         assert ((ExpiringCell) expCol).getTimeToLive() == 42 && expCol.getLocalDeletionTime() == 2000000000;
+        reader.selfRef().release();
     }
 
     @Test
@@ -129,6 +131,7 @@ public class SSTableImportTest extends SchemaLoader
         assert expCol.value().equals(hexToBytes("76616c4143"));
         assert expCol instanceof ExpiringCell;
         assert ((ExpiringCell) expCol).getTimeToLive() == 42 && expCol.getLocalDeletionTime() == 2000000000;
+        reader.selfRef().release();
     }
 
     @Test
@@ -148,6 +151,7 @@ public class SSTableImportTest extends SchemaLoader
         Cell c = cf.getColumn(Util.cellname("colAA"));
         assert c instanceof CounterCell : c;
         assert ((CounterCell) c).total() == 42;
+        reader.selfRef().release();
     }
 
     @Test
@@ -168,6 +172,7 @@ public class SSTableImportTest extends SchemaLoader
         QueryFilter qf2 = QueryFilter.getIdentityFilter(Util.dk("726f7741", BytesType.instance), "AsciiKeys", System.currentTimeMillis());
         OnDiskAtomIterator iter2 = qf2.getSSTableColumnIterator(reader);
         assert !iter2.hasNext(); // "bytes" key does not exist
+        reader.selfRef().release();
     }
 
     @Test
@@ -186,6 +191,7 @@ public class SSTableImportTest extends SchemaLoader
         QueryFilter qf = QueryFilter.getIdentityFilter(Util.dk("rowA"), "AsciiKeys", System.currentTimeMillis());
         OnDiskAtomIterator iter = qf.getSSTableColumnIterator(reader);
         assert iter.hasNext(); // "bytes" key exists
+        reader.selfRef().release();
     }
     
     @Test
@@ -207,6 +213,7 @@ public class SSTableImportTest extends SchemaLoader
         assertThat(result.size(), is(2));
         assertThat(result, hasItem(withElements(1, "NY", 1980)));
         assertThat(result, hasItem(withElements(2, "CA", 2014)));
+        reader.selfRef().release();
     }
 
     @Test(expected=AssertionError.class)
diff --git a/test/unit/org/apache/cassandra/utils/BloomFilterTest.java b/test/unit/org/apache/cassandra/utils/BloomFilterTest.java
index 4180a8cc5d..aee0880e7e 100644
--- a/test/unit/org/apache/cassandra/utils/BloomFilterTest.java
+++ b/test/unit/org/apache/cassandra/utils/BloomFilterTest.java
@@ -30,10 +30,7 @@ import java.util.HashSet;
 import java.util.Iterator;
 import java.util.Set;
 
-import org.junit.Assert;
-import org.junit.Before;
-import org.junit.Ignore;
-import org.junit.Test;
+import org.junit.*;
 
 import org.apache.cassandra.io.util.DataOutputBuffer;
 import org.apache.cassandra.io.util.DataOutputStreamAndChannel;
@@ -45,7 +42,7 @@ public class BloomFilterTest
 
     public BloomFilterTest()
     {
-        bf = FilterFactory.getFilter(10000L, FilterTestHelper.MAX_FAILURE_RATE, true);
+
     }
 
     public static IFilter testSerialize(IFilter f) throws IOException
@@ -64,9 +61,15 @@ public class BloomFilterTest
 
 
     @Before
-    public void clear()
+    public void setup()
+    {
+        bf = FilterFactory.getFilter(10000L, FilterTestHelper.MAX_FAILURE_RATE, true);
+    }
+
+    @After
+    public void destroy()
     {
-        bf.clear();
+        bf.close();
     }
 
     @Test(expected = UnsupportedOperationException.class)
@@ -114,12 +117,13 @@ public class BloomFilterTest
         FilterTestHelper.testFalsePositives(bf2,
                                             new KeyGenerator.WordGenerator(skipEven, 2),
                                             new KeyGenerator.WordGenerator(1, 2));
+        bf2.close();
     }
 
     @Test
     public void testSerialize() throws IOException
     {
-        BloomFilterTest.testSerialize(bf);
+        BloomFilterTest.testSerialize(bf).close();
     }
 
     public void testManyHashes(Iterator<ByteBuffer> keys)
@@ -137,6 +141,7 @@ public class BloomFilterTest
                 hashes.add(hashIndex);
             }
             collisions += (MAX_HASH_COUNT - hashes.size());
+            bf.close();
         }
         assert collisions <= 100;
     }
@@ -167,6 +172,7 @@ public class BloomFilterTest
         FilterFactory.serialize(filter, out);
         filter.bitset.serialize(out);
         out.close();
+        filter.close();
         
         DataInputStream in = new DataInputStream(new FileInputStream(file));
         BloomFilter filter2 = (BloomFilter) FilterFactory.deserialize(in, true);
diff --git a/test/unit/org/apache/cassandra/utils/SerializationsTest.java b/test/unit/org/apache/cassandra/utils/SerializationsTest.java
index 976a3eb15a..b0a23fdbd1 100644
--- a/test/unit/org/apache/cassandra/utils/SerializationsTest.java
+++ b/test/unit/org/apache/cassandra/utils/SerializationsTest.java
@@ -39,6 +39,7 @@ public class SerializationsTest extends AbstractSerializationsTester
         DataOutputStreamAndChannel out = getOutput("utils.BloomFilter.bin");
         FilterFactory.serialize(bf, out);
         out.close();
+        bf.close();
     }
 
     @Test
@@ -48,7 +49,9 @@ public class SerializationsTest extends AbstractSerializationsTester
             testBloomFilterWrite(true);
 
         DataInputStream in = getInput("utils.BloomFilter.bin");
-        assert FilterFactory.deserialize(in, true) != null;
+        IFilter bf = FilterFactory.deserialize(in, true);
+        assert bf != null;
+        bf.close();
         in.close();
     }
 
diff --git a/test/unit/org/apache/cassandra/utils/concurrent/RefCountedTest.java b/test/unit/org/apache/cassandra/utils/concurrent/RefCountedTest.java
index fe22d2102b..a9247cd22b 100644
--- a/test/unit/org/apache/cassandra/utils/concurrent/RefCountedTest.java
+++ b/test/unit/org/apache/cassandra/utils/concurrent/RefCountedTest.java
@@ -44,9 +44,9 @@ public class RefCountedTest
     public void testLeak() throws InterruptedException
     {
         Tidier tidier = new Tidier();
-        RefCounted obj = RefCounted.Impl.get(tidier);
+        Ref<?> obj = new Ref(null, tidier);
         obj.tryRef();
-        obj.sharedRef().release();
+        obj.release();
         System.gc();
         System.gc();
         Thread.sleep(1000);
@@ -57,7 +57,7 @@ public class RefCountedTest
     public void testSeriousLeak() throws InterruptedException
     {
         Tidier tidier = new Tidier();
-        RefCounted.Impl.get(tidier);
+        new Ref(null, tidier);
         System.gc();
         System.gc();
         System.gc();
@@ -73,9 +73,9 @@ public class RefCountedTest
         try
         {
             tidier = new Tidier();
-            RefCounted obj = RefCounted.Impl.get(tidier);
-            obj.sharedRef().release();
-            obj.sharedRef().release();
+            Ref<?> obj = new Ref(null, tidier);
+            obj.release();
+            obj.release();
             Assert.assertTrue(false);
         }
         catch (Exception e)
