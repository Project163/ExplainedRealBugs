diff --git a/modules/accord b/modules/accord
index f9dc83e404..fe306fc553 160000
--- a/modules/accord
+++ b/modules/accord
@@ -1 +1 @@
-Subproject commit f9dc83e404253645db02d54c913174b796fae123
+Subproject commit fe306fc5539b40d1c9d49f9afd0ca45bb74c49d3
diff --git a/src/java/org/apache/cassandra/db/compaction/CompactionIterator.java b/src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
index 6c6707b15e..7d2364b844 100644
--- a/src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
+++ b/src/java/org/apache/cassandra/db/compaction/CompactionIterator.java
@@ -31,7 +31,6 @@ import javax.annotation.Nonnull;
 
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Ordering;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -94,6 +93,7 @@ import org.apache.cassandra.service.accord.IAccordService.AccordCompactionInfos;
 import org.apache.cassandra.service.accord.JournalKey;
 import org.apache.cassandra.service.accord.api.AccordAgent;
 import org.apache.cassandra.service.accord.api.TokenKey;
+import org.apache.cassandra.service.accord.journal.AccordTopologyUpdate;
 import org.apache.cassandra.service.paxos.PaxosRepairHistory;
 import org.apache.cassandra.service.paxos.uncommitted.PaxosRows;
 import org.apache.cassandra.utils.TimeUUID;
@@ -789,7 +789,7 @@ public class CompactionIterator extends CompactionInfo.Holder implements Unfilte
         AccordCommandsForKeyPurger(CommandsForKeyAccessor accessor, Supplier<IAccordService> accordService)
         {
             this.accessor = accessor;
-            this.compactionInfos = new AccordCompactionInfos(null, accordService.get().getCompactionInfo());
+            this.compactionInfos = accordService.get().getCompactionInfo();
         }
 
         protected void beginPartition(UnfilteredRowIterator partition)
@@ -837,6 +837,8 @@ public class CompactionIterator extends CompactionInfo.Holder implements Unfilte
         JournalKey key = null;
         Object builder = null;
         FlyweightSerializer<Object, Object> serializer = null;
+        // Initialize topology serializer during compaction to avoid deserializing redundant epochs
+        FlyweightSerializer<AccordTopologyUpdate, Object> topologySerializer;
         Object[] firstClustering = null;
         final int userVersion;
         long lastDescriptor = -1;
@@ -852,6 +854,7 @@ public class CompactionIterator extends CompactionInfo.Holder implements Unfilte
             this.infos = service.getCompactionInfo();
             this.recordColumn = cfs.metadata().getColumn(ColumnIdentifier.getInterned("record", false));
             this.versionColumn = cfs.metadata().getColumn(ColumnIdentifier.getInterned("user_version", false));
+            this.topologySerializer = (FlyweightSerializer<AccordTopologyUpdate, Object>) (FlyweightSerializer) new AccordTopologyUpdate.AccumulatingSerializer(() -> infos.minEpoch);
         }
 
         @SuppressWarnings("unchecked")
@@ -978,7 +981,10 @@ public class CompactionIterator extends CompactionInfo.Holder implements Unfilte
             try (DataInputBuffer in = new DataInputBuffer(record, false))
             {
                 int userVersion = Int32Type.instance.compose(row.getCell(versionColumn).buffer());
-                serializer.deserialize(key, builder, in, userVersion);
+                if (key.type == JournalKey.Type.TOPOLOGY_UPDATE)
+                    topologySerializer.deserialize(key, builder, in, userVersion);
+                else
+                    serializer.deserialize(key, builder, in, userVersion);
                 if (firstClustering == null)
                     firstClustering = row.clustering().getBufferArray();
             }
diff --git a/src/java/org/apache/cassandra/exceptions/RequestFailure.java b/src/java/org/apache/cassandra/exceptions/RequestFailure.java
index 6946b812aa..9f6d0575ce 100644
--- a/src/java/org/apache/cassandra/exceptions/RequestFailure.java
+++ b/src/java/org/apache/cassandra/exceptions/RequestFailure.java
@@ -43,7 +43,7 @@ import static org.apache.cassandra.exceptions.ExceptionSerializer.nullableRemote
 public class RequestFailure
 {
     public static final RequestFailure UNKNOWN = new RequestFailure(RequestFailureReason.UNKNOWN);
-    public static final RequestFailure UNKNOWN_TOPOLOGY = new RequestFailure(RequestFailureReason.UNKNOWN_TOPOLOGY);
+    public static final RequestFailure ACCORD_DISABLED = new RequestFailure(RequestFailureReason.ACCORD_DISABLED);
     public static final RequestFailure READ_TOO_MANY_TOMBSTONES = new RequestFailure(RequestFailureReason.READ_TOO_MANY_TOMBSTONES);
     public static final RequestFailure TIMEOUT = new RequestFailure(RequestFailureReason.TIMEOUT);
     public static final RequestFailure INCOMPATIBLE_SCHEMA = new RequestFailure(RequestFailureReason.INCOMPATIBLE_SCHEMA);
@@ -55,7 +55,7 @@ public class RequestFailure
     public static final RequestFailure COORDINATOR_BEHIND = new RequestFailure(RequestFailureReason.COORDINATOR_BEHIND);
     public static final RequestFailure READ_TOO_MANY_INDEXES = new RequestFailure(RequestFailureReason.READ_TOO_MANY_INDEXES);
     public static final RequestFailure RETRY_ON_DIFFERENT_TRANSACTION_SYSTEM = new RequestFailure(RequestFailureReason.RETRY_ON_DIFFERENT_TRANSACTION_SYSTEM);
-    public static final RequestFailure BOOTING = new RequestFailure(RequestFailureReason.RETRY_ON_DIFFERENT_TRANSACTION_SYSTEM);
+    public static final RequestFailure BOOTING = new RequestFailure(RequestFailureReason.BOOTING);
 
     static
     {
@@ -135,7 +135,7 @@ public class RequestFailure
         {
             default: throw new IllegalStateException("Unhandled request failure reason " + reason);
             case UNKNOWN: return UNKNOWN;
-            case UNKNOWN_TOPOLOGY: return UNKNOWN_TOPOLOGY;
+            case ACCORD_DISABLED: return ACCORD_DISABLED;
             case READ_TOO_MANY_TOMBSTONES: return READ_TOO_MANY_TOMBSTONES;
             case TIMEOUT: return TIMEOUT;
             case INCOMPATIBLE_SCHEMA: return INCOMPATIBLE_SCHEMA;
diff --git a/src/java/org/apache/cassandra/exceptions/RequestFailureReason.java b/src/java/org/apache/cassandra/exceptions/RequestFailureReason.java
index b356400d19..38b921eb6a 100644
--- a/src/java/org/apache/cassandra/exceptions/RequestFailureReason.java
+++ b/src/java/org/apache/cassandra/exceptions/RequestFailureReason.java
@@ -51,7 +51,7 @@ public enum RequestFailureReason
     INDEX_BUILD_IN_PROGRESS                 (503),
     RETRY_ON_DIFFERENT_TRANSACTION_SYSTEM   (504),
     BOOTING                                 (505),
-    UNKNOWN_TOPOLOGY                        (506)
+    ACCORD_DISABLED                         (506)
     ;
 
     static
diff --git a/src/java/org/apache/cassandra/net/Verb.java b/src/java/org/apache/cassandra/net/Verb.java
index 20173ad443..aadd094517 100644
--- a/src/java/org/apache/cassandra/net/Verb.java
+++ b/src/java/org/apache/cassandra/net/Verb.java
@@ -81,14 +81,15 @@ import org.apache.cassandra.service.SnapshotVerbHandler;
 import org.apache.cassandra.service.accord.AccordService;
 import org.apache.cassandra.service.accord.AccordSyncPropagator;
 import org.apache.cassandra.service.accord.AccordSyncPropagator.Notification;
-import org.apache.cassandra.service.accord.FetchTopology;
-import org.apache.cassandra.service.accord.FetchMinEpoch;
+import org.apache.cassandra.service.accord.FetchTopologies;
+import org.apache.cassandra.service.accord.WatermarkCollector;
 import org.apache.cassandra.service.accord.interop.AccordInteropApply;
 import org.apache.cassandra.service.accord.interop.AccordInteropRead;
 import org.apache.cassandra.service.accord.interop.AccordInteropReadRepair;
 import org.apache.cassandra.service.accord.interop.AccordInteropStableThenRead;
 import org.apache.cassandra.service.accord.serializers.AcceptSerializers;
 import org.apache.cassandra.service.accord.serializers.ApplySerializers;
+import org.apache.cassandra.service.accord.serializers.AwaitSerializers;
 import org.apache.cassandra.service.accord.serializers.BeginInvalidationSerializers;
 import org.apache.cassandra.service.accord.serializers.CheckStatusSerializers;
 import org.apache.cassandra.service.accord.serializers.CommitSerializers;
@@ -103,7 +104,6 @@ import org.apache.cassandra.service.accord.serializers.GetDurableBeforeSerialize
 import org.apache.cassandra.service.accord.serializers.ReadDataSerializers;
 import org.apache.cassandra.service.accord.serializers.RecoverySerializers;
 import org.apache.cassandra.service.accord.serializers.SetDurableSerializers;
-import org.apache.cassandra.service.accord.serializers.AwaitSerializers;
 import org.apache.cassandra.service.consensus.migration.ConsensusKeyMigrationState;
 import org.apache.cassandra.service.consensus.migration.ConsensusKeyMigrationState.ConsensusKeyMigrationFinished;
 import org.apache.cassandra.service.paxos.Commit;
@@ -352,21 +352,21 @@ public enum Verb
     ACCORD_SET_GLOBALLY_DURABLE_REQ (157, P2, rpcTimeout,   MISC,               () -> SetDurableSerializers.globallyDurable,AccordService::requestHandlerOrNoop, ACCORD_SIMPLE_RSP                         ),
 
     ACCORD_SYNC_NOTIFY_RSP          (158, P2, writeTimeout, MISC,               () -> EnumSerializer.simpleReply,           RESPONSE_HANDLER),
-    ACCORD_SYNC_NOTIFY_REQ          (159, P2, writeTimeout, MISC,               () -> Notification.listSerializer,          () -> AccordSyncPropagator.verbHandler,       ACCORD_SYNC_NOTIFY_RSP             ),
+    ACCORD_SYNC_NOTIFY_REQ          (159, P2, writeTimeout, MISC,               () -> Notification.serializer,          () -> AccordSyncPropagator.verbHandler,       ACCORD_SYNC_NOTIFY_RSP             ),
 
 
     CONSENSUS_KEY_MIGRATION         (160, P1, writeTimeout,  MISC,              () -> ConsensusKeyMigrationFinished.serializer,() -> ConsensusKeyMigrationState.consensusKeyMigrationFinishedHandler),
 
     ACCORD_INTEROP_READ_RSP         (161, P2, writeTimeout, IMMEDIATE,          () -> AccordInteropRead.replySerializer,         AccordService::responseHandlerOrNoop),
-    ACCORD_INTEROP_READ_REQ         (162, P2, writeTimeout, IMMEDIATE,          () -> AccordInteropRead.requestSerializer,       AccordService::requestHandlerOrNoop, ACCORD_INTEROP_READ_RSP),
+    ACCORD_INTEROP_READ_REQ         (162, P2, writeTimeout, IMMEDIATE,          () -> AccordInteropRead.requestSerializer,       AccordService::requestHandlerOrNoop,     ACCORD_INTEROP_READ_RSP),
     ACCORD_INTEROP_STABLE_THEN_READ_REQ(163, P2, writeTimeout, IMMEDIATE,       () -> AccordInteropStableThenRead.requestSerializer, AccordService::requestHandlerOrNoop, ACCORD_INTEROP_READ_RSP),
     ACCORD_INTEROP_READ_REPAIR_RSP  (164, P2, writeTimeout, IMMEDIATE,          () -> AccordInteropReadRepair.replySerializer,   AccordService::responseHandlerOrNoop),
-    ACCORD_INTEROP_READ_REPAIR_REQ  (165, P2, writeTimeout, IMMEDIATE,          () -> AccordInteropReadRepair.requestSerializer, AccordService::requestHandlerOrNoop, ACCORD_INTEROP_READ_REPAIR_RSP),
-    ACCORD_INTEROP_APPLY_REQ        (166, P2, writeTimeout, IMMEDIATE,          () -> AccordInteropApply.serializer,             AccordService::requestHandlerOrNoop,             ACCORD_APPLY_RSP),
-    ACCORD_FETCH_MIN_EPOCH_RSP      (167, P0, shortTimeout, FETCH_METADATA,     () -> FetchMinEpoch.Response.serializer,         RESPONSE_HANDLER),
-    ACCORD_FETCH_MIN_EPOCH_REQ      (168, P0, shortTimeout, FETCH_METADATA,     () -> FetchMinEpoch.serializer,                  () -> FetchMinEpoch.handler, ACCORD_FETCH_MIN_EPOCH_RSP),
-    ACCORD_FETCH_TOPOLOGY_RSP       (169, P0, shortTimeout, FETCH_METADATA,     () -> FetchTopology.Response.serializer,         RESPONSE_HANDLER),
-    ACCORD_FETCH_TOPOLOGY_REQ       (170, P0, shortTimeout, FETCH_METADATA,     () -> FetchTopology.serializer,                  () -> FetchTopology.handler, ACCORD_FETCH_TOPOLOGY_RSP),
+    ACCORD_INTEROP_READ_REPAIR_REQ  (165, P2, writeTimeout, IMMEDIATE,          () -> AccordInteropReadRepair.requestSerializer, AccordService::requestHandlerOrNoop,     ACCORD_INTEROP_READ_REPAIR_RSP),
+    ACCORD_INTEROP_APPLY_REQ        (166, P2, writeTimeout, IMMEDIATE,          () -> AccordInteropApply.serializer,             AccordService::requestHandlerOrNoop,     ACCORD_APPLY_RSP),
+    ACCORD_FETCH_WATERMARKS_RSP     (167, P0, shortTimeout, FETCH_METADATA,     () -> WatermarkCollector.serializer,             RESPONSE_HANDLER),
+    ACCORD_FETCH_WATERMARKS_REQ     (168, P0, shortTimeout, FETCH_METADATA,     () -> NoPayload.serializer,                      AccordService::watermarkHandlerOrNoop,   ACCORD_FETCH_WATERMARKS_RSP),
+    ACCORD_FETCH_TOPOLOGY_RSP       (169, P0, shortTimeout, FETCH_METADATA,     () -> FetchTopologies.responseSerializer,        RESPONSE_HANDLER),
+    ACCORD_FETCH_TOPOLOGY_REQ       (170, P0, shortTimeout, FETCH_METADATA,     () -> FetchTopologies.serializer,                () -> FetchTopologies.handler,           ACCORD_FETCH_TOPOLOGY_RSP),
 
     // generic failure response
     FAILURE_RSP            (99,  P0, noTimeout,       REQUEST_RESPONSE,  () -> RequestFailure.serializer,            RESPONSE_HANDLER                             ),
diff --git a/src/java/org/apache/cassandra/service/accord/AccordConfigurationService.java b/src/java/org/apache/cassandra/service/accord/AccordConfigurationService.java
index f01def7639..0fbeb7d190 100644
--- a/src/java/org/apache/cassandra/service/accord/AccordConfigurationService.java
+++ b/src/java/org/apache/cassandra/service/accord/AccordConfigurationService.java
@@ -51,7 +51,6 @@ import org.apache.cassandra.locator.InetAddressAndPort;
 import org.apache.cassandra.net.MessageDelivery;
 import org.apache.cassandra.net.MessagingService;
 import org.apache.cassandra.repair.SharedContext;
-import org.apache.cassandra.service.accord.AccordKeyspace.EpochDiskState;
 import org.apache.cassandra.tcm.ClusterMetadata;
 import org.apache.cassandra.tcm.ClusterMetadataService;
 import org.apache.cassandra.tcm.Epoch;
@@ -62,6 +61,7 @@ import org.apache.cassandra.utils.Simulate;
 import org.apache.cassandra.utils.concurrent.AsyncPromise;
 import org.apache.cassandra.utils.concurrent.Future;
 
+import static accord.topology.TopologyManager.TopologyRange;
 import static org.apache.cassandra.service.accord.AccordTopology.tcmIdToAccord;
 import static org.apache.cassandra.utils.Simulate.With.MONITORS;
 
@@ -70,12 +70,9 @@ import static org.apache.cassandra.utils.Simulate.With.MONITORS;
 public class AccordConfigurationService extends AbstractConfigurationService<AccordConfigurationService.EpochState, AccordConfigurationService.EpochHistory> implements AccordEndpointMapper, AccordSyncPropagator.Listener, Shutdownable
 {
     private final AccordSyncPropagator syncPropagator;
-    private final DiskStateManager diskStateManager;
+    public final WatermarkCollector watermarkCollector;
 
-    @GuardedBy("this")
-    private EpochDiskState diskState = EpochDiskState.EMPTY;
-
-    private enum State { INITIALIZED, LOADING, STARTED, SHUTDOWN }
+    private enum State { INITIALIZED, STARTED, SHUTDOWN }
 
     @GuardedBy("this")
     private State state = State.INITIALIZED;
@@ -130,90 +127,6 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
         }
     }
 
-    @VisibleForTesting
-    interface DiskStateManager
-    {
-        /**
-         * Loads local states known to the _current_ node.
-         */
-        EpochDiskState loadLocalTopologyState(AccordKeyspace.TopologyLoadConsumer consumer);
-
-        EpochDiskState setNotifyingLocalSync(long epoch, Set<Node.Id> pending, EpochDiskState diskState);
-
-        EpochDiskState setCompletedLocalSync(long epoch, EpochDiskState diskState);
-
-        EpochDiskState markLocalSyncAck(Node.Id id, long epoch, EpochDiskState diskState);
-
-        EpochDiskState saveTopology(Topology topology, EpochDiskState diskState);
-
-        EpochDiskState markRemoteTopologySync(Node.Id node, long epoch, EpochDiskState diskState);
-
-        EpochDiskState markClosed(Ranges ranges, long epoch, EpochDiskState diskState);
-
-        EpochDiskState markRetired(Ranges ranges, long epoch, EpochDiskState diskState);
-
-        EpochDiskState truncateTopologyUntil(long epoch, EpochDiskState diskState);
-    }
-
-    enum SystemTableDiskStateManager implements DiskStateManager
-    {
-        instance;
-
-        @Override
-        public EpochDiskState loadLocalTopologyState(AccordKeyspace.TopologyLoadConsumer consumer)
-        {
-            return AccordKeyspace.loadTopologies(consumer);
-        }
-
-        @Override
-        public EpochDiskState setNotifyingLocalSync(long epoch, Set<Node.Id> notify, EpochDiskState diskState)
-        {
-            return AccordKeyspace.setNotifyingLocalSync(epoch, notify, diskState);
-        }
-
-        @Override
-        public EpochDiskState setCompletedLocalSync(long epoch, EpochDiskState diskState)
-        {
-            return AccordKeyspace.setCompletedLocalSync(epoch, diskState);
-        }
-
-        @Override
-        public EpochDiskState markLocalSyncAck(Node.Id id, long epoch, EpochDiskState diskState)
-        {
-            return AccordKeyspace.markLocalSyncAck(id, epoch, diskState);
-        }
-
-        @Override
-        public EpochDiskState saveTopology(Topology topology, EpochDiskState diskState)
-        {
-            return AccordKeyspace.saveTopology(topology, diskState);
-        }
-
-        @Override
-        public EpochDiskState markRemoteTopologySync(Node.Id node, long epoch, EpochDiskState diskState)
-        {
-            return AccordKeyspace.markRemoteTopologySync(node, epoch, diskState);
-        }
-
-        @Override
-        public EpochDiskState markClosed(Ranges ranges, long epoch, EpochDiskState diskState)
-        {
-            return AccordKeyspace.markClosed(ranges, epoch, diskState);
-        }
-
-        @Override
-        public EpochDiskState markRetired(Ranges ranges, long epoch, EpochDiskState diskState)
-        {
-            return AccordKeyspace.markRetired(ranges, epoch, diskState);
-        }
-
-        @Override
-        public EpochDiskState truncateTopologyUntil(long epoch, EpochDiskState diskState)
-        {
-            return AccordKeyspace.truncateTopologyUntil(epoch, diskState);
-        }
-    }
-
     //TODO (required): should not be public
     public final ChangeListener listener = new MetadataChangeListener();
     private class MetadataChangeListener implements ChangeListener
@@ -225,16 +138,17 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
         }
     }
 
-    public AccordConfigurationService(Node.Id node, MessageDelivery messagingService, IFailureDetector failureDetector, DiskStateManager diskStateManager, ScheduledExecutorPlus scheduledTasks)
+    public AccordConfigurationService(Node.Id node, MessageDelivery messagingService, IFailureDetector failureDetector, ScheduledExecutorPlus scheduledTasks)
     {
         super(node);
         this.syncPropagator = new AccordSyncPropagator(localId, this, messagingService, failureDetector, scheduledTasks, this);
-        this.diskStateManager = diskStateManager;
+        this.watermarkCollector = new WatermarkCollector();
+        listeners.add(watermarkCollector);
     }
 
     public AccordConfigurationService(Node.Id node)
     {
-        this(node, MessagingService.instance(), FailureDetector.instance, SystemTableDiskStateManager.instance, ScheduledExecutors.scheduledTasks);
+        this(node, MessagingService.instance(), FailureDetector.instance, ScheduledExecutors.scheduledTasks);
     }
 
     @Override
@@ -249,20 +163,6 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
     public synchronized void start()
     {
         Invariants.require(state == State.INITIALIZED, "Expected state to be INITIALIZED but was %s", state);
-        state = State.LOADING;
-
-        EndpointMapping snapshot = mapping;
-        diskStateManager.loadLocalTopologyState((epoch, syncStatus, pendingSyncNotify, remoteSyncComplete, closed, redundant) -> {
-            getOrCreateEpochState(epoch).setSyncStatus(syncStatus);
-            // TODO (expected, correctness): since this is loading old topologies, might see nodes no longer present (host replacement, decom, shrink, etc.); attempt to remove unknown nodes
-            if (Objects.requireNonNull(syncStatus) == SyncStatus.NOTIFYING)
-                syncPropagator.reportSyncComplete(epoch, Sets.filter(pendingSyncNotify, snapshot::containsId), localId);
-
-            remoteSyncComplete.forEach(id -> receiveRemoteSyncComplete(id, epoch));
-            // TODO (required): disk doesn't get updated until we see our own notification, so there is an edge case where this instance notified others and fails in the middle, but Apply was already sent!  This could leave partial closed/redudant accross the cluster
-            receiveClosed(closed, epoch);
-            receiveRetired(redundant, epoch);
-        });
         state = State.STARTED;
 
         // for all nodes removed, or pending removal, mark them as removed, so we don't wait on their replies
@@ -311,12 +211,6 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
         return mapping.mappedEndpointOrNull(id);
     }
 
-    @VisibleForTesting
-    synchronized EpochDiskState diskState()
-    {
-        return diskState;
-    }
-
     @VisibleForTesting
     synchronized void updateMapping(EndpointMapping mapping)
     {
@@ -446,7 +340,7 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
         getOrCreateEpochState(epoch - 1).acknowledged().addCallback(() -> reportMetadata(metadata));
     }
 
-    private final Map<Long, Future<Topology>> pendingTopologies = new ConcurrentHashMap<>();
+    private final Map<Long, Future<Void>> pendingTopologies = new ConcurrentHashMap<>();
 
     @Override
     public void fetchTopologyForEpoch(long epoch)
@@ -457,28 +351,29 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
             fetchTopologyInternal(i);
     }
 
-    private void fetchTopologyInternal(long epoch)
+    private static final Object Success = new Object();
+
+    protected void fetchTopologyInternal(long epoch)
     {
         pendingTopologies.computeIfAbsent(epoch, (epoch_) -> {
-            AsyncPromise<Topology> future = new AsyncPromise<>();
+            AsyncPromise<Void> future = new AsyncPromise<>();
             fetchTopologyAsync(epoch_,
-                               (topology, throwable) -> {
-                                      if (topology != null)
-                                      {
-                                          future.setSuccess(topology);
-                                      }
-                                      else
-                                      {
-                                          Invariants.require(future == pendingTopologies.remove(epoch_));
-                                          future.setFailure(Invariants.nonNull(throwable));
-                                          fetchTopologyForEpoch(epoch_);
-                                      }
+                               (success, throwable) -> {
+                                   Future<Void> removed = pendingTopologies.remove(epoch_);
+                                   Invariants.require(future == removed, "%s should be equal to %s", future, removed);
+                                   if (success != null)
+                                       future.setSuccess(null);
+                                   else
+                                   {
+                                       future.setFailure(Invariants.nonNull(throwable));
+                                       fetchTopologyForEpoch(epoch_);
+                                   }
                                   });
             return future;
         });
     }
 
-    private void fetchTopologyAsync(long epoch, BiConsumer<Topology, ? super Throwable> onResult)
+    private void fetchTopologyAsync(long epoch, BiConsumer<Object, ? super Throwable> onResult)
     {
         // It's not safe for this to block on CMS so for now pick a thread pool to handle it
         Stage.ACCORD_MIGRATION.execute(() -> {
@@ -492,6 +387,7 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
                 onResult.accept(null, t);
                 return;
             }
+
             // In most cases, after fetching log from CMS, we will be caught up to the required epoch.
             // This TCM will also notify Accord via reportMetadata, so we do not need to fetch topologies.
             // If metadata has reported has skipped one or more epochs, and is _ahead_ of the requested epoch,
@@ -499,7 +395,7 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
             ClusterMetadata metadata = ClusterMetadata.current();
             if (metadata.epoch.getEpoch() == epoch)
             {
-                onResult.accept(AccordTopology.createAccordTopology(metadata), null);
+                onResult.accept(Success, null);
                 return;
             }
 
@@ -508,25 +404,26 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
                 Set<InetAddressAndPort> peers = new HashSet<>(metadata.directory.allJoinedEndpoints());
                 peers.remove(FBUtilities.getBroadcastAddressAndPort());
                 if (peers.isEmpty())
+                {
+                    onResult.accept(Success, null);
                     return;
+                }
 
-                // TODO (required): fetch only _missing_ topologies.
-                Topology topology = FetchTopology.fetch(SharedContext.Global.instance, peers, epoch).get();
-                Invariants.require(topology.epoch() == epoch);
-                reportTopology(topology);
-                onResult.accept(topology, null);
+                // Fetching only one epoch here since later epochs might have already been requested concurrently
+                TopologyRange result = FetchTopologies.fetch(SharedContext.Global.instance, peers, epoch, epoch).get();
+                result.forEach(this::reportTopology, epoch, 1);
+                onResult.accept(Success, null);
             }
             catch (Throwable e)
             {
                 if (currentEpoch() >= epoch)
                 {
-                    onResult.accept(getTopologyForEpoch(epoch), null);
+                    onResult.accept(Success, null);
                     return;
                 }
                 if (e instanceof InterruptedException)
                     Thread.currentThread().interrupt();
                 onResult.accept(null, e);
-                throw new RuntimeException(e.getCause());
             }
         });
     }
@@ -534,7 +431,9 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
     @Override
     public void reportTopology(Topology topology, boolean isLoad, boolean startSync)
     {
-        Invariants.require(topology.epoch() <= ClusterMetadata.current().epoch.getEpoch());
+        long tcmEpoch = ClusterMetadata.current().epoch.getEpoch();
+        Invariants.require(topology.epoch() <= tcmEpoch,
+                           "Reported topology %s not known to TCM", topology.epoch(), tcmEpoch);
         super.reportTopology(topology, isLoad, startSync);
     }
 
@@ -546,55 +445,50 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
         if (!startSync || epochState.syncStatus != SyncStatus.NOT_STARTED)
             return;
 
-        Set<Node.Id> notify = topology.nodes().stream().filter(i -> !localId.equals(i)).collect(Collectors.toSet());
         synchronized (this)
         {
             if (epochState.syncStatus != SyncStatus.NOT_STARTED)
                 return;
-            diskState = diskStateManager.setNotifyingLocalSync(epoch, notify, diskState);
             epochState.setSyncStatus(SyncStatus.NOTIFYING);
         }
+
+        // TODO (required): replace with SortedArraySet when it is available
+        Set<Node.Id> notify = new HashSet<>(topology.nodes());
+        notify.remove(localId);
         syncPropagator.reportSyncComplete(epoch, notify, localId);
     }
 
     @Override
     public synchronized void onEndpointAck(Node.Id id, long epoch)
     {
-        EpochState epochState = getOrCreateEpochState(epoch);
-        if (epochState.syncStatus != SyncStatus.NOTIFYING)
-            return;
-        diskState = diskStateManager.markLocalSyncAck(id, epoch, diskState);
     }
 
     @Override
     public void onComplete(long epoch)
     {
+        if (epochs.wasTruncated(epoch))
+            return;
+
         EpochState epochState = getOrCreateEpochState(epoch);
         synchronized (this)
         {
             epochState.setSyncStatus(SyncStatus.COMPLETED);
-            diskState = diskStateManager.setCompletedLocalSync(epoch, diskState);
         }
     }
 
-    @Override
-    protected synchronized void topologyUpdatePreListenerNotify(Topology topology)
-    {
-        if (state == State.STARTED)
-            diskState = diskStateManager.saveTopology(topology, diskState);
-    }
-
     @Override
     protected synchronized void receiveRemoteSyncCompletePreListenerNotify(Node.Id node, long epoch)
     {
-        if (state == State.STARTED)
-            diskState = diskStateManager.markRemoteTopologySync(node, epoch, diskState);
     }
 
     @Override
     public void reportEpochClosed(Ranges ranges, long epoch)
     {
         checkStarted();
+        EpochHistory epochs = this.epochs;
+        if (epoch < minEpoch() || epochs.wasTruncated(epoch))
+            return;
+
         Topology topology = getTopologyForEpoch(epoch);
         syncPropagator.reportClosed(epoch, topology.nodes(), ranges);
     }
@@ -617,36 +511,21 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
     @Override
     public void receiveClosed(Ranges ranges, long epoch)
     {
-        synchronized (this)
-        {
-            diskState = diskStateManager.markClosed(ranges, epoch, diskState);
-        }
         super.receiveClosed(ranges, epoch);
     }
 
     @Override
     public void receiveRetired(Ranges ranges, long epoch)
     {
-        synchronized (this)
-        {
-            diskState = diskStateManager.markRetired(ranges, epoch, diskState);
-        }
         super.receiveRetired(ranges, epoch);
     }
 
     @Override
-    protected void truncateTopologiesPreListenerNotify(long epoch)
+    public void reportEpochRemoved(long epoch)
     {
-        checkStarted();
+        epochs.truncateUntil(epoch);
     }
-
-    @Override
-    protected synchronized void truncateTopologiesPostListenerNotify(long epoch)
-    {
-        if (state == State.STARTED)
-            diskState = diskStateManager.truncateTopologyUntil(epoch, diskState);
-    }
-
+    
     private synchronized void checkStarted()
     {
         State state = this.state;
@@ -721,11 +600,6 @@ public class AccordConfigurationService extends AbstractConfigurationService<Acc
         {
             return new EpochSnapshot(epoch, SyncStatus.COMPLETED, ResultStatus.SUCCESS, ResultStatus.SUCCESS, ResultStatus.SUCCESS);
         }
-
-        public static EpochSnapshot notStarted(long epoch)
-        {
-            return new EpochSnapshot(epoch, SyncStatus.NOT_STARTED, ResultStatus.SUCCESS, ResultStatus.SUCCESS, ResultStatus.SUCCESS);
-        }
     }
 
     @VisibleForTesting
diff --git a/src/java/org/apache/cassandra/service/accord/AccordFastPathCoordinator.java b/src/java/org/apache/cassandra/service/accord/AccordFastPathCoordinator.java
index 4c878ce8a3..4e214e664f 100644
--- a/src/java/org/apache/cassandra/service/accord/AccordFastPathCoordinator.java
+++ b/src/java/org/apache/cassandra/service/accord/AccordFastPathCoordinator.java
@@ -342,7 +342,6 @@ public abstract class AccordFastPathCoordinator implements ChangeListener, Confi
     }
 
     @Override public void onRemoteSyncComplete(Node.Id node, long epoch) {}
-    @Override public void truncateTopologyUntil(long epoch) {}
     @Override public void onEpochClosed(Ranges ranges, long epoch) {}
     @Override public void onEpochRetired(Ranges ranges, long epoch) {}
 }
diff --git a/src/java/org/apache/cassandra/service/accord/AccordJournal.java b/src/java/org/apache/cassandra/service/accord/AccordJournal.java
index 4ce3e2a64c..c8f5387e31 100644
--- a/src/java/org/apache/cassandra/service/accord/AccordJournal.java
+++ b/src/java/org/apache/cassandra/service/accord/AccordJournal.java
@@ -39,6 +39,7 @@ import accord.local.CommandStores.RangesForEpoch;
 import accord.local.DurableBefore;
 import accord.local.Node;
 import accord.local.RedundantBefore;
+import accord.primitives.EpochSupplier;
 import accord.primitives.Ranges;
 import accord.primitives.SaveStatus;
 import accord.primitives.Status.Durability;
@@ -68,6 +69,7 @@ import org.apache.cassandra.net.MessagingService;
 import org.apache.cassandra.service.accord.AccordJournalValueSerializers.IdentityAccumulator;
 import org.apache.cassandra.service.accord.JournalKey.JournalKeySupport;
 import org.apache.cassandra.service.accord.api.AccordAgent;
+import org.apache.cassandra.service.accord.journal.AccordTopologyUpdate;
 import org.apache.cassandra.service.accord.serializers.CommandSerializers;
 import org.apache.cassandra.service.accord.serializers.CommandSerializers.ExecuteAtSerializer;
 import org.apache.cassandra.service.accord.serializers.DepsSerializers;
@@ -294,16 +296,17 @@ public class AccordJournal implements accord.api.Journal, RangeSearcher.Supplier
     }
 
     @Override
-    public Iterator<TopologyUpdate> replayTopologies()
+    public Iterator<AccordTopologyUpdate.ImmutableTopoloyImage> replayTopologies()
     {
-        AccordJournalValueSerializers.MapAccumulator<Long, TopologyUpdate> accumulator = readAll(new JournalKey(TxnId.NONE, JournalKey.Type.TOPOLOGY_UPDATE, 0));
-        return accumulator.get().values().iterator();
+        AccordTopologyUpdate.Accumulator accumulator = readAll(TopologyUpdateKey);
+        return accumulator.images();
     }
 
+    private static final JournalKey TopologyUpdateKey = new JournalKey(TxnId.NONE, JournalKey.Type.TOPOLOGY_UPDATE, 0);
     @Override
     public void saveTopology(TopologyUpdate topologyUpdate, Runnable onFlush)
     {
-        RecordPointer pointer = appendInternal(new JournalKey(TxnId.NONE, JournalKey.Type.TOPOLOGY_UPDATE, 0), topologyUpdate);
+        RecordPointer pointer = appendInternal(TopologyUpdateKey, AccordTopologyUpdate.newTopology(topologyUpdate));
         if (onFlush != null)
             journal.onDurable(pointer, onFlush);
     }
@@ -380,9 +383,9 @@ public class AccordJournal implements accord.api.Journal, RangeSearcher.Supplier
         return builder;
     }
 
-    private RecordPointer appendInternal(JournalKey key, Object write)
+    private <T> RecordPointer appendInternal(JournalKey key, T write)
     {
-        AccordJournalValueSerializers.FlyweightSerializer<Object, ?> serializer = (AccordJournalValueSerializers.FlyweightSerializer<Object, ?>) key.type.serializer;
+        AccordJournalValueSerializers.FlyweightSerializer<T, ?> serializer = (AccordJournalValueSerializers.FlyweightSerializer<T, ?>) key.type.serializer;
         return journal.asyncWrite(key, (out, userVersion) -> serializer.serialize(key, write, out, userVersion));
     }
 
@@ -419,7 +422,7 @@ public class AccordJournal implements accord.api.Journal, RangeSearcher.Supplier
     }
 
     @Override
-    public void purge(CommandStores commandStores)
+    public void purge(CommandStores commandStores, EpochSupplier minEpoch)
     {
         journal.closeCurrentSegmentForTestingIfNonEmpty();
         journal.runCompactorForTesting();
@@ -819,4 +822,4 @@ public class AccordJournal implements accord.api.Journal, RangeSearcher.Supplier
             }
         }
     }
-}
+}
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/service/accord/AccordJournalValueSerializers.java b/src/java/org/apache/cassandra/service/accord/AccordJournalValueSerializers.java
index 12c61ec7ca..0ca7cfcf28 100644
--- a/src/java/org/apache/cassandra/service/accord/AccordJournalValueSerializers.java
+++ b/src/java/org/apache/cassandra/service/accord/AccordJournalValueSerializers.java
@@ -19,28 +19,20 @@
 package org.apache.cassandra.service.accord;
 
 import java.io.IOException;
-import java.util.Map;
 import java.util.NavigableMap;
-import java.util.TreeMap;
-import java.util.function.Function;
 
 import com.google.common.collect.ImmutableSortedMap;
 
-import accord.api.Journal;
 import accord.local.DurableBefore;
 import accord.local.RedundantBefore;
 import accord.primitives.Ranges;
 import accord.primitives.Timestamp;
 import accord.primitives.TxnId;
-import accord.topology.Topology;
-import accord.utils.Invariants;
-import org.agrona.collections.Int2ObjectHashMap;
 import org.apache.cassandra.io.util.DataInputPlus;
 import org.apache.cassandra.io.util.DataOutputPlus;
 import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.service.accord.journal.AccordTopologyUpdate;
 import org.apache.cassandra.service.accord.serializers.CommandStoreSerializers;
-import org.apache.cassandra.service.accord.serializers.KeySerializers;
-import org.apache.cassandra.service.accord.serializers.TopologySerializers;
 
 import static accord.api.Journal.Load.ALL;
 import static accord.local.CommandStores.RangesForEpoch;
@@ -295,6 +287,7 @@ public class AccordJournalValueSerializers
     public static class RangesForEpochSerializer
     implements FlyweightSerializer<RangesForEpoch, Accumulator<RangesForEpoch, RangesForEpoch>>
     {
+        public static final RangesForEpochSerializer instance = new RangesForEpochSerializer();
         public IdentityAccumulator<RangesForEpoch> mergerFor(JournalKey key)
         {
             return new IdentityAccumulator<>(null);
@@ -303,18 +296,7 @@ public class AccordJournalValueSerializers
         @Override
         public void serialize(JournalKey key, RangesForEpoch from, DataOutputPlus out, int userVersion) throws IOException
         {
-            out.writeUnsignedVInt32(from.size());
-            from.forEach((epoch, ranges) -> {
-                try
-                {
-                    out.writeLong(epoch);
-                    KeySerializers.ranges.serialize(ranges, out, messagingVersion);
-                }
-                catch (Throwable t)
-                {
-                    throw new IllegalStateException("Serialization error", t);
-                }
-            });
+            AccordTopologyUpdate.RangesForEpochSerializer.instance.serialize(from, out, userVersion);
         }
 
         @Override
@@ -326,103 +308,7 @@ public class AccordJournalValueSerializers
         @Override
         public void deserialize(JournalKey key, Accumulator<RangesForEpoch, RangesForEpoch> into, DataInputPlus in, int userVersion) throws IOException
         {
-            int size = in.readUnsignedVInt32();
-            Ranges[] ranges = new Ranges[size];
-            long[] epochs = new long[size];
-            for (int i = 0; i < ranges.length; i++)
-            {
-                epochs[i] = in.readLong();
-                ranges[i] = KeySerializers.ranges.deserialize(in, messagingVersion);
-            }
-            Invariants.require(ranges.length == epochs.length);
-            into.update(new RangesForEpoch(epochs, ranges));
-        }
-    }
-
-    public static class MapAccumulator<K extends Comparable<K>, V> extends Accumulator<NavigableMap<K, V>, V>
-    {
-        private final Function<V, K> getKey;
-
-        public MapAccumulator(Function<V, K> getKey)
-        {
-            super(new TreeMap<>());
-            this.getKey = getKey;
-        }
-
-        @Override
-        protected NavigableMap<K, V> accumulate(NavigableMap<K, V> accumulator, V newValue)
-        {
-            V prev = accumulator.put(getKey.apply(newValue), newValue);
-            Invariants.require(prev == null || prev.equals(newValue));
-            return accumulator;
-        }
-    }
-
-    public static class TopologyUpdateSerializer
-    implements FlyweightSerializer<Journal.TopologyUpdate, MapAccumulator<Long, Journal.TopologyUpdate>>
-    {
-        private final RangesForEpochSerializer rangesForEpochSerializer = new RangesForEpochSerializer();
-
-        @Override
-        public MapAccumulator<Long, Journal.TopologyUpdate> mergerFor(JournalKey key)
-        {
-            return new MapAccumulator<>(topologyUpdate -> topologyUpdate.global.epoch());
-        }
-
-        @Override
-        public void serialize(JournalKey key, Journal.TopologyUpdate from, DataOutputPlus out, int userVersion) throws IOException
-        {
-            out.writeInt(1);
-            serializeOne(key, from, out, userVersion);
-        }
-
-        private void serializeOne(JournalKey key, Journal.TopologyUpdate from, DataOutputPlus out, int userVersion) throws IOException
-        {
-            out.writeInt(from.commandStores.size());
-            for (Map.Entry<Integer, RangesForEpoch> e : from.commandStores.entrySet())
-            {
-                out.writeInt(e.getKey());
-                rangesForEpochSerializer.serialize(key, e.getValue(), out, userVersion);
-            }
-            TopologySerializers.topology.serialize(from.local, out, userVersion);
-            TopologySerializers.topology.serialize(from.global, out, userVersion);
-        }
-
-        @Override
-        public void reserialize(JournalKey key, MapAccumulator<Long, Journal.TopologyUpdate> from, DataOutputPlus out, int userVersion) throws IOException
-        {
-            out.writeInt(from.accumulated.size());
-            for (Journal.TopologyUpdate update : from.accumulated.values())
-                serializeOne(key, update, out, userVersion);
-        }
-
-        @Override
-        public void deserialize(JournalKey key, MapAccumulator<Long, Journal.TopologyUpdate> into, DataInputPlus in, int userVersion) throws IOException
-        {
-            int size = in.readInt();
-            Accumulator<RangesForEpoch, RangesForEpoch> acc = new Accumulator<>(null)
-            {
-                @Override
-                protected RangesForEpoch accumulate(RangesForEpoch oldValue, RangesForEpoch newValue)
-                {
-                    return this.accumulated = newValue;
-                }
-            };
-            for (int i = 0; i < size; i++)
-            {
-                int commandStoresSize = in.readInt();
-                Int2ObjectHashMap<RangesForEpoch> commandStores = new Int2ObjectHashMap<>();
-                for (int j = 0; j < commandStoresSize; j++)
-                {
-                    acc.update(null);
-                    int commandStoreId = in.readInt();
-                    rangesForEpochSerializer.deserialize(key, acc, in, userVersion);
-                    commandStores.put(commandStoreId, acc.accumulated);
-                }
-                Topology local = TopologySerializers.topology.deserialize(in, userVersion);
-                Topology global = TopologySerializers.topology.deserialize(in, userVersion);
-                into.update(new Journal.TopologyUpdate(commandStores, local, global));
-            }
+            into.update(AccordTopologyUpdate.RangesForEpochSerializer.instance.deserialize(in, userVersion));
         }
     }
 }
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/service/accord/AccordKeyspace.java b/src/java/org/apache/cassandra/service/accord/AccordKeyspace.java
index b676b110e3..fed2683e32 100644
--- a/src/java/org/apache/cassandra/service/accord/AccordKeyspace.java
+++ b/src/java/org/apache/cassandra/service/accord/AccordKeyspace.java
@@ -19,17 +19,13 @@
 package org.apache.cassandra.service.accord;
 
 import java.io.IOException;
-import java.io.UncheckedIOException;
 import java.nio.ByteBuffer;
 import java.util.ArrayList;
-import java.util.Collections;
 import java.util.Iterator;
 import java.util.List;
 import java.util.Set;
 import java.util.concurrent.TimeUnit;
 import java.util.function.Consumer;
-import java.util.stream.Collectors;
-import javax.annotation.Nullable;
 
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.ImmutableMap;
@@ -38,16 +34,12 @@ import com.google.common.collect.Lists;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import accord.local.Node;
 import accord.local.RedundantBefore;
 import accord.local.cfk.CommandsForKey;
 import accord.local.cfk.Serialize;
-import accord.primitives.Ranges;
 import accord.primitives.TxnId;
-import accord.topology.Topology;
 import accord.utils.Invariants;
 import org.apache.cassandra.cql3.ColumnIdentifier;
-import org.apache.cassandra.cql3.UntypedResultSet;
 import org.apache.cassandra.cql3.statements.schema.CreateTableStatement;
 import org.apache.cassandra.db.Clustering;
 import org.apache.cassandra.db.ClusteringComparator;
@@ -112,10 +104,8 @@ import org.apache.cassandra.schema.Tables;
 import org.apache.cassandra.schema.Types;
 import org.apache.cassandra.schema.UserFunctions;
 import org.apache.cassandra.schema.Views;
-import org.apache.cassandra.service.accord.AccordConfigurationService.SyncStatus;
 import org.apache.cassandra.service.accord.api.TokenKey;
 import org.apache.cassandra.service.accord.serializers.CommandSerializers;
-import org.apache.cassandra.service.accord.serializers.KeySerializers;
 import org.apache.cassandra.utils.AbstractIterator;
 import org.apache.cassandra.utils.Clock.Global;
 import org.apache.cassandra.utils.CloseableIterator;
@@ -123,14 +113,11 @@ import org.apache.cassandra.utils.MergeIterator;
 import org.apache.cassandra.utils.btree.BTreeSet;
 import org.apache.cassandra.utils.concurrent.OpOrder;
 
-import static accord.utils.Invariants.require;
 import static java.lang.String.format;
 import static java.util.Collections.emptyMap;
-import static org.apache.cassandra.cql3.QueryProcessor.executeInternal;
 import static org.apache.cassandra.db.partitions.PartitionUpdate.singleRowUpdate;
 import static org.apache.cassandra.db.rows.BTreeRow.singleCellRow;
 import static org.apache.cassandra.schema.SchemaConstants.ACCORD_KEYSPACE_NAME;
-import static org.apache.cassandra.service.accord.serializers.KeySerializers.blobMapToRanges;
 
 public class AccordKeyspace
 {
@@ -138,11 +125,9 @@ public class AccordKeyspace
 
     public static final String JOURNAL = "journal";
     public static final String COMMANDS_FOR_KEY = "commands_for_key";
-    public static final String TOPOLOGIES = "topologies";
-    public static final String EPOCH_METADATA = "epoch_metadata";
     public static final String JOURNAL_INDEX_NAME = "record";
 
-    public static final Set<String> TABLE_NAMES = ImmutableSet.of(COMMANDS_FOR_KEY, TOPOLOGIES, EPOCH_METADATA, JOURNAL);
+    public static final Set<String> TABLE_NAMES = ImmutableSet.of(COMMANDS_FOR_KEY, JOURNAL);
 
     // TODO (desired): implement a custom type so we can get correct sort order
     public static final TupleType TIMESTAMP_TYPE = new TupleType(Lists.newArrayList(LongType.instance, LongType.instance, Int32Type.instance));
@@ -510,26 +495,7 @@ public class AccordKeyspace
         }
     }
 
-    public static final TableMetadata Topologies =
-        parse(TOPOLOGIES,
-              "accord topologies",
-              "CREATE TABLE %s (" +
-              "epoch bigint primary key, " +
-              "sync_state int, " +
-              "pending_sync_notify set<int>, " + // nodes that need to be told we're synced
-              "remote_sync_complete set<int>, " +  // nodes that have told us they're synced
-              "closed map<blob, blob>, " +
-              "retired map<blob, blob>" +
-              ')').build();
-
-    public static final TableMetadata EpochMetadata =
-        parse(EPOCH_METADATA,
-              "global epoch info",
-              "CREATE TABLE %s (" +
-              "key int primary key, " +
-              "min_epoch bigint, " +
-              "max_epoch bigint " +
-              ')').build();
+    public static final CommandsForKeyAccessor CommandsForKeysAccessor = new CommandsForKeyAccessor(CommandsForKeys);
 
     private static TableMetadata.Builder parse(String name, String description, String cql)
     {
@@ -549,7 +515,7 @@ public class AccordKeyspace
         return KeyspaceMetadata.create(ACCORD_KEYSPACE_NAME, KeyspaceParams.local(), tables(), Views.none(), Types.none(), UserFunctions.none());
     }
 
-    public static Tables TABLES = Tables.of(CommandsForKeys, Topologies, EpochMetadata, Journal);
+    public static Tables TABLES = Tables.of(CommandsForKeys, Journal);
     public static Tables tables()
     {
         return TABLES;
@@ -577,76 +543,6 @@ public class AccordKeyspace
         return (cell != null && !cell.isTombstone()) ? cellValue(cell) : null;
     }
 
-    public static class EpochDiskState
-    {
-        public static final EpochDiskState EMPTY = new EpochDiskState(0, 0);
-        public final long minEpoch;
-        public final long maxEpoch;
-
-        private EpochDiskState(long minEpoch, long maxEpoch)
-        {
-            Invariants.requireArgument(minEpoch >= 0, "Min Epoch %d < 0", minEpoch);
-            Invariants.requireArgument(maxEpoch >= minEpoch, "Max epoch %d < min %d", maxEpoch, minEpoch);
-            this.minEpoch = minEpoch;
-            this.maxEpoch = maxEpoch;
-        }
-
-        public static EpochDiskState create(long minEpoch, long maxEpoch)
-        {
-            if (minEpoch == maxEpoch && minEpoch == 0)
-                return EMPTY;
-            return new EpochDiskState(minEpoch, maxEpoch);
-        }
-
-        public static EpochDiskState create(long epoch)
-        {
-            return create(epoch, epoch);
-        }
-
-        public boolean isEmpty()
-        {
-            return minEpoch == maxEpoch && maxEpoch == 0;
-        }
-
-        @VisibleForTesting
-        EpochDiskState withNewMaxEpoch(long epoch)
-        {
-            Invariants.requireArgument(epoch > maxEpoch, "Epoch %d <= %d (max)", epoch, maxEpoch);
-            return EpochDiskState.create(Math.max(1, minEpoch), epoch);
-        }
-
-        private EpochDiskState withNewMinEpoch(long epoch)
-        {
-            Invariants.requireArgument(epoch > minEpoch, "epoch %d <= %d (min)", epoch, minEpoch);
-            Invariants.requireArgument(epoch <= maxEpoch, "epoch %d > %d (max)", epoch, maxEpoch);
-            return EpochDiskState.create(epoch, maxEpoch);
-        }
-
-        @Override
-        public String toString()
-        {
-            return "EpochDiskState{" +
-                   "minEpoch=" + minEpoch +
-                   ", maxEpoch=" + maxEpoch +
-                   '}';
-        }
-
-        @Override
-        public boolean equals(Object o)
-        {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-            EpochDiskState diskState = (EpochDiskState) o;
-            return minEpoch == diskState.minEpoch && maxEpoch == diskState.maxEpoch;
-        }
-
-        @Override
-        public int hashCode()
-        {
-            throw new UnsupportedOperationException();
-        }
-    }
-
     public static class JournalColumns
     {
         static final ClusteringComparator keyComparator = Journal.partitionKeyAsClusteringComparator();
@@ -692,11 +588,6 @@ public class AccordKeyspace
             return JournalKey.Type.fromId(ByteType.instance.compose(partitionKeyComponents[type.position()]));
         }
 
-        public static TxnId getTxnId(DecoratedKey key)
-        {
-            return getTxnId(splitPartitionKey(key));
-        }
-
         public static TxnId getTxnId(ByteBuffer[] partitionKeyComponents)
         {
             ByteBuffer buffer = partitionKeyComponents[id.position()];
@@ -710,179 +601,6 @@ public class AccordKeyspace
         }
     }
 
-    private static EpochDiskState saveEpochDiskState(EpochDiskState diskState)
-    {
-        String cql = "INSERT INTO " + ACCORD_KEYSPACE_NAME + '.' + EPOCH_METADATA + ' ' +
-                     "(key, min_epoch, max_epoch) VALUES (0, ?, ?);";
-        executeInternal(cql, diskState.minEpoch, diskState.maxEpoch);
-        return diskState;
-    }
-
-    @Nullable
-    @VisibleForTesting
-    public static EpochDiskState loadEpochDiskState()
-    {
-        String cql = "SELECT * FROM " + ACCORD_KEYSPACE_NAME + '.' + EPOCH_METADATA + ' ' +
-                     "WHERE key=0";
-        UntypedResultSet result = executeInternal(format(cql, ACCORD_KEYSPACE_NAME, EPOCH_METADATA));
-        if (result.isEmpty())
-            return null;
-        UntypedResultSet.Row row = result.one();
-        return EpochDiskState.create(row.getLong("min_epoch"), row.getLong("max_epoch"));
-    }
-
-    /**
-     * Update the disk state for this epoch, if it's higher than the one we have one disk.
-     * <p>
-     * This is meant to be called before any update involving the new epoch, not after. This way if the update
-     * fails, we can detect and cleanup. If we updated disk state after an update and it failed, we could "forget"
-     * about (now acked) topology updates after a restart.
-     */
-    private static EpochDiskState maybeUpdateMaxEpoch(EpochDiskState diskState, long epoch)
-    {
-        if (diskState.isEmpty())
-            return saveEpochDiskState(EpochDiskState.create(epoch));
-        Invariants.requireArgument(epoch >= diskState.minEpoch, "Epoch %d < %d (min)", epoch, diskState.minEpoch);
-        if (epoch > diskState.maxEpoch)
-        {
-            diskState = diskState.withNewMaxEpoch(epoch);
-            saveEpochDiskState(diskState);
-        }
-        return diskState;
-    }
-
-    public static EpochDiskState saveTopology(Topology topology, EpochDiskState diskState)
-    {
-        return maybeUpdateMaxEpoch(diskState, topology.epoch());
-    }
-
-    public static EpochDiskState markRemoteTopologySync(Node.Id node, long epoch, EpochDiskState diskState)
-    {
-        diskState = maybeUpdateMaxEpoch(diskState, epoch);
-        String cql = "UPDATE " + ACCORD_KEYSPACE_NAME + '.' + TOPOLOGIES + ' ' +
-                     "SET remote_sync_complete = remote_sync_complete + ? WHERE epoch = ?";
-        executeInternal(cql,
-                        Collections.singleton(node.id), epoch);
-        return diskState;
-    }
-
-    public static EpochDiskState markClosed(Ranges ranges, long epoch, EpochDiskState diskState)
-    {
-        diskState = maybeUpdateMaxEpoch(diskState, epoch);
-        String cql = "UPDATE " + ACCORD_KEYSPACE_NAME + '.' + TOPOLOGIES + ' ' +
-                     "SET closed = closed + ? WHERE epoch = ?";
-        executeInternal(cql, KeySerializers.rangesToBlobMap(ranges), epoch);
-        return diskState;
-    }
-
-    public static EpochDiskState markRetired(Ranges ranges, long epoch, EpochDiskState diskState)
-    {
-        diskState = maybeUpdateMaxEpoch(diskState, epoch);
-        String cql = "UPDATE " + ACCORD_KEYSPACE_NAME + '.' + TOPOLOGIES + ' ' +
-                     "SET retired = retired + ? WHERE epoch = ?";
-        executeInternal(cql, KeySerializers.rangesToBlobMap(ranges), epoch);
-        return diskState;
-    }
-
-    public static EpochDiskState setNotifyingLocalSync(long epoch, Set<Node.Id> pending, EpochDiskState diskState)
-    {
-        diskState = maybeUpdateMaxEpoch(diskState, epoch);
-        String cql = "UPDATE " + ACCORD_KEYSPACE_NAME + '.' + TOPOLOGIES + ' ' +
-                     "SET sync_state = ?, pending_sync_notify = ? WHERE epoch = ?";
-        executeInternal(cql,
-                        SyncStatus.NOTIFYING.ordinal(),
-                        pending.stream().map(i -> i.id).collect(Collectors.toSet()),
-                        epoch);
-        return diskState;
-    }
-
-    public static EpochDiskState markLocalSyncAck(Node.Id node, long epoch, EpochDiskState diskState)
-    {
-        diskState = maybeUpdateMaxEpoch(diskState, epoch);
-        String cql = "UPDATE " + ACCORD_KEYSPACE_NAME + '.' + TOPOLOGIES + ' ' +
-                     "SET pending_sync_notify = pending_sync_notify - ? WHERE epoch = ?";
-        executeInternal(cql,
-                        Collections.singleton(node.id), epoch);
-        return diskState;
-    }
-
-    public static EpochDiskState setCompletedLocalSync(long epoch, EpochDiskState diskState)
-    {
-        diskState = maybeUpdateMaxEpoch(diskState, epoch);
-        String cql = "UPDATE " + ACCORD_KEYSPACE_NAME + '.' + TOPOLOGIES + ' ' +
-                     "SET sync_state = ?, pending_sync_notify = {} WHERE epoch = ?";
-        executeInternal(cql,
-                        SyncStatus.COMPLETED.ordinal(),
-                        epoch);
-        return diskState;
-    }
-
-    public static EpochDiskState truncateTopologyUntil(final long epoch, EpochDiskState diskState)
-    {
-        while (diskState.minEpoch < epoch)
-        {
-            long delete = diskState.minEpoch;
-            diskState = diskState.withNewMinEpoch(delete + 1);
-            saveEpochDiskState(diskState);
-            String cql = "DELETE FROM " + ACCORD_KEYSPACE_NAME + '.' + TOPOLOGIES + ' ' +
-                         "WHERE epoch = ?";
-            executeInternal(cql, delete);
-        }
-        return diskState;
-    }
-
-    public interface TopologyLoadConsumer
-    {
-        void load(long epoch, SyncStatus syncStatus, Set<Node.Id> pendingSyncNotify, Set<Node.Id> remoteSyncComplete, Ranges closed, Ranges redundant);
-    }
-
-    @VisibleForTesting
-    public static void loadEpoch(long epoch, TopologyLoadConsumer consumer) throws IOException
-    {
-        UntypedResultSet result = executeInternal(String.format("SELECT * FROM %s.%s WHERE epoch=?", ACCORD_KEYSPACE_NAME, TOPOLOGIES), epoch);
-        if (result.isEmpty())
-        {
-            // topology updates disk state for epoch but doesn't save the topology to the table, so there maybe an epoch we know about, but no fields are present
-            consumer.load(epoch, SyncStatus.NOT_STARTED, Collections.emptySet(), Collections.emptySet(), Ranges.EMPTY, Ranges.EMPTY);
-            return;
-        }
-        require(!result.isEmpty(), "Nothing found for epoch %d", epoch);
-        UntypedResultSet.Row row = result.one();
-
-        SyncStatus syncStatus = row.has("sync_state")
-                                ? SyncStatus.values()[row.getInt("sync_state")]
-                                : SyncStatus.NOT_STARTED;
-        Set<Node.Id> pendingSyncNotify = row.has("pending_sync_notify")
-                                         ? row.getSet("pending_sync_notify", Int32Type.instance).stream().map(Node.Id::new).collect(Collectors.toSet())
-                                         : Collections.emptySet();
-        Set<Node.Id> remoteSyncComplete = row.has("remote_sync_complete")
-                                          ? row.getSet("remote_sync_complete", Int32Type.instance).stream().map(Node.Id::new).collect(Collectors.toSet())
-                                          : Collections.emptySet();
-        Ranges closed = row.has("closed") ? blobMapToRanges(row.getMap("closed", BytesType.instance, BytesType.instance)) : Ranges.EMPTY;
-        Ranges redundant = row.has("redundant") ? blobMapToRanges(row.getMap("redundant", BytesType.instance, BytesType.instance)) : Ranges.EMPTY;
-
-        consumer.load(epoch, syncStatus, pendingSyncNotify, remoteSyncComplete, closed, redundant);
-    }
-
-    public static EpochDiskState loadTopologies(TopologyLoadConsumer consumer)
-    {
-        try
-        {
-            EpochDiskState diskState = loadEpochDiskState();
-            if (diskState == null)
-                return EpochDiskState.EMPTY;
-
-            for (long epoch = diskState.minEpoch; epoch < diskState.maxEpoch; epoch++)
-                loadEpoch(epoch, consumer);
-
-            return diskState;
-        }
-        catch (IOException e)
-        {
-            throw new UncheckedIOException(e);
-        }
-    }
-
     @VisibleForTesting
     public static void unsafeClear()
     {
diff --git a/src/java/org/apache/cassandra/service/accord/AccordService.java b/src/java/org/apache/cassandra/service/accord/AccordService.java
index 912617fbe3..90b906bfde 100644
--- a/src/java/org/apache/cassandra/service/accord/AccordService.java
+++ b/src/java/org/apache/cassandra/service/accord/AccordService.java
@@ -36,7 +36,6 @@ import javax.annotation.Nullable;
 import javax.annotation.concurrent.GuardedBy;
 
 import com.google.common.annotations.VisibleForTesting;
-import com.google.common.collect.Streams;
 import com.google.common.primitives.Ints;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
@@ -48,8 +47,6 @@ import accord.coordinate.KeyBarriers;
 import accord.impl.AbstractConfigurationService;
 import accord.impl.DefaultLocalListeners;
 import accord.impl.DefaultRemoteListeners;
-import accord.local.UniqueTimeService.AtomicUniqueTimeWithStaleReservation;
-import accord.local.durability.DurabilityService;
 import accord.impl.RequestCallbacks;
 import accord.impl.SizeOfIntersectionSorter;
 import accord.impl.progresslog.DefaultProgressLogs;
@@ -62,8 +59,10 @@ import accord.local.Node.Id;
 import accord.local.PreLoadContext;
 import accord.local.SafeCommand;
 import accord.local.ShardDistributor.EvenSplit;
+import accord.local.UniqueTimeService.AtomicUniqueTimeWithStaleReservation;
 import accord.local.cfk.CommandsForKey;
 import accord.local.cfk.SafeCommandsForKey;
+import accord.local.durability.DurabilityService;
 import accord.local.durability.ShardDurability;
 import accord.messages.Reply;
 import accord.messages.Request;
@@ -104,12 +103,12 @@ import org.apache.cassandra.service.accord.AccordSyncPropagator.Notification;
 import org.apache.cassandra.service.accord.TimeOnlyRequestBookkeeping.LatencyRequestBookkeeping;
 import org.apache.cassandra.service.accord.api.AccordAgent;
 import org.apache.cassandra.service.accord.api.AccordRoutableKey;
-import org.apache.cassandra.service.accord.api.TokenKey.KeyspaceSplitter;
-import org.apache.cassandra.service.accord.api.TokenKey;
 import org.apache.cassandra.service.accord.api.AccordScheduler;
 import org.apache.cassandra.service.accord.api.AccordTimeService;
 import org.apache.cassandra.service.accord.api.AccordTopologySorter;
 import org.apache.cassandra.service.accord.api.CompositeTopologySorter;
+import org.apache.cassandra.service.accord.api.TokenKey;
+import org.apache.cassandra.service.accord.api.TokenKey.KeyspaceSplitter;
 import org.apache.cassandra.service.accord.interop.AccordInteropAdapter.AccordInteropFactory;
 import org.apache.cassandra.service.accord.txn.TxnQuery;
 import org.apache.cassandra.service.accord.txn.TxnRead;
@@ -134,6 +133,7 @@ import static accord.messages.SimpleReply.Ok;
 import static accord.primitives.Routable.Domain.Key;
 import static accord.primitives.Txn.Kind.Write;
 import static accord.primitives.TxnId.Cardinality.cardinality;
+import static accord.topology.TopologyManager.TopologyRange;
 import static java.util.concurrent.TimeUnit.NANOSECONDS;
 import static java.util.concurrent.TimeUnit.SECONDS;
 import static org.apache.cassandra.config.DatabaseDescriptor.getAccordCommandStoreShardCount;
@@ -143,6 +143,7 @@ import static org.apache.cassandra.config.DatabaseDescriptor.getAccordShardDurab
 import static org.apache.cassandra.config.DatabaseDescriptor.getPartitioner;
 import static org.apache.cassandra.metrics.ClientRequestsMetricsHolder.accordReadBookkeeping;
 import static org.apache.cassandra.metrics.ClientRequestsMetricsHolder.accordWriteBookkeeping;
+import static org.apache.cassandra.service.accord.journal.AccordTopologyUpdate.ImmutableTopoloyImage;
 import static org.apache.cassandra.service.consensus.migration.ConsensusRequestRouter.getTableMetadata;
 import static org.apache.cassandra.utils.Clock.Global.nanoTime;
 
@@ -187,6 +188,13 @@ public class AccordService implements IAccordService, Shutdownable
         return instance != null;
     }
 
+    public static IVerbHandler<Void> watermarkHandlerOrNoop()
+    {
+        if (!isSetup()) return ignore -> {};
+        AccordService i = (AccordService) instance();
+        return i.configService().watermarkCollector.handler;
+    }
+
     public static IVerbHandler<? extends Request> requestHandlerOrNoop()
     {
         if (!isSetup()) return ignore -> {};
@@ -322,6 +330,7 @@ public class AccordService implements IAccordService, Shutdownable
         unsafeStartupWithOverrides(null);
     }
 
+    @VisibleForTesting
     public synchronized void unsafeStartupWithOverrides(@Nullable Journal.TopologyUpdate overrideNullTopologyUpdate)
     {
         if (state != State.INIT)
@@ -332,44 +341,60 @@ public class AccordService implements IAccordService, Shutdownable
         ClusterMetadata metadata = ClusterMetadata.current();
         configService.updateMapping(metadata);
 
-        Long minEpoch = null;
-        List<Topology> local = new ArrayList<>();
-        Iterator<Journal.TopologyUpdate> iter = journal.replayTopologies();
-        Journal.TopologyUpdate lastSeen = null;
+        long highestKnown = -1;
+        List<ImmutableTopoloyImage> images = new ArrayList<>();
+
+        // Collect locally known topologies
+        Iterator<ImmutableTopoloyImage> iter = journal.replayTopologies();
+        Journal.TopologyUpdate prev = null;
         while (iter.hasNext())
         {
-            Journal.TopologyUpdate update = iter.next();
-            local.add(update.global);
-            Invariants.require(lastSeen == null || update.global.epoch() > lastSeen.global.epoch());
-            lastSeen = update;
+            ImmutableTopoloyImage next = iter.next();
+            // Due to partial compaction, we can clean up only some of the old epochs, creating gaps. We skip these epochs here.
+            if (prev != null && next.global.epoch() > prev.global.epoch() + 1)
+                images.clear();
+
+            images.add(next);
+            prev = next;
         }
-        if (lastSeen == null)
-            lastSeen = overrideNullTopologyUpdate;
 
-        if (lastSeen != null)
+        if (prev == null)
+            prev = overrideNullTopologyUpdate;
+
+        // Instantiate latest topology from the log, if known
+        if (prev != null)
         {
-            node.commandStores().initializeTopologyUnsafe(lastSeen);
-            minEpoch = lastSeen.global.epoch();
+            node.commandStores().initializeTopologyUnsafe(prev);
+            highestKnown = prev.global.epoch();
         }
 
         try
         {
-            // Fetch topologies up to current
-            List<Topology> remote = fetchTopologies(minEpoch, metadata);
-            Streams.concat(local.stream(), remote.stream())
-                   .forEach(configService::reportTopology);
+            TopologyRange remote = fetchTopologies(highestKnown + 1);
 
-            ClusterMetadataService.instance().log().addListener(configService.listener);
-            ClusterMetadata next = ClusterMetadata.current();
+            // Replay local epochs
+            for (ImmutableTopoloyImage image : images)
+                configService.reportTopology(image.global);
 
-            // if metadata was updated before we were able to add a listener, fetch remaining topologies
-            if (next.epoch.isAfter(metadata.epoch))
+            if (remote != null)
+                remote.forEach(configService::reportTopology, highestKnown + 1, Integer.MAX_VALUE);
+            else if (images.isEmpty()) // First boot, single-node cluster
+                configService.reportTopology(AccordTopology.createAccordTopology(metadata));
+
+            ClusterMetadataService.instance().log().addListener(configService.listener);
             {
-                remote = fetchTopologies(metadata.epoch.getEpoch(), next);
-                for (Topology topology : remote)
-                    configService.reportTopology(topology);
+                metadata = ClusterMetadata.current();
+                highestKnown = configService.currentEpoch();
+                if (metadata.epoch.getEpoch() > highestKnown)
+                {
+                    remote = fetchTopologies(highestKnown + 1);
+                    if (remote != null)
+                        remote.forEach(configService::reportTopology, highestKnown + 1, Integer.MAX_VALUE);
+                }
             }
 
+            WatermarkCollector.fetchAndReportWatermarksAsync(configService());
+
             int attempt = 0;
             int waitSeconds = 5;
             while (true)
@@ -408,17 +433,9 @@ public class AccordService implements IAccordService, Shutdownable
     /**
      * Queries peers to discover min epoch, and then fetches all topologies between min and current epochs
      */
-    private List<Topology> fetchTopologies(Long highestKnown, ClusterMetadata metadata) throws ExecutionException, InterruptedException
+    private TopologyRange fetchTopologies(long from) throws ExecutionException, InterruptedException
     {
-        Invariants.require(highestKnown == null || highestKnown <= metadata.epoch.getEpoch(),
-                           "Accord epochs should never be ahead of TCM ones, but %s was ahead of %s", (Object) highestKnown, metadata.epoch.getEpoch());
-
-        // All epochs are known and reported
-        if (highestKnown != null && highestKnown == metadata.epoch.getEpoch())
-            return Collections.emptyList();
-        // All epochs except current one are reported, no need to fetch from peers
-        if (highestKnown != null && highestKnown + 1 == metadata.epoch.getEpoch())
-            return Collections.singletonList(AccordTopology.createAccordTopology(metadata));
+        ClusterMetadata metadata = ClusterMetadata.current();
 
         Set<InetAddressAndPort> peers = new HashSet<>();
         peers.addAll(metadata.directory.allAddresses());
@@ -426,61 +443,42 @@ public class AccordService implements IAccordService, Shutdownable
 
         // No peers: single node cluster or first node to boot
         if (peers.isEmpty())
-            return Collections.singletonList(AccordTopology.createAccordTopology(metadata));
-
-        // Bootstrap, fetch min epoch
-        if (highestKnown == null)
-        {
-            Long fetched = findMinEpoch(SharedContext.Global.instance, peers);
-            if (fetched != null)
-                logger.info("Discovered min epoch of {} by querying {}", fetched, peers);
-
-            // No other node has advanced epoch just yet
-            if (fetched == null || fetched == metadata.epoch.getEpoch())
-                return Collections.singletonList(AccordTopology.createAccordTopology(metadata));
-
-            highestKnown = fetched;
-        }
-
-        long maxEpoch = metadata.epoch.getEpoch();
+            return null;
 
-        // If we are behind minEpoch, catch up to at least minEpoch
-        if (metadata.epoch.getEpoch() < highestKnown)
+        Iterator<InetAddressAndPort> iter = peers.iterator();
+        while (iter.hasNext())
         {
-            highestKnown = metadata.epoch.getEpoch();
-            maxEpoch = highestKnown;
+            InetAddressAndPort peer = iter.next();
+            try
+            {
+                logger.info("Fetching topologies for epochs [{}, {}] from {}", from, metadata.epoch.getEpoch(), peer);
+                Invariants.require(from <= metadata.epoch.getEpoch(),
+                                   "Accord epochs should never be ahead of TCM ones, but %d was ahead of %d", from, metadata.epoch.getEpoch());
+
+                Future<TopologyRange> futures = FetchTopologies.fetch(SharedContext.Global.instance,
+                                                                      Collections.singleton(peer),
+                                                                      from,
+                                                                      Long.MAX_VALUE);
+                TopologyRange response = futures.get();
+                logger.info("Fetched topologies {}", response);
+
+                // We're behind and need to catch up CMS first.
+                if (response.current > ClusterMetadata.current().epoch.getEpoch())
+                    ClusterMetadataService.instance().fetchLogFromCMS(Epoch.create(response.current));
+
+                if (response.current >= from)
+                    return response;
+                metadata = ClusterMetadata.current();
+            }
+            catch (Throwable e)
+            {
+                logger.info("Failed to fetch epochs [{}, {}] from {}", from, metadata.epoch.getEpoch(), peer);
+            }
         }
 
-        List<Future<Topology>> futures = new ArrayList<>();
-        logger.info("Fetching topologies for epochs [{}, {}].", highestKnown, maxEpoch);
-
-        for (long epoch = highestKnown; epoch <= maxEpoch; epoch++)
-            futures.add(FetchTopology.fetch(SharedContext.Global.instance, peers, epoch));
-
-        FBUtilities.waitOnFutures(futures);
-        List<Topology> topologies = new ArrayList<>(futures.size());
-        for (Future<Topology> future : futures)
-            topologies.add(future.get());
-
-        return topologies;
-    }
-
-    @VisibleForTesting
-    static Long findMinEpoch(SharedContext context, Set<InetAddressAndPort> peers)
-    {
-        try
-        {
-            return FetchMinEpoch.fetch(context, peers).get();
-        }
-        catch (InterruptedException e)
-        {
-            Thread.currentThread().interrupt();
-            throw new UncheckedInterruptedException(e);
-        }
-        catch (ExecutionException e)
-        {
-            throw new RuntimeException(e.getCause());
-        }
+        // After trying to contact all peers, and retrying according to retry spec on them, we give up.
+        // If there were no new known TCM epochs, we still allow Accord to start up, assuming there are no new epochs.
+        return null;
     }
 
     @Override
@@ -612,6 +610,7 @@ public class AccordService implements IAccordService, Shutdownable
         return configService.currentEpoch();
     }
 
+
     @Override
     public TopologyManager topology()
     {
@@ -885,22 +884,20 @@ public class AccordService implements IAccordService, Shutdownable
     }
 
     @Override
-    public void receive(Message<List<Notification>> message)
+    public void receive(Message<Notification> message)
     {
         receive(MessagingService.instance(), configService, message);
     }
 
     @VisibleForTesting
-    public static void receive(MessageDelivery sink, AbstractConfigurationService<?, ?> configService, Message<List<Notification>> message)
-    {
-        List<AccordSyncPropagator.Notification> notifications = message.payload;
-        notifications.forEach(notification -> {
-            notification.syncComplete.forEach(id -> configService.receiveRemoteSyncComplete(id, notification.epoch));
-            if (!notification.closed.isEmpty())
-                configService.receiveClosed(notification.closed, notification.epoch);
-            if (!notification.redundant.isEmpty())
-                configService.receiveRetired(notification.redundant, notification.epoch);
-        });
+    public static void receive(MessageDelivery sink, AbstractConfigurationService<?, ?> configService, Message<Notification> message)
+    {
+        AccordSyncPropagator.Notification notification = message.payload;
+        notification.syncComplete.forEach(id -> configService.receiveRemoteSyncComplete(id, notification.epoch));
+        if (!notification.closed.isEmpty())
+            configService.receiveClosed(notification.closed, notification.epoch);
+        if (!notification.retired.isEmpty())
+            configService.receiveRetired(notification.retired, notification.epoch);
         sink.respond(Ok, message);
     }
 
@@ -950,7 +947,7 @@ public class AccordService implements IAccordService, Shutdownable
     @Override
     public AccordCompactionInfos getCompactionInfo()
     {
-        AccordCompactionInfos compactionInfos = new AccordCompactionInfos(node.durableBefore());
+        AccordCompactionInfos compactionInfos = new AccordCompactionInfos(node.durableBefore(), node.topology().minEpoch());
         node.commandStores().forEachCommandStore(commandStore -> {
             compactionInfos.put(commandStore.id(), ((AccordCommandStore)commandStore).getCompactionInfo());
         });
diff --git a/src/java/org/apache/cassandra/service/accord/AccordSyncPropagator.java b/src/java/org/apache/cassandra/service/accord/AccordSyncPropagator.java
index 9e9134158c..59edd7c8bb 100644
--- a/src/java/org/apache/cassandra/service/accord/AccordSyncPropagator.java
+++ b/src/java/org/apache/cassandra/service/accord/AccordSyncPropagator.java
@@ -23,13 +23,11 @@ import java.util.Arrays;
 import java.util.Collection;
 import java.util.Collections;
 import java.util.HashSet;
-import java.util.List;
 import java.util.Set;
 import java.util.concurrent.TimeUnit;
 
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterables;
-
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -58,16 +56,18 @@ import org.apache.cassandra.service.accord.serializers.TopologySerializers;
 import org.apache.cassandra.tcm.ClusterMetadata;
 import org.apache.cassandra.utils.CollectionSerializers;
 
-import static org.apache.cassandra.utils.CollectionSerializers.newListSerializer;
-
 /**
- * Notifies remote replicas that the local replica has synchronised coordination information for this epoch
+ * Receives information about closed, retired ranges, and about sync completion, and
+ * propagates this information to the peers.
+ *
+ * Notifies remote replicas that the local replica has synchronised coordination
+ * information for this epoch.
  */
 public class AccordSyncPropagator
 {
     private static final Logger logger = LoggerFactory.getLogger(AccordSyncPropagator.class);
 
-    public static final IVerbHandler<List<Notification>> verbHandler = message -> {
+    public static final IVerbHandler<Notification> verbHandler = message -> {
         if (!AccordService.isSetup())
             return;
         AccordService.instance().receive(message);
@@ -141,7 +141,7 @@ public class AccordSyncPropagator
                 else syncComplete = ImmutableSet.copyOf(Iterables.filter(syncComplete, v -> !notification.syncComplete.contains(v)));
             }
             closed = closed.without(notification.closed);
-            retired = retired.without(notification.redundant);
+            retired = retired.without(notification.retired);
             return syncComplete.isEmpty() && closed.isEmpty() && retired.isEmpty();
         }
 
@@ -152,30 +152,25 @@ public class AccordSyncPropagator
                    "epoch=" + epoch +
                    ", syncComplete=" + syncComplete +
                    ", closed=" + closed +
-                   ", redundant=" + retired +
+                   ", retired=" + retired +
                    '}';
         }
     }
 
     static class PendingEpochs extends Long2ObjectHashMap<PendingEpoch>
     {
-        boolean ack(List<Notification> notifications)
+        boolean ack(Notification notification)
         {
-            for (Notification notification : notifications)
-            {
-                PendingEpoch epoch = get(notification.epoch);
-                if (epoch == null)
-                    continue;
-                if (epoch.ack(notification))
-                    remove(notification.epoch);
-            }
+            PendingEpoch epoch = get(notification.epoch);
+            if (epoch != null && epoch.ack(notification))
+                remove(notification.epoch);
             return isEmpty();
         }
     }
 
     static class PendingNodes extends Int2ObjectHashMap<PendingEpochs>
     {
-        boolean ack(Node.Id id, List<Notification> notifications)
+        boolean ack(Node.Id id, Notification notifications)
         {
             PendingEpochs node = get(id.id);
             if (node == null)
@@ -277,15 +272,15 @@ public class AccordSyncPropagator
         report(epoch, notify, PendingEpoch::closed, closed);
     }
 
-    public void reportRetired(long epoch, Collection<Node.Id> notify, Ranges redundant)
+    public void reportRetired(long epoch, Collection<Node.Id> notify, Ranges retired)
     {
-        report(epoch, notify, PendingEpoch::retired, redundant);
+        report(epoch, notify, PendingEpoch::retired, retired);
     }
 
     private synchronized <T> void report(long epoch, Collection<Node.Id> notify, ReportPending<T> report, T param)
     {
         // TODO (efficiency, now): for larger clusters this can be a problem as we trigger 1 msg for each instance, so in a 1k cluster its 1k messages; this can cause a thundering herd problem
-        // this is mostly a problem for reportSyncComplete as we include every node in the cluster, for reportClosed/reportRedundant these tend to use only the nodes that are replicas of the range,
+        // this is mostly a problem for reportSyncComplete as we include every node in the cluster, for reportClosed/reportRetired these tend to use only the nodes that are replicas of the range,
         // and there is currently an assumption that sub-ranges are done, so only impacting a handful of nodes.
         // TODO (correctness, now): during a host replacement multiple epochs are generated (move the range, remove the node), so its possible that notify will never be able to send the notification as the node is leaving the cluster
         notify.forEach(id -> {
@@ -293,7 +288,7 @@ public class AccordSyncPropagator
                                                .computeIfAbsent(epoch, PendingEpoch::new);
             Notification notification = report.report(pendingEpoch, param);
             if (notification != null)
-                notify(id, Collections.singletonList(notification));
+                notify(id, notification);
         });
     }
 
@@ -307,10 +302,10 @@ public class AccordSyncPropagator
         });
     }
 
-    private boolean notify(Node.Id to, List<Notification> notifications)
+    private boolean notify(Node.Id to, Notification notification)
     {
         InetAddressAndPort toEp = endpointMapper.mappedEndpoint(to);
-        Message<List<Notification>> msg = Message.out(Verb.ACCORD_SYNC_NOTIFY_REQ, notifications);
+        Message<Notification> msg = Message.out(Verb.ACCORD_SYNC_NOTIFY_REQ, notification);
         RequestCallback<SimpleReply> cb = new RequestCallback<>()
         {
             @Override
@@ -321,30 +316,25 @@ public class AccordSyncPropagator
                 // TODO review is it a good idea to call the listener while not holding the `AccordSyncPropagator` lock?
                 synchronized (AccordSyncPropagator.this)
                 {
-                    pending.ack(to, notifications);
-                    for (Notification notification : notifications)
+                    pending.ack(to, notification);
+                    long epoch = notification.epoch;
+                    if (notification.syncComplete.contains(localId))
                     {
-                        long epoch = notification.epoch;
-                        if (notification.syncComplete.contains(localId))
-                        {
-                            if (hasSyncCompletedFor(epoch))
-                                completedEpochs.add(epoch);
-                        }
+                        if (hasSyncCompletedFor(epoch))
+                            completedEpochs.add(epoch);
                     }
                 }
-                for (Notification notification : notifications)
-                {
-                    long epoch = notification.epoch;
-                    listener.onEndpointAck(to, epoch);
-                    if (completedEpochs.contains(epoch))
-                        listener.onComplete(epoch);
-                }
+
+                long epoch = notification.epoch;
+                listener.onEndpointAck(to, epoch);
+                if (completedEpochs.contains(epoch))
+                    listener.onComplete(epoch);
             }
 
             @Override
             public void onFailure(InetAddressAndPort from, RequestFailure failure)
             {
-                scheduler.schedule(() -> AccordSyncPropagator.this.notify(to, notifications), 1, TimeUnit.MINUTES);
+                scheduler.schedule(() -> AccordSyncPropagator.this.notify(to, notification), 1, TimeUnit.SECONDS);
             }
 
             @Override
@@ -363,8 +353,8 @@ public class AccordSyncPropagator
                 cb.onResponse(msg.responseWith(SimpleReply.Ok));
                 return true;
             }
-            logger.warn("Node{} is not alive, unable to notify of {}", to, notifications);
-            scheduler.schedule(() -> notify(to, notifications), 1, TimeUnit.MINUTES);
+            logger.warn("Node{} is not alive, unable to notify of {}", to, notification);
+            scheduler.schedule(() -> notify(to, notification), 1, TimeUnit.MINUTES);
             return false;
         }
         messagingService.sendWithCallback(msg, toEp, cb);
@@ -373,7 +363,7 @@ public class AccordSyncPropagator
 
     public static class Notification
     {
-        public static final IVersionedSerializer<Notification> serializer = new IVersionedSerializer<Notification>()
+        public static final IVersionedSerializer<Notification> serializer = new IVersionedSerializer<>()
         {
             @Override
             public void serialize(Notification notification, DataOutputPlus out, int version) throws IOException
@@ -381,7 +371,7 @@ public class AccordSyncPropagator
                 out.writeLong(notification.epoch);
                 CollectionSerializers.serializeCollection(notification.syncComplete, out, version, TopologySerializers.nodeId);
                 KeySerializers.ranges.serialize(notification.closed, out, version);
-                KeySerializers.ranges.serialize(notification.redundant, out, version);
+                KeySerializers.ranges.serialize(notification.retired, out, version);
             }
 
             @Override
@@ -399,21 +389,20 @@ public class AccordSyncPropagator
                 return TypeSizes.LONG_SIZE
                        + CollectionSerializers.serializedCollectionSize(notification.syncComplete, version, TopologySerializers.nodeId)
                        + KeySerializers.ranges.serializedSize(notification.closed, version)
-                       + KeySerializers.ranges.serializedSize(notification.redundant, version);
+                       + KeySerializers.ranges.serializedSize(notification.retired, version);
             }
         };
-        public static final IVersionedSerializer<List<Notification>> listSerializer = newListSerializer(serializer);
 
         final long epoch;
         final Collection<Node.Id> syncComplete;
-        final Ranges closed, redundant;
+        final Ranges closed, retired;
 
-        public Notification(long epoch, Collection<Node.Id> syncComplete, Ranges closed, Ranges redundant)
+        public Notification(long epoch, Collection<Node.Id> syncComplete, Ranges closed, Ranges retired)
         {
             this.epoch = epoch;
             this.syncComplete = syncComplete;
             this.closed = closed;
-            this.redundant = redundant;
+            this.retired = retired;
         }
 
         @Override
@@ -423,7 +412,7 @@ public class AccordSyncPropagator
                    "epoch=" + epoch +
                    ", syncComplete=" + syncComplete +
                    ", closed=" + closed +
-                   ", redundant=" + redundant +
+                   ", retired=" + retired +
                    '}';
         }
     }
diff --git a/src/java/org/apache/cassandra/service/accord/FetchMinEpoch.java b/src/java/org/apache/cassandra/service/accord/FetchMinEpoch.java
deleted file mode 100644
index 495ea7e0db..0000000000
--- a/src/java/org/apache/cassandra/service/accord/FetchMinEpoch.java
+++ /dev/null
@@ -1,183 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service.accord;
-
-import java.io.IOException;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Objects;
-import java.util.Set;
-import javax.annotation.Nullable;
-
-import com.google.common.annotations.VisibleForTesting;
-import com.google.common.collect.Iterators;
-
-import org.apache.cassandra.db.TypeSizes;
-import org.apache.cassandra.exceptions.RequestFailure;
-import org.apache.cassandra.io.IVersionedSerializer;
-import org.apache.cassandra.io.util.DataInputPlus;
-import org.apache.cassandra.io.util.DataOutputPlus;
-import org.apache.cassandra.locator.InetAddressAndPort;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.net.Verb;
-import org.apache.cassandra.repair.SharedContext;
-import org.apache.cassandra.service.WaitStrategy;
-import org.apache.cassandra.utils.concurrent.Future;
-import org.apache.cassandra.utils.concurrent.FutureCombiner;
-
-import static org.apache.cassandra.net.MessageDelivery.RetryErrorMessage;
-import static org.apache.cassandra.net.MessageDelivery.RetryPredicate;
-import static org.apache.cassandra.net.MessageDelivery.logger;
-import static org.apache.cassandra.service.accord.api.AccordWaitStrategies.retryFetchMinEpoch;
-
-// TODO (required, efficiency): this can be simplified: we seem to always use "entire range"
-public class FetchMinEpoch
-{
-    private static final FetchMinEpoch instance = new FetchMinEpoch();
-
-    public static final IVersionedSerializer<FetchMinEpoch> serializer = new IVersionedSerializer<>()
-    {
-        @Override
-        public void serialize(FetchMinEpoch t, DataOutputPlus out, int version)
-        {
-        }
-
-        @Override
-        public FetchMinEpoch deserialize(DataInputPlus in, int version)
-        {
-            return FetchMinEpoch.instance;
-        }
-
-        @Override
-        public long serializedSize(FetchMinEpoch t, int version)
-        {
-            return 0;
-        }
-    };
-
-    public static final IVerbHandler<FetchMinEpoch> handler = message -> {
-        if (AccordService.started())
-        {
-            Long epoch = AccordService.instance().minEpoch();
-            MessagingService.instance().respond(new Response(epoch), message);
-        }
-        else
-        {
-            logger.error("Accord service is not started, responding with error to {}", message);
-            MessagingService.instance().respondWithFailure(RequestFailure.BOOTING, message);
-        }
-    };
-
-    private FetchMinEpoch()
-    {
-    }
-
-    public static Future<Long> fetch(SharedContext context, Set<InetAddressAndPort> peers)
-    {
-        List<Future<Long>> accum = new ArrayList<>(peers.size());
-        for (InetAddressAndPort peer : peers)
-            accum.add(fetch(context, peer));
-        // TODO (required): we are collecting only successes, but we need some threshold
-        return FutureCombiner.successfulOf(accum).map(epochs -> {
-            Long min = null;
-            for (Long epoch : epochs)
-            {
-                if (epoch == null) continue;
-                if (min == null) min = epoch;
-                else min = Math.min(min, epoch);
-            }
-            return min;
-        });
-    }
-
-    @VisibleForTesting
-    static Future<Long> fetch(SharedContext context, InetAddressAndPort to)
-    {
-        WaitStrategy backoff = retryFetchMinEpoch();
-        return context.messaging().<FetchMinEpoch, Response>sendWithRetries(backoff,
-                                                                            context.optionalTasks()::schedule,
-                                                                            Verb.ACCORD_FETCH_MIN_EPOCH_REQ,
-                                                                            FetchMinEpoch.instance,
-                                                                            Iterators.cycle(to),
-                                                                            RetryPredicate.ALWAYS_RETRY,
-                                                                            RetryErrorMessage.EMPTY)
-                      .map(m -> m.payload.minEpoch);
-    }
-
-    public static class Response
-    {
-        public static final IVersionedSerializer<Response> serializer = new IVersionedSerializer<>()
-        {
-            @Override
-            public void serialize(Response t, DataOutputPlus out, int version) throws IOException
-            {
-                out.writeBoolean(t.minEpoch != null);
-                if (t.minEpoch != null)
-                    out.writeUnsignedVInt(t.minEpoch);
-            }
-
-            @Override
-            public Response deserialize(DataInputPlus in, int version) throws IOException
-            {
-                boolean notNull = in.readBoolean();
-                return new Response(notNull ? in.readUnsignedVInt() : null);
-            }
-
-            @Override
-            public long serializedSize(Response t, int version)
-            {
-                int size = TypeSizes.BOOL_SIZE;
-                if (t.minEpoch != null)
-                    size += TypeSizes.sizeofUnsignedVInt(t.minEpoch);
-                return size;
-            }
-        };
-        @Nullable
-        public final Long minEpoch;
-
-        public Response(@Nullable Long minEpoch)
-        {
-            this.minEpoch = minEpoch;
-        }
-
-        @Override
-        public boolean equals(Object o)
-        {
-            if (this == o) return true;
-            if (o == null || getClass() != o.getClass()) return false;
-            Response response = (Response) o;
-            return Objects.equals(minEpoch, response.minEpoch);
-        }
-
-        @Override
-        public int hashCode()
-        {
-            return Objects.hash(minEpoch);
-        }
-
-        @Override
-        public String toString()
-        {
-            return "Response{" +
-                   "minEpoch=" + minEpoch +
-                   '}';
-        }
-    }
-}
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/service/accord/FetchTopologies.java b/src/java/org/apache/cassandra/service/accord/FetchTopologies.java
new file mode 100644
index 0000000000..9cef181178
--- /dev/null
+++ b/src/java/org/apache/cassandra/service/accord/FetchTopologies.java
@@ -0,0 +1,159 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.List;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import accord.topology.Topology;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.exceptions.RequestFailure;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.MessageDelivery;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.MessagingUtils;
+import org.apache.cassandra.net.Verb;
+import org.apache.cassandra.repair.SharedContext;
+import org.apache.cassandra.service.accord.serializers.TopologySerializers;
+import org.apache.cassandra.utils.concurrent.Future;
+
+import static accord.topology.TopologyManager.TopologyRange;
+import static org.apache.cassandra.service.accord.api.AccordWaitStrategies.retryFetchTopology;
+
+/**
+ * Fetch Accord topologies form remote peer.
+ */
+public class FetchTopologies
+{
+    private static final Logger logger = LoggerFactory.getLogger(FetchTopologies.class);
+    public String toString()
+    {
+        return "FetchTopology{" +
+               "epoch=" + minEpoch +
+               '}';
+    }
+
+    private final long minEpoch;
+    private final long maxEpoch;
+
+    public static final IVersionedSerializer<FetchTopologies> serializer = new IVersionedSerializer<>()
+    {
+        @Override
+        public void serialize(FetchTopologies t, DataOutputPlus out, int version) throws IOException
+        {
+            out.writeUnsignedVInt(t.minEpoch);
+            out.writeUnsignedVInt(t.maxEpoch);
+        }
+
+        @Override
+        public FetchTopologies deserialize(DataInputPlus in, int version) throws IOException
+        {
+            return new FetchTopologies(in.readUnsignedVInt(), in.readUnsignedVInt());
+        }
+
+        @Override
+        public long serializedSize(FetchTopologies t, int version)
+        {
+            return TypeSizes.sizeofUnsignedVInt(t.minEpoch) +
+                   TypeSizes.sizeofUnsignedVInt(t.maxEpoch);
+        }
+    };
+
+    public FetchTopologies(long minEpoch, long maxEpoch)
+    {
+        this.minEpoch = minEpoch;
+        this.maxEpoch = maxEpoch;
+    }
+
+    // TODO (required): messaging version after version patch
+    public static final IVersionedSerializer<TopologyRange> responseSerializer = new IVersionedSerializer<>()
+    {
+            @Override
+            public void serialize(TopologyRange t, DataOutputPlus out, int version) throws IOException
+            {
+                out.writeUnsignedVInt(t.min);
+                out.writeUnsignedVInt(t.current);
+                out.writeUnsignedVInt(t.firstNonEmpty);
+                out.writeUnsignedVInt32(t.topologies.size());
+
+                for (Topology topology : t.topologies)
+                    TopologySerializers.topology.serialize(topology, out, version);
+            }
+
+            @Override
+            public TopologyRange deserialize(DataInputPlus in, int version) throws IOException
+            {
+                long min = in.readUnsignedVInt();
+                long current = in.readUnsignedVInt();
+                long firstNonEmpty = in.readUnsignedVInt();
+                int count = in.readUnsignedVInt32();
+                List<Topology> topologies = new ArrayList<>(count);
+                for (int i = 0; i < count; ++i)
+                    topologies.add(TopologySerializers.topology.deserialize(in, version));
+                return new TopologyRange(min, current, firstNonEmpty, topologies);
+            }
+
+            @Override
+            public long serializedSize(TopologyRange t, int version)
+            {
+                long size = TypeSizes.sizeofUnsignedVInt(t.min);
+                size += TypeSizes.sizeofUnsignedVInt(t.current);
+                size += TypeSizes.sizeofUnsignedVInt(t.firstNonEmpty);
+                size += TypeSizes.sizeofUnsignedVInt(t.topologies.size());
+                for (Topology topology : t.topologies)
+                    size += TopologySerializers.topology.serializedSize(topology, version);
+                return size;
+            }
+        };
+
+    public static final IVerbHandler<FetchTopologies> handler = message -> {
+        if (!AccordService.isSetup())
+        {
+            logger.debug("Accord unitialized, responding with failure to {}", message.payload);
+            MessagingService.instance().respondWithFailure(RequestFailure.UNKNOWN, message);
+            return;
+        }
+
+        TopologyRange topologies = AccordService.instance().topology().between(message.payload.minEpoch, message.payload.maxEpoch);
+        logger.debug("Responding with {} failure to {}", topologies, message.payload);
+        MessagingService.instance().respond(topologies, message);
+    };
+
+    public static Future<TopologyRange> fetch(SharedContext context, Collection<InetAddressAndPort> peers, long minEpoch, long maxEpoch)
+    {
+        FetchTopologies request = new FetchTopologies(minEpoch, maxEpoch);
+        return context.messaging().<FetchTopologies, TopologyRange>sendWithRetries(retryFetchTopology(),
+                                                                                   context.optionalTasks()::schedule,
+                                                                                   Verb.ACCORD_FETCH_TOPOLOGY_REQ,
+                                                                                   request,
+                                                                                   MessagingUtils.tryAliveFirst(context, peers, Verb.ACCORD_FETCH_TOPOLOGY_REQ.name()),
+                                                                                   (attempt, from, failure) -> true,
+                                                                                   MessageDelivery.RetryErrorMessage.EMPTY)
+                      .map(m -> m.payload);
+    }
+}
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/service/accord/FetchTopology.java b/src/java/org/apache/cassandra/service/accord/FetchTopology.java
deleted file mode 100644
index b58358e9aa..0000000000
--- a/src/java/org/apache/cassandra/service/accord/FetchTopology.java
+++ /dev/null
@@ -1,141 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service.accord;
-
-import java.io.IOException;
-import java.util.Collection;
-
-import accord.topology.Topology;
-import org.apache.cassandra.db.TypeSizes;
-import org.apache.cassandra.exceptions.RequestFailure;
-import org.apache.cassandra.io.IVersionedSerializer;
-import org.apache.cassandra.io.util.DataInputPlus;
-import org.apache.cassandra.io.util.DataOutputPlus;
-import org.apache.cassandra.locator.InetAddressAndPort;
-import org.apache.cassandra.net.IVerbHandler;
-import org.apache.cassandra.net.MessageDelivery;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.net.MessagingUtils;
-import org.apache.cassandra.net.Verb;
-import org.apache.cassandra.repair.SharedContext;
-import org.apache.cassandra.service.WaitStrategy;
-import org.apache.cassandra.service.accord.serializers.TopologySerializers;
-import org.apache.cassandra.utils.concurrent.Future;
-
-import static org.apache.cassandra.service.accord.api.AccordWaitStrategies.retryFetchTopology;
-
-public class FetchTopology
-{
-    public String toString()
-    {
-        return "FetchTopology{" +
-               "epoch=" + epoch +
-               '}';
-    }
-
-    private final long epoch;
-
-    public static final IVersionedSerializer<FetchTopology> serializer = new IVersionedSerializer<>()
-    {
-        @Override
-        public void serialize(FetchTopology t, DataOutputPlus out, int version) throws IOException
-        {
-            out.writeLong(t.epoch);
-        }
-
-        @Override
-        public FetchTopology deserialize(DataInputPlus in, int version) throws IOException
-        {
-            return new FetchTopology(in.readLong());
-        }
-
-        @Override
-        public long serializedSize(FetchTopology t, int version)
-        {
-            return Long.BYTES;
-        }
-    };
-
-    public FetchTopology(long epoch)
-    {
-        this.epoch = epoch;
-    }
-
-    public static class Response
-    {
-        // TODO (required): messaging version after version patch
-        public static final IVersionedSerializer<Response> serializer = new IVersionedSerializer<>()
-        {
-            @Override
-            public void serialize(Response t, DataOutputPlus out, int version) throws IOException
-            {
-                out.writeUnsignedVInt(t.epoch);
-                TopologySerializers.topology.serialize(t.topology, out, version);
-            }
-
-            @Override
-            public Response deserialize(DataInputPlus in, int version) throws IOException
-            {
-                long epoch = in.readUnsignedVInt();
-                Topology topology = TopologySerializers.topology.deserialize(in, version);
-                return new Response(epoch, topology);
-            }
-
-            @Override
-            public long serializedSize(Response t, int version)
-            {
-                return TypeSizes.sizeofUnsignedVInt(t.epoch)
-                       + TopologySerializers.topology.serializedSize(t.topology, version);
-            }
-        };
-
-        private final long epoch;
-        private final Topology topology;
-
-        public Response(long epoch, Topology topology)
-        {
-            this.epoch = epoch;
-            this.topology = topology;
-        }
-    }
-
-    public static final IVerbHandler<FetchTopology> handler = message -> {
-        long epoch = message.payload.epoch;
-
-        Topology topology;
-        if (AccordService.isSetup() && (topology = AccordService.instance().topology().maybeGlobalForEpoch(epoch)) != null)
-            MessagingService.instance().respond(new Response(epoch, topology), message);
-        else
-            MessagingService.instance().respondWithFailure(RequestFailure.UNKNOWN_TOPOLOGY, message);
-    };
-
-    public static Future<Topology> fetch(SharedContext context, Collection<InetAddressAndPort> peers, long epoch)
-    {
-        FetchTopology request = new FetchTopology(epoch);
-        WaitStrategy backoff = retryFetchTopology();
-        return context.messaging().<FetchTopology, Response>sendWithRetries(backoff,
-                                                                            context.optionalTasks()::schedule,
-                                                                            Verb.ACCORD_FETCH_TOPOLOGY_REQ,
-                                                                            request,
-                                                                            MessagingUtils.tryAliveFirst(SharedContext.Global.instance, peers, Verb.ACCORD_FETCH_TOPOLOGY_REQ.name()),
-                                                                            (attempt, from, failure) -> true,
-                                                                            MessageDelivery.RetryErrorMessage.EMPTY)
-                      .map(m -> m.payload.topology);
-    }
-}
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/service/accord/IAccordService.java b/src/java/org/apache/cassandra/service/accord/IAccordService.java
index 01dddf520a..2530a080fa 100644
--- a/src/java/org/apache/cassandra/service/accord/IAccordService.java
+++ b/src/java/org/apache/cassandra/service/accord/IAccordService.java
@@ -112,7 +112,7 @@ public interface IAccordService
      */
     Future<Void> epochReady(Epoch epoch);
 
-    void receive(Message<List<AccordSyncPropagator.Notification>> message);
+    void receive(Message<AccordSyncPropagator.Notification> message);
 
     class AccordCompactionInfo
     {
@@ -133,16 +133,19 @@ public interface IAccordService
     class AccordCompactionInfos extends Int2ObjectHashMap<AccordCompactionInfo>
     {
         public final DurableBefore durableBefore;
+        public final long minEpoch;
 
-        public AccordCompactionInfos(DurableBefore durableBefore)
+        public AccordCompactionInfos(DurableBefore durableBefore, long minEpoch)
         {
             this.durableBefore = durableBefore;
+            this.minEpoch = minEpoch;
         }
 
-        public AccordCompactionInfos(DurableBefore durableBefore, AccordCompactionInfos copy)
+        public AccordCompactionInfos(DurableBefore durableBefore, long minEpoch, AccordCompactionInfos copy)
         {
             super(copy);
             this.durableBefore = durableBefore;
+            this.minEpoch = minEpoch;
         }
     }
 
@@ -264,12 +267,12 @@ public interface IAccordService
         }
 
         @Override
-        public void receive(Message<List<AccordSyncPropagator.Notification>> message) {}
+        public void receive(Message<AccordSyncPropagator.Notification> message) {}
 
         @Override
         public AccordCompactionInfos getCompactionInfo()
         {
-            return new AccordCompactionInfos(DurableBefore.EMPTY);
+            return new AccordCompactionInfos(DurableBefore.EMPTY, 0);
         }
 
         @Override
@@ -436,7 +439,7 @@ public interface IAccordService
         }
 
         @Override
-        public void receive(Message<List<Notification>> message)
+        public void receive(Message<Notification> message)
         {
             delegate.receive(message);
         }
diff --git a/src/java/org/apache/cassandra/service/accord/JournalKey.java b/src/java/org/apache/cassandra/service/accord/JournalKey.java
index b1acbd31a2..b299f91613 100644
--- a/src/java/org/apache/cassandra/service/accord/JournalKey.java
+++ b/src/java/org/apache/cassandra/service/accord/JournalKey.java
@@ -30,6 +30,7 @@ import accord.utils.Invariants;
 import org.apache.cassandra.io.util.DataInputPlus;
 import org.apache.cassandra.io.util.DataOutputPlus;
 import org.apache.cassandra.journal.KeySupport;
+import org.apache.cassandra.service.accord.journal.AccordTopologyUpdate;
 import org.apache.cassandra.utils.ByteArrayUtil;
 
 import static org.apache.cassandra.db.TypeSizes.BYTE_SIZE;
@@ -261,7 +262,7 @@ public final class JournalKey
         SAFE_TO_READ                 (3, new SafeToReadSerializer()),
         BOOTSTRAP_BEGAN_AT           (4, new BootstrapBeganAtSerializer()),
         RANGES_FOR_EPOCH             (5, new RangesForEpochSerializer()),
-        TOPOLOGY_UPDATE              (6, new TopologyUpdateSerializer()),
+        TOPOLOGY_UPDATE              (6, AccordTopologyUpdate.AccumulatingSerializer.defaultInstance),
         ;
 
         public final int id;
diff --git a/src/java/org/apache/cassandra/service/accord/WatermarkCollector.java b/src/java/org/apache/cassandra/service/accord/WatermarkCollector.java
new file mode 100644
index 0000000000..13e0d529ac
--- /dev/null
+++ b/src/java/org/apache/cassandra/service/accord/WatermarkCollector.java
@@ -0,0 +1,252 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.Iterators;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import accord.api.ConfigurationService;
+import accord.local.Node;
+import accord.primitives.Range;
+import accord.primitives.Ranges;
+import accord.topology.Topology;
+import accord.utils.Invariants;
+import accord.utils.async.AsyncResult;
+import org.agrona.collections.Int2ObjectHashMap;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.locator.InetAddressAndPort;
+import org.apache.cassandra.net.IVerbHandler;
+import org.apache.cassandra.net.Message;
+import org.apache.cassandra.net.MessageDelivery;
+import org.apache.cassandra.net.MessagingService;
+import org.apache.cassandra.net.NoPayload;
+import org.apache.cassandra.net.Verb;
+import org.apache.cassandra.repair.SharedContext;
+import org.apache.cassandra.tcm.ClusterMetadata;
+import org.apache.cassandra.utils.FBUtilities;
+
+import static org.apache.cassandra.service.accord.api.AccordWaitStrategies.retryFetchWatermarks;
+
+/**
+ * Collects watermarks of closed and retired epochs per range, and synced epochs per node.
+ */
+public class WatermarkCollector implements ConfigurationService.Listener
+{
+    private static final Logger logger = LoggerFactory.getLogger(WatermarkCollector.class);
+
+    final Map<Range, Long> closed;
+    final Map<Range, Long> retired;
+    final Int2ObjectHashMap<Long> synced;
+
+    WatermarkCollector()
+    {
+        closed = new HashMap<>();
+        retired = new HashMap<>();
+        synced = new Int2ObjectHashMap<>();
+    }
+
+    @Override public AsyncResult<Void> onTopologyUpdate(Topology topology, boolean isLoad, boolean startSync)
+    {
+        return null;
+    }
+
+    @Override
+    public void onRemoteSyncComplete(Node.Id node, long epoch)
+    {
+        synced.compute(node.id, (k, prev) -> prev == null ? epoch : Long.max(prev, epoch));
+    }
+
+    @Override
+    public void onEpochClosed(Ranges ranges, long epoch)
+    {
+        synchronized (this)
+        {
+            for (Range range : ranges)
+                this.closed.compute(range, (k, prev) -> prev == null ? epoch : Long.max(prev, epoch));
+        }
+    }
+
+    @Override
+    public void onEpochRetired(Ranges ranges, long epoch)
+    {
+        synchronized (this)
+        {
+            for (Range range : ranges)
+                this.retired.compute(range, (k, prev) -> prev == null ? epoch : Long.max(prev, epoch));
+        }
+    }
+
+    public final IVerbHandler<Void> handler = new IVerbHandler<Void>()
+    {
+        public void doVerb(Message<Void> message) throws IOException
+        {
+            Invariants.require(AccordService.started());
+            Snapshot snapshot;
+            synchronized (WatermarkCollector.this)
+            {
+                snapshot = new Snapshot(new HashMap<>(closed), new HashMap<>(retired), new Int2ObjectHashMap<>(synced));
+            }
+            MessagingService.instance().respond(snapshot, message);
+        }
+    };
+
+    @VisibleForTesting
+    static void fetchAndReportWatermarksAsync(AccordConfigurationService configService)
+    {
+        SharedContext context = SharedContext.Global.instance;
+        Set<InetAddressAndPort> peers = new HashSet<>();
+        peers.addAll(ClusterMetadata.current().directory.allAddresses());
+        peers.remove(FBUtilities.getBroadcastAddressAndPort());
+
+        context.messaging().<NoPayload, Snapshot>sendWithRetries(retryFetchWatermarks(),
+                                                                 context.optionalTasks()::schedule,
+                                                                 Verb.ACCORD_FETCH_WATERMARKS_REQ,
+                                                                 NoPayload.noPayload,
+                                                                 Iterators.cycle(peers),
+                                                                 MessageDelivery.RetryPredicate.ALWAYS_RETRY,
+                                                                 MessageDelivery.RetryErrorMessage.EMPTY)
+               .addCallback((m, fail) -> {
+                   if (fail != null)
+                   {
+                       return;
+                   }
+                   Snapshot snapshot = m.payload;
+                   long minEpoch = configService.minEpoch();
+                   for (Map.Entry<Range, Long> e : snapshot.closed.entrySet())
+                   {
+                       Ranges r = Ranges.of(e.getKey());
+                       for (long epoch = minEpoch; epoch <= e.getValue(); epoch++)
+                           configService.receiveClosed(r, e.getValue());
+                   }
+                   for (Map.Entry<Range, Long> e : snapshot.retired.entrySet())
+                   {
+                       Ranges r = Ranges.of(e.getKey());
+                       for (long epoch = minEpoch; epoch <= e.getValue(); epoch++)
+                           configService.receiveRetired(r, e.getValue());
+                   }
+                   for (Map.Entry<Integer, Long> e : snapshot.synced.entrySet())
+                   {
+                       Node.Id node = new Node.Id(e.getKey());
+                       for (long epoch = minEpoch; epoch <= e.getValue(); epoch++)
+                           configService.receiveRemoteSyncComplete(node, epoch);
+                   }
+               });
+    }
+
+    public static class Snapshot
+    {
+        public final Map<Range, Long> closed;
+        public final Map<Range, Long> retired;
+        public final Int2ObjectHashMap<Long> synced;
+
+        public Snapshot(Map<Range, Long> closed, Map<Range, Long> retired, Int2ObjectHashMap<Long> synced)
+        {
+            this.closed = closed;
+            this.retired = retired;
+            this.synced = synced;
+        }
+    }
+
+    public static final IVersionedSerializer<Snapshot> serializer = new IVersionedSerializer<>()
+    {
+        public void serialize(Snapshot t, DataOutputPlus out, int version) throws IOException
+        {
+            out.writeUnsignedVInt32(t.closed.size());
+            for (Map.Entry<Range, Long> e : t.closed.entrySet())
+            {
+                TokenRange.serializer.serialize((TokenRange) e.getKey(), out, version);
+                out.writeUnsignedVInt(e.getValue());
+            }
+            out.writeUnsignedVInt32(t.retired.size());
+            for (Map.Entry<Range, Long> e : t.retired.entrySet())
+            {
+                TokenRange.serializer.serialize((TokenRange) e.getKey(), out, version);
+                out.writeUnsignedVInt(e.getValue());
+            }
+            out.writeUnsignedVInt32(t.synced.size());
+            for (Map.Entry<Integer, Long> e : t.synced.entrySet())
+            {
+                out.writeUnsignedVInt32(e.getKey());
+                out.writeUnsignedVInt(e.getValue());
+            }
+        }
+
+        // TODO (desired): we do not have to deserialize to report these values
+        public Snapshot deserialize(DataInputPlus in, int version) throws IOException
+        {
+            int closedSize = in.readUnsignedVInt32();
+            Map<Range, Long> closed = new HashMap<>();
+            for (int i = 0; i < closedSize; i++)
+            {
+                closed.put(TokenRange.serializer.deserialize(in, version),
+                           in.readUnsignedVInt());
+            }
+            int retiredSize = in.readUnsignedVInt32();
+            Map<Range, Long> retired = new HashMap<>();
+            for (int i = 0; i < retiredSize; i++)
+            {
+                retired.put(TokenRange.serializer.deserialize(in, version),
+                            in.readUnsignedVInt());
+            }
+            int syncedSize = in.readUnsignedVInt32();
+            Int2ObjectHashMap<Long> synced = new Int2ObjectHashMap<>();
+            for (int i = 0; i < syncedSize; i++)
+            {
+                synced.put(in.readUnsignedVInt32(),
+                           (Long) in.readUnsignedVInt());
+            }
+            return new Snapshot(closed, retired, synced);
+        }
+
+        public long serializedSize(Snapshot t, int version)
+        {
+            int size = 0;
+            size += TypeSizes.sizeofUnsignedVInt(t.closed.size());
+            for (Map.Entry<Range, Long> e : t.closed.entrySet())
+            {
+                size += TokenRange.serializer.serializedSize((TokenRange) e.getKey(), version);
+                size += TypeSizes.sizeofUnsignedVInt(e.getValue());
+            }
+            size += TypeSizes.sizeofUnsignedVInt(t.retired.size());
+            for (Map.Entry<Range, Long> e : t.retired.entrySet())
+            {
+                size += TokenRange.serializer.serializedSize((TokenRange) e.getKey(), version);
+                size += TypeSizes.sizeofUnsignedVInt(e.getValue());
+            }
+            size += TypeSizes.sizeofUnsignedVInt(t.synced.size());
+            for (Map.Entry<Integer, Long> e : t.synced.entrySet())
+            {
+                size += TypeSizes.sizeofUnsignedVInt(e.getKey());
+                size += TypeSizes.sizeofUnsignedVInt(e.getValue());
+            }
+            return size;
+        }
+    };
+}
diff --git a/src/java/org/apache/cassandra/service/accord/api/AccordWaitStrategies.java b/src/java/org/apache/cassandra/service/accord/api/AccordWaitStrategies.java
index e60d340b97..6fcacee42a 100644
--- a/src/java/org/apache/cassandra/service/accord/api/AccordWaitStrategies.java
+++ b/src/java/org/apache/cassandra/service/accord/api/AccordWaitStrategies.java
@@ -71,7 +71,7 @@ public class AccordWaitStrategies
         return slowTxnPreaccept;
     }
 
-    public static RetryStrategy retryFetchMinEpoch()
+    public static RetryStrategy retryFetchWatermarks()
     {
         return retryFetchMinEpoch;
     }
diff --git a/src/java/org/apache/cassandra/service/accord/journal/AccordTopologyUpdate.java b/src/java/org/apache/cassandra/service/accord/journal/AccordTopologyUpdate.java
new file mode 100644
index 0000000000..e48882e15e
--- /dev/null
+++ b/src/java/org/apache/cassandra/service/accord/journal/AccordTopologyUpdate.java
@@ -0,0 +1,416 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.service.accord.journal;
+
+import java.io.IOException;
+import java.util.Iterator;
+import java.util.Map;
+import java.util.NavigableMap;
+import java.util.TreeMap;
+import java.util.function.Function;
+
+import accord.api.Journal;
+import accord.local.CommandStores;
+import accord.primitives.EpochSupplier;
+import accord.primitives.Ranges;
+import accord.topology.Topology;
+import accord.utils.Invariants;
+import accord.utils.UnhandledEnum;
+import org.agrona.collections.Int2ObjectHashMap;
+import org.apache.cassandra.db.TypeSizes;
+import org.apache.cassandra.io.IVersionedSerializer;
+import org.apache.cassandra.io.util.DataInputPlus;
+import org.apache.cassandra.io.util.DataOutputPlus;
+import org.apache.cassandra.service.accord.AccordConfigurationService;
+import org.apache.cassandra.service.accord.AccordJournalValueSerializers;
+import org.apache.cassandra.service.accord.JournalKey;
+import org.apache.cassandra.service.accord.serializers.KeySerializers;
+import org.apache.cassandra.service.accord.serializers.TopologySerializers;
+
+import static org.apache.cassandra.service.accord.JournalKey.Type.TOPOLOGY_UPDATE;
+
+public interface AccordTopologyUpdate
+{
+    Kind kind();
+    void applyTo(TopologyImage accumulator);
+    long epoch();
+
+    static AccordTopologyUpdate newTopology(Journal.TopologyUpdate update)
+    {
+        return new NewTopology(update);
+    }
+    class RangesForEpochSerializer implements IVersionedSerializer<CommandStores.RangesForEpoch>
+    {
+        public static final RangesForEpochSerializer instance = new RangesForEpochSerializer();
+
+        @Override
+        public void serialize(CommandStores.RangesForEpoch from, DataOutputPlus out, int version) throws IOException
+        {
+            out.writeUnsignedVInt32(from.size());
+            from.forEach((epoch, ranges) -> {
+                try
+                {
+                    out.writeLong(epoch);
+                    KeySerializers.ranges.serialize(ranges, out, version);
+                }
+                catch (Throwable t)
+                {
+                    throw new IllegalStateException("Serialization error", t);
+                }
+            });
+        }
+
+        @Override
+        public CommandStores.RangesForEpoch deserialize(DataInputPlus in, int version) throws IOException
+        {
+            int size = in.readUnsignedVInt32();
+            Ranges[] ranges = new Ranges[size];
+            long[] epochs = new long[size];
+            for (int i = 0; i < ranges.length; i++)
+            {
+                epochs[i] = in.readLong();
+                ranges[i] = KeySerializers.ranges.deserialize(in, version);
+            }
+            Invariants.require(ranges.length == epochs.length);
+            return new CommandStores.RangesForEpoch(epochs, ranges);
+        }
+
+        @Override
+        public long serializedSize(CommandStores.RangesForEpoch t, int version)
+        {
+            return TypeSizes.sizeofUnsignedVInt(t.size());
+        }
+    }
+
+    class TopologyUpdateSerializer implements IVersionedSerializer<Journal.TopologyUpdate>
+    {
+        public static final TopologyUpdateSerializer instance = new TopologyUpdateSerializer();
+
+        @Override
+        public void serialize(Journal.TopologyUpdate from, DataOutputPlus out, int version) throws IOException
+        {
+            out.writeUnsignedVInt32(from.commandStores.size());
+            for (Map.Entry<Integer, CommandStores.RangesForEpoch> e : from.commandStores.entrySet())
+            {
+                out.writeUnsignedVInt32(e.getKey());
+                RangesForEpochSerializer.instance.serialize(e.getValue(), out, version);
+            }
+            TopologySerializers.topology.serialize(from.local, out, version);
+            TopologySerializers.topology.serialize(from.global, out, version);
+        }
+
+        @Override
+        public Journal.TopologyUpdate deserialize(DataInputPlus in, int version) throws IOException
+        {
+            int commandStoresSize = in.readUnsignedVInt32();
+            Int2ObjectHashMap<CommandStores.RangesForEpoch> commandStores = new Int2ObjectHashMap<>();
+            for (int j = 0; j < commandStoresSize; j++)
+            {
+                int commandStoreId = in.readUnsignedVInt32();
+                CommandStores.RangesForEpoch rangesForEpoch = RangesForEpochSerializer.instance.deserialize(in, version);
+                commandStores.put(commandStoreId, rangesForEpoch);
+            }
+            Topology local = TopologySerializers.topology.deserialize(in, version);
+            Topology global = TopologySerializers.topology.deserialize(in, version);
+            return new Journal.TopologyUpdate(commandStores, local, global);
+        }
+
+        @Override
+        public long serializedSize(Journal.TopologyUpdate from, int version)
+        {
+            long size = TypeSizes.sizeofUnsignedVInt(from.commandStores.size());
+            for (Map.Entry<Integer, CommandStores.RangesForEpoch> e : from.commandStores.entrySet())
+            {
+                size += TypeSizes.sizeofUnsignedVInt(e.getKey());
+                size += RangesForEpochSerializer.instance.serializedSize(e.getValue(), version);
+            }
+
+            size += TopologySerializers.topology.serializedSize(from.local, version);
+            size += TopologySerializers.topology.serializedSize(from.global, version);
+            return size;
+        }
+    }
+
+    class Serializer implements IVersionedSerializer<AccordTopologyUpdate>
+    {
+        public static Serializer instance = new Serializer();
+
+        public void serialize(AccordTopologyUpdate t, DataOutputPlus out, int version) throws IOException
+        {
+            out.writeUnsignedVInt(t.epoch());
+            out.writeUnsignedVInt32(t.kind().ordinal());
+            switch (t.kind())
+            {
+                case NewTopology:
+                    TopologyUpdateSerializer.instance.serialize(((NewTopology) t).update, out, version);
+                    break;
+                case Topologies:
+                    TopologyImage image = (TopologyImage) t;
+
+                    out.writeBoolean(image.update != null);
+                    if (image.update != null)
+                        TopologyUpdateSerializer.instance.serialize(image.update, out, version);
+                    if (image.syncStatus == null)
+                        out.writeByte(Byte.MAX_VALUE);
+                    else
+                        out.writeByte(image.syncStatus.ordinal());
+
+                    KeySerializers.ranges.serialize(image.closed, out, version);
+                    KeySerializers.ranges.serialize(image.retired, out, version);
+                    break;
+                default:
+                    throw new UnhandledEnum(t.kind());
+            }
+        }
+
+        public AccordTopologyUpdate deserialize(DataInputPlus in, int version) throws IOException
+        {
+            int epoch = in.readUnsignedVInt32();
+            Kind kind = Kind.values()[in.readUnsignedVInt32()];
+            switch (kind)
+            {
+                case NewTopology:
+                    return new NewTopology(TopologyUpdateSerializer.instance.deserialize(in, version));
+                case Topologies:
+                {
+                    TopologyImage image = new TopologyImage(epoch);
+                    if (in.readBoolean())
+                        image.update = TopologyUpdateSerializer.instance.deserialize(in, version);
+
+                    byte syncStateByte = in.readByte();
+                    if (syncStateByte != Byte.MAX_VALUE)
+                        image.syncStatus = AccordConfigurationService.SyncStatus.values()[syncStateByte];
+
+                    image.closed = KeySerializers.ranges.deserialize(in, version);
+                    image.retired = KeySerializers.ranges.deserialize(in, version);
+                    return image;
+                }
+                default:
+                    throw new UnhandledEnum(kind);
+            }
+        }
+
+        public long serializedSize(AccordTopologyUpdate t, int version)
+        {
+            long size = TypeSizes.sizeofUnsignedVInt(t.epoch());
+            size += TypeSizes.sizeofUnsignedVInt(t.kind().ordinal());
+
+            switch (t.kind())
+            {
+                case NewTopology:
+                    size += TopologyUpdateSerializer.instance.serializedSize(((NewTopology) t).update, version);
+                    break;
+                case Topologies:
+                    TopologyImage image = (TopologyImage) t;
+
+                    size += TypeSizes.sizeof(image.update != null);
+                    if (image.update != null)
+                        size += TopologyUpdateSerializer.instance.serializedSize(image.update, version);
+
+                    size += Byte.BYTES;
+
+                    size += KeySerializers.ranges.serializedSize(image.closed, version);
+                    size += KeySerializers.ranges.serializedSize(image.retired, version);
+                    break;
+                default:
+                    throw new UnhandledEnum(t.kind());
+            }
+            return size;
+        }
+    }
+
+    enum Kind
+    {
+        NewTopology,
+        Topologies
+    }
+
+    class ImmutableTopoloyImage extends Journal.TopologyUpdate
+    {
+        public ImmutableTopoloyImage(TopologyImage image)
+        {
+            super(image.update.commandStores, image.update.local, image.update.global);
+        }
+    }
+
+    class TopologyImage implements AccordTopologyUpdate
+    {
+        private Journal.TopologyUpdate update;
+        private AccordConfigurationService.SyncStatus syncStatus = null;
+
+        private Ranges closed = Ranges.EMPTY;
+        private Ranges retired = Ranges.EMPTY;
+
+        private final long epoch;
+
+        public TopologyImage(long epoch)
+        {
+            this.epoch = epoch;
+        }
+
+        @Override
+        public long epoch()
+        {
+            return this.epoch;
+        }
+
+        @Override
+        public Kind kind()
+        {
+            return Kind.Topologies;
+        }
+
+        @Override
+        public void applyTo(TopologyImage accumulator)
+        {
+            Invariants.require(accumulator.epoch == epoch);
+            Invariants.require(accumulator.update == null || accumulator.update.equals(update));
+            accumulator.update = update;
+            // We're iterating in _reverse_ order
+            if (accumulator.syncStatus == null)
+                accumulator.syncStatus = syncStatus;
+            accumulator.closed = accumulator.closed.with(closed);
+            accumulator.retired = accumulator.retired.with(retired);
+        }
+    }
+
+    class NewTopology implements AccordTopologyUpdate
+    {
+        private final Journal.TopologyUpdate update;
+        private final long epoch;
+
+        public NewTopology(Journal.TopologyUpdate update)
+        {
+            this.epoch = update.global.epoch();
+            this.update = update;
+        }
+
+        @Override
+        public long epoch()
+        {
+            return this.epoch;
+        }
+
+        @Override
+        public Kind kind()
+        {
+            return Kind.NewTopology;
+        }
+
+        @Override
+        public void applyTo(TopologyImage accumulator)
+        {
+            Invariants.require(accumulator.epoch == epoch);
+            Invariants.require(accumulator.update == null);
+            accumulator.update = update;
+        }
+    }
+
+    class Accumulator
+    extends AccordJournalValueSerializers.Accumulator<NavigableMap<Long, TopologyImage>, AccordTopologyUpdate>
+    {
+        public Accumulator()
+        {
+            super(new TreeMap<>());
+        }
+
+        @Override
+        public void update(AccordTopologyUpdate newValue)
+        {
+            super.update(newValue);
+        }
+
+        public Iterator<ImmutableTopoloyImage> images()
+        {
+            return map(get().values().iterator(), ImmutableTopoloyImage::new);
+        }
+
+        @Override
+        protected NavigableMap<Long, TopologyImage> accumulate(NavigableMap<Long, TopologyImage> allEpochs, AccordTopologyUpdate update)
+        {
+            update.applyTo(allEpochs.computeIfAbsent(update.epoch(), v -> new TopologyImage(update.epoch())));
+            return allEpochs;
+        }
+    }
+
+    static <FROM, TO> Iterator<TO> map(Iterator<FROM> iter, Function<FROM, TO> fn)
+    {
+        return new Iterator<TO>()
+        {
+            public boolean hasNext()
+            {
+                return iter.hasNext();
+            }
+
+            public TO next()
+            {
+                return fn.apply(iter.next());
+            }
+        };
+    }
+
+    class AccumulatingSerializer
+    implements AccordJournalValueSerializers.FlyweightSerializer<AccordTopologyUpdate, Accumulator>
+    {
+        public static final AccumulatingSerializer defaultInstance = new AccumulatingSerializer(() -> 0);
+
+        private final EpochSupplier minEpoch;
+        public AccumulatingSerializer(EpochSupplier minEpoch)
+        {
+            this.minEpoch = minEpoch;
+        }
+
+        @Override
+        public Accumulator mergerFor(JournalKey key)
+        {
+            Invariants.require(key.type == TOPOLOGY_UPDATE);
+            return new Accumulator();
+        }
+
+        @Override
+        public void serialize(JournalKey key, AccordTopologyUpdate from, DataOutputPlus out, int version) throws IOException
+        {
+            out.writeUnsignedVInt32(1);
+            Serializer.instance.serialize(from, out, version);
+        }
+
+        @Override
+        public void reserialize(JournalKey key, Accumulator from, DataOutputPlus out, int version) throws IOException
+        {
+            out.writeUnsignedVInt32(from.get().size());
+            for (TopologyImage value : from.get().values())
+                Serializer.instance.serialize(value, out, version);
+        }
+
+        @Override
+        public void deserialize(JournalKey key, Accumulator into, DataInputPlus in, int version) throws IOException
+        {
+            long minEpoch = this.minEpoch.epoch();
+            int count = in.readUnsignedVInt32();
+            while (--count >= 0)
+            {
+                AccordTopologyUpdate update = Serializer.instance.deserialize(in, version);
+                if (update.epoch() >= minEpoch)
+                    into.update(update);
+                else
+                    return;
+            }
+        }
+    }
+}
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/service/accord/serializers/KeySerializers.java b/src/java/org/apache/cassandra/service/accord/serializers/KeySerializers.java
index 64e9369611..e967531aa3 100644
--- a/src/java/org/apache/cassandra/service/accord/serializers/KeySerializers.java
+++ b/src/java/org/apache/cassandra/service/accord/serializers/KeySerializers.java
@@ -882,7 +882,6 @@ public class KeySerializers
         }
     }
 
-    // TODO: port these to burn test serializers / journal
     public static Map<ByteBuffer, ByteBuffer> rangesToBlobMap(Ranges ranges)
     {
         TreeMap<ByteBuffer, ByteBuffer> result = new TreeMap<>();
diff --git a/src/java/org/apache/cassandra/service/accord/serializers/TopologySerializers.java b/src/java/org/apache/cassandra/service/accord/serializers/TopologySerializers.java
index fca97c5d45..91d5bf1bdf 100644
--- a/src/java/org/apache/cassandra/service/accord/serializers/TopologySerializers.java
+++ b/src/java/org/apache/cassandra/service/accord/serializers/TopologySerializers.java
@@ -125,8 +125,6 @@ public class TopologySerializers
 
     public static class ShardSerializer implements IVersionedSerializer<Shard>
     {
-        private static final int PENDING_REMOVAL = 1;
-        private static final int MUST_WITNESS = 2;
         protected IVersionedSerializer<Range> range;
 
         public ShardSerializer(IVersionedSerializer<Range> range)
diff --git a/test/distributed/org/apache/cassandra/distributed/test/AuthTest.java b/test/distributed/org/apache/cassandra/distributed/test/AuthTest.java
index f424691eac..cd63a45ac9 100644
--- a/test/distributed/org/apache/cassandra/distributed/test/AuthTest.java
+++ b/test/distributed/org/apache/cassandra/distributed/test/AuthTest.java
@@ -80,6 +80,8 @@ public class AuthTest extends TestBaseImpl
                                         .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(2, 1))
                                         .withConfig(config -> config.with(NETWORK, GOSSIP, NATIVE_PROTOCOL)
                                                                     .set("authenticator", "PasswordAuthenticator")
+                                                                    // Test drops all TCM communication, which precludes topology discovery
+                                                                    .set("accord.enabled", false)
                                                                     .set("credentials_validity", "2s")) // revert to OSS default
                                         .start())
         {
diff --git a/test/distributed/org/apache/cassandra/distributed/test/topology/DecommissionAvoidTimeouts.java b/test/distributed/org/apache/cassandra/distributed/test/topology/DecommissionAvoidTimeouts.java
index 69ccafdd45..ddf59ae840 100644
--- a/test/distributed/org/apache/cassandra/distributed/test/topology/DecommissionAvoidTimeouts.java
+++ b/test/distributed/org/apache/cassandra/distributed/test/topology/DecommissionAvoidTimeouts.java
@@ -74,6 +74,7 @@ public abstract class DecommissionAvoidTimeouts extends TestBaseImpl
     @Test
     public void test() throws Exception
     {
+        IInvokableInstance paused = null;
         try (Cluster cluster = Cluster.build(8)
                                       .withRacks(2, 4)
                                       .withInstanceInitializer(new BB())
@@ -103,11 +104,13 @@ public abstract class DecommissionAvoidTimeouts extends TestBaseImpl
                 toDecom.coordinator().execute("INSERT INTO " + table + "(pk) VALUES (?)", ConsistencyLevel.EACH_QUORUM, key);
             }
 
-            Callable<?> pending = pauseBeforeCommit(cluster.get(1), (e) -> e instanceof PrepareLeave.StartLeave);
+            paused = cluster.get(1);
+            Callable<?> pending = pauseBeforeCommit(paused, (e) -> e instanceof PrepareLeave.StartLeave);
             CompletableFuture<Void> nodetool = CompletableFuture.runAsync(() -> toDecom.nodetoolResult("decommission").asserts().success());
             ClusterUtils.awaitGossipStateMatch(cluster, cluster.get(DECOM_NODE), ApplicationState.SEVERITY);
             pending.call();
-            unpauseCommits(cluster.get(1));
+            unpauseCommits(paused);
+            paused = null;
 
             cluster.forEach(i -> i.runOnInstance(() -> ((DynamicEndpointSnitch) DatabaseDescriptor.getNodeProximity()).updateScores()));
             cluster.filters().verbs(Verb.GOSSIP_DIGEST_SYN.id).drop();
@@ -169,6 +172,12 @@ public abstract class DecommissionAvoidTimeouts extends TestBaseImpl
                 // ignore
             }
         }
+        finally
+        {
+            if (paused != null)
+                unpauseCommits(paused);
+
+        }
     }
 
     protected abstract String getQuery(String table);
diff --git a/test/distributed/org/apache/cassandra/fuzz/ring/ConsistentBootstrapTest.java b/test/distributed/org/apache/cassandra/fuzz/ring/ConsistentBootstrapTest.java
index c854a5da76..08af4f4d4b 100644
--- a/test/distributed/org/apache/cassandra/fuzz/ring/ConsistentBootstrapTest.java
+++ b/test/distributed/org/apache/cassandra/fuzz/ring/ConsistentBootstrapTest.java
@@ -19,6 +19,8 @@
 package org.apache.cassandra.fuzz.ring;
 
 import java.util.concurrent.Callable;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.Consumer;
 
 import org.junit.Assert;
 import org.junit.Test;
@@ -32,6 +34,7 @@ import org.apache.cassandra.distributed.api.Feature;
 import org.apache.cassandra.distributed.api.IInstanceConfig;
 import org.apache.cassandra.distributed.api.IInvokableInstance;
 import org.apache.cassandra.distributed.api.TokenSupplier;
+import org.apache.cassandra.distributed.shared.ClusterUtils;
 import org.apache.cassandra.distributed.shared.NetworkTopology;
 import org.apache.cassandra.distributed.test.log.FuzzTestBase;
 import org.apache.cassandra.harry.SchemaSpec;
@@ -66,17 +69,16 @@ public class ConsistentBootstrapTest extends FuzzTestBase
     public void bootstrapFuzzTest() throws Throwable
     {
         Generator<SchemaSpec> schemaGen = SchemaGenerators.schemaSpecGen(KEYSPACE, "bootstrap_fuzz", 1000);
-        IInvokableInstance forShutdown = null;
         try (Cluster cluster = builder().withNodes(3)
                                         .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(4))
                                         .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(4, "dc0", "rack0"))
                                         .withConfig((config) -> config.with(Feature.NETWORK, Feature.GOSSIP)
                                                                       .set("write_request_timeout", "10s")
                                                                       .set("metadata_snapshot_frequency", 5))
-                                        .start())
+                                        .start();
+             CloseableRef<IInvokableInstance> forShutdown = new CloseableRef<>(ClusterUtils::unpauseCommits))
         {
             IInvokableInstance cmsInstance = cluster.get(1);
-            forShutdown = cmsInstance;
             waitForCMSToQuiesce(cluster, cmsInstance);
 
             withRandom(rng -> {
@@ -107,6 +109,7 @@ public class ConsistentBootstrapTest extends FuzzTestBase
                                                     .set(Constants.KEY_DTEST_FULL_STARTUP, true);
                     IInvokableInstance newInstance = cluster.bootstrap(config);
 
+                    forShutdown.set(cmsInstance);
                     // Prime the CMS node to pause before the finish join event is committed
                     Callable<?> pending = pauseBeforeCommit(cmsInstance, (e) -> e instanceof PrepareJoin.FinishJoin);
                     new Thread(() -> newInstance.startup()).start();
@@ -123,6 +126,7 @@ public class ConsistentBootstrapTest extends FuzzTestBase
 
                     // wait for the cluster to all witness the finish join event
                     unpauseCommits(cmsInstance);
+                    forShutdown.set(null);
                     waitForCMSToQuiesce(cluster, bootstrapVisible.call());
                 }, "Finish bootstrap");
                 writeAndValidate.run();
@@ -134,31 +138,42 @@ public class ConsistentBootstrapTest extends FuzzTestBase
                                                         history);
             });
         }
-        catch (Throwable t)
+    }
+
+    public static class CloseableRef<T> extends AtomicReference<T> implements AutoCloseable
+    {
+        private final Consumer<T> onClose;
+
+        public CloseableRef(Consumer<T> onClose)
         {
-            if (forShutdown != null)
-                unpauseCommits(forShutdown);
-            throw t;
+            this.onClose = onClose;
         }
-    }
 
+        @Override
+        public void close() throws Exception
+        {
+            T v = getAndSet(null);
+            if (v != null)
+                onClose.accept(v);
+        }
+    }
     @Test
     public void coordinatorIsBehindTest() throws Throwable
     {
         Generator<SchemaSpec> schemaGen = SchemaGenerators.schemaSpecGen(KEYSPACE, "coordinator_is_behind", 1000);
 
-        IInvokableInstance forShutdown = null;
         try (Cluster cluster = builder().withNodes(3)
                                         .withTokenSupplier(TokenSupplier.evenlyDistributedTokens(4))
                                         .withNodeIdTopology(NetworkTopology.singleDcNetworkTopology(4, "dc0", "rack0"))
                                         .withConfig((config) -> config.with(Feature.NETWORK, Feature.GOSSIP)
                                                                       .set("write_request_timeout", "10s")
                                                                       .set("accord.enabled", false)
+                                                                      .set("cms_await_timeout", "60s")
                                                                       .set("metadata_snapshot_frequency", 5))
-                                        .start())
+                                        .start();
+             CloseableRef<IInvokableInstance> forShutdown = new CloseableRef<>(ClusterUtils::unpauseCommits))
         {
             IInvokableInstance cmsInstance = cluster.get(1);
-            forShutdown = cmsInstance;
             waitForCMSToQuiesce(cluster, cmsInstance);
 
             withRandom(rng -> {
@@ -196,6 +211,7 @@ public class ConsistentBootstrapTest extends FuzzTestBase
 
                 // Prime the CMS node to pause before the finish join event is committed
                 Callable<?> pending = pauseBeforeCommit(cmsInstance, (e) -> e instanceof PrepareJoin.MidJoin);
+                forShutdown.set(cmsInstance);
                 IInstanceConfig config = cluster.newInstanceConfig()
                                                 .set("auto_bootstrap", true)
                                                 .set(Constants.KEY_DTEST_FULL_STARTUP, true)
@@ -266,14 +282,9 @@ public class ConsistentBootstrapTest extends FuzzTestBase
 
                 cluster.filters().reset();
                 unpauseCommits(cmsInstance);
+                forShutdown.set(null);
                 startup.join();
             });
         }
-        catch (Throwable t)
-        {
-            if (forShutdown != null)
-                unpauseCommits(forShutdown);
-            throw t;
-        }
     }
 }
\ No newline at end of file
diff --git a/test/distributed/org/apache/cassandra/fuzz/topology/TopologyMixupTestBase.java b/test/distributed/org/apache/cassandra/fuzz/topology/TopologyMixupTestBase.java
index f38ad8108c..f9cef82836 100644
--- a/test/distributed/org/apache/cassandra/fuzz/topology/TopologyMixupTestBase.java
+++ b/test/distributed/org/apache/cassandra/fuzz/topology/TopologyMixupTestBase.java
@@ -708,11 +708,15 @@ public abstract class TopologyMixupTestBase<S extends TopologyMixupTestBase.Sche
             int quorum = topologyHistory.quorum();
             // find what ranges are able to handle 1 node loss
             Set<Range> safeRanges = new HashSet<>();
+            Set<Integer> cms = new HashSet<>();
+            for (Integer node : cmsGroup)
+                cms.add(node);
+
             ring.rangesToReplicas((range, replicas) -> {
                 IntHashSet alive = new IntHashSet();
                 for (int peer : replicas)
                 {
-                    if (up.contains(peer))
+                    if (up.contains(peer) && !cms.contains(peer))
                         alive.add(peer);
                 }
                 if (quorum < alive.size())
diff --git a/test/distributed/org/apache/cassandra/service/accord/AccordJournalBurnTest.java b/test/distributed/org/apache/cassandra/service/accord/AccordJournalBurnTest.java
index d1653c6228..ee963915f2 100644
--- a/test/distributed/org/apache/cassandra/service/accord/AccordJournalBurnTest.java
+++ b/test/distributed/org/apache/cassandra/service/accord/AccordJournalBurnTest.java
@@ -20,8 +20,11 @@ package org.apache.cassandra.service.accord;
 
 import java.nio.file.Files;
 import java.util.List;
+import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicInteger;
 
+import javax.annotation.Nullable;
+
 import org.junit.Before;
 import org.junit.Test;
 import org.slf4j.Logger;
@@ -53,6 +56,7 @@ import org.apache.cassandra.service.accord.serializers.KeySerializers;
 import org.apache.cassandra.service.accord.serializers.ResultSerializers;
 import org.apache.cassandra.service.accord.serializers.TopologySerializers;
 import org.apache.cassandra.tools.FieldUtil;
+import org.apache.cassandra.utils.concurrent.Condition;
 
 import static accord.impl.PrefixedIntHashKey.ranges;
 
@@ -123,12 +127,10 @@ public class AccordJournalBurnTest extends BurnTestBase
             {
                 ServerTestUtils.daemonInitialization();
 
-                TableMetadata[] metadatas = new TableMetadata[3 + nodes.size()];
+                TableMetadata[] metadatas = new TableMetadata[1 + nodes.size()];
                 metadatas[0] = AccordKeyspace.CommandsForKeys;
-                metadatas[1] = AccordKeyspace.Topologies;
-                metadatas[2] = AccordKeyspace.EpochMetadata;
                 for (int i = 0; i < nodes.size(); i++)
-                    metadatas[3 + i] = AccordKeyspace.journalMetadata("journal_" + nodes.get(i), false);
+                    metadatas[1 + i] = AccordKeyspace.journalMetadata("journal_" + nodes.get(i), false);
 
                 AccordKeyspace.TABLES = Tables.of(metadatas);
                 setUp();
@@ -152,6 +154,18 @@ public class AccordJournalBurnTest extends BurnTestBase
                          cfs.disableAutoCompaction();
                          AccordJournal journal = new AccordJournal(new TestParams()
                          {
+                             @Override
+                             public FlushMode flushMode()
+                             {
+                                 return FlushMode.PERIODIC;
+                             }
+
+                             @Override
+                             public long flushPeriod(TimeUnit units)
+                             {
+                                 return 1;
+                             }
+
                              @Override
                              public int segmentSize()
                              {
@@ -165,9 +179,39 @@ public class AccordJournalBurnTest extends BurnTestBase
                              }
                          }, new AccordAgent(), directory, cfs)
                          {
+                             @Override
+                             public void saveCommand(int store, CommandUpdate update, @Nullable Runnable onFlush)
+                             {
+                                 Condition condition = Condition.newOneTimeCondition();
+                                 super.saveCommand(store, update, condition::signal);
+                                 condition.awaitUninterruptibly();
+                                 if (onFlush != null)
+                                     onFlush.run();
+                             }
+
+                             @Override
+                             public void saveStoreState(int store, FieldUpdates fieldUpdates, @Nullable Runnable onFlush)
+                             {
+                                 Condition condition = Condition.newOneTimeCondition();
+                                 super.saveStoreState(store, fieldUpdates, condition::signal);
+                                 if (onFlush != null)
+                                     onFlush.run();
+                             }
+
+                             @Override
+                             public void saveTopology(TopologyUpdate topologyUpdate, @Nullable Runnable onFlush)
+                             {
+                                 Condition condition = Condition.newOneTimeCondition();
+                                 super.saveTopology(topologyUpdate, condition::signal);
+                                 if (onFlush != null)
+                                     onFlush.run();
+                             }
+
+                             @Override
                              public void replay(CommandStores commandStores)
                              {
-                                 closeCurrentSegmentForTestingIfNonEmpty();
+                                 // Make sure to replay _only_ static segments
+                                 this.closeCurrentSegmentForTestingIfNonEmpty();
                                  super.replay(commandStores);
                              }
                          };
diff --git a/test/simulator/test/org/apache/cassandra/simulator/test/AccordHarrySimulationTest.java b/test/simulator/test/org/apache/cassandra/simulator/test/AccordHarrySimulationTest.java
index 96ede9a8c8..8506fb6157 100644
--- a/test/simulator/test/org/apache/cassandra/simulator/test/AccordHarrySimulationTest.java
+++ b/test/simulator/test/org/apache/cassandra/simulator/test/AccordHarrySimulationTest.java
@@ -49,8 +49,7 @@ public class AccordHarrySimulationTest extends HarrySimulatorTest
                                                                Verb.ACCORD_BEGIN_RECOVER_RSP, Verb.ACCORD_BEGIN_RECOVER_REQ, Verb.ACCORD_BEGIN_INVALIDATE_RSP));
 
         Set<Verb> somewhatLossy = new HashSet<>(Arrays.asList(Verb.ACCORD_SYNC_NOTIFY_RSP, Verb.ACCORD_SYNC_NOTIFY_REQ, Verb.ACCORD_APPLY_AND_WAIT_REQ,
-                                                              Verb.ACCORD_FETCH_MIN_EPOCH_RSP, Verb.ACCORD_FETCH_MIN_EPOCH_REQ, Verb.ACCORD_FETCH_TOPOLOGY_RSP,
-                                                              Verb.ACCORD_FETCH_TOPOLOGY_REQ));
+                                                              Verb.ACCORD_FETCH_TOPOLOGY_RSP, Verb.ACCORD_FETCH_TOPOLOGY_REQ));
 
         Map<Verb, FutureActionScheduler> schedulers = new HashMap<>();
         for (Verb verb : Verb.values())
diff --git a/test/simulator/test/org/apache/cassandra/simulator/test/EpochStressTest.java b/test/simulator/test/org/apache/cassandra/simulator/test/EpochStressTest.java
index 2f1d635392..8aaab3d78a 100644
--- a/test/simulator/test/org/apache/cassandra/simulator/test/EpochStressTest.java
+++ b/test/simulator/test/org/apache/cassandra/simulator/test/EpochStressTest.java
@@ -61,9 +61,9 @@ import static org.apache.cassandra.simulator.cluster.ClusterActions.Options.noAc
  *
  * And then run your test using the following settings (omit add-* if you are running on jdk8):
  *
- -Dstorage-config=/Users/dcapwell/src/github/apache/cassandra/cep-15-accord/test/conf
+ -Dstorage-config=$MODULE_DIR$/test/conf
  -Djava.awt.headless=true
- -javaagent:/Users/dcapwell/src/github/apache/cassandra/cep-15-accord/lib/jamm-0.4.0.jar
+ -javaagent:$MODULE_DIR$/lib/jamm-0.4.0.jar
  -ea
  -Dcassandra.debugrefcount=true
  -Xss384k
@@ -85,15 +85,15 @@ import static org.apache.cassandra.simulator.cluster.ClusterActions.Options.noAc
  -Dcassandra.test.messagingService.nonGracefulShutdown=true
  -Dcassandra.use_nix_recursive_delete=true
  -Dcie-cassandra.disable_schema_drop_log=true
- -Dlogback.configurationFile=file:///Users/dcapwell/src/github/apache/cassandra/cep-15-accord/test/conf/logback-simulator.xml
+ -Dlogback.configurationFile=file://$MODULE_DIR$/test/conf/logback-simulator.xml
  -Dcassandra.ring_delay_ms=10000
  -Dcassandra.tolerate_sstable_size=true
  -Dcassandra.skip_sync=true
  -Dcassandra.debugrefcount=false
  -Dcassandra.test.simulator.determinismcheck=strict
  -Dcassandra.test.simulator.print_asm=none
- -javaagent:/Users/dcapwell/src/github/apache/cassandra/cep-15-accord/build/test/lib/jars/simulator-asm.jar
- -Xbootclasspath/a:/Users/dcapwell/src/github/apache/cassandra/cep-15-accord/build/test/lib/jars/simulator-bootstrap.jar
+ -javaagent:$MODULE_DIR$/build/test/lib/jars/simulator-asm.jar
+ -Xbootclasspath/a:$MODULE_DIR$/build/test/lib/jars/simulator-bootstrap.jar
  -XX:ActiveProcessorCount=4
  -XX:-TieredCompilation
  -XX:-BackgroundCompilation
@@ -121,6 +121,7 @@ import static org.apache.cassandra.simulator.cluster.ClusterActions.Options.noAc
  --add-opens jdk.management/com.sun.management.internal=ALL-UNNAMED
  --add-opens jdk.management.jfr/jdk.management.jfr=ALL-UNNAMED
  --add-opens java.desktop/com.sun.beans.introspect=ALL-UNNAMED
+
  */
 public class EpochStressTest extends SimulationTestBase
 {
diff --git a/test/unit/org/apache/cassandra/db/compaction/CompactionAccordIteratorsTest.java b/test/unit/org/apache/cassandra/db/compaction/CompactionAccordIteratorsTest.java
index f3137c8c51..97e85986c6 100644
--- a/test/unit/org/apache/cassandra/db/compaction/CompactionAccordIteratorsTest.java
+++ b/test/unit/org/apache/cassandra/db/compaction/CompactionAccordIteratorsTest.java
@@ -263,7 +263,7 @@ public class CompactionAccordIteratorsTest
     {
         IAccordService mockAccordService = mock(IAccordService.class);
         IAccordService.AccordCompactionInfo compactionInfo = new IAccordService.AccordCompactionInfo(commandStore.id(), redundantBefore, commandStore.unsafeGetRangesForEpoch(), ((AccordCommandStore)commandStore).tableId());
-        IAccordService.AccordCompactionInfos compactionInfos = new IAccordService.AccordCompactionInfos(durableBefore);
+        IAccordService.AccordCompactionInfos compactionInfos = new IAccordService.AccordCompactionInfos(durableBefore, 0);
         compactionInfos.put(commandStore.id(), compactionInfo);
         when(mockAccordService.getCompactionInfo()).thenReturn(compactionInfos);
         return mockAccordService;
diff --git a/test/unit/org/apache/cassandra/db/virtual/AccordDebugKeyspaceTest.java b/test/unit/org/apache/cassandra/db/virtual/AccordDebugKeyspaceTest.java
index ea03f381c0..fec005868d 100644
--- a/test/unit/org/apache/cassandra/db/virtual/AccordDebugKeyspaceTest.java
+++ b/test/unit/org/apache/cassandra/db/virtual/AccordDebugKeyspaceTest.java
@@ -241,8 +241,6 @@ public class AccordDebugKeyspaceTest extends CQLTester
                 case ACCORD_AWAIT_REQ:
                 case ACCORD_AWAIT_RSP:
                 case ACCORD_AWAIT_ASYNC_RSP_REQ:
-                case ACCORD_FETCH_MIN_EPOCH_REQ:
-                case ACCORD_FETCH_MIN_EPOCH_RSP:
                     return true;
                 default:
                     // many code paths don't log the error...
diff --git a/test/unit/org/apache/cassandra/db/virtual/AccordVirtualTablesTest.java b/test/unit/org/apache/cassandra/db/virtual/AccordVirtualTablesTest.java
index dc01f9f37f..d53906cc3c 100644
--- a/test/unit/org/apache/cassandra/db/virtual/AccordVirtualTablesTest.java
+++ b/test/unit/org/apache/cassandra/db/virtual/AccordVirtualTablesTest.java
@@ -92,12 +92,12 @@ public class AccordVirtualTablesTest extends CQLTester
     {
         TopologyManager tm = empty();
         long e1 = 1;
-        tm.onTopologyUpdate(topology(e1, T1), () -> ConfigurationService.EpochReady.done(e1));
+        tm.onTopologyUpdate(topology(e1, T1), () -> ConfigurationService.EpochReady.done(e1), e -> {});
         assertRows(execute("SELECT * FROM " + VIRTUAL_VIEWS + "." + AccordVirtualTables.EPOCHS),
                    row(e1, true, SUCCESS, SUCCESS, SUCCESS, SUCCESS));
 
         long e2 = 2;
-        tm.onTopologyUpdate(topology(e2, T1), () -> pendingReady(e1));
+        tm.onTopologyUpdate(topology(e2, T1), () -> pendingReady(e1), e -> {});
         assertRows(execute("SELECT * FROM " + VIRTUAL_VIEWS + "." + AccordVirtualTables.EPOCHS),
                    row(e2, false, PENDING, PENDING, PENDING, PENDING),
                    row(e1, true, SUCCESS, SUCCESS, SUCCESS, SUCCESS));
@@ -108,7 +108,7 @@ public class AccordVirtualTablesTest extends CQLTester
     {
         TopologyManager tm = empty();
         long e1 = 1;
-        tm.onTopologyUpdate(topology(e1, T1), () -> ConfigurationService.EpochReady.done(e1));
+        tm.onTopologyUpdate(topology(e1, T1), () -> ConfigurationService.EpochReady.done(e1), e -> {});
 
         // the range was added in the first epoch, so its fully synced
         assertRows(execute("SELECT * FROM " + VIRTUAL_VIEWS + "." + AccordVirtualTables.TABLE_EPOCHS),
@@ -116,7 +116,7 @@ public class AccordVirtualTablesTest extends CQLTester
 
         // range is no longer "added" so doesn't show up as synced!
         long e2 = 2;
-        tm.onTopologyUpdate(topology(e2, T1), () -> ConfigurationService.EpochReady.done(e2));
+        tm.onTopologyUpdate(topology(e2, T1), () -> ConfigurationService.EpochReady.done(e2), e -> {});
         assertRows(execute("SELECT * FROM " + VIRTUAL_VIEWS + "." + AccordVirtualTables.TABLE_EPOCHS),
                    row(e1, T1_META.keyspace, T1_META.name, FULL_RANGE, List.of(), List.of(), List.of(), FULL_RANGE));
 
diff --git a/test/unit/org/apache/cassandra/exceptions/RequestFailureReasonTest.java b/test/unit/org/apache/cassandra/exceptions/RequestFailureReasonTest.java
index 3b89fe9c64..9162a87e85 100644
--- a/test/unit/org/apache/cassandra/exceptions/RequestFailureReasonTest.java
+++ b/test/unit/org/apache/cassandra/exceptions/RequestFailureReasonTest.java
@@ -40,7 +40,10 @@ public class RequestFailureReasonTest
     { 8, "NOT_CMS" },
     { 9, "INVALID_ROUTING" },
     { 10, "COORDINATOR_BEHIND" },
-    { 503, "INDEX_BUILD_IN_PROGRESS" }
+    { 503, "INDEX_BUILD_IN_PROGRESS" },
+    { 504, "RETRY_ON_DIFFERENT_TRANSACTION_SYSTEM" },
+    { 505, "BOOTING" },
+    { 506, "ACCORD_DISABLED" }
     };
 
     @Test
diff --git a/test/unit/org/apache/cassandra/index/accord/RouteIndexTest.java b/test/unit/org/apache/cassandra/index/accord/RouteIndexTest.java
index 8451ba609d..02db16b993 100644
--- a/test/unit/org/apache/cassandra/index/accord/RouteIndexTest.java
+++ b/test/unit/org/apache/cassandra/index/accord/RouteIndexTest.java
@@ -426,7 +426,7 @@ public class RouteIndexTest extends CQLTester.InMemory
         @Override
         public void runUnit(Sut sut)
         {
-            sut.journal.get().purge(sut.stores.get());
+            sut.journal.get().purge(sut.stores.get(), () -> 0);
         }
     };
 
@@ -464,7 +464,6 @@ public class RouteIndexTest extends CQLTester.InMemory
         private final Gen<TokenRange> rangeGen;
         private final Gen<Domain> domainGen;
         private final ColumnFamilyStore journalTable;
-        //        private final AccordJournal journal;
         private AccordService accordService;
         private int hlc = 1000;
 
@@ -642,7 +641,7 @@ public class RouteIndexTest extends CQLTester.InMemory
 
     private static IAccordService.AccordCompactionInfos emptyCompactionInfo(TableId tableId, RedundantBefore redundantBefore, Int2ObjectHashMap<RangesForEpoch> storeRangesForEpoch)
     {
-        IAccordService.AccordCompactionInfos compactionInfos = new IAccordService.AccordCompactionInfos(DurableBefore.EMPTY);
+        IAccordService.AccordCompactionInfos compactionInfos = new IAccordService.AccordCompactionInfos(DurableBefore.EMPTY, 0);
         for (int i = 0; i < storeRangesForEpoch.size(); i++)
             compactionInfos.put(i, new AccordCompactionInfo(i, redundantBefore, storeRangesForEpoch.get(i), tableId));
         return compactionInfos;
diff --git a/test/unit/org/apache/cassandra/service/accord/AccordConfigurationServiceTest.java b/test/unit/org/apache/cassandra/service/accord/AccordConfigurationServiceTest.java
index b7c3c5c705..09ca44d9d8 100644
--- a/test/unit/org/apache/cassandra/service/accord/AccordConfigurationServiceTest.java
+++ b/test/unit/org/apache/cassandra/service/accord/AccordConfigurationServiceTest.java
@@ -19,6 +19,7 @@
 package org.apache.cassandra.service.accord;
 
 import java.net.UnknownHostException;
+import java.nio.file.Files;
 import java.util.ArrayList;
 import java.util.Collections;
 import java.util.Iterator;
@@ -26,16 +27,11 @@ import java.util.List;
 import java.util.Optional;
 import java.util.UUID;
 
-import com.google.common.collect.Sets;
-import org.junit.Assert;
-import org.junit.Before;
 import org.junit.BeforeClass;
 import org.junit.Test;
 
 import accord.api.Journal;
 import accord.impl.AbstractConfigurationServiceTest;
-import accord.impl.TestAgent;
-import accord.impl.basic.InMemoryJournal;
 import accord.local.Node.Id;
 import accord.topology.Topology;
 import accord.utils.SortedArrays.SortedArrayList;
@@ -45,11 +41,14 @@ import org.apache.cassandra.SchemaLoader;
 import org.apache.cassandra.ServerTestUtils;
 import org.apache.cassandra.concurrent.ScheduledExecutors;
 import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.ColumnFamilyStore;
 import org.apache.cassandra.db.Keyspace;
 import org.apache.cassandra.db.marshal.Int32Type;
 import org.apache.cassandra.dht.Murmur3Partitioner;
 import org.apache.cassandra.dht.Range;
 import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.util.File;
+import org.apache.cassandra.journal.TestParams;
 import org.apache.cassandra.locator.InetAddressAndPort;
 import org.apache.cassandra.locator.Replica;
 import org.apache.cassandra.net.ConnectionType;
@@ -59,10 +58,12 @@ import org.apache.cassandra.net.RequestCallback;
 import org.apache.cassandra.schema.DistributedSchema;
 import org.apache.cassandra.schema.KeyspaceMetadata;
 import org.apache.cassandra.schema.KeyspaceParams;
+import org.apache.cassandra.schema.Schema;
 import org.apache.cassandra.schema.TableId;
 import org.apache.cassandra.schema.TableMetadata;
 import org.apache.cassandra.schema.Tables;
-import org.apache.cassandra.service.accord.AccordKeyspace.EpochDiskState;
+import org.apache.cassandra.service.accord.api.AccordAgent;
+import org.apache.cassandra.service.accord.journal.AccordTopologyUpdate;
 import org.apache.cassandra.tcm.ClusterMetadata;
 import org.apache.cassandra.tcm.ValidatingClusterMetadataService;
 import org.apache.cassandra.tcm.membership.Location;
@@ -75,13 +76,7 @@ import org.apache.cassandra.utils.MockFailureDetector;
 import org.apache.cassandra.utils.concurrent.Future;
 
 import static accord.impl.AbstractConfigurationServiceTest.TestListener;
-import static java.lang.String.format;
-import static org.apache.cassandra.cql3.QueryProcessor.executeInternal;
 import static org.apache.cassandra.cql3.statements.schema.CreateTableStatement.parse;
-import static org.apache.cassandra.schema.SchemaConstants.ACCORD_KEYSPACE_NAME;
-import static org.apache.cassandra.service.accord.AccordKeyspace.EPOCH_METADATA;
-import static org.apache.cassandra.service.accord.AccordKeyspace.TOPOLOGIES;
-import static org.apache.cassandra.service.accord.AccordKeyspace.loadEpoch;
 
 public class AccordConfigurationServiceTest
 {
@@ -160,144 +155,84 @@ public class AccordConfigurationServiceTest
     @BeforeClass
     public static void beforeClass() throws Throwable
     {
-        DatabaseDescriptor.daemonInitialization();
-        DatabaseDescriptor.setPartitionerUnsafe(Murmur3Partitioner.instance);
         ServerTestUtils.daemonInitialization();
+        DatabaseDescriptor.setPartitionerUnsafe(Murmur3Partitioner.instance);
+
         SchemaLoader.prepareServer();
         SchemaLoader.createKeyspace("ks", KeyspaceParams.simple(1),
                                     parse("CREATE TABLE tbl (k int, c int, v int, primary key (k, c)) WITH transactional_mode='full'", "ks"));
     }
 
-    @Before
-    public void setup()
-    {
-        Keyspace.open(ACCORD_KEYSPACE_NAME).getColumnFamilyStore(TOPOLOGIES).truncateBlocking();
-        Keyspace.open(ACCORD_KEYSPACE_NAME).getColumnFamilyStore(EPOCH_METADATA).truncateBlocking();
-    }
-
-    @Test
-    public void initialEpochTest() throws Throwable
-    {
-        ValidatingClusterMetadataService cms = ValidatingClusterMetadataService.createAndRegister(Version.MIN_ACCORD_VERSION);
-
-
-        AccordConfigurationService service = new AccordConfigurationService(ID1, new Messaging(), new MockFailureDetector(), AccordConfigurationService.SystemTableDiskStateManager.instance, ScheduledExecutors.scheduledTasks);
-        Assert.assertEquals(null, AccordKeyspace.loadEpochDiskState());
-        service.start();
-        Assert.assertEquals(null, AccordKeyspace.loadEpochDiskState());
-        Assert.assertTrue(executeInternal(format("SELECT * FROM %s.%s WHERE epoch=1", ACCORD_KEYSPACE_NAME, TOPOLOGIES)).isEmpty());
-
-        Topology topology1 = createTopology(cms);
-        service.reportTopology(topology1);
-        loadEpoch(1, (epoch, syncStatus, pendingSync, remoteSync, closed, redundant) -> {
-            Assert.assertTrue(remoteSync.isEmpty());
-        });
-        Assert.assertEquals(EpochDiskState.create(1), service.diskState());
-
-        service.receiveRemoteSyncComplete(ID1, 1);
-        service.receiveRemoteSyncComplete(ID2, 1);
-        loadEpoch(1, (epoch, syncStatus, pendingSync, remoteSync, closed, redundant) -> {
-            Assert.assertEquals(Sets.newHashSet(ID1, ID2), remoteSync);
-        });
-    }
-
     @Test
-    public void loadTest()
+    public void loadTest() throws Throwable
     {
         ValidatingClusterMetadataService cms = ValidatingClusterMetadataService.createAndRegister(Version.MIN_ACCORD_VERSION);
 
-        InMemoryJournal journal = new InMemoryJournal(ID1, new TestAgent());
-        AccordConfigurationService service = new AccordConfigurationService(ID1, new Messaging(), new MockFailureDetector(), AccordConfigurationService.SystemTableDiskStateManager.instance, ScheduledExecutors.scheduledTasks);
-        TestListener listener = new TestListener(service, true) {
-            @Override
-            public AsyncResult<Void> onTopologyUpdate(Topology topology, boolean isLoad, boolean startSync)
+        AccordJournal journal = null;
+        try
+        {
+            journal = initJournal();
+            AccordConfigurationService service = new AccordConfigurationService(ID1, new Messaging(), new MockFailureDetector(), ScheduledExecutors.scheduledTasks);
+            AccordJournal journal_ = journal;
+            TestListener listener = new TestListener(service, true)
             {
-                // Fake journal save
-                journal.saveTopology(new Journal.TopologyUpdate(new Int2ObjectHashMap<>(), topology, topology), () -> {});
-                return super.onTopologyUpdate(topology, isLoad, startSync);
-            }
-        };
-        service.registerListener(listener);
-        service.start();
-
-        Topology topology1 = createTopology(cms);
-        service.updateMapping(mappingForEpoch(cms.metadata().epoch.getEpoch() + 1));
-        service.reportTopology(topology1);
-        service.receiveRemoteSyncComplete(ID1, 1);
-        service.receiveRemoteSyncComplete(ID2, 1);
-        service.receiveRemoteSyncComplete(ID3, 1);
-
-        Topology topology2 = createTopology(cms);
-        service.reportTopology(topology2);
-        service.receiveRemoteSyncComplete(ID1, 2);
-
-        Topology topology3 = createTopology(cms);
-        service.reportTopology(topology3);
-
-        AccordConfigurationService loaded = new AccordConfigurationService(ID1, new Messaging(), new MockFailureDetector(), AccordConfigurationService.SystemTableDiskStateManager.instance, ScheduledExecutors.scheduledTasks);
-        loaded.updateMapping(mappingForEpoch(cms.metadata().epoch.getEpoch() + 1));
-        listener = new AbstractConfigurationServiceTest.TestListener(loaded, true);
-        loaded.registerListener(listener);
-        Iterator<Journal.TopologyUpdate> iter = journal.replayTopologies();
-        // Simulate journal replay
-        while (iter.hasNext())
-            loaded.reportTopology(iter.next().global);
-        loaded.start();
-
-        listener.assertNoTruncates();
-        listener.assertTopologiesFor(1L, 2L, 3L);
-        listener.assertTopologyForEpoch(1, topology1);
-        listener.assertTopologyForEpoch(2, topology2);
-        listener.assertTopologyForEpoch(3, topology3);
-        listener.assertSyncsFor(1L, 2L);
-        listener.assertSyncsForEpoch(1, ID1, ID2, ID3);
-        listener.assertSyncsForEpoch(2, ID1);
+                @Override
+                public AsyncResult<Void> onTopologyUpdate(Topology topology, boolean isLoad, boolean startSync)
+                {
+                    // Fake journal save
+                    journal_.saveTopology(new Journal.TopologyUpdate(new Int2ObjectHashMap<>(), topology, topology), () -> {});
+                    return super.onTopologyUpdate(topology, isLoad, startSync);
+                }
+            };
+            service.registerListener(listener);
+            service.start();
+
+            Topology topology1 = createTopology(cms);
+            service.updateMapping(mappingForEpoch(cms.metadata().epoch.getEpoch() + 1));
+            service.reportTopology(topology1);
+            service.receiveRemoteSyncComplete(ID1, 1);
+            service.receiveRemoteSyncComplete(ID2, 1);
+            service.receiveRemoteSyncComplete(ID3, 1);
+
+            Topology topology2 = createTopology(cms);
+            service.reportTopology(topology2);
+            service.receiveRemoteSyncComplete(ID1, 2);
+
+            Topology topology3 = createTopology(cms);
+            service.reportTopology(topology3);
+
+            AccordConfigurationService loaded = new AccordConfigurationService(ID1, new Messaging(), new MockFailureDetector(), ScheduledExecutors.scheduledTasks);
+            loaded.updateMapping(mappingForEpoch(cms.metadata().epoch.getEpoch() + 1));
+            listener = new AbstractConfigurationServiceTest.TestListener(loaded, true);
+            loaded.registerListener(listener);
+            Iterator<AccordTopologyUpdate.ImmutableTopoloyImage> iter = journal.replayTopologies();
+            // Simulate journal replay
+            while (iter.hasNext())
+                loaded.reportTopology(iter.next().global);
+            loaded.start();
+
+            listener.assertTopologiesFor(1L, 2L, 3L);
+            listener.assertTopologyForEpoch(1, topology1);
+            listener.assertTopologyForEpoch(2, topology2);
+            listener.assertTopologyForEpoch(3, topology3);
+        }
+        finally
+        {
+            journal.shutdown();
+        }
     }
 
-    @Test
-    public void truncateTest()
+    private static AccordJournal initJournal() throws Throwable
     {
-        ValidatingClusterMetadataService cms = ValidatingClusterMetadataService.createAndRegister(Version.MIN_ACCORD_VERSION);
-        InMemoryJournal journal = new InMemoryJournal(ID1, new TestAgent());
-        AccordConfigurationService service = new AccordConfigurationService(ID1, new Messaging(), new MockFailureDetector(), AccordConfigurationService.SystemTableDiskStateManager.instance, ScheduledExecutors.scheduledTasks);
-        TestListener serviceListener = new TestListener(service, true) {
-            @Override
-            public AsyncResult<Void> onTopologyUpdate(Topology topology, boolean isLoad, boolean startSync)
-            {
-                // Fake journal save
-                journal.saveTopology(new Journal.TopologyUpdate(new Int2ObjectHashMap<>(), topology, topology), () -> {});
-                return super.onTopologyUpdate(topology, isLoad, startSync);
-            }
-        };
-        service.registerListener(serviceListener);
-        service.start();
-
-        Topology topology1 = createTopology(cms);
-        service.updateMapping(mappingForEpoch(cms.metadata().epoch.getEpoch() + 1));
-        service.reportTopology(topology1);
-
-        Topology topology2 = createTopology(cms);
-        service.reportTopology(topology2);
-
-        Topology topology3 = createTopology(cms);
-        service.reportTopology(topology3);
-        service.truncateTopologiesUntil(3);
-        journal.truncateTopologiesForTesting(3);
-        Assert.assertEquals(EpochDiskState.create(3), service.diskState());
-        serviceListener.assertTruncates(3L);
-
-        AccordConfigurationService loaded = new AccordConfigurationService(ID1, new Messaging(), new MockFailureDetector(), AccordConfigurationService.SystemTableDiskStateManager.instance, ScheduledExecutors.scheduledTasks);
-        loaded.updateMapping(mappingForEpoch(cms.metadata().epoch.getEpoch() + 1));
-        TestListener loadListener = new TestListener(loaded, true);
-        loaded.registerListener(loadListener);
-        Iterator<Journal.TopologyUpdate> iter = journal.replayTopologies();
-        // Simulate journal replay
-        while (iter.hasNext())
-            loaded.reportTopology(iter.next().global);
-        loaded.start();
-        loadListener.assertTopologiesFor(3L);
+        File directory = new File(Files.createTempDirectory("config_service_test"));
+        directory.deleteRecursiveOnExit();
+        Keyspace ks = Schema.instance.getKeyspaceInstance("system_accord");
+        ColumnFamilyStore cfs = ks.getColumnFamilyStore(AccordKeyspace.JOURNAL);
+        AccordJournal journal = new AccordJournal(new TestParams(), new AccordAgent(), directory, cfs);
+        journal.start(null);
+        journal.unsafeSetStarted();
+        return journal;
     }
-
     private static Topology createTopology(ValidatingClusterMetadataService cms)
     {
         ClusterMetadata previous = cms.metadata();
diff --git a/test/unit/org/apache/cassandra/service/accord/AccordSyncPropagatorTest.java b/test/unit/org/apache/cassandra/service/accord/AccordSyncPropagatorTest.java
index 08f5b8401f..948dc80afb 100644
--- a/test/unit/org/apache/cassandra/service/accord/AccordSyncPropagatorTest.java
+++ b/test/unit/org/apache/cassandra/service/accord/AccordSyncPropagatorTest.java
@@ -308,7 +308,7 @@ public class AccordSyncPropagatorTest
                         throw new IllegalStateException("Unknown action: " + action);
                 }
                 callbacks.put(message.id(), cb);
-                scheduler.schedule(() -> AccordService.receive(this, node(to).configurationService, (Message<List<AccordSyncPropagator.Notification>>) message.withFrom(mappedEndpoint(from))), 500, TimeUnit.MILLISECONDS);
+                scheduler.schedule(() -> AccordService.receive(this, node(to).configurationService, (Message<AccordSyncPropagator.Notification>) message.withFrom(mappedEndpoint(from))), 500, TimeUnit.MILLISECONDS);
                 scheduler.schedule(() -> {
                     RequestCallback<?> removed = callbacks.remove(message.id());
                     if (removed != null)
@@ -455,6 +455,11 @@ public class AccordSyncPropagatorTest
                 instances.get(localId).propagator.reportRetired(epoch, topology.nodes(), ranges);
             }
 
+            @Override
+            public void reportEpochRemoved(long epoch)
+            {
+            }
+
             @Override
             public void onEndpointAck(Node.Id id, long epoch)
             {
diff --git a/test/unit/org/apache/cassandra/service/accord/EpochSyncTest.java b/test/unit/org/apache/cassandra/service/accord/EpochSyncTest.java
index 6a03368717..68ec5058de 100644
--- a/test/unit/org/apache/cassandra/service/accord/EpochSyncTest.java
+++ b/test/unit/org/apache/cassandra/service/accord/EpochSyncTest.java
@@ -49,7 +49,6 @@ import org.slf4j.LoggerFactory;
 
 import accord.api.ConfigurationService;
 import accord.api.ConfigurationService.EpochReady;
-import accord.api.Journal;
 import accord.impl.DefaultTimeouts;
 import accord.impl.SizeOfIntersectionSorter;
 import accord.impl.TestAgent;
@@ -108,6 +107,7 @@ import org.apache.cassandra.tcm.transformations.PrepareLeave;
 import org.apache.cassandra.utils.ByteArrayUtil;
 import org.apache.cassandra.utils.Pair;
 import org.assertj.core.api.Assertions;
+import org.assertj.core.description.Description;
 
 import static accord.utils.Property.commands;
 import static accord.utils.Property.stateful;
@@ -133,21 +133,22 @@ public class EpochSyncTest
                     cluster.processAll();
                     cluster.validate(true);
                 })
-                .addAllIf(Cluster::hasPendingWork, b ->
-                        b.addIf(c -> !c.status(s -> s == Cluster.Status.Registered).isEmpty(), (rs, state) -> {
-                                    long epoch = state.cms.metadata().epoch.getEpoch() + 1;
-                                    Node.Id pick = rs.pick(state.status(s -> s == Cluster.Status.Registered));
-                                    return new SimpleCommand<>(pick + " Start Joining; epoch=" + epoch,
-                                            c -> c.increment(pick));
-                                })
-                                .addIf(c -> !c.cms.metadata().inProgressSequences.isEmpty(),
-                                        (rs, state) -> new SimpleCommand<>("Next Epoch Step; epoch=" + (state.cms.metadata().epoch.getEpoch() + 1),
-                                                Cluster::incrementInProgressSequences))
-                )
-                .addAllIf(Cluster::hasNoPendingWork, b ->
-                        b.addIf(cluster -> cluster.joined().size() <= cluster.maxNodes, EpochSyncTest::addNode)
-                                .addIf(cluster -> cluster.joined().size() > cluster.minNodes, EpochSyncTest::removeNode)
-                )
+                .addAllIf(Cluster::hasPendingWork, b -> {
+                    b.addIf(c -> !c.status(s -> s == Cluster.Status.Registered).isEmpty(),
+                            (rs, state) -> {
+                                long epoch = state.cms.metadata().epoch.getEpoch() + 1;
+                                Node.Id pick = rs.pick(state.status(s -> s == Cluster.Status.Registered));
+                                return new SimpleCommand<>(String.format("%s Start Joining; epoch=%d", pick, epoch),
+                                                           c -> c.increment(pick));
+                            })
+                     .addIf(c -> !c.cms.metadata().inProgressSequences.isEmpty(),
+                            (rs, state) -> new SimpleCommand<>(String.format("Next Epoch Step; epoch=%d", state.cms.metadata().epoch.getEpoch() + 1),
+                                                               Cluster::incrementInProgressSequences));
+                })
+                .addAllIf(Cluster::hasNoPendingWork, b -> {
+                    b.addIf(cluster -> cluster.joined().size() <= cluster.maxNodes, EpochSyncTest::addNode)
+                     .addIf(cluster -> cluster.joined().size() > cluster.minNodes, EpochSyncTest::removeNode);
+                })
                 .addIf(Cluster::hasWork, EpochSyncTest::processSome)
                 .add(rs -> new SimpleCommand<>("Validate", c -> c.validate(false)))
                 .add((rs, cluster) -> new SimpleCommand<>("Bump Epoch " + (cluster.cms.metadata().epoch.getEpoch() + 1), Cluster::bumpEpoch))
@@ -172,7 +173,7 @@ public class EpochSyncTest
         long epoch = cluster.cms.metadata().epoch.getEpoch() + 1;
         long finalToken = token;
         return new SimpleCommand<>("Start Node " + id + "; token=" + token + ", epoch=" + epoch,
-                                 c -> c.registerNode(id, finalToken));
+                                   c -> c.registerNode(id, finalToken));
     }
 
     private static SimpleCommand<Cluster> removeNode(RandomSource rs, Cluster cluster)
@@ -187,8 +188,7 @@ public class EpochSyncTest
     private static SimpleCommand<Cluster> processSome(RandomSource rs) {
         return new SimpleCommand<>("Process Some",
                 c -> {//noinspection StatementWithEmptyBody
-                    for (int i = 0, attempts = rs.nextInt(1, 100); i < attempts && c.processOne(); i++) {
-                    }
+                    for (int i = 0, attempts = rs.nextInt(1, 100); i < attempts && c.processOne(); i++) {}
                 });
     }
 
@@ -208,6 +208,7 @@ public class EpochSyncTest
         private final ScheduledExecutorPlus scheduler;
         private int nodeCounter = 0;
         private final ValidatingClusterMetadataService cms = ValidatingClusterMetadataService.createAndRegister(NodeVersion.CURRENT_METADATA_VERSION);
+
         private final IFailureDetector fd = new IFailureDetector()
         {
             @Override
@@ -394,7 +395,7 @@ public class EpochSyncTest
                 if (removed.contains(id)) continue; // ignore removed nodes
                 AccordConfigurationService conf = inst.config;
                 TopologyManager tm = inst.topology;
-                for (long epoch = inst.epoch.getEpoch(); epoch <= cms.metadata().epoch.getEpoch(); epoch++)
+                for (long epoch = Math.max(tm.firstNonEmpty(), inst.epoch.getEpoch()); epoch <= cms.metadata().epoch.getEpoch(); epoch++)
                 {
                     // validate config
                     EpochSnapshot snapshot = conf.getEpochSnapshot(epoch);
@@ -425,9 +426,20 @@ public class EpochSyncTest
                         // TopologyManager defines syncComplete for an epoch as (epoch - 1).syncComplete.  This means that an epoch has reached quorum, but will still miss ranges as previous epochs have not
                         if (!ranges.equals(actual) && tm.minEpoch() != epoch && !ranges.equals(tm.syncComplete(epoch - 1).mergeTouching()))
                             continue;
+                        long epoch_ = epoch;
                         Assertions.assertThat(actual)
-                                  .describedAs("node%s does not have all expected sync ranges for epoch %d; missing %s; peers=%s; previous epochs %s", id, epoch, ranges.without(actual), topology.nodes(),
-                                               LongStream.range(inst.epoch.getEpoch(), epoch + 1).mapToObj(e -> e + " -> " + conf.getEpochSnapshot(e).syncStatus + "(synced=" + globalSynced(e) + "): " + tm.syncComplete(e)).collect(Collectors.joining("\n")))
+                                  .describedAs(new Description()
+                                  {
+                                      public String value()
+                                      {
+                                          return String.format("node%s does not have all expected sync ranges for epoch %d; missing %s; peers=%s; previous epochs %s",
+                                                               id, epoch_, ranges.without(actual), topology.nodes(),
+                                                               LongStream.range(inst.epoch.getEpoch(), epoch_ + 1)
+                                                                         .mapToObj(e -> String.format("%d -> %s(synced=%s): %s", e, conf.getEpochSnapshot(e).syncStatus, globalSynced(e), tm.syncComplete(e)))
+                                                                         .collect(Collectors.joining("\n")));
+
+                                      }
+                                  })
                                   .isEqualTo(ranges);
                     }
                 }
@@ -670,22 +682,19 @@ public class EpochSyncTest
                 // TODO (review): Should there be a real scheduler here? Is it possible to adapt the Scheduler interface to scheduler used in this test?
                 TimeService time = TimeService.ofNonMonotonic(globalExecutor::currentTimeMillis, TimeUnit.MILLISECONDS);
                 this.topology = new TopologyManager(SizeOfIntersectionSorter.SUPPLIER, new TestAgent.RethrowAgent(), id, time, new DefaultTimeouts(time));
-                AccordConfigurationService.DiskStateManager instance = MockDiskStateManager.instance;
-                Journal journal = null; // TODO
-                config = new AccordConfigurationService(node, messagingService, failureDetector, instance, scheduler);
+                config = new AccordConfigurationService(node, messagingService, failureDetector, scheduler);
                 config.registerListener(new ConfigurationService.Listener()
                 {
                     @Override
                     public AsyncResult<Void> onTopologyUpdate(Topology topology, boolean isLoad, boolean startSync)
                     {
-//                        EpochReady ready = EpochReady.done(topology.epoch());
                         AsyncResult<Void> metadata = schedule(rs.nextInt(1, 10), TimeUnit.SECONDS, (Callable<Void>) () -> null).beginAsResult();
                         AsyncResult<Void> coordination = metadata.flatMap(ignore -> schedule(rs.nextInt(1, 10), TimeUnit.SECONDS, (Callable<Void>) () -> null)).beginAsResult();
                         AsyncResult<Void> data = coordination.flatMap(ignore -> schedule(rs.nextInt(1, 10), TimeUnit.SECONDS, (Callable<Void>) () -> null)).beginAsResult();
                         AsyncResult<Void> reads = data.flatMap(ignore -> schedule(rs.nextInt(1, 10), TimeUnit.SECONDS, (Callable<Void>) () -> null)).beginAsResult();
                         EpochReady ready = new EpochReady(topology.epoch(), metadata, coordination, data, reads);
 
-                        topology().onTopologyUpdate(topology, () -> ready);
+                        topology().onTopologyUpdate(topology, () -> ready, e -> {});
                         ready.coordinate.addCallback(() -> topology().onEpochSyncComplete(id, topology.epoch()));
                         if (topology().minEpoch() == topology.epoch() && topology().epoch() != topology.epoch())
                             return ready.coordinate;
@@ -700,9 +709,10 @@ public class EpochSyncTest
                     }
 
                     @Override
-                    public void truncateTopologyUntil(long epoch)
+                    public void onRemoveNode(long epoch, Node.Id removed)
                     {
-                        topology.truncateTopologyUntil(epoch);
+                        // TODO
+                        //topology.onRemoveNode(epoch, removed);
                     }
 
                     @Override
@@ -720,7 +730,7 @@ public class EpochSyncTest
 
                 Map<Verb, IVerbHandler<?>> handlers = new EnumMap<>(Verb.class);
                 //noinspection unchecked
-                handlers.put(Verb.ACCORD_SYNC_NOTIFY_REQ, msg -> AccordService.receive(messagingService, config, (Message<List<AccordSyncPropagator.Notification>>) (Message<?>) msg));
+                handlers.put(Verb.ACCORD_SYNC_NOTIFY_REQ, msg -> AccordService.receive(messagingService, config, (Message<AccordSyncPropagator.Notification>) (Message<?>) msg));
                 this.messaging = messagingService;
                 this.reciver = messagingService.receiver(new SimulatedMessageDelivery.SimpleVerbHandler(handlers));
             }
diff --git a/test/unit/org/apache/cassandra/service/accord/FetchMinEpochTest.java b/test/unit/org/apache/cassandra/service/accord/FetchMinEpochTest.java
deleted file mode 100644
index 1c3db5939c..0000000000
--- a/test/unit/org/apache/cassandra/service/accord/FetchMinEpochTest.java
+++ /dev/null
@@ -1,239 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service.accord;
-
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.concurrent.ExecutionException;
-import java.util.function.Supplier;
-import java.util.stream.Collectors;
-import java.util.stream.Stream;
-
-import com.google.common.collect.ImmutableMap;
-import com.google.common.collect.ImmutableSet;
-import org.junit.Assert;
-import org.junit.Test;
-
-import accord.utils.Gen;
-import accord.utils.Gens;
-import accord.utils.RandomSource;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.config.StringRetryStrategy;
-import org.apache.cassandra.dht.Murmur3Partitioner;
-import org.apache.cassandra.io.IVersionedSerializers;
-import org.apache.cassandra.io.util.DataOutputBuffer;
-import org.apache.cassandra.locator.InetAddressAndPort;
-import org.apache.cassandra.net.MessageDelivery;
-import org.apache.cassandra.net.MessagingService;
-import org.apache.cassandra.net.SimulatedMessageDelivery.Action;
-import org.apache.cassandra.service.accord.api.AccordWaitStrategies;
-import org.apache.cassandra.utils.SimulatedMiniCluster;
-import org.apache.cassandra.utils.SimulatedMiniCluster.Node;
-import org.apache.cassandra.utils.concurrent.Future;
-import org.assertj.core.api.Assertions;
-
-import static accord.utils.Property.qt;
-import static org.apache.cassandra.net.MessagingService.Version.VERSION_51;
-import static org.assertj.core.api.Assertions.assertThat;
-
-public class FetchMinEpochTest
-{
-    static
-    {
-        DatabaseDescriptor.clientInitialization();
-        DatabaseDescriptor.setPartitionerUnsafe(Murmur3Partitioner.instance);
-    }
-
-    private static final Gen<Gen<Action>> ACTION_DISTRIBUTION = Gens.enums().allMixedDistribution(Action.class);
-    private static final List<MessagingService.Version> SUPPORTED = Stream.of(MessagingService.Version.values()).filter(v -> v.compareTo(VERSION_51) >= 0).collect(Collectors.toList());
-
-    private static void boundedRetries(int retries)
-    {
-        AccordWaitStrategies.setRetryFetchMinEpoch(new StringRetryStrategy("0<=200ms*2^attempts<=60s,retries=" + retries));
-    }
-
-    @Test
-    public void responseSerde()
-    {
-        Gen<Long> all = Gens.longs().all();
-        Gen<Long> nulls = ignore -> null;
-        Gen<Long> domain = rs -> rs.nextBoolean() ? nulls.next(rs) : all.next(rs);
-        DataOutputBuffer output = new DataOutputBuffer();
-        qt().forAll(domain.map(FetchMinEpoch.Response::new)).check(rsp -> {
-            for (MessagingService.Version version : SUPPORTED)
-                IVersionedSerializers.testSerde(output, FetchMinEpoch.Response.serializer, rsp, version.value);
-        });
-    }
-
-    @Test
-    public void fetchOneNodeAlwaysFails()
-    {
-        int expectedMaxAttempts = 3;
-        int expectedMaxRetries = expectedMaxAttempts - 1;
-        boundedRetries(expectedMaxRetries);
-        qt().check(rs -> {
-            SimulatedMiniCluster cluster = new SimulatedMiniCluster.Builder(rs, node -> msg -> {throw new IllegalStateException();}).build();
-            Node from = cluster.createNodeAndJoin();
-            Node to = cluster.createNodeAndJoin();
-
-            Future<Long> f = FetchMinEpoch.fetch(from, to.broadcastAddressAndPort());
-            assertThat(f).isNotDone();
-            cluster.processAll();
-            assertThat(f).isDone();
-            MessageDelivery.GivingUpException maxRetries = getMaxRetriesException(f);
-            Assertions.assertThat(maxRetries.attempts).isEqualTo(expectedMaxAttempts);
-        });
-    }
-
-    @Test
-    public void fetchOneNode()
-    {
-        int maxRetries = 42;
-        boundedRetries(maxRetries);
-        qt().check(rs -> {
-            long epoch = rs.nextLong(0, Long.MAX_VALUE);
-            SimulatedMiniCluster cluster = new SimulatedMiniCluster.Builder(rs, node -> msg -> node.messaging().respond(new FetchMinEpoch.Response(epoch), msg)).build();
-            Node from = cluster.createNodeAndJoin();
-            {
-                Supplier<Action> safeActionGen = actionGen(rs, maxRetries);
-                from.messagingActions((self, msg, to) -> safeActionGen.get());
-            }
-            Node to = cluster.createNodeAndJoin();
-
-            Future<Long> f = FetchMinEpoch.fetch(from, to.broadcastAddressAndPort());
-            assertThat(f).isNotDone();
-            cluster.processAll();
-            assertThat(f).isDone();
-            assertThat(f.get()).isEqualTo(epoch);
-        });
-    }
-
-    @Test
-    public void fetchManyNodesAllNodesFail()
-    {
-        int expectedMaxAttempts = 3;
-        boundedRetries(expectedMaxAttempts);
-        qt().check(rs -> {
-            SimulatedMiniCluster cluster = new SimulatedMiniCluster.Builder(rs, node -> msg -> {throw new IllegalStateException();}).build();
-
-            Node from = cluster.createNodeAndJoin();
-            Node to1 = cluster.createNodeAndJoin();
-            Node to2 = cluster.createNodeAndJoin();
-            Node to3 = cluster.createNodeAndJoin();
-            Node to4 = cluster.createNodeAndJoin();
-
-            Future<Long> f = FetchMinEpoch.fetch(from, ImmutableSet.of(to1.broadcastAddressAndPort(),
-                                                                       to2.broadcastAddressAndPort(),
-                                                                       to3.broadcastAddressAndPort(),
-                                                                       to4.broadcastAddressAndPort()));
-            assertThat(f).isNotDone();
-            cluster.processAll();
-            assertThat(f).isDone();
-            assertThat(f.get()).isNull();
-        });
-    }
-
-    @Test
-    public void fetchManyNodes()
-    {
-        boundedRetries(Integer.MAX_VALUE); // networking should be unbounded, but the actions should be bounded
-        int maxRetries = 3;
-        qt().check(rs -> {
-            Map<Integer, Long> nodeToEpoch = new HashMap<>();
-            Long min = null;
-            for (int i = 2; i < 6; i++)
-            {
-                Long epoch = rs.nextBoolean() ? null : rs.nextLong();
-                nodeToEpoch.put(i, epoch);
-                if (min == null)        min = epoch;
-                else if (epoch != null) min = Math.min(min, epoch);
-            }
-
-            SimulatedMiniCluster cluster = new SimulatedMiniCluster.Builder(rs, node -> msg -> node.messaging().respond(new FetchMinEpoch.Response(nodeToEpoch.get(node.id().id())), msg)).build();
-
-            Node from = cluster.createNodeAndJoin();
-            Node to1 = cluster.createNodeAndJoin();
-            Node to2 = cluster.createNodeAndJoin();
-            Node to3 = cluster.createNodeAndJoin();
-            Node to4 = cluster.createNodeAndJoin();
-            Map<InetAddressAndPort, Supplier<Action>> nodeToActions = ImmutableMap.of(to1.broadcastAddressAndPort(), actionGen(rs, maxRetries),
-                                                                                      to2.broadcastAddressAndPort(), actionGen(rs, maxRetries),
-                                                                                      to3.broadcastAddressAndPort(), actionGen(rs, maxRetries),
-                                                                                      to4.broadcastAddressAndPort(), actionGen(rs, maxRetries));
-            from.messagingActions((self, msg, to) -> nodeToActions.get(to).get());
-
-            Future<Long> f = FetchMinEpoch.fetch(from, ImmutableSet.of(to1.broadcastAddressAndPort(),
-                                                                       to2.broadcastAddressAndPort(),
-                                                                       to3.broadcastAddressAndPort(),
-                                                                       to4.broadcastAddressAndPort()));
-            assertThat(f).isNotDone();
-            cluster.processAll();
-            assertThat(f).isDone();
-            assertThat(f.get()).isEqualTo(min);
-        });
-    }
-
-    private static Supplier<Action> actionGen(RandomSource rs, int maxRetries)
-    {
-        RandomSource actionSource = rs.fork();
-        Gen<Action> actionGen = ACTION_DISTRIBUTION.next(actionSource);
-        // it is very possible that DELIVER is very rare, which will cause the test to run for a long time and could fail in CI,
-        // when a long history of non-DELIVER is seen, start to force DELIVER to bound the amount of processing in the test
-        Gen<Action> safeActionGen = new Gen<>()
-        {
-            private int notDelivers = 0;
-            @Override
-            public Action next(RandomSource rng)
-            {
-                if (notDelivers > maxRetries - 1)
-                    return Action.DELIVER;
-                Action action = actionGen.next(rng);
-                if (action == Action.DELIVER) notDelivers = 0;
-                else notDelivers++;
-                return action;
-            }
-        };
-        return safeActionGen.asSupplier(actionSource);
-    }
-
-
-    private static MessageDelivery.GivingUpException getMaxRetriesException(Future<Long> f) throws InterruptedException, ExecutionException
-    {
-        MessageDelivery.GivingUpException maxRetries;
-        try
-        {
-            f.get();
-            Assert.fail("Future should have failed");
-            throw new AssertionError("Unreachable");
-        }
-        catch (ExecutionException e)
-        {
-            if (e.getCause() instanceof MessageDelivery.GivingUpException)
-            {
-                maxRetries = (MessageDelivery.GivingUpException) e.getCause();
-            }
-            else
-            {
-                throw e;
-            }
-        }
-        return maxRetries;
-    }
-}
\ No newline at end of file
diff --git a/test/unit/org/apache/cassandra/service/accord/LoggingDiskStateManager.java b/test/unit/org/apache/cassandra/service/accord/LoggingDiskStateManager.java
deleted file mode 100644
index 5c8f24de9a..0000000000
--- a/test/unit/org/apache/cassandra/service/accord/LoggingDiskStateManager.java
+++ /dev/null
@@ -1,101 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service.accord;
-
-import accord.local.Node;
-import accord.primitives.Ranges;
-import accord.topology.Topology;
-import com.google.common.annotations.VisibleForTesting;
-import org.slf4j.Logger;
-import org.slf4j.LoggerFactory;
-
-import java.util.Set;
-
-/**
- * When trying to inspect the order in which disk state is modified, this class can aid by adding logging.  This class
- * mostly exists for testing to aid in debugging.
- */
-@SuppressWarnings("unused")
-@VisibleForTesting
-public class LoggingDiskStateManager implements AccordConfigurationService.DiskStateManager
-{
-    private static final Logger logger = LoggerFactory.getLogger(LoggingDiskStateManager.class);
-    private final Node.Id self;
-    private final AccordConfigurationService.DiskStateManager delegate;
-
-    public LoggingDiskStateManager(Node.Id self, AccordConfigurationService.DiskStateManager delegate) {
-        this.self = self;
-        this.delegate = delegate;
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState loadLocalTopologyState(AccordKeyspace.TopologyLoadConsumer consumer) {
-        logger.info("[node={}] Calling loadTopologies()", self);
-        return delegate.loadLocalTopologyState(consumer);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState setNotifyingLocalSync(long epoch, Set<Node.Id> pending, AccordKeyspace.EpochDiskState diskState) {
-        logger.info("[node={}] Calling setNotifyingLocalSync({}, {}, {})", self, epoch, pending, diskState);
-        return delegate.setNotifyingLocalSync(epoch, pending, diskState);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState setCompletedLocalSync(long epoch, AccordKeyspace.EpochDiskState diskState) {
-        logger.info("[node={}] Calling setCompletedLocalSync({}, {})", self, epoch, diskState);
-        return delegate.setCompletedLocalSync(epoch, diskState);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState markLocalSyncAck(Node.Id id, long epoch, AccordKeyspace.EpochDiskState diskState) {
-        logger.info("[node={}] Calling markLocalSyncAck({}, {}, {})", self, id, epoch, diskState);
-        return delegate.markLocalSyncAck(id, epoch, diskState);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState saveTopology(Topology topology, AccordKeyspace.EpochDiskState diskState) {
-        logger.info("[node={}] Calling saveTopology({}, {})", self, topology.epoch(), diskState);
-        return delegate.saveTopology(topology, diskState);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState markRemoteTopologySync(Node.Id id, long epoch, AccordKeyspace.EpochDiskState diskState) {
-        logger.info("[node={}] Calling markRemoteTopologySync({}, {}, {})", self, id, epoch, diskState);
-        return delegate.markRemoteTopologySync(id, epoch, diskState);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState markClosed(Ranges ranges, long epoch, AccordKeyspace.EpochDiskState diskState) {
-        logger.info("[node={}] Calling markClosed({}, {}, {})", self, ranges, epoch, diskState);
-        return delegate.markClosed(ranges, epoch, diskState);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState markRetired(Ranges ranges, long epoch, AccordKeyspace.EpochDiskState diskState)
-    {
-        logger.info("[node={}] Calling markRetired({}, {}, {})", self, ranges, epoch, diskState);
-        return delegate.markRetired(ranges, epoch, diskState);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState truncateTopologyUntil(long epoch, AccordKeyspace.EpochDiskState diskState) {
-        logger.info("[node={}] Calling truncateTopologyUntil({}, {})", self, epoch, diskState);
-        return delegate.truncateTopologyUntil(epoch, diskState);
-    }
-}
diff --git a/test/unit/org/apache/cassandra/service/accord/MockDiskStateManager.java b/test/unit/org/apache/cassandra/service/accord/MockDiskStateManager.java
deleted file mode 100644
index c3b9bbf433..0000000000
--- a/test/unit/org/apache/cassandra/service/accord/MockDiskStateManager.java
+++ /dev/null
@@ -1,95 +0,0 @@
-/*
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.service.accord;
-
-import accord.local.Node;
-import accord.primitives.Ranges;
-import accord.topology.Topology;
-import accord.utils.Invariants;
-
-import java.util.Set;
-
-public enum MockDiskStateManager implements AccordConfigurationService.DiskStateManager
-{
-    instance;
-
-    @Override
-    public AccordKeyspace.EpochDiskState loadLocalTopologyState(AccordKeyspace.TopologyLoadConsumer consumer)
-    {
-        return AccordKeyspace.EpochDiskState.EMPTY;
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState setNotifyingLocalSync(long epoch, Set<Node.Id> pending, AccordKeyspace.EpochDiskState diskState)
-    {
-        return maybeUpdateMaxEpoch(diskState, epoch);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState setCompletedLocalSync(long epoch, AccordKeyspace.EpochDiskState diskState)
-    {
-        return maybeUpdateMaxEpoch(diskState, epoch);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState markLocalSyncAck(Node.Id id, long epoch, AccordKeyspace.EpochDiskState diskState)
-    {
-        return maybeUpdateMaxEpoch(diskState, epoch);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState saveTopology(Topology topology, AccordKeyspace.EpochDiskState diskState)
-    {
-        return maybeUpdateMaxEpoch(diskState, topology.epoch());
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState markRemoteTopologySync(Node.Id node, long epoch, AccordKeyspace.EpochDiskState diskState)
-    {
-        return maybeUpdateMaxEpoch(diskState, epoch);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState markClosed(Ranges ranges, long epoch, AccordKeyspace.EpochDiskState diskState)
-    {
-        return maybeUpdateMaxEpoch(diskState, epoch);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState markRetired(Ranges ranges, long epoch, AccordKeyspace.EpochDiskState diskState)
-    {
-        return maybeUpdateMaxEpoch(diskState, epoch);
-    }
-
-    @Override
-    public AccordKeyspace.EpochDiskState truncateTopologyUntil(long epoch, AccordKeyspace.EpochDiskState diskState)
-    {
-        return maybeUpdateMaxEpoch(diskState, epoch);
-    }
-
-    private static AccordKeyspace.EpochDiskState maybeUpdateMaxEpoch(AccordKeyspace.EpochDiskState diskState, long epoch)
-    {
-        if (diskState.isEmpty())
-            return AccordKeyspace.EpochDiskState.create(epoch);
-        Invariants.requireArgument(epoch >= diskState.minEpoch, "Epoch %d < %d (min)", epoch, diskState.minEpoch);
-        if (epoch > diskState.maxEpoch)
-            diskState = diskState.withNewMaxEpoch(epoch);
-        return diskState;
-    }
-}
diff --git a/test/unit/org/apache/cassandra/service/accord/SimulatedAccordCommandStore.java b/test/unit/org/apache/cassandra/service/accord/SimulatedAccordCommandStore.java
index 9fb2c7173b..d8c09544db 100644
--- a/test/unit/org/apache/cassandra/service/accord/SimulatedAccordCommandStore.java
+++ b/test/unit/org/apache/cassandra/service/accord/SimulatedAccordCommandStore.java
@@ -62,6 +62,7 @@ import accord.messages.Reply;
 import accord.messages.TxnRequest;
 import accord.primitives.AbstractUnseekableKeys;
 import accord.primitives.Ballot;
+import accord.primitives.EpochSupplier;
 import accord.primitives.FullRoute;
 import accord.primitives.Ranges;
 import accord.primitives.Routable;
@@ -483,9 +484,9 @@ public class SimulatedAccordCommandStore implements AutoCloseable
         }
 
         @Override
-        public void purge(CommandStores commandStores)
+        public void purge(CommandStores commandStores, EpochSupplier epochSupplier)
         {
-            super.purge(commandStores);
+            super.purge(commandStores, epochSupplier);
             index.truncateForTesting();
         }
 
