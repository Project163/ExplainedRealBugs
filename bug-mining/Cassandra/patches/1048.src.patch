diff --git a/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java b/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java
index c3431a93d9..f3aa9506c8 100644
--- a/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java
+++ b/src/java/org/apache/cassandra/io/compress/CompressedRandomAccessReader.java
@@ -135,7 +135,7 @@ public class CompressedRandomAccessReader extends RandomAccessReader
     {
         super(new File(dataFilePath), metadata.chunkLength, skipIOCache);
         this.metadata = metadata;
-        compressed = new byte[metadata.chunkLength];
+        compressed = new byte[Snappy.maxCompressedLength(metadata.chunkLength)];
         // can't use super.read(...) methods
         // that is why we are allocating special InputStream to read data from disk
         // from already open file descriptor
diff --git a/src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java b/src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java
index a262a25638..64907a8be7 100644
--- a/src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java
+++ b/src/java/org/apache/cassandra/io/compress/CompressedSequentialWriter.java
@@ -53,7 +53,7 @@ public class CompressedSequentialWriter extends SequentialWriter
         super(file, CHUNK_LENGTH, skipIOCache);
 
         // buffer for compression should be the same size as buffer itself
-        compressed = new byte[buffer.length];
+        compressed = new byte[Snappy.maxCompressedLength(buffer.length)];
 
         /* Index File (-CompressionInfo.db component) and it's header */
         metadataWriter = new CompressionMetadata.Writer(indexFilePath);
@@ -85,6 +85,7 @@ public class CompressedSequentialWriter extends SequentialWriter
         chunkCount++;
 
         // write data itself
+        assert compressedLength <= compressed.length;
         out.write(compressed, 0, compressedLength);
 
         // next chunk should be written right after current
