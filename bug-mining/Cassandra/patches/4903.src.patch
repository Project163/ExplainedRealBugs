diff --git a/CHANGES.txt b/CHANGES.txt
index 9cba02b633..7745e8cfdc 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -1,4 +1,5 @@
 3.0.15
+ * Fix support for SuperColumn tables (CASSANDRA-12373)
  * Handle limit correctly on tables with strict liveness (CASSANDRA-13883)
  * Fix missing original update in TriggerExecutor (CASSANDRA-13894)
  * Remove non-rpc-ready nodes from counter leader candidates (CASSANDRA-13043)
diff --git a/src/java/org/apache/cassandra/config/CFMetaData.java b/src/java/org/apache/cassandra/config/CFMetaData.java
index 1eb991a1fa..fd1c9e5164 100644
--- a/src/java/org/apache/cassandra/config/CFMetaData.java
+++ b/src/java/org/apache/cassandra/config/CFMetaData.java
@@ -31,6 +31,8 @@ import com.google.common.base.MoreObjects;
 import com.google.common.base.Objects;
 import com.google.common.collect.ImmutableSet;
 import com.google.common.collect.Iterables;
+import com.google.common.collect.Iterators;
+import com.google.common.collect.Maps;
 import com.google.common.collect.Sets;
 import org.apache.commons.lang3.ArrayUtils;
 import org.apache.commons.lang3.builder.HashCodeBuilder;
@@ -40,6 +42,7 @@ import org.slf4j.LoggerFactory;
 
 import org.apache.cassandra.cql3.ColumnIdentifier;
 import org.apache.cassandra.cql3.QueryProcessor;
+import org.apache.cassandra.cql3.SuperColumnCompatibility;
 import org.apache.cassandra.cql3.statements.CFStatement;
 import org.apache.cassandra.cql3.statements.CreateTableStatement;
 import org.apache.cassandra.db.*;
@@ -114,6 +117,33 @@ public final class CFMetaData
     // for those tables in practice).
     private volatile ColumnDefinition compactValueColumn;
 
+    /**
+     * These two columns are "virtual" (e.g. not persisted together with schema).
+     *
+     * They are stored here to avoid re-creating during SELECT and UPDATE queries, where
+     * they are used to allow presenting supercolumn families in the CQL-compatible
+     * format. See {@link SuperColumnCompatibility} for more details.
+     **/
+    private volatile ColumnDefinition superCfKeyColumn;
+    private volatile ColumnDefinition superCfValueColumn;
+
+    public boolean isSuperColumnKeyColumn(ColumnDefinition cd)
+    {
+        return cd.name.equals(superCfKeyColumn.name);
+    }
+
+    public boolean isSuperColumnValueColumn(ColumnDefinition cd)
+    {
+        return cd.name.equals(superCfValueColumn.name);
+    }
+
+    public ColumnDefinition superColumnValueColumn()
+    {
+        return superCfValueColumn;
+    }
+
+    public ColumnDefinition superColumnKeyColumn() { return superCfKeyColumn; }
+
     /*
      * All of these methods will go away once CFMetaData becomes completely immutable.
      */
@@ -242,7 +272,9 @@ public final class CFMetaData
                        List<ColumnDefinition> partitionKeyColumns,
                        List<ColumnDefinition> clusteringColumns,
                        PartitionColumns partitionColumns,
-                       IPartitioner partitioner)
+                       IPartitioner partitioner,
+                       ColumnDefinition superCfKeyColumn,
+                       ColumnDefinition superCfValueColumn)
     {
         this.cfId = cfId;
         this.ksName = keyspace;
@@ -253,7 +285,8 @@ public final class CFMetaData
         ksAndCFBytes = Arrays.copyOf(ksBytes, ksBytes.length + cfBytes.length);
         System.arraycopy(cfBytes, 0, ksAndCFBytes, ksBytes.length, cfBytes.length);
 
-        this.isDense = isDense;
+        this.isDense = isSuper ? (isDense || SuperColumnCompatibility.recalculateIsDense(partitionColumns.regulars)) : isDense;
+
         this.isCompound = isCompound;
         this.isSuper = isSuper;
         this.isCounter = isCounter;
@@ -283,32 +316,64 @@ public final class CFMetaData
         this.clusteringColumns = clusteringColumns;
         this.partitionColumns = partitionColumns;
 
-        this.serializers = new Serializers(this);
+        this.superCfKeyColumn = superCfKeyColumn;
+        this.superCfValueColumn = superCfValueColumn;
 
+        //This needs to happen before serializers are set
+        //because they use comparator.subtypes()
         rebuild();
+
+        this.serializers = new Serializers(this);
     }
 
     // This rebuild informations that are intrinsically duplicate of the table definition but
     // are kept because they are often useful in a different format.
     private void rebuild()
     {
-        this.comparator = new ClusteringComparator(extractTypes(clusteringColumns));
+        if (isCompactTable())
+        {
+            this.compactValueColumn = isSuper() ?
+                                      SuperColumnCompatibility.getCompactValueColumn(partitionColumns) :
+                                      CompactTables.getCompactValueColumn(partitionColumns);
+        }
 
-        Map<ByteBuffer, ColumnDefinition> newColumnMetadata = new HashMap<>();
-        for (ColumnDefinition def : partitionKeyColumns)
-            newColumnMetadata.put(def.name.bytes, def);
-        for (ColumnDefinition def : clusteringColumns)
-            newColumnMetadata.put(def.name.bytes, def);
-        for (ColumnDefinition def : partitionColumns)
-            newColumnMetadata.put(def.name.bytes, def);
+        Map<ByteBuffer, ColumnDefinition> newColumnMetadata = Maps.newHashMapWithExpectedSize(partitionKeyColumns.size() + clusteringColumns.size() + partitionColumns.size());
 
+        if (isSuper() && isDense())
+        {
+            CompactTables.DefaultNames defaultNames = SuperColumnCompatibility.columnNameGenerator(partitionKeyColumns, clusteringColumns, partitionColumns);
+            if (superCfKeyColumn == null)
+                superCfKeyColumn = SuperColumnCompatibility.getSuperCfKeyColumn(this, clusteringColumns, defaultNames);
+            if (superCfValueColumn == null)
+                superCfValueColumn = SuperColumnCompatibility.getSuperCfValueColumn(this, partitionColumns, superCfKeyColumn, defaultNames);
+
+            for (ColumnDefinition def : partitionKeyColumns)
+                newColumnMetadata.put(def.name.bytes, def);
+            newColumnMetadata.put(clusteringColumns.get(0).name.bytes, clusteringColumns.get(0));
+            newColumnMetadata.put(superCfKeyColumn.name.bytes, SuperColumnCompatibility.getSuperCfSschemaRepresentation(superCfKeyColumn));
+            newColumnMetadata.put(superCfValueColumn.name.bytes, superCfValueColumn);
+            newColumnMetadata.put(compactValueColumn.name.bytes, compactValueColumn);
+            clusteringColumns = Arrays.asList(clusteringColumns().get(0));
+            partitionColumns = PartitionColumns.of(compactValueColumn);
+        }
+        else
+        {
+            for (ColumnDefinition def : partitionKeyColumns)
+                newColumnMetadata.put(def.name.bytes, def);
+            for (ColumnDefinition def : clusteringColumns)
+                newColumnMetadata.put(def.name.bytes, def);
+            for (ColumnDefinition def : partitionColumns)
+                newColumnMetadata.put(def.name.bytes, def);
+        }
         this.columnMetadata = newColumnMetadata;
 
         List<AbstractType<?>> keyTypes = extractTypes(partitionKeyColumns);
         this.keyValidator = keyTypes.size() == 1 ? keyTypes.get(0) : CompositeType.getInstance(keyTypes);
 
-        if (isCompactTable())
-            this.compactValueColumn = CompactTables.getCompactValueColumn(partitionColumns, isSuper());
+        if (isSuper())
+            this.comparator = new ClusteringComparator(clusteringColumns.get(0).type);
+        else
+            this.comparator = new ClusteringComparator(extractTypes(clusteringColumns));
     }
 
     public Indexes getIndexes()
@@ -361,7 +426,9 @@ public final class CFMetaData
                               partitions,
                               clusterings,
                               builder.build(),
-                              partitioner);
+                              partitioner,
+                              null,
+                              null);
     }
 
     private static List<AbstractType<?>> extractTypes(List<ColumnDefinition> clusteringColumns)
@@ -462,7 +529,9 @@ public final class CFMetaData
                                        copy(partitionKeyColumns),
                                        copy(clusteringColumns),
                                        copy(partitionColumns),
-                                       partitioner),
+                                       partitioner,
+                                       superCfKeyColumn,
+                                       superCfValueColumn),
                         this);
     }
 
@@ -479,7 +548,9 @@ public final class CFMetaData
                                        copy(partitionKeyColumns),
                                        copy(clusteringColumns),
                                        copy(partitionColumns),
-                                       partitioner),
+                                       partitioner,
+                                       superCfKeyColumn,
+                                       superCfValueColumn),
                         this);
     }
 
@@ -578,22 +649,39 @@ public final class CFMetaData
         return columnMetadata.values();
     }
 
+    private Iterator<ColumnDefinition> nonPkColumnIterator()
+    {
+        final boolean noNonPkColumns = isCompactTable() && CompactTables.hasEmptyCompactValue(this) && !isSuper();
+        if (noNonPkColumns)
+        {
+            return Collections.<ColumnDefinition>emptyIterator();
+        }
+        else if (isStaticCompactTable())
+        {
+            return partitionColumns.statics.selectOrderIterator();
+        }
+        else if (isSuper())
+        {
+            if (isDense)
+                return Iterators.forArray(superCfKeyColumn, superCfValueColumn);
+            else
+                return Iterators.filter(partitionColumns.iterator(), (c) -> !c.type.isCollection());
+        }
+        else
+            return partitionColumns().selectOrderIterator();
+    }
+
     // An iterator over all column definitions but that respect the order of a SELECT *.
-    // This also "hide" the clustering/regular columns for a non-CQL3 non-dense table for backward compatibility
-    // sake (those are accessible through thrift but not through CQL currently).
+    // This also hides the clustering/regular columns for a non-CQL3 non-dense table for backward compatibility
+    // sake (those are accessible through thrift but not through CQL currently) and exposes the key and value
+    // columns for supercolumn family.
     public Iterator<ColumnDefinition> allColumnsInSelectOrder()
     {
-        final boolean isStaticCompactTable = isStaticCompactTable();
-        final boolean noNonPkColumns = isCompactTable() && CompactTables.hasEmptyCompactValue(this);
         return new AbstractIterator<ColumnDefinition>()
         {
             private final Iterator<ColumnDefinition> partitionKeyIter = partitionKeyColumns.iterator();
-            private final Iterator<ColumnDefinition> clusteringIter = isStaticCompactTable ? Collections.<ColumnDefinition>emptyIterator() : clusteringColumns.iterator();
-            private final Iterator<ColumnDefinition> otherColumns = noNonPkColumns
-                                                                  ? Collections.<ColumnDefinition>emptyIterator()
-                                                                  : (isStaticCompactTable
-                                                                     ?  partitionColumns.statics.selectOrderIterator()
-                                                                     :  partitionColumns.selectOrderIterator());
+            private final Iterator<ColumnDefinition> clusteringIter = isStaticCompactTable() ? Collections.<ColumnDefinition>emptyIterator() : clusteringColumns.iterator();
+            private final Iterator<ColumnDefinition> otherColumns = nonPkColumnIterator();
 
             protected ColumnDefinition computeNext()
             {
@@ -751,6 +839,8 @@ public final class CFMetaData
 
         boolean changeAffectsStatements = !partitionColumns.equals(cfm.partitionColumns);
         partitionColumns = cfm.partitionColumns;
+        superCfKeyColumn = cfm.superCfKeyColumn;
+        superCfValueColumn = cfm.superCfValueColumn;
 
         rebuild();
 
@@ -784,8 +874,12 @@ public final class CFMetaData
         if (!cfm.cfId.equals(cfId))
             throw new ConfigurationException(String.format("Column family ID mismatch (found %s; expected %s)",
                                                            cfm.cfId, cfId));
-        if (!cfm.flags.equals(flags))
-            throw new ConfigurationException("types do not match.");
+
+        // Dense flag can get set, see CASSANDRA-12373 for details. We have to remove flag from both parts because
+        // there's no guaranteed call order in the call.
+
+        if (!cfm.flags.equals(flags) && (!isSuper() || !Sets.difference(cfm.flags, Sets.immutableEnumSet(Flag.DENSE)).equals(Sets.difference(flags, Sets.immutableEnumSet(Flag.DENSE)))))
+            throw new ConfigurationException("Types do not match: " + cfm.flags + " != " + flags);
     }
 
 
@@ -819,7 +913,7 @@ public final class CFMetaData
      */
     public ColumnDefinition getColumnDefinition(ColumnIdentifier name)
     {
-        return columnMetadata.get(name.bytes);
+       return getColumnDefinition(name.bytes);
     }
 
     // In general it is preferable to work with ColumnIdentifier to make it
@@ -859,8 +953,8 @@ public final class CFMetaData
         if (isCounter())
         {
             for (ColumnDefinition def : partitionColumns())
-                if (!(def.type instanceof CounterColumnType) && !CompactTables.isSuperColumnMapColumn(def))
-                    throw new ConfigurationException("Cannot add a non counter column (" + def.name + ") in a counter column family");
+                if (!(def.type instanceof CounterColumnType) && (!isSuper() || isSuperColumnValueColumn(def)))
+                    throw new ConfigurationException("Cannot add a non counter column (" + def + ") in a counter column family");
         }
         else
         {
@@ -874,6 +968,7 @@ public final class CFMetaData
 
         // initialize a set of names NOT in the CF under consideration
         KeyspaceMetadata ksm = Schema.instance.getKSMetaData(ksName);
+
         Set<String> indexNames = ksm == null ? new HashSet<>() : ksm.existingIndexNames(cfName);
         for (IndexMetadata index : indexes)
         {
@@ -888,8 +983,6 @@ public final class CFMetaData
         return this;
     }
 
-
-
     // The comparator to validate the definition name with thrift.
     public AbstractType<?> thriftColumnNameType()
     {
@@ -963,13 +1056,14 @@ public final class CFMetaData
     public void renameColumn(ColumnIdentifier from, ColumnIdentifier to) throws InvalidRequestException
     {
         ColumnDefinition def = getColumnDefinition(from);
+
         if (def == null)
             throw new InvalidRequestException(String.format("Cannot rename unknown column %s in keyspace %s", from, cfName));
 
         if (getColumnDefinition(to) != null)
             throw new InvalidRequestException(String.format("Cannot rename column %s to %s in keyspace %s; another column of that name already exist", from, to, cfName));
 
-        if (def.isPartOfCellName(isCQLTable(), isSuper()))
+        if (def.isPartOfCellName(isCQLTable(), isSuper()) && !isDense())
         {
             throw new InvalidRequestException(String.format("Cannot rename non PRIMARY KEY part %s", from));
         }
@@ -987,8 +1081,28 @@ public final class CFMetaData
                                                                                 .collect(Collectors.joining(","))));
         }
 
-        ColumnDefinition newDef = def.withNewName(to);
-        addOrReplaceColumnDefinition(newDef);
+        if (isSuper() && isDense())
+        {
+            if (isSuperColumnKeyColumn(def))
+            {
+                columnMetadata.remove(superCfKeyColumn.name.bytes);
+                superCfKeyColumn = superCfKeyColumn.withNewName(to);
+                columnMetadata.put(superCfKeyColumn.name.bytes, SuperColumnCompatibility.getSuperCfSschemaRepresentation(superCfKeyColumn));
+            }
+            else if (isSuperColumnValueColumn(def))
+            {
+                columnMetadata.remove(superCfValueColumn.name.bytes);
+                superCfValueColumn = superCfValueColumn.withNewName(to);
+                columnMetadata.put(superCfValueColumn.name.bytes, superCfValueColumn);
+            }
+            else
+                addOrReplaceColumnDefinition(def.withNewName(to));
+        }
+        else
+        {
+            addOrReplaceColumnDefinition(def.withNewName(to));
+        }
+
 
         // removeColumnDefinition doesn't work for partition key (expectedly) but renaming one is fine so we still
         // want to update columnMetadata.
@@ -1098,9 +1212,12 @@ public final class CFMetaData
 
     public AbstractType<?> makeLegacyDefaultValidator()
     {
-        return isCounter()
-             ? CounterColumnType.instance
-             : (isCompactTable() ? compactValueColumn().type : BytesType.instance);
+        if (isCounter())
+            return CounterColumnType.instance;
+        else if (isCompactTable())
+            return isSuper() ? ((MapType)compactValueColumn().type).valueComparator() : compactValueColumn().type;
+        else
+            return BytesType.instance;
     }
 
     public static Set<Flag> flagsFromStrings(Set<String> strings)
@@ -1198,7 +1315,7 @@ public final class CFMetaData
 
         public static Builder createSuper(String keyspace, String table, boolean isCounter)
         {
-            return create(keyspace, table, false, false, true, isCounter);
+            return create(keyspace, table, true, true, true, isCounter);
         }
 
         public Builder withPartitioner(IPartitioner partitioner)
@@ -1314,7 +1431,9 @@ public final class CFMetaData
                                   partitions,
                                   clusterings,
                                   builder.build(),
-                                  partitioner.orElseGet(DatabaseDescriptor::getPartitioner));
+                                  partitioner.orElseGet(DatabaseDescriptor::getPartitioner),
+                                  null,
+                                  null);
         }
     }
 
diff --git a/src/java/org/apache/cassandra/cql3/AbstractMarker.java b/src/java/org/apache/cassandra/cql3/AbstractMarker.java
index cd26bd793d..14170b1be0 100644
--- a/src/java/org/apache/cassandra/cql3/AbstractMarker.java
+++ b/src/java/org/apache/cassandra/cql3/AbstractMarker.java
@@ -17,7 +17,6 @@
  */
 package org.apache.cassandra.cql3;
 
-import java.util.Collections;
 import java.util.List;
 
 import org.apache.cassandra.cql3.functions.Function;
@@ -58,7 +57,7 @@ public abstract class AbstractMarker extends Term.NonTerminal
      */
     public static class Raw extends Term.Raw
     {
-        protected final int bindIndex;
+        private final int bindIndex;
 
         public Raw(int bindIndex)
         {
@@ -89,6 +88,11 @@ public abstract class AbstractMarker extends Term.NonTerminal
         {
             return "?";
         }
+
+        public int bindIndex()
+        {
+            return bindIndex;
+        }
     }
 
     /** A MultiColumnRaw version of AbstractMarker.Raw */
@@ -140,7 +144,7 @@ public abstract class AbstractMarker extends Term.NonTerminal
         @Override
         public AbstractMarker prepare(String keyspace, ColumnSpecification receiver) throws InvalidRequestException
         {
-            return new Lists.Marker(bindIndex, makeInReceiver(receiver));
+            return new Lists.Marker(bindIndex(), makeInReceiver(receiver));
         }
     }
 }
diff --git a/src/java/org/apache/cassandra/cql3/ColumnCondition.java b/src/java/org/apache/cassandra/cql3/ColumnCondition.java
index 60e67f3a05..99e243c056 100644
--- a/src/java/org/apache/cassandra/cql3/ColumnCondition.java
+++ b/src/java/org/apache/cassandra/cql3/ColumnCondition.java
@@ -60,6 +60,12 @@ public class ColumnCondition
             assert this.inValues == null;
     }
 
+    // Public for SuperColumn tables support only
+    public Term value()
+    {
+        return value;
+    }
+
     public static ColumnCondition condition(ColumnDefinition column, Term value, Operator op)
     {
         return new ColumnCondition(column, null, value, null, op);
diff --git a/src/java/org/apache/cassandra/cql3/ColumnConditions.java b/src/java/org/apache/cassandra/cql3/ColumnConditions.java
index cb09b1a273..5ec4cb495f 100644
--- a/src/java/org/apache/cassandra/cql3/ColumnConditions.java
+++ b/src/java/org/apache/cassandra/cql3/ColumnConditions.java
@@ -104,6 +104,12 @@ public final class ColumnConditions extends AbstractConditions
         staticConditions.forEach(p -> p.addFunctionsTo(functions));
     }
 
+    // Public for SuperColumn tables support only
+    public Collection<ColumnCondition> columnConditions()
+    {
+        return this.columnConditions;
+    }
+
     /**
      * Creates a new <code>Builder</code> for <code>ColumnConditions</code>.
      * @return a new <code>Builder</code> for <code>ColumnConditions</code>
diff --git a/src/java/org/apache/cassandra/cql3/Constants.java b/src/java/org/apache/cassandra/cql3/Constants.java
index a2bacdf014..f37d9001c8 100644
--- a/src/java/org/apache/cassandra/cql3/Constants.java
+++ b/src/java/org/apache/cassandra/cql3/Constants.java
@@ -281,7 +281,8 @@ public abstract class Constants
 
     public static class Marker extends AbstractMarker
     {
-        protected Marker(int bindIndex, ColumnSpecification receiver)
+        // Constructor is public only for the SuperColumn tables support
+        public Marker(int bindIndex, ColumnSpecification receiver)
         {
             super(bindIndex, receiver);
             assert !receiver.type.isCollection();
diff --git a/src/java/org/apache/cassandra/cql3/Maps.java b/src/java/org/apache/cassandra/cql3/Maps.java
index 4772369b6d..4b6f0fe5d5 100644
--- a/src/java/org/apache/cassandra/cql3/Maps.java
+++ b/src/java/org/apache/cassandra/cql3/Maps.java
@@ -325,6 +325,91 @@ public abstract class Maps
         }
     }
 
+    // Currently only used internally counters support in SuperColumn families.
+    // Addition on the element level inside the collections are otherwise not supported in the CQL.
+    public static class AdderByKey extends Operation
+    {
+        private final Term k;
+
+        public AdderByKey(ColumnDefinition column, Term t, Term k)
+        {
+            super(column, t);
+            this.k = k;
+        }
+
+        @Override
+        public void collectMarkerSpecification(VariableSpecifications boundNames)
+        {
+            super.collectMarkerSpecification(boundNames);
+            k.collectMarkerSpecification(boundNames);
+        }
+
+        public void execute(DecoratedKey partitionKey, UpdateParameters params) throws InvalidRequestException
+        {
+            assert column.type.isMultiCell() : "Attempted to set a value for a single key on a frozen map";
+
+            ByteBuffer key = k.bindAndGet(params.options);
+            ByteBuffer value = t.bindAndGet(params.options);
+
+            if (key == null)
+                throw new InvalidRequestException("Invalid null map key");
+            if (key == ByteBufferUtil.UNSET_BYTE_BUFFER)
+                throw new InvalidRequestException("Invalid unset map key");
+
+            if (value == null)
+                throw new InvalidRequestException("Invalid null value for counter increment");
+            if (value == ByteBufferUtil.UNSET_BYTE_BUFFER)
+                return;
+
+            long increment = ByteBufferUtil.toLong(value);
+            params.addCounter(column, increment, CellPath.create(key));
+        }
+    }
+
+    // Currently only used internally counters support in SuperColumn families.
+    // Addition on the element level inside the collections are otherwise not supported in the CQL.
+    public static class SubtracterByKey extends Operation
+    {
+        private final Term k;
+
+        public SubtracterByKey(ColumnDefinition column, Term t, Term k)
+        {
+            super(column, t);
+            this.k = k;
+        }
+
+        @Override
+        public void collectMarkerSpecification(VariableSpecifications boundNames)
+        {
+            super.collectMarkerSpecification(boundNames);
+            k.collectMarkerSpecification(boundNames);
+        }
+
+        public void execute(DecoratedKey partitionKey, UpdateParameters params) throws InvalidRequestException
+        {
+            assert column.type.isMultiCell() : "Attempted to set a value for a single key on a frozen map";
+
+            ByteBuffer key = k.bindAndGet(params.options);
+            ByteBuffer value = t.bindAndGet(params.options);
+
+            if (key == null)
+                throw new InvalidRequestException("Invalid null map key");
+            if (key == ByteBufferUtil.UNSET_BYTE_BUFFER)
+                throw new InvalidRequestException("Invalid unset map key");
+
+            if (value == null)
+                throw new InvalidRequestException("Invalid null value for counter increment");
+            if (value == ByteBufferUtil.UNSET_BYTE_BUFFER)
+                return;
+
+            long increment = ByteBufferUtil.toLong(value);
+            if (increment == Long.MIN_VALUE)
+                throw new InvalidRequestException("The negation of " + increment + " overflows supported counter precision (signed 8 bytes integer)");
+
+            params.addCounter(column, -increment, CellPath.create(key));
+        }
+    }
+
     public static class Putter extends Operation
     {
         public Putter(ColumnDefinition column, Term t)
diff --git a/src/java/org/apache/cassandra/cql3/MultiColumnRelation.java b/src/java/org/apache/cassandra/cql3/MultiColumnRelation.java
index 143106d1bb..1bfac3f0fb 100644
--- a/src/java/org/apache/cassandra/cql3/MultiColumnRelation.java
+++ b/src/java/org/apache/cassandra/cql3/MultiColumnRelation.java
@@ -27,6 +27,7 @@ import org.apache.cassandra.cql3.Term.MultiColumnRaw;
 import org.apache.cassandra.cql3.Term.Raw;
 import org.apache.cassandra.cql3.restrictions.MultiColumnRestriction;
 import org.apache.cassandra.cql3.restrictions.Restriction;
+import org.apache.cassandra.cql3.restrictions.SingleColumnRestriction;
 import org.apache.cassandra.cql3.statements.Bound;
 import org.apache.cassandra.exceptions.InvalidRequestException;
 
@@ -162,7 +163,7 @@ public class MultiColumnRelation extends Relation
                                               boolean inclusive) throws InvalidRequestException
     {
         List<ColumnDefinition> receivers = receivers(cfm);
-        Term term = toTerm(receivers(cfm), getValue(), cfm.ksName, boundNames);
+        Term term = toTerm(receivers, getValue(), cfm.ksName, boundNames);
         return new MultiColumnRestriction.SliceRestriction(receivers, bound, inclusive, term);
     }
 
@@ -238,4 +239,65 @@ public class MultiColumnRelation extends Relation
                       .append(valuesOrMarker)
                       .toString();
     }
+
+    @Override
+    public Relation toSuperColumnAdapter()
+    {
+        return new SuperColumnMultiColumnRelation(entities, relationType, valuesOrMarker, inValues, inMarker);
+    }
+
+    /**
+     * Required for SuperColumn compatibility, in order to map the SuperColumn key restrictions from the regular
+     * column to the collection key one.
+     */
+    private class SuperColumnMultiColumnRelation extends MultiColumnRelation
+    {
+        private SuperColumnMultiColumnRelation(List<ColumnIdentifier.Raw> entities, Operator relationType, MultiColumnRaw valuesOrMarker, List<? extends MultiColumnRaw> inValues, Tuples.INRaw inMarker)
+        {
+            super(entities, relationType, valuesOrMarker, inValues, inMarker);
+        }
+
+        @Override
+        protected Restriction newSliceRestriction(CFMetaData cfm,
+                                                  VariableSpecifications boundNames,
+                                                  Bound bound,
+                                                  boolean inclusive) throws InvalidRequestException
+        {
+            assert cfm.isSuper() && cfm.isDense();
+            List<ColumnDefinition> receivers = receivers(cfm);
+            Term term = toTerm(receivers, getValue(), cfm.ksName, boundNames);
+            return new SingleColumnRestriction.SuperColumnMultiSliceRestriction(receivers.get(0), bound, inclusive, term);
+        }
+
+        @Override
+        protected Restriction newEQRestriction(CFMetaData cfm,
+                                               VariableSpecifications boundNames) throws InvalidRequestException
+        {
+            assert cfm.isSuper() && cfm.isDense();
+            List<ColumnDefinition> receivers = receivers(cfm);
+            Term term = toTerm(receivers, getValue(), cfm.ksName, boundNames);
+            return new SingleColumnRestriction.SuperColumnMultiEQRestriction(receivers.get(0), term);
+        }
+
+        @Override
+        protected List<ColumnDefinition> receivers(CFMetaData cfm) throws InvalidRequestException
+        {
+            assert cfm.isSuper() && cfm.isDense();
+            List<ColumnDefinition> names = new ArrayList<>(getEntities().size());
+
+            for (ColumnIdentifier.Raw raw : getEntities())
+            {
+                ColumnDefinition def = toColumnDefinition(cfm, raw);
+
+                checkTrue(def.isClusteringColumn() ||
+                          cfm.isSuperColumnKeyColumn(def),
+                          "Multi-column relations can only be applied to clustering columns but was applied to: %s", def.name);
+
+                checkFalse(names.contains(def), "Column \"%s\" appeared twice in a relation: %s", def.name, this);
+
+                names.add(def);
+            }
+            return names;
+        }
+    }
 }
diff --git a/src/java/org/apache/cassandra/cql3/Operation.java b/src/java/org/apache/cassandra/cql3/Operation.java
index ecd37c42c8..4b8d5ba254 100644
--- a/src/java/org/apache/cassandra/cql3/Operation.java
+++ b/src/java/org/apache/cassandra/cql3/Operation.java
@@ -192,6 +192,11 @@ public abstract class Operation
             // it's stupid and 2) the result would seem random to the user.
             return false;
         }
+
+        public Term.Raw value()
+        {
+            return value;
+        }
     }
 
     public static class SetElement implements RawUpdate
@@ -241,6 +246,72 @@ public abstract class Operation
         }
     }
 
+    // Currently only used internally counters support in SuperColumn families.
+    // Addition on the element level inside the collections are otherwise not supported in the CQL.
+    public static class ElementAddition implements RawUpdate
+    {
+        private final Term.Raw selector;
+        private final Term.Raw value;
+
+        public ElementAddition(Term.Raw selector, Term.Raw value)
+        {
+            this.selector = selector;
+            this.value = value;
+        }
+
+        public Operation prepare(String keyspace, ColumnDefinition receiver) throws InvalidRequestException
+        {
+            assert receiver.type instanceof MapType;
+            Term k = selector.prepare(keyspace, Maps.keySpecOf(receiver));
+            Term v = value.prepare(keyspace, Maps.valueSpecOf(receiver));
+
+            return new Maps.AdderByKey(receiver, v, k);
+        }
+
+        protected String toString(ColumnSpecification column)
+        {
+            return String.format("%s = %s + %s", column.name, column.name, value);
+        }
+
+        public boolean isCompatibleWith(RawUpdate other)
+        {
+            return !(other instanceof SetValue);
+        }
+    }
+
+    // Currently only used internally counters support in SuperColumn families.
+    // Addition on the element level inside the collections are otherwise not supported in the CQL.
+    public static class ElementSubtraction implements RawUpdate
+    {
+        private final Term.Raw selector;
+        private final Term.Raw value;
+
+        public  ElementSubtraction(Term.Raw selector, Term.Raw value)
+        {
+            this.selector = selector;
+            this.value = value;
+        }
+
+        public Operation prepare(String keyspace, ColumnDefinition receiver) throws InvalidRequestException
+        {
+            assert receiver.type instanceof MapType;
+            Term k = selector.prepare(keyspace, Maps.keySpecOf(receiver));
+            Term v = value.prepare(keyspace, Maps.valueSpecOf(receiver));
+
+            return new Maps.SubtracterByKey(receiver, v, k);
+        }
+
+        protected String toString(ColumnSpecification column)
+        {
+            return String.format("%s = %s + %s", column.name, column.name, value);
+        }
+
+        public boolean isCompatibleWith(RawUpdate other)
+        {
+            return !(other instanceof SetValue);
+        }
+    }
+
     public static class Addition implements RawUpdate
     {
         private final Term.Raw value;
@@ -284,6 +355,11 @@ public abstract class Operation
         {
             return !(other instanceof SetValue);
         }
+
+        public Term.Raw value()
+        {
+            return value;
+        }
     }
 
     public static class Substraction implements RawUpdate
@@ -332,6 +408,11 @@ public abstract class Operation
         {
             return !(other instanceof SetValue);
         }
+
+        public Term.Raw value()
+        {
+            return value;
+        }
     }
 
     public static class Prepend implements RawUpdate
diff --git a/src/java/org/apache/cassandra/cql3/Relation.java b/src/java/org/apache/cassandra/cql3/Relation.java
index 334464f067..a88932e3e7 100644
--- a/src/java/org/apache/cassandra/cql3/Relation.java
+++ b/src/java/org/apache/cassandra/cql3/Relation.java
@@ -26,6 +26,7 @@ import org.apache.cassandra.cql3.restrictions.Restriction;
 import org.apache.cassandra.cql3.statements.Bound;
 import org.apache.cassandra.exceptions.InvalidRequestException;
 import org.apache.cassandra.exceptions.UnrecognizedEntityException;
+import sun.reflect.generics.reflectiveObjects.NotImplementedException;
 
 import static org.apache.cassandra.cql3.statements.RequestValidations.invalidRequest;
 
@@ -147,6 +148,15 @@ public abstract class Relation {
         }
     }
 
+    /**
+     * Required for SuperColumn compatibility, creates an adapter Relation that remaps all restrictions required for
+     * SuperColumn tables.
+     */
+    public Relation toSuperColumnAdapter()
+    {
+        throw invalidRequest("Unsupported operation (" + this + ") on super column family");
+    }
+
     /**
      * Creates a new EQ restriction instance.
      *
diff --git a/src/java/org/apache/cassandra/cql3/SingleColumnRelation.java b/src/java/org/apache/cassandra/cql3/SingleColumnRelation.java
index 05ba42dbc9..455ae0c376 100644
--- a/src/java/org/apache/cassandra/cql3/SingleColumnRelation.java
+++ b/src/java/org/apache/cassandra/cql3/SingleColumnRelation.java
@@ -40,7 +40,7 @@ import static org.apache.cassandra.cql3.statements.RequestValidations.checkTrue;
  * a value (term). For example, <key> > "start" or "colname1" = "somevalue".
  *
  */
-public final class SingleColumnRelation extends Relation
+public class SingleColumnRelation extends Relation
 {
     private final ColumnIdentifier.Raw entity;
     private final Term.Raw mapKey;
@@ -303,4 +303,78 @@ public final class SingleColumnRelation extends Relation
     {
         return isEQ() || (isIN() && inValues != null && inValues.size() == 1);
     }
+
+    @Override
+    public Relation toSuperColumnAdapter()
+    {
+        return new SuperColumnSingleColumnRelation(entity, mapKey, relationType, value);
+    }
+
+    /**
+     * Required for SuperColumn compatibility, in order to map the SuperColumn key restrictions from the regular
+     * column to the collection key one.
+     */
+    private class SuperColumnSingleColumnRelation extends SingleColumnRelation
+    {
+        SuperColumnSingleColumnRelation(ColumnIdentifier.Raw entity, Raw mapKey, Operator type, Raw value)
+        {
+            super(entity, mapKey, type, value, inValues);
+        }
+
+        @Override
+        public Restriction newSliceRestriction(CFMetaData cfm,
+                                               VariableSpecifications boundNames,
+                                               Bound bound,
+                                               boolean inclusive) throws InvalidRequestException
+        {
+            ColumnDefinition columnDef = toColumnDefinition(cfm, entity);
+            if (cfm.isSuperColumnKeyColumn(columnDef))
+            {
+                Term term = toTerm(toReceivers(columnDef, cfm.isDense()), value, cfm.ksName, boundNames);
+                return new SingleColumnRestriction.SuperColumnKeySliceRestriction(cfm.superColumnKeyColumn(), bound, inclusive, term);
+            }
+            else
+            {
+                return super.newSliceRestriction(cfm, boundNames, bound, inclusive);
+            }
+        }
+
+        @Override
+        protected Restriction newEQRestriction(CFMetaData cfm,
+                                               VariableSpecifications boundNames) throws InvalidRequestException
+        {
+            ColumnDefinition columnDef = toColumnDefinition(cfm, entity);
+            if (cfm.isSuperColumnKeyColumn(columnDef))
+            {
+                Term term = toTerm(toReceivers(columnDef, cfm.isDense()), value, cfm.ksName, boundNames);
+                return new SingleColumnRestriction.SuperColumnKeyEQRestriction(cfm.superColumnKeyColumn(), term);
+            }
+            else
+            {
+                return super.newEQRestriction(cfm, boundNames);
+            }
+        }
+
+        @Override
+        protected Restriction newINRestriction(CFMetaData cfm,
+                                               VariableSpecifications boundNames) throws InvalidRequestException
+        {
+            ColumnDefinition columnDef = toColumnDefinition(cfm, entity);
+            if (cfm.isSuperColumnKeyColumn(columnDef))
+            {
+                List<? extends ColumnSpecification> receivers = Collections.singletonList(cfm.superColumnKeyColumn());
+                List<Term> terms = toTerms(receivers, inValues, cfm.ksName, boundNames);
+                if (terms == null)
+                {
+                    Term term = toTerm(receivers, value, cfm.ksName, boundNames);
+                    return new SingleColumnRestriction.SuperColumnKeyINRestrictionWithMarkers(cfm.superColumnKeyColumn(), (Lists.Marker) term);
+                }
+                return new SingleColumnRestriction.SuperColumnKeyINRestrictionWithValues(cfm.superColumnKeyColumn(), terms);
+            }
+            else
+            {
+                return super.newINRestriction(cfm, boundNames);
+            }
+        }
+    }
 }
diff --git a/src/java/org/apache/cassandra/cql3/SuperColumnCompatibility.java b/src/java/org/apache/cassandra/cql3/SuperColumnCompatibility.java
new file mode 100644
index 0000000000..d4c14dff10
--- /dev/null
+++ b/src/java/org/apache/cassandra/cql3/SuperColumnCompatibility.java
@@ -0,0 +1,765 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql3;
+
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.HashSet;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Set;
+import java.util.TreeSet;
+
+import org.apache.cassandra.config.CFMetaData;
+import org.apache.cassandra.config.ColumnDefinition;
+import org.apache.cassandra.cql3.restrictions.Restriction;
+import org.apache.cassandra.cql3.restrictions.SingleColumnRestriction;
+import org.apache.cassandra.cql3.restrictions.TermSlice;
+import org.apache.cassandra.cql3.selection.Selection;
+import org.apache.cassandra.cql3.statements.Bound;
+import org.apache.cassandra.db.Clustering;
+import org.apache.cassandra.db.Columns;
+import org.apache.cassandra.db.CompactTables;
+import org.apache.cassandra.db.PartitionColumns;
+import org.apache.cassandra.db.filter.ColumnFilter;
+import org.apache.cassandra.db.filter.RowFilter;
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.db.marshal.MapType;
+import org.apache.cassandra.db.marshal.UTF8Type;
+import org.apache.cassandra.db.rows.Cell;
+import org.apache.cassandra.db.rows.CellPath;
+import org.apache.cassandra.db.rows.ComplexColumnData;
+import org.apache.cassandra.db.rows.Row;
+import org.apache.cassandra.db.rows.RowIterator;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.Pair;
+
+import static org.apache.cassandra.cql3.statements.RequestValidations.checkFalse;
+import static org.apache.cassandra.cql3.statements.RequestValidations.checkNotNull;
+import static org.apache.cassandra.cql3.statements.RequestValidations.checkTrue;
+import static org.apache.cassandra.cql3.statements.RequestValidations.invalidRequest;
+import static org.apache.cassandra.cql3.statements.SelectStatement.getComponents;
+
+/**
+ * Class incapsulating the helper logic to handle SELECT / UPDATE / INSERT special-cases related
+ * to SuperColumn tables in applicable scenarios.
+ *
+ * SuperColumn families have a special layout and are represented as a Map internally. These tables
+ * have two special columns (called `column2` and `value` by default):
+ *
+ *   * `column2`, {@link CFMetaData#superCfValueColumn}, a key of the SuperColumn map, exposed as a
+ *   REGULAR column, but stored in schema tables as a CLUSTERING column to make a distinction from
+ *   the SC value column in case of renames.
+ *   * `value`, {@link CFMetaData#compactValueColumn()}, a value of the SuperColumn map, exposed and
+ *   stored as a REGULAR column
+ *
+ * These columns have to be translated to this internal representation as key and value, correspondingly.
+ *
+ * In CQL terms, the SuperColumn families is encoded with:
+ *
+ *   CREATE TABLE super (
+ *      key [key_validation_class],
+ *      super_column_name [comparator],
+ *      [column_metadata_1] [type1],
+ *      ...,
+ *      [column_metadata_n] [type1],
+ *      "" map<[sub_comparator], [default_validation_class]>
+ *      PRIMARY KEY (key, super_column_name)
+ *   )
+ *
+ * In other words, every super column is encoded by a row. That row has one column for each defined
+ * "column_metadata", but it also has a special map column (whose name is the empty string as this is
+ * guaranteed to never conflict with a user-defined "column_metadata") which stores the super column
+ * "dynamic" sub-columns.
+ *
+ * On write path, `column2` and `value` columns are translated to the key and value of the
+ * underlying map. During the read, the inverse conversion is done. Deletes are converted into
+ * discards by the key in the underlying map. Counters are handled by translating an update to a
+ * counter update with a cell path. See {@link SuperColumnRestrictions} for the details.
+ *
+ * Since non-dense SuperColumn families do not modify the contents of the internal map through in CQL
+ * and do not expose this via CQL either, reads, writes and deletes are handled normally.
+ *
+ * Sidenote: a _dense_ SuperColumn Familiy is the one that has no added REGULAR columns.
+ */
+public class SuperColumnCompatibility
+{
+    // We use an empty value for the 1) this can't conflict with a user-defined column and 2) this actually
+    // validate with any comparator which makes it convenient for columnDefinitionComparator().
+    public static final ByteBuffer SUPER_COLUMN_MAP_COLUMN = ByteBufferUtil.EMPTY_BYTE_BUFFER;
+    public static final String SUPER_COLUMN_MAP_COLUMN_STR = UTF8Type.instance.compose(SUPER_COLUMN_MAP_COLUMN);
+
+    /**
+     * Dense flag might have been incorrectly set if the node was upgraded from 2.x before CASSANDRA-12373.
+     *
+     * For 3.x created tables, the flag is set correctly in ThriftConversion code.
+     */
+    public static boolean recalculateIsDense(Columns columns)
+    {
+        return columns.size() == 1 && columns.getComplex(0).name.toString().isEmpty();
+    }
+
+    /**
+     * For _dense_ SuperColumn Families, the supercolumn key column has to be translated to the collection subselection
+     * query in order to avoid reading an entire collection and then filtering out the results.
+     */
+    public static ColumnFilter getColumnFilter(CFMetaData cfm, QueryOptions queryOptions, SuperColumnRestrictions restrictions)
+    {
+        assert cfm.isSuper() && cfm.isDense();
+
+        ColumnFilter.Builder builder = ColumnFilter.selectionBuilder();
+        builder.add(cfm.compactValueColumn());
+
+        if (restrictions.keySliceRestriction != null)
+        {
+            SingleColumnRestriction.SuperColumnKeySliceRestriction restriction = restrictions.keySliceRestriction;
+            TermSlice slice = restriction.slice;
+
+            ByteBuffer start = slice.hasBound(Bound.START) ? slice.bound(Bound.START).bindAndGet(queryOptions) : null;
+            ByteBuffer end = slice.hasBound(Bound.END) ? slice.bound(Bound.END).bindAndGet(queryOptions) : null;
+
+            builder.slice(cfm.compactValueColumn(),
+                          start == null ? CellPath.BOTTOM : CellPath.create(start),
+                          end == null ? CellPath.TOP : CellPath.create(end));
+        }
+        else if (restrictions.keyEQRestriction != null)
+        {
+            SingleColumnRestriction.SuperColumnKeyEQRestriction restriction = restrictions.keyEQRestriction;
+            ByteBuffer value = restriction.bindValue(queryOptions);
+            builder.select(cfm.compactValueColumn(), CellPath.create(value));
+        }
+        else if (restrictions.keyINRestriction != null)
+        {
+            SingleColumnRestriction.SuperColumnKeyINRestriction cast = restrictions.keyINRestriction;
+            Set<ByteBuffer> keyINRestrictionValues = new TreeSet<ByteBuffer>(((MapType) cfm.compactValueColumn().type).getKeysType());
+            keyINRestrictionValues.addAll(cast.getValues(queryOptions));
+
+            for (ByteBuffer value : keyINRestrictionValues)
+                builder.select(cfm.compactValueColumn(), CellPath.create(value));
+        }
+        else if (restrictions.multiEQRestriction != null)
+        {
+            SingleColumnRestriction.SuperColumnMultiEQRestriction restriction = restrictions.multiEQRestriction;
+            ByteBuffer value = restriction.secondValue;
+            builder.select(cfm.compactValueColumn(), CellPath.create(value));
+        }
+
+        return builder.build();
+    }
+
+    /**
+     * For _dense_ SuperColumn Families.
+     *
+     * On read path, instead of writing row per map, we have to write a row per key/value pair in map.
+     *
+     * For example:
+     *
+     *   | partition-key | clustering-key | { key1: value1, key2: value2 } |
+     *
+     * Will be translated to:
+     *
+     *   | partition-key | clustering-key | key1 | value1 |
+     *   | partition-key | clustering-key | key2 | value2 |
+     *
+     */
+    public static void processPartition(CFMetaData cfm, Selection selection, RowIterator partition, Selection.ResultSetBuilder result, int protocolVersion,
+                                        SuperColumnRestrictions restrictions, QueryOptions queryOptions)
+    {
+        assert cfm.isDense();
+        ByteBuffer[] keyComponents = getComponents(cfm, partition.partitionKey());
+
+        int nowInSeconds = FBUtilities.nowInSeconds();
+        while (partition.hasNext())
+        {
+            Row row = partition.next();
+
+            ComplexColumnData ccd = row.getComplexColumnData(cfm.compactValueColumn());
+
+            if (ccd == null)
+                continue;
+
+            Iterator<Cell> cellIter = ccd.iterator();
+
+            outer:
+            while (cellIter.hasNext())
+            {
+                Cell cell = cellIter.next();
+                ByteBuffer superColumnKey = cell.path().get(0);
+
+                if (restrictions != null)
+                {
+                    // Slice on SuperColumn key
+                    if (restrictions.keySliceRestriction != null)
+                    {
+                        for (Bound bound : Bound.values())
+                        {
+                            if (restrictions.keySliceRestriction.hasBound(bound) &&
+                                !restrictions.keySliceRestriction.isInclusive(bound))
+                            {
+                                ByteBuffer excludedValue = restrictions.keySliceRestriction.bindValue(queryOptions);
+                                if (excludedValue.equals(superColumnKey))
+                                    continue outer;
+                            }
+                        }
+                    }
+
+                    // Multi-column restriction on clustering+SuperColumn key
+                    if (restrictions.multiSliceRestriction != null &&
+                        cfm.comparator.compare(row.clustering(), new Clustering(restrictions.multiSliceRestriction.firstValue)) == 0)
+                    {
+                        AbstractType t = ((MapType) cfm.compactValueColumn().type).getKeysType();
+                        int cmp = t.compare(superColumnKey, restrictions.multiSliceRestriction.secondValue);
+
+                        if ((cmp == 0 && !restrictions.multiSliceRestriction.trueInclusive) ||     // EQ
+                            (restrictions.multiSliceRestriction.hasBound(Bound.END) && cmp > 0) || // LT
+                            (restrictions.multiSliceRestriction.hasBound(Bound.START) && cmp < 0)) // GT
+                            continue outer;
+                    }
+                }
+
+                result.newRow(protocolVersion);
+
+                for (ColumnDefinition def : selection.getColumns())
+                {
+                    if (cfm.isSuperColumnKeyColumn(def))
+                    {
+                        result.add(superColumnKey);
+                    }
+                    else if (cfm.isSuperColumnValueColumn(def))
+                    {
+                        result.add(cell, nowInSeconds);
+                    }
+                    else
+                    {
+                        switch (def.kind)
+                        {
+                            case PARTITION_KEY:
+                                result.add(keyComponents[def.position()]);
+                                break;
+                            case CLUSTERING:
+                                result.add(row.clustering().get(def.position()));
+                                break;
+                            case REGULAR:
+                            case STATIC:
+                                throw new AssertionError(String.format("Invalid column '%s' found in SuperColumn table", def.name.toString()));
+                        }
+                    }
+                }
+            }
+        }
+    }
+
+    /**
+     * For _dense_ SuperColumn Families.
+     *
+     * On the write path, we have to do combine the columns into a key/value pair:
+     *
+     * So inserting a row:
+     *
+     *     | partition-key | clustering-key | key1 | value1 |
+     *
+     * Would result into:
+     *
+     *     | partition-key | clustering-key | {key1: value1} |
+     *
+     * or adding / overwriting the value for `key1`.
+     */
+    public static void prepareInsertOperations(CFMetaData cfm,
+                                               List<ColumnIdentifier.Raw> columnNames,
+                                               WhereClause.Builder whereClause,
+                                               List<Term.Raw> columnValues,
+                                               VariableSpecifications boundNames,
+                                               Operations operations)
+    {
+        List<ColumnDefinition> defs = new ArrayList<>(columnNames.size());
+        for (int i = 0; i < columnNames.size(); i++)
+        {
+            ColumnIdentifier id = columnNames.get(i).prepare(cfm);
+            defs.add(cfm.getColumnDefinition(id));
+        }
+
+        prepareInsertOperations(cfm, defs, boundNames, columnValues, whereClause, operations);
+    }
+
+    /**
+     * For _dense_ SuperColumn Families.
+     *
+     * {@link #prepareInsertOperations(CFMetaData, List, VariableSpecifications, List, WhereClause.Builder, Operations)},
+     * but for INSERT JSON queries
+     */
+    public static void prepareInsertJSONOperations(CFMetaData cfm,
+                                                   List<ColumnDefinition> defs,
+                                                   VariableSpecifications boundNames,
+                                                   Json.Prepared prepared,
+                                                   WhereClause.Builder whereClause,
+                                                   Operations operations)
+    {
+        List<Term.Raw> columnValues = new ArrayList<>(defs.size());
+        for (ColumnDefinition def : defs)
+            columnValues.add(prepared.getRawTermForColumn(def));
+
+        prepareInsertOperations(cfm, defs, boundNames, columnValues, whereClause, operations);
+    }
+
+    private static void prepareInsertOperations(CFMetaData cfm,
+                                                List<ColumnDefinition> defs,
+                                                VariableSpecifications boundNames,
+                                                List<Term.Raw> columnValues,
+                                                WhereClause.Builder whereClause,
+                                                Operations operations)
+    {
+        assert cfm.isDense();
+        assert defs.size() == columnValues.size();
+
+        Term.Raw superColumnKey = null;
+        Term.Raw superColumnValue = null;
+
+        for (int i = 0, size = defs.size(); i < size; i++)
+        {
+            ColumnDefinition def = defs.get(i);
+            Term.Raw raw = columnValues.get(i);
+
+            if (cfm.isSuperColumnKeyColumn(def))
+            {
+                superColumnKey = raw;
+                collectMarkerSpecifications(raw, boundNames, def);
+            }
+            else if (cfm.isSuperColumnValueColumn(def))
+            {
+                superColumnValue = raw;
+                collectMarkerSpecifications(raw, boundNames, def);
+            }
+            else if (def.isPrimaryKeyColumn())
+            {
+                whereClause.add(new SingleColumnRelation(new ColumnIdentifier.ColumnIdentifierValue(def.name), Operator.EQ, raw));
+            }
+            else
+            {
+                throw invalidRequest("Invalid column {} in where clause");
+            }
+        }
+
+        checkTrue(superColumnValue != null,
+                  "Column value is mandatory for SuperColumn tables");
+        checkTrue(superColumnKey != null,
+                  "Column key is mandatory for SuperColumn tables");
+
+        Operation operation = new Operation.SetElement(superColumnKey, superColumnValue).prepare(cfm.ksName, cfm.compactValueColumn());
+        operations.add(operation);
+    }
+
+    /**
+     * Collect the marker specifications for the bound columns manually, since the operations on a column are
+     * converted to the operations on the collection element.
+     */
+    private static void collectMarkerSpecifications(Term.Raw raw, VariableSpecifications boundNames, ColumnDefinition def)
+    {
+        if (raw instanceof AbstractMarker.Raw)
+            boundNames.add(((AbstractMarker.Raw) raw).bindIndex(), def);
+    }
+
+    /**
+     * For _dense_ SuperColumn Families.
+     *
+     * During UPDATE operation, the update by clustering (with correponding relation in WHERE clause)
+     * has to be substituted with an update to the map that backs the given SuperColumn.
+     *
+     * For example, an update such as:
+     *
+     *     UPDATE ... SET value = 'value1' WHERE key = 'pk' AND column1 = 'ck' AND column2 = 'mk'
+     *
+     * Will update the value under key 'mk' in the map, backing the SuperColumn, located in the row
+     * with clustering 'ck' in the partition with key 'pk'.
+     */
+    public static WhereClause prepareUpdateOperations(CFMetaData cfm,
+                                                      WhereClause whereClause,
+                                                      List<Pair<ColumnIdentifier.Raw, Operation.RawUpdate>> updates,
+                                                      VariableSpecifications boundNames,
+                                                      Operations operations)
+    {
+        assert cfm.isDense();
+        Term.Raw superColumnKey = null;
+        Term.Raw superColumnValue = null;
+
+        List<Relation> newRelations = new ArrayList<>(whereClause.relations.size());
+        for (int i = 0; i < whereClause.relations.size(); i++)
+        {
+            SingleColumnRelation relation = (SingleColumnRelation) whereClause.relations.get(i);
+            ColumnIdentifier id = relation.getEntity().prepare(cfm);
+            ColumnDefinition def = cfm.getColumnDefinition(id);
+
+            if (cfm.isSuperColumnKeyColumn(def))
+            {
+                superColumnKey = relation.getValue();
+                collectMarkerSpecifications(superColumnKey, boundNames, def);
+            }
+            else
+            {
+                newRelations.add(relation);
+            }
+        }
+
+        checkTrue(superColumnKey != null,
+                  "Column key is mandatory for SuperColumn tables");
+
+        for (Pair<ColumnIdentifier.Raw, Operation.RawUpdate> entry : updates)
+        {
+            ColumnIdentifier id = entry.left.prepare(cfm);
+            ColumnDefinition def = cfm.getColumnDefinition(id);
+
+            if (!cfm.isSuperColumnValueColumn(def))
+                throw invalidRequest("Column `%s` of type `%s` found in SET part", def.name, def.type.asCQL3Type());
+
+            Operation operation;
+
+            if (entry.right instanceof Operation.Addition)
+            {
+                Operation.Addition op = (Operation.Addition) entry.right;
+                superColumnValue = op.value();
+
+                operation = new Operation.ElementAddition(superColumnKey, superColumnValue).prepare(cfm.ksName, cfm.compactValueColumn());
+            }
+            else if (entry.right instanceof Operation.Substraction)
+            {
+                Operation.Substraction op = (Operation.Substraction) entry.right;
+                superColumnValue = op.value();
+
+                operation = new Operation.ElementSubtraction(superColumnKey, superColumnValue).prepare(cfm.ksName, cfm.compactValueColumn());
+            }
+            else if (entry.right instanceof Operation.SetValue)
+            {
+                Operation.SetValue op = (Operation.SetValue) entry.right;
+                superColumnValue = op.value();
+
+                operation = new Operation.SetElement(superColumnKey, superColumnValue).prepare(cfm.ksName, cfm.compactValueColumn());
+            }
+            else
+            {
+                throw invalidRequest("Invalid operation `%s` on column `%s` of type `%s` found in SET part", entry.right, def.name, def.type.asCQL3Type());
+            }
+
+            collectMarkerSpecifications(superColumnValue, boundNames, def);
+            operations.add(operation);
+        }
+
+        checkTrue(superColumnValue != null,
+                  "Column value is mandatory for SuperColumn tables");
+
+        return newRelations.size() != whereClause.relations.size() ? whereClause.copy(newRelations) : whereClause;
+    }
+
+    /**
+     * Rebuilds LWT conditions on SuperColumn _value_ column.
+     *
+     * Conditions have to be changed to correspond the internal representation of SuperColumn value, since it's not
+     * a separate column, but a value in a hidden compact value column.
+     */
+    public static Conditions rebuildLWTColumnConditions(Conditions conditions, CFMetaData cfm, WhereClause whereClause)
+    {
+        if (conditions.isEmpty() || conditions.isIfExists() || conditions.isIfNotExists())
+            return conditions;
+
+        ColumnConditions.Builder builder = ColumnConditions.newBuilder();
+        Collection<ColumnCondition> columnConditions = ((ColumnConditions) conditions).columnConditions();
+
+        Pair<ColumnDefinition, Relation> superColumnKeyRelation = SuperColumnCompatibility.getSuperColumnKeyRelation(whereClause.relations, cfm);
+
+        checkNotNull(superColumnKeyRelation,
+                     "Lightweight transactions on SuperColumn tables are only supported with supplied SuperColumn key");
+
+        for (ColumnCondition columnCondition : columnConditions)
+        {
+            checkTrue(cfm.isSuperColumnValueColumn(columnCondition.column),
+                      "Lightweight transactions are only supported on the value column of SuperColumn tables");
+
+            Term.Raw value = superColumnKeyRelation.right.getValue();
+            Term collectionElemnt = value instanceof AbstractMarker.Raw ?
+                                    new Constants.Marker(((AbstractMarker.Raw) value).bindIndex(),
+                                                         superColumnKeyRelation.left) :
+                                    value.prepare(cfm.ksName, superColumnKeyRelation.left);
+            builder.add(ColumnCondition.condition(cfm.compactValueColumn(),
+                                                  collectionElemnt,
+                                                  columnCondition.value(), columnCondition.operator));
+        }
+
+        return builder.build();
+    }
+
+    /**
+     * Returns a relation on the SuperColumn key
+     */
+    private static Pair<ColumnDefinition, Relation> getSuperColumnKeyRelation(List<Relation> relations, CFMetaData cfm)
+    {
+        for (int i = 0; i < relations.size(); i++)
+        {
+            SingleColumnRelation relation = (SingleColumnRelation) relations.get(i);
+            ColumnIdentifier id = relation.getEntity().prepare(cfm);
+            ColumnDefinition def = cfm.getColumnDefinition(id);
+
+            if (cfm.isSuperColumnKeyColumn(def))
+                return Pair.create(def, relation);
+        }
+        return null;
+    }
+
+    /**
+     * For _dense_ SuperColumn Families.
+     *
+     * Delete, when the "regular" columns are present, have to be translated into
+     * deletion of value in the internal map by key.
+     *
+     * For example, delete such as:
+     *
+     *     DELETE FROM ... WHERE key = 'pk' AND column1 = 'ck' AND column2 = 'mk'
+     *
+     * Will delete a value under 'mk' from the map, located in the row with clustering key 'ck' in the partition
+     * with key 'pk'.
+     */
+    public static WhereClause prepareDeleteOperations(CFMetaData cfm,
+                                                      WhereClause whereClause,
+                                                      VariableSpecifications boundNames,
+                                                      Operations operations)
+    {
+        assert cfm.isDense();
+        List<Relation> newRelations = new ArrayList<>(whereClause.relations.size());
+
+        for (int i = 0; i < whereClause.relations.size(); i++)
+        {
+            Relation orig = whereClause.relations.get(i);
+
+            checkFalse(orig.isMultiColumn(),
+                       "Multi-column relations cannot be used in WHERE clauses for UPDATE and DELETE statements: %s", orig);
+            checkFalse(orig.onToken(),
+                       "Token relations cannot be used in WHERE clauses for UPDATE and DELETE statements: %s", orig);
+
+            SingleColumnRelation relation = (SingleColumnRelation) orig;
+            ColumnIdentifier id = relation.getEntity().prepare(cfm);
+            ColumnDefinition def = cfm.getColumnDefinition(id);
+
+            if (cfm.isSuperColumnKeyColumn(def))
+            {
+                Term.Raw value = relation.getValue();
+
+                if (value instanceof AbstractMarker.Raw)
+                    boundNames.add(((AbstractMarker.Raw) value).bindIndex(), def);
+
+                Operation operation = new Maps.DiscarderByKey(cfm.compactValueColumn(), value.prepare(cfm.ksName, def));
+                operations.add(operation);
+            }
+            else
+            {
+                newRelations.add(relation);
+            }
+        }
+
+        return newRelations.size() != whereClause.relations.size() ? whereClause.copy(newRelations) : whereClause;
+    }
+
+    /**
+     * Create a column name generator for SuperColumns
+     */
+    public static CompactTables.DefaultNames columnNameGenerator(List<ColumnDefinition> partitionKeyColumns,
+                                                                 List<ColumnDefinition> clusteringColumns,
+                                                                 PartitionColumns partitionColumns)
+    {
+        Set<String> names = new HashSet<>();
+        // If the clustering column was renamed, the supercolumn key's default nname still can't be `column1` (SuperColumn
+        // key renames are handled separately by looking up an existing column).
+        names.add("column1");
+        for (ColumnDefinition columnDefinition: partitionKeyColumns)
+            names.add(columnDefinition.name.toString());
+        for (ColumnDefinition columnDefinition: clusteringColumns)
+            names.add(columnDefinition.name.toString());
+        for (ColumnDefinition columnDefinition: partitionColumns)
+            names.add(columnDefinition.name.toString());
+
+        return CompactTables.defaultNameGenerator(names);
+    }
+
+    /**
+     * Find a SuperColumn key column if it's available (for example, when it was renamed) or create one with a default name.
+     */
+    public static ColumnDefinition getSuperCfKeyColumn(CFMetaData cfm, List<ColumnDefinition> clusteringColumns, CompactTables.DefaultNames defaultNames)
+    {
+        assert cfm.isDense();
+
+        MapType mapType = (MapType) cfm.compactValueColumn().type;
+        // Pre CASSANDRA-12373 3.x-created supercolumn family
+        if (clusteringColumns.size() == 1)
+        {
+            // create a new one with a default name
+            ColumnIdentifier identifier = ColumnIdentifier.getInterned(defaultNames.defaultClusteringName(), true);
+            return new ColumnDefinition(cfm.ksName, cfm.cfName, identifier, mapType.getKeysType(), ColumnDefinition.NO_POSITION, ColumnDefinition.Kind.REGULAR);
+        }
+
+        // Upgrade path: table created in 2.x, handle pre-created columns and/or renames.
+        assert clusteringColumns.size() == 2 : clusteringColumns;
+        ColumnDefinition cd = clusteringColumns.get(1);
+
+        assert cd.type.equals(mapType.getKeysType()) : cd.type + " != " + mapType.getKeysType();
+        return new ColumnDefinition(cfm.ksName, cfm.cfName, cd.name, mapType.getKeysType(), ColumnDefinition.NO_POSITION, ColumnDefinition.Kind.REGULAR);
+    }
+
+    /**
+     * Find a SuperColumn value column if it's available (for example, when it was renamed) or create one with a default name.
+     */
+    public static ColumnDefinition getSuperCfValueColumn(CFMetaData cfm, PartitionColumns partitionColumns, ColumnDefinition superCfKeyColumn, CompactTables.DefaultNames defaultNames)
+    {
+        assert cfm.isDense();
+
+        MapType mapType = (MapType) cfm.compactValueColumn().type;
+        for (ColumnDefinition def: partitionColumns.regulars)
+        {
+            if (!def.name.bytes.equals(SUPER_COLUMN_MAP_COLUMN) && def.type.equals(mapType.getValuesType()) && !def.equals(superCfKeyColumn))
+                return def;
+        }
+
+        ColumnIdentifier identifier = ColumnIdentifier.getInterned(defaultNames.defaultCompactValueName(), true);
+        return new ColumnDefinition(cfm.ksName, cfm.cfName, identifier, mapType.getValuesType(), ColumnDefinition.NO_POSITION, ColumnDefinition.Kind.REGULAR);
+    }
+
+    /**
+     * SuperColumn key is stored in {@link CFMetaData#columnMetadata} as a clustering column (to make sure we can make
+     * a distinction between the SuperColumn key and SuperColumn value columns, especially when they have the same type
+     * and were renamed), but exposed as {@link CFMetaData#superCfKeyColumn} as a regular column to be compatible with
+     * the storage engine.
+     *
+     * This remapping is necessary to facilitate the column metadata part.
+     */
+    public static ColumnDefinition getSuperCfSschemaRepresentation(ColumnDefinition superCfKeyColumn)
+    {
+        return new ColumnDefinition(superCfKeyColumn.ksName, superCfKeyColumn.cfName, superCfKeyColumn.name, superCfKeyColumn.type, 1, ColumnDefinition.Kind.CLUSTERING);
+    }
+
+    public static boolean isSuperColumnMapColumn(ColumnDefinition column)
+    {
+        return column.isRegular() && column.name.bytes.equals(SuperColumnCompatibility.SUPER_COLUMN_MAP_COLUMN);
+    }
+
+    public static ColumnDefinition getCompactValueColumn(PartitionColumns columns)
+    {
+        for (ColumnDefinition column : columns.regulars)
+        {
+            if (isSuperColumnMapColumn(column))
+                return column;
+        }
+        throw new AssertionError("Invalid super column table definition, no 'dynamic' map column");
+    }
+
+    /**
+     * Restrictions are the trickiest part of the SuperColumn integration.
+     * See specific docs on each field. For the purpose of this doc, the "default" column names are used,
+     * `column2` and `value`. Detailed description and semantics of these fields can be found in this class'
+     * header comment.
+     */
+    public static class SuperColumnRestrictions
+    {
+        /**
+         * Restrictions in the form of:
+         *   ... AND (column1, column2) > ('value1', 1)
+         * Multi-column restrictions. `column1` will be handled normally by the clustering bounds,
+         * and `column2` value has to be "saved" and filtered out in `processPartition`, as there's no
+         * direct mapping of multi-column restrictions to clustering + cell path. The first row
+         * is special-cased to make sure the semantics of multi-column restrictions are preserved.
+         */
+        private final SingleColumnRestriction.SuperColumnMultiSliceRestriction multiSliceRestriction;
+
+        /**
+         * Restrictions in the form of:
+         *   ... AND (column1, column2) = ('value1', 1)
+         * Multi-column restriction with EQ does have a direct mapping: `column1` will be handled
+         * normally by the clustering bounds, and the `column2` will be special-cased by the
+         * {@link #getColumnFilter(CFMetaData, QueryOptions, SuperColumnRestrictions)} as a collection path lookup.
+         */
+        private final SingleColumnRestriction.SuperColumnMultiEQRestriction multiEQRestriction;
+
+        /**
+         * Restrictions in the form of:
+         *   ... AND column2 >= 5
+         * For non-filtering cases (when the preceding clustering column and a partition key are
+         * restricted), will be handled in {@link #getColumnFilter(CFMetaData, QueryOptions, SuperColumnRestrictions)}
+         * like an inclusive bounds lookup.
+         *
+         * For the restrictions taking a form of
+         *   ... AND column2 > 5
+         * (non-inclusive ones), the items that match `=` will be filtered out
+         * by {@link #processPartition(CFMetaData, Selection, RowIterator, Selection.ResultSetBuilder, int, SuperColumnRestrictions, QueryOptions)}
+         *
+         * Unfortunately, there are no good ways to do it other than here:
+         * {@link RowFilter} can't be used in this case, since the complex collection cells are not yet rows by that
+         * point.
+         * {@link ColumnFilter} (which is used for inclusive slices) can't be changed to support exclusive slices as it would
+         * require a protocol change in order to add a Kind. So exclusive slices are a combination of inclusive plus
+         * an ad-hoc filter.
+         */
+        private final SingleColumnRestriction.SuperColumnKeySliceRestriction keySliceRestriction;
+
+        /**
+         * Restrictions in the form of:
+         *   ... AND column2 IN (1, 2, 3)
+         * For non-filtering cases (when the preceeding clustering column and a partition key are
+         * restricted), are handled in {@link #getColumnFilter(CFMetaData, QueryOptions, SuperColumnRestrictions)} by
+         * adding multiple collection paths to the {@link ColumnFilter}
+         */
+        private final SingleColumnRestriction.SuperColumnKeyINRestriction keyINRestriction;
+
+        /**
+         * Restrictions in the form of:
+         *   ... AND column2 = 1
+         * For non-filtering cases (when the preceeding clustering column and a partition key are
+         * restricted), will be handled by converting the restriction to the column filter on
+         * the collection key in {@link #getColumnFilter(CFMetaData, QueryOptions, SuperColumnRestrictions)}
+         */
+        private final SingleColumnRestriction.SuperColumnKeyEQRestriction keyEQRestriction;
+
+        public SuperColumnRestrictions(Iterator<Restriction> restrictions)
+        {
+            // In order to keep the fields final, assignments have to be done outside the loop
+            SingleColumnRestriction.SuperColumnMultiSliceRestriction multiSliceRestriction = null;
+            SingleColumnRestriction.SuperColumnKeySliceRestriction keySliceRestriction = null;
+            SingleColumnRestriction.SuperColumnKeyINRestriction keyINRestriction = null;
+            SingleColumnRestriction.SuperColumnMultiEQRestriction multiEQRestriction = null;
+            SingleColumnRestriction.SuperColumnKeyEQRestriction keyEQRestriction = null;
+
+            while (restrictions.hasNext())
+            {
+                Restriction restriction = restrictions.next();
+
+                if (restriction instanceof SingleColumnRestriction.SuperColumnMultiSliceRestriction)
+                    multiSliceRestriction = (SingleColumnRestriction.SuperColumnMultiSliceRestriction) restriction;
+                else if (restriction instanceof SingleColumnRestriction.SuperColumnKeySliceRestriction)
+                    keySliceRestriction = (SingleColumnRestriction.SuperColumnKeySliceRestriction) restriction;
+                else if (restriction instanceof SingleColumnRestriction.SuperColumnKeyINRestriction)
+                    keyINRestriction = (SingleColumnRestriction.SuperColumnKeyINRestriction) restriction;
+                else if (restriction instanceof SingleColumnRestriction.SuperColumnMultiEQRestriction)
+                    multiEQRestriction = (SingleColumnRestriction.SuperColumnMultiEQRestriction) restriction;
+                else if (restriction instanceof SingleColumnRestriction.SuperColumnKeyEQRestriction)
+                    keyEQRestriction = (SingleColumnRestriction.SuperColumnKeyEQRestriction) restriction;
+            }
+
+            this.multiSliceRestriction = multiSliceRestriction;
+            this.keySliceRestriction = keySliceRestriction;
+            this.keyINRestriction = keyINRestriction;
+            this.multiEQRestriction = multiEQRestriction;
+            this.keyEQRestriction = keyEQRestriction;
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/cql3/UpdateParameters.java b/src/java/org/apache/cassandra/cql3/UpdateParameters.java
index d070f61127..7d095060be 100644
--- a/src/java/org/apache/cassandra/cql3/UpdateParameters.java
+++ b/src/java/org/apache/cassandra/cql3/UpdateParameters.java
@@ -125,7 +125,7 @@ public class UpdateParameters
         // the "compact" one. As such, deleting the row or deleting that single cell is equivalent. We favor the later however
         // because that makes it easier when translating back to the old format layout (for thrift and pre-3.0 backward
         // compatibility) as we don't have to special case for the row deletion. This is also in line with what we used to do pre-3.0.
-        if (metadata.isCompactTable() && builder.clustering() != Clustering.STATIC_CLUSTERING)
+        if (metadata.isCompactTable() && builder.clustering() != Clustering.STATIC_CLUSTERING && !metadata.isSuper())
             addTombstone(metadata.compactValueColumn());
         else
             builder.addRowDeletion(Row.Deletion.regular(deletionTime));
@@ -155,6 +155,11 @@ public class UpdateParameters
     }
 
     public void addCounter(ColumnDefinition column, long increment) throws InvalidRequestException
+    {
+        addCounter(column, increment, null);
+    }
+
+    public void addCounter(ColumnDefinition column, long increment, CellPath path) throws InvalidRequestException
     {
         assert ttl == LivenessInfo.NO_TTL;
 
@@ -170,7 +175,7 @@ public class UpdateParameters
         //
         // We set counterid to a special value to differentiate between regular pre-2.0 local shards from pre-2.1 era
         // and "counter update" temporary state cells. Please see CounterContext.createUpdate() for further details.
-        builder.addCell(BufferCell.live(metadata, column, timestamp, CounterContext.instance().createUpdate(increment)));
+        builder.addCell(BufferCell.live(metadata, column, timestamp, CounterContext.instance().createUpdate(increment), path));
     }
 
     public void setComplexDeletionTime(ColumnDefinition column)
diff --git a/src/java/org/apache/cassandra/cql3/WhereClause.java b/src/java/org/apache/cassandra/cql3/WhereClause.java
index 9d4e51a711..c56c8e06c3 100644
--- a/src/java/org/apache/cassandra/cql3/WhereClause.java
+++ b/src/java/org/apache/cassandra/cql3/WhereClause.java
@@ -34,9 +34,13 @@ public final class WhereClause
 
     private WhereClause(Builder builder)
     {
-        this.relations = builder.relations.build();
-        this.expressions = builder.expressions.build();
+        this(builder.relations.build(), builder.expressions.build());
+    }
 
+    private WhereClause(List<Relation> relations, List<CustomIndexExpression> expressions)
+    {
+        this.relations = relations;
+        this.expressions = expressions;
     }
 
     public static WhereClause empty()
@@ -44,6 +48,11 @@ public final class WhereClause
         return EMPTY;
     }
 
+    public WhereClause copy(List<Relation> newRelations)
+    {
+        return new WhereClause(newRelations, expressions);
+    }
+
     public boolean containsCustomExpressions()
     {
         return !expressions.isEmpty();
diff --git a/src/java/org/apache/cassandra/cql3/restrictions/PrimaryKeyRestrictionSet.java b/src/java/org/apache/cassandra/cql3/restrictions/PrimaryKeyRestrictionSet.java
index a5f4a2412a..860d3f0845 100644
--- a/src/java/org/apache/cassandra/cql3/restrictions/PrimaryKeyRestrictionSet.java
+++ b/src/java/org/apache/cassandra/cql3/restrictions/PrimaryKeyRestrictionSet.java
@@ -31,8 +31,6 @@ import org.apache.cassandra.exceptions.InvalidRequestException;
 import org.apache.cassandra.index.SecondaryIndexManager;
 import org.apache.cassandra.utils.btree.BTreeSet;
 
-import static org.apache.cassandra.cql3.statements.RequestValidations.invalidRequest;
-
 /**
  * A set of single column restrictions on a primary key part (partition key or clustering key).
  */
diff --git a/src/java/org/apache/cassandra/cql3/restrictions/SingleColumnRestriction.java b/src/java/org/apache/cassandra/cql3/restrictions/SingleColumnRestriction.java
index 6296b97106..5985962e59 100644
--- a/src/java/org/apache/cassandra/cql3/restrictions/SingleColumnRestriction.java
+++ b/src/java/org/apache/cassandra/cql3/restrictions/SingleColumnRestriction.java
@@ -118,9 +118,9 @@ public abstract class SingleColumnRestriction extends AbstractRestriction
      */
     protected abstract boolean isSupportedBy(Index index);
 
-    public static final class EQRestriction extends SingleColumnRestriction
+    public static class EQRestriction extends SingleColumnRestriction
     {
-        private final Term value;
+        public final Term value;
 
         public EQRestriction(ColumnDefinition columnDef, Term value)
         {
@@ -308,7 +308,7 @@ public abstract class SingleColumnRestriction extends AbstractRestriction
 
     public static class SliceRestriction extends SingleColumnRestriction
     {
-        private final TermSlice slice;
+        public final TermSlice slice;
 
         public SliceRestriction(ColumnDefinition columnDef, Bound bound, boolean inclusive, Term term)
         {
@@ -404,7 +404,7 @@ public abstract class SingleColumnRestriction extends AbstractRestriction
             return String.format("SLICE%s", slice);
         }
 
-        private SliceRestriction(ColumnDefinition columnDef, TermSlice slice)
+        SliceRestriction(ColumnDefinition columnDef, TermSlice slice)
         {
             super(columnDef);
             this.slice = slice;
@@ -646,4 +646,202 @@ public abstract class SingleColumnRestriction extends AbstractRestriction
             return index.supportsExpression(columnDef, Operator.IS_NOT);
         }
     }
+
+    /**
+     * Super Column Compatibiltiy
+     */
+
+    public static class SuperColumnMultiEQRestriction extends EQRestriction
+    {
+        public ByteBuffer firstValue;
+        public ByteBuffer secondValue;
+
+        public SuperColumnMultiEQRestriction(ColumnDefinition columnDef, Term value)
+        {
+            super(columnDef, value);
+        }
+
+        @Override
+        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)
+        {
+            Term term = value.bind(options);
+
+            assert (term instanceof Tuples.Value);
+            firstValue = ((Tuples.Value)term).getElements().get(0);
+            secondValue = ((Tuples.Value)term).getElements().get(1);
+
+            builder.addElementToAll(firstValue);
+            checkFalse(builder.containsNull(), "Invalid null value in condition for column %s", columnDef.name);
+            checkFalse(builder.containsUnset(), "Invalid unset value for column %s", columnDef.name);
+            return builder;
+        }
+    }
+
+    public static class SuperColumnMultiSliceRestriction extends SliceRestriction
+    {
+        public ByteBuffer firstValue;
+        public ByteBuffer secondValue;
+
+        // These are here to avoid polluting SliceRestriction
+        public final Bound bound;
+        public final boolean trueInclusive;
+        public SuperColumnMultiSliceRestriction(ColumnDefinition columnDef, Bound bound, boolean inclusive, Term term)
+        {
+            super(columnDef, bound, true, term);
+            this.bound = bound;
+            this.trueInclusive = inclusive;
+
+        }
+
+        @Override
+        public MultiCBuilder appendBoundTo(MultiCBuilder builder, Bound bound, QueryOptions options)
+        {
+            Bound b = reverseBoundIfNeeded(getFirstColumn(), bound);
+
+            if (!hasBound(b))
+                return builder;
+
+            Term term = slice.bound(b);
+
+            assert (term instanceof Tuples.Value);
+            firstValue = ((Tuples.Value)term).getElements().get(0);
+            secondValue = ((Tuples.Value)term).getElements().get(1);
+
+            checkBindValueSet(firstValue, "Invalid unset value for column %s", columnDef.name);
+            checkBindValueSet(secondValue, "Invalid unset value for column %s", columnDef.name);
+            return builder.addElementToAll(firstValue);
+
+        }
+    }
+
+    public static final class SuperColumnKeyEQRestriction extends EQRestriction
+    {
+        public SuperColumnKeyEQRestriction(ColumnDefinition columnDef, Term value)
+        {
+            super(columnDef, value);
+        }
+
+        public ByteBuffer bindValue(QueryOptions options)
+        {
+            return value.bindAndGet(options);
+        }
+
+        @Override
+        public MultiCBuilder appendBoundTo(MultiCBuilder builder, Bound bound, QueryOptions options)
+        {
+            // no-op
+            return builder;
+        }
+
+        @Override
+        public void addRowFilterTo(RowFilter filter, SecondaryIndexManager indexManager, QueryOptions options) throws InvalidRequestException
+        {
+            // no-op
+        }
+    }
+
+    public static abstract class SuperColumnKeyINRestriction extends INRestriction
+    {
+        public SuperColumnKeyINRestriction(ColumnDefinition columnDef)
+        {
+            super(columnDef);
+        }
+
+        @Override
+        public MultiCBuilder appendTo(MultiCBuilder builder, QueryOptions options)
+        {
+            // no-op
+            return builder;
+        }
+
+        @Override
+        public void addRowFilterTo(RowFilter filter,
+                                   SecondaryIndexManager indexManager,
+                                   QueryOptions options) throws InvalidRequestException
+        {
+            // no-op
+        }
+
+        public void addFunctionsTo(List<Function> functions)
+        {
+            // no-op
+        }
+
+        MultiColumnRestriction toMultiColumnRestriction()
+        {
+            // no-op
+            return null;
+        }
+
+        public abstract List<ByteBuffer> getValues(QueryOptions options) throws InvalidRequestException;
+    }
+
+    public static class SuperColumnKeyINRestrictionWithMarkers extends SuperColumnKeyINRestriction
+    {
+        protected final AbstractMarker marker;
+
+        public SuperColumnKeyINRestrictionWithMarkers(ColumnDefinition columnDef, AbstractMarker marker)
+        {
+            super(columnDef);
+            this.marker = marker;
+        }
+
+        public List<ByteBuffer> getValues(QueryOptions options) throws InvalidRequestException
+        {
+            Terminal term = marker.bind(options);
+            checkNotNull(term, "Invalid null value for column %s", columnDef.name);
+            checkFalse(term == Constants.UNSET_VALUE, "Invalid unset value for column %s", columnDef.name);
+            Term.MultiItemTerminal lval = (Term.MultiItemTerminal) term;
+            return lval.getElements();
+        }
+    }
+
+    public static class SuperColumnKeyINRestrictionWithValues extends SuperColumnKeyINRestriction
+    {
+        private final List<Term> values;
+
+        public SuperColumnKeyINRestrictionWithValues(ColumnDefinition columnDef, List<Term> values)
+        {
+            super(columnDef);
+            this.values = values;
+        }
+
+        public List<ByteBuffer> getValues(QueryOptions options) throws InvalidRequestException
+        {
+            List<ByteBuffer> buffers = new ArrayList<>(values.size());
+            for (Term value : values)
+                buffers.add(value.bindAndGet(options));
+            return buffers;
+        }
+    }
+
+    public static class SuperColumnKeySliceRestriction extends SliceRestriction
+    {
+        // These are here to avoid polluting SliceRestriction
+        private Term term;
+
+        public SuperColumnKeySliceRestriction(ColumnDefinition columnDef, Bound bound, boolean inclusive, Term term)
+        {
+            super(columnDef, bound, inclusive, term);
+            this.term = term;
+        }
+
+        public ByteBuffer bindValue(QueryOptions options)
+        {
+            return term.bindAndGet(options);
+        }
+
+        @Override
+        public MultiCBuilder appendBoundTo(MultiCBuilder builder, Bound bound, QueryOptions options)
+        {
+            // no-op
+            return builder;
+        }
+
+        @Override
+        public void addRowFilterTo(RowFilter filter, SecondaryIndexManager indexManager, QueryOptions options) throws InvalidRequestException
+        {
+            // no-op
+        }
+    }
 }
diff --git a/src/java/org/apache/cassandra/cql3/restrictions/StatementRestrictions.java b/src/java/org/apache/cassandra/cql3/restrictions/StatementRestrictions.java
index d025d8af02..84c69584cc 100644
--- a/src/java/org/apache/cassandra/cql3/restrictions/StatementRestrictions.java
+++ b/src/java/org/apache/cassandra/cql3/restrictions/StatementRestrictions.java
@@ -21,6 +21,7 @@ import java.nio.ByteBuffer;
 import java.util.*;
 
 import com.google.common.base.Joiner;
+import com.google.common.collect.Iterators;
 
 import org.apache.cassandra.config.CFMetaData;
 import org.apache.cassandra.config.ColumnDefinition;
@@ -160,7 +161,10 @@ public final class StatementRestrictions
             }
             else
             {
-                addRestriction(relation.toRestriction(cfm, boundNames));
+                if (cfm.isSuper() && cfm.isDense() && !relation.onToken())
+                    addRestriction(relation.toSuperColumnAdapter().toRestriction(cfm, boundNames));
+                else
+                    addRestriction(relation.toRestriction(cfm, boundNames));
             }
         }
 
@@ -233,9 +237,16 @@ public final class StatementRestrictions
                                      Joiner.on(", ").join(nonPrimaryKeyColumns));
             }
             if (hasQueriableIndex)
+            {
                 usesSecondaryIndexing = true;
-            else if (!allowFiltering)
+            }
+            else if (!allowFiltering && !cfm.isSuper())
+            {
                 throw invalidRequest(StatementRestrictions.REQUIRES_ALLOW_FILTERING_MESSAGE);
+            }
+
+            checkFalse(clusteringColumnsRestrictions.isEmpty() && cfm.isSuper(),
+                       "Filtering is not supported on SuperColumn tables");
 
             indexRestrictions.add(nonPrimaryKeyRestrictions);
         }
@@ -847,4 +858,15 @@ public final class StatementRestrictions
                && (clusteringColumnsRestrictions.isEQ() || clusteringColumnsRestrictions.isIN());
     }
 
+
+    private SuperColumnCompatibility.SuperColumnRestrictions cached;
+    public SuperColumnCompatibility.SuperColumnRestrictions getSuperColumnRestrictions()
+    {
+        assert cfm.isSuper() && cfm.isDense();
+
+        if (cached == null)
+            cached = new SuperColumnCompatibility.SuperColumnRestrictions(Iterators.concat(((PrimaryKeyRestrictionSet) clusteringColumnsRestrictions).iterator(),
+                                                                                           nonPrimaryKeyRestrictions.iterator()));
+        return cached;
+    }
 }
diff --git a/src/java/org/apache/cassandra/cql3/restrictions/TermSlice.java b/src/java/org/apache/cassandra/cql3/restrictions/TermSlice.java
index ac66b961ab..4b138770f9 100644
--- a/src/java/org/apache/cassandra/cql3/restrictions/TermSlice.java
+++ b/src/java/org/apache/cassandra/cql3/restrictions/TermSlice.java
@@ -26,7 +26,7 @@ import org.apache.cassandra.cql3.functions.Function;
 import org.apache.cassandra.cql3.statements.Bound;
 import org.apache.cassandra.index.Index;
 
-final class TermSlice
+public final class TermSlice
 {
     /**
      * The slice boundaries.
diff --git a/src/java/org/apache/cassandra/cql3/statements/CreateViewStatement.java b/src/java/org/apache/cassandra/cql3/statements/CreateViewStatement.java
index 708d551eac..47304b6d0d 100644
--- a/src/java/org/apache/cassandra/cql3/statements/CreateViewStatement.java
+++ b/src/java/org/apache/cassandra/cql3/statements/CreateViewStatement.java
@@ -137,6 +137,8 @@ public class CreateViewStatement extends SchemaAlteringStatement
 
         if (cfm.isCounter())
             throw new InvalidRequestException("Materialized views are not supported on counter tables");
+        if (cfm.isSuper())
+            throw new InvalidRequestException("Materialized views are not supported on SuperColumn tables");
         if (cfm.isView())
             throw new InvalidRequestException("Materialized views cannot be created against other materialized views");
 
diff --git a/src/java/org/apache/cassandra/cql3/statements/DeleteStatement.java b/src/java/org/apache/cassandra/cql3/statements/DeleteStatement.java
index 4888b43fcb..a0919d7a1f 100644
--- a/src/java/org/apache/cassandra/cql3/statements/DeleteStatement.java
+++ b/src/java/org/apache/cassandra/cql3/statements/DeleteStatement.java
@@ -116,7 +116,7 @@ public class DeleteStatement extends ModificationStatement
     public static class Parsed extends ModificationStatement.Parsed
     {
         private final List<Operation.RawDeletion> deletions;
-        private final WhereClause whereClause;
+        private WhereClause whereClause;
 
         public Parsed(CFName name,
                       Attributes.Raw attrs,
@@ -139,17 +139,25 @@ public class DeleteStatement extends ModificationStatement
         {
             Operations operations = new Operations(type);
 
-            for (Operation.RawDeletion deletion : deletions)
+            if (cfm.isSuper() && cfm.isDense())
             {
-                ColumnDefinition def = getColumnDefinition(cfm, deletion.affectedColumn());
-
-                // For compact, we only have one value except the key, so the only form of DELETE that make sense is without a column
-                // list. However, we support having the value name for coherence with the static/sparse case
-                checkFalse(def.isPrimaryKeyColumn(), "Invalid identifier %s for deletion (should not be a PRIMARY KEY part)", def.name);
-
-                Operation op = deletion.prepare(cfm.ksName, def);
-                op.collectMarkerSpecification(boundNames);
-                operations.add(op);
+                conditions = SuperColumnCompatibility.rebuildLWTColumnConditions(conditions, cfm, whereClause);
+                whereClause = SuperColumnCompatibility.prepareDeleteOperations(cfm, whereClause, boundNames, operations);
+            }
+            else
+            {
+                for (Operation.RawDeletion deletion : deletions)
+                {
+                    ColumnDefinition def = getColumnDefinition(cfm, deletion.affectedColumn());
+
+                    // For compact, we only have one value except the key, so the only form of DELETE that make sense is without a column
+                    // list. However, we support having the value name for coherence with the static/sparse case
+                    checkFalse(def.isPrimaryKeyColumn(), "Invalid identifier %s for deletion (should not be a PRIMARY KEY part)", def.name);
+
+                    Operation op = deletion.prepare(cfm.ksName, def);
+                    op.collectMarkerSpecification(boundNames);
+                    operations.add(op);
+                }
             }
 
             StatementRestrictions restrictions = newRestrictions(cfm,
diff --git a/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java b/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java
index 0afd34d13a..28fc90f5ee 100644
--- a/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java
+++ b/src/java/org/apache/cassandra/cql3/statements/ModificationStatement.java
@@ -544,10 +544,19 @@ public abstract class ModificationStatement implements CQLStatement
                 defs.addAll(cfm.partitionKeyColumns());
                 defs.addAll(cfm.clusteringColumns());
             }
-            for (ColumnDefinition def : columnsWithConditions)
-                defs.add(def);
-            selection = Selection.forColumns(cfm, new ArrayList<>(defs));
 
+
+            if (cfm.isSuper() && cfm.isDense())
+            {
+                defs.add(cfm.superColumnValueColumn());
+            }
+            else
+            {
+                for (ColumnDefinition def : columnsWithConditions)
+                    defs.add(def);
+            }
+
+            selection = Selection.forColumns(cfm, new ArrayList<>(defs));
         }
 
         Selection.ResultSetBuilder builder = selection.resultSetBuilder(false);
diff --git a/src/java/org/apache/cassandra/cql3/statements/SelectStatement.java b/src/java/org/apache/cassandra/cql3/statements/SelectStatement.java
index 3882a23ba1..2e090fa338 100644
--- a/src/java/org/apache/cassandra/cql3/statements/SelectStatement.java
+++ b/src/java/org/apache/cassandra/cql3/statements/SelectStatement.java
@@ -18,7 +18,16 @@
 package org.apache.cassandra.cql3.statements;
 
 import java.nio.ByteBuffer;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.Iterator;
+import java.util.List;
+import java.util.Map;
+import java.util.NavigableSet;
+import java.util.SortedSet;
 
 import com.google.common.base.Objects;
 import com.google.common.base.Predicate;
@@ -29,13 +38,40 @@ import org.slf4j.LoggerFactory;
 import org.apache.cassandra.auth.Permission;
 import org.apache.cassandra.config.CFMetaData;
 import org.apache.cassandra.config.ColumnDefinition;
-import org.apache.cassandra.cql3.*;
+import org.apache.cassandra.cql3.CFName;
+import org.apache.cassandra.cql3.CQLStatement;
+import org.apache.cassandra.cql3.ColumnIdentifier;
+import org.apache.cassandra.cql3.ColumnSpecification;
+import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.cql3.QueryProcessor;
+import org.apache.cassandra.cql3.ResultSet;
+import org.apache.cassandra.cql3.SuperColumnCompatibility;
+import org.apache.cassandra.cql3.Term;
+import org.apache.cassandra.cql3.VariableSpecifications;
+import org.apache.cassandra.cql3.WhereClause;
 import org.apache.cassandra.cql3.functions.Function;
 import org.apache.cassandra.cql3.restrictions.StatementRestrictions;
 import org.apache.cassandra.cql3.selection.RawSelector;
 import org.apache.cassandra.cql3.selection.Selection;
-import org.apache.cassandra.db.*;
-import org.apache.cassandra.db.filter.*;
+import org.apache.cassandra.db.Clustering;
+import org.apache.cassandra.db.ColumnFamilyStore;
+import org.apache.cassandra.db.ConsistencyLevel;
+import org.apache.cassandra.db.DataRange;
+import org.apache.cassandra.db.DecoratedKey;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.PartitionPosition;
+import org.apache.cassandra.db.PartitionRangeReadCommand;
+import org.apache.cassandra.db.ReadOrderGroup;
+import org.apache.cassandra.db.ReadQuery;
+import org.apache.cassandra.db.SinglePartitionReadCommand;
+import org.apache.cassandra.db.Slice;
+import org.apache.cassandra.db.Slices;
+import org.apache.cassandra.db.filter.ClusteringIndexFilter;
+import org.apache.cassandra.db.filter.ClusteringIndexNamesFilter;
+import org.apache.cassandra.db.filter.ClusteringIndexSliceFilter;
+import org.apache.cassandra.db.filter.ColumnFilter;
+import org.apache.cassandra.db.filter.DataLimits;
+import org.apache.cassandra.db.filter.RowFilter;
 import org.apache.cassandra.db.marshal.CollectionType;
 import org.apache.cassandra.db.marshal.CompositeType;
 import org.apache.cassandra.db.marshal.Int32Type;
@@ -45,7 +81,11 @@ import org.apache.cassandra.db.rows.Row;
 import org.apache.cassandra.db.rows.RowIterator;
 import org.apache.cassandra.db.view.View;
 import org.apache.cassandra.dht.AbstractBounds;
-import org.apache.cassandra.exceptions.*;
+import org.apache.cassandra.exceptions.InvalidRequestException;
+import org.apache.cassandra.exceptions.RequestExecutionException;
+import org.apache.cassandra.exceptions.RequestValidationException;
+import org.apache.cassandra.exceptions.UnauthorizedException;
+import org.apache.cassandra.exceptions.UnrecognizedEntityException;
 import org.apache.cassandra.index.SecondaryIndexManager;
 import org.apache.cassandra.serializers.MarshalException;
 import org.apache.cassandra.service.ClientState;
@@ -479,7 +519,8 @@ public class SelectStatement implements CQLStatement
         {
             QueryProcessor.validateKey(key);
             DecoratedKey dk = cfm.decorateKey(ByteBufferUtil.clone(key));
-            commands.add(SinglePartitionReadCommand.create(cfm, nowInSec, queriedColumns, rowFilter, limit, dk, filter));
+            ColumnFilter cf = (cfm.isSuper() && cfm.isDense()) ? SuperColumnCompatibility.getColumnFilter(cfm, options, restrictions.getSuperColumnRestrictions()) : queriedColumns;
+            commands.add(SinglePartitionReadCommand.create(cfm, nowInSec, cf, rowFilter, limit, dk, filter));
         }
 
         return new SinglePartitionReadCommand.Group(commands, limit);
@@ -730,6 +771,12 @@ public class SelectStatement implements CQLStatement
     void processPartition(RowIterator partition, QueryOptions options, Selection.ResultSetBuilder result, int nowInSec)
     throws InvalidRequestException
     {
+        if (cfm.isSuper() && cfm.isDense())
+        {
+            SuperColumnCompatibility.processPartition(cfm, selection, partition, result, options.getProtocolVersion(), restrictions.getSuperColumnRestrictions(), options);
+            return;
+        }
+
         int protocolVersion = options.getProtocolVersion();
 
         ByteBuffer[] keyComponents = getComponents(cfm, partition.partitionKey());
diff --git a/src/java/org/apache/cassandra/cql3/statements/UpdateStatement.java b/src/java/org/apache/cassandra/cql3/statements/UpdateStatement.java
index 6f872d4644..641b6bbc31 100644
--- a/src/java/org/apache/cassandra/cql3/statements/UpdateStatement.java
+++ b/src/java/org/apache/cassandra/cql3/statements/UpdateStatement.java
@@ -17,7 +17,6 @@
  */
 package org.apache.cassandra.cql3.statements;
 
-import java.util.Collection;
 import java.util.Collections;
 import java.util.List;
 
@@ -32,6 +31,7 @@ import org.apache.cassandra.db.partitions.PartitionUpdate;
 import org.apache.cassandra.utils.ByteBufferUtil;
 import org.apache.cassandra.utils.Pair;
 
+import static com.google.common.collect.Lists.newArrayList;
 import static org.apache.cassandra.cql3.statements.RequestValidations.checkContainsNoDuplicates;
 import static org.apache.cassandra.cql3.statements.RequestValidations.checkFalse;
 import static org.apache.cassandra.cql3.statements.RequestValidations.checkTrue;
@@ -81,7 +81,7 @@ public class UpdateStatement extends ModificationStatement
             // For a dense layout, when we translate it to thrift, we don't have a row marker. So we don't accept an insert/update
             // that only sets the PK unless the is no declared non-PK columns (which we recognize because in that case the compact
             // value is of type "EmptyType").
-            if (cfm.isCompactTable() && updates.isEmpty())
+            if ((cfm.isCompactTable() && !cfm.isSuper()) && updates.isEmpty())
             {
                 checkTrue(CompactTables.hasEmptyCompactValue(cfm),
                           "Column %s is mandatory for this COMPACT STORAGE table",
@@ -155,24 +155,33 @@ public class UpdateStatement extends ModificationStatement
             Operations operations = new Operations(type);
             boolean hasClusteringColumnsSet = false;
 
-            for (int i = 0; i < columnNames.size(); i++)
+            if (cfm.isSuper() && cfm.isDense())
             {
-                ColumnDefinition def = getColumnDefinition(cfm, columnNames.get(i));
-
-                if (def.isClusteringColumn())
-                    hasClusteringColumnsSet = true;
-
-                Term.Raw value = columnValues.get(i);
-
-                if (def.isPrimaryKeyColumn())
-                {
-                    whereClause.add(new SingleColumnRelation(columnNames.get(i), Operator.EQ, value));
-                }
-                else
+                // SuperColumn familiy updates are always row-level
+                hasClusteringColumnsSet = true;
+                SuperColumnCompatibility.prepareInsertOperations(cfm, columnNames, whereClause, columnValues, boundNames, operations);
+            }
+            else
+            {
+                for (int i = 0; i < columnNames.size(); i++)
                 {
-                    Operation operation = new Operation.SetValue(value).prepare(keyspace(), def);
-                    operation.collectMarkerSpecification(boundNames);
-                    operations.add(operation);
+                    ColumnDefinition def = getColumnDefinition(cfm, columnNames.get(i));
+
+                    if (def.isClusteringColumn())
+                        hasClusteringColumnsSet = true;
+
+                    Term.Raw value = columnValues.get(i);
+
+                    if (def.isPrimaryKeyColumn())
+                    {
+                        whereClause.add(new SingleColumnRelation(columnNames.get(i), Operator.EQ, value));
+                    }
+                    else
+                    {
+                        Operation operation = new Operation.SetValue(value).prepare(cfm.ksName, def);
+                        operation.collectMarkerSpecification(boundNames);
+                        operations.add(operation);
+                    }
                 }
             }
 
@@ -218,30 +227,36 @@ public class UpdateStatement extends ModificationStatement
         {
             checkFalse(cfm.isCounter(), "INSERT statements are not allowed on counter tables, use UPDATE instead");
 
-            Collection<ColumnDefinition> defs = cfm.allColumns();
+            List<ColumnDefinition> defs = newArrayList(cfm.allColumnsInSelectOrder());
             Json.Prepared prepared = jsonValue.prepareAndCollectMarkers(cfm, defs, boundNames);
 
             WhereClause.Builder whereClause = new WhereClause.Builder();
             Operations operations = new Operations(type);
             boolean hasClusteringColumnsSet = false;
 
-            for (ColumnDefinition def : defs)
+            if (cfm.isSuper() && cfm.isDense())
             {
-                if (def.isClusteringColumn())
-                    hasClusteringColumnsSet = true;
-
-                Term.Raw raw = prepared.getRawTermForColumn(def);
-                if (def.isPrimaryKeyColumn())
-                {
-                    whereClause.add(new SingleColumnRelation(new ColumnIdentifier.ColumnIdentifierValue(def.name),
-                                                             Operator.EQ,
-                                                             raw));
-                }
-                else
+                hasClusteringColumnsSet = true;
+                SuperColumnCompatibility.prepareInsertJSONOperations(cfm, defs, boundNames, prepared, whereClause, operations);
+            }
+            else
+            {
+                for (ColumnDefinition def : defs)
                 {
-                    Operation operation = new Operation.SetValue(raw).prepare(keyspace(), def);
-                    operation.collectMarkerSpecification(boundNames);
-                    operations.add(operation);
+                    if (def.isClusteringColumn())
+                        hasClusteringColumnsSet = true;
+
+                    Term.Raw raw = prepared.getRawTermForColumn(def);
+                    if (def.isPrimaryKeyColumn())
+                    {
+                        whereClause.add(new SingleColumnRelation(new ColumnIdentifier.ColumnIdentifierValue(def.name), Operator.EQ, raw));
+                    }
+                    else
+                    {
+                        Operation operation = new Operation.SetValue(raw).prepare(cfm.ksName, def);
+                        operation.collectMarkerSpecification(boundNames);
+                        operations.add(operation);
+                    }
                 }
             }
 
@@ -270,7 +285,7 @@ public class UpdateStatement extends ModificationStatement
     {
         // Provided for an UPDATE
         private final List<Pair<ColumnIdentifier.Raw, Operation.RawUpdate>> updates;
-        private final WhereClause whereClause;
+        private WhereClause whereClause;
 
         /**
          * Creates a new UpdateStatement from a column family name, columns map, consistency
@@ -302,17 +317,25 @@ public class UpdateStatement extends ModificationStatement
         {
             Operations operations = new Operations(type);
 
-            for (Pair<ColumnIdentifier.Raw, Operation.RawUpdate> entry : updates)
+            if (cfm.isSuper() && cfm.isDense())
+            {
+                conditions = SuperColumnCompatibility.rebuildLWTColumnConditions(conditions, cfm, whereClause);
+                whereClause = SuperColumnCompatibility.prepareUpdateOperations(cfm, whereClause, updates, boundNames, operations);
+            }
+            else
             {
-                ColumnDefinition def = getColumnDefinition(cfm, entry.left);
+                for (Pair<ColumnIdentifier.Raw, Operation.RawUpdate> entry : updates)
+                {
+                    ColumnDefinition def = getColumnDefinition(cfm, entry.left);
 
-                checkFalse(def.isPrimaryKeyColumn(), "PRIMARY KEY part %s found in SET part", def.name);
+                    checkFalse(def.isPrimaryKeyColumn(), "PRIMARY KEY part %s found in SET part", def.name);
 
-                Operation operation = entry.right.prepare(keyspace(), def);
-                operation.collectMarkerSpecification(boundNames);
-                operations.add(operation);
+                    Operation operation = entry.right.prepare(cfm.ksName, def);
+                    operation.collectMarkerSpecification(boundNames);
+                    operations.add(operation);
+                }
             }
-
+            
             StatementRestrictions restrictions = newRestrictions(cfm,
                                                                  boundNames,
                                                                  operations,
diff --git a/src/java/org/apache/cassandra/db/Columns.java b/src/java/org/apache/cassandra/db/Columns.java
index cad295c495..eb4f7611ad 100644
--- a/src/java/org/apache/cassandra/db/Columns.java
+++ b/src/java/org/apache/cassandra/db/Columns.java
@@ -422,6 +422,7 @@ public class Columns extends AbstractCollection<ColumnDefinition> implements Col
             {
                 ByteBuffer name = ByteBufferUtil.readWithVIntLength(in);
                 ColumnDefinition column = metadata.getColumnDefinition(name);
+
                 if (column == null)
                 {
                     // If we don't find the definition, it could be we have data for a dropped column, and we shouldn't
diff --git a/src/java/org/apache/cassandra/db/CompactTables.java b/src/java/org/apache/cassandra/db/CompactTables.java
index e31fda3766..9da4d94261 100644
--- a/src/java/org/apache/cassandra/db/CompactTables.java
+++ b/src/java/org/apache/cassandra/db/CompactTables.java
@@ -17,13 +17,15 @@
  */
 package org.apache.cassandra.db;
 
-import java.nio.ByteBuffer;
-import java.util.*;
+import java.util.HashSet;
+import java.util.Set;
 
 import org.apache.cassandra.config.CFMetaData;
 import org.apache.cassandra.config.ColumnDefinition;
-import org.apache.cassandra.db.marshal.*;
-import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.cql3.SuperColumnCompatibility;
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.db.marshal.EmptyType;
+import org.apache.cassandra.db.marshal.UTF8Type;
 
 /**
  * Small utility methods pertaining to the encoding of COMPACT STORAGE tables.
@@ -54,39 +56,14 @@ import org.apache.cassandra.utils.ByteBufferUtil;
  * On variation is that if the table comparator is a CompositeType, then the underlying table will have one clustering column by
  * element of the CompositeType, but the rest of the layout is as above.
  *
- * As far as thrift is concerned, one exception to this is super column families, which have a different layout. Namely, a super
- * column families is encoded with:
- *   CREATE TABLE super (
- *      key [key_validation_class],
- *      super_column_name [comparator],
- *      [column_metadata_1] [type1],
- *      ...,
- *      [column_metadata_n] [type1],
- *      "" map<[sub_comparator], [default_validation_class]>
- *      PRIMARY KEY (key, super_column_name)
- *   )
- * In other words, every super column is encoded by a row. That row has one column for each defined "column_metadata", but it also
- * has a special map column (whose name is the empty string as this is guaranteed to never conflict with a user-defined
- * "column_metadata") which stores the super column "dynamic" sub-columns.
+ * SuperColumn families handling and detailed format description can be found in {@code SuperColumnCompatibility}.
  */
 public abstract class CompactTables
 {
-    // We use an empty value for the 1) this can't conflict with a user-defined column and 2) this actually
-    // validate with any comparator which makes it convenient for columnDefinitionComparator().
-    public static final ByteBuffer SUPER_COLUMN_MAP_COLUMN = ByteBufferUtil.EMPTY_BYTE_BUFFER;
-    public static final String SUPER_COLUMN_MAP_COLUMN_STR = UTF8Type.instance.compose(SUPER_COLUMN_MAP_COLUMN);
-
     private CompactTables() {}
 
-    public static ColumnDefinition getCompactValueColumn(PartitionColumns columns, boolean isSuper)
+    public static ColumnDefinition getCompactValueColumn(PartitionColumns columns)
     {
-        if (isSuper)
-        {
-            for (ColumnDefinition column : columns.regulars)
-                if (column.name.bytes.equals(SUPER_COLUMN_MAP_COLUMN))
-                    return column;
-            throw new AssertionError("Invalid super column table definition, no 'dynamic' map column");
-        }
         assert columns.regulars.simpleColumnCount() == 1 && columns.regulars.complexColumnCount() == 0;
         return columns.regulars.getSimple(0);
     }
@@ -104,11 +81,6 @@ public abstract class CompactTables
         return metadata.compactValueColumn().type instanceof EmptyType;
     }
 
-    public static boolean isSuperColumnMapColumn(ColumnDefinition column)
-    {
-        return column.kind == ColumnDefinition.Kind.REGULAR && column.name.bytes.equals(SUPER_COLUMN_MAP_COLUMN);
-    }
-
     public static DefaultNames defaultNameGenerator(Set<String> usedNames)
     {
         return new DefaultNames(new HashSet<String>(usedNames));
diff --git a/src/java/org/apache/cassandra/db/LegacyLayout.java b/src/java/org/apache/cassandra/db/LegacyLayout.java
index 4f7bc22ae9..40b9fd3645 100644
--- a/src/java/org/apache/cassandra/db/LegacyLayout.java
+++ b/src/java/org/apache/cassandra/db/LegacyLayout.java
@@ -24,6 +24,7 @@ import java.nio.ByteBuffer;
 import java.security.MessageDigest;
 import java.util.*;
 
+import org.apache.cassandra.cql3.SuperColumnCompatibility;
 import org.apache.cassandra.utils.AbstractIterator;
 import com.google.common.collect.Iterators;
 import com.google.common.collect.Lists;
@@ -298,7 +299,7 @@ public abstract class LegacyLayout
             // What it is depends if this a cell for a declared "static" column or a "dynamic" column part of the
             // super-column internal map.
             assert columnName != null; // This should never be null for supercolumns, see decodeForSuperColumn() above
-            values[clusteringSize] = columnName.equals(CompactTables.SUPER_COLUMN_MAP_COLUMN)
+            values[clusteringSize] = columnName.equals(SuperColumnCompatibility.SUPER_COLUMN_MAP_COLUMN)
                                    ? collectionElement
                                    : columnName;
         }
diff --git a/src/java/org/apache/cassandra/db/SerializationHeader.java b/src/java/org/apache/cassandra/db/SerializationHeader.java
index 494c2a3d2f..b2ed26e72a 100644
--- a/src/java/org/apache/cassandra/db/SerializationHeader.java
+++ b/src/java/org/apache/cassandra/db/SerializationHeader.java
@@ -332,6 +332,7 @@ public class SerializationHeader
             for (ByteBuffer name : typeMap.keySet())
             {
                 ColumnDefinition column = metadata.getColumnDefinition(name);
+
                 if (column == null)
                 {
                     // TODO: this imply we don't read data for a column we don't yet know about, which imply this is theoretically
diff --git a/src/java/org/apache/cassandra/schema/LegacySchemaMigrator.java b/src/java/org/apache/cassandra/schema/LegacySchemaMigrator.java
index ac9cfd9538..01c5e3ec52 100644
--- a/src/java/org/apache/cassandra/schema/LegacySchemaMigrator.java
+++ b/src/java/org/apache/cassandra/schema/LegacySchemaMigrator.java
@@ -30,6 +30,7 @@ import org.slf4j.LoggerFactory;
 import org.apache.cassandra.config.*;
 import org.apache.cassandra.cql3.ColumnIdentifier;
 import org.apache.cassandra.cql3.QueryProcessor;
+import org.apache.cassandra.cql3.SuperColumnCompatibility;
 import org.apache.cassandra.cql3.UntypedResultSet;
 import org.apache.cassandra.cql3.functions.FunctionName;
 import org.apache.cassandra.cql3.functions.UDAggregate;
@@ -271,10 +272,11 @@ public final class LegacySchemaMigrator
                                       SystemKeyspace.LEGACY_TRIGGERS);
         UntypedResultSet triggerRows = query(triggersQuery, keyspaceName, tableName);
 
-        return decodeTableMetadata(tableRow, columnRows, triggerRows);
+        return decodeTableMetadata(tableName, tableRow, columnRows, triggerRows);
     }
 
-    private static CFMetaData decodeTableMetadata(UntypedResultSet.Row tableRow,
+    private static CFMetaData decodeTableMetadata(String tableName,
+                                                  UntypedResultSet.Row tableRow,
                                                   UntypedResultSet columnRows,
                                                   UntypedResultSet triggerRows)
     {
@@ -297,7 +299,7 @@ public final class LegacySchemaMigrator
         if (rawIsDense != null && !rawIsDense)
             isDense = false;
         else
-            isDense = calculateIsDense(rawComparator, columnRows);
+            isDense = calculateIsDense(rawComparator, columnRows, isSuper);
 
         // now, if switched to sparse, remove redundant compact_value column and the last clustering column,
         // directly copying CASSANDRA-11502 logic. See CASSANDRA-11315.
@@ -389,7 +391,7 @@ public final class LegacySchemaMigrator
      * information for table just created through thrift, nor for table prior to CASSANDRA-7744, so this
      * method does its best to infer whether the table is dense or not based on other elements.
      */
-    private static boolean calculateIsDense(AbstractType<?> comparator, UntypedResultSet columnRows)
+    private static boolean calculateIsDense(AbstractType<?> comparator, UntypedResultSet columnRows, boolean isSuper)
     {
         /*
          * As said above, this method is only here because we need to deal with thrift upgrades.
@@ -411,8 +413,15 @@ public final class LegacySchemaMigrator
          * in the latter case only if the comparator is exactly CompositeType(UTF8Type).
          */
         for (UntypedResultSet.Row columnRow : columnRows)
+        {
             if ("regular".equals(columnRow.getString("type")))
                 return false;
+        }
+
+        // If we've checked the columns for supercf and found no regulars, it's dense. Relying on the emptiness
+        // of the value column is not enough due to index calculation.
+        if (isSuper)
+            return true;
 
         int maxClusteringIdx = -1;
         for (UntypedResultSet.Row columnRow : columnRows)
@@ -431,18 +440,11 @@ public final class LegacySchemaMigrator
         {
             String kind = columnRow.getString("type");
 
-            if ("compact_value".equals(kind))
+            if (!isSuper && "compact_value".equals(kind))
                 continue;
 
-            if ("clustering_key".equals(kind))
-            {
-                int position = columnRow.has("component_index") ? columnRow.getInt("component_index") : 0;
-                if (isSuper && position != 0)
-                    continue;
-
-                if (!isSuper && !isCompound)
-                    continue;
-            }
+            if ("clustering_key".equals(kind) && !isSuper && !isCompound)
+                continue;
 
             filteredRows.add(columnRow);
         }
@@ -569,14 +571,9 @@ public final class LegacySchemaMigrator
     // Should only be called on compact tables
     private static boolean checkNeedsUpgrade(Iterable<UntypedResultSet.Row> defs, boolean isSuper, boolean isStaticCompactTable)
     {
+        // For SuperColumn tables, re-create a compact value column
         if (isSuper)
-        {
-            // Check if we've added the "supercolumn map" column yet or not
-            for (UntypedResultSet.Row row : defs)
-                if (row.getString("column_name").isEmpty())
-                    return false;
             return true;
-        }
 
         // For static compact tables, we need to upgrade if the regular definitions haven't been converted to static yet,
         // i.e. if we don't have a static definition yet.
@@ -626,7 +623,7 @@ public final class LegacySchemaMigrator
 
         if (isSuper)
         {
-            defs.add(ColumnDefinition.regularDef(ksName, cfName, CompactTables.SUPER_COLUMN_MAP_COLUMN_STR, MapType.getInstance(subComparator, defaultValidator, true)));
+            defs.add(ColumnDefinition.regularDef(ksName, cfName, SuperColumnCompatibility.SUPER_COLUMN_MAP_COLUMN_STR, MapType.getInstance(subComparator, defaultValidator, true)));
         }
         else if (isStaticCompactTable)
         {
diff --git a/src/java/org/apache/cassandra/thrift/CassandraServer.java b/src/java/org/apache/cassandra/thrift/CassandraServer.java
index cb74b15e27..256f65102b 100644
--- a/src/java/org/apache/cassandra/thrift/CassandraServer.java
+++ b/src/java/org/apache/cassandra/thrift/CassandraServer.java
@@ -36,6 +36,7 @@ import org.slf4j.LoggerFactory;
 import org.apache.cassandra.auth.Permission;
 import org.apache.cassandra.config.*;
 import org.apache.cassandra.cql3.QueryOptions;
+import org.apache.cassandra.cql3.SuperColumnCompatibility;
 import org.apache.cassandra.cql3.statements.ParsedStatement;
 import org.apache.cassandra.db.*;
 import org.apache.cassandra.db.context.CounterContext;
@@ -432,11 +433,15 @@ public class CassandraServer implements Cassandra.Iface
             ByteBuffer finish = range.reversed ? range.start : range.finish;
             builder.slice(def, start.hasRemaining() ? CellPath.create(start) : CellPath.BOTTOM, finish.hasRemaining() ? CellPath.create(finish) : CellPath.TOP);
 
+            if (metadata.isDense())
+                return builder.build();
+
             // We also want to add any staticly defined column if it's within the range
             AbstractType<?> cmp = metadata.thriftColumnNameType();
+
             for (ColumnDefinition column : metadata.partitionColumns())
             {
-                if (CompactTables.isSuperColumnMapColumn(column))
+                if (SuperColumnCompatibility.isSuperColumnMapColumn(column))
                     continue;
 
                 ByteBuffer name = column.name.bytes;
diff --git a/src/java/org/apache/cassandra/thrift/ThriftConversion.java b/src/java/org/apache/cassandra/thrift/ThriftConversion.java
index bccfd8ab46..e8256a8ce2 100644
--- a/src/java/org/apache/cassandra/thrift/ThriftConversion.java
+++ b/src/java/org/apache/cassandra/thrift/ThriftConversion.java
@@ -18,7 +18,6 @@
 package org.apache.cassandra.thrift;
 
 import java.util.*;
-import java.util.regex.Matcher;
 
 import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Strings;
@@ -27,6 +26,7 @@ import com.google.common.collect.Maps;
 import org.apache.cassandra.config.*;
 import org.apache.cassandra.cql3.ColumnIdentifier;
 import org.apache.cassandra.cql3.Operator;
+import org.apache.cassandra.cql3.SuperColumnCompatibility;
 import org.apache.cassandra.cql3.statements.IndexTarget;
 import org.apache.cassandra.db.CompactTables;
 import org.apache.cassandra.db.LegacyLayout;
@@ -273,7 +273,8 @@ public class ThriftConversion
                                       hasKeyAlias ? null : keyValidator,
                                       rawComparator,
                                       subComparator,
-                                      defaultValidator);
+                                      defaultValidator,
+                                      isDense);
             }
 
             // We do not allow Thrift views, so we always set it to false
@@ -368,7 +369,8 @@ public class ThriftConversion
                                               AbstractType<?> keyValidator,
                                               AbstractType<?> comparator,
                                               AbstractType<?> subComparator,
-                                              AbstractType<?> defaultValidator)
+                                              AbstractType<?> defaultValidator,
+                                              boolean isDense)
     {
         CompactTables.DefaultNames names = CompactTables.defaultNameGenerator(defs);
         if (keyValidator != null)
@@ -389,7 +391,12 @@ public class ThriftConversion
         {
             // SuperColumn tables: we use a special map to hold dynamic values within a given super column
             defs.add(ColumnDefinition.clusteringDef(ks, cf, names.defaultClusteringName(), comparator, 0));
-            defs.add(ColumnDefinition.regularDef(ks, cf, CompactTables.SUPER_COLUMN_MAP_COLUMN_STR, MapType.getInstance(subComparator, defaultValidator, true)));
+            defs.add(ColumnDefinition.regularDef(ks, cf, SuperColumnCompatibility.SUPER_COLUMN_MAP_COLUMN_STR, MapType.getInstance(subComparator, defaultValidator, true)));
+            if (isDense)
+            {
+                defs.add(ColumnDefinition.clusteringDef(ks, cf, names.defaultClusteringName(), subComparator, 1));
+                defs.add(ColumnDefinition.regularDef(ks, cf, names.defaultCompactValueName(), defaultValidator));
+            }
         }
         else
         {
diff --git a/test/unit/org/apache/cassandra/SchemaLoader.java b/test/unit/org/apache/cassandra/SchemaLoader.java
index 87e8e1e029..16869739a4 100644
--- a/test/unit/org/apache/cassandra/SchemaLoader.java
+++ b/test/unit/org/apache/cassandra/SchemaLoader.java
@@ -387,25 +387,18 @@ public class SchemaLoader
             .compression(getCompressionParameters());
     }
 
-    // TODO: Fix superCFMD failing on legacy table creation. Seems to be applying composite comparator to partition key
     public static CFMetaData superCFMD(String ksName, String cfName, AbstractType subcc)
     {
         return superCFMD(ksName, cfName, BytesType.instance, subcc);
     }
+
     public static CFMetaData superCFMD(String ksName, String cfName, AbstractType cc, AbstractType subcc)
     {
-        return superCFMD(ksName, cfName, "cols", cc, subcc);
-    }
-    public static CFMetaData superCFMD(String ksName, String cfName, String ccName, AbstractType cc, AbstractType subcc)
-    {
-        //This is busted
-//        return CFMetaData.Builder.createSuper(ksName, cfName, false)
-//            .addPartitionKey("0", BytesType.instance)
-//            .addClusteringColumn("1", cc)
-//            .addClusteringColumn("2", subcc)
-//            .addRegularColumn("3", AsciiType.instance)
-//            .build();
-        return standardCFMD(ksName, cfName);
+        return CFMetaData.Builder.createSuper(ksName, cfName, false)
+                                 .addPartitionKey("key", BytesType.instance)
+                                 .addClusteringColumn("column1", cc)
+                                 .addRegularColumn("", MapType.getInstance(AsciiType.instance, subcc, true))
+                                 .build();
 
     }
     public static CFMetaData compositeIndexCFMD(String ksName, String cfName, boolean withIndex) throws ConfigurationException
diff --git a/test/unit/org/apache/cassandra/cql3/ViewTest.java b/test/unit/org/apache/cassandra/cql3/ViewTest.java
index 1107a64707..4a4fe1a8ce 100644
--- a/test/unit/org/apache/cassandra/cql3/ViewTest.java
+++ b/test/unit/org/apache/cassandra/cql3/ViewTest.java
@@ -23,6 +23,7 @@ import static org.junit.Assert.*;
 import java.util.ArrayList;
 import java.util.HashSet;
 import java.util.List;
+import java.util.UUID;
 import java.util.concurrent.Future;
 import java.util.concurrent.TimeUnit;
 import com.google.common.util.concurrent.Uninterruptibles;
@@ -36,6 +37,7 @@ import org.junit.Test;
 import com.datastax.driver.core.ResultSet;
 import com.datastax.driver.core.Row;
 import com.datastax.driver.core.exceptions.InvalidQueryException;
+import org.apache.cassandra.SchemaLoader;
 import org.apache.cassandra.concurrent.SEPExecutor;
 import org.apache.cassandra.concurrent.Stage;
 import org.apache.cassandra.concurrent.StageManager;
@@ -45,8 +47,12 @@ import org.apache.cassandra.db.ColumnFamilyStore;
 import org.apache.cassandra.db.Keyspace;
 import org.apache.cassandra.db.SystemKeyspace;
 import org.apache.cassandra.db.compaction.CompactionManager;
+import org.apache.cassandra.db.marshal.AsciiType;
+import org.apache.cassandra.schema.KeyspaceParams;
 import org.apache.cassandra.utils.FBUtilities;
 
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
 
 public class ViewTest extends CQLTester
 {
@@ -389,7 +395,30 @@ public class ViewTest extends CQLTester
     }
 
     @Test
-    public void testCreateMvWithTTL() throws Throwable
+    public void testSuperCoumn() throws Throwable
+    {
+        String keyspace = createKeyspaceName();
+        String table = createTableName();
+        SchemaLoader.createKeyspace(keyspace,
+                                    KeyspaceParams.simple(1),
+                                    SchemaLoader.superCFMD(keyspace, table, AsciiType.instance, AsciiType.instance));
+
+        execute("USE " + keyspace);
+        executeNet(protocolVersion, "USE " + keyspace);
+
+        try
+        {
+            createView("mv_super_column", "CREATE MATERIALIZED VIEW %s AS SELECT * FROM " + keyspace + "." + table + " WHERE key IS NOT NULL AND column1 IS NOT NULL PRIMARY KEY (key,column1)");
+            Assert.fail("MV on SuperColumn table should fail");
+        }
+        catch (InvalidQueryException e)
+        {
+            assertEquals("Materialized views are not supported on SuperColumn tables", e.getMessage());
+        }
+    }
+
+    @Test
+    public void testDurationsTable() throws Throwable
     {
         createTable("CREATE TABLE %s (" +
                     "k int PRIMARY KEY, " +
diff --git a/test/unit/org/apache/cassandra/cql3/validation/ThriftIntegrationTest.java b/test/unit/org/apache/cassandra/cql3/validation/ThriftIntegrationTest.java
new file mode 100644
index 0000000000..def489e9c9
--- /dev/null
+++ b/test/unit/org/apache/cassandra/cql3/validation/ThriftIntegrationTest.java
@@ -0,0 +1,942 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql3.validation;
+
+import java.nio.ByteBuffer;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.atomic.AtomicInteger;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.Test;
+
+import org.apache.cassandra.cql3.ColumnIdentifier;
+import org.apache.cassandra.cql3.UntypedResultSet;
+import org.apache.cassandra.cql3.validation.operations.ThriftCQLTester;
+import org.apache.cassandra.db.Keyspace;
+import org.apache.cassandra.db.marshal.AsciiType;
+import org.apache.cassandra.db.marshal.CounterColumnType;
+import org.apache.cassandra.db.marshal.Int32Type;
+import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.locator.SimpleStrategy;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.thrift.Cassandra;
+import org.apache.cassandra.thrift.CfDef;
+import org.apache.cassandra.thrift.Column;
+import org.apache.cassandra.thrift.ColumnDef;
+import org.apache.cassandra.thrift.ColumnOrSuperColumn;
+import org.apache.cassandra.thrift.ColumnParent;
+import org.apache.cassandra.thrift.ColumnPath;
+import org.apache.cassandra.thrift.CounterColumn;
+import org.apache.cassandra.thrift.CounterSuperColumn;
+import org.apache.cassandra.thrift.Deletion;
+import org.apache.cassandra.thrift.KsDef;
+import org.apache.cassandra.thrift.Mutation;
+import org.apache.cassandra.thrift.SlicePredicate;
+import org.apache.cassandra.thrift.SliceRange;
+import org.apache.cassandra.thrift.SuperColumn;
+import org.apache.cassandra.utils.ByteBufferUtil;
+import org.apache.cassandra.utils.FBUtilities;
+
+import static org.apache.cassandra.thrift.ConsistencyLevel.ONE;
+import static org.junit.Assert.assertEquals;
+
+public class ThriftIntegrationTest extends ThriftCQLTester
+{
+    final static AtomicInteger seqNumber = new AtomicInteger();
+    final String KEYSPACE = "thrift_compact_table_with_supercolumns_test_" + seqNumber.incrementAndGet();
+
+    @Before
+    public void setupSuperColumnFamily() throws Throwable
+    {
+        StorageService.instance.setRpcReady(true);
+
+        final String denseTableName = createTableName();
+        final String sparseTableName =  currentSparseTable();
+        final String counterTableName = currentCounterTable();
+
+        CfDef cfDef = new CfDef().setColumn_type("Super")
+                                 .setSubcomparator_type(Int32Type.instance.toString())
+                                 .setComparator_type(AsciiType.instance.toString())
+                                 .setDefault_validation_class(AsciiType.instance.toString())
+                                 .setKey_validation_class(AsciiType.instance.toString())
+                                 .setKeyspace(KEYSPACE)
+                                 .setName(denseTableName);
+
+        CfDef sparseCfDef = new CfDef().setColumn_type("Super")
+                                       .setComparator_type(AsciiType.instance.toString())
+                                       .setSubcomparator_type(AsciiType.instance.toString())
+                                       .setKey_validation_class(AsciiType.instance.toString())
+                                       .setColumn_metadata(Arrays.asList(new ColumnDef(ByteBufferUtil.bytes("col1"), LongType.instance.toString()),
+                                                                         new ColumnDef(ByteBufferUtil.bytes("col2"), LongType.instance.toString())))
+                                       .setKeyspace(KEYSPACE)
+                                       .setName(sparseTableName);
+
+        CfDef counterCfDef = new CfDef().setColumn_type("Super")
+                                        .setSubcomparator_type(AsciiType.instance.toString())
+                                        .setComparator_type(AsciiType.instance.toString())
+                                        .setDefault_validation_class(CounterColumnType.instance.toString())
+                                        .setKey_validation_class(AsciiType.instance.toString())
+                                        .setKeyspace(KEYSPACE)
+                                        .setName(counterTableName);
+
+        KsDef ksDef = new KsDef(KEYSPACE,
+                                SimpleStrategy.class.getName(),
+                                Arrays.asList(cfDef, sparseCfDef, counterCfDef));
+
+        ksDef.setStrategy_options(Collections.singletonMap("replication_factor", "1"));
+
+        Cassandra.Client client = getClient();
+        client.system_add_keyspace(ksDef);
+        client.set_keyspace(KEYSPACE);
+    }
+
+    @After
+    public void tearDown() throws Throwable
+    {
+        getClient().send_system_drop_keyspace(KEYSPACE);
+    }
+
+    @Test
+    public void testCounterTableReads() throws Throwable
+    {
+        populateCounterTable();
+        beforeAndAfterFlush(this::testCounterTableReadsInternal);
+    }
+
+    private void testCounterTableReadsInternal() throws Throwable
+    {
+        UntypedResultSet resultSet = execute(String.format("select * from %s.%s", KEYSPACE, currentCounterTable()));
+        assertRows(resultSet,
+                   row("key1", "ck1", "counter1", 10L),
+                   row("key1", "ck1", "counter2", 5L),
+                   row("key2", "ck1", "counter1", 10L),
+                   row("key2", "ck1", "counter2", 5L));
+    }
+
+    @Test
+    public void testCounterTableThriftUpdates() throws Throwable
+    {
+        populateCounterTable();
+
+        Cassandra.Client client = getClient();
+        Mutation mutation = new Mutation();
+        ColumnOrSuperColumn csoc = new ColumnOrSuperColumn();
+        csoc.setCounter_super_column(new CounterSuperColumn(ByteBufferUtil.bytes("ck1"),
+                                                            Arrays.asList(new CounterColumn(ByteBufferUtil.bytes("counter1"), 1))));
+        mutation.setColumn_or_supercolumn(csoc);
+
+        Mutation mutation2 = new Mutation();
+        ColumnOrSuperColumn csoc2 = new ColumnOrSuperColumn();
+        csoc2.setCounter_super_column(new CounterSuperColumn(ByteBufferUtil.bytes("ck1"),
+                                                             Arrays.asList(new CounterColumn(ByteBufferUtil.bytes("counter1"), 100))));
+        mutation2.setColumn_or_supercolumn(csoc2);
+        client.batch_mutate(Collections.singletonMap(ByteBufferUtil.bytes("key1"),
+                                                     Collections.singletonMap(currentCounterTable(), Arrays.asList(mutation))),
+                            ONE);
+        client.batch_mutate(Collections.singletonMap(ByteBufferUtil.bytes("key2"),
+                                                     Collections.singletonMap(currentCounterTable(), Arrays.asList(mutation2))),
+                            ONE);
+
+        beforeAndAfterFlush(() -> {
+            UntypedResultSet resultSet = execute(String.format("select * from %s.%s", KEYSPACE, currentCounterTable()));
+            assertRows(resultSet,
+                       row("key1", "ck1", "counter1", 11L),
+                       row("key1", "ck1", "counter2", 5L),
+                       row("key2", "ck1", "counter1", 110L),
+                       row("key2", "ck1", "counter2", 5L));
+        });
+    }
+
+    @Test
+    public void testCounterTableCqlUpdates() throws Throwable
+    {
+        populateCounterTable();
+
+        execute(String.format("UPDATE %s.%s set value = value + 1 WHERE key = ? AND column1 = ? AND column2 = ?", KEYSPACE, currentCounterTable()),
+                "key1", "ck1", "counter1");
+        execute(String.format("UPDATE %s.%s set value = value + 100 WHERE key = 'key2' AND column1 = 'ck1' AND column2 = 'counter1'", KEYSPACE, currentCounterTable()));
+
+        execute(String.format("UPDATE %s.%s set value = value - ? WHERE key = 'key1' AND column1 = 'ck1' AND column2 = 'counter2'", KEYSPACE, currentCounterTable()), 2L);
+        execute(String.format("UPDATE %s.%s set value = value - ? WHERE key = 'key2' AND column1 = 'ck1' AND column2 = 'counter2'", KEYSPACE, currentCounterTable()), 100L);
+
+        beforeAndAfterFlush(() -> {
+            UntypedResultSet resultSet = execute(String.format("select * from %s.%s", KEYSPACE, currentCounterTable()));
+            assertRows(resultSet,
+                       row("key1", "ck1", "counter1", 11L),
+                       row("key1", "ck1", "counter2", 3L),
+                       row("key2", "ck1", "counter1", 110L),
+                       row("key2", "ck1", "counter2", -95L));
+        });
+    }
+
+    @Test
+    public void testCounterTableCqlDeletes() throws Throwable
+    {
+        populateCounterTable();
+
+        assertRows(execute(String.format("select * from %s.%s", KEYSPACE, currentCounterTable())),
+                   row("key1", "ck1", "counter1", 10L),
+                   row("key1", "ck1", "counter2", 5L),
+                   row("key2", "ck1", "counter1", 10L),
+                   row("key2", "ck1", "counter2", 5L));
+
+        execute(String.format("DELETE value FROM %s.%s WHERE key = ? AND column1 = ? AND column2 = ?", KEYSPACE, currentCounterTable()),
+                "key1", "ck1", "counter1");
+
+        assertRows(execute(String.format("select * from %s.%s", KEYSPACE, currentCounterTable())),
+                   row("key1", "ck1", "counter2", 5L),
+                   row("key2", "ck1", "counter1", 10L),
+                   row("key2", "ck1", "counter2", 5L));
+
+        execute(String.format("DELETE FROM %s.%s WHERE key = ? AND column1 = ?", KEYSPACE, currentCounterTable()),
+                "key1", "ck1");
+
+        assertRows(execute(String.format("select * from %s.%s", KEYSPACE, currentCounterTable())),
+                   row("key2", "ck1", "counter1", 10L),
+                   row("key2", "ck1", "counter2", 5L));
+
+        execute(String.format("DELETE FROM %s.%s WHERE key = ?", KEYSPACE, currentCounterTable()),
+                "key2");
+
+        assertEmpty(execute(String.format("select * from %s.%s", KEYSPACE, currentCounterTable())));
+    }
+
+    @Test
+    public void testDenseTableAlter() throws Throwable
+    {
+        populateDenseTable();
+
+        alterTable(String.format("ALTER TABLE %s.%s RENAME column1 TO renamed_column1", KEYSPACE, currentDenseTable()));
+        alterTable(String.format("ALTER TABLE %s.%s RENAME column2 TO renamed_column2", KEYSPACE, currentDenseTable()));
+        alterTable(String.format("ALTER TABLE %s.%s RENAME key TO renamed_key", KEYSPACE, currentDenseTable()));
+        alterTable(String.format("ALTER TABLE %s.%s RENAME value TO renamed_value", KEYSPACE, currentDenseTable()));
+
+        beforeAndAfterFlush(() -> {
+            UntypedResultSet resultSet = execute(String.format("select * from %s.%s", KEYSPACE, currentDenseTable()));
+            assertEquals("renamed_key", resultSet.metadata().get(0).name.toString());
+            assertEquals("renamed_column1", resultSet.metadata().get(1).name.toString());
+                                assertEquals("renamed_column2", resultSet.metadata().get(2).name.toString());
+                                assertEquals("renamed_value", resultSet.metadata().get(3).name.toString());
+            assertRows(resultSet,
+                       row("key1", "val1", 1, "value1"),
+                       row("key1", "val1", 2, "value2"),
+                       row("key1", "val2", 4, "value4"),
+                       row("key1", "val2", 5, "value5"),
+                       row("key2", "val1", 1, "value1"),
+                       row("key2", "val1", 2, "value2"),
+                       row("key2", "val2", 4, "value4"),
+                       row("key2", "val2", 5, "value5"));
+        });
+    }
+
+    @Test
+    public void testDenseTableReads() throws Throwable
+    {
+        populateDenseTable();
+        beforeAndAfterFlush(this::testDenseTableReadsInternal);
+    }
+
+    private void testDenseTableReadsInternal() throws Throwable
+    {
+        UntypedResultSet resultSet = execute(String.format("select * from %s.%s", KEYSPACE, currentDenseTable()));
+        assertEquals("key", resultSet.metadata().get(0).name.toString());
+        assertEquals("column1", resultSet.metadata().get(1).name.toString());
+        assertEquals("column2", resultSet.metadata().get(2).name.toString());
+        assertEquals("value", resultSet.metadata().get(3).name.toString());
+
+
+        assertRows(resultSet,
+                   row("key1", "val1", 1, "value1"),
+                   row("key1", "val1", 2, "value2"),
+                   row("key1", "val2", 4, "value4"),
+                   row("key1", "val2", 5, "value5"),
+                   row("key2", "val1", 1, "value1"),
+                   row("key2", "val1", 2, "value2"),
+                   row("key2", "val2", 4, "value4"),
+                   row("key2", "val2", 5, "value5"));
+
+        assertRows(execute(String.format("select * from %s.%s LIMIT 5", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 1, "value1"),
+                   row("key1", "val1", 2, "value2"),
+                   row("key1", "val2", 4, "value4"),
+                   row("key1", "val2", 5, "value5"),
+                   row("key2", "val1", 1, "value1"));
+
+        assertRows(execute(String.format("select value, column2, column1, key from %s.%s", KEYSPACE, currentDenseTable())),
+                   row("value1", 1, "val1", "key1"),
+                   row("value2", 2, "val1", "key1"),
+                   row("value4", 4, "val2", "key1"),
+                   row("value5", 5, "val2", "key1"),
+                   row("value1", 1, "val1", "key2"),
+                   row("value2", 2, "val1", "key2"),
+                   row("value4", 4, "val2", "key2"),
+                   row("value5", 5, "val2", "key2"));
+
+        assertRows(execute(String.format("select * from %s.%s WHERE key = ? AND column1 = ?", KEYSPACE, currentDenseTable()), "key1", "val2"),
+                   row("key1", "val2", 4, "value4"),
+                   row("key1", "val2", 5, "value5"));
+
+        assertRows(execute(String.format("select * from %s.%s where key IN ('key1', 'key2') and column1 = 'val1' and column2 = 2", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 2, "value2"),
+                   row("key2", "val1", 2, "value2"));
+        assertRows(execute(String.format("select * from %s.%s where key IN ('key1', 'key2') and column1 = 'val1' and column2 > 1", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 2, "value2"),
+                   row("key2", "val1", 2, "value2"));
+        assertRows(execute(String.format("select * from %s.%s where key IN ('key1', 'key2') and column1 = 'val1' and column2 >= 2", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 2, "value2"),
+                   row("key2", "val1", 2, "value2"));
+        assertEmpty(execute(String.format("select * from %s.%s where key IN ('key1', 'key2') and column1 = 'val1' and column2 > 2", KEYSPACE, currentDenseTable())));
+
+        assertRows(execute(String.format("select column2, key from %s.%s WHERE key = ? AND column1 = ? and column2 = 5", KEYSPACE, currentDenseTable()), "key1", "val2"),
+                   row(5, "key1"));
+        assertRows(execute(String.format("select * from %s.%s WHERE key = ? AND column1 = ? and column2 >= ?", KEYSPACE, currentDenseTable()), "key1", "val2", 5),
+                   row("key1", "val2", 5, "value5"));
+        assertRows(execute(String.format("select * from %s.%s WHERE key = ? AND column1 = ? and column2 > ?", KEYSPACE, currentDenseTable()), "key1", "val2", 4),
+                   row("key1", "val2", 5, "value5"));
+        assertRows(execute(String.format("select * from %s.%s WHERE key = ? AND column1 = ? and column2 < ?", KEYSPACE, currentDenseTable()), "key1", "val2", 5),
+                   row("key1", "val2", 4, "value4"));
+        assertRows(execute(String.format("select * from %s.%s WHERE key = ? AND column1 = ? and column2 <= ?", KEYSPACE, currentDenseTable()), "key1", "val2", 5),
+                   row("key1", "val2", 4, "value4"),
+                   row("key1", "val2", 5, "value5"));
+
+        assertRows(execute(String.format("select * from %s.%s where key = 'key1' and column1 in ('val1', 'val2') and column2 IN (1, 4)", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 1, "value1"),
+                   row("key1", "val2", 4, "value4"));
+
+        assertRows(execute(String.format("select * from %s.%s where key = 'key1' and column1 in ('val1', 'val2')", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 1, "value1"),
+                   row("key1", "val1", 2, "value2"),
+                   row("key1", "val2", 4, "value4"),
+                   row("key1", "val2", 5, "value5"));
+
+        assertRows(execute(String.format("select * from %s.%s where key = 'key1' and column1 in ('val1', 'val2') and column2 = 1", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 1, "value1"));
+
+        assertRows(execute(String.format("select * from %s.%s where key = 'key1' and (column1, column2) = ('val2', 4)", KEYSPACE, currentDenseTable())),
+                   row("key1", "val2", 4, "value4"));
+
+        assertRows(execute(String.format("select * from %s.%s where key = 'key1' and (column1, column2) >= ('val2', 4)", KEYSPACE, currentDenseTable())),
+                   row("key1", "val2", 4, "value4"),
+                   row("key1", "val2", 5, "value5"));
+
+        assertRows(execute(String.format("select * from %s.%s where key = 'key1' and (column1, column2) > ('val1', 1)", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 2, "value2"),
+                   row("key1", "val2", 4, "value4"),
+                   row("key1", "val2", 5, "value5"));
+
+        assertRows(execute(String.format("select * from %s.%s where key = 'key1' and (column1, column2) > ('val2', 1)", KEYSPACE, currentDenseTable())),
+                   row("key1", "val2", 4, "value4"),
+                   row("key1", "val2", 5, "value5"));
+
+        resultSet = execute(String.format("select key as a, column1 as b, column2 as c, value as d " +
+                                          "from %s.%s WHERE key = ? AND column1 = ?", KEYSPACE, currentDenseTable()), "key1", "val2");
+        assertRows(resultSet,
+                   row("key1", "val2", 4, "value4"),
+                   row("key1", "val2", 5, "value5"));
+        assertEquals(resultSet.metadata().get(2).type, Int32Type.instance);
+        assertEquals(resultSet.metadata().get(3).type, AsciiType.instance);
+
+        assertRows(execute(String.format("select column2, value from %s.%s WHERE key = ? AND column1 = ?", KEYSPACE, currentDenseTable()), "key1", "val2"),
+                   row(4, "value4"),
+                   row(5, "value5"));
+
+        assertRows(execute(String.format("select column1, value from %s.%s WHERE key = ? AND column1 = ?", KEYSPACE, currentDenseTable()), "key2", "val1"),
+                   row("val1", "value1"),
+                   row("val1", "value2"));
+
+        assertInvalidMessage("Secondary indexes are not supported on COMPACT STORAGE tables that have clustering columns",
+                             String.format("CREATE INDEX ON %s.%s (column2)", KEYSPACE, currentDenseTable()));
+        assertInvalidMessage("Secondary indexes are not supported on COMPACT STORAGE tables that have clustering columns",
+                             String.format("CREATE INDEX ON %s.%s (value)", KEYSPACE, currentDenseTable()));
+
+        assertRows(execute(String.format("SELECT JSON * FROM %s.%s WHERE key = ? AND column1 = ?", KEYSPACE, currentDenseTable()), "key1", "val2"),
+                   row("{\"key\": \"key1\", \"column1\": \"val2\", \"column2\": 4, \"value\": \"value4\"}"),
+                   row("{\"key\": \"key1\", \"column1\": \"val2\", \"column2\": 5, \"value\": \"value5\"}"));
+    }
+
+    @Test
+    public void testDenseTablePartialCqlInserts() throws Throwable
+    {
+        assertInvalidMessage("Column value is mandatory for SuperColumn tables",
+                             String.format("INSERT INTO %s.%s (key, column1, column2) VALUES ('key1', 'val1', 1)", KEYSPACE, currentDenseTable()));
+
+        // That's slightly different from 2.X, since null map keys are not allowed
+        assertInvalidMessage("Column key is mandatory for SuperColumn tables",
+                             String.format("INSERT INTO %s.%s (key, column1, value) VALUES ('key1', 'val1', 'value1')", KEYSPACE, currentDenseTable()));
+
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key1', 'val1', 1, NULL)", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key1', 'val1', 1, ?)", KEYSPACE, currentDenseTable()), unset());
+        assertEmpty(execute(String.format("select * from %s.%s", KEYSPACE, currentDenseTable())));
+    }
+
+    @Test
+    public void testDenseTableCqlInserts() throws Throwable
+    {
+        Cassandra.Client client = getClient();
+
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES (?, ?, ?, ?)", KEYSPACE, currentDenseTable()),
+                "key1", "val1", 1, "value1");
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES (?, ?, 2, ?)", KEYSPACE, currentDenseTable()),
+                "key1", "val1", "value2");
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key1', 'val2', 4, 'value4')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key1', 'val2', 5, 'value5')", KEYSPACE, currentDenseTable()));
+
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key2', 'val1', 1, 'value1')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key2', 'val1', 2, 'value2')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key2', 'val2', 4, 'value4')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key2', 'val2', 5, 'value5')", KEYSPACE, currentDenseTable()));
+
+        ColumnPath path = new ColumnPath(currentDenseTable());
+        path.setSuper_column(ByteBufferUtil.bytes("val1"));
+
+        ColumnOrSuperColumn cosc = client.get(ByteBufferUtil.bytes("key1"), path, ONE);
+        assertEquals(cosc.getSuper_column().columns.get(0).name, ByteBufferUtil.bytes(1));
+        assertEquals(cosc.getSuper_column().columns.get(0).value, ByteBufferUtil.bytes("value1"));
+        assertEquals(cosc.getSuper_column().columns.get(1).name, ByteBufferUtil.bytes(2));
+        assertEquals(cosc.getSuper_column().columns.get(1).value, ByteBufferUtil.bytes("value2"));
+    }
+
+    @Test
+    public void testDenseTableCqlUpdates() throws Throwable
+    {
+        assertInvalidMessage("Column key is mandatory for SuperColumn tables",
+                             String.format("UPDATE %s.%s SET column2 = 1, value = 'value1' WHERE key = 'key1' AND column1 = 'val1'", KEYSPACE, currentDenseTable()));
+        assertInvalidMessage("Column `column2` of type `int` found in SET part",
+                             String.format("UPDATE %s.%s SET column2 = 1, value = 'value1' WHERE key = 'key1' AND column1 = 'val1' AND column2 = 1", KEYSPACE, currentDenseTable()));
+        assertInvalidMessage("Some clustering keys are missing: column1",
+                             String.format("UPDATE %s.%s SET value = 'value1' WHERE key = 'key1' AND column2 = 1", KEYSPACE, currentDenseTable()));
+
+        execute(String.format("UPDATE %s.%s SET value = 'value1' WHERE key = 'key1' AND column1 = 'val1' AND column2 = 1", KEYSPACE, currentDenseTable()));
+        execute(String.format("UPDATE %s.%s SET value = 'value2' WHERE key = 'key1' AND column1 = 'val1' AND column2 = 2", KEYSPACE, currentDenseTable()));
+
+        execute(String.format("UPDATE %s.%s SET value = ? WHERE key = ? AND column1 = ? AND column2 = ?", KEYSPACE, currentDenseTable()),
+                "value1", "key2", "val2", 1);
+        execute(String.format("UPDATE %s.%s SET value = 'value2' WHERE key = 'key2' AND column1 = ? AND column2 = ?", KEYSPACE, currentDenseTable()),
+                "val2", 2);
+
+        Cassandra.Client client = getClient();
+        ColumnPath path = new ColumnPath(currentDenseTable());
+        path.setSuper_column(ByteBufferUtil.bytes("val1"));
+
+        ColumnOrSuperColumn cosc = client.get(ByteBufferUtil.bytes("key1"), path, ONE);
+        assertEquals(cosc.getSuper_column().columns.get(0).name, ByteBufferUtil.bytes(1));
+        assertEquals(cosc.getSuper_column().columns.get(0).value, ByteBufferUtil.bytes("value1"));
+        assertEquals(cosc.getSuper_column().columns.get(1).name, ByteBufferUtil.bytes(2));
+        assertEquals(cosc.getSuper_column().columns.get(1).value, ByteBufferUtil.bytes("value2"));
+
+        path = new ColumnPath(currentDenseTable());
+        path.setSuper_column(ByteBufferUtil.bytes("val2"));
+
+        cosc = client.get(ByteBufferUtil.bytes("key2"), path, ONE);
+        assertEquals(cosc.getSuper_column().columns.get(0).name, ByteBufferUtil.bytes(1));
+        assertEquals(cosc.getSuper_column().columns.get(0).value, ByteBufferUtil.bytes("value1"));
+        assertEquals(cosc.getSuper_column().columns.get(1).name, ByteBufferUtil.bytes(2));
+        assertEquals(cosc.getSuper_column().columns.get(1).value, ByteBufferUtil.bytes("value2"));
+    }
+
+
+    @Test
+    public void testDenseTableCqlDeletes() throws Throwable
+    {
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key1', 'val1', 1, 'value1')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key1', 'val1', 2, 'value2')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key1', 'val2', 4, 'value4')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key1', 'val2', 5, 'value5')", KEYSPACE, currentDenseTable()));
+
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key2', 'val1', 1, 'value1')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key2', 'val1', 2, 'value2')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key2', 'val2', 4, 'value4')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key2', 'val2', 5, 'value5')", KEYSPACE, currentDenseTable()));
+
+        execute(String.format("DELETE FROM %s.%s WHERE key = 'key1' AND column1 = 'val2' AND column2 = 5", KEYSPACE, currentDenseTable()));
+        assertRows(execute(String.format("SELECT * FROM %s.%s WHERE key = 'key1'", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 1, "value1"),
+                   row("key1", "val1", 2, "value2"),
+                   row("key1", "val2", 4, "value4"));
+        execute(String.format("DELETE FROM %s.%s WHERE key = 'key1' AND column1 = 'val2'", KEYSPACE, currentDenseTable()));
+        assertRows(execute(String.format("SELECT * FROM %s.%s WHERE key = 'key1'", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 1, "value1"),
+                   row("key1", "val1", 2, "value2"));
+        execute(String.format("DELETE FROM %s.%s WHERE key = 'key1'", KEYSPACE, currentDenseTable()));
+        assertEmpty(execute(String.format("SELECT * FROM %s.%s WHERE key = 'key1'", KEYSPACE, currentDenseTable())));
+
+        Cassandra.Client client = getClient();
+
+        Mutation mutation1 = new Mutation();
+        SlicePredicate slicePredicate = new SlicePredicate();
+        slicePredicate.setSlice_range(new SliceRange(ByteBufferUtil.bytes("val1"), ByteBufferUtil.bytes("val1"), false, 1));
+        Deletion deletion1 = new Deletion();
+        deletion1.setTimestamp(FBUtilities.timestampMicros());
+        deletion1.setPredicate(slicePredicate);
+        mutation1.setDeletion(deletion1);
+        client.batch_mutate(Collections.singletonMap(ByteBufferUtil.bytes("key2"),
+                                                     Collections.singletonMap(currentDenseTable(), Arrays.asList(mutation1))),
+                            ONE);
+        assertRows(execute(String.format("SELECT * FROM %s.%s WHERE key = 'key2'", KEYSPACE, currentDenseTable())),
+                   row("key2", "val2", 4, "value4"),
+                   row("key2", "val2", 5, "value5"));
+
+        Mutation mutation2 = new Mutation();
+        Deletion deletion2 = new Deletion();
+        deletion2.setTimestamp(FBUtilities.timestampMicros());
+        deletion2.setSuper_column(ByteBufferUtil.bytes("val2"));
+        mutation2.setDeletion(deletion2);
+        client.batch_mutate(Collections.singletonMap(ByteBufferUtil.bytes("key2"),
+                                                     Collections.singletonMap(currentDenseTable(), Arrays.asList(mutation2))),
+                            ONE);
+
+        assertEmpty(execute(String.format("SELECT * FROM %s.%s WHERE key = 'key2'", KEYSPACE, currentDenseTable())));
+
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key1', 'val1', 1, 'value1')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key2', 'val1', 1, 'value1')", KEYSPACE, currentDenseTable()));
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES ('key3', 'val1', 1, 'value1')", KEYSPACE, currentDenseTable()));
+
+        execute(String.format("DELETE FROM %s.%s WHERE key IN ('key1', 'key2')", KEYSPACE, currentDenseTable()));
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentDenseTable())),
+                   row("key3", "val1", 1, "value1"));
+
+        assertInvalidMessage("Multi-column relations cannot be used in WHERE clauses for UPDATE and DELETE statements",
+                             String.format("DELETE FROM %s.%s WHERE key = 'key3' AND (column1, column2) = ('val1', 1)", KEYSPACE, currentDenseTable()));
+
+        assertInvalidMessage("Token relations cannot be used in WHERE clauses for UPDATE and DELETE statements: token(key) > token('key3')",
+                             String.format("DELETE FROM %s.%s WHERE token(key) > token('key3')", KEYSPACE, currentDenseTable()));
+    }
+
+    @Test
+    public void testSparseTableAlter() throws Throwable
+    {
+        populateSparseTable();
+
+        alterTable(String.format("ALTER TABLE %s.%s RENAME column1 TO renamed_column1", KEYSPACE, currentSparseTable()));
+        alterTable(String.format("ALTER TABLE %s.%s RENAME key TO renamed_key", KEYSPACE, currentSparseTable()));
+        assertInvalidMessage("Cannot rename non PRIMARY KEY part col1",
+                             String.format("ALTER TABLE %s.%s RENAME col1 TO renamed_col1", KEYSPACE, currentSparseTable()));
+        assertInvalidMessage("Cannot rename non PRIMARY KEY part col2",
+                             String.format("ALTER TABLE %s.%s RENAME col2 TO renamed_col2", KEYSPACE, currentSparseTable()));
+        assertInvalidMessage("Cannot rename unknown column column2 in keyspace",
+                             String.format("ALTER TABLE %s.%s RENAME column2 TO renamed_column2", KEYSPACE, currentSparseTable()));
+        assertInvalidMessage("Cannot rename unknown column value in keyspace",
+                             String.format("ALTER TABLE %s.%s RENAME value TO renamed_value", KEYSPACE, currentSparseTable()));
+
+
+        UntypedResultSet resultSet = execute(String.format("select * from %s.%s", KEYSPACE, currentSparseTable()));
+        assertEquals("renamed_key", resultSet.metadata().get(0).name.toString());
+        assertEquals("renamed_column1", resultSet.metadata().get(1).name.toString());
+        assertEquals("col1", resultSet.metadata().get(2).name.toString());
+        assertEquals("col2", resultSet.metadata().get(3).name.toString());
+
+        assertRows(resultSet,
+                   row("key1", "val1", 3L, 4L),
+                   row("key1", "val2", 3L, 4L),
+                   row("key2", "val1", 3L, 4L),
+                   row("key2", "val2", 3L, 4L));
+    }
+
+    @Test
+    public void testSparseTableCqlReads() throws Throwable
+    {
+        populateSparseTable();
+        beforeAndAfterFlush(this::testSparseTableCqlReadsInternal);
+    }
+
+    private void testSparseTableCqlReadsInternal() throws Throwable
+    {
+        UntypedResultSet resultSet = execute(String.format("select * from %s.%s", KEYSPACE, currentSparseTable()));
+        assertEquals("key", resultSet.metadata().get(0).name.toString());
+        assertEquals("column1", resultSet.metadata().get(1).name.toString());
+        assertEquals("col1", resultSet.metadata().get(2).name.toString());
+        assertEquals("col2", resultSet.metadata().get(3).name.toString());
+
+        assertRows(resultSet,
+                   row("key1", "val1", 3L, 4L),
+                   row("key1", "val2", 3L, 4L),
+                   row("key2", "val1", 3L, 4L),
+                   row("key2", "val2", 3L, 4L));
+
+        assertRows(execute(String.format("select col1, col2, column1, key from %s.%s", KEYSPACE, currentSparseTable())),
+                   row(3L, 4L, "val1", "key1"),
+                   row(3L, 4L, "val2", "key1"),
+                   row(3L, 4L, "val1", "key2"),
+                   row(3L, 4L, "val2", "key2"));
+
+        assertInvalidMessage("Undefined name value in selection clause",
+                             String.format("select value from %s.%s", KEYSPACE, currentSparseTable()));
+
+        assertRows(execute(String.format("select * from %s.%s WHERE key = ? AND column1 = ?", KEYSPACE, currentSparseTable()), "key1", "val2"),
+                   row("key1", "val2", 3L, 4L));
+
+        resultSet = execute(String.format("select col1 as a, col2 as b, column1 as c, key as d from %s.%s WHERE key = ? AND column1 = ?", KEYSPACE, currentSparseTable()), "key1", "val2");
+        assertRows(resultSet,
+                   row(3L, 4L, "val2", "key1"));
+        assertEquals(resultSet.metadata().get(0).name, ColumnIdentifier.getInterned("a", true));
+        assertEquals(resultSet.metadata().get(1).name, ColumnIdentifier.getInterned("b", true));
+        assertEquals(resultSet.metadata().get(2).name, ColumnIdentifier.getInterned("c", true));
+        assertEquals(resultSet.metadata().get(3).name, ColumnIdentifier.getInterned("d", true));
+
+        assertRows(execute(String.format("select col1, col2 from %s.%s WHERE key = ? AND column1 = ?", KEYSPACE, currentSparseTable()), "key1", "val2"),
+                   row(3L, 4L));
+
+        assertInvalidMessage("Secondary indexes are not supported on COMPACT STORAGE tables that have clustering columns",
+                             String.format("CREATE INDEX ON %s.%s (column1)", KEYSPACE, currentSparseTable()));
+        assertInvalidMessage("Secondary indexes are not supported on COMPACT STORAGE tables that have clustering columns",
+                             String.format("CREATE INDEX ON %s.%s (col1)", KEYSPACE, currentSparseTable()));
+
+        assertRows(execute(String.format("SELECT JSON * FROM %s.%s WHERE key = ? AND column1 = ?", KEYSPACE, currentSparseTable()), "key1", "val2"),
+                   row("{\"key\": \"key1\", \"column1\": \"val2\", \"col1\": 3, \"col2\": 4}"));
+    }
+
+    @Test
+    public void testSparseTableCqlInserts() throws Throwable
+    {
+        execute(String.format("insert into %s.%s (key, column1, col1, col2) values ('key1', 'val1', 1, 2)", KEYSPACE, currentSparseTable()));
+        execute(String.format("insert into %s.%s (key, column1, col1, col2) values ('key1', 'val2', 3, 4)", KEYSPACE, currentSparseTable()));
+        execute(String.format("insert into %s.%s (key, column1, col1, col2) values ('key2', 'val1', 5, 6)", KEYSPACE, currentSparseTable()));
+        execute(String.format("insert into %s.%s (key, column1, col1, col2) values ('key2', 'val2', 7, 8)", KEYSPACE, currentSparseTable()));
+
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentSparseTable())),
+                   row("key1", "val1", 1L, 2L),
+                   row("key1", "val2", 3L, 4L),
+                   row("key2", "val1", 5L, 6L),
+                   row("key2", "val2", 7L, 8L));
+
+        execute(String.format("truncate %s.%s", KEYSPACE, currentSparseTable()));
+
+        execute(String.format("insert into %s.%s (key, column1) values ('key1', 'val1')", KEYSPACE, currentSparseTable()));
+        assertRows(execute(String.format("select * from %s.%s", KEYSPACE, currentSparseTable())));
+
+        execute(String.format("insert into %s.%s (key, column1, col1) values ('key1', 'val1', 1)", KEYSPACE, currentSparseTable()));
+        execute(String.format("insert into %s.%s (key, column1, col2) values ('key1', 'val1', 2)", KEYSPACE, currentSparseTable()));
+        assertRows(execute(String.format("select * from %s.%s", KEYSPACE, currentSparseTable())),
+                   row("key1", "val1", 1L, 2L));
+
+        Cassandra.Client client = getClient();
+        ColumnPath path = new ColumnPath(currentSparseTable());
+        path.setSuper_column(ByteBufferUtil.bytes("val1"));
+
+        ColumnOrSuperColumn cosc = client.get(ByteBufferUtil.bytes("key1"), path, ONE);
+        assertEquals(cosc.getSuper_column().columns.get(0).value, ByteBufferUtil.bytes(1L));
+        assertEquals(cosc.getSuper_column().columns.get(0).name, ByteBufferUtil.bytes("col1"));
+        assertEquals(cosc.getSuper_column().columns.get(1).value, ByteBufferUtil.bytes(2L));
+        assertEquals(cosc.getSuper_column().columns.get(1).name, ByteBufferUtil.bytes("col2"));
+    }
+
+    @Test
+    public void testSparseTableCqlUpdates() throws Throwable
+    {
+        execute(String.format("UPDATE %s.%s set col1 = 1, col2 = 2 WHERE key = 'key1' AND column1 = 'val1'", KEYSPACE, currentSparseTable()));
+        execute(String.format("UPDATE %s.%s set col1 = 3, col2 = 4 WHERE key = 'key1' AND column1 = 'val2'", KEYSPACE, currentSparseTable()));
+        execute(String.format("UPDATE %s.%s set col1 = 5, col2 = 6 WHERE key = 'key2' AND column1 = 'val1'", KEYSPACE, currentSparseTable()));
+        execute(String.format("UPDATE %s.%s set col1 = 7, col2 = 8 WHERE key = 'key2' AND column1 = 'val2'", KEYSPACE, currentSparseTable()));
+
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentSparseTable())),
+                   row("key1", "val1", 1L, 2L),
+                   row("key1", "val2", 3L, 4L),
+                   row("key2", "val1", 5L, 6L),
+                   row("key2", "val2", 7L, 8L));
+
+        Cassandra.Client client = getClient();
+        ColumnPath path = new ColumnPath(currentSparseTable());
+        path.setSuper_column(ByteBufferUtil.bytes("val1"));
+
+        ColumnOrSuperColumn cosc = client.get(ByteBufferUtil.bytes("key1"), path, ONE);
+        assertEquals(cosc.getSuper_column().columns.get(0).value, ByteBufferUtil.bytes(1L));
+        assertEquals(cosc.getSuper_column().columns.get(0).name, ByteBufferUtil.bytes("col1"));
+        assertEquals(cosc.getSuper_column().columns.get(1).value, ByteBufferUtil.bytes(2L));
+        assertEquals(cosc.getSuper_column().columns.get(1).name, ByteBufferUtil.bytes("col2"));
+    }
+
+    @Test
+    public void testSparseTableCqlDeletes() throws Throwable
+    {
+        execute(String.format("insert into %s.%s (key, column1, col1, col2) values ('key1', 'val1', 1, 2)", KEYSPACE, currentSparseTable()));
+        execute(String.format("insert into %s.%s (key, column1, col1, col2) values ('key1', 'val2', 3, 4)", KEYSPACE, currentSparseTable()));
+        execute(String.format("insert into %s.%s (key, column1, col1, col2) values ('key2', 'val1', 5, 6)", KEYSPACE, currentSparseTable()));
+        execute(String.format("insert into %s.%s (key, column1, col1, col2) values ('key2', 'val2', 7, 8)", KEYSPACE, currentSparseTable()));
+
+        execute(String.format("DELETE col1 FROM %s.%s WHERE key = 'key1' AND column1 = 'val1'", KEYSPACE, currentSparseTable()));
+
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentSparseTable())),
+                   row("key1", "val1", null, 2L),
+                   row("key1", "val2", 3L, 4L),
+                   row("key2", "val1", 5L, 6L),
+                   row("key2", "val2", 7L, 8L));
+
+        execute(String.format("DELETE FROM %s.%s WHERE key = 'key1' AND column1 = 'val2'", KEYSPACE, currentSparseTable()));
+
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentSparseTable())),
+                   row("key1", "val1", null, 2L),
+                   row("key2", "val1", 5L, 6L),
+                   row("key2", "val2", 7L, 8L));
+
+        execute(String.format("DELETE FROM %s.%s WHERE key = 'key2'", KEYSPACE, currentSparseTable()));
+
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentSparseTable())),
+                   row("key1", "val1", null, 2L));
+    }
+
+    @Test
+    public void testInsertJson() throws Throwable
+    {
+        execute(String.format("INSERT INTO %s.%s JSON ?", KEYSPACE, currentDenseTable()),
+                "{\"key\": \"key5\", \"column1\": \"val2\", \"column2\": 4, \"value\": \"value4\"}");
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentDenseTable())),
+                   row("key5", "val2", 4, "value4"));
+
+        execute(String.format("INSERT INTO %s.%s JSON ?", KEYSPACE, currentSparseTable()),
+                "{\"key\": \"key1\", \"column1\": \"val1\", \"col1\": 1, \"col2\": 2}");
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentSparseTable())),
+                   row("key1", "val1", 1L, 2L));
+    }
+
+    @Test
+    public void testFiltering() throws Throwable
+    {
+        assertInvalidMessage("Filtering is not supported on SuperColumn tables",
+                             String.format("select * from %s.%s WHERE value = ?", KEYSPACE, currentDenseTable()),
+                             "value5");
+        assertInvalidMessage("Filtering is not supported on SuperColumn tables",
+                             String.format("select * from %s.%s WHERE value = ? ALLOW FILTERING", KEYSPACE, currentDenseTable()),
+                             "value5");
+        assertInvalidMessage("Filtering is not supported on SuperColumn tables",
+                             String.format("SELECT * FROM %s.%s WHERE value = 'value2' ALLOW FILTERING", KEYSPACE, currentDenseTable()));
+        assertInvalidMessage("Filtering is not supported on SuperColumn tables",
+                             String.format("SELECT * FROM %s.%s WHERE column2 = 1 ALLOW FILTERING", KEYSPACE, currentDenseTable()));
+    }
+
+    @Test
+    public void testLwt() throws Throwable
+    {
+        assertRows(execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES (?, ?, ?, ?) IF NOT EXISTS", KEYSPACE, currentDenseTable()),
+                           "key1", "val1", 1, "value1"),
+                   row(true));
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 1, "value1"));
+        assertRows(execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES (?, ?, ?, ?) IF NOT EXISTS", KEYSPACE, currentDenseTable()),
+                           "key1", "val1", 1, "value1"),
+                   row(false, "key1", "val1", 1, "value1"));
+
+        // in 2.2 this query was a no-op
+        assertInvalidMessage("Lightweight transactions on SuperColumn tables are only supported with supplied SuperColumn key",
+                             String.format("UPDATE %s.%s SET value = 'changed' WHERE key = ? AND column1 = ? IF value = ?", KEYSPACE, currentDenseTable()));
+
+        assertRows(execute(String.format("UPDATE %s.%s SET value = 'changed' WHERE key = ? AND column1 = ? AND column2 = ? IF value = ?", KEYSPACE, currentDenseTable()),
+                           "key1", "val1", 1, "value1"),
+                   row(true));
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 1, "changed"));
+        assertRows(execute(String.format("UPDATE %s.%s SET value = 'changed' WHERE key = ? AND column1 = ? AND column2 = ? IF value = ?", KEYSPACE, currentDenseTable()),
+                           "key1", "val1", 1, "value1"),
+                   row(false, "changed"));
+
+        assertRows(execute(String.format("UPDATE %s.%s SET value = 'changed2' WHERE key = ? AND column1 = ? AND column2 = ? IF value > ?", KEYSPACE, currentDenseTable()),
+                           "key1", "val1", 1, "a"),
+                   row(true));
+        assertRows(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentDenseTable())),
+                   row("key1", "val1", 1, "changed2"));
+        assertRows(execute(String.format("UPDATE %s.%s SET value = 'changed2' WHERE key = ? AND column1 = ? AND column2 = ? IF value < ?", KEYSPACE, currentDenseTable()),
+                           "key1", "val1", 1, "a"),
+                   row(false, "changed2"));
+
+        assertInvalidMessage("PRIMARY KEY column 'column2' cannot have IF conditions",
+                             String.format("UPDATE %s.%s SET value = 'changed2' WHERE key = ? AND column1 = ? AND column2 = ? IF value > ? AND column2 = ?", KEYSPACE, currentDenseTable()));
+
+        assertInvalidMessage("Lightweight transactions on SuperColumn tables are only supported with supplied SuperColumn key",
+                             String.format("UPDATE %s.%s SET value = 'changed2' WHERE key = ? AND column1 = ? IF value > ?", KEYSPACE, currentDenseTable()));
+
+        execute(String.format("DELETE FROM %s.%s WHERE key = 'key1' AND column1 = 'val1' AND column2 = 1 IF EXISTS", KEYSPACE, currentDenseTable()));
+        assertEmpty(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentDenseTable())));
+
+        execute(String.format("INSERT INTO %s.%s (key, column1, column2, value) VALUES (?, ?, ?, ?)", KEYSPACE, currentDenseTable()),
+                "key1", "val1", 1, "value1");
+
+        assertRows(execute(String.format("DELETE FROM %s.%s WHERE key = 'key1' AND column1 = 'val1' AND column2 = 1 IF value = 'value1'", KEYSPACE, currentDenseTable())),
+                   row(true));
+        assertEmpty(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentDenseTable())));
+
+        assertRows(execute(String.format("DELETE FROM %s.%s WHERE key = 'key1' AND column1 = 'val1' AND column2 = 1 IF value = 'value1'", KEYSPACE, currentDenseTable())),
+                   row(false));
+        assertEmpty(execute(String.format("SELECT * FROM %s.%s", KEYSPACE, currentDenseTable())));
+    }
+
+    @Test
+    public void testCqlAggregateFunctions() throws Throwable
+    {
+        populateDenseTable();
+        populateSparseTable();
+
+        assertRows(execute(String.format("select count(*) from %s.%s", KEYSPACE, currentDenseTable())),
+                   row(8L));
+        assertRows(execute(String.format("select count(*) from %s.%s", KEYSPACE, currentSparseTable())),
+                   row(4L));
+
+        assertRows(execute(String.format("select count(*) from %s.%s where key = ? AND column1 = ?", KEYSPACE, currentDenseTable()), "key1", "val1"),
+                   row(2L));
+        assertRows(execute(String.format("select count(*) from %s.%s where key = ? AND column1 = ?", KEYSPACE, currentSparseTable()), "key1", "val1"),
+                   row(1L));
+        assertRows(execute(String.format("select count(*) from %s.%s where key = ?", KEYSPACE, currentSparseTable()), "key1"),
+                   row(2L));
+
+        assertRows(execute(String.format("select max(value) from %s.%s", KEYSPACE, currentDenseTable())),
+                   row("value5"));
+        assertRows(execute(String.format("select max(col1) from %s.%s", KEYSPACE, currentSparseTable())),
+                   row(3L));
+
+        assertRows(execute(String.format("select avg(column2) from %s.%s", KEYSPACE, currentDenseTable())),
+                   row(3));
+        assertRows(execute(String.format("select avg(col1) from %s.%s", KEYSPACE, currentSparseTable())),
+                   row(3L));
+    }
+
+    private void populateDenseTable() throws Throwable
+    {
+        Cassandra.Client client = getClient();
+
+        Mutation mutation = new Mutation();
+        ColumnOrSuperColumn csoc = new ColumnOrSuperColumn();
+        csoc.setSuper_column(getSuperColumnForInsert(ByteBufferUtil.bytes("val1"),
+                                                     Arrays.asList(getColumnForInsert(ByteBufferUtil.bytes(1), ByteBufferUtil.bytes("value1")),
+                                                                   getColumnForInsert(ByteBufferUtil.bytes(2), ByteBufferUtil.bytes("value2")))));
+        mutation.setColumn_or_supercolumn(csoc);
+
+        Mutation mutation2 = new Mutation();
+        ColumnOrSuperColumn csoc2 = new ColumnOrSuperColumn();
+        csoc2.setSuper_column(getSuperColumnForInsert(ByteBufferUtil.bytes("val2"),
+                                                      Arrays.asList(getColumnForInsert(ByteBufferUtil.bytes(4), ByteBufferUtil.bytes("value4")),
+                                                                    getColumnForInsert(ByteBufferUtil.bytes(5), ByteBufferUtil.bytes("value5")))));
+        mutation2.setColumn_or_supercolumn(csoc2);
+
+        client.batch_mutate(Collections.singletonMap(ByteBufferUtil.bytes("key1"),
+                                                     Collections.singletonMap(currentDenseTable(), Arrays.asList(mutation, mutation2))),
+                            ONE);
+
+        client.batch_mutate(Collections.singletonMap(ByteBufferUtil.bytes("key2"),
+                                                     Collections.singletonMap(currentDenseTable(), Arrays.asList(mutation, mutation2))),
+                            ONE);
+    }
+
+    private void populateSparseTable() throws Throwable
+    {
+        Cassandra.Client client = getClient();
+
+        Mutation mutation = new Mutation();
+        ColumnOrSuperColumn csoc = new ColumnOrSuperColumn();
+        csoc.setSuper_column(getSuperColumnForInsert(ByteBufferUtil.bytes("val1"),
+                                                     Arrays.asList(getColumnForInsert(ByteBufferUtil.bytes("value1"), ByteBufferUtil.bytes(1L)),
+                                                                   getColumnForInsert(ByteBufferUtil.bytes("value2"), ByteBufferUtil.bytes(2L)),
+                                                                   getColumnForInsert(ByteBufferUtil.bytes("col1"), ByteBufferUtil.bytes(3L)),
+                                                                   getColumnForInsert(ByteBufferUtil.bytes("col2"), ByteBufferUtil.bytes(4L)))));
+        mutation.setColumn_or_supercolumn(csoc);
+
+        Mutation mutation2 = new Mutation();
+        ColumnOrSuperColumn csoc2 = new ColumnOrSuperColumn();
+        csoc2.setSuper_column(getSuperColumnForInsert(ByteBufferUtil.bytes("val2"),
+                                                      Arrays.asList(getColumnForInsert(ByteBufferUtil.bytes("value1"), ByteBufferUtil.bytes(1L)),
+                                                                    getColumnForInsert(ByteBufferUtil.bytes("value2"), ByteBufferUtil.bytes(2L)),
+                                                                    getColumnForInsert(ByteBufferUtil.bytes("col1"), ByteBufferUtil.bytes(3L)),
+                                                                    getColumnForInsert(ByteBufferUtil.bytes("col2"), ByteBufferUtil.bytes(4L)))));
+        mutation2.setColumn_or_supercolumn(csoc2);
+
+        client.batch_mutate(Collections.singletonMap(ByteBufferUtil.bytes("key1"),
+                                                     Collections.singletonMap(currentSparseTable(), Arrays.asList(mutation, mutation2))),
+                            ONE);
+
+        client.batch_mutate(Collections.singletonMap(ByteBufferUtil.bytes("key2"),
+                                                     Collections.singletonMap(currentSparseTable(), Arrays.asList(mutation, mutation2))),
+                            ONE);
+    }
+
+    private void populateCounterTable() throws Throwable
+    {
+        Cassandra.Client client = getClient();
+
+        ColumnParent cp = new ColumnParent(currentCounterTable());
+        cp.setSuper_column(ByteBufferUtil.bytes("ck1"));
+        client.add(ByteBufferUtil.bytes("key1"),
+                   cp,
+                   new CounterColumn(ByteBufferUtil.bytes("counter1"), 10L),
+                   ONE);
+        cp = new ColumnParent(currentCounterTable());
+        cp.setSuper_column(ByteBufferUtil.bytes("ck1"));
+        client.add(ByteBufferUtil.bytes("key1"),
+                   cp,
+                   new CounterColumn(ByteBufferUtil.bytes("counter2"), 5L),
+                   ONE);
+        cp = new ColumnParent(currentCounterTable());
+        cp.setSuper_column(ByteBufferUtil.bytes("ck1"));
+        client.add(ByteBufferUtil.bytes("key2"),
+                   cp,
+                   new CounterColumn(ByteBufferUtil.bytes("counter1"), 10L),
+                   ONE);
+        cp = new ColumnParent(currentCounterTable());
+        cp.setSuper_column(ByteBufferUtil.bytes("ck1"));
+        client.add(ByteBufferUtil.bytes("key2"),
+                   cp,
+                   new CounterColumn(ByteBufferUtil.bytes("counter2"), 5L),
+                   ONE);
+    }
+
+    private String currentCounterTable()
+    {
+        return currentTable() + "_counter";
+    }
+
+    private String currentSparseTable()
+    {
+        return currentTable() + "_sparse";
+    }
+
+    private String currentDenseTable()
+    {
+        return currentTable();
+    }
+
+    private Column getColumnForInsert(ByteBuffer columnName, ByteBuffer value)
+    {
+        Column column = new Column();
+        column.setName(columnName);
+        column.setValue(value);
+        column.setTimestamp(System.currentTimeMillis());
+        return column;
+    }
+
+    private SuperColumn getSuperColumnForInsert(ByteBuffer columnName, List<Column> columns)
+    {
+        SuperColumn column = new SuperColumn();
+        column.setName(columnName);
+        for (Column c : columns)
+            column.addToColumns(c);
+        return column;
+    }
+
+    public void beforeAndAfterFlush(CheckedFunction runnable) throws Throwable
+    {
+        runnable.apply();
+        flushAll();
+        runnable.apply();
+    }
+
+    private void flushAll()
+    {
+        for (String cfName : new String[]{ currentTable(), currentSparseTable(), currentCounterTable() })
+            Keyspace.open(KEYSPACE).getColumnFamilyStore(cfName);
+    }
+}
diff --git a/test/unit/org/apache/cassandra/cql3/validation/operations/ThriftCQLTester.java b/test/unit/org/apache/cassandra/cql3/validation/operations/ThriftCQLTester.java
new file mode 100644
index 0000000000..a77e861474
--- /dev/null
+++ b/test/unit/org/apache/cassandra/cql3/validation/operations/ThriftCQLTester.java
@@ -0,0 +1,90 @@
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.cql3.validation.operations;
+
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.ServerSocket;
+import java.net.UnknownHostException;
+
+import org.junit.AfterClass;
+import org.junit.BeforeClass;
+
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.cql3.*;
+import org.apache.cassandra.dht.ByteOrderedPartitioner;
+import org.apache.cassandra.dht.Murmur3Partitioner;
+import org.apache.cassandra.service.*;
+import org.apache.cassandra.thrift.*;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.transport.TFramedTransport;
+import org.apache.thrift.transport.TSocket;
+
+public class ThriftCQLTester extends CQLTester
+{
+    private Cassandra.Client client;
+
+    private static ThriftServer thriftServer;
+    private static int thriftPort;
+
+    static {
+        try (ServerSocket serverSocket = new ServerSocket(0))
+        {
+            thriftPort = serverSocket.getLocalPort();
+        }
+        catch (IOException e)
+        {
+            // ignore
+        }
+    }
+
+    @BeforeClass
+    public static void setup() throws Exception
+    {
+        StorageService.instance.initServer(0);
+
+        if (thriftServer == null || ! thriftServer.isRunning())
+        {
+            thriftServer = new ThriftServer(InetAddress.getLocalHost(), thriftPort, 50);
+            thriftServer.start();
+        }
+    }
+
+    @AfterClass
+    public static void teardown()
+    {
+        if (thriftServer != null && thriftServer.isRunning())
+        {
+            thriftServer.stop();
+        }
+    }
+
+    public Cassandra.Client getClient() throws Throwable
+    {
+        return getClient(InetAddress.getLocalHost().getHostName(), thriftPort);
+    }
+
+    public Cassandra.Client getClient(String hostname, int thriftPort) throws Throwable
+	{
+        if (client == null)
+            client = new Cassandra.Client(new TBinaryProtocol(new TFramedTransportFactory().openTransport(hostname, thriftPort)));
+
+        return client;
+    }
+}
diff --git a/test/unit/org/apache/cassandra/db/ColumnFamilyStoreCQLHelperTest.java b/test/unit/org/apache/cassandra/db/ColumnFamilyStoreCQLHelperTest.java
index c2e5cb79bf..714b61acd2 100644
--- a/test/unit/org/apache/cassandra/db/ColumnFamilyStoreCQLHelperTest.java
+++ b/test/unit/org/apache/cassandra/db/ColumnFamilyStoreCQLHelperTest.java
@@ -650,9 +650,8 @@ public class ColumnFamilyStoreCQLHelperTest extends CQLTester
         final String TABLE = "test_table_1";
 
         CFMetaData cfm = CFMetaData.Builder.createSuper(KEYSPACE, TABLE, false)
-                                           .addPartitionKey("pk", BytesType.instance)
-                                           .addClusteringColumn("c1", AsciiType.instance)
-                                           .addClusteringColumn("c2", AsciiType.instance)
+                                           .addPartitionKey("key", BytesType.instance)
+                                           .addClusteringColumn("column1", AsciiType.instance)
                                            .addRegularColumn("", MapType.getInstance(Int32Type.instance, AsciiType.instance, true))
                                            .build();
 
@@ -668,11 +667,10 @@ public class ColumnFamilyStoreCQLHelperTest extends CQLTester
         "Approximate structure, for reference:\n" +
         "(this should not be used to reproduce this schema)\n\n" +
         "CREATE TABLE IF NOT EXISTS " + KEYSPACE + "." + TABLE + " (\n" +
-        "\tpk blob,\n" +
-        "\tc1 ascii,\n" +
-        "\tc2 ascii,\n" +
+        "\tkey blob,\n" +
+        "\tcolumn1 ascii,\n" +
         "\t\"\" map<int, ascii>,\n" +
-        "\tPRIMARY KEY (pk, c1, c2))\n" +
+        "\tPRIMARY KEY (key, column1))\n" +
         "\tWITH ID = " + cfs.metadata.cfId + "\n" +
         "\tAND COMPACT STORAGE"));
     }
diff --git a/test/unit/org/apache/cassandra/schema/LegacySchemaMigratorTest.java b/test/unit/org/apache/cassandra/schema/LegacySchemaMigratorTest.java
index fe91ddc7a8..f7b0e47f1d 100644
--- a/test/unit/org/apache/cassandra/schema/LegacySchemaMigratorTest.java
+++ b/test/unit/org/apache/cassandra/schema/LegacySchemaMigratorTest.java
@@ -164,14 +164,8 @@ public class LegacySchemaMigratorTest
                                                                     .compaction(CompactionParams.scts(compactionOptions)),
                                                         SchemaLoader.standardCFMD(ks1, "StandardGCGS0").gcGraceSeconds(0),
                                                         SchemaLoader.standardCFMD(ks1, "StandardLong1"),
-                                                        SchemaLoader.superCFMD(ks1, "Super1", LongType.instance),
-                                                        SchemaLoader.superCFMD(ks1, "Super2", UTF8Type.instance),
-                                                        SchemaLoader.superCFMD(ks1, "Super5", BytesType.instance),
-                                                        SchemaLoader.superCFMD(ks1, "Super6", LexicalUUIDType.instance, UTF8Type.instance),
                                                         SchemaLoader.keysIndexCFMD(ks1, "Indexed1", true),
                                                         SchemaLoader.keysIndexCFMD(ks1, "Indexed2", false),
-                                                        SchemaLoader.superCFMD(ks1, "SuperDirectGC", BytesType.instance)
-                                                                    .gcGraceSeconds(0),
                                                         SchemaLoader.jdbcCFMD(ks1, "JdbcUtf8", UTF8Type.instance)
                                                                     .addColumnDefinition(SchemaLoader.utf8Column(ks1, "JdbcUtf8")),
                                                         SchemaLoader.jdbcCFMD(ks1, "JdbcLong", LongType.instance),
@@ -190,8 +184,6 @@ public class LegacySchemaMigratorTest
         keyspaces.add(KeyspaceMetadata.create(ks2,
                                               KeyspaceParams.simple(1),
                                               Tables.of(SchemaLoader.standardCFMD(ks2, "Standard1"),
-                                                        SchemaLoader.superCFMD(ks2, "Super3", BytesType.instance),
-                                                        SchemaLoader.superCFMD(ks2, "Super4", TimeUUIDType.instance),
                                                         SchemaLoader.keysIndexCFMD(ks2, "Indexed1", true),
                                                         SchemaLoader.compositeIndexCFMD(ks2, "Indexed2", true),
                                                         SchemaLoader.compositeIndexCFMD(ks2, "Indexed3", true)
@@ -206,10 +198,7 @@ public class LegacySchemaMigratorTest
         // Keyspace 4
         keyspaces.add(KeyspaceMetadata.create(ks4,
                                               KeyspaceParams.simple(3),
-                                              Tables.of(SchemaLoader.standardCFMD(ks4, "Standard1"),
-                                                        SchemaLoader.superCFMD(ks4, "Super3", BytesType.instance),
-                                                        SchemaLoader.superCFMD(ks4, "Super4", TimeUUIDType.instance),
-                                                        SchemaLoader.superCFMD(ks4, "Super5", TimeUUIDType.instance, BytesType.instance))));
+                                              Tables.of(SchemaLoader.standardCFMD(ks4, "Standard1"))));
 
         // Keyspace 5
         keyspaces.add(KeyspaceMetadata.create(ks5,
