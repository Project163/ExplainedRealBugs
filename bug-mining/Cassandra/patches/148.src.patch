diff --git a/src/java/org/apache/cassandra/db/ColumnFamily.java b/src/java/org/apache/cassandra/db/ColumnFamily.java
index c3bddd4296..cb878337db 100644
--- a/src/java/org/apache/cassandra/db/ColumnFamily.java
+++ b/src/java/org/apache/cassandra/db/ColumnFamily.java
@@ -35,7 +35,6 @@ import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.io.ICompactSerializer2;
 import org.apache.cassandra.db.filter.QueryPath;
 import org.apache.cassandra.db.marshal.AbstractType;
-import org.apache.cassandra.db.marshal.MarshalException;
 
 
 public final class ColumnFamily implements IColumnContainer
@@ -121,12 +120,13 @@ public final class ColumnFamily implements IColumnContainer
      *  We need to go through each column
      *  in the column family and resolve it before adding
     */
-    void addColumns(ColumnFamily cf)
+    public void addAll(ColumnFamily cf)
     {
         for (IColumn column : cf.getSortedColumns())
         {
             addColumn(column);
         }
+        delete(cf);
     }
 
     public ICompactSerializer2<IColumn> getColumnSerializer()
@@ -415,8 +415,7 @@ public final class ColumnFamily implements IColumnContainer
         for (ColumnFamily cf2 : columnFamilies)
         {
             assert cf.name().equals(cf2.name());
-            cf.addColumns(cf2);
-            cf.delete(cf2);
+            cf.addAll(cf2);
         }
         return cf;
     }
diff --git a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
index 001c644b4b..96bb18ba65 100644
--- a/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
+++ b/src/java/org/apache/cassandra/db/ColumnFamilyStore.java
@@ -560,26 +560,6 @@ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean
         CompactionManager.instance().submit(this);
     }
 
-    private PriorityQueue<FileStruct> initializePriorityQueue(Collection<SSTableReader> sstables, List<Range> ranges) throws IOException
-    {
-        PriorityQueue<FileStruct> pq = new PriorityQueue<FileStruct>();
-        if (sstables.size() > 1 || (ranges != null && sstables.size() > 0))
-        {
-            FileStruct fs = null;
-            for (SSTableReader sstable : sstables)
-            {
-                fs = sstable.getFileStruct();
-                fs.advance(true);
-                if (fs.isExhausted())
-                {
-                    continue;
-                }
-                pq.add(fs);
-            }
-        }
-        return pq;
-    }
-
     /*
      * Group files of similar size into buckets.
      */
@@ -766,150 +746,67 @@ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean
      */
     List<SSTableReader> doFileAntiCompaction(Collection<SSTableReader> sstables, List<Range> ranges, EndPoint target) throws IOException
     {
-        List<SSTableReader> results = new ArrayList<SSTableReader>();
-        long startTime = System.currentTimeMillis();
-        long totalBytesRead = 0;
-        long totalBytesWritten = 0;
-        long totalkeysRead = 0;
-        long totalkeysWritten = 0;
-        String rangeFileLocation;
-        String mergedFileName;
+        logger_.info("AntiCompacting [" + StringUtils.join(sstables, ",") + "]");
         // Calculate the expected compacted filesize
-        long expectedRangeFileSize = getExpectedCompactedFileSize(sstables);
-        /* in the worst case a node will be giving out half of its data so we take a chance */
-        expectedRangeFileSize = expectedRangeFileSize / 2;
-        rangeFileLocation = DatabaseDescriptor.getDataFileLocationForTable(table_, expectedRangeFileSize);
-        // If the compaction file path is null that means we have no space left for this compaction.
-        if (rangeFileLocation == null)
-        {
-            logger_.error("Total bytes to be written for range compaction  ..."
-                          + expectedRangeFileSize + "   is greater than the safe limit of the disk space available.");
-            return results;
-        }
-        PriorityQueue<FileStruct> pq = initializePriorityQueue(sstables, ranges);
-        if (pq.isEmpty())
+        long expectedRangeFileSize = getExpectedCompactedFileSize(sstables) / 2;
+        String compactionFileLocation = DatabaseDescriptor.getDataFileLocationForTable(table_, expectedRangeFileSize);
+        if (compactionFileLocation == null)
         {
-            return results;
+            throw new UnsupportedOperationException("disk full");
         }
+        List<SSTableReader> results = new ArrayList<SSTableReader>();
 
-        mergedFileName = getTempSSTableFileName();
-        SSTableWriter rangeWriter = null;
-        String lastkey = null;
-        List<FileStruct> lfs = new ArrayList<FileStruct>();
-        DataOutputBuffer bufOut = new DataOutputBuffer();
-        int expectedBloomFilterSize = SSTableReader.getApproximateKeyCount(sstables);
-        expectedBloomFilterSize = (expectedBloomFilterSize > 0) ? expectedBloomFilterSize : SSTableReader.indexInterval();
+        long startTime = System.currentTimeMillis();
+        long totalkeysWritten = 0;
+
+        int expectedBloomFilterSize = Math.max(SSTableReader.indexInterval(), SSTableReader.getApproximateKeyCount(sstables) / 2);
         if (logger_.isDebugEnabled())
           logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
-        List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();
 
-        while (pq.size() > 0 || lfs.size() > 0)
+        SSTableWriter writer = null;
+        CompactionIterator ci = new CompactionIterator(sstables);
+
+        try
         {
-            FileStruct fs = null;
-            if (pq.size() > 0)
+            if (!ci.hasNext())
             {
-                fs = pq.poll();
+                logger_.warn("Nothing to compact (all files empty or corrupt). This should not happen.");
+                return results;
             }
-            if (fs != null
-                && (lastkey == null || lastkey.equals(fs.getKey())))
-            {
-                // The keys are the same so we need to add this to the
-                // ldfs list
-                lastkey = fs.getKey();
-                lfs.add(fs);
-            }
-            else
+
+            while (ci.hasNext())
             {
-                Collections.sort(lfs, new FileStructComparator());
-                ColumnFamily columnFamily;
-                bufOut.reset();
-                if (lfs.size() > 1)
+                CompactionIterator.CompactedRow row = ci.next();
+                if (Range.isTokenInRanges(StorageService.getPartitioner().getToken(row.key), ranges))
                 {
-                    for (FileStruct filestruct : lfs)
-                    {
-                        // We want to add only 2 and resolve them right there in order to save on memory footprint
-                        if (columnFamilies.size() > 1)
-                        {
-                            // Now merge the 2 column families
-                            merge(columnFamilies);
-                        }
-                        // deserialize into column families
-                        columnFamilies.add(filestruct.getColumnFamily());
-                    }
-                    // Now after merging all crap append to the sstable
-                    columnFamily = resolveAndRemoveDeleted(columnFamilies);
-                    columnFamilies.clear();
-                    if (columnFamily != null)
-                    {
-                        ColumnFamily.serializer().serializeWithIndexes(columnFamily, bufOut);
-                    }
-                }
-                else
-                {
-                    // TODO deserializing only to reserialize is dumb
-                    FileStruct filestruct = lfs.get(0);
-                    ColumnFamily.serializer().serializeWithIndexes(filestruct.getColumnFamily(), bufOut);
-                }
-                if (Range.isTokenInRanges(StorageService.getPartitioner().getToken(lastkey), ranges))
-                {
-                    if (rangeWriter == null)
+                    if (writer == null)
                     {
                         if (target != null)
                         {
-                            rangeFileLocation = rangeFileLocation + File.separator + "bootstrap";
+                            compactionFileLocation = compactionFileLocation + File.separator + "bootstrap";
                         }
-                        FileUtils.createDirectory(rangeFileLocation);
-                        String fname = new File(rangeFileLocation, mergedFileName).getAbsolutePath();
-                        rangeWriter = new SSTableWriter(fname, expectedBloomFilterSize, StorageService.getPartitioner());
+                        FileUtils.createDirectory(compactionFileLocation);
+                        String newFilename = new File(compactionFileLocation, getTempSSTableFileName()).getAbsolutePath();
+                        writer = new SSTableWriter(newFilename, expectedBloomFilterSize, StorageService.getPartitioner());
                     }
-                    rangeWriter.append(lastkey, bufOut);
-                }
-                totalkeysWritten++;
-                for (FileStruct filestruct : lfs)
-                {
-                    filestruct.advance(true);
-                    if (filestruct.isExhausted())
-                    {
-                        continue;
-                    }
-                    /* keep on looping until we find a key in the range */
-                    while (!Range.isTokenInRanges(StorageService.getPartitioner().getToken(filestruct.getKey()), ranges))
-                    {
-                        filestruct.advance(true);
-                        if (filestruct.isExhausted())
-                        {
-                            break;
-                        }
-                    }
-                    if (!filestruct.isExhausted())
-                    {
-                        pq.add(filestruct);
-                    }
-                    totalkeysRead++;
-                }
-                lfs.clear();
-                lastkey = null;
-                if (fs != null)
-                {
-                    // Add back the fs since we processed the rest of
-                    // filestructs
-                    pq.add(fs);
+                    writer.append(row.key, row.buffer);
+                    totalkeysWritten++;
                 }
             }
         }
-
-        if (rangeWriter != null)
+        finally
         {
-            results.add(rangeWriter.closeAndOpenReader());
+            ci.close();
         }
 
-        if (logger_.isDebugEnabled())
+        if (writer != null)
         {
-            logger_.debug("Total time taken for range split   ..." + (System.currentTimeMillis() - startTime));
-            logger_.debug("Total bytes Read for range split  ..." + totalBytesRead);
-            logger_.debug("Total bytes written for range split  ..."
-                          + totalBytesWritten + "   Total keys read ..." + totalkeysRead);
+            results.add(writer.closeAndOpenReader());
+            String format = "AntiCompacted to %s.  %d/%d bytes for %d keys.  Time: %dms.";
+            long dTime = System.currentTimeMillis() - startTime;
+            logger_.info(String.format(format, writer.getFilename(), getTotalBytes(sstables), results.get(0).length(), totalkeysWritten, dTime));
         }
+
         return results;
     }
 
@@ -938,111 +835,59 @@ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean
         }
 
         long startTime = System.currentTimeMillis();
-        long totalBytesRead = 0;
-        long totalkeysRead = 0;
         long totalkeysWritten = 0;
-        PriorityQueue<FileStruct> pq = initializePriorityQueue(sstables, null);
-
-        if (pq.isEmpty())
-        {
-            logger_.warn("Nothing to compact (all files empty or corrupt). This should not happen.");
-            // TODO clean out bad files, if any
-            return 0;
-        }
 
-        int expectedBloomFilterSize = SSTableReader.getApproximateKeyCount(sstables);
-        if (expectedBloomFilterSize < 0)
-            expectedBloomFilterSize = SSTableReader.indexInterval();
-        String newFilename = new File(compactionFileLocation, getTempSSTableFileName()).getAbsolutePath();
-        SSTableWriter writer = new SSTableWriter(newFilename, expectedBloomFilterSize, StorageService.getPartitioner());
-        SSTableReader ssTable = null;
-        String lastkey = null;
-        List<FileStruct> lfs = new ArrayList<FileStruct>();
-        DataOutputBuffer bufOut = new DataOutputBuffer();
+        int expectedBloomFilterSize = Math.max(SSTableReader.indexInterval(), SSTableReader.getApproximateKeyCount(sstables));
         if (logger_.isDebugEnabled())
           logger_.debug("Expected bloom filter size : " + expectedBloomFilterSize);
-        List<ColumnFamily> columnFamilies = new ArrayList<ColumnFamily>();
 
-        while (pq.size() > 0 || lfs.size() > 0)
+        SSTableWriter writer;
+        CompactionIterator ci = new CompactionIterator(sstables);
+
+        try
         {
-            FileStruct fs = null;
-            if (pq.size() > 0)
+            if (!ci.hasNext())
             {
-                fs = pq.poll();
+                logger_.warn("Nothing to compact (all files empty or corrupt). This should not happen.");
+                return 0;
             }
-            if (fs != null
-                && (lastkey == null || lastkey.equals(fs.getKey())))
-            {
-                // The keys are the same so we need to add this to the
-                // ldfs list
-                lastkey = fs.getKey();
-                lfs.add(fs);
-            }
-            else
-            {
-                Collections.sort(lfs, new FileStructComparator());
-                ColumnFamily columnFamily;
-                bufOut.reset();
-                if (lfs.size() > 1)
-                {
-                    for (FileStruct filestruct : lfs)
-                    {
-                        // We want to add only 2 and resolve them right there in order to save on memory footprint
-                        if (columnFamilies.size() > 1)
-                        {
-                            merge(columnFamilies);
-                        }
-                        // deserialize into column families
-                        columnFamilies.add(filestruct.getColumnFamily());
-                    }
-                    // Now after merging all crap append to the sstable
-                    columnFamily = resolveAndRemoveDeleted(columnFamilies);
-                    columnFamilies.clear();
-                    if (columnFamily != null)
-                    {
-                        ColumnFamily.serializer().serializeWithIndexes(columnFamily, bufOut);
-                    }
-                }
-                else
-                {
-                    // TODO deserializing only to reserialize is dumb
-                    FileStruct filestruct = lfs.get(0);
-                    ColumnFamily.serializer().serializeWithIndexes(filestruct.getColumnFamily(), bufOut);
-                }
 
-                writer.append(lastkey, bufOut);
-                totalkeysWritten++;
+            String newFilename = new File(compactionFileLocation, getTempSSTableFileName()).getAbsolutePath();
+            writer = new SSTableWriter(newFilename, expectedBloomFilterSize, StorageService.getPartitioner());
 
-                for (FileStruct filestruct : lfs)
-                {
-                    filestruct.advance(true);
-                    if (filestruct.isExhausted())
-                    {
-                        continue;
-                    }
-                    pq.add(filestruct);
-                    totalkeysRead++;
-                }
-                lfs.clear();
-                lastkey = null;
-                if (fs != null)
-                {
-                    /* Add back the fs since we processed the rest of filestructs */
-                    pq.add(fs);
-                }
+            while (ci.hasNext())
+            {
+                CompactionIterator.CompactedRow row = ci.next();
+                writer.append(row.key, row.buffer);
+                totalkeysWritten++;
             }
         }
-        ssTable = writer.closeAndOpenReader();
+        finally
+        {
+            ci.close();
+        }
+
+        SSTableReader ssTable = writer.closeAndOpenReader();
         ssTables_.add(ssTable);
         ssTables_.markCompacted(sstables);
         CompactionManager.instance().submit(ColumnFamilyStore.this);
 
-        String format = "Compacted to %s.  %d/%d bytes for %d/%d keys read/written.  Time: %dms.";
+        String format = "Compacted to %s.  %d/%d bytes for %d keys.  Time: %dms.";
         long dTime = System.currentTimeMillis() - startTime;
-        logger_.info(String.format(format, writer.getFilename(), totalBytesRead, ssTable.length(), totalkeysRead, totalkeysWritten, dTime));
+        logger_.info(String.format(format, writer.getFilename(), getTotalBytes(sstables), ssTable.length(), totalkeysWritten, dTime));
         return sstables.size();
     }
 
+    private long getTotalBytes(Iterable<SSTableReader> sstables)
+    {
+        long sum = 0;
+        for (SSTableReader sstable : sstables)
+        {
+            sum += sstable.length();
+        }
+        return sum;
+    }
+
     public static List<Memtable> getUnflushedMemtables(String cfName)
     {
         return new ArrayList<Memtable>(getMemtablesPendingFlushNotNull(cfName));
@@ -1341,23 +1186,24 @@ public final class ColumnFamilyStore implements ColumnFamilyStoreMBean
         // sstables
         for (SSTableReader sstable : ssTables_)
         {
-            final SSTableScanner fs = sstable.getScanner();
-            fs.seekTo(startWith);
-            iterators.add(new Iterator<String>()
+            final SSTableScanner scanner = sstable.getScanner();
+            scanner.seekTo(startWith);
+            Iterator<String> iter = new Iterator<String>()
             {
                 public boolean hasNext()
                 {
-                    return fs.hasNext();
+                    return scanner.hasNext();
                 }
                 public String next()
                 {
-                    return fs.next().getKey();
+                    return scanner.next().getKey();
                 }
                 public void remove()
                 {
                     throw new UnsupportedOperationException();
                 }
-            });
+            };
+            iterators.add(iter);
         }
 
         Iterator<String> collated = IteratorUtils.collatedIterator(comparator, iterators);
diff --git a/src/java/org/apache/cassandra/db/FileStructComparator.java b/src/java/org/apache/cassandra/db/FileStructComparator.java
deleted file mode 100644
index e81a992fad..0000000000
--- a/src/java/org/apache/cassandra/db/FileStructComparator.java
+++ /dev/null
@@ -1,31 +0,0 @@
-/*
-* Licensed to the Apache Software Foundation (ASF) under one
-* or more contributor license agreements.  See the NOTICE file
-* distributed with this work for additional information
-* regarding copyright ownership.  The ASF licenses this file
-* to you under the Apache License, Version 2.0 (the
-* "License"); you may not use this file except in compliance
-* with the License.  You may obtain a copy of the License at
-*
-*    http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing,
-* software distributed under the License is distributed on an
-* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-* KIND, either express or implied.  See the License for the
-* specific language governing permissions and limitations
-* under the License.
-*/
-package org.apache.cassandra.db;
-
-import java.util.Comparator;
-
-import org.apache.cassandra.io.FileStruct;
-
-class FileStructComparator implements Comparator<FileStruct>
-{
-    public int compare(FileStruct f, FileStruct f2)
-    {
-        return f.getFileName().compareTo(f2.getFileName());
-    }
-}
\ No newline at end of file
diff --git a/src/java/org/apache/cassandra/db/Memtable.java b/src/java/org/apache/cassandra/db/Memtable.java
index d88e004202..696ae5a645 100644
--- a/src/java/org/apache/cassandra/db/Memtable.java
+++ b/src/java/org/apache/cassandra/db/Memtable.java
@@ -153,7 +153,7 @@ public class Memtable implements Comparable<Memtable>
         {
             int oldSize = oldCf.size();
             int oldObjectCount = oldCf.getColumnCount();
-            oldCf.addColumns(columnFamily);
+            oldCf.addAll(columnFamily);
             int newSize = oldCf.size();
             int newObjectCount = oldCf.getColumnCount();
             resolveSize(oldSize, newSize);
diff --git a/src/java/org/apache/cassandra/io/CompactionIterator.java b/src/java/org/apache/cassandra/io/CompactionIterator.java
new file mode 100644
index 0000000000..b65e132017
--- /dev/null
+++ b/src/java/org/apache/cassandra/io/CompactionIterator.java
@@ -0,0 +1,113 @@
+package org.apache.cassandra.io;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Comparator;
+
+import org.apache.commons.collections.iterators.CollatingIterator;
+
+import org.apache.cassandra.utils.ReducingIterator;
+import org.apache.cassandra.db.ColumnFamily;
+
+public class CompactionIterator extends ReducingIterator<IteratingRow, CompactionIterator.CompactedRow> implements Closeable
+{
+    private final List<IteratingRow> rows = new ArrayList<IteratingRow>();
+
+    @SuppressWarnings("unchecked")
+    public CompactionIterator(Iterable<SSTableReader> sstables) throws IOException
+    {
+        super(getCollatingIterator(sstables));
+    }
+
+    @SuppressWarnings("unchecked")
+    private static CollatingIterator getCollatingIterator(Iterable<SSTableReader> sstables) throws IOException
+    {
+        // CollatingIterator has a bug that causes NPE when you try to use default comparator. :(
+        CollatingIterator iter = new CollatingIterator(new Comparator()
+        {
+            public int compare(Object o1, Object o2)
+            {
+                return ((Comparable)o1).compareTo(o2);
+            }
+        });
+        for (SSTableReader sstable : sstables)
+        {
+            iter.addIterator(sstable.getScanner());
+        }
+        return iter;
+    }
+
+    @Override
+    protected boolean isEqual(IteratingRow o1, IteratingRow o2)
+    {
+        return o1.getKey().equals(o2.getKey());
+    }
+
+    public void reduce(IteratingRow current)
+    {
+        rows.add(current);
+    }
+
+    protected CompactedRow getReduced()
+    {
+        try
+        {
+            return getReducedRaw();
+        }
+        catch (IOException e)
+        {
+            throw new RuntimeException(e);
+        }
+    }
+
+    protected CompactedRow getReducedRaw() throws IOException
+    {
+        DataOutputBuffer buffer = new DataOutputBuffer();
+        String key = rows.get(0).getKey();
+        if (rows.size() > 1)
+        {
+            ColumnFamily cf = null;
+            for (IteratingRow row : rows)
+            {
+                if (cf == null)
+                {
+                    cf = row.getColumnFamily();
+                }
+                else
+                {
+                    cf.addAll(row.getColumnFamily());
+                }
+            }
+            ColumnFamily.serializer().serializeWithIndexes(cf, buffer);
+        }
+        else
+        {
+            assert rows.size() == 1;
+            rows.get(0).echoData(buffer);
+        }
+        rows.clear();
+        return new CompactedRow(key, buffer);
+    }
+
+    public void close() throws IOException
+    {
+        for (Object o : ((CollatingIterator)source).getIterators())
+        {
+            ((SSTableScanner)o).close();
+        }
+    }
+
+    public static class CompactedRow
+    {
+        public final String key;
+        public final DataOutputBuffer buffer;
+
+        public CompactedRow(String key, DataOutputBuffer buffer)
+        {
+            this.key = key;
+            this.buffer = buffer;
+        }
+    }
+}
diff --git a/src/java/org/apache/cassandra/io/FileStruct.java b/src/java/org/apache/cassandra/io/FileStruct.java
deleted file mode 100644
index b561239861..0000000000
--- a/src/java/org/apache/cassandra/io/FileStruct.java
+++ /dev/null
@@ -1,195 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.cassandra.io;
-
-import java.io.IOException;
-import java.io.File;
-import java.util.Iterator;
-
-import org.apache.cassandra.db.IColumn;
-import org.apache.cassandra.db.ColumnFamily;
-import org.apache.cassandra.config.DatabaseDescriptor;
-
-import org.apache.log4j.Logger;
-import com.google.common.collect.AbstractIterator;
-
-
-public class FileStruct implements Comparable<FileStruct>, Iterator<String>
-{
-    private static Logger logger = Logger.getLogger(FileStruct.class);
-
-    private IteratingRow row;
-    private boolean exhausted = false;
-    private BufferedRandomAccessFile file;
-    private SSTableReader sstable;
-    private FileStructIterator iterator;
-
-    FileStruct(SSTableReader sstable) throws IOException
-    {
-        // TODO this is used for both compactions and key ranges.  the buffer sizes we want
-        // to use for these ops are very different.  here we are leaning towards the key-range
-        // use case since that is more common.  What we really want is to split those
-        // two uses of this class up.
-        this.file = new BufferedRandomAccessFile(sstable.getFilename(), "r", 256 * 1024);
-        this.sstable = sstable;
-    }
-
-    public String getFileName()
-    {
-        return file.getPath();
-    }
-
-    public void close() throws IOException
-    {
-        file.close();
-    }
-
-    public boolean isExhausted()
-    {
-        return exhausted;
-    }
-
-    public String getKey()
-    {
-        return row.getKey();
-    }
-
-    public ColumnFamily getColumnFamily()
-    {
-        return row.getEmptyColumnFamily();
-    }
-
-    public int compareTo(FileStruct f)
-    {
-        return sstable.getPartitioner().getDecoratedKeyComparator().compare(getKey(), f.getKey());
-    }
-
-    public void seekTo(String seekKey)
-    {
-        try
-        {
-            long position = sstable.getNearestPosition(seekKey);
-            if (position < 0)
-            {
-                exhausted = true;
-                return;
-            }
-            file.seek(position);
-            advance(false);
-        }
-        catch (IOException e)
-        {
-            throw new RuntimeException("corrupt sstable", e);
-        }
-    }
-
-    /*
-     * Read the next key from the data file.
-     * Caller must check isExhausted after each call to see if further
-     * reads are valid.
-     * Do not mix with calls to the iterator interface (next/hasnext).
-     * @deprecated -- prefer the iterator interface.
-     */
-    public void advance(boolean materialize) throws IOException
-    {
-        // TODO r/m materialize option -- use iterableness!
-        if (exhausted)
-        {
-            throw new IndexOutOfBoundsException();
-        }
-
-        if (file.isEOF())
-        {
-            file.close();
-            exhausted = true;
-            return;
-        }
-
-        row = new IteratingRow(file, sstable);
-        if (materialize)
-        {
-            while (row.hasNext())
-            {
-                IColumn column = row.next();
-                row.getEmptyColumnFamily().addColumn(column);
-            }
-        }
-        else
-        {
-            row.skipRemaining();
-        }
-    }
-
-    public boolean hasNext()
-    {
-        if (iterator == null)
-            iterator = new FileStructIterator();
-        return iterator.hasNext();
-    }
-
-    /** do not mix with manual calls to advance(). */
-    public String next()
-    {
-        if (iterator == null)
-            iterator = new FileStructIterator();
-        return iterator.next();
-    }
-
-    public void remove()
-    {
-        throw new UnsupportedOperationException();
-    }
-
-    private class FileStructIterator extends AbstractIterator<String>
-    {
-        public FileStructIterator()
-        {
-            if (row == null)
-            {
-                if (!isExhausted())
-                {
-                    forward();
-                }
-            }
-        }
-
-        private void forward()
-        {
-            try
-            {
-                advance(false);
-            }
-            catch (IOException e)
-            {
-                throw new RuntimeException(e);
-            }
-        }
-
-        protected String computeNext()
-        {
-            if (isExhausted())
-            {
-                return endOfData();
-            }
-            String oldKey = getKey();
-            forward();
-            return oldKey;
-        }
-    }
-}
diff --git a/src/java/org/apache/cassandra/io/IteratingRow.java b/src/java/org/apache/cassandra/io/IteratingRow.java
index 5ace95f0e0..628fe50b9e 100644
--- a/src/java/org/apache/cassandra/io/IteratingRow.java
+++ b/src/java/org/apache/cassandra/io/IteratingRow.java
@@ -37,7 +37,6 @@ public class IteratingRow extends AbstractIterator<IColumn> implements Comparabl
 {
     private final String key;
     private final long finishedAt;
-    private final ColumnFamily emptyColumnFamily;
     private final BufferedRandomAccessFile file;
     private SSTableReader sstable;
     private long dataStart;
@@ -51,10 +50,6 @@ public class IteratingRow extends AbstractIterator<IColumn> implements Comparabl
         int dataSize = file.readInt();
         dataStart = file.getFilePointer();
         finishedAt = dataStart + dataSize;
-        // legacy stuff to support FileStruct:
-        IndexHelper.skipBloomFilterAndIndex(file);
-        emptyColumnFamily = ColumnFamily.serializer().deserializeFromSSTableNoColumns(sstable.makeColumnFamily(), file);
-        file.readInt();
     }
 
     public String getKey()
@@ -62,11 +57,6 @@ public class IteratingRow extends AbstractIterator<IColumn> implements Comparabl
         return key;
     }
 
-    public ColumnFamily getEmptyColumnFamily()
-    {
-        return emptyColumnFamily;
-    }
-
     public void echoData(DataOutput out) throws IOException
     {
         file.seek(dataStart);
diff --git a/src/java/org/apache/cassandra/io/SSTableReader.java b/src/java/org/apache/cassandra/io/SSTableReader.java
index fc94fca07b..81a71fd3cf 100644
--- a/src/java/org/apache/cassandra/io/SSTableReader.java
+++ b/src/java/org/apache/cassandra/io/SSTableReader.java
@@ -333,11 +333,6 @@ public class SSTableReader extends SSTable implements Comparable<SSTableReader>
         return partitioner;
     }
 
-    public FileStruct getFileStruct() throws IOException
-    {
-        return new FileStruct(this);
-    }
-
     public SSTableScanner getScanner() throws IOException
     {
         return new SSTableScanner(this);
diff --git a/test/unit/org/apache/cassandra/db/ColumnFamilyTest.java b/test/unit/org/apache/cassandra/db/ColumnFamilyTest.java
index 40e58890e4..4219ff9d62 100644
--- a/test/unit/org/apache/cassandra/db/ColumnFamilyTest.java
+++ b/test/unit/org/apache/cassandra/db/ColumnFamilyTest.java
@@ -20,8 +20,6 @@ package org.apache.cassandra.db;
 
 import java.io.IOException;
 import java.util.Arrays;
-import java.util.HashSet;
-import java.util.Random;
 import java.util.TreeMap;
 
 import org.junit.Test;
@@ -125,8 +123,8 @@ public class ColumnFamilyTest
         cf_old.addColumn(QueryPath.column("col2".getBytes()), val2, 1);
         cf_old.addColumn(QueryPath.column("col3".getBytes()), val2, 2);
 
-        cf_result.addColumns(cf_new);
-        cf_result.addColumns(cf_old);
+        cf_result.addAll(cf_new);
+        cf_result.addAll(cf_old);
 
         assert 3 == cf_result.getColumnCount() : "Count is " + cf_new.getColumnCount();
         //addcolumns will only add if timestamp >= old timestamp
