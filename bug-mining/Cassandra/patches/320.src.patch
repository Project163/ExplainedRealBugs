diff --git a/contrib/word_count/bin/word_count_setup b/contrib/word_count/bin/word_count_setup
index 9f9372acf2..8fc7a4ce99 100644
--- a/contrib/word_count/bin/word_count_setup
+++ b/contrib/word_count/bin/word_count_setup
@@ -53,4 +53,8 @@ if [ "x$JAVA" = "x" ]; then
     exit 1
 fi
 
-$JAVA -Xmx1G -ea -cp $CLASSPATH WordCountSetup
+HOST=localhost
+PORT=9160
+FRAMED=false
+
+$JAVA -Xmx1G -ea -Dcassandra.host=$HOST -Dcassandra.port=$PORT -Dcassandra.framed=$FRAMED -cp $CLASSPATH WordCountSetup
diff --git a/contrib/word_count/build.xml b/contrib/word_count/build.xml
index 3c68b04916..ed68323e2f 100644
--- a/contrib/word_count/build.xml
+++ b/contrib/word_count/build.xml
@@ -77,7 +77,7 @@
            <zipfileset dir="${cassandra.dir}/build/lib/jars/" prefix="lib">
                <include name="**/*.jar" />
            </zipfileset>
-           <fileset file="${basedir}/storage-conf.xml" />
+           <fileset file="${basedir}/cassandra.yaml" />
         </jar>
     </target>
 
diff --git a/contrib/word_count/cassandra.yaml b/contrib/word_count/cassandra.yaml
new file mode 100644
index 0000000000..2464da5448
--- /dev/null
+++ b/contrib/word_count/cassandra.yaml
@@ -0,0 +1,199 @@
+# Cassandra storage config YAML 
+# See http://wiki.apache.org/cassandra/StorageConfiguration for
+# explanations of configuration directives.
+
+# name of the cluster
+cluster_name: 'Test Cluster'
+
+# Set to true to make new [non-seed] nodes automatically migrate the
+# right data to themselves.
+auto_bootstrap: false
+
+# authentication backend, implementing IAuthenticator; used to limit keyspace access
+authenticator: org.apache.cassandra.auth.AllowAllAuthenticator
+
+# any IPartitioner may be used, including your own as long as it is on
+# the classpath.  Out of the box, Cassandra provides
+# org.apache.cassandra.dht.RandomPartitioner
+# org.apache.cassandra.dht.OrderPreservingPartitioner, and
+# org.apache.cassandra.dht.CollatingOrderPreservingPartitioner.
+partitioner: org.apache.cassandra.dht.RandomPartitioner
+
+# directories where Cassandra should store data on disk.
+data_file_directories:
+    - /var/lib/cassandra/data
+
+# Addresses of hosts that are deemed contact points. 
+# Cassandra nodes use this list of hosts to find each other and learn
+# the topology of the ring.  You must change this if you are running
+# multiple nodes!
+seeds:
+    - 127.0.0.1
+
+# Access mode.  mmapped i/o is substantially faster, but only practical on
+# a 64bit machine (which notably does not include EC2 "small" instances)
+# or relatively small datasets.  "auto", the safe choice, will enable
+# mmapping on a 64bit JVM.  Other values are "mmap", "mmap_index_only"
+# (which may allow you to get part of the benefits of mmap on a 32bit
+# machine by mmapping only index files) and "standard".
+# (The buffer size settings that follow only apply to standard,
+# non-mmapped i/o.)
+disk_access_mode: auto
+
+# Unlike most systems, in Cassandra writes are faster than reads, so
+# you can afford more of those in parallel.  A good rule of thumb is 2
+# concurrent reads per processor core.  Increase ConcurrentWrites to
+# the number of clients writing at once if you enable CommitLogSync +
+# CommitLogSyncDelay. -->
+concurrent_reads: 8
+concurrent_writes: 32
+
+# Buffer size to use when performing contiguous column slices. 
+# Increase this to the size of the column slices you typically perform
+sliced_buffer_size_in_kb: 64
+
+# TCP port, for commands and data
+storage_port: 7000
+
+# Address to bind to and tell other nodes to connect to. You _must_
+# change this if you want multiple nodes to be able to communicate!
+listen_address: localhost
+
+# The address to bind the Thrift RPC service to
+rpc_address: localhost
+# port for Thrift to listen on
+rpc_port: 9160
+# Whether or not to use a framed transport for Thrift.
+thrift_framed_transport: false
+snapshot_before_compaction: false
+
+# The threshold size in megabytes the binary memtable must grow to,
+# before it's submitted for flushing to disk.
+binary_memtable_throughput_in_mb: 256
+# Number of minutes to keep a memtable in memory
+memtable_flush_after_mins: 60
+# Size of the memtable in memory before it is dumped
+memtable_throughput_in_mb: 64
+# Number of objects in millions in the memtable before it is dumped
+memtable_operations_in_millions: 0.3
+# Buffer size to use when flushing !memtables to disk.
+flush_data_buffer_size_in_mb: 32
+# Increase (decrease) the index buffer size relative to the data
+# buffer if you have few (many) columns per key.
+flush_index_buffer_size_in_mb: 8
+
+column_index_size_in_kb: 64
+row_warning_threshold_in_mb: 512
+
+# commit log
+commitlog_directory: /var/lib/cassandra/commitlog
+
+# Size to allow commitlog to grow to before creating a new segment 
+commitlog_rotation_threshold_in_mb: 128
+
+# commitlog_sync may be either "periodic" or "batch." 
+# When in batch mode, Cassandra won't ack writes until the commit log
+# has been fsynced to disk.  It will wait up to
+# CommitLogSyncBatchWindowInMS milliseconds for other writes, before
+# performing the sync.
+commitlog_sync: periodic
+
+# the other option is "timed," where writes may be acked immediately
+# and the CommitLog is simply synced every commitlog_sync_period_in_ms
+# milliseconds.
+commitlog_sync_period_in_ms: 10000
+
+# Time to wait for a reply from other nodes before failing the command 
+rpc_timeout_in_ms: 10000
+
+# time to wait before garbage collecting tombstones (deletion markers)
+gc_grace_seconds: 864000
+
+# endpoint_snitch -- Set this to a class that implements
+# IEndpointSnitch, which will let Cassandra know enough
+# about your network topology to route requests efficiently.
+# Out of the box, Cassandra provides
+# org.apache.cassandra.locator.SimpleSnitch,
+# org.apache.cassandra.locator.RackInferringSnitch, and
+# org.apache.cassandra.locator.PropertyFileSnitch.
+endpoint_snitch: org.apache.cassandra.locator.SimpleSnitch
+
+# A ColumnFamily is the Cassandra concept closest to a relational table. 
+#
+# Keyspaces are separate groups of ColumnFamilies.  Except in very
+# unusual circumstances you will have one Keyspace per application.
+#
+# Keyspace required parameters:
+# - name: name of the keyspace; "system" and "definitions" are 
+#   reserved for Cassandra Internals.
+# - replica_placement_strategy: the class that determines how replicas
+#   are distributed among nodes.  Must implement IReplicaPlacementStrategy.
+#   Out of the box, Cassandra provides 
+#   org.apache.cassandra.locator.RackUnawareStrategy and
+#   org.apache.cassandra.locator.RackAwareStrategy.  RackAwareStrategy
+#   place one replica in each of two datacenter, and other replicas in
+#   different racks in one.
+# - replication_factor: Number of replicas of each row
+# - column_families: column families associated with this keyspace
+#
+# ColumnFamily required parameters:
+# - name: name of the ColumnFamily.  Must not contain the character "-".
+# - compare_with: tells Cassandra how to sort the columns for slicing
+#   operations. The default is BytesType, which is a straightforward
+#   lexical comparison of the bytes in each column.  Other options are
+#   AsciiType, UTF8Type, LexicalUUIDType, TimeUUIDType, and LongType.
+#   You can also specify the fully-qualified class name to a class of
+#   your choice extending org.apache.cassandra.db.marshal.AbstractType.
+#
+# ColumnFamily optional parameters:
+# - keys_cached: specifies the number of keys per sstable whose
+#   locations we keep in memory in "mostly LRU" order.  (JUST the key
+#   locations, NOT any column values.) Specify a fraction (value less
+#   than 1) or an absolute number of keys to cache.  Defaults to 200000
+#   keys.
+# - rows_cached: specifies the number of rows whose entire contents we
+#   cache in memory. Do not use this on ColumnFamilies with large rows,
+#   or ColumnFamilies with high write:read ratios. Specify a fraction
+#   (value less than 1) or an absolute number of rows to cache.
+#   Defaults to 0. (i.e. row caching is off by default)
+# - comment: used to attach additional human-readable information about 
+#   the column family to its definition.
+# - read_repair_chance: specifies the probability with which read
+#   repairs should be invoked on non-quorum reads.  must be between 0
+#   and 1. defaults to 1.0 (always read repair).
+# - preload_row_cache: If true, will populate row cache on startup.
+#   Defaults to false.
+#
+keyspaces:
+    - name: Keyspace1
+      replica_placement_strategy: org.apache.cassandra.locator.RackUnawareStrategy
+      replication_factor: 1
+      column_families:
+        - name: Standard1
+          compare_with: BytesType
+
+        - name: Standard2
+          compare_with: UTF8Type
+          read_repair_chance: 0.1
+          keys_cached: 100
+
+        - name: StandardByUUID1
+          compare_with: TimeUUIDType
+
+        - name: Super1
+          column_type: Super
+          compare_with: BytesType
+          compare_subcolumns_with: BytesType
+
+        - name: Super2
+          column_type: Super
+          compare_subcolumns_with: UTF8Type
+          preload_row_cache: true
+          rows_cached: 10000
+          keys_cached: 50
+          comment: 'A column family with supercolumns, whose column and subcolumn names are UTF8 strings'
+
+        - name: Super3
+          column_type: Super
+          compare_with: LongType
+          comment: 'A column family with supercolumns, whose column names are Longs (8 bytes)'
diff --git a/contrib/word_count/src/WordCountSetup.java b/contrib/word_count/src/WordCountSetup.java
index c48e3fb42d..1649c4d2fd 100644
--- a/contrib/word_count/src/WordCountSetup.java
+++ b/contrib/word_count/src/WordCountSetup.java
@@ -16,16 +16,19 @@
  * limitations under the License.
  */
 
-import java.util.Arrays;
+import java.util.*;
 
+import org.apache.cassandra.thrift.*;
+import org.apache.thrift.TException;
+import org.apache.thrift.protocol.TBinaryProtocol;
+import org.apache.thrift.protocol.TProtocol;
+import org.apache.thrift.transport.TFramedTransport;
+import org.apache.thrift.transport.TSocket;
+import org.apache.thrift.transport.TTransport;
+import org.apache.thrift.transport.TTransportException;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
-import org.apache.cassandra.db.*;
-import org.apache.cassandra.service.StorageProxy;
-import org.apache.cassandra.service.StorageService;
-import org.apache.cassandra.thrift.ConsistencyLevel;
-
 public class WordCountSetup
 {
     private static final Logger logger = LoggerFactory.getLogger(WordCountSetup.class);
@@ -34,47 +37,93 @@ public class WordCountSetup
 
     public static void main(String[] args) throws Exception
     {
-        StorageService.instance.initClient();
-        logger.info("Sleeping " + WordCount.RING_DELAY);
-        Thread.sleep(WordCount.RING_DELAY);
-        assert !StorageService.instance.getLiveNodes().isEmpty();
+        Cassandra.Iface client = createConnection();
+
+        setupKeyspace(client);
 
-        RowMutation rm;
-        ColumnFamily cf;
-        byte[] columnName;
+        client.set_keyspace(WordCount.KEYSPACE);
 
-        // text0: no rows
+        Map<byte[], Map<String,List<Mutation>>> mutationMap;
+        Column c;
 
         // text1: 1 row, 1 word
-        columnName = "text1".getBytes();
-        rm = new RowMutation(WordCount.KEYSPACE, "Key0".getBytes());
-        cf = ColumnFamily.create(WordCount.KEYSPACE, WordCount.COLUMN_FAMILY);
-        cf.addColumn(new Column(columnName, "word1".getBytes(), 0));
-        rm.add(cf);
-        StorageProxy.mutateBlocking(Arrays.asList(rm), ConsistencyLevel.ONE);
+        c = new Column("text1".getBytes(), "word1".getBytes(), System.currentTimeMillis());
+        mutationMap = getMutationMap("key0".getBytes(), WordCount.COLUMN_FAMILY, c);
+        client.batch_mutate(mutationMap, ConsistencyLevel.ONE);
         logger.info("added text1");
 
-        // text2: 1 row, 2 words
-        columnName = "text2".getBytes();
-        rm = new RowMutation(WordCount.KEYSPACE, "Key0".getBytes());
-        cf = ColumnFamily.create(WordCount.KEYSPACE, WordCount.COLUMN_FAMILY);
-        cf.addColumn(new Column(columnName, "word1 word2".getBytes(), 0));
-        rm.add(cf);
-        StorageProxy.mutateBlocking(Arrays.asList(rm), ConsistencyLevel.ONE);
+        // text1: 1 row, 2 word
+        c = new Column("text2".getBytes(), "word1 word2".getBytes(), System.currentTimeMillis());
+        mutationMap = getMutationMap("key0".getBytes(), WordCount.COLUMN_FAMILY, c);
+        client.batch_mutate(mutationMap, ConsistencyLevel.ONE);
         logger.info("added text2");
 
         // text3: 1000 rows, 1 word
-        columnName = "text3".getBytes();
-        for (int i = 0; i < 1000; i++)
+        mutationMap = new HashMap<byte[],Map<String,List<Mutation>>>();
+        for (int i=0; i<1000; i++)
         {
-            rm = new RowMutation(WordCount.KEYSPACE, ("Key" + i).getBytes());
-            cf = ColumnFamily.create(WordCount.KEYSPACE, WordCount.COLUMN_FAMILY);
-            cf.addColumn(new Column(columnName, "word1".getBytes(), 0));
-            rm.add(cf);
-            StorageProxy.mutateBlocking(Arrays.asList(rm), ConsistencyLevel.ONE);
+            c = new Column("text3".getBytes(), "word1".getBytes(), System.currentTimeMillis());
+            addToMutationMap(mutationMap, ("key" + i).getBytes(), WordCount.COLUMN_FAMILY, c);
         }
+        client.batch_mutate(mutationMap, ConsistencyLevel.ONE);
         logger.info("added text3");
 
         System.exit(0);
     }
+
+    private static Map<byte[],Map<String,List<Mutation>>> getMutationMap(byte[] key, String cf, Column c) {
+        Map<byte[],Map<String,List<Mutation>>> mutationMap = new HashMap<byte[],Map<String,List<Mutation>>>();
+        addToMutationMap(mutationMap, key, cf, c);
+        return mutationMap;
+    }
+
+    private static void addToMutationMap(Map<byte[],Map<String,List<Mutation>>> mutationMap, byte[] key, String cf, Column c)
+    {
+        Map<String,List<Mutation>> cfMutation = new HashMap<String,List<Mutation>>();
+        List<Mutation> mList = new ArrayList<Mutation>();
+        ColumnOrSuperColumn cc = new ColumnOrSuperColumn();
+        Mutation m = new Mutation();
+
+        cc.setColumn(c);
+        m.setColumn_or_supercolumn(cc);
+        mList.add(m);
+        cfMutation.put(cf, mList);
+        mutationMap.put(key, cfMutation);
+    }
+
+    private static void setupKeyspace(Cassandra.Iface client) throws TException, InvalidRequestException
+    {
+        List<CfDef> cfDefList = new ArrayList<CfDef>();
+        CfDef cfDef = new CfDef(WordCount.KEYSPACE, WordCount.COLUMN_FAMILY);
+        cfDefList.add(cfDef);
+
+        client.system_add_keyspace(new KsDef(WordCount.KEYSPACE, "org.apache.cassandra.locator.RackUnawareStrategy", 1, cfDefList));
+    }
+
+    private static Cassandra.Iface createConnection() throws TTransportException
+    {
+        if(System.getProperty("cassandra.host") == null || System.getProperty("cassandra.port") == null)
+        {
+           logger.warn("cassandra.host or cassandra.port is not defined, using default");
+        }
+        return createConnection( System.getProperty("cassandra.host","localhost"),
+                                 Integer.valueOf(System.getProperty("cassandra.port","9160")),
+                                 Boolean.valueOf(System.getProperty("cassandra.framed", "false")) );
+    }
+
+    private static Cassandra.Client createConnection(String host, Integer port, boolean framed) throws TTransportException
+    {
+        TSocket socket = new TSocket(host, port);
+        TTransport trans;
+
+        if(framed)
+            trans = new TFramedTransport(socket);
+        else
+            trans = socket;
+
+        trans.open();
+        TProtocol protocol = new TBinaryProtocol(trans);
+
+        return new Cassandra.Client(protocol);
+    }
 }
diff --git a/contrib/word_count/storage-conf.xml b/contrib/word_count/storage-conf.xml
deleted file mode 100644
index 10ce5d14e9..0000000000
--- a/contrib/word_count/storage-conf.xml
+++ /dev/null
@@ -1,382 +0,0 @@
-<!--
- ~ Licensed to the Apache Software Foundation (ASF) under one
- ~ or more contributor license agreements.  See the NOTICE file
- ~ distributed with this work for additional information
- ~ regarding copyright ownership.  The ASF licenses this file
- ~ to you under the Apache License, Version 2.0 (the
- ~ "License"); you may not use this file except in compliance
- ~ with the License.  You may obtain a copy of the License at
- ~
- ~    http://www.apache.org/licenses/LICENSE-2.0
- ~
- ~ Unless required by applicable law or agreed to in writing,
- ~ software distributed under the License is distributed on an
- ~ "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
- ~ KIND, either express or implied.  See the License for the
- ~ specific language governing permissions and limitations
- ~ under the License.
--->
-<Storage>
-  <!--======================================================================-->
-  <!-- Basic Configuration                                                  -->
-  <!--======================================================================-->
-
-  <!-- 
-   ~ The name of this cluster.  This is mainly used to prevent machines in
-   ~ one logical cluster from joining another.
-  -->
-  <ClusterName>Test Cluster</ClusterName>
-
-  <!--
-   ~ Turn on to make new [non-seed] nodes automatically migrate the right data 
-   ~ to themselves.  (If no InitialToken is specified, they will pick one 
-   ~ such that they will get half the range of the most-loaded node.)
-   ~ If a node starts up without bootstrapping, it will mark itself bootstrapped
-   ~ so that you can't subsequently accidently bootstrap a node with
-   ~ data on it.  (You can reset this by wiping your data and commitlog
-   ~ directories.)
-   ~
-   ~ Off by default so that new clusters and upgraders from 0.4 don't
-   ~ bootstrap immediately.  You should turn this on when you start adding
-   ~ new nodes to a cluster that already has data on it.  (If you are upgrading
-   ~ from 0.4, start your cluster with it off once before changing it to true.
-   ~ Otherwise, no data will be lost but you will incur a lot of unnecessary
-   ~ I/O before your cluster starts up.)
-  -->
-  <AutoBootstrap>false</AutoBootstrap>
-
-  <!--
-   ~ Keyspaces and ColumnFamilies:
-   ~ A ColumnFamily is the Cassandra concept closest to a relational
-   ~ table.  Keyspaces are separate groups of ColumnFamilies.  Except in
-   ~ very unusual circumstances you will have one Keyspace per application.
-
-   ~ There is an implicit keyspace named 'system' for Cassandra internals.
-  -->
-  <Keyspaces>
-    <Keyspace Name="Keyspace1">
-      <!--
-       ~ ColumnFamily definitions have one required attribute (Name)
-       ~ and several optional ones.
-       ~
-       ~ The CompareWith attribute tells Cassandra how to sort the columns
-       ~ for slicing operations.  The default is BytesType, which is a
-       ~ straightforward lexical comparison of the bytes in each column.
-       ~ Other options are AsciiType, UTF8Type, LexicalUUIDType, TimeUUIDType,
-       ~ and LongType.  You can also specify the fully-qualified class
-       ~ name to a class of your choice extending
-       ~ org.apache.cassandra.db.marshal.AbstractType.
-       ~ 
-       ~ SuperColumns have a similar CompareSubcolumnsWith attribute.
-       ~ 
-       ~ BytesType: Simple sort by byte value.  No validation is performed.
-       ~ AsciiType: Like BytesType, but validates that the input can be 
-       ~            parsed as US-ASCII.
-       ~ UTF8Type: A string encoded as UTF8
-       ~ LongType: A 64bit long
-       ~ LexicalUUIDType: A 128bit UUID, compared lexically (by byte value)
-       ~ TimeUUIDType: a 128bit version 1 UUID, compared by timestamp
-       ~
-       ~ (To get the closest approximation to 0.3-style supercolumns, you
-       ~ would use CompareWith=UTF8Type CompareSubcolumnsWith=LongType.)
-       ~
-       ~ An optional `Comment` attribute may be used to attach additional
-       ~ human-readable information about the column family to its definition.
-       ~ 
-       ~ The optional KeysCached attribute specifies
-       ~ the number of keys per sstable whose locations we keep in
-       ~ memory in "mostly LRU" order.  (JUST the key locations, NOT any
-       ~ column values.) Specify a fraction (value less than 1), a percentage
-       ~ (ending in a % sign) or an absolute number of keys to cache.
-       ~ KeysCached defaults to 200000 keys.
-       ~
-       ~ The optional RowsCached attribute specifies the number of rows
-       ~ whose entire contents we cache in memory. Do not use this on
-       ~ ColumnFamilies with large rows, or ColumnFamilies with high write:read
-       ~ ratios. Specify a fraction (value less than 1), a percentage (ending in
-       ~ a % sign) or an absolute number of rows to cache. 
-       ~ RowsCached defaults to 0, i.e., row cache is off by default.
-       ~
-       ~ Remember, when using caches as a percentage, they WILL grow with
-       ~ your data set!
-      -->
-      <ColumnFamily Name="Standard1" CompareWith="BytesType"/>
-      <ColumnFamily Name="Standard2" 
-                    CompareWith="UTF8Type"
-                    KeysCached="100%"/>
-      <ColumnFamily Name="StandardByUUID1" CompareWith="TimeUUIDType" />
-      <ColumnFamily Name="Super1"
-                    ColumnType="Super"
-                    CompareWith="BytesType"
-                    CompareSubcolumnsWith="BytesType" />
-      <ColumnFamily Name="Super2"
-                    ColumnType="Super"
-                    CompareWith="UTF8Type"
-                    CompareSubcolumnsWith="UTF8Type"
-                    RowsCached="10000"
-                    KeysCached="50%"
-                    Comment="A column family with supercolumns, whose column and subcolumn names are UTF8 strings"/>
-
-      <!--
-       ~ Strategy: Setting this to the class that implements
-       ~ IReplicaPlacementStrategy will change the way the node picker works.
-       ~ Out of the box, Cassandra provides
-       ~ org.apache.cassandra.locator.RackUnawareStrategy and
-       ~ org.apache.cassandra.locator.RackAwareStrategy (place one replica in
-       ~ a different datacenter, and the others on different racks in the same
-       ~ one.)
-      -->
-      <ReplicaPlacementStrategy>org.apache.cassandra.locator.RackUnawareStrategy</ReplicaPlacementStrategy>
-
-      <!-- Number of replicas of the data -->
-      <ReplicationFactor>1</ReplicationFactor>
-
-      <!--
-       ~ EndpointSnitch: Setting this to the class that implements
-       ~ AbstractEndpointSnitch, which lets Cassandra know enough
-       ~ about your network topology to route requests efficiently.
-       ~ Out of the box, Cassandra provides org.apache.cassandra.locator.EndpointSnitch,
-       ~ and PropertyFileEndpointSnitch is available in contrib/.
-      -->
-      <EndpointSnitch>org.apache.cassandra.locator.EndpointSnitch</EndpointSnitch>
-        
-    </Keyspace>
-  </Keyspaces>
-
-  <!--
-   ~ Authenticator: any IAuthenticator may be used, including your own as long
-   ~ as it is on the classpath.  Out of the box, Cassandra provides
-   ~ org.apache.cassandra.auth.AllowAllAuthenticator and,
-   ~ org.apache.cassandra.auth.SimpleAuthenticator 
-   ~ (SimpleAuthenticator uses access.properties and passwd.properties by
-   ~ default).
-   ~
-   ~ If you don't specify an authenticator, AllowAllAuthenticator is used.
-  -->
-  <Authenticator>org.apache.cassandra.auth.AllowAllAuthenticator</Authenticator>
-
-  <!--
-   ~ Partitioner: any IPartitioner may be used, including your own as long
-   ~ as it is on the classpath.  Out of the box, Cassandra provides
-   ~ org.apache.cassandra.dht.RandomPartitioner,
-   ~ org.apache.cassandra.dht.OrderPreservingPartitioner, and
-   ~ org.apache.cassandra.dht.CollatingOrderPreservingPartitioner.
-   ~ (CollatingOPP colates according to EN,US rules, not naive byte
-   ~ ordering.  Use this as an example if you need locale-aware collation.)
-   ~ Range queries require using an order-preserving partitioner.
-   ~
-   ~ Achtung!  Changing this parameter requires wiping your data
-   ~ directories, since the partitioner can modify the sstable on-disk
-   ~ format.
-  -->
-  <Partitioner>org.apache.cassandra.dht.RandomPartitioner</Partitioner>
-
-  <!--
-   ~ If you are using an order-preserving partitioner and you know your key
-   ~ distribution, you can specify the token for this node to use. (Keys
-   ~ are sent to the node with the "closest" token, so distributing your
-   ~ tokens equally along the key distribution space will spread keys
-   ~ evenly across your cluster.)  This setting is only checked the first
-   ~ time a node is started. 
-
-   ~ This can also be useful with RandomPartitioner to force equal spacing
-   ~ of tokens around the hash space, especially for clusters with a small
-   ~ number of nodes.
-  -->
-  <InitialToken></InitialToken>
-
-  <!--
-   ~ Directories: Specify where Cassandra should store different data on
-   ~ disk.  Keep the data disks and the CommitLog disks separate for best
-   ~ performance
-  -->
-  <CommitLogDirectory>/var/lib/cassandra/commitlog</CommitLogDirectory>
-  <DataFileDirectories>
-      <DataFileDirectory>/var/lib/cassandra/data</DataFileDirectory>
-  </DataFileDirectories>
-
-
-  <!--
-   ~ Addresses of hosts that are deemed contact points. Cassandra nodes
-   ~ use this list of hosts to find each other and learn the topology of
-   ~ the ring. You must change this if you are running multiple nodes!
-  -->
-  <Seeds>
-      <Seed>127.0.0.1</Seed>
-  </Seeds>
-
-
-  <!-- Miscellaneous -->
-
-  <!-- Time to wait for a reply from other nodes before failing the command -->
-  <RpcTimeoutInMillis>10000</RpcTimeoutInMillis>
-  <!-- Size to allow commitlog to grow to before creating a new segment -->
-  <CommitLogRotationThresholdInMB>128</CommitLogRotationThresholdInMB>
-
-
-  <!-- Local hosts and ports -->
-
-  <!-- 
-   ~ Address to bind to and tell other nodes to connect to.  You _must_
-   ~ change this if you want multiple nodes to be able to communicate!  
-   ~
-   ~ Leaving it blank leaves it up to InetAddress.getLocalHost(). This
-   ~ will always do the Right Thing *if* the node is properly configured
-   ~ (hostname, name resolution, etc), and the Right Thing is to use the
-   ~ address associated with the hostname (it might not be).
-  -->
-  <!--<ListenAddress>localhost</ListenAddress>-->
-  <ListenAddress>127.0.0.2</ListenAddress>
-  <!-- internal communications port -->
-  <StoragePort>7000</StoragePort>
-
-  <!--
-   ~ The address to bind the Thrift RPC service to. Unlike ListenAddress
-   ~ above, you *can* specify 0.0.0.0 here if you want Thrift to listen on
-   ~ all interfaces.
-   ~
-   ~ Leaving this blank has the same effect it does for ListenAddress,
-   ~ (i.e. it will be based on the configured hostname of the node).
-  -->
-  <ThriftAddress>127.0.0.2</ThriftAddress>
-  <!-- Thrift RPC port (the port clients connect to). -->
-  <ThriftPort>9160</ThriftPort>
-  <!-- 
-   ~ Whether or not to use a framed transport for Thrift. If this option
-   ~ is set to true then you must also use a framed transport on the 
-   ~ client-side, (framed and non-framed transports are not compatible).
-  -->
-  <ThriftFramedTransport>false</ThriftFramedTransport>
-
-
-  <!--======================================================================-->
-  <!-- Memory, Disk, and Performance                                        -->
-  <!--======================================================================-->
-
-  <!--
-   ~ Access mode.  mmapped i/o is substantially faster, but only practical on
-   ~ a 64bit machine (which notably does not include EC2 "small" instances)
-   ~ or relatively small datasets.  "auto", the safe choice, will enable
-   ~ mmapping on a 64bit JVM.  Other values are "mmap", "mmap_index_only"
-   ~ (which may allow you to get part of the benefits of mmap on a 32bit
-   ~ machine by mmapping only index files) and "standard".
-   ~ (The buffer size settings that follow only apply to standard,
-   ~ non-mmapped i/o.)
-   -->
-  <DiskAccessMode>auto</DiskAccessMode>
-
-  <!--
-   ~ Size of compacted row above which to log a warning.  (If compacted
-   ~ rows do not fit in memory, Cassandra will crash.  This is explained
-   ~ in http://wiki.apache.org/cassandra/CassandraLimitations and is
-   ~ scheduled to be fixed in 0.7.)
-  -->
-  <RowWarningThresholdInMB>512</RowWarningThresholdInMB>
-
-  <!--
-   ~ Buffer size to use when performing contiguous column slices. Increase
-   ~ this to the size of the column slices you typically perform. 
-   ~ (Name-based queries are performed with a buffer size of 
-   ~ ColumnIndexSizeInKB.)
-  -->
-  <SlicedBufferSizeInKB>64</SlicedBufferSizeInKB>
-
-  <!--
-   ~ Buffer size to use when flushing memtables to disk. (Only one 
-   ~ memtable is ever flushed at a time.) Increase (decrease) the index
-   ~ buffer size relative to the data buffer if you have few (many) 
-   ~ columns per key.  Bigger is only better _if_ your memtables get large
-   ~ enough to use the space. (Check in your data directory after your
-   ~ app has been running long enough.) -->
-  <FlushDataBufferSizeInMB>32</FlushDataBufferSizeInMB>
-  <FlushIndexBufferSizeInMB>8</FlushIndexBufferSizeInMB>
-
-  <!--
-   ~ Add column indexes to a row after its contents reach this size.
-   ~ Increase if your column values are large, or if you have a very large
-   ~ number of columns.  The competing causes are, Cassandra has to
-   ~ deserialize this much of the row to read a single column, so you want
-   ~ it to be small - at least if you do many partial-row reads - but all
-   ~ the index data is read for each access, so you don't want to generate
-   ~ that wastefully either.
-  -->
-  <ColumnIndexSizeInKB>64</ColumnIndexSizeInKB>
-
-  <!--
-   ~ Flush memtable after this much data has been inserted, including
-   ~ overwritten data.  There is one memtable per column family, and 
-   ~ this threshold is based solely on the amount of data stored, not
-   ~ actual heap memory usage (there is some overhead in indexing the
-   ~ columns).
-  -->
-  <MemtableThroughputInMB>64</MemtableThroughputInMB>
-  <!--
-   ~ Throughput setting for Binary Memtables.  Typically these are
-   ~ used for bulk load so you want them to be larger.
-  -->
-  <BinaryMemtableThroughputInMB>256</BinaryMemtableThroughputInMB>
-  <!--
-   ~ The maximum number of columns in millions to store in memory per
-   ~ ColumnFamily before flushing to disk.  This is also a per-memtable
-   ~ setting.  Use with MemtableThroughputInMB to tune memory usage.
-  -->
-  <MemtableOperationsInMillions>0.3</MemtableOperationsInMillions>
-  <!--
-   ~ The maximum time to leave a dirty memtable unflushed.
-   ~ (While any affected columnfamilies have unflushed data from a
-   ~ commit log segment, that segment cannot be deleted.)
-   ~ This needs to be large enough that it won't cause a flush storm
-   ~ of all your memtables flushing at once because none has hit
-   ~ the size or count thresholds yet.  For production, a larger
-   ~ value such as 1440 is recommended.
-  -->
-  <MemtableFlushAfterMinutes>60</MemtableFlushAfterMinutes>
-
-  <!--
-   ~ Unlike most systems, in Cassandra writes are faster than reads, so
-   ~ you can afford more of those in parallel.  A good rule of thumb is 2
-   ~ concurrent reads per processor core.  Increase ConcurrentWrites to
-   ~ the number of clients writing at once if you enable CommitLogSync +
-   ~ CommitLogSyncDelay. -->
-  <ConcurrentReads>8</ConcurrentReads>
-  <ConcurrentWrites>32</ConcurrentWrites>
-
-  <!--
-   ~ CommitLogSync may be either "periodic" or "batch."  When in batch
-   ~ mode, Cassandra won't ack writes until the commit log has been
-   ~ fsynced to disk.  It will wait up to CommitLogSyncBatchWindowInMS
-   ~ milliseconds for other writes, before performing the sync.
-
-   ~ This is less necessary in Cassandra than in traditional databases
-   ~ since replication reduces the odds of losing data from a failure
-   ~ after writing the log entry but before it actually reaches the disk.
-   ~ So the other option is "periodic," where writes may be acked immediately
-   ~ and the CommitLog is simply synced every CommitLogSyncPeriodInMS
-   ~ milliseconds.
-  -->
-  <CommitLogSync>periodic</CommitLogSync>
-  <!--
-   ~ Interval at which to perform syncs of the CommitLog in periodic mode.
-   ~ Usually the default of 10000ms is fine; increase it if your i/o
-   ~ load is such that syncs are taking excessively long times.
-  -->
-  <CommitLogSyncPeriodInMS>10000</CommitLogSyncPeriodInMS>
-  <!--
-   ~ Delay (in milliseconds) during which additional commit log entries
-   ~ may be written before fsync in batch mode.  This will increase
-   ~ latency slightly, but can vastly improve throughput where there are
-   ~ many writers.  Set to zero to disable (each entry will be synced
-   ~ individually).  Reasonable values range from a minimal 0.1 to 10 or
-   ~ even more if throughput matters more than latency.
-  -->
-  <!-- <CommitLogSyncBatchWindowInMS>1</CommitLogSyncBatchWindowInMS> --> 
-
-  <!--
-   ~ Time to wait before garbage-collection deletion markers.  Set this to
-   ~ a large enough value that you are confident that the deletion marker
-   ~ will be propagated to all replicas by the time this many seconds has
-   ~ elapsed, even in the face of hardware failures.  The default value is
-   ~ ten days.
-  -->
-  <GCGraceSeconds>864000</GCGraceSeconds>
-</Storage>
diff --git a/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java b/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
index 5c88c636a1..7abcd00714 100644
--- a/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
+++ b/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java
@@ -24,14 +24,11 @@ package org.apache.cassandra.hadoop;
 import java.io.IOException;
 import java.net.InetAddress;
 import java.net.UnknownHostException;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
-import java.util.SortedMap;
-import java.util.TreeMap;
+import java.util.*;
 
 import com.google.common.collect.AbstractIterator;
 
+import org.apache.cassandra.auth.AllowAllAuthenticator;
 import org.apache.cassandra.auth.SimpleAuthenticator;
 import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.db.*;
@@ -112,8 +109,8 @@ public class ColumnFamilyRecordReader extends RecordReader<byte[], SortedMap<byt
         private String startToken;
         private int totalRead = 0;
         private int i = 0;
-        private AbstractType comparator = DatabaseDescriptor.getComparator(keyspace, cfName);
-        
+        private AbstractType comparator = null;
+
         private void maybeInit()
         {
             // check if we need another batch 
@@ -151,7 +148,17 @@ public class ColumnFamilyRecordReader extends RecordReader<byte[], SortedMap<byt
             try
             {
                 client.set_keyspace(keyspace);
-            	client.login(authRequest);
+                if (!(DatabaseDescriptor.getAuthenticator() instanceof AllowAllAuthenticator))
+                {
+                    client.login(authRequest);
+                }
+
+                // Get the keyspace information to get the comparator
+                Map<String, Map<String,String>> desc = client.describe_keyspace(keyspace);
+                Map<String,String> ksProps = desc.get(cfName);
+                String compClass = ksProps.get("CompareWith");
+                comparator = (AbstractType) Class.forName(compClass).newInstance();
+
                 rows = client.get_range_slices(new ColumnParent(cfName),
                                                predicate,
                                                keyRange,
diff --git a/src/java/org/apache/cassandra/hadoop/ConfigHelper.java b/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
index 9ef1b8fcf0..9028590d59 100644
--- a/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
+++ b/src/java/org/apache/cassandra/hadoop/ConfigHelper.java
@@ -23,7 +23,6 @@ package org.apache.cassandra.hadoop;
 
 import org.apache.cassandra.thrift.InvalidRequestException;
 import org.apache.cassandra.thrift.SlicePredicate;
-import org.apache.cassandra.thrift.ThriftValidation;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.thrift.TDeserializer;
 import org.apache.thrift.TException;
@@ -59,14 +58,7 @@ public class ConfigHelper
         {
             throw new UnsupportedOperationException("columnfamily may not be null");
         }
-        try
-        {
-            ThriftValidation.validateColumnFamily(keyspace, columnFamily);
-        }
-        catch (InvalidRequestException e)
-        {
-            throw new RuntimeException(e);
-        }
+
         conf.set(KEYSPACE_CONFIG, keyspace);
         conf.set(COLUMNFAMILY_CONFIG, columnFamily);
     }
