diff --git a/CHANGES.txt b/CHANGES.txt
index 2b99fb669b..a1b0d9008e 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -9,6 +9,7 @@
  * add placeholders for missing rows in range query pseudo-RR (CASSANDRA-2680)
  * remove no-op HHOM.renameHints (CASSANDRA-2693)
  * clone super columns to avoid modifying them during flush (CASSANDRA-2675)
+ * close scrub file handles (CASSANDRA-2669)
 
 
 0.7.6
diff --git a/src/java/org/apache/cassandra/db/CompactionManager.java b/src/java/org/apache/cassandra/db/CompactionManager.java
index 444d4d6b51..6753de8149 100644
--- a/src/java/org/apache/cassandra/db/CompactionManager.java
+++ b/src/java/org/apache/cassandra/db/CompactionManager.java
@@ -505,25 +505,29 @@ public class CompactionManager implements CompactionManagerMBean
     private void doScrub(ColumnFamilyStore cfs) throws IOException
     {
         assert !cfs.isIndex();
-
         for (final SSTableReader sstable : cfs.getSSTables())
-        {
-            logger.info("Scrubbing " + sstable);
-
-            // Calculate the expected compacted filesize
-            String compactionFileLocation = cfs.table.getDataFileLocation(sstable.length());
-            if (compactionFileLocation == null)
-                throw new IOException("disk full");
-            int expectedBloomFilterSize = Math.max(DatabaseDescriptor.getIndexInterval(),
-                                                   (int)(SSTableReader.getApproximateKeyCount(Arrays.asList(sstable))));
+            scrubOne(cfs, sstable);
+    }
 
-            // loop through each row, deserializing to check for damage.
-            // we'll also loop through the index at the same time, using the position from the index to recover if the
-            // row header (key or data size) is corrupt. (This means our position in the index file will be one row
-            // "ahead" of the data file.)
-            final BufferedRandomAccessFile dataFile = BufferedRandomAccessFile.getUncachingReader(sstable.getFilename());
-            String indexFilename = sstable.descriptor.filenameFor(Component.PRIMARY_INDEX);
-            BufferedRandomAccessFile indexFile = BufferedRandomAccessFile.getUncachingReader(indexFilename);
+    private void scrubOne(ColumnFamilyStore cfs, SSTableReader sstable) throws IOException
+    {
+        logger.info("Scrubbing " + sstable);
+        // Calculate the expected compacted filesize
+        String compactionFileLocation = cfs.table.getDataFileLocation(sstable.length());
+        if (compactionFileLocation == null)
+            throw new IOException("disk full");
+        int expectedBloomFilterSize = Math.max(DatabaseDescriptor.getIndexInterval(),
+                                               (int)(SSTableReader.getApproximateKeyCount(Arrays.asList(sstable))));
+
+        // loop through each row, deserializing to check for damage.
+        // we'll also loop through the index at the same time, using the position from the index to recover if the
+        // row header (key or data size) is corrupt. (This means our position in the index file will be one row
+        // "ahead" of the data file.)
+        final BufferedRandomAccessFile dataFile = BufferedRandomAccessFile.getUncachingReader(sstable.getFilename());
+        String indexFilename = sstable.descriptor.filenameFor(Component.PRIMARY_INDEX);
+        BufferedRandomAccessFile indexFile = BufferedRandomAccessFile.getUncachingReader(indexFilename);
+        try
+        {
             ByteBuffer nextIndexKey = ByteBufferUtil.readWithShortLength(indexFile);
             {
                 // throw away variable so we don't have a side effect in the assert
@@ -662,6 +666,11 @@ public class CompactionManager implements CompactionManagerMBean
                     logger.info("Scrub of " + sstable + " complete; looks like all " + emptyRows + " rows were tombstoned");
             }
         }
+        finally
+        {
+            FileUtils.closeQuietly(dataFile);
+            FileUtils.closeQuietly(indexFile);
+        }
     }
 
     private void throwIfFatal(Throwable th)
