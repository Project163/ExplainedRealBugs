diff --git a/CHANGES.txt b/CHANGES.txt
index 5e3f8c6d6b..cb4d5bca9d 100644
--- a/CHANGES.txt
+++ b/CHANGES.txt
@@ -53,6 +53,7 @@ Merged from 2.2:
  * Package tools/bin scripts as executable (CASSANDRA-16151)
  * Fixed a NullPointerException when calling nodetool enablethrift (CASSANDRA-16127)
  * Automatically drop compact storage on tables for which it is safe (CASSANDRA-16048)
+ * Fixed NullPointerException for COMPACT STORAGE tables with null clustering (CASSANDRA-16241)
 
 4.0-beta2
  * Add addition incremental repair visibility to nodetool repair_admin (CASSANDRA-14939)
diff --git a/src/java/org/apache/cassandra/db/AbstractArrayClusteringPrefix.java b/src/java/org/apache/cassandra/db/AbstractArrayClusteringPrefix.java
index b698f5d9fa..211eeb0460 100644
--- a/src/java/org/apache/cassandra/db/AbstractArrayClusteringPrefix.java
+++ b/src/java/org/apache/cassandra/db/AbstractArrayClusteringPrefix.java
@@ -41,8 +41,11 @@ public abstract class AbstractArrayClusteringPrefix extends AbstractOnHeapCluste
     public ByteBuffer[] getBufferArray()
     {
         ByteBuffer[] out = new ByteBuffer[values.length];
-        for (int i=0; i<values.length; i++)
-            out[i] = ByteBuffer.wrap(values[i]);
+        for (int i = 0; i < values.length; i++)
+        {
+            // Compact tables allowed null clustering elements, so take those into account: 
+            out[i] = values[i] == null ? null : ByteBuffer.wrap(values[i]);
+        }
         return out;
     }
 
diff --git a/test/distributed/org/apache/cassandra/distributed/upgrade/CompactStorage3to4UpgradeTest.java b/test/distributed/org/apache/cassandra/distributed/upgrade/CompactStorage3to4UpgradeTest.java
index 5317517b49..bed1393dc4 100644
--- a/test/distributed/org/apache/cassandra/distributed/upgrade/CompactStorage3to4UpgradeTest.java
+++ b/test/distributed/org/apache/cassandra/distributed/upgrade/CompactStorage3to4UpgradeTest.java
@@ -30,7 +30,6 @@ import org.apache.cassandra.distributed.api.ConsistencyLevel;
 import org.apache.cassandra.distributed.api.ICoordinator;
 import org.apache.cassandra.distributed.shared.Versions;
 import org.apache.cassandra.exceptions.StartupException;
-import org.apache.cassandra.schema.TableMetadata;
 
 import static org.apache.cassandra.distributed.shared.AssertUtils.assertRows;
 
@@ -110,11 +109,10 @@ public class CompactStorage3to4UpgradeTest extends UpgradeTestBase
             .nodes(2)
             .upgrade(Versions.Major.v30, Versions.Major.v4)
             .setup(cluster -> cluster.schemaChange(CREATE_TABLE_C1_ONLY))
-            .runAfterNodeUpgrade((cluster, node) -> {
-                Assert.fail("should never run because we don't expect the node to start");
-            })
+            .runAfterNodeUpgrade((cluster, node) -> Assert.fail("should never run because we don't expect the node to start"))
             .run();
-        } catch (RuntimeException e)
+        } 
+        catch (RuntimeException e)
         {
             validateError(e);
         }
@@ -129,16 +127,41 @@ public class CompactStorage3to4UpgradeTest extends UpgradeTestBase
             .nodes(2)
             .upgrade(Versions.Major.v30, Versions.Major.v4)
             .setup(cluster -> cluster.schemaChange(CREATE_TABLE_R_ONLY))
-            .runAfterNodeUpgrade((cluster, node) -> {
-                Assert.fail("should never run because we don't expect the node to start");
-            })
+            .runAfterNodeUpgrade((cluster, node) -> Assert.fail("should never run because we don't expect the node to start"))
             .run();
-        } catch (RuntimeException e)
+        } 
+        catch (RuntimeException e)
         {
             validateError(e);
         }
     }
 
+    @Test
+    public void testNullClusteringValues() throws Throwable
+    {
+        new TestCase().nodes(1)
+                      .upgrade(Versions.Major.v30, Versions.Major.v4)
+                      .setup(cluster -> {
+                          String create = "CREATE TABLE %s.%s(k int, c1 int, c2 int, v int, PRIMARY KEY (k, c1, c2)) " +
+                                          "WITH compaction = { 'class':'LeveledCompactionStrategy', 'enabled':'false'} AND COMPACT STORAGE";
+                          cluster.schemaChange(String.format(create, KEYSPACE, TABLE_NAME));
+                          
+                          String insert = "INSERT INTO %s.%s(k, c1, v) values (?, ?, ?)";
+                          cluster.get(1).executeInternal(String.format(insert, KEYSPACE, TABLE_NAME), 1, 1, 1);
+                          cluster.get(1).flush(KEYSPACE);
+
+                          cluster.get(1).executeInternal(String.format(insert, KEYSPACE, TABLE_NAME), 2, 2, 2);
+                          cluster.get(1).flush(KEYSPACE);
+                          
+                          cluster.schemaChange(String.format("ALTER TABLE %s.%s DROP COMPACT STORAGE", KEYSPACE, TABLE_NAME));
+                      })
+                      .runAfterNodeUpgrade((cluster, node) -> {
+                          cluster.get(1).forceCompact(KEYSPACE, TABLE_NAME);
+                          Object[][] actual = cluster.get(1).executeInternal(String.format("SELECT * FROM %s.%s", KEYSPACE, TABLE_NAME));
+                          assertRows(actual, new Object[] {1, 1, null, 1}, new Object[] {2, 2, null, 2});
+                      })
+                      .run();
+    }
 
     public void validateResults(DropCompactTestHelper helper, UpgradeableCluster cluster, int node)
     {
