<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:22:45 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-3003] Trunk single-pass streaming doesn&apos;t handle large row correctly</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-3003</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;For normal column family, trunk streaming always buffer the whole row into memory. In uses&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  ColumnFamily.serializer().deserializeColumns(in, cf, true, true);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;on the input bytes.&lt;br/&gt;
We must avoid this for rows that don&apos;t fit in the inMemoryLimit.&lt;/p&gt;

&lt;p&gt;Note that for regular column families, for a given row, there is actually no need to even recreate the bloom filter of column index, nor to deserialize the columns. It is enough to filter the key and row size to feed the index writer, but then simply dump the rest on disk directly. This would make streaming more efficient, avoid a lot of object creation and avoid the pitfall of big rows.&lt;/p&gt;

&lt;p&gt;Counters column family are unfortunately trickier, because each column needs to be deserialized (to mark them as &apos;fromRemote&apos;). However, we don&apos;t need to do the double pass of LazilyCompactedRow for that. We can simply use a SSTableIdentityIterator and deserialize/reserialize input as it comes.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12518240">CASSANDRA-3003</key>
            <summary>Trunk single-pass streaming doesn&apos;t handle large row correctly</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10000" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Urgent</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yukim">Yuki Morishita</assignee>
                                    <reporter username="slebresne">Sylvain Lebresne</reporter>
                        <labels>
                            <label>streaming</label>
                    </labels>
                <created>Mon, 8 Aug 2011 19:52:25 +0000</created>
                <updated>Tue, 16 Apr 2019 09:32:54 +0000</updated>
                            <resolved>Mon, 5 Sep 2011 14:57:33 +0000</resolved>
                                        <fixVersion>1.0.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                                                                <comments>
                            <comment id="13081158" author="slebresne" created="Mon, 8 Aug 2011 19:55:52 +0000"  >&lt;p&gt;Marking critical, because at least for counter column family, when the row is larger than the inMemoryLimit, the code will actually crash because it will use lazilyCompactedRow which will try to do it&apos;s 2 passes.&lt;/p&gt;</comment>
                            <comment id="13082198" author="stuhood" created="Wed, 10 Aug 2011 07:06:27 +0000"  >&lt;p&gt;Oof... I don&apos;t know how I missed this one in review: very, very sorry Yuki/Sylvain.&lt;/p&gt;

&lt;p&gt;Perhaps we can use this as an opportunity to switch to using only PrecompactedRow (for narrow rows which might go to cache) or EchoedRow (for wide rows, which go directly to disk)?&lt;/p&gt;

&lt;p&gt;In order to use EchoedRow, we&apos;d have to move where we do CounterContext cleanup: I&apos;ve suggested in the past that it could be done at read time if we added &quot;fromRemote&quot; as a field in the metadata of an SSTable. Every SSTable*Iterator would be affected, because they&apos;d need to respect the fromRemote field.&lt;/p&gt;

&lt;p&gt;Alternatively, we could revert 2920 and 2677 (which I would hate: this has been a huge cleanup).&lt;/p&gt;

&lt;p&gt;EDIT: Oops, apparently I didn&apos;t review this one. Anyway!&lt;/p&gt;</comment>
                            <comment id="13082251" author="slebresne" created="Wed, 10 Aug 2011 09:28:44 +0000"  >&lt;blockquote&gt;&lt;p&gt;In order to use EchoedRow, we&apos;d have to move where we do CounterContext cleanup&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I really think it is not very hard to do &apos;inline&apos;. We really just want to deserialize, cleanup, reserialize. It should be super easy to add some &quot;CounterCleanedRow&quot; that does that.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;at it could be done at read time if we added &quot;fromRemote&quot; as a field in the metadata of an SSTable&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, but it does sound a bit complicated to me compared to doing the cleanup right away during streaming. It would also be less efficient, because until we have compacted the streamed sstable, each read will have to call the cleanup over and over, while we really only care to have it done twice (unless we completely change where we do cleanup).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Perhaps we can use this as an opportunity to switch to using only PrecompactedRow (for narrow rows which might go to cache) or EchoedRow (for wide rows, which go directly to disk)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree in that there is no point in doing manual deserialization there. About the PrecompactedRow for narrow rows which might go to cache, I&apos;ll just precise that it is worth using PrecompactedRow only if 1) we are doing AES streaming and 2) the row is in cache in the first place (which we can know since we always at least deserialize the row key).&lt;/p&gt;</comment>
                            <comment id="13082800" author="stuhood" created="Thu, 11 Aug 2011 00:16:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;I really think it is not very hard to do &apos;inline&apos;. We really just want to deserialize, cleanup, reserialize. It should be super easy to add some &quot;CounterCleanedRow&quot; that does that.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;m probably missing something, but isn&apos;t the problem that this can&apos;t be done without two passes for rows that are too large to fit in memory? And you can&apos;t perform two passes without buffering data somewhere? I suggested removing the cleanup step out of streaming because then the row could be echoed to disk without modification.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It would also be less efficient, because until we have compacted the streamed sstable, each read will have to call the cleanup over and over&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is true, but compaction is fairly likely to trigger soon after a big batch of streamed files arrives, since they will trigger compaction thresholds.&lt;/p&gt;</comment>
                            <comment id="13082857" author="yukim" created="Thu, 11 Aug 2011 01:51:42 +0000"  >&lt;p&gt;Stu, Sylvan,&lt;/p&gt;

&lt;p&gt;Let me try to fix this by using EchoedRow to serialize directly to disk, and creating new &quot;CounterCleanedRow&quot; suggested by Sylvain above.&lt;/p&gt;</comment>
                            <comment id="13082991" author="slebresne" created="Thu, 11 Aug 2011 07:41:39 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m probably missing something, but isn&apos;t the problem that this can&apos;t be done without two passes for rows that are too large to fit in memory?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Hum true. What we need to do is deserialize each row with the &apos;fromRemote&apos; flag on so that the delta are cleaned up, and them reserialize the result. But that will potentially reduce the column serialized size (and thus modify the row total size and the column index). Now we could imagine to remember the offset of the beginning of the row, to load the column index in memory and update it during the first pass (it would likely be ok to simply update the index offsets without changing the index structure itself), and to seek back at the end to write the updated data size and column index. However, this unfortunately won&apos;t be doable with the current SequentialWriter (and CompressedSequentialWriter) since we cannot seek back (without truncating). Retrospectively, it would have been nicer to have the cleaning of a counter context not change its size &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;So yeah, it sucks. I&apos;m still mildly fan of moving the cleanup because it &quot;feels wrong&quot; somehow. It feels it would be better to have that delta cleaning done sooner than latter. But this may end up being the simplest/more efficient solution.&lt;/p&gt;</comment>
                            <comment id="13083111" author="jbellis" created="Thu, 11 Aug 2011 13:33:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;it would have been nicer to have the cleaning of a counter context not change its size&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can we pad it somehow?&lt;/p&gt;</comment>
                            <comment id="13083123" author="slebresne" created="Thu, 11 Aug 2011 14:06:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;Can we pad it somehow?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s doable. Basically a context is an array of shards, with a header that is a (variable) list of which of those shards are a delta. When we cleanup the delta we remove the header basically. We could have a specific cleanup for streaming that just set all the header to -1. But we probably want to do that only for the cleanup during streaming, and have compaction clean those afterwards, otherwise it is ugly. I don&apos;t know how much easier it is than cleaning during reads, though it avoids having to add a new info for sstable metadata.&lt;/p&gt;</comment>
                            <comment id="13087942" author="jbellis" created="Fri, 19 Aug 2011 20:24:28 +0000"  >&lt;p&gt;How is this looking, Yuki?&lt;/p&gt;</comment>
                            <comment id="13088358" author="yukim" created="Sun, 21 Aug 2011 13:05:59 +0000"  >&lt;p&gt;Instead of creating CounterCleanedRow, I added appendFromStream method to SSTW, which handles both normal and counter column.&lt;/p&gt;

&lt;p&gt;I still need to work on SSTII because attached patch causes problem when iterating over cleaned up CounterColumns with 0-padding added during streaming.&lt;br/&gt;
That also causes StreamingTransferTest fail.&lt;/p&gt;

&lt;p&gt;Will post update version soon.&lt;/p&gt;</comment>
                            <comment id="13091838" author="yukim" created="Fri, 26 Aug 2011 15:52:06 +0000"  >&lt;p&gt;V2 attached and ready for the review.&lt;br/&gt;
For Counter columns, instead of padding in place of removed delta, v2 just &quot;mark&quot; the counter column to clear delta later, by multiplying #elt by -1 in order to keep the header size for later removal. Marking only occur when deserialize &quot;fromRemote&quot;, and actual removal of delta is done when reading again from disk after the streaming.&lt;/p&gt;</comment>
                            <comment id="13091873" author="jbellis" created="Fri, 26 Aug 2011 16:52:35 +0000"  >&lt;p&gt;Does CounterColumn.create work for both &quot;normal,&quot; non-streamed counter updates, as well as streaming?  Or do we need two distinct paths there?&lt;/p&gt;</comment>
                            <comment id="13092158" author="yukim" created="Sat, 27 Aug 2011 01:19:40 +0000"  >&lt;p&gt;Looks like we need distinct paths. Counter reads from remote in the read path also get marked (have negative #elt) and may cause problem. I&apos;ll take a look.&lt;/p&gt;</comment>
                            <comment id="13093833" author="yukim" created="Tue, 30 Aug 2011 15:42:20 +0000"  >&lt;p&gt;v3 attached. It marks counter column to delete delta after deserializing it from stream without clearing all delta. In this way, marking does not affect regular counter update.&lt;/p&gt;</comment>
                            <comment id="13094176" author="yukim" created="Tue, 30 Aug 2011 23:04:19 +0000"  >&lt;p&gt;In v3, I forgot to handle the case where counter columns inside super column. I&apos;ll update soon.&lt;/p&gt;</comment>
                            <comment id="13094676" author="yukim" created="Wed, 31 Aug 2011 16:45:42 +0000"  >&lt;p&gt;Added handling of counters inside SuperColumn.&lt;/p&gt;</comment>
                            <comment id="13094753" author="slebresne" created="Wed, 31 Aug 2011 18:26:03 +0000"  >&lt;p&gt;I think this is a little bit sad to deserialize all the columns in the non-counter case. We do need to do it right now because of the computation of the max timestamp, but maybe we could have the other side send use the max timestamp as part of the stream header (but I agree, it&apos;s a bit more complicated).&lt;/p&gt;

&lt;p&gt;For the record, the handling of counter columns amounts to the initial proposition of Stu of moving the cleanup to the reads (though the solution is slightly different). So the &quot;we&apos;ll cleanup on each read before the sstable is compacted&quot; remark does hold here, but I don&apos;t see a better solution right now and the &quot;those sstables will likely be compacted quickly&quot; argument probably make this ok anyway.&lt;/p&gt;

&lt;p&gt;Other comments:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;we need to use Integer.MIN_VALUE as the value for expireBefore when deserializing the columns, otherwise the expired columns will be converted to DeletedColumns, which will change there serialized size (and thus screw up the data size and column index)&lt;/li&gt;
	&lt;li&gt;for markDeltaAsDeleted, we must check if the length is already negative and leave it so if it is, otherwise if a streamed sstable get re-streamed to another node before it was compacted, we could end up not cleaning the delta correctly.&lt;/li&gt;
	&lt;li&gt;it would be nice in SSTW.appendFromStream() to assert the sanity of our little deserialize-reserialize dance and assert what we did write the number of bytes that we wrote in the header.&lt;/li&gt;
	&lt;li&gt;the patch change a clearAllDelta to a markDeltaAsDeleted in CounterColumnTest which is bogus (and the test does fail with that change).&lt;/li&gt;
	&lt;li&gt;I would markDeltaAsDeleted to markForClearingDelta as this describe what the function does better&lt;/li&gt;
	&lt;li&gt;nitpick: there is a few space at end of lines in some comments (I know I know, I&apos;m picky).&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13096885" author="yukim" created="Sun, 4 Sep 2011 15:37:02 +0000"  >&lt;p&gt;Sylvain,&lt;/p&gt;

&lt;p&gt;Thank you for the review.&lt;br/&gt;
For now, I leave the max timestamp calculation part as it is done during streaming.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;we need to use Integer.MIN_VALUE as the value for expireBefore when deserializing the columns, otherwise the expired columns will be converted to DeletedColumns, which will change there serialized size (and thus screw up the data size and column index)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fixed.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;for markDeltaAsDeleted, we must check if the length is already negative and leave it so if it is, otherwise if a streamed sstable get re-streamed to another node before it was compacted, we could end up not cleaning the delta correctly.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;it would be nice in SSTW.appendFromStream() to assert the sanity of our little deserialize-reserialize dance and assert what we did write the number of bytes that we wrote in the header.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Nice point. I added the same assertion as other append() does.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;the patch change a clearAllDelta to a markDeltaAsDeleted in CounterColumnTest which is bogus (and the test does fail with that change).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I forgot to revert this one. I should have run test before submitting...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I would markDeltaAsDeleted to markForClearingDelta as this describe what the function does better&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fixed.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;nitpick: there is a few space at end of lines in some comments (I know I know, I&apos;m picky).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Fixed this one too, I guess.&lt;/p&gt;</comment>
                            <comment id="13097165" author="slebresne" created="Mon, 5 Sep 2011 14:57:18 +0000"  >&lt;p&gt;lgtm, +1.&lt;/p&gt;

&lt;p&gt;Committed with a tiny change to use a cheaper array backed column family in appendToStream, since we deserialize in order (and in a single thread).&lt;/p&gt;</comment>
                            <comment id="13097174" author="hudson" created="Mon, 5 Sep 2011 15:23:41 +0000"  >&lt;p&gt;Integrated in Cassandra #1074 (See &lt;a href=&quot;https://builds.apache.org/job/Cassandra/1074/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Cassandra/1074/&lt;/a&gt;)&lt;br/&gt;
    Handle large rows with single-pass streaming&lt;br/&gt;
patch by yukim; reviewed by slebresne for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-3003&quot; title=&quot;Trunk single-pass streaming doesn&amp;#39;t handle large row correctly&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-3003&quot;&gt;&lt;del&gt;CASSANDRA-3003&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;slebresne : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1165306&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1165306&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/cassandra/trunk/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/cassandra/trunk/src/java/org/apache/cassandra/db/CounterColumn.java&lt;/li&gt;
	&lt;li&gt;/cassandra/trunk/src/java/org/apache/cassandra/db/context/CounterContext.java&lt;/li&gt;
	&lt;li&gt;/cassandra/trunk/src/java/org/apache/cassandra/io/sstable/SSTableWriter.java&lt;/li&gt;
	&lt;li&gt;/cassandra/trunk/src/java/org/apache/cassandra/streaming/IncomingStreamReader.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                    <attachments>
                            <attachment id="12492255" name="3003-v3.txt" size="10857" author="yukim" created="Tue, 30 Aug 2011 15:42:20 +0000"/>
                            <attachment id="12492974" name="3003-v5.txt" size="11135" author="yukim" created="Sun, 4 Sep 2011 15:25:49 +0000"/>
                            <attachment id="12491091" name="ASF.LICENSE.NOT.GRANTED--3003-v1.txt" size="9316" author="yukim" created="Sun, 21 Aug 2011 13:05:58 +0000"/>
                            <attachment id="12491798" name="ASF.LICENSE.NOT.GRANTED--3003-v2.txt" size="10721" author="yukim" created="Fri, 26 Aug 2011 15:52:06 +0000"/>
                            <attachment id="12492476" name="v3003-v4.txt" size="11764" author="yukim" created="Wed, 31 Aug 2011 16:45:42 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[yukim]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>20932</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            14 years, 12 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0ges7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>93825</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>slebresne</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[slebresne]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12963"><![CDATA[Critical]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>