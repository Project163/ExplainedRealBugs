<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:10:18 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-9] Cassandra silently loses data when a single row gets large (under &quot;heavy load&quot;)</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-9</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;When you insert a large number of columns in a single row, cassandra silently loses some or all of these inserts while flushing memtable to disk (potentialy leaving you with zero-sized data files). This happens when the memtable threshold is violated, i.e. when currentSize_ &amp;gt;= threshold_ (MemTableSizeInMB) OR  currentObjectCount_ &amp;gt;= thresholdCount_ (MemTableObjectCountInMillions). This was a problem with the old code in code.google and the code with the jdk7 dependencies also. No OutOfMemory errors are thrown, there is nothing relevant in the logs. It is not clear why this happens under heavy load (when no throttle is used) as it works fine when when you pace requests. I have confirmed this with another member of the community.&lt;/p&gt;


&lt;p&gt;In storage-conf.xml:&lt;/p&gt;

&lt;p&gt;   &amp;lt;HashingStrategy&amp;gt;RANDOM&amp;lt;/HashingStrategy&amp;gt;&lt;br/&gt;
   &amp;lt;MemtableSizeInMB&amp;gt;32&amp;lt;/MemtableSizeInMB&amp;gt;&lt;br/&gt;
   &amp;lt;MemtableObjectCountInMillions&amp;gt;1&amp;lt;/MemtableObjectCountInMillions&amp;gt;&lt;br/&gt;
   &amp;lt;Tables&amp;gt;&lt;br/&gt;
      &amp;lt;Table Name=&quot;MyTable&quot;&amp;gt;&lt;br/&gt;
          &amp;lt;ColumnFamily ColumnType=&quot;Super&quot; ColumnSort=&quot;Name&quot; Name=&quot;MySuper&quot;&amp;gt;&amp;lt;/ColumnFamily&amp;gt;&lt;br/&gt;
      &amp;lt;/Table&amp;gt;&lt;br/&gt;
  &amp;lt;/Tables&amp;gt;&lt;/p&gt;

&lt;p&gt;You can also test it with different values for thresholdCount_ In db/Memtable.java, say:&lt;br/&gt;
    private int thresholdCount_ = 512*1024;&lt;/p&gt;


&lt;p&gt;Here is a small program that will help you reproduce this (hopefully):&lt;/p&gt;

&lt;p&gt;    private static void doWrite() throws Throwable&lt;br/&gt;
    {&lt;br/&gt;
        int numRequests=0;&lt;br/&gt;
        int numRequestsPerSecond = 3;&lt;br/&gt;
        Table table = Table.open(&quot;MyTable&quot;);&lt;br/&gt;
        Random random = new Random();&lt;br/&gt;
        byte[] bytes = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;8&amp;#93;&lt;/span&gt;;&lt;br/&gt;
        String key = &quot;MyKey&quot;;&lt;br/&gt;
        int totalUsed = 0;&lt;br/&gt;
        int total = 0;&lt;br/&gt;
        for (int i = 0; i &amp;lt; 1500; ++i) {&lt;br/&gt;
            RowMutation rm = new RowMutation(&quot;MyTable&quot;, key);&lt;br/&gt;
            random.nextBytes(bytes);&lt;br/&gt;
            int[] used = new int&lt;span class=&quot;error&quot;&gt;&amp;#91;500*1024&amp;#93;&lt;/span&gt;;&lt;br/&gt;
            for (int z=0; z&amp;lt;500*1024;z++) &lt;/p&gt;
{
                used[z]=0;
            }
&lt;p&gt;            int n = random.nextInt(16*1024);&lt;br/&gt;
            for ( int k = 0; k &amp;lt; n; ++k ) {&lt;br/&gt;
                int j = random.nextInt(500*1024);&lt;br/&gt;
                if ( used&lt;span class=&quot;error&quot;&gt;&amp;#91;j&amp;#93;&lt;/span&gt; == 0 ) &lt;/p&gt;
{
                    used[j] = 1;
                    ++totalUsed;
                    //int w = random.nextInt(4);
                    int w = 0;
                    rm.add(&quot;MySuper:SuperColumn-&quot; + j + &quot;:Column-&quot; + i, bytes, w);
                }
&lt;p&gt;            }&lt;br/&gt;
            rm.apply();&lt;br/&gt;
            total += n;&lt;br/&gt;
            System.out.println(&quot;n=&quot;&lt;ins&gt;n&lt;/ins&gt; &quot; total=&quot;+ total+&quot; totalUsed=&quot;+totalUsed);&lt;br/&gt;
            //Thread.sleep(1000*numRequests/numRequestsPerSecond);&lt;br/&gt;
            numRequests++;&lt;br/&gt;
        }&lt;br/&gt;
        System.out.println(&quot;Write done&quot;);&lt;br/&gt;
    }&lt;/p&gt;


&lt;p&gt;PS. Please note that (a) I&apos;m no java guru and (b) I have tried this initially with a C++ thrift client. The outcome is always the same: zero-sized data files under heavy load &amp;#8212; it works fine when you pace requests.&lt;/p&gt;</description>
                <environment>&lt;p&gt;code in trunk, linux-2.6.27-gentoo-r1/, java version &quot;1.7.0-nio2&quot;, 4GB, Intel Core 2 Duo&lt;/p&gt;</environment>
        <key id="12419172">CASSANDRA-9</key>
            <summary>Cassandra silently loses data when a single row gets large (under &quot;heavy load&quot;)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jbellis">Jonathan Ellis</assignee>
                                    <reporter username="neophytos">Neophytos Demetriou</reporter>
                        <labels>
                    </labels>
                <created>Sun, 22 Mar 2009 11:35:35 +0000</created>
                <updated>Tue, 16 Apr 2019 09:33:40 +0000</updated>
                            <resolved>Wed, 15 Apr 2009 20:31:11 +0000</resolved>
                                        <fixVersion>0.3</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                                                                <comments>
                            <comment id="12688138" author="junrao" created="Sun, 22 Mar 2009 15:44:50 +0000"  >&lt;p&gt;Neo,&lt;/p&gt;

&lt;p&gt;Is the problem specific to super CF or does it show up in regular CF too? Also, have you tried flushing smaller amount of data to disk. In cassandra, if you insert a row with key &quot;FlushKey&quot;, it forces a flush on the CF referenced in the insertion.&lt;/p&gt;

&lt;p&gt;Jun&lt;/p&gt;</comment>
                            <comment id="12688142" author="neophytos" created="Sun, 22 Mar 2009 16:25:38 +0000"  >&lt;p&gt;Hi Jun, I&apos;ve done most of the tests using super CFs. Having said that I just tried it with a name-sorted regular CF and the outcome seems to be the same (zero-sized data files when no throttle is used). Please don&apos;t take my word for regular CFs and try it out. One of the reasons it took me so long to report this in public was that I was not sure if (a) it was a problem specific to the hardware I&apos;m using or (b) misuse of Cassandra&apos;s constructs. &lt;/p&gt;

&lt;p&gt;For the case of super CFs, I did extensive testing with the code.google codebase and I did try lowering the thresholds (i.e. threshold_ and thresholdCount_ in Memtable.java). The result of that was that it would write some of the files fine while others were zero-sized. I&apos;ve tried it by forcing flushes (FlushKey), no.&lt;/p&gt;</comment>
                            <comment id="12688143" author="neophytos" created="Sun, 22 Mar 2009 16:27:04 +0000"  >&lt;p&gt;Last line should read: I&apos;ve not tried it by forcing flushes (FlushKey), no. &lt;/p&gt;</comment>
                            <comment id="12688410" author="jbellis" created="Mon, 23 Mar 2009 20:58:08 +0000"  >&lt;p&gt;This is your old friend the ConcurrentModificationException, Neophytos.  Only the ThreadPoolExecutor is eating the exception.  Took me hours to figure out where the hell the exception was disappearing to...  Here&apos;s a patch that exposes it.  Not sure what the fix for the CME is yet but at least it&apos;s out in the open and reproducible. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="12688436" author="jbellis" created="Mon, 23 Mar 2009 21:53:16 +0000"  >&lt;p&gt;Okay, I see the problem.&lt;/p&gt;

&lt;p&gt;Doing an insert (-&amp;gt; CFS.apply -&amp;gt; MT.put) checks for threshold violation; if it&apos;s okay, it schedules a Putter with the mutation on the Memtable.apartments thread pool.&lt;/p&gt;

&lt;p&gt;If it is NOT okay, it schedules a flush &amp;#8211; &lt;em&gt;on another thread pool&lt;/em&gt; (MemtableManager&apos;s flusher).&lt;/p&gt;

&lt;p&gt;So, what happens is, you have a bunch of Putter objects, each with a reference to the old Memtable, in the first threadpool, when the flush starts in the 2nd.  These Putters cause the CME when they get to resolve() while the flush is computing the column index.  (This is why it is easier to make this happen on large rows: index computation takes longer).&lt;/p&gt;

&lt;p&gt;I think the easiest fix will be to make the apartments threadpool (executorservice) non-static, and just have one per memtable; then flush could wait for the service to finish before doing its thing.  Memory and thread churn will be nominal since memtable flush is so rare, relatively speaking.  Creating a new thread after flush is no big deal.&lt;/p&gt;

&lt;p&gt;I&apos;ll get on the patch, just wanted to post an update in the meantime so nobody else needs to bang his head on this.&lt;/p&gt;</comment>
                            <comment id="12688443" author="jbellis" created="Mon, 23 Mar 2009 22:16:43 +0000"  >&lt;p&gt;Here is the patch, following my proposed fix above.  Works like a champ.&lt;/p&gt;</comment>
                            <comment id="12688456" author="neophytos" created="Mon, 23 Mar 2009 23:26:34 +0000"  >&lt;p&gt;Confirmed. Thank you Jonathan.&lt;/p&gt;</comment>
                            <comment id="12688459" author="neophytos" created="Mon, 23 Mar 2009 23:36:38 +0000"  >&lt;p&gt;Just a quick diff against code in trunk for your convenience. Please verify with Jonathan&apos;s patch before commit.&lt;/p&gt;</comment>
                            <comment id="12688704" author="jbellis" created="Tue, 24 Mar 2009 14:53:54 +0000"  >&lt;p&gt;Here is a cleaner solution that adds the flush to the ExecutorService terminated() method which seems cleaner than having the flush itself (running in the Manager service) reach back to the Memtable service and block while waiting for shutdown.  (In a busy system we don&apos;t want to block the Manager service.)&lt;/p&gt;

&lt;p&gt;Note that this also handles waiting for gets() to finish before flushing &amp;#8211; any logic purely in put() will not be able to do that, because get never checks threshold or acquires a lock.&lt;/p&gt;</comment>
                            <comment id="12688722" author="avinash.lakshman@gmail.com" created="Tue, 24 Mar 2009 15:50:26 +0000"  >&lt;p&gt;I am going to look at this once I get into work. I will apply/fix this today.&lt;/p&gt;</comment>
                            <comment id="12688729" author="jbellis" created="Tue, 24 Mar 2009 16:04:00 +0000"  >&lt;p&gt;v3 applies cleanly against trunk.&lt;/p&gt;</comment>
                            <comment id="12688898" author="avinash.lakshman@gmail.com" created="Tue, 24 Mar 2009 21:58:02 +0000"  >&lt;p&gt;The problem was identified by Jonathan Ellis. I have a fix checked in that requires a change only in the Memtable class. Neophytos has verified that my change actually works. But the credit goes to Jonathan for identifying the problem which was the harder part of this whole exercise. I am deeming this case as closed.&lt;/p&gt;</comment>
                            <comment id="12688903" author="jbellis" created="Tue, 24 Mar 2009 22:12:51 +0000"  >&lt;p&gt;the problem with r758044 is it does not address GETs &amp;#8211; you can still have Getter ops on the the service added after the Flusher, so they will execute during / after the flush.  that is why I split the apartments_ into a per-memtable instance var and run the flush on terminate.  it&apos;s the cleanest way to be correct w/ gets w/o introducing explicit locks.&lt;/p&gt;</comment>
                            <comment id="12688919" author="avinash.lakshman@gmail.com" created="Tue, 24 Mar 2009 22:32:19 +0000"  >&lt;p&gt;Hmm. Should that matter. Gets do not modify the collection. I was under the impression that CME occurs when one thread tries to modify a collection when another is iterating over it. I will continue to look. Of course the whole apartment concept was introduced to eliminate locks.&lt;/p&gt;</comment>
                            <comment id="12688932" author="jbellis" created="Tue, 24 Mar 2009 23:18:02 +0000"  >&lt;p&gt;Gets won&apos;t cause the CME, flush will. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  calling columnFamily.clear(); as it goes can cause a CME as Get looks through it.  Of course even if it did not you would get back invalid results operating on a half-cleared-out Memtable.&lt;/p&gt;


&lt;p&gt;In general it is just difficult to reason about concurrency when an object is being accessed from multiple threads at a time, even if it were okay today to &quot;cheat&quot; a bit, it will probably bite us down the road.&lt;/p&gt;</comment>
                            <comment id="12688937" author="avinash.lakshman@gmail.com" created="Tue, 24 Mar 2009 23:49:12 +0000"  >&lt;p&gt;Not sure if there can ever be a Getter on the queue after a Flusher has been enqueued. Once you are in the state where a Flusher() has been enqueued there can be no Getter() for the same Memtable. Anyways I will look into it tonight again.&lt;/p&gt;</comment>
                            <comment id="12688952" author="jbellis" created="Wed, 25 Mar 2009 00:21:54 +0000"  >&lt;p&gt;&amp;gt; Not sure if there can ever be a Getter on the queue after a Flusher has been enqueued. Once you are in the state where a Flusher() has been enqueued there can be no Getter() for the same Memtable.&lt;/p&gt;

&lt;p&gt;That is how easy it is to be fooled in these things &amp;#8211; that is what we want, but we are not enforcing that.&lt;/p&gt;

&lt;p&gt;In particular note that the line&lt;/p&gt;

&lt;p&gt;    		cf = apartments_.get(cfName_).submit(call).get();&lt;/p&gt;

&lt;p&gt;is not atomic.&lt;/p&gt;

&lt;p&gt;GET thread can execute apartments_.get(cfName_)&lt;/p&gt;

&lt;p&gt;then PUT thread gets CPU (or executes concurrently on another core), switches memtable, and queues Flusher.&lt;/p&gt;

&lt;p&gt;Thread A gets the CPU back and calls submit.  Getter is now on queue after Flusher.&lt;/p&gt;</comment>
                            <comment id="12688953" author="avinash.lakshman@gmail.com" created="Wed, 25 Mar 2009 00:26:36 +0000"  >&lt;p&gt;Ahh. I see. For some reason I was seeing from inside the apartment. This is no good. I will fix it tonight. &lt;/p&gt;</comment>
                            <comment id="12688955" author="jbellis" created="Wed, 25 Mar 2009 00:36:11 +0000"  >&lt;p&gt;Sorry, that&apos;s the right result but the wrong explanation.  (Son was howling at me &amp;#8211; very distracting.)&lt;/p&gt;

&lt;p&gt;It is the getter creation / submit that is problematic, not the apartment get/submit.&lt;/p&gt;

&lt;p&gt;that is,&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;thread A                                                              thread B
&lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Getter(key, cfName, filter);
                                                                             &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; Flusher(cLogCtx);
                                                                             submit(flusher);
submit(getter);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="12688956" author="sandeep_tata" created="Wed, 25 Mar 2009 00:43:59 +0000"  >&lt;p&gt;I think today, because of the way the code stands, you won&apos;t enqueue Getters on the table after you enqueue a flusher. But, I don&apos;t see how simply adding a flusher to the apartment&apos;s DebuggableThreadPool (instead of running the flusher in a separate thread) guarantees that there are no concurrent Putters/Getters still in the threadpool. Am I&apos;m missing something? &lt;/p&gt;

&lt;p&gt;I agree that running flush in terminated() by overriding the method is the cleanest approach. You don&apos;t have to rely on the fact that the rest of the code (today) is such that you won&apos;t end up queuing a Getter after a Flusher (I&apos;m guessing this is what Jonathan meant by &quot;cheat&quot; a bit today). This guarantee is precisely the reason ThreadPoolExecutor provides this hook.&lt;/p&gt;


</comment>
                            <comment id="12688960" author="sandeep_tata" created="Wed, 25 Mar 2009 01:05:03 +0000"  >&lt;p&gt;Ah, there&apos;s a whole bunch of worker threads talking to CFStore (and therefore memtable) &amp;#8211; I see how we can end up with Getters after adding a Flusher&lt;/p&gt;</comment>
                            <comment id="12688980" author="avinash.lakshman@gmail.com" created="Wed, 25 Mar 2009 02:44:03 +0000"  >&lt;p&gt;It is an issue that is actually a non-issue. In the worst case the Getter will return NULL since it read an empty memtable (maybe memtable got cleared). But that is fine because now the disk read will happen from buffer cache. It is not incorrect. No harm will be done.&lt;/p&gt;</comment>
                            <comment id="12688983" author="jbellis" created="Wed, 25 Mar 2009 03:04:27 +0000"  >&lt;p&gt;Worst case, the flush clear() happens while the getter is iterating columns, and you get CME.&lt;/p&gt;</comment>
                            <comment id="12688986" author="avinash.lakshman@gmail.com" created="Wed, 25 Mar 2009 03:32:59 +0000"  >&lt;p&gt;Get rid of clear() &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. It is a useless call anyway.&lt;/p&gt;</comment>
                            <comment id="12688988" author="avinash.lakshman@gmail.com" created="Wed, 25 Mar 2009 03:54:29 +0000"  >&lt;p&gt;Actually I take that back. There is no CME unless iterators are involved. But nevertheless the safest thing would be to not do the clear(). And I think everything will be good.&lt;/p&gt;</comment>
                            <comment id="12688991" author="jbellis" created="Wed, 25 Mar 2009 04:07:46 +0000"  >&lt;p&gt;I thought the point of clear was to free up memory as the flush progresses.  Isn&apos;t that worth a dozen more lines of code?&lt;/p&gt;</comment>
                            <comment id="12689079" author="jbellis" created="Wed, 25 Mar 2009 11:15:38 +0000"  >&lt;p&gt;Ah, I see, we were talking about different clear(). Yes, the one from the end that you removed is always irrelevant (and not going to cause a CME).&lt;/p&gt;

&lt;p&gt;It is the columnFamily.clear() in the middle that is still there that both frees up memory (\yes, of course by &quot;free up memory&quot; I mean &quot;make it available to be GC&apos;d&quot;) and can cause CME on the iterations that GET does.&lt;/p&gt;</comment>
                            <comment id="12689648" author="junrao" created="Thu, 26 Mar 2009 20:50:12 +0000"  >&lt;p&gt;Looking at the latest code. Both flush and put on a CF are submitted to the same ExecutorPool for that CF. Since the ExecutorPool has 1 thread in it, this means that the flushing of an old memtable will not run concurrently with the updates on a new memtable in the same CF. This limits concurrency.&lt;/p&gt;
</comment>
                            <comment id="12689652" author="avinash.lakshman@gmail.com" created="Thu, 26 Mar 2009 20:55:19 +0000"  >&lt;p&gt;The put thread does not run the flush. You submit to the put thread. It submits it to the manager service. Maybe I am missing something here?&lt;/p&gt;</comment>
                            <comment id="12689654" author="jbellis" created="Thu, 26 Mar 2009 20:56:20 +0000"  >&lt;p&gt;Wrong.  The Flusher that goes on the Memtable executor is just a stub that kicks off the real flush in the MemtableManager&apos;s executor.&lt;/p&gt;

&lt;p&gt;So when you have a Getter queued after that flusher, which can happen as I described above, the getter can get a CME while it is iterating through columns at the same time as the real flush calls cf.clear().&lt;/p&gt;</comment>
                            <comment id="12689655" author="jbellis" created="Thu, 26 Mar 2009 20:59:10 +0000"  >&lt;p&gt;(I was writing my comment at the same time as Avinash, so my &quot;Wrong&quot; was referring to Jun&apos;s assertion that &quot;the flushing of an old memtable will not run concurrently with the updates on a new memtable&quot;.)&lt;/p&gt;</comment>
                            <comment id="12689668" author="junrao" created="Thu, 26 Mar 2009 21:15:05 +0000"  >&lt;p&gt;OK. I see it now. The real work of Flush is done in a separate thread. Sorry for the false alarm.&lt;/p&gt;</comment>
                            <comment id="12696267" author="jbellis" created="Mon, 6 Apr 2009 21:45:11 +0000"  >&lt;p&gt;Fixes potential CME with GETs.&lt;/p&gt;</comment>
                            <comment id="12697528" author="jbellis" created="Thu, 9 Apr 2009 15:04:26 +0000"  >&lt;p&gt;(Todd pointed out that having a per-Memtable executor is also more efficient by not needing to hash CF names to look up the executor.)&lt;/p&gt;</comment>
                            <comment id="12697536" author="jbellis" created="Thu, 9 Apr 2009 15:29:00 +0000"  >&lt;p&gt;rebased to HEAD&lt;/p&gt;</comment>
                            <comment id="12697582" author="johanoskarsson" created="Thu, 9 Apr 2009 18:02:14 +0000"  >&lt;p&gt;From my limited understanding of that code the latest patch gets a +1, looks clean. But I&apos;d recommend that someone with a bit more experience look at it.&lt;/p&gt;</comment>
                            <comment id="12697834" author="avinash.lakshman@gmail.com" created="Fri, 10 Apr 2009 15:09:53 +0000"  >&lt;p&gt;I am just confused about one thing here. Why is there a chance of a CME on a get? I mean as far as I know a CME occurs when one thread is iterating (using an iterator) and another tries to modify the collection. That is not something that can happen here on a get, I think. So if that is the case there is no need for this change. Hash function cf name lookup is a non issue here.&lt;/p&gt;</comment>
                            <comment id="12697835" author="jbellis" created="Fri, 10 Apr 2009 15:16:58 +0000"  >&lt;p&gt;Right.  Get iterates over the columns, depending on the filter used.  Flush still clears out each CF as it is flushed:&lt;/p&gt;

&lt;p&gt;                ssTable.append(partitioner.decorateKey(key), buffer);&lt;br/&gt;
                bf.add(key);&lt;br/&gt;
                columnFamily.clear();&lt;/p&gt;

&lt;p&gt;This is behavior I want to keep since the overhead can be relatively high when column values are small.  And the new code is simpler to reason about since you only ever have one thread accessing things rather than executing gets during the flush.  (If we took the clear() out, we would be ok for now, but what if someone changes flush in six months?  One thread at a time is safer, especially vs &lt;em&gt;almost&lt;/em&gt; always one thread at a time which becomes easy to forget the exceptions.)&lt;/p&gt;</comment>
                            <comment id="12697846" author="jbellis" created="Fri, 10 Apr 2009 15:50:27 +0000"  >&lt;p&gt;Forgot to mention one more benefit to executor-per-memtable: this lets us easily call forceFlush in tests and then wait for the flush to finish before proceeding to do tests on the flushed sstable.  (That is why #59 blocks on this.)&lt;/p&gt;</comment>
                            <comment id="12697850" author="avinash.lakshman@gmail.com" created="Fri, 10 Apr 2009 16:03:41 +0000"  >&lt;p&gt;Why would you want to wait to do tests? In the real world that is not what happens. You should be able to do reads even before the flush is complete. It should be seamless. Even a new memtable is served out the old one is maintained till the flush is complete. So this should really matter. If you just want to test the writes into SSTable then write into it and then test. I think this should not be a reason for the proposed change. May I missing something here.&lt;/p&gt;</comment>
                            <comment id="12697851" author="jbellis" created="Fri, 10 Apr 2009 16:06:21 +0000"  >&lt;p&gt;It&apos;s a side benefit for the change, not a motivation.&lt;/p&gt;

&lt;p&gt;Certainly testing a flush and making sure the resulting sstable has the same data that the memtable did is a good test to have.&lt;/p&gt;</comment>
                            <comment id="12698600" author="tlipcon" created="Tue, 14 Apr 2009 00:33:10 +0000"  >&lt;p&gt;Here&apos;s a review against the newest patch:&lt;/p&gt;

&lt;p&gt;First, some style nits in Memtable.java:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;runningExecutorServices member variable should have a trailing _ for style consistency&lt;/li&gt;
	&lt;li&gt;same with executor_&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Regarding the actual contents of the patch, I sort of dislike subclassing the executor to do work on terminate, but it&apos;s the cleanest solution I can think of, so +1&lt;/p&gt;</comment>
                            <comment id="12698653" author="jbellis" created="Tue, 14 Apr 2009 05:03:56 +0000"  >&lt;p&gt;I will make the style changes; thanks for the review, Todd.&lt;/p&gt;

&lt;p&gt;Any further discussion needed before commit?&lt;/p&gt;</comment>
                            <comment id="12699362" author="jbellis" created="Wed, 15 Apr 2009 20:31:11 +0000"  >&lt;p&gt;committed&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12422158">CASSANDRA-59</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12422613">CASSANDRA-76</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12405077" name="0001-better-fix-for-9-v2.patch" size="8505" author="jbellis" created="Thu, 9 Apr 2009 15:29:00 +0000"/>
                            <attachment id="12404773" name="0001-better-fix-for-9.patch" size="8804" author="jbellis" created="Mon, 6 Apr 2009 21:45:10 +0000"/>
                            <attachment id="12403456" name="executor.patch" size="2045" author="jbellis" created="Mon, 23 Mar 2009 20:58:27 +0000"/>
                            <attachment id="12403467" name="shutdown-before-flush-against-trunk.patch" size="6703" author="neophytos" created="Mon, 23 Mar 2009 23:36:38 +0000"/>
                            <attachment id="12403525" name="shutdown-before-flush-v2.patch" size="7481" author="jbellis" created="Tue, 24 Mar 2009 14:53:54 +0000"/>
                            <attachment id="12403528" name="shutdown-before-flush-v3-trunk.patch" size="7056" author="jbellis" created="Tue, 24 Mar 2009 16:03:59 +0000"/>
                            <attachment id="12403465" name="shutdown-before-flush.patch" size="8801" author="jbellis" created="Mon, 23 Mar 2009 22:16:43 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[jbellis]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>19513</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            16 years, 32 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0fwc7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>90837</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>