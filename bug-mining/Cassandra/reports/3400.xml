<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:51:08 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-8366] Repair grows data on nodes, causes load to become unbalanced</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-8366</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;There seems to be something weird going on when repairing data.&lt;/p&gt;

&lt;p&gt;I have a program that runs 2 hours which inserts 250 random numbers and reads 250 times per second. It creates 2 keyspaces with SimpleStrategy and RF of 3. &lt;/p&gt;

&lt;p&gt;I use size-tiered compaction for my cluster. &lt;/p&gt;

&lt;p&gt;After those 2 hours I run a repair and the load of all nodes goes up. If I run incremental repair the load goes up alot more. I saw the load shoot up 8 times the original size multiple times with incremental repair. (from 2G to 16G)&lt;/p&gt;


&lt;p&gt;with node 9 8 7 and 6 the repro procedure looked like this:&lt;br/&gt;
(Note that running full repair first is not a requirement to reproduce.)&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;After 2 hours of 250 reads + 250 writes per second:
UN  9  583.39 MB  256     ?       28220962-26ae-4eeb-8027-99f96e377406  rack1
UN  8  584.01 MB  256     ?       f2de6ea1-de88-4056-8fde-42f9c476a090  rack1
UN  7  583.72 MB  256     ?       2b6b5d66-13c8-43d8-855c-290c0f3c3a0b  rack1
UN  6  583.84 MB  256     ?       b8bd67f1-a816-46ff-b4a4-136ad5af6d4b  rack1

Repair -pr -par on all nodes sequentially
UN  9  746.29 MB  256     ?       28220962-26ae-4eeb-8027-99f96e377406  rack1
UN  8  751.02 MB  256     ?       f2de6ea1-de88-4056-8fde-42f9c476a090  rack1
UN  7  748.89 MB  256     ?       2b6b5d66-13c8-43d8-855c-290c0f3c3a0b  rack1
UN  6  758.34 MB  256     ?       b8bd67f1-a816-46ff-b4a4-136ad5af6d4b  rack1

repair -inc -par on all nodes sequentially
UN  9  2.41 GB    256     ?       28220962-26ae-4eeb-8027-99f96e377406  rack1
UN  8  2.53 GB    256     ?       f2de6ea1-de88-4056-8fde-42f9c476a090  rack1
UN  7  2.6 GB     256     ?       2b6b5d66-13c8-43d8-855c-290c0f3c3a0b  rack1
UN  6  2.17 GB    256     ?       b8bd67f1-a816-46ff-b4a4-136ad5af6d4b  rack1

after rolling restart
UN  9  1.47 GB    256     ?       28220962-26ae-4eeb-8027-99f96e377406  rack1
UN  8  1.5 GB     256     ?       f2de6ea1-de88-4056-8fde-42f9c476a090  rack1
UN  7  2.46 GB    256     ?       2b6b5d66-13c8-43d8-855c-290c0f3c3a0b  rack1
UN  6  1.19 GB    256     ?       b8bd67f1-a816-46ff-b4a4-136ad5af6d4b  rack1

compact all nodes sequentially
UN  9  989.99 MB  256     ?       28220962-26ae-4eeb-8027-99f96e377406  rack1
UN  8  994.75 MB  256     ?       f2de6ea1-de88-4056-8fde-42f9c476a090  rack1
UN  7  1.46 GB    256     ?       2b6b5d66-13c8-43d8-855c-290c0f3c3a0b  rack1
UN  6  758.82 MB  256     ?       b8bd67f1-a816-46ff-b4a4-136ad5af6d4b  rack1

repair -inc -par on all nodes sequentially
UN  9  1.98 GB    256     ?       28220962-26ae-4eeb-8027-99f96e377406  rack1
UN  8  2.3 GB     256     ?       f2de6ea1-de88-4056-8fde-42f9c476a090  rack1
UN  7  3.71 GB    256     ?       2b6b5d66-13c8-43d8-855c-290c0f3c3a0b  rack1
UN  6  1.68 GB    256     ?       b8bd67f1-a816-46ff-b4a4-136ad5af6d4b  rack1

restart once more
UN  9  2 GB       256     ?       28220962-26ae-4eeb-8027-99f96e377406  rack1
UN  8  2.05 GB    256     ?       f2de6ea1-de88-4056-8fde-42f9c476a090  rack1
UN  7  4.1 GB     256     ?       2b6b5d66-13c8-43d8-855c-290c0f3c3a0b  rack1
UN  6  1.68 GB    256     ?       b8bd67f1-a816-46ff-b4a4-136ad5af6d4b  rack1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is there something im missing or is this strange behavior?&lt;/p&gt;</description>
                <environment>&lt;p&gt;4 node cluster&lt;br/&gt;
2.1.2 Cassandra&lt;br/&gt;
Inserts and reads are done with CQL driver&lt;/p&gt;</environment>
        <key id="12757352">CASSANDRA-8366</key>
            <summary>Repair grows data on nodes, causes load to become unbalanced</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="marcuse">Marcus Eriksson</assignee>
                                    <reporter username="Jan Karlsson">Jan Karlsson</reporter>
                        <labels>
                    </labels>
                <created>Mon, 24 Nov 2014 09:32:04 +0000</created>
                <updated>Tue, 16 Apr 2019 09:31:27 +0000</updated>
                            <resolved>Tue, 3 Mar 2015 10:27:26 +0000</resolved>
                                        <fixVersion>2.1.5</fixVersion>
                                    <component>Consistency/Repair</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14229328" author="aboudreault" created="Mon, 1 Dec 2014 01:52:17 +0000"  >&lt;p&gt;I have been able to reproduce the issue with 2.1.2 and branch cassandra-2.1. From my tests, the issue seems to be related the parallel incremental repairs. I don&apos;t see the issue with  full repairs. With full repairs, the storage size increases but everything is fine after a compaction.  With incremental repairs, I&apos;ve seen nodes going from 1.5G to 15G of storage size. &lt;/p&gt;

&lt;p&gt;It looks like something is broken with inc repairs. Most of the time, I get one of the following errors during the repairs:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Repair session 6f6c4ae0-78d6-11e4-9b48-b56034537865 for range (3074457345618258602,-9223372036854775808] failed with error org.apache.cassandra.exceptions.RepairException: &lt;a href=&quot;#6f6c4ae0-78d6-11e4-9b48-b56034537865 on r1/Standard1, (3074457345618258602,-9223372036854775808&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;repair #6f6c4ae0-78d6-11e4-9b48-b56034537865 on r1/Standard1, (3074457345618258602,-9223372036854775808&lt;/a&gt;] Sync failed between /127.0.0.1 and /127.0.0.3&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Repair failed with error Did not get positive replies from all endpoints. List of failed endpoint(s): &lt;span class=&quot;error&quot;&gt;&amp;#91;127.0.0.1&amp;#93;&lt;/span&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So this issue might be related to &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8316&quot; title=&quot;&amp;quot;Did not get positive replies from all endpoints&amp;quot; error on incremental repair&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8316&quot;&gt;&lt;del&gt;CASSANDRA-8316&lt;/del&gt;&lt;/a&gt; .  I&apos;ve attached the script I used to reproduce the issue and also 3 result files.&lt;/p&gt;</comment>
                            <comment id="14276894" author="aboudreault" created="Wed, 14 Jan 2015 13:15:31 +0000"  >&lt;p&gt;Since we have a working patch for 8316. I plan to replay with this ticket in the next days.&lt;/p&gt;</comment>
                            <comment id="14286873" author="aboudreault" created="Thu, 22 Jan 2015 03:07:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt; I&apos;m currently testing this with the 8316 patch. I am a little bit worried about the fact that I am not able to get my nodes repaired and rebalanced properly after a &quot;Did not get a positive ...&quot; error. This depends when the issue happens but if it happens soon, the inc repair seems to double my cluster size. I thought that running a full sequential repair, then cleanup and re-compact all nodes would fix the balancing issue, but it doesn&apos;t. I don&apos;t seem to be able to reproduce that balancing issue without incremental repairs or if  the error &quot;Did not get positive...&quot; doesn&apos;t occur. Should I try anything else to restore my nodes storage to its initial size prior the failed inc repair? &lt;/p&gt;</comment>
                            <comment id="14287048" author="krummas" created="Thu, 22 Jan 2015 07:19:04 +0000"  >&lt;p&gt;Does it stream data?&lt;br/&gt;
Is disk space usage on disk the same as the one in nodetool status?&lt;br/&gt;
What happens if you restart the nodes?&lt;br/&gt;
What happens if you run a major compaction?&lt;br/&gt;
Are they really supposed to get back to the size before the repair? If some repair sessions actually finish, maybe the data should be there?&lt;/p&gt;</comment>
                            <comment id="14289415" author="aboudreault" created="Fri, 23 Jan 2015 15:42:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt; I&apos;m attaching a new version of the test script. (testv2.sh). This one has some improvements and gives more details after each operations (it shows sstable size, wait properly that all compaction tasks finish, display  streaming status, it flushes nodes, it cleans nodes etc.).&lt;/p&gt;

&lt;p&gt;I&apos;ve run  3 times the script to see the differences. &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;run1 is the only real successful result. The reason is that I compact all nodes right after the cassandra-stress operation. Apparently, this removed the need to repair, so everything is fine and at the end of the script all nodes are at the proper size (1.43G).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;run2 doesn&apos;t compact after the stress. The repair is then ran and we only see the &quot;Did not get a positive answer&quot; until the end of the node2 repair. So we can see that the keyspace r1 has been successfully repaired for node1 and node2. The repair for node3 failed but it seems that the 2 other repairs have taken care to repair things so everything is OK at the end of the script. (node size ~1.43G). However, this run grows node size significantly: 1.4G -&amp;gt; 9G (after the repair).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;run3 doesn&apos;t compact after the stress. This time, the repair fails at the beginning (node1 repair call). This makes the node2 and node2 repairs fails too. After flushing + cleaning + compacting, all nodes have an extra 1G of data, which I don&apos;t know what they are. There is no streaming, all compaction is done and looks like I cannot get rid of them. This is not in the log, but I restarted my cluster again, then retried to full repair sequentially all nodes then re-cleaning, re-compacting and nothing changed. I let the cluster ran all night long to be sure. I have not deleted this cluster so if you need more information, I just have to restart it.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Do you see anything wrong in my tests? Ping me on IRC if you want to discuss more about this ticket. &lt;/p&gt;

</comment>
                            <comment id="14320462" author="aboudreault" created="Fri, 13 Feb 2015 18:01:08 +0000"  >&lt;p&gt;Adding a log file run with latest cassandra-2.1 branch: &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12698771/12698771_results-10000000-inc-repairs.txt&quot; title=&quot;results-10000000-inc-repairs.txt attached to CASSANDRA-8366&quot;&gt;results-10000000-inc-repairs.txt&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="14323935" author="krummas" created="Tue, 17 Feb 2015 08:37:14 +0000"  >&lt;p&gt;The problem is that we don&apos;t block the nodetool command until anticompaction has completed on the other nodes, this means that when nodetool repair exits, we might still be doing anticompaction on the other nodes, so if you start a new repair immediately once it exits, you might get that error.&lt;/p&gt;

&lt;p&gt;I doubt anyone does this in real life, but we should probably fix it (for 3.0 as it needs a new message, AntiCompactionCompleted or something)&lt;/p&gt;

&lt;p&gt;I added a sleep after each repair command and it completed without errors&lt;/p&gt;</comment>
                            <comment id="14324163" author="aboudreault" created="Tue, 17 Feb 2015 13:10:39 +0000"  >&lt;p&gt;What about the node loads?&lt;/p&gt;</comment>
                            <comment id="14324190" author="krummas" created="Tue, 17 Feb 2015 14:03:59 +0000"  >&lt;p&gt;I tried it once more, with autocompaction disabled to remove a bit of randomness (incremental repairs are sensitive to compactions as that will make an actually repaired sstable not be anticompacted since it has been compacted away). &lt;/p&gt;

&lt;p&gt;after 1 run with incremental repair:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ du -sch /home/marcuse/.ccm/8366/node?/data/r1/                                                                                    
1,5G    /home/marcuse/.ccm/8366/node1/data/r1/
1,5G    /home/marcuse/.ccm/8366/node2/data/r1/
1,5G    /home/marcuse/.ccm/8366/node3/data/r1/
4,4G    total
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;all sstables were marked as repaired&lt;/p&gt;

&lt;p&gt;and, after 1 run with standard repair:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ du -sch /home/marcuse/.ccm/8366/node?/data/r1/
1,5G    /home/marcuse/.ccm/8366/node1/data/r1/
1,5G    /home/marcuse/.ccm/8366/node2/data/r1/
1,5G    /home/marcuse/.ccm/8366/node3/data/r1/
4,4G    total
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;but, after an incremental repair with compactions enabled:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ du -sch /home/marcuse/.ccm/8366/node?/data/r1/
2,3G    /home/marcuse/.ccm/8366/node1/data/r1/
2,8G    /home/marcuse/.ccm/8366/node2/data/r1/
2,0G    /home/marcuse/.ccm/8366/node3/data/r1/
6,9G    total
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And, the reason is that we validate the wrong sstables:&lt;/p&gt;

&lt;p&gt;1. we send out a prepare message to all nodes, the nodes select which sstables to repair&lt;br/&gt;
2. time passes, sstables get compacted (basically randomly)&lt;br/&gt;
3. we start validating the sstables out of the ones we picked in (1) &lt;b&gt;that still exist&lt;/b&gt;. This set will differ between nodes.&lt;br/&gt;
4. overstream, pain&lt;/p&gt;

&lt;p&gt;Bug. Stand by for patch&lt;/p&gt;</comment>
                            <comment id="14324206" author="aboudreault" created="Tue, 17 Feb 2015 14:10:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt;Thank you for your work and tests. I will stay on touch to test the patch when ready.&lt;/p&gt;</comment>
                            <comment id="14324306" author="krummas" created="Tue, 17 Feb 2015 15:27:53 +0000"  >&lt;p&gt;attaching patch that picks the sstables to compact as late as possible.&lt;/p&gt;

&lt;p&gt;Actually a semi-backport of &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-7586&quot; title=&quot;Mark SSTables as repaired after full repairs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-7586&quot;&gt;&lt;del&gt;CASSANDRA-7586&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We will still have a slightly bigger live size on the nodes after one of these repairs as some sstables will not get anticompacted due to being compacted away (we could probably improve this as well, but in another ticket), but it is much better:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ du -sch /home/marcuse/.ccm/8366/node?/data/r1/
1,8G    /home/marcuse/.ccm/8366/node1/data/r1/
1,8G    /home/marcuse/.ccm/8366/node2/data/r1/
1,8G    /home/marcuse/.ccm/8366/node3/data/r1/
5,2G    total
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="14328898" author="aboudreault" created="Fri, 20 Feb 2015 13:18:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt; The patch looks good to me. I agree that there is still a slightly data overhead, but there is a huge improvement. I&apos;ve written a dtest for this ticket: &lt;a href=&quot;https://github.com/riptano/cassandra-dtest/pull/171&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/riptano/cassandra-dtest/pull/171&lt;/a&gt; and here are the results:&lt;/p&gt;

&lt;p&gt;Result Without the patch (notice the execution time):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;dtest: DEBUG: Total Load size: 10.96GB
--------------------- &amp;gt;&amp;gt; end captured logging &amp;lt;&amp;lt; ---------------------
----------------------------------------------------------------------
Ran 1 test in 5999.675s   

FAILED (failures=1)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Result with the patch (The test tolerate 25% of data overhead, so maximum 5.5GB):&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;----------------------------------------------------------------------
Ran 1 test in 2313.278s

OK
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Running repairs with the patch is a lot faster. Is there a ticket already created about the sstables will not get anticompacted? &lt;/p&gt;

&lt;p&gt;Thanks for the work!&lt;/p&gt;</comment>
                            <comment id="14334831" author="krummas" created="Tue, 24 Feb 2015 12:33:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8858&quot; title=&quot;Avoid not doing anticompaction on compacted away sstables&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8858&quot;&gt;&lt;del&gt;CASSANDRA-8858&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14339436" author="yukim" created="Thu, 26 Feb 2015 23:55:18 +0000"  >&lt;p&gt;You should use &lt;tt&gt;cfs.selectAndReference&lt;/tt&gt; instead of looping and referencing in CompactionManager change.&lt;br/&gt;
Especially &lt;tt&gt;Refs.ref&lt;/tt&gt; throws exception when referencing failed. &lt;tt&gt;selectAndReference&lt;/tt&gt; uses &lt;tt&gt;tryRef&lt;/tt&gt; which retries until reference is acquired.&lt;/p&gt;

&lt;p&gt;Otherwise +1.&lt;/p&gt;</comment>
                            <comment id="14344882" author="krummas" created="Tue, 3 Mar 2015 10:27:26 +0000"  >&lt;p&gt;ok, thanks all, committed with that change&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12964332">CASSANDRA-11696</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12755300">CASSANDRA-8316</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12787842">CASSANDRA-9109</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12699271" name="0001-8366.patch" size="7388" author="marcuse" created="Tue, 17 Feb 2015 15:27:53 +0000"/>
                            <attachment id="12698771" name="results-10000000-inc-repairs.txt" size="24617" author="aboudreault" created="Fri, 13 Feb 2015 18:01:08 +0000"/>
                            <attachment id="12684336" name="results-17500000_inc_repair.txt" size="5435" author="aboudreault" created="Mon, 1 Dec 2014 01:52:17 +0000"/>
                            <attachment id="12684332" name="results-5000000_1_inc_repairs.txt" size="4924" author="aboudreault" created="Mon, 1 Dec 2014 01:52:17 +0000"/>
                            <attachment id="12684333" name="results-5000000_2_inc_repairs.txt" size="13273" author="aboudreault" created="Mon, 1 Dec 2014 01:52:17 +0000"/>
                            <attachment id="12684334" name="results-5000000_full_repair_then_inc_repairs.txt" size="8203" author="aboudreault" created="Mon, 1 Dec 2014 01:52:17 +0000"/>
                            <attachment id="12684335" name="results-5000000_inc_repairs_not_parallel.txt" size="3493" author="aboudreault" created="Mon, 1 Dec 2014 01:52:17 +0000"/>
                            <attachment id="12694173" name="run1_with_compact_before_repair.log" size="60193" author="aboudreault" created="Fri, 23 Jan 2015 15:42:12 +0000"/>
                            <attachment id="12694174" name="run2_no_compact_before_repair.log" size="80185" author="aboudreault" created="Fri, 23 Jan 2015 15:42:12 +0000"/>
                            <attachment id="12694172" name="run3_no_compact_before_repair.log" size="69181" author="aboudreault" created="Fri, 23 Jan 2015 15:42:12 +0000"/>
                            <attachment id="12684331" name="test.sh" size="1551" author="aboudreault" created="Mon, 1 Dec 2014 01:52:17 +0000"/>
                            <attachment id="12694171" name="testv2.sh" size="3869" author="aboudreault" created="Fri, 23 Jan 2015 15:42:12 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[marcuse]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 38 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i22pfj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311421" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Reproduced In</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12326774">2.1.1</customfieldvalue>
    <customfieldvalue id="12328841">2.1.2</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>yukim</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[yukim]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311124" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Tester</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>aboudreault</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>