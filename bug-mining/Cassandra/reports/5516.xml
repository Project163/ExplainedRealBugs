<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:17:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-15229] Segregate Network and Chunk Cache BufferPools and Recirculate Partially Freed Chunks</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-15229</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;The BufferPool was never intended to be used for a &lt;tt&gt;ChunkCache&lt;/tt&gt;, and we need to either change our behaviour to handle uncorrelated lifetimes or use something else.  This is particularly important with the default chunk size for compressed sstables being reduced.  If we address the problem, we should also utilise the BufferPool for native transport connections like we do for internode messaging, and reduce the number of pooling solutions we employ.&lt;br/&gt;
Probably the best thing to do is to improve BufferPool&#8217;s behaviour when used for things with uncorrelated lifetimes, which essentially boils down to tracking those chunks that have not been freed and re-circulating them when we run out of completely free blocks.  We should probably also permit instantiating separate &lt;tt&gt;BufferPool&lt;/tt&gt;, so that we can insulate internode messaging from the &lt;tt&gt;ChunkCache&lt;/tt&gt;, or at least have separate memory bounds for each, and only share fully-freed chunks.&lt;br/&gt;
With these improvements we can also safely increase the &lt;tt&gt;BufferPool&lt;/tt&gt; chunk size to 128KiB or 256KiB, to guarantee we can fit compressed pages and reduce the amount of global coordination and per-allocation overhead.  We don&#8217;t need 1KiB granularity for allocations, nor 16 byte granularity for tiny allocations.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Since &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-5863&quot; title=&quot;In process (uncompressed) page cache&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-5863&quot;&gt;&lt;del&gt;CASSANDRA-5863&lt;/del&gt;&lt;/a&gt;, chunk cache is implemented to use buffer pool. When local pool is full, one of its chunks will be evicted and only put back to global pool when all buffers in the evicted chunk are released. But due to chunk cache, buffers can be held for long period of time, preventing evicted chunk to be recycled even though most of space in the evicted chunk are free.&lt;/p&gt;

&lt;p&gt;There two things need to be improved:&lt;br/&gt;
1. Evicted chunk with free space should be recycled to global pool, even if it&apos;s not fully free. It&apos;s doable in 4.0.&lt;br/&gt;
2. Reduce fragmentation caused by different buffer size. With #1, partially freed chunk will be available for allocation, but &quot;holes&quot; in the partially freed chunk are with different sizes. We should consider allocating fixed buffer size which is unlikely to fit in 4.0.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13245177">CASSANDRA-15229</key>
            <summary>Segregate Network and Chunk Cache BufferPools and Recirculate Partially Freed Chunks</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jasonstack">Zhao Yang</assignee>
                                    <reporter username="benedict">Benedict Elliott Smith</reporter>
                        <labels>
                    </labels>
                <created>Tue, 16 Jul 2019 10:07:20 +0000</created>
                <updated>Thu, 20 Jul 2023 17:37:01 +0000</updated>
                            <resolved>Thu, 15 Oct 2020 15:17:42 +0000</resolved>
                                        <fixVersion>4.0-beta3</fixVersion>
                    <fixVersion>4.0</fixVersion>
                                    <component>Local/Caching</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>15</watches>
                                                                                                                <comments>
                            <comment id="16968687" author="benedict" created="Wed, 6 Nov 2019 20:36:26 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-15358&quot; title=&quot;LARGE_MESSAGE connection allocates heap buffer when BufferPool exhausted&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-15358&quot;&gt;&lt;del&gt;CASSANDRA-15358&lt;/del&gt;&lt;/a&gt; highlights that we should also have different arenas for memory, or at least distinct limits even if the underlying memory pool is shared.  It should never be the case that &lt;tt&gt;ChunkCache&lt;/tt&gt; can prevent network or other operations from getting memory from the pool&lt;/p&gt;</comment>
                            <comment id="17062682" author="jasonstack" created="Thu, 19 Mar 2020 15:31:07 +0000"  >&lt;p&gt;It looks like current &lt;tt&gt;normal&lt;/tt&gt; chunk size is already 128kb with 2kb allocation granularity and 32byte tiny allocation granularity.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt; do you plan to work on this ticket after &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-15358&quot; title=&quot;LARGE_MESSAGE connection allocates heap buffer when BufferPool exhausted&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-15358&quot;&gt;&lt;del&gt;CASSANDRA-15358&lt;/del&gt;&lt;/a&gt;? if not, I can take a stab at this ticket - seems fun and challenging..&lt;/p&gt;</comment>
                            <comment id="17062689" author="beobal" created="Thu, 19 Mar 2020 15:36:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;If we address the problem, we should also utilise the BufferPool for native transport connections like we do for internode messaging, and reduce the number of pooling solutions we employ.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Just FYI, I&apos;m looking at this aspect of the ticket for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-15299&quot; title=&quot;CASSANDRA-13304 follow-up: improve checksumming and compression in protocol v5-beta&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-15299&quot;&gt;&lt;del&gt;CASSANDRA-15299&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17065411" author="jasonstack" created="Tue, 24 Mar 2020 08:04:29 +0000"  >&lt;p&gt;I am planning to implement the multiple buffer pool approach on top of &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-15358&quot; title=&quot;LARGE_MESSAGE connection allocates heap buffer when BufferPool exhausted&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-15358&quot;&gt;&lt;del&gt;CASSANDRA-15358&lt;/del&gt;&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Permanent Pool - used by chunk cache, hints and other disk access - bounded by file_cache_size_in_mb&lt;/li&gt;
	&lt;li&gt;Ephemeral Pool - used by internode connection, client-server native transport - bounded by network_cache_size_in_mb, default min(128mb, system_memory/8)&lt;/li&gt;
&lt;/ul&gt;


&lt;blockquote&gt;&lt;p&gt;With these improvements we can also safely increase the BufferPool chunk size to 128KiB or 256KiB, to guarantee we can fit compressed pages and reduce the amount of global coordination and per-allocation overhead. We don&#8217;t need 1KiB granularity for allocations, nor 16 byte granularity for tiny allocations.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am not sure if it&apos;s still necessary to increase chunk-size and min-allocation size, given they are already 128kb and 2kb..&lt;/p&gt;</comment>
                            <comment id="17065472" author="benedict" created="Tue, 24 Mar 2020 09:57:07 +0000"  >&lt;p&gt;I won&apos;t be looking at this for some time, so you&apos;re welcome to have a go.&lt;/p&gt;

&lt;p&gt;fwiw, this approach is fine but not sufficient.  The underlying implementation needs to be updated to ensure it is not wasting memory in the &quot;permanent&quot; pool, as currently it will be dramatically worse than just allocating and freeing system memory for the permanent pool.  Perhaps we should in fact consider just allocating system memory for the permanent pool.&lt;/p&gt;

&lt;p&gt;wrt naming, transient and permanent both have the same etymology, so sound more related, even if ephemeral and permanent are technically equally good as antonyms.&lt;/p&gt;</comment>
                            <comment id="17066543" author="jasonstack" created="Wed, 25 Mar 2020 09:17:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;&#160;The underlying implementation needs to be updated to ensure it is not wasting memory in the &quot;permanent&quot; pool, as currently it will be dramatically worse than just allocating and freeing system memory for the permanent pool. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good point. If the hit rate in permanent pool is low because pool is full most of the time, then it makes sense to just allocate system direct memory and avoid the overhead of accessing buffer pool.. I will run some multi-node benchmark to find out.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;transient and permanent both have the same etymology, so sound more related,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am open to change the naming.. Didn&apos;t pick &quot;transient&quot; in the first place because it is used as java key word and also used by transient replicas..&lt;/p&gt;</comment>
                            <comment id="17072895" author="stefania" created="Wed, 1 Apr 2020 15:53:42 +0000"  >&lt;p&gt;We hit this buffer pool regression problem in our DSE fork a while ago. Because our chunk cache became much larger when it replaced the OS page cache, off-heap memory was growing significantly beyond the limits configured. This was partly due to some leaks, but the fragmentation in the current design of the buffer pool was a big part of it.&lt;/p&gt;

&lt;p&gt;This is how we solved it:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;a bump-the-pointer slab approach for the transient pool, not to dissimilar from the current implementation. We then exploit our thread per core architecture: core threads get a dedicated slab each, other threads share a global slab.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;a bitmap-based slab approach for the permanent pool, which is only used by the chunk cache. These slabs can only issue buffers of the same size, one bit is flipped in the bitmap for each buffer issued. When multiple buffers are requested, the slab tries to issue consecutive addresses but this is not guaranteed since we want to avoid memory fragmentation. We have global lists of these slabs, sorted by buffer size where each size is a power-of-two. Slabs are taken out of these lists when they are full, and they are put back into circulation when they have space available. The lists are global but core threads get a thread-local stash of buffers, i.e. they request multiple buffers at the same time in order to reduce contention on the global lists.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;We changed the chunk cache to always store buffers of the same size. If we need to read chunks of a different size, we use an array of buffers in the cache and we request multiple buffers at the same time. If we get consecutive addresses, we optimize for this case by building a single byte buffer over the first address. We also optimized the chunk cache to store memory addresses rather than byte buffers, which significantly reduced heap usage. The byte buffers are materialized on the fly.&lt;/p&gt;

&lt;p&gt;For the permanent case, we made the choice of constraining the size of the buffers in the cache so that memory in the pool could be fully used. This may or may not be what people prefer. Our choice was due to the large size of the cache, 20+ GB. An approach that allows some memory fragmentation may be sufficient for smaller cache sizes.&lt;/p&gt;

&lt;p&gt;Please let me know if there is interest in porting this solution to 4.0 or 4.x. I can share the code if needed.&lt;/p&gt;
</comment>
                            <comment id="17074656" author="benedict" created="Fri, 3 Apr 2020 15:23:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;a bump-the-pointer slab approach for the transient pool, not to dissimilar from the current implementation. We then exploit our thread per core architecture: core threads get a dedicated slab each, other threads share a global slab.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The current implementation isn&apos;t really a bump the pointer allocator?  It&apos;s bitmap based, though with a very tiny bitmap.  Could you elaborate on how these work, as my intuition is that anything designed for a thread-per-core architecture probably won&apos;t translate so well to the present state of the world.  Though, either way, I suppose this is probably orthogonal to this ticket as we only need to address the &lt;tt&gt;ChunkCache&lt;/tt&gt; part.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We also optimized the chunk cache to store memory addresses rather than byte buffers, which significantly reduced heap usage. The byte buffers are materialized on the fly.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This would be a huge improvement, and a welcome backport if it is easy - though it might (I would guess) depend on &lt;tt&gt;Unsafe&lt;/tt&gt;, which may be going away soon.  It&apos;s orthogonal to this ticket, though, I think.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We changed the chunk cache to always store buffers of the same size.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;We have global lists of these slabs, sorted by buffer size where each size is a power-of-two.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;How do these two statements reconcile?&lt;/p&gt;

&lt;p&gt;Is it your opinion that your entire &lt;tt&gt;ChunkCache&lt;/tt&gt; implementation can be dropped wholesale into 4.0?  I would assume it is still primarily multi-threaded.  If so, it might be preferable to trying to fix the existing &lt;tt&gt;ChunkCache&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="17076347" author="stefania" created="Mon, 6 Apr 2020 13:40:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;The current implementation isn&apos;t really a bump the pointer allocator? It&apos;s bitmap based, though with a very tiny bitmap. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sorry it&apos;s been a while. Of course the current implementation is also bitmap based. The point is that it is not suitable for long lived buffers, similarly to our bump the pointer strategy. The transient case is easy to solve, either approach would work.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Could you elaborate on how these work, as my intuition is that anything designed for a thread-per-core architecture probably won&apos;t translate so well to the present state of the world. Though, either way, I suppose this is probably orthogonal to this ticket as we only need to address the &lt;tt&gt;ChunkCache&lt;/tt&gt; part.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The thread-per-core architecture makes it easy to identify threads that do most of the work and cause most of the contention. However, thread identification can be achieved also with thread pools or we can simply give all threads a local stash of buffers, provided that we return it when the thread dies. I don&apos;t think there is any other dependency on TPC beyond this. &lt;/p&gt;

&lt;p&gt;The design choice was mostly dictated by the size of the cache: with AIO reads the OS page cache is bypassed, and the chunk cache needs therefore to be very large, which is not the case if we use Java NIO reads or if we eventually implement asynchronous reads with the new uring API, bypassing AIO completely (which I do recommend). &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We also optimized the chunk cache to store memory addresses rather than byte buffers, which significantly reduced heap usage. The byte buffers are materialized on the fly.&lt;/p&gt;&lt;/blockquote&gt;

&lt;blockquote&gt;&lt;p&gt;This would be a huge improvement, and a welcome backport if it is easy - though it might (I would guess) depend on Unsafe, which may be going away soon. It&apos;s orthogonal to this ticket, though, I think&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes it&apos;s based on the Unsafe. The addresses come from the slabs, and then we use the Unsafe to create hollow buffers and to set the address. This is an optimization and it clearly belongs to a separate ticket.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;    We changed the chunk cache to always store buffers of the same size.&lt;/p&gt;

&lt;p&gt;    We have global lists of these slabs, sorted by buffer size where each size is a power-of-two.&lt;/p&gt;

&lt;p&gt;How do these two statements reconcile?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;So let&apos;s assume the current workload is mostly on a table with 4k chunks, which translate to 4k buffers in the cache. Let&apos;s also assume that the workload is shifting towards another table, with 8k chunks. Alternatively, let&apos;s assume compression is ON, and an ALTER TABLE changes the chunk size. So now the chunk cache is slowly evicting 4k buffers and retaining 8k buffers. These buffers come from two different lists: the list of slabs serving 4k and the list serving 8k. Even if we collect all unused 4k slabs, until each slab has every single buffer returned, there will be wasted memory and we do not control how long that will take. To be fair, it&apos;s an extreme case, and we were perhaps over cautions in addressing this possibility by fixing the size of buffers in the cache. So it&apos;s possible that the redesigned buffer pool may work even with the current chunk cache implementation. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Is it your opinion that your entire ChunkCache implementation can be dropped wholesale into 4.0? I would assume it is still primarily multi-threaded. If so, it might be preferable to trying to fix the existing ChunkCache&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The changes to the chunk cache are not trivial and should be left as a follow up for 4.x or later in my opinion. &lt;/p&gt;

&lt;p&gt;The changes to the buffer pool can be dropped in 4.0 if you think that:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;they are safe even in the presence of the case described above.&lt;/li&gt;
	&lt;li&gt;they are justified: memory wasted due to fragmentation is perhaps not an issue with a cache as little as 512 MB&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I&apos;ll try to share some code so you can have a clearer picture. &lt;/p&gt;</comment>
                            <comment id="17076712" author="benedict" created="Mon, 6 Apr 2020 22:05:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;memory wasted due to fragmentation is perhaps not an issue with a cache as little as 512 MB&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;My view is that having a significant proportion of memory wasted to fragmentation is a serious bug, irregardless of the total amount of memory that is wasted.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The point is that it is not suitable for long lived buffers, similarly to our bump the pointer strategy.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s not poorly suited to long lived buffers its it?  Only to buffers with widely divergent lifetimes.  If the lifetimes are loosely correlated then the length of the lifetime is mostly irrelevant I think.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The changes to the buffer pool can be dropped in 4.0 if you think that&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If you mean introducing a new pool specifically for &lt;tt&gt;ChunkCache&lt;/tt&gt;. I&apos;m fine with it as an alternative to permitting &lt;tt&gt;BufferPool&lt;/tt&gt; to mitigate worst case behaviour for the &lt;tt&gt;ChunkCache&lt;/tt&gt;.  But verifying a replacement for &lt;tt&gt;BufferPool&lt;/tt&gt; is a lot more work, and we use the &lt;tt&gt;BufferPool&lt;/tt&gt; extensively in networking now, which requires non-uniform buffer sizes.&lt;/p&gt;

&lt;p&gt;Honestly, given chunks are normally the same size, simply re-using the evicted buffer if possible, and if not allocating new system memory, seems probably sufficient to me.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;ll try to share some code so you can have a clearer picture.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks, that sounds great.  I may not get to it immediately, but look forward to taking a look hopefully soon.&lt;/p&gt;</comment>
                            <comment id="17077539" author="stefania" created="Tue, 7 Apr 2020 20:16:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;My view is that having a significant proportion of memory wasted to fragmentation is a serious bug, irregardless of the total amount of memory that is wasted.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;That&apos;s absolutely true. However, it&apos;s also true that none of our users reported any problems when the cache was 512 MB and the default file access mode was mmap. Perhaps there are users in open source that reported problems, I haven&apos;t done a Jira search. So my point was simply meant to say that we should be mindful of changing critical code late in a release cycle if the existing code is performing adequately.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;It&apos;s not poorly suited to long lived buffers its it? Only to buffers with widely divergent lifetimes.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I implied the fact that lifetimes are divergent, since we&apos;re trying to support a cache, sorry about the confusion.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Honestly, given chunks are normally the same size, simply re-using the evicted buffer if possible, and if not allocating new system memory, seems probably sufficient to me.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;m not too sure that chunks are normally the same size. For data files, they depend on the compression parameters or on the partition sizes, both could be different for different tables. Also, indexes would use different chunk sizes surely? We observed that the chunk cache gradually tends to shift from buffers coming from data files to buffers coming from index files, as indexes are accessed more frequently. We have a different index implementation though.&lt;/p&gt;

&lt;p&gt;&#160;&lt;br/&gt;
&lt;blockquote&gt;&lt;/blockquote&gt;I&apos;ll try to share some code so you can have a clearer picture.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Thanks, that sounds great. I may not get to it immediately, but look forward to taking a look hopefully soon.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;ve dropped some files on this &lt;a href=&quot;https://github.com/stef1927/cassandra/tree/15229-4.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;. The buffer pool is in org.apache.cassandra.utils.memory.buffers.&#160; The starting point is the &lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...stef1927:15229-4.0#diff-72046b5d367f6e120594b58c973bed71R24&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;BufferPool&lt;/a&gt; and its concrete implementations or the &lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...stef1927:15229-4.0#diff-4fc5fae1de112fc5eb0bd865af532f0aR31&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;BufferFactory&lt;/a&gt;. I&apos;ve also dropped some related utility classes but not all of them, so clearly the code doesn&apos;t compile and the unit tests are also missing.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17077609" author="benedict" created="Tue, 7 Apr 2020 22:18:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;That&apos;s absolutely true. However, it&apos;s also true that none of our users reported any problems when the cache was 512 MB...&#160; So my point was simply meant to say that we should be mindful of changing critical code late in a release cycle if the existing code is performing adequately.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;There has been at least one report, I&apos;m fairly sure, but it is besides the point IMO. &#160;I personally don&apos;t endorse this adequacy criterion: we&apos;ve done a lot of inadequate things users haven&apos;t noticed.&lt;/p&gt;

&lt;p&gt;As also mentioned in other fora (perhaps not this ticket), the situation will be much worse in 4.0, since the default page size has shrunk and the &lt;tt&gt;BufferPool&lt;/tt&gt; page size has grown. &#160;These changes are very likely to lead to a minority of assigned memory being usable, and a majority being served from system memory. &#160;In which case we might as well just do that for everything. &#160;This is a simple solution with similar performance characteristics and no wasted memory. &#160;Given the cache fronts decompression the advantage of an internal memory pool here was anyway surely always fairly minimal? &#160;Was it ever demonstrated to be beneficial?&lt;/p&gt;

&lt;p&gt;An alternative is perhaps to remove the&#160;&lt;tt&gt;ChunkCache&lt;/tt&gt; entirely, or at least deprecate it, or mark it &quot;experimental&quot;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Also, indexes would use different chunk sizes surely?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Perhaps, but should we be caching indexes at all? &#160;I thought the purpose of the chunk cache was for compressed data that has been uncompressed, and I&apos;m fairly sure we don&apos;t compress indexes? &#160;Since it is probably of limited value for data that is already uncompressed, versus the file system cache. &#160;But I have not looked closely at any of the &lt;tt&gt;ChunkCache;&lt;/tt&gt;&#160;only close enough &#160;to notice the problematic behaviour mentioned in this ticket.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I&apos;ve dropped some files on this &lt;a href=&quot;https://github.com/stef1927/cassandra/tree/15229-4.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks, that&apos;s greatly appreciated, though I won&apos;t be able to take a look for a couple of weeks at least unfortunately.&lt;/p&gt;</comment>
                            <comment id="17080314" author="jasonstack" created="Fri, 10 Apr 2020 07:46:51 +0000"  >&lt;p&gt;Discussed with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=stefania&quot; class=&quot;user-hover&quot; rel=&quot;stefania&quot;&gt;stefania&lt;/a&gt; offline, there are two issues with buffer pool:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Chunk cache holds a piece of buffer preventing entire chunk from recycling for arbitrary period.&lt;/li&gt;
	&lt;li&gt;Even if we recirculate the partially freed chunk, due to different allocation sizes, fragmentation will reduce utilization. That&apos;s why forked version uses uniform allocation size.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The first issue should be solvable and less risky for 4.0.. Here is the performance comparison against recirculating partially freed chunk.&lt;/p&gt;

&lt;p&gt;Setup: single node 16T - 8GB heap - 250m rows - mixed read 40k qps - write 10k qps - with 128 file cache&lt;br/&gt;
 &lt;a href=&quot;https://github.com/jasonstack/cassandra/pull/8&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;baseline&lt;/a&gt;: initiate 2 buffer pools, one for chunk cache, one for network.&lt;br/&gt;
 &lt;a href=&quot;https://github.com/jasonstack/cassandra/pull/11/files&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;recirculate-partially-freed-chunk&lt;/a&gt;: baseline + partially freed chunk recirculation.&lt;/p&gt;

&lt;p&gt;baseline:&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12999514/12999514_15229-hit-rate.png&quot; height=&quot;400&quot; width=&quot;400&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12999513/12999513_15229-count.png&quot; height=&quot;400&quot; width=&quot;400&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;recirculation: &lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12999516/12999516_15229-recirculate-hit-rate.png&quot; height=&quot;400&quot; width=&quot;400&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12999515/12999515_15229-recirculate-count.png&quot; height=&quot;400&quot; width=&quot;400&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;QPS:&#160; &lt;br/&gt;
&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12999519/12999519_15229-recirculate.png&quot; height=&quot;600&quot; width=&quot;600&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;With partially freed chunk recirculation, latency is improved and buffer pool misses are reduced..&lt;/p&gt;

&lt;p&gt;Should we proceed with recirculating partially freed chunk + a separate pool for network cache in 4.0 and then port forked buffer pool with uniform allocation size in 4.x?&lt;/p&gt;</comment>
                            <comment id="17080534" author="benedict" created="Fri, 10 Apr 2020 14:38:10 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jasonstack&quot; class=&quot;user-hover&quot; rel=&quot;jasonstack&quot;&gt;jasonstack&lt;/a&gt;, I&apos;m on leave at the moment, so I cannot properly review your patch.  This was in the vicinity of what I had originally been considering, however I see one potential problem with this specific instantiation of the approach, and wonder anyway if we shouldn&apos;t take a different tack:&lt;/p&gt;

&lt;p&gt;Recirculating immediately will lead to greater inefficiency in allocation, as we will attempt to reuse partially freed chunks in preference to entirely freed chunks, leading to a great deal more churn in the active blocks. This will affect the networking pooling as much as the chunk cache.  At the very least this behaviour should be enabled only for the &lt;tt&gt;ChunkCache&lt;/tt&gt;, but ideally might have e.g. two queues, one with guaranteed-free chunks, another (perhaps for ease a superset) containing those chunks that might or mightn&apos;t be free.&lt;/p&gt;

&lt;p&gt;This also isn&apos;t a &lt;em&gt;trivial&lt;/em&gt; behavioural change, and I continue to wonder if using &lt;tt&gt;Unsafe.allocateMemory&lt;/tt&gt; wouldn&apos;t be simpler, more efficient, less risky and produce less fragmentation.&lt;/p&gt;</comment>
                            <comment id="17082081" author="jasonstack" created="Mon, 13 Apr 2020 06:02:38 +0000"  >&lt;blockquote&gt;
&lt;p&gt;Recirculating immediately will lead to greater inefficiency in allocation, as we will attempt to reuse partially freed chunks in preference to entirely freed chunks, leading to a great deal more churn in the active blocks. This will affect the networking pooling as much as the chunk cache.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In networking, most of the time, buffer will be release immediately after allocation and  with &lt;tt&gt;recycleWhenFree=false&lt;/tt&gt;, fully freed chunk will be reused instead of being recycled to global list. Partial-recycle is unlikely affect networking usage. I am happy to test it..&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt; At the very least this behaviour should be enabled only for the ChunkCache, but ideally might have e.g. two queues, one with guaranteed-free chunks, another (perhaps for ease a superset) containing those chunks that might or mightn&apos;t be free.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s a good idea to have a separate queue and let partially freed chunk to have lower priority than fully freed chunk. So partially freed chunks will likely have larger freed space comparing to reusing them immediately.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;if using Unsafe.allocateMemory wouldn&apos;t be simpler, more efficient, less risky and produce less fragmentation.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It is simpler, but not efficient.. Without slab allocation, will it create fragmentation in system direct memory? &lt;/p&gt;

&lt;p&gt;I tested with &quot;Bytebuffer#allocateDirect&quot; and &quot;Unsafe#allocateMemory&quot;, both latencies are slightly worse than baseline. &lt;/p&gt;

&lt;p&gt;btw, I think it&apos;d be nice to add a new metrics to track direct bytebuffer allocation outside of buffer pool because they may be held by chunk cache for a long time.&lt;/p&gt;

&lt;p&gt;Chunk cache with &lt;a href=&quot;https://github.com/jasonstack/cassandra/commit/c3f286c1148d13f00364872413733822a4a2c475&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Bytebuffer.allocateDirect&lt;/a&gt;:&lt;br/&gt;
 &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12999741/12999741_15229-direct.png&quot; height=&quot;400&quot; width=&quot;600&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Chunk cache with &lt;a href=&quot;https://github.com/jasonstack/cassandra/commit/3dadd884ff0d8e19d3dd46a07a290762755df312&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Unsafe.allocateMemory&lt;/a&gt;:&lt;br/&gt;
 &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12999742/12999742_15229-unsafe.png&quot; height=&quot;400&quot; width=&quot;600&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;</comment>
                            <comment id="17082202" author="benedict" created="Mon, 13 Apr 2020 10:04:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;In networking, most of the time, buffer will be release immediately after allocation and with recycleWhenFree=false, fully freed chunk will be reused instead of being recycled to global list. Partial-recycle is unlikely affect networking usage. I am happy to test it..&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It is famously difficult to prove a negative, particularly via external testing.  It will be untrue in some circumstances, most notably large message processing (which happens asynchronously).  I would need to review the buffer control flow in messaging to confirm it is sufficiently low risk to modify the behaviour here, so I would prefer we not modify it in a way that is not easily verified.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;will it create fragmentation in system direct memory?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;&lt;del&gt;Not easily completely ruled out, but given this data will be allocated mostly in its own virtual page space (given all allocations are much larger than a normal page), it hopefully shouldn&apos;t be an insurmountable problem for most allocators given the availability of almost unlimited virtual page space on modern systems.&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;edit: while this may be true, it&apos;s a bit of a stretch as I haven&apos;t looked at any modern allocator remotely recently, and I should not extrapolate in this way (however it&apos;s anyway probably not something to worry about if we&apos;re allocating relatively regular sizes)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I tested with &quot;Bytebuffer#allocateDirect&quot; and &quot;Unsafe#allocateMemory&quot;, both latencies are slightly worse than baseline.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Did you perform the simple optimisation of rounding up to the &amp;gt;= 2KiB boundary (for equivalent behaviour), then re-using any buffer that is correctly sized when evicting to make room for a new item?  It might well be possible to make this yet more efficient than &lt;tt&gt;BufferPool&lt;/tt&gt; by reducing this boundary to e.g. 1KiB, or perhaps as little as 512B.&lt;/p&gt;

&lt;p&gt;So if I were doing this myself, I think I would be starting at this point and if necessary would move towards further reusing the buffers we already have in the cache - since it is already a pool of them.  I would just be looking to smooth out the random distribution of sizes used with e.g. a handful of queues each containing a single size of buffer and at most a handful of items each.  This feels like a simpler solution to me, particularly as it does not affect any other pool users.&lt;/p&gt;

&lt;p&gt;However, I&#8217;m not doing the work (nor maybe reviewing it), so if you are willing to at least enable the behaviour only for the ChunkCache so this change cannot have any unintended negative effect for those users not expected to benefit, my main concern will be alleviated.&lt;/p&gt;</comment>
                            <comment id="17101139" author="jmckenzie" created="Wed, 6 May 2020 20:10:56 +0000"  >&lt;p&gt;If it&apos;s not too much bother, could we update the Since Version on this and add a little detail in the description as to the impact of this regression on the system?&lt;/p&gt;</comment>
                            <comment id="17104629" author="jasonstack" created="Mon, 11 May 2020 16:06:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;I would just be looking to smooth out the random distribution of sizes used with e.g. a handful of queues each containing a single size of buffer and at most a handful of items each.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It looks simpler in its initial form, but I am wondering whether it will eventually grow/evolve into another buffer pool.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;so if you are willing to at least enable the behaviour only for the ChunkCache so this change cannot have any unintended negative effect for those users not expected to benefit, my main concern will be alleviated.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;+1, partially freed chunk recirculation is only enabled for permanent pool, not for temporary pool.&lt;/p&gt;
&lt;hr /&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/apache/cassandra/pull/535/files&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Patch&lt;/a&gt; / &lt;a href=&quot;https://circleci.com/workflow-run/096afbe1-ec99-4d5f-bdaa-06f538b8280f&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Circle&lt;/a&gt;:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Initialize 2 buffer pool instances, one for chunk cache (default 512mb) called &lt;tt&gt;&quot;Permanent Pool&quot;&lt;/tt&gt;, one for network (default 128mb) called &lt;tt&gt;&quot;Temporary Pool&quot;&lt;/tt&gt;. So they won&apos;t interfere each other.&lt;/li&gt;
	&lt;li&gt;Improve buffer pool metrics to track:
	&lt;ul&gt;
		&lt;li&gt;&lt;tt&gt;&quot;overflowSize&quot;&lt;/tt&gt; - buffer size that is allocated outside of buffer pool.&lt;/li&gt;
		&lt;li&gt;&lt;tt&gt;&quot;UsedSize&quot;&lt;/tt&gt; - buffer size that is currently being allocated.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Allow partially freed chunk to be recycled in Permanent Pool to improve cache utilization due to chunk cache holding buffer for arbitrary time period. Note that due to various allocation sizes, fragmentation still exists in partially freed chunk.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17163598" author="iamaleksey" created="Thu, 23 Jul 2020 13:52:53 +0000"  >&lt;p&gt;Would be great to have a second reviewer here (I volunteer to be one of the two).&lt;/p&gt;</comment>
                            <comment id="17163602" author="benedict" created="Thu, 23 Jul 2020 13:55:42 +0000"  >&lt;p&gt;I can, but probably not for a few weeks yet.&lt;/p&gt;</comment>
                            <comment id="17204787" author="iamaleksey" created="Wed, 30 Sep 2020 14:43:38 +0000"  >&lt;p&gt;Thanks for fixing the test issues in the past couple commits (and sorry for the delay in review).&lt;/p&gt;

&lt;p&gt;One thing I&apos;m not a fan of is names of the two pools - permanent and temporary - as neither describe their respective pools. Something along the lines of &apos;long lived&apos; and &apos;short-lived&apos; would work better. Or, perhaps, name them after their use cases - &apos;chunk-cache&apos; and &apos;networking&apos; pools.&lt;/p&gt;

&lt;p&gt;Other than that:&lt;/p&gt;

&lt;p&gt;1. &lt;tt&gt;PermanentBufferPool&lt;/tt&gt; - unused class&lt;br/&gt;
2. &lt;tt&gt;Chunk#fullyRecycled&lt;/tt&gt; is never read, only written to&lt;br/&gt;
3. &lt;tt&gt;putUnusedPortion()&lt;/tt&gt; probably shouldn&#8217;t update overflow metric, as this will double-count some of the size when it&#8217;s &lt;tt&gt;put()&lt;/tt&gt; back&lt;br/&gt;
4. nit: &lt;tt&gt;else if&lt;/tt&gt; on L807 doesn&#8217;t need a pair of braces for the first two conditions&lt;/p&gt;</comment>
                            <comment id="17204807" author="maedhroz" created="Wed, 30 Sep 2020 15:24:13 +0000"  >&lt;p&gt;I&apos;m still in the middle of my review, but just want to +1 something like &lt;tt&gt;chunkCache&lt;/tt&gt; and &lt;tt&gt;networking&lt;/tt&gt; for the names of the pools.&lt;/p&gt;</comment>
                            <comment id="17204853" author="benedict" created="Wed, 30 Sep 2020 16:16:59 +0000"  >&lt;p&gt;Ditto&lt;/p&gt;</comment>
                            <comment id="17205042" author="maedhroz" created="Wed, 30 Sep 2020 21:36:43 +0000"  >&lt;p&gt;Finished my review, and dropped my comment inline in the PR.&lt;/p&gt;

&lt;p&gt;Looking at the larger picture of this issue, &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-16036&quot; title=&quot;Add flag to disable chunk cache and disable by default&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-16036&quot;&gt;&lt;del&gt;CASSANDRA-16036&lt;/del&gt;&lt;/a&gt;, and our general goal of making sure 4.0 does not have any egregious performance issues around the chunk cache, it seems like the most pressing thing in front of us is making sure compaction doesn&apos;t absolutely trash the chunk cache. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dcapwell&quot; class=&quot;user-hover&quot; rel=&quot;dcapwell&quot;&gt;dcapwell&lt;/a&gt; seems to have &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-16036?focusedCommentId=17173291&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-17173291&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;documented this&lt;/a&gt; pretty clearly, and both tests done in that issue and Fallout tests done by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jasonstack&quot; class=&quot;user-hover&quot; rel=&quot;jasonstack&quot;&gt;jasonstack&lt;/a&gt; would be capable of verifying the effectiveness of any changes around this. The question is whether we should work through that here or as part of another issue.&lt;/p&gt;

&lt;p&gt;I don&apos;t know the compaction code super well, but it seems like we could avoid most of the cache churn mess by having the &lt;tt&gt;ISSTableScanner&lt;/tt&gt; implementations returned by &lt;tt&gt;SSTableReader#getScanner()&lt;/tt&gt; use file handles that don&apos;t use &lt;tt&gt;CachingRebufferer&lt;/tt&gt;. &lt;tt&gt;FileHandle.Builder#complete()&lt;/tt&gt; already seems to roughly have the logic we would need to produce the correct (uncached) &lt;tt&gt;RebuffererFactory&lt;/tt&gt;. If it&apos;s that simple, and we&apos;ve already got a performance testing scaffolding set up here, perhaps it would make sense to roll into this Jira...&lt;/p&gt;</comment>
                            <comment id="17205054" author="dcapwell" created="Wed, 30 Sep 2020 21:59:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;The question is whether we should work through that here or as part of another issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I am 100% fine tackling that in a different issue.  Looking at the first line in the description &quot;The BufferPool was never intended to be used for a ChunkCache, and we need to either change our behaviour to handle uncorrelated lifetimes or use something else&quot;, the separate pools does isolate the issue so networking isn&apos;t impacted by the chunk cache; this gives breathing room to figure scope of chunk cache work.&lt;/p&gt;</comment>
                            <comment id="17205072" author="maedhroz" created="Wed, 30 Sep 2020 22:28:26 +0000"  >&lt;p&gt;I&apos;m okay with another issue as well, but the issue title here is pretty broad &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;(Also looking for feedback on the last paragraph of my last comment...)&lt;/p&gt;</comment>
                            <comment id="17208085" author="iamaleksey" created="Mon, 5 Oct 2020 13:38:47 +0000"  >&lt;p&gt;LGTM with most recent feedback addressed.&lt;/p&gt;</comment>
                            <comment id="17208267" author="maedhroz" created="Mon, 5 Oct 2020 19:11:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jasonstack&quot; class=&quot;user-hover&quot; rel=&quot;jasonstack&quot;&gt;jasonstack&lt;/a&gt; The only thing left to resolve seems like the discussion &lt;a href=&quot;https://github.com/apache/cassandra/pull/535#discussion_r497796381&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;. Otherwise, LGTM&lt;/p&gt;</comment>
                            <comment id="17214770" author="jasonstack" created="Thu, 15 Oct 2020 15:17:42 +0000"  >&lt;p&gt;thanks for the review and feedback, merged to &lt;a href=&quot;https://github.com/apache/cassandra/commit/699a1f74fcc1da1952da6b2b0309c9e2474c67f4&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;trunk&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13321440">CASSANDRA-16036</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13330455">CASSANDRA-16158</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13527812">CASSANDRA-18313</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12999513" name="15229-count.png" size="176275" author="jasonstack" created="Fri, 10 Apr 2020 07:10:26 +0000"/>
                            <attachment id="12999741" name="15229-direct.png" size="340530" author="jasonstack" created="Mon, 13 Apr 2020 05:40:35 +0000"/>
                            <attachment id="12999514" name="15229-hit-rate.png" size="208960" author="jasonstack" created="Fri, 10 Apr 2020 07:10:27 +0000"/>
                            <attachment id="12999515" name="15229-recirculate-count.png" size="166190" author="jasonstack" created="Fri, 10 Apr 2020 07:10:26 +0000"/>
                            <attachment id="12999516" name="15229-recirculate-hit-rate.png" size="190912" author="jasonstack" created="Fri, 10 Apr 2020 07:10:27 +0000"/>
                            <attachment id="12999517" name="15229-recirculate-size.png" size="138881" author="jasonstack" created="Fri, 10 Apr 2020 07:10:27 +0000"/>
                            <attachment id="12999519" name="15229-recirculate.png" size="456175" author="jasonstack" created="Fri, 10 Apr 2020 07:11:47 +0000"/>
                            <attachment id="12999518" name="15229-size.png" size="132622" author="jasonstack" created="Fri, 10 Apr 2020 07:10:27 +0000"/>
                            <attachment id="12999742" name="15229-unsafe.png" size="323711" author="jasonstack" created="Mon, 13 Apr 2020 05:40:35 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[jasonstack]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12313825" key="com.atlassian.jira.plugin.system.customfieldtypes:cascadingselect">
                        <customfieldname>Bug Category</customfieldname>
                        <customfieldvalues>
                                                    <customfieldvalue key="12984" cascade-level=""><![CDATA[Degradation]]></customfieldvalue>
                                <customfieldvalue key="12997" cascade-level="1"><![CDATA[Performance Bug/Regression]]></customfieldvalue>
            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12313821" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Complexity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12966"><![CDATA[Challenging]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313822" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Discovered By</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12975"><![CDATA[Code Inspection]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12313922" key="jira.plugin.projectspecificselectfield.jpssf:multicftype">
                        <customfieldname>Impacts</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="13100"><![CDATA[None]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 4 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12313921" key="jira.plugin.projectspecificselectfield.jpssf:multicftype">
                        <customfieldname>Platform</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="13076"><![CDATA[All]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z04pnk:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[aleksey]]></customfieldvalue>
        <customfieldvalue><![CDATA[maedhroz]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311420" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12335055">3.6</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313924" key="com.atlassian.jira.plugin.system.customfieldtypes:textfield">
                        <customfieldname>Source Control Link</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;p&gt;&lt;a href=&quot;https://github.com/apache/cassandra/commit/699a1f74fcc1da1952da6b2b0309c9e2474c67f4&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/cassandra/commit/699a1f74fcc1da1952da6b2b0309c9e2474c67f4&lt;/a&gt;&lt;/p&gt;</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313823" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Test and Documentation Plan</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;p&gt;added unit test and tested performance.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://app.circleci.com/pipelines/github/jasonstack/cassandra/319/workflows/1a931613-d2d0-402e-b21b-058f4b8614c3&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://app.circleci.com/pipelines/github/jasonstack/cassandra/319/workflows/1a931613-d2d0-402e-b21b-058f4b8614c3&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;CI running: &lt;a href=&quot;https://ci-cassandra.apache.org/job/Cassandra-devbranch/83/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://ci-cassandra.apache.org/job/Cassandra-devbranch/83/&lt;/a&gt;&lt;/p&gt;</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>