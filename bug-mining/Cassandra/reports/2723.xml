<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:43:58 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-6948] After Bootstrap or Replace node startup, EXPIRING_MAP_REAPER is shutdown and cannot be restarted, causing callbacks to collect indefinitely</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-6948</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;Since ExpiringMap.shutdown() shuts down the static executor service, it cannot be restarted (and in fact reset() makes no attempt to do so). As such callbacks that receive no response are never removed from the map, and eventually either than server will run out of memory or will loop around the integer space and start reusing messageids that have not been expired, causing assertions to be thrown and messages to fail to be sent. It appears that this situation only arises on bootstrap or node replacement, as MessagingService is shutdown before being attached to the listen address.&lt;/p&gt;

&lt;p&gt;This can cause the following errors to begin occurring in the log:&lt;/p&gt;

&lt;p&gt;ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;Native-Transport-Requests:7636&amp;#93;&lt;/span&gt; 2014-03-28 13:32:10,638 ErrorMessage.java (line 222) Unexpected exception during request&lt;br/&gt;
java.lang.AssertionError: Callback already exists for id -1665979622! (CallbackInfo(target=/10.106.160.84, callback=org.apache.cassandra.service.WriteResponseHandler@5d36d8ea, serializer=org.apache.cassandra.db.WriteResponse$WriteResponseSerializer@6ed37f0b))&lt;br/&gt;
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:549)&lt;br/&gt;
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:601)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy.mutateCounter(StorageProxy.java:984)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:449)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:524)&lt;br/&gt;
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:521)&lt;br/&gt;
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:505)&lt;br/&gt;
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:188)&lt;br/&gt;
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:358)&lt;br/&gt;
	at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:131)&lt;br/&gt;
	at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:304)&lt;br/&gt;
	at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)&lt;br/&gt;
	at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:744)&lt;br/&gt;
ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicateOnWriteStage:102766&amp;#93;&lt;/span&gt; 2014-03-28 13:32:10,638 CassandraDaemon.java (line 196) Exception in thread Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;ReplicateOnWriteStage:102766,5,main&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.AssertionError: Callback already exists for id -1665979620! (CallbackInfo(target=/10.106.160.84, callback=org.apache.cassandra.service.WriteResponseHandler@3bdb1a75, serializer=org.apache.cassandra.db.WriteResponse$WriteResponseSerializer@6ed37f0b))&lt;br/&gt;
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:549)&lt;br/&gt;
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:601)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:806)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy$8$1.runMayThrow(StorageProxy.java:1074)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1896)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:744)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12704247">CASSANDRA-6948</key>
            <summary>After Bootstrap or Replace node startup, EXPIRING_MAP_REAPER is shutdown and cannot be restarted, causing callbacks to collect indefinitely</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="brandon.williams">Brandon Williams</assignee>
                                    <reporter username="keithwrightbos">Keith Wright</reporter>
                        <labels>
                    </labels>
                <created>Fri, 28 Mar 2014 13:39:52 +0000</created>
                <updated>Tue, 16 Apr 2019 09:31:50 +0000</updated>
                            <resolved>Mon, 7 Apr 2014 20:17:25 +0000</resolved>
                                        <fixVersion>2.0.7</fixVersion>
                    <fixVersion>2.1 beta2</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="13950715" author="keithwrightbos" created="Fri, 28 Mar 2014 13:46:59 +0000"  >&lt;p&gt;Another stack where its not a mutate counter:&lt;/p&gt;

&lt;p&gt;java.lang.AssertionError: Callback already exists for id -1665977341! (CallbackInfo(target=/10.106.160.84, callback=org.apache.cassandra.service.WriteResponseHandler@5a74c5ff, serializer=org.apache.cassandra.db.WriteResponse$WriteResponseSerializer@6ed37f0b))&lt;br/&gt;
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:549)&lt;br/&gt;
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:601)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:806)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy$2.apply(StorageProxy.java:118)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:682)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:454)&lt;br/&gt;
	at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:524)&lt;br/&gt;
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:521)&lt;br/&gt;
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:505)&lt;br/&gt;
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:188)&lt;br/&gt;
	at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:358)&lt;br/&gt;
	at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:131)&lt;br/&gt;
	at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:304)&lt;br/&gt;
	at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)&lt;br/&gt;
	at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:744)&lt;/p&gt;</comment>
                            <comment id="13950721" author="elreydetodo@gmail.com" created="Fri, 28 Mar 2014 13:51:28 +0000"  >&lt;p&gt;I&apos;ve seen this exception in my own logs, and it seems to somehow degrade the performance of the node (their writes/sec drops precipitously after the event starts compared to others in the cluster). Restarting seems to make them behave normally again.&lt;/p&gt;

&lt;p&gt;Since this seems to be a result of an update attempt, this begs the question of whether my data is likely to have been written or not. Any idea if it is, or if they are being lost? Is there any way of knowing what exactly that callback is doing? I don&apos;t think it&apos;s a callback I have attached, so I assume it&apos;s something internal to Cassandra..?&lt;/p&gt;</comment>
                            <comment id="13950744" author="benedict" created="Fri, 28 Mar 2014 14:06:05 +0000"  >&lt;p&gt;Could you attach a jstack of the machine throwing the exception, and a copy of your cassandra.yaml?&lt;/p&gt;
</comment>
                            <comment id="13950780" author="keithwrightbos" created="Fri, 28 Mar 2014 14:30:39 +0000"  >&lt;p&gt;Cassandra.yaml attached.  We restarted the nodes where this was occurring so I don&apos;t have a good JStack at this time however I will attach it when it re-occurs.&lt;/p&gt;</comment>
                            <comment id="13950782" author="benedict" created="Fri, 28 Mar 2014 14:32:13 +0000"  >&lt;p&gt;Do you have a full log history since the previous restart at least?&lt;/p&gt;</comment>
                            <comment id="13950798" author="keithwrightbos" created="Fri, 28 Mar 2014 14:45:55 +0000"  >&lt;p&gt;Attached for one of the nodes showing the issue. &lt;/p&gt;</comment>
                            <comment id="13950807" author="benedict" created="Fri, 28 Mar 2014 14:50:37 +0000"  >&lt;p&gt;These don&apos;t seem to go back to node startup? They both start with the exception, I would like to see log info from before this happened, as this is almost certainly a delayed manifestation of an earlier (potentially &lt;b&gt;much&lt;/b&gt; earlier) problem&lt;/p&gt;</comment>
                            <comment id="13950854" author="benedict" created="Fri, 28 Mar 2014 15:12:18 +0000"  >&lt;p&gt;Also, could you confirm you haven&apos;t modified any of the RPC timeouts at any point using JMX?&lt;/p&gt;</comment>
                            <comment id="13950859" author="keithwrightbos" created="Fri, 28 Mar 2014 15:14:02 +0000"  >&lt;p&gt;Confirmed we are running OOTB with no JMX changes whatsoever&lt;/p&gt;</comment>
                            <comment id="13950870" author="benedict" created="Fri, 28 Mar 2014 15:16:49 +0000"  >&lt;p&gt;Thanks. Could you give some indication of the amount if time this box had been up for before exhibiting this, and the number of requests it would have processed (just ballpark order of magnitude)?&lt;/p&gt;</comment>
                            <comment id="13950883" author="keithwrightbos" created="Fri, 28 Mar 2014 15:25:20 +0000"  >&lt;p&gt;All recent logs attached&lt;/p&gt;</comment>
                            <comment id="13950892" author="keithwrightbos" created="Fri, 28 Mar 2014 15:29:43 +0000"  >&lt;p&gt;Its been up and running for ~2 weeks.  It started showing the issue again ~3 hours ago.  I have attached a graph showing write requests for the last week as compared to another server.  You can see that they were lower for this server until ~3 days ago when they recovered on their own (we did not restart the node but we did restart clients that day which likely caused the recovery).&lt;/p&gt;

&lt;p&gt;I have also attached read/write request volume.  Note we&apos;re using RF 3 for all keyspaces.&lt;/p&gt;</comment>
                            <comment id="13950897" author="benedict" created="Fri, 28 Mar 2014 15:35:06 +0000"  >&lt;p&gt;Thanks. Regrettably those logs are all still well after the problems began occurring (they appear to all be from today!), which has caused the logs to get completely spammed. Do you have any earlier logs, or have they been trashed by log recycling?&lt;/p&gt;</comment>
                            <comment id="13950924" author="keithwrightbos" created="Fri, 28 Mar 2014 15:50:33 +0000"  >&lt;p&gt;This issue actually just started occurring on another node (010).  When captured the thread dump in the cassandra log file attached prior restarting the node.&lt;/p&gt;

&lt;p&gt;Have also attached older logs from the previous node (008)&lt;/p&gt;</comment>
                            <comment id="13950929" author="benedict" created="Fri, 28 Mar 2014 15:51:40 +0000"  >&lt;p&gt;Attaching a patch that should increase the amount of debugging information available to us, and also ensures the ExpiringMap shutdown code works (on the off-chance this is the cause)&lt;/p&gt;</comment>
                            <comment id="13950936" author="keithwrightbos" created="Fri, 28 Mar 2014 15:56:23 +0000"  >&lt;p&gt;One other thing to note is that this cluster is now our production cluster.  We just migrated from a 1.2 cluster where we were having issues.  As part of this migration, our client code was instantiating two instances of cluster from the datastax client to allow for writing to go to both clusters.  Could that be a possible cause?  We are going to restart all of our nodes as we are no longer writing to both clusters.&lt;/p&gt;

&lt;p&gt;Because this is now our production cluster, I am reluctant to apply patches.&lt;/p&gt;</comment>
                            <comment id="13950954" author="benedict" created="Fri, 28 Mar 2014 16:14:06 +0000"  >&lt;p&gt;What does the cassandra.log contain? Seems to be a binary file...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;As part of this migration, our client code was instantiating two instances of cluster from the datastax client to allow for writing to go to both clusters&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This shouldn&apos;t have been an issue. Were the clusters aware of each other (i.e. multiple datacenters in same cluster) or were they completely distinct clusters?&lt;/p&gt;</comment>
                            <comment id="13950958" author="keithwrightbos" created="Fri, 28 Mar 2014 16:17:11 +0000"  >&lt;p&gt;Cassandra.log contains logging output from 010 node that just showed the issue including thread dump prior to restart.  Let me know if you need me to upload it again.&lt;/p&gt;

&lt;p&gt;These were completely distinct clusters.  FWIW, I have also noticed that applying DDLs on nodes doesn&apos;t appear to always get applied to all nodes.  I am planning on filing a ticket for that as soon as I can reproduce in a testing environment.  Workaround is to just apply the same DDL on the nodes where the change did not appear.  I&apos;ve often found 008 (the same node showing the assertion error to the highest degree) to be the DDL culprit.&lt;/p&gt;</comment>
                            <comment id="13950983" author="benedict" created="Fri, 28 Mar 2014 16:31:11 +0000"  >&lt;blockquote&gt;&lt;p&gt;Cassandra.log contains logging output&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It seems to be a binary file. Could you confirm the contents, and repackage and reupload?&lt;/p&gt;

&lt;p&gt;The old log files, btw, also still don&apos;t go back to node restart: the first log file has them with lower frequency, but still exhibits these exceptions.&lt;/p&gt;</comment>
                            <comment id="13950995" author="keithwrightbos" created="Fri, 28 Mar 2014 16:38:10 +0000"  >&lt;p&gt;OK here it is again gzip compressed.&lt;/p&gt;</comment>
                            <comment id="13951000" author="benedict" created="Fri, 28 Mar 2014 16:41:32 +0000"  >&lt;p&gt;The file is still a 300MB binary file, not a textual log file. Could you confirm the source file you&apos;re uploading is what you think it is?&lt;/p&gt;</comment>
                            <comment id="13951012" author="keithwrightbos" created="Fri, 28 Mar 2014 16:49:38 +0000"  >&lt;p&gt;Worked fine for me after I gunzip.  If there still doesn&apos;t work, I can try extracting the relevant stacktraces.&lt;/p&gt;

&lt;p&gt;One other point, we bootstrapped a new node into the cluster ~5 days ago and so far that node has never shown this issue.  In addition, it appears that restarting the nodes causes the issue to stop.  So perhaps its related to just the total number of write operations?&lt;/p&gt;</comment>
                            <comment id="13951022" author="benedict" created="Fri, 28 Mar 2014 16:53:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;So perhaps its related to just the total number of write operations?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The exceptions can only plausibly manifest after 4B write operations, however the &lt;em&gt;cause&lt;/em&gt; of the exceptions will be occurring earlier, and probably resulting in increased heap utilisation and a steady burdening of the server.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Worked fine for me after I gunzip. If there still doesn&apos;t work, I can try extracting the relevant stacktraces.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Weird. It&apos;s possible gunzip isn&apos;t behaving, but I consistently get a 300MB nonsense file (again, I&apos;m assuming it&apos;s meant to be a text file and not a heap dump - but jhat didn&apos;t like it either). If you &lt;em&gt;could&lt;/em&gt; extract the relevant stack traces that would be great. Also, those earlier log files from one of these servers would be great. It would be very helpful to get a full history from startup to first exception.&lt;/p&gt;</comment>
                            <comment id="13951044" author="keithwrightbos" created="Fri, 28 Mar 2014 16:59:32 +0000"  >&lt;p&gt;Ah so its possible its the 4B since restart.  I did a bulk load of data into this new cluster from hadoop directly via CQL.  Was running at 150k writes/sec across the cluster.  Given RF 3 that would be 450K/sec across 11 nodes for ~2 days =&amp;gt; ~7 billion writes.  From tpstats of a node:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;kwright@lxpcasdal011 ~&amp;#93;&lt;/span&gt;$ nodetool tpstats&lt;br/&gt;
Pool Name                    Active   Pending      Completed   Blocked  All time blocked&lt;br/&gt;
ReadStage                         3         4     1398160265         0                 0&lt;br/&gt;
RequestResponseStage              0         0     9085977126         0                 0&lt;br/&gt;
MutationStage                     0         0    12080278784         0                 0&lt;br/&gt;
ReadRepairStage                   0         0      125960131         0                 0&lt;br/&gt;
ReplicateOnWriteStage             0         0      439434930         0                 0&lt;br/&gt;
GossipStage                       0         0        4119112         0                 0&lt;br/&gt;
AntiEntropyStage                  0         0              0         0                 0&lt;br/&gt;
MigrationStage                    0         0            449         0                 0&lt;br/&gt;
MemoryMeter                       0         0            588         0                 0&lt;br/&gt;
MemtablePostFlusher               0         0         150778         0                 0&lt;br/&gt;
FlushWriter                       0         0         127833         0                86&lt;br/&gt;
MiscStage                         0         0              0         0                 0&lt;br/&gt;
PendingRangeCalculator            0         0             23         0                 0&lt;br/&gt;
commitlog_archiver                0         0              0         0                 0&lt;br/&gt;
InternalResponseStage             0         0            259         0                 0&lt;br/&gt;
HintedHandoff                     2         4            839         0                 0&lt;/p&gt;


&lt;p&gt;Unfortunately I don&apos;t have any older logs than what I provided, sorry.  I&apos;ll try to extract the stacktrace info for you and attach&lt;/p&gt;</comment>
                            <comment id="13951051" author="keithwrightbos" created="Fri, 28 Mar 2014 17:03:54 +0000"  >&lt;p&gt;This is a non-gzip version of cassandra.log from node where issue was occurring and we did a thread dump.&lt;/p&gt;</comment>
                            <comment id="13951060" author="benedict" created="Fri, 28 Mar 2014 17:14:27 +0000"  >&lt;p&gt;OK, I think I have a plausible explanation: Was this the first time the nodes in the new cluster had been started? i.e. have they ever exhibited the issue after being restarted?&lt;/p&gt;</comment>
                            <comment id="13951080" author="keithwrightbos" created="Fri, 28 Mar 2014 17:23:35 +0000"  >&lt;p&gt;Yes I believe it is true that this is the first time the nodes have ever been restarted.&lt;/p&gt;</comment>
                            <comment id="13951087" author="benedict" created="Fri, 28 Mar 2014 17:25:32 +0000"  >&lt;p&gt;OK. Well good news is it probably won&apos;t bite you again until you start a new cluster or bootstrap a new node into the cluster, and restarting a newly bootstrapped node immediately after bootstrap should fix it.&lt;/p&gt;

&lt;p&gt;Note that replacing a node can trigger the same issue.&lt;/p&gt;

&lt;p&gt;I will update the description with details and provide a patch.&lt;/p&gt;</comment>
                            <comment id="13951103" author="keithwrightbos" created="Fri, 28 Mar 2014 17:36:48 +0000"  >&lt;p&gt;Great, now that all nodes have been restarted, I&apos;ll make sure to let you know if I see the issue again.  In fact, I&apos;m planning on bootstrapping some more nodes next week so I&apos;ll also validate that restart after bootstrap ensures the issue does not occur.  Any chance this is somehow related to the issue where I&apos;m not seeing DDLs apply across all nodes even though they&apos;re shown as successful?&lt;/p&gt;</comment>
                            <comment id="13951109" author="benedict" created="Fri, 28 Mar 2014 17:40:43 +0000"  >&lt;blockquote&gt;&lt;p&gt;Any chance this is somehow related to the issue where I&apos;m not seeing DDLs apply across all nodes even though they&apos;re shown as successful?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s quite possible, yes, although cannot say for certain. If you don&apos;t see any further DDL problems it&apos;s probably down to this, but otherwise please file a separate issue.&lt;/p&gt;</comment>
                            <comment id="13951195" author="benedict" created="Fri, 28 Mar 2014 18:36:21 +0000"  >&lt;p&gt;After reading &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-3335&quot; title=&quot;ThreadPoolExecutor creates threads as non-daemon and will block on shutdown by default&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-3335&quot;&gt;&lt;del&gt;CASSANDRA-3335&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-3737&quot; title=&quot;Its impossible to removetoken joining down node&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-3737&quot;&gt;&lt;del&gt;CASSANDRA-3737&lt;/del&gt;&lt;/a&gt;, I cannot find a good justification for the shutdown method:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;The executor service itself blocks nothing (it is independent of the callback registration, only cleans up concurrently), and is on a daemon thread, so is completely harmless.&lt;/li&gt;
	&lt;li&gt;The shutdown parameter which ostensibly prevents callbacks from being added both does not work, and would be dangerous if it &lt;em&gt;did&lt;/em&gt; work, as any thread caught before a &lt;em&gt;restart&lt;/em&gt; would block indefinitely, and having a thread restart from that position after a clearing of the map seems problematic to me also; it seems much simpler to just let everyone be, since that&apos;s what is happening already and seems to work fine.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13951482" author="jbellis" created="Fri, 28 Mar 2014 22:05:34 +0000"  >&lt;p&gt;Why is bootstrap shutting down the MS ExpiringMap executor?&lt;/p&gt;</comment>
                            <comment id="13951487" author="benedict" created="Fri, 28 Mar 2014 22:11:42 +0000"  >&lt;p&gt;Good question. &lt;del&gt;I didn&apos;t try to figure out the bootstrap code, as it&apos;s quite unfamiliar, and isn&apos;t necessary for fixing this. It happens in checkForEndpointCollision and in prepareReplacementInfo. I can&apos;t see a good reason for it, but ruling out a good reason would require understanding that area of the codebase a lot better.&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;Was answering the wrong question (why it shutsdown MS). ExpiringMap executor makes no sense to ever shutdown, whatever the bootstrap code does, but happened as a matter of course when MS.shutdown() is called.&lt;/p&gt;</comment>
                            <comment id="13958066" author="brandon.williams" created="Wed, 2 Apr 2014 19:21:09 +0000"  >&lt;p&gt;This makes sense, since replacement and checkForEndpointCollision both spin up, and then shutdown MS and a couple of things, and MS probably never really expected such a thing.  Committed.&lt;/p&gt;</comment>
                            <comment id="13958091" author="jbellis" created="Wed, 2 Apr 2014 19:46:21 +0000"  >&lt;p&gt;Hang on, I think I remember why we shut down EM: we don&apos;t want to be processing callbacks that are going to kick off hint storage or read repair once we&apos;re supposed to be Done.&lt;/p&gt;

&lt;p&gt;(This should be done on Drain, maybe on normal shutdown, not during Bootstrap.  Point is that just ripping out the shutdown code is probably not the right fix.)&lt;/p&gt;</comment>
                            <comment id="13958103" author="benedict" created="Wed, 2 Apr 2014 19:54:07 +0000"  >&lt;p&gt;Note that this wasn&apos;t being achieved before - which was my main justification for saying it was safe. The EM shutdown was only turning off the reaper thread, not preventing items being inserted nor destroying items already present - i.e. it was actually marginally increasing the number of callbacks that were possible to get called.&lt;/p&gt;

&lt;p&gt;What we want to do is clear the EM on MS shutdown, and until registration perhaps throw an error if we try to send a message. The &quot;wait forever&quot; )that was never run but was ostensibly the tack taken before) looks dangerous to me, since MS can restart and leave threads hanging forever. Either way, I don&apos;t think the EM needs to know anything about it, nor have a shutdown method.&lt;/p&gt;</comment>
                            <comment id="13958107" author="brandon.williams" created="Wed, 2 Apr 2014 20:01:12 +0000"  >&lt;p&gt;Ok, reverted for now.  &lt;/p&gt;</comment>
                            <comment id="13958180" author="brandon.williams" created="Wed, 2 Apr 2014 21:12:37 +0000"  >&lt;p&gt;Let&apos;s just not shut MS down until it&apos;s really time to do so.&lt;/p&gt;</comment>
                            <comment id="13961731" author="benedict" created="Mon, 7 Apr 2014 09:08:28 +0000"  >&lt;p&gt;Your patch doesn&apos;t address the fact that ExpiringMap.shutdown() is broken - we should stop the reaper being shutdown (since it will die with the system anyway, is &lt;em&gt;only&lt;/em&gt; helpful, and if we introduce another MS.shutdown() somewhere in future it will prevent a regression of this bug) and &lt;em&gt;possibly&lt;/em&gt; actually set the internal shutdown flag, so that the indefinite block on put that was previously intended happens (although since it has never happened before, I&apos;m not sure it won&apos;t cause some negative effect, which is why I didn&apos;t do this in my patch)&lt;/p&gt;

&lt;p&gt;As to fixing this bug for the moment though, LGTM&lt;/p&gt;</comment>
                            <comment id="13962205" author="brandon.williams" created="Mon, 7 Apr 2014 20:17:25 +0000"  >&lt;p&gt;Noted and committed in the spirit of the simplest thing we can do. &lt;/p&gt;</comment>
                            <comment id="13977420" author="rlow" created="Tue, 22 Apr 2014 21:05:36 +0000"  >&lt;p&gt;See discussion on &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6476&quot; title=&quot;Assertion error in MessagingService.addCallback&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6476&quot;&gt;&lt;del&gt;CASSANDRA-6476&lt;/del&gt;&lt;/a&gt;, it applies to 1.2.15 on replace. Can this be reopened and fixed on 1.2?&lt;/p&gt;</comment>
                            <comment id="13977452" author="brandon.williams" created="Tue, 22 Apr 2014 21:19:31 +0000"  >&lt;p&gt;Already have a patch on &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6476&quot; title=&quot;Assertion error in MessagingService.addCallback&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6476&quot;&gt;&lt;del&gt;CASSANDRA-6476&lt;/del&gt;&lt;/a&gt;, let&apos;s handle it there.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12638339" name="6948-v2.txt" size="4503" author="brandon.williams" created="Wed, 2 Apr 2014 21:12:37 +0000"/>
                            <attachment id="12637433" name="6948.debug.txt" size="3617" author="benedict" created="Fri, 28 Mar 2014 15:51:40 +0000"/>
                            <attachment id="12637469" name="6948.txt" size="7251" author="benedict" created="Fri, 28 Mar 2014 18:36:21 +0000"/>
                            <attachment id="12637425" name="Screen Shot 2014-03-28 at 11.27.56 AM.png" size="37777" author="keithwrightbos" created="Fri, 28 Mar 2014 15:29:43 +0000"/>
                            <attachment id="12637426" name="Screen Shot 2014-03-28 at 11.29.24 AM.png" size="32448" author="keithwrightbos" created="Fri, 28 Mar 2014 15:29:43 +0000"/>
                            <attachment id="12637451" name="cassandra.log.min" size="4683725" author="keithwrightbos" created="Fri, 28 Mar 2014 17:03:54 +0000"/>
                            <attachment id="12637407" name="cassandra.yaml" size="32672" author="keithwrightbos" created="Fri, 28 Mar 2014 14:30:39 +0000"/>
                            <attachment id="12637432" name="logs.old.tar.gz" size="8238782" author="keithwrightbos" created="Fri, 28 Mar 2014 15:50:33 +0000"/>
                            <attachment id="12637423" name="logs.tar.gz" size="7828804" author="keithwrightbos" created="Fri, 28 Mar 2014 15:25:20 +0000"/>
                            <attachment id="12637409" name="system.log.1.gz" size="371237" author="keithwrightbos" created="Fri, 28 Mar 2014 14:45:55 +0000"/>
                            <attachment id="12637408" name="system.log.gz" size="155363" author="keithwrightbos" created="Fri, 28 Mar 2014 14:45:55 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>11.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[brandon.williams]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>382581</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 31 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1twu7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>382849</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>benedict</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[benedict]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311420" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12326170">2.0.6</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>