<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:00:48 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-10969] long-running cluster sees bad gossip generation when a node restarts</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-10969</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;One of the nodes in a long-running Cassandra 2.1.1 cluster (not under my control) restarted.  The remaining nodes are logging errors like this:&lt;br/&gt;
    &quot;received an invalid gossip generation for peer xxx.xxx.xxx.xxx; local generation = 1414613355, received generation = 1450978722&quot;&lt;/p&gt;

&lt;p&gt;The gap between the local and received generation numbers exceeds the one-year threshold added for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8113&quot; title=&quot;Gossip should ignore generation numbers too far in the future&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8113&quot;&gt;&lt;del&gt;CASSANDRA-8113&lt;/del&gt;&lt;/a&gt;.  The system clocks are up-to-date for all nodes.&lt;/p&gt;

&lt;p&gt;If this is a bug, the latest released Gossiper.java code in 2.1.x, 2.2.x, and 3.0.x seems not to have changed the behavior that I&apos;m seeing.&lt;/p&gt;

&lt;p&gt;I presume that restarting the remaining nodes will clear up the problem, whence the minor priority.&lt;/p&gt;</description>
                <environment>&lt;p&gt;4-node Cassandra 2.1.1 cluster, each node running on a Linux 2.6.32-431.20.3.dl6.x86_64 VM&lt;/p&gt;</environment>
        <key id="12927350">CASSANDRA-10969</key>
            <summary>long-running cluster sees bad gossip generation when a node restarts</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jkni">Joel Knighton</assignee>
                                    <reporter username="tdh">T. David Hudson</reporter>
                        <labels>
                    </labels>
                <created>Tue, 5 Jan 2016 20:01:29 +0000</created>
                <updated>Tue, 16 Apr 2019 09:30:45 +0000</updated>
                            <resolved>Fri, 22 Jan 2016 14:46:35 +0000</resolved>
                                        <fixVersion>2.1.13</fixVersion>
                    <fixVersion>2.2.5</fixVersion>
                    <fixVersion>3.0.3</fixVersion>
                    <fixVersion>3.3</fixVersion>
                                    <component>Legacy/Coordination</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="15083749" author="jkni" created="Tue, 5 Jan 2016 20:45:37 +0000"  >&lt;p&gt;Your observations on this ticket and your comment on &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8113&quot; title=&quot;Gossip should ignore generation numbers too far in the future&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8113&quot;&gt;&lt;del&gt;CASSANDRA-8113&lt;/del&gt;&lt;/a&gt; are correct; we should handle the possibility of a legitimately long-running cluster properly. In my opinion, the current behavior is a bug, and I&apos;ll work on a fix.&lt;/p&gt;

&lt;p&gt;You are also correct that a rolling restart should fix this because a generation of 0 (as after a restart) is special-cased in the check introduced.&lt;/p&gt;</comment>
                            <comment id="15087314" author="tdh" created="Thu, 7 Jan 2016 12:52:27 +0000"  >&lt;p&gt;A single-pass rolling restart proved insufficient; there&apos;s probably an additional problem with gossip in this area.&lt;/p&gt;

&lt;p&gt;Node 1&apos;s gossip generation had been being rejected by nodes 2, 3, and 4.  N2 was the first to be restarted.  Nodetool status on N2 showed N1 up, at least for a little while (until N3 got restarted?).  Then nodetool status on N2 started reporting N1 down, and in its log, it was rejecting N1&apos;s generation based on an old generation, despite that its system.local had a new generation.  Nodetool gossipinfo on N2 was reporting an old generation for N1.  After N3 and N4 had been restarted, nodetool status commands on N2 and N3 were still reporting N1 down, but N4 was reporting N1 up.  Restarting N1 made no difference.  Restarting N2 and then N3 again was required for the cluster to become fully up.&lt;/p&gt;</comment>
                            <comment id="15094418" author="jkni" created="Tue, 12 Jan 2016 18:11:43 +0000"  >&lt;p&gt;Sorry - I missed your reply.&lt;/p&gt;

&lt;p&gt;I suspect the issue was that on restart, N2 first gossiped with N3 or N4 that contained the old generation. This would have contaminated it and put it in the same state as before.&lt;/p&gt;

&lt;p&gt;If N4 restarted and first gossiped with N1, it would have received the new generation. The odds are then much better for N2 or N3 to gossip with a node with the correct generation on restart.&lt;/p&gt;

&lt;p&gt;It now seems clear that rolling restarts will eventually solve the issue based on which with node gossip first occurs, but a single rolling restart may not be sufficient. My apologies if my initial advice caused any pain.&lt;/p&gt;

&lt;p&gt;The planned patch will remove the need for a rolling restart in the first place, solving the issue. I&apos;m testing it now.&lt;/p&gt;

&lt;p&gt;Thanks for the detailed reports; it makes debugging the issue much easier.&lt;/p&gt;</comment>
                            <comment id="15096477" author="tdh" created="Wed, 13 Jan 2016 16:27:05 +0000"  >&lt;p&gt;If N2 after its first restart hadn&apos;t briefly reported N1 up in its nodetool status, it would&apos;ve been clearer that the behavior was simply that of getting an old generation from other old-generation nodes.  I thought, however, that gossip had to succeed with a peer before nodetool status would report a node up.  So, N2 would&apos;ve had to gossip first with N1, in order for that interaction to succeed.  If so, gossiping with N3 or N4 after that, or perhaps some other event, must&apos;ve put it back in its bad in-memory state.  If so, gossip might have trouble staying current where the failure scenario being addressed by this bug hasn&apos;t occurred.&lt;/p&gt;</comment>
                            <comment id="15096514" author="jkni" created="Wed, 13 Jan 2016 16:48:25 +0000"  >&lt;p&gt;A node shows in nodetool status if its part of a set of &quot;live nodes&quot;. A node will be marked &quot;live&quot; when we first add some of its gossip state locally if it can be reached with an echo message.&lt;/p&gt;

&lt;p&gt;I suspect what happened is that N2 gossiped with N3 or N4, first applied N3 or N4&apos;s (outdated) gossip information about N1, sent an echo message to N1 to check if it was alive, received a reply, and marked N1 alive.&lt;/p&gt;

&lt;p&gt;At this point, N1 will show as up in nodetool status. Then, since no new gossip deltas are applied for N1 (because of the generation gap), the failure detector marked N1 as down for N2.&lt;/p&gt;

&lt;p&gt;I could try to confirm this with N1 and N2&apos;s logs.&lt;/p&gt;</comment>
                            <comment id="15096604" author="jkni" created="Wed, 13 Jan 2016 17:21:28 +0000"  >&lt;p&gt;I&apos;ve pushed a patch that uses the local node&apos;s time instead of the stored generation for the remote node when deciding whether a generation for a remote node has jumped too far ahead. This patch implicitly depends on the fact that generation is initialized by time, but we implicitly use this assumption elsewhere, so we&apos;re no worse off than before the change. Moreover, if the method of generation selection changed, many of the tests would detect this incompatibility.&lt;/p&gt;

&lt;p&gt;It is clear that using the stored generation doesn&apos;t work for long-running clusters. We have no closer approximation for time of the remote node than the local time. Since we already depend on time being reasonably well-synchronized, this seems safe enough to me.&lt;/p&gt;

&lt;p&gt;Ideally, we&apos;d have a better way to prevent this, but I think this issue should be fixed in 2.1/2.2 and this is the only non-intrusive solution that occurs to me.&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;branch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;testall&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;dtest&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/jkni/cassandra/tree/10969-2.1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;10969-2.1&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-10969-2.1-testall&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-10969-2.1-dtest&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/jkni/cassandra/tree/10969-2.2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;10969-2.2&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-10969-2.2-testall&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-10969-2.2-dtest&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/jkni/cassandra/tree/10969-3.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;10969-3.0&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-10969-3.0-testall&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-10969-3.0-dtest&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/jkni/cassandra/tree/10969-3.3&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;10969-3.3&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-10969-3.3-testall&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-10969-3.3-dtest&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/jkni/cassandra/tree/10969-trunk&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;10969-trunk&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-10969-trunk-testall&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/jkni/job/jkni-10969-trunk-dtest&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;EDIT: CI looks as clean as can be expected.&lt;/p&gt;

&lt;p&gt;EDIT 2: Also, I pushed separate branches to better cover CI. The 2.1 branch is its own patch, the 2.2 patch should merge cleanly through 2.2-&amp;gt;3.0-&amp;gt;3.3-&amp;gt;trunk.&lt;/p&gt;</comment>
                            <comment id="15102244" author="adcha" created="Fri, 15 Jan 2016 18:24:33 +0000"  >&lt;p&gt;Hi,&lt;br/&gt;
we are running in this problem, cassandra version 2.1.2, we did a full restart of two DC of 5 nodes each due to power outage, the nodes were running fine for a long time, nodetool status was reporting all nodes UN, but now after the full restart the nodetool report incoerent info about nodes, we are seeing this message in the log &quot; received an invalid gossip generation for peer /x.x.x.x; local generation = 1417171692, received generation = 1452847182&quot;&lt;br/&gt;
we look at local and peers tables but we didn&apos;t found where the local generation is stored.&lt;br/&gt;
do u know how to solve the problem? we are thinking about gossip info but the documentation is not clear for us.&lt;/p&gt;

&lt;p&gt;thanks&lt;/p&gt;</comment>
                            <comment id="15102281" author="jkni" created="Fri, 15 Jan 2016 18:50:31 +0000"  >&lt;p&gt;Yes, this is the situation described in this ticket.&lt;/p&gt;

&lt;p&gt;In 2.1.2, functionality to prevent gossip corruption is incorrectly implemented. This can harm the ability of long-running clusters to function properly after a restart.&lt;/p&gt;

&lt;p&gt;The generation for local nodes is store in memory. When a node starts, it will receive the generations of other nodes through gossip.&lt;/p&gt;

&lt;p&gt;After a restart, this problem can occur when a node (say, Node C) first gossips with a different remote node (say, Node A) that has the old generation for a remote node (say, Node B). Then, Node C can no longer get gossip updates for Node B. If Node C had first gossiped with Node B after the restart, then Node C will be fine and can continue to receive gossip updates from Node B.&lt;/p&gt;

&lt;p&gt;Eventually, rolling restarts of the cluster will solve the issue. It may take several rolling restarts since a node may first gossip with a node with old stored generations, but this will eventually resolve the problem (with increasing probability of success, as fewer nodes in the cluster will have old generations stored).&lt;/p&gt;

&lt;p&gt;If you ran into this problem with a development or local cluster, you could accelerate this process by restarting the whole cluster at once, but this is clearly unacceptable for a production cluster.&lt;/p&gt;</comment>
                            <comment id="15104879" author="stefania" created="Mon, 18 Jan 2016 07:16:24 +0000"  >&lt;p&gt;This LGTM, moving to Ready To Commit.&lt;/p&gt;

&lt;p&gt;Please note comment above re. merging: &lt;em&gt;The 2.1 branch is its own patch, the 2.2 patch should merge cleanly through 2.2-&amp;gt;3.0-&amp;gt;3.3-&amp;gt;trunk.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jkni/cassandra/tree/10969-2.1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/jkni/cassandra/tree/10969-2.1&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://github.com/jkni/cassandra/tree/10969-2.2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/jkni/cassandra/tree/10969-2.2&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15105256" author="adcha" created="Mon, 18 Jan 2016 13:09:27 +0000"  >&lt;p&gt;yes, i confirm the problem is resolved by restarting the whole cluster.&lt;br/&gt;
thanks&lt;/p&gt;</comment>
                            <comment id="15112477" author="slebresne" created="Fri, 22 Jan 2016 14:46:35 +0000"  >&lt;p&gt;Committed, thanks.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[jkni]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 43 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2qth3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>stefania</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[stefania]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311420" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12326774">2.1.1</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>