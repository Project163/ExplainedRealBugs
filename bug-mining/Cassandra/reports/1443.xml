<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:29:13 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-3831] scaling to large clusters in GossipStage impossible due to calculatePendingRanges </title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-3831</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;(most observations below are from 0.8, but I just now tested on&lt;br/&gt;
trunk and I can trigger this problem &lt;b&gt;just&lt;/b&gt; by bootstrapping a ~180&lt;br/&gt;
nod cluster concurrently, presumably due to the number of nodes that&lt;br/&gt;
are simultaneously in bootstrap state)&lt;/p&gt;

&lt;p&gt;It turns out that:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;(1) calculatePendingRanges is not just expensive, it&apos;s computationally complex - cubic or worse&lt;/li&gt;
	&lt;li&gt;(2) it gets called &lt;b&gt;NOT&lt;/b&gt; just once per node being bootstrapped/leaving etc, but is called repeatedly &lt;b&gt;while&lt;/b&gt; nodes are in these states&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;As a result, clusters start exploding when you start reading 100-300&lt;br/&gt;
nodes. The GossipStage will get backed up because a single&lt;br/&gt;
calculdatePenginRanges takes seconds, and depending on what the&lt;br/&gt;
average heartbeat interval is in relation to this, this can lead to&lt;br/&gt;
&lt;b&gt;massive&lt;/b&gt; cluster-wide flapping.&lt;/p&gt;

&lt;p&gt;This all started because we hit this in production; several nodes&lt;br/&gt;
would start flapping several other nodes as down, with many nodes&lt;br/&gt;
seeing the entire cluster, or a large portion of it, as down. Logging&lt;br/&gt;
in to some of these nodes you would see that they would be constantly&lt;br/&gt;
flapping up/down for minutes at a time until one became lucky and it&lt;br/&gt;
stabilized.&lt;/p&gt;

&lt;p&gt;In the end we had to perform an emergency full-cluster restart with&lt;br/&gt;
gossip patched to force-forget certain nodes in bootstrapping state.&lt;/p&gt;

&lt;p&gt;I can&apos;t go into all details here from the post-mortem (just the&lt;br/&gt;
write-up would take a day), but in short:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;We graphed the number of hosts in the cluster that had more than 5&lt;br/&gt;
  Down (in a cluster that should have 0 down) on a minutely timeline.&lt;/li&gt;
	&lt;li&gt;We also graphed the number of hosts in the cluster that had GossipStage backed up.&lt;/li&gt;
	&lt;li&gt;The two graphs correlated &lt;b&gt;extremely&lt;/b&gt; well&lt;/li&gt;
	&lt;li&gt;jstack sampling showed it being CPU bound doing mostly sorting under calculatePendingRanges&lt;/li&gt;
	&lt;li&gt;We were never able to exactly reproduce it with normal RING_DELAY and gossip intervals, even on a 184 node cluster (the production cluster is around 180).&lt;/li&gt;
	&lt;li&gt;Dropping RING_DELAY and in particular dropping gossip interval to 10 ms instead of 1000 ms, we were able to observe all of the behavior we saw in production.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So our steps to reproduce are:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Launch 184 node cluster w/ gossip interval at 10ms and RING_DELAY at 1 second.&lt;/li&gt;
	&lt;li&gt;Do something like: &lt;tt&gt;while [ 1 ] ; do date ; echo decom ; nodetool decommission ; date ; echo done leaving decommed for a while ; sleep 3 ; date ; echo done restarting; sudo rm -rf /data/disk1/commitlog/* ; sudo rm -rf /data/diskarray/tables/* ; sudo monit restart cassandra ;date ; echo restarted waiting for a while ; sleep 40; done&lt;/tt&gt; (or just do a manual decom/bootstrap once, it triggers every time)&lt;/li&gt;
	&lt;li&gt;Watch all nodes flap massively and not recover at all, or maybe after a &lt;b&gt;long&lt;/b&gt; time.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I observed the flapping using a python script that every 5 second&lt;br/&gt;
(randomly spread out) asked for unreachable nodes from &lt;b&gt;all&lt;/b&gt; nodes in&lt;br/&gt;
the cluster, and printed any nodes and their counts when they had&lt;br/&gt;
unreachables &amp;gt; 5. The cluster can be observed instantly going into&lt;br/&gt;
massive flapping when leaving/bootstrap is initiated. Script needs&lt;br/&gt;
Cassandra running with Jolokia enabled for http/json access to&lt;br/&gt;
JMX. Can provide scrit if needed after cleanup.&lt;/p&gt;

&lt;p&gt;The phi conviction, based on logging I added, was legitimate. Using&lt;br/&gt;
the 10 ms interval the average heartbeat interval ends up being like 25&lt;br/&gt;
ms or something like that. As a result, a single ~ 2 second delay in&lt;br/&gt;
gossip stage is huge in comparison to those 25 ms, and so we go past&lt;br/&gt;
the phi conviction threshold. This is much more sensitive than in&lt;br/&gt;
production, but it&apos;s the &lt;b&gt;same&lt;/b&gt; effect, even if it triggers less&lt;br/&gt;
easily for real.&lt;/p&gt;

&lt;p&gt;The best work around currently internally is to memoize&lt;br/&gt;
calculatePendingRanges so that we don&apos;t re-calculate if token meta&lt;br/&gt;
data, list of moving, list of bootstrapping and list of leaving are&lt;br/&gt;
all the same as on prior calculation. It&apos;s not entirely clear at this&lt;br/&gt;
point whether there is a clean fix to avoid executing&lt;br/&gt;
calculatePendingRanges more than once per unique node in this state.&lt;/p&gt;

&lt;p&gt;It should be noted though that even if that is fixed, it is not&lt;br/&gt;
acceptable to spend several seconds doing these calculations on a ~&lt;br/&gt;
200 node cluster and it needs to be made fundamentally more efficient.&lt;/p&gt;

&lt;p&gt;Here is a dump of thoughts by me in an internal JIRA ticket (not&lt;br/&gt;
exhaustive, I just went as far as to show that there is an issue;&lt;br/&gt;
there might be worse things I missed, but worse than cubic is bad&lt;br/&gt;
enough that I stopped):&lt;/p&gt;

&lt;p&gt;(Comment uses 0.8 source.)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Okay, so let&apos;s break down the computational complexity here.&lt;/p&gt;

&lt;p&gt;Suppose ring size is &lt;tt&gt;n&lt;/tt&gt; and number of bootstrapping/leaving tokens is &lt;tt&gt;m&lt;/tt&gt;.  One of two places that take time (by measurement) is this part of calculatePendingRanges():&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;       &lt;span class=&quot;code-comment&quot;&gt;// At &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; stage pendingRanges has been updated according to leave operations. We can
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// now &lt;span class=&quot;code-keyword&quot;&gt;continue&lt;/span&gt; the calculation by checking bootstrapping nodes.
&lt;/span&gt;
        &lt;span class=&quot;code-comment&quot;&gt;// For each of the bootstrapping nodes, simply add and remove them one by one to
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// allLeftMetadata and check in between what their ranges would be.
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (Map.Entry&amp;lt;Token, InetAddress&amp;gt; entry : bootstrapTokens.entrySet())
        {
            InetAddress endpoint = entry.getValue();

            allLeftMetadata.updateNormalToken(entry.getKey(), endpoint);
            &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (Range range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
                pendingRanges.put(range, endpoint);
            allLeftMetadata.removeEndpoint(endpoint);
        }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&apos;ll ignore stuff that&apos;s log&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; or better.&lt;/p&gt;

&lt;p&gt;The outer loops is &lt;tt&gt;O(m)&lt;/tt&gt;. The inner loop is &lt;tt&gt;O&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/tt&gt;, making aggregate so far &lt;tt&gt;O(nm)&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;We have a call in there to updateNormalTokens() which implies a sorting, which his &lt;tt&gt;O(n log&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;)&lt;/tt&gt;. So now we&apos;re at &lt;tt&gt;O(n log&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; m)&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Next up we call &lt;tt&gt;getAddressRanges()&lt;/tt&gt; which immediately does another &lt;tt&gt;O(n log&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/tt&gt; sort. we&apos;re still at &lt;tt&gt;O(n log&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; m&lt;/tt&gt;. It then iterates (linear) and:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;calls &lt;tt&gt;getPrimaryRangeFor()&lt;/tt&gt; for each.&lt;/li&gt;
	&lt;li&gt;calls &lt;tt&gt;calculateNaturalEndpoints&lt;/tt&gt; for each.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The former ends up sorting again, so now we&apos;re at &lt;tt&gt;O(n log&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; n log&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; m&lt;/tt&gt; (worse than quadratic).&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;NTS.calculateNaturalEndpoints&lt;/tt&gt; starts by collecting token meta data for nodes in the DC, by using &lt;tt&gt;updateNormalToken&lt;/tt&gt;, which &lt;b&gt;implies sorting&lt;/b&gt;. Woha woha. Now we&apos;re at &lt;tt&gt;O(n log&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; n log &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; n log&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; m)&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;I might have missed things that are even worse, but this is bad enough to warrant this ticket. To put into perspective, 168 ^ 3 is 4.7 million.&lt;/p&gt;&lt;/blockquote&gt;</description>
                <environment></environment>
        <key id="12540841">CASSANDRA-3831</key>
            <summary>scaling to large clusters in GossipStage impossible due to calculatePendingRanges </summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="scode">Peter Schuller</assignee>
                                    <reporter username="scode">Peter Schuller</reporter>
                        <labels>
                    </labels>
                <created>Wed, 1 Feb 2012 23:44:27 +0000</created>
                <updated>Tue, 16 Apr 2019 09:32:41 +0000</updated>
                            <resolved>Thu, 9 Feb 2012 10:31:55 +0000</resolved>
                                        <fixVersion>1.1.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="13198465" author="scode" created="Thu, 2 Feb 2012 02:08:08 +0000"  >&lt;p&gt;Correction: All above applies, except on &apos;trunk&apos;: I have not confirmed that calculatePendingTokens() indeed gets called repeatedly (not just once per node that starts a bootstrap/leave/etc) on trunk. I have only confirmed it being CPU spinning there, and gossip stage being backed up as a result.&lt;/p&gt;

&lt;p&gt;On 0.8, it is specifically confirmed that it does get called repeatedly both in my test case and when we saw this happen in production.&lt;/p&gt;</comment>
                            <comment id="13200251" author="scode" created="Sat, 4 Feb 2012 01:24:52 +0000"  >&lt;p&gt;I am attaching &lt;tt&gt;CASSANDRA&amp;#45;3831&amp;#45;memoization&amp;#45;not&amp;#45;for&amp;#45;inclusion.txt&lt;/tt&gt; as an &quot;FYI&quot; and in case it helps others. It&apos;s against 0.8, and implements memoization of calculate pending ranges.&lt;/p&gt;

&lt;p&gt;The correct/clean fix is probably to change behavior so that it doesn&apos;t get called unnecessarily to begin with (and to make sure the computational complexity is reasonable when it does get called). This patch was made specifically to address the production issue we are having in a minimally dangerous fashion, and is not to be taken as a suggested fix.&lt;/p&gt;</comment>
                            <comment id="13200464" author="jbellis" created="Sat, 4 Feb 2012 15:12:49 +0000"  >&lt;p&gt;calculatePendingRanges is only supposed to be called when the ring changes.  So I&apos;d say the right fix would be to eliminate whatever is breaking that design, rather than adding a memoization bandaid.&lt;/p&gt;

&lt;p&gt;(I eyeballed 1.1 and didn&apos;t see anything obvious, so either it&apos;s subtle or it got fixed post-0.8.)&lt;/p&gt;

&lt;p&gt;I don&apos;t suppose your CPU spinning test got any more of a call tree to go on?&lt;/p&gt;</comment>
                            <comment id="13200524" author="scode" created="Sat, 4 Feb 2012 19:56:09 +0000"  >&lt;p&gt;I agree (I did say that myself already &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;). The memoization (+ being ready to change the cluster-wide phi convict threshold through JMX) was just the safest way to fix the situation on our production cluster so that we could continue to add capacity. It was never intended as a suggested fix. But I still wanted to upload it instead of keeping the patch private, in case someone&apos;s helped by it.&lt;/p&gt;

&lt;p&gt;But the larger issue is that calculatePendingRanges must be faster to begin with. Even if only called once, if it takes 1-4 seconds on a ~ 180 node cluster and it&apos;s worse than &lt;tt&gt;O(n^3)&lt;/tt&gt; it&apos;s &lt;b&gt;way&lt;/b&gt; too slow and won&apos;t scale. First due to the failure detector, and of course at some point it&apos;s just too slow to even wait for the calculation to complete at all (from a RING_DELAY standpoint for example).&lt;/p&gt;

&lt;p&gt;I&apos;ll see later this weekend about doing more tests on trunk confirm/deny whether it is getting called multiple times. As I indicated I never confirmed that particular bit on trunk and it&apos;s very possible it doesn&apos;t happen there.&lt;/p&gt;

&lt;p&gt;I haven&apos;t had time to seriously look at suggesting changes to fix the computational complexity. Might be very easy for all I know; I just haven&apos;t looked at it yet.&lt;/p&gt;</comment>
                            <comment id="13200967" author="scode" created="Sun, 5 Feb 2012 23:45:30 +0000"  >&lt;p&gt;With respect to trunk and whether it gets called repeatedly, I gave that a try now. I picked a random node to tail (and running a version that logs calculate pending ranges and its time), and did a decommission of another node:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; INFO [GossipStage:1] 2012-02-05 23:33:32,675 StorageService.java (line 1275) calculate pending ranges called, took 885 ms
 INFO [OptionalTasks:1] 2012-02-05 23:34:00,828 HintedHandOffManager.java (line 180) Deleting any stored hints &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; /XXX.XXX.XXX.XXX
 INFO [GossipStage:1] 2012-02-05 23:34:00,828 StorageService.java (line 1275) calculate pending ranges called, took 0 ms
 INFO [GossipStage:1] 2012-02-05 23:34:00,829 StorageService.java (line 1275) calculate pending ranges called, took 0 ms
 INFO [GossipStage:1] 2012-02-05 23:34:00,829 StorageService.java (line 1217) Removing token 23117008622346363007022731483136427400 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; /XXX.XXX.XXX.XXX
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At least two of those are expected - once when it goes into leaving, and once when it drops out of the cluster. Not sure at this point why we see a third call.&lt;/p&gt;

&lt;p&gt;For bootstrapping the guy back into the cluster I see:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; INFO [GossipStage:1] 2012-02-05 23:38:15,832 StorageService.java (line 1275) calculate pending ranges called, took 1413 ms
 INFO [GossipStage:1] 2012-02-05 23:38:45,229 ColumnFamilyStore.java (line 590) Enqueuing flush of Memtable-LocationInfo@659873291(35/43 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-02-05 23:38:45,229 Memtable.java (line 252) Writing Memtable-LocationInfo@659873291(35/43 serialized/live bytes, 1 ops)
 INFO [FlushWriter:4] 2012-02-05 23:38:45,236 Memtable.java (line 293) Completed flushing /data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-20-Data.db (89 bytes)
 INFO [GossipStage:1] 2012-02-05 23:38:45,236 StorageService.java (line 1275) calculate pending ranges called, took 0 ms
 INFO [CompactionExecutor:21] 2012-02-05 23:38:45,237 CompactionTask.java (line 115) Compacting [SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-20-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-19-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-18-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-17-Data.db&apos;&lt;/span&gt;)]
 INFO [CompactionExecutor:21] 2012-02-05 23:38:45,248 CompactionTask.java (line 226) Compacted to [/data/diskarray/tables/system/LocationInfo/system-LocationInfo-hc-21-Data.db,].  7,388 to 7,047 (~95% of original) bytes &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 4 keys at 0.610958MB/s.  Time: 11ms.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So it got called twice - which is expected. Once when it entered in joining state, and once when it flipped into Normal.&lt;/p&gt;

&lt;p&gt;Relatedly:&lt;/p&gt;

&lt;p&gt;On the other hand, on the node that is &lt;b&gt;bootstrapping&lt;/b&gt;, I (and this is expected) have calculate pending ranges calls lots of times - presumably (not confirmed) at least once for every node in the cluster, as the gossiper emits events to inform it of each. If there is a single node bootstrapping this is kind of okay because the lack of &quot;another&quot; guy bootstrapping means the calculations are quick. But if other nodes are bootstrapping too (highly likely if you&apos;re doing capacity adds on large clusters) that would be expected to take a long time to process. This could throw off the node bootstrapping which tries to wait for RING_DELAY on start-up, but is spending a lot of that time doing these calculations rather than staying up-to-date with ring information (for the record though I have not specifically timed/tested this particular case).&lt;/p&gt;</comment>
                            <comment id="13200980" author="scode" created="Mon, 6 Feb 2012 00:36:33 +0000"  >&lt;p&gt;Attaching &lt;tt&gt;CASSANDRA&amp;#45;3831&amp;#45;trunk&amp;#45;group&amp;#45;add&amp;#45;dc&amp;#45;tokens.txt&lt;/tt&gt; which adds a &quot;group-update&quot; interface to TokenMetadata that allows NTS to use it when constructing it&apos;s local per-dc meta datas.&lt;/p&gt;

&lt;p&gt;This is not even close to a complete fix for this issue, but I do think it is a &quot;clean&quot; change because it makes sense in terms of TokenMetadata API to provide a group-update method given the expense involved. And given it&apos;s existence, it makes sense for NTS to use it.&lt;/p&gt;

&lt;p&gt;This change mitigates the problem significantly on the ~ 180 node test cluster since it takes a way an &lt;tt&gt;n&lt;/tt&gt; from the complexity, and should significantly raise the bar of how many nodes in a cluster is realistic without other changes.&lt;/p&gt;

&lt;p&gt;I think this might be a fix worthwhile committing because it feels safe and is maybe a candidate for the 1.1 release, assuming review doesn&apos;t yield anything obvious. But, leaving the JIRA open for a more overarching fix (I&apos;m not sure what that is at the moment; I&apos;m mulling it over).&lt;/p&gt;
</comment>
                            <comment id="13200983" author="scode" created="Mon, 6 Feb 2012 00:56:42 +0000"  >&lt;p&gt;I filed &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-3856&quot; title=&quot;consider using persistent data structures for some things&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-3856&quot;&gt;&lt;del&gt;CASSANDRA-3856&lt;/del&gt;&lt;/a&gt; which might related to a proper fix to this.&lt;/p&gt;</comment>
                            <comment id="13202635" author="jbellis" created="Tue, 7 Feb 2012 19:08:16 +0000"  >&lt;p&gt;committed the group add patch with a minor tweak to move the isempty check to the top so we can skip lock/unlock too, and a more important one from &lt;tt&gt;sortedTokens = sortedTokens()&lt;/tt&gt; (which is a no-op) to &lt;tt&gt;sortedTokens = sortTokens()&lt;/tt&gt;.&lt;/p&gt;</comment>
                            <comment id="13203223" author="scode" created="Wed, 8 Feb 2012 03:29:42 +0000"  >&lt;p&gt;Wow, that&apos;s embarrassing. Thanks for catching that!&lt;/p&gt;</comment>
                            <comment id="13204421" author="slebresne" created="Thu, 9 Feb 2012 10:11:24 +0000"  >&lt;p&gt;Since code has been committed so can we close this one and open a separate ticket for the remaining work for the sake of keeping track of what went into 1.1.0 and was doesn&apos;t?&lt;/p&gt;</comment>
                            <comment id="13204430" author="scode" created="Thu, 9 Feb 2012 10:31:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-3881&quot; title=&quot;reduce computational complexity of processing topology changes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-3881&quot;&gt;&lt;del&gt;CASSANDRA-3881&lt;/del&gt;&lt;/a&gt; filed for further work.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12541352">CASSANDRA-3856</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12541955">CASSANDRA-3881</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12513221" name="CASSANDRA-3831-memoization-not-for-inclusion.txt" size="6098" author="scode" created="Sat, 4 Feb 2012 01:24:52 +0000"/>
                            <attachment id="12513381" name="CASSANDRA-3831-trunk-group-add-dc-tokens.txt" size="4838" author="scode" created="Mon, 6 Feb 2012 00:36:33 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[scode]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>226195</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 years, 41 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0goqf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>95437</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>