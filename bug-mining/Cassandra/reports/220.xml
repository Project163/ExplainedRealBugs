<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:11:58 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-150] multiple seeds (only when seed count = node count?) can cause cluster partition</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-150</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;happens fairly frequently on my test cluster of 5 nodes.  (i normally restart all nodes at once when updating the code.  haven&apos;t tested w/ restarting one machine at a time.)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12424825">CASSANDRA-150</key>
            <summary>multiple seeds (only when seed count = node count?) can cause cluster partition</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10003" iconUrl="https://issues.apache.org/jira/images/icons/priorities/trivial.svg">Low</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jaakko">Jaakko Laine</assignee>
                                    <reporter username="jbellis">Jonathan Ellis</reporter>
                        <labels>
                    </labels>
                <created>Thu, 7 May 2009 16:13:22 +0000</created>
                <updated>Tue, 16 Apr 2019 09:33:38 +0000</updated>
                            <resolved>Thu, 26 Nov 2009 02:16:53 +0000</resolved>
                                        <fixVersion>0.5</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="12706971" author="jbellis" created="Thu, 7 May 2009 16:21:35 +0000"  >&lt;p&gt;daishi&apos;s &quot;Unable to find a live Endpoint we might be out of live nodes&quot;  bug was almost certainly caused by the same thing.  (he has a 3-node cluster.)&lt;/p&gt;

&lt;p&gt;for both of us switching to a single seed has fixed the issue for now.  (but ultimately we do want to support multiple seeds for redundancy.)&lt;/p&gt;</comment>
                            <comment id="12781937" author="jaakko" created="Tue, 24 Nov 2009 13:59:46 +0000"  >&lt;p&gt;Network partition may happen if (1) cluster size is at least four nodes, (2) all nodes are seeds and (3) at least two nodes boot &quot;simultaneously&quot;.&lt;/p&gt;

&lt;p&gt;Gossiping cycle works as follows:&lt;br/&gt;
&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; gossip to random live node&lt;br/&gt;
(ii) gossip to random unreachable node&lt;br/&gt;
(iii) if the node gossiped to at &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/information.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; was not seed, gossip to random seed&lt;/p&gt;

&lt;p&gt;Suppose there are four nodes in the cluster: nodeA, nodeB, nodeC and nodeD, all of them seeds. Suppose they are all brought online at the same time. Following event sequence leads to partition:&lt;/p&gt;

&lt;p&gt;(1) nodeA comes online. No live nodes (and no unreachable either, of course), so gossip to random seed. Let&apos;s suppose nodeA chooses nodeB. It sends nodeB gossip.&lt;br/&gt;
(2) nodeB gets nodeA&apos;s gossip and marks it live. It sends its own gossip, and since it has a live node (nodeA), it sends gossip according to gossip&apos;s first rule. nodeA is seed, so no gossip is sent to random seed at (iii).&lt;br/&gt;
(3) nodeC comes online. It has not seen other live nodes yet, so it will gossip to random seed. Let&apos;s suppose it chooses nodeD.&lt;br/&gt;
(4) nodeD comes online and sees nodeC&apos;s gossip. Since it now has a live node, it will send nodeC gossip according to the first rule. Since nodeC is seed, again no gossip is sent to random seed.&lt;/p&gt;

&lt;p&gt;(there are other sequences as well, but basic idea is the same)&lt;/p&gt;

&lt;p&gt;Now all nodes know of one live node, so they will always send gossip according to the first rule. Since this node is seed, they will never send gossip to random seed according to rule three. This will prevent them from finding rest of the cluster. One non-seed node will break this loop, as gossip sent to it will trigger gossip to random seed.&lt;/p&gt;

&lt;p&gt;While investigating this, I noticed we might have caused some harm to scalability of gossip mechanism when we added two new application states for node movement. I&apos;ll fix this bug tomorrow when checking if there is a problem.&lt;/p&gt;</comment>
                            <comment id="12781952" author="jbellis" created="Tue, 24 Nov 2009 14:32:03 +0000"  >&lt;p&gt;That makes total sense.  Nice work!&lt;/p&gt;</comment>
                            <comment id="12782403" author="jaakko" created="Wed, 25 Nov 2009 12:58:37 +0000"  >&lt;p&gt;Added extra condition to send gossip to random seed also if liveEndpoints.size + unreachableEndpoints.size is less than seeds.size. This will cause us to send same gossip to same seed twice occasionally (when the seed is live, but we have not yet seen enough nodes), but I think this is OK, as this is quite special case and will go away as soon as we&apos;ve seen enough nodes.&lt;/p&gt;

&lt;p&gt;Another option would be to add extra parameter excludeThisNode to sendGossip and not send gossip if random returns that address, but IMHO this option is messy and gains very little.&lt;/p&gt;</comment>
                            <comment id="12782675" author="jbellis" created="Wed, 25 Nov 2009 23:25:05 +0000"  >&lt;p&gt;I don&apos;t think this quite works &amp;#8211; e.g. the guy on the mailing list with 3 seeds in a 4 node cluster.  I think we have to make the check &quot;have we seen all the seeds yet&quot; rather than &quot;have we seen as many nodes as there are seeds&quot;&lt;/p&gt;</comment>
                            <comment id="12782713" author="jaakko" created="Thu, 26 Nov 2009 01:33:26 +0000"  >&lt;p&gt;This kind of partition cannot happen if there are less than four seeds or there is even a single non-seed node. There must be enough seeds to form at least two separate network closures of at least two seeds each. If there has been a problem with 3/4 cluster, it must be different from this as there are two preconditions that are not met.&lt;/p&gt;

&lt;p&gt;Gossip rule #1 sends gossip to a live node and rule #3 sends to a random seed if the node in #1 was not seed. If there is even a single non-seed node, it will trigger gossip to a random seed every time gossip is sent to it. Eventually this will break the network closures. What the patch basically does is it aggressively searches for seeds as long as it has found at least as many nodes as there are seeds. It does not matter even if this does not include all seeds, as that means there are non-seeds in liveEndpoints, which triggers search for random seed every time gossip is sent to it. So basically this is just to help Gossiper to get started, not to find all seeds. Whether it finds all seeds or at least one non-seed does not matter, it can continue from there.&lt;/p&gt;

&lt;p&gt;Now of course the &quot;correct&quot; checks for this condition would be to on each gossip round check (1) whether liveEndpoints and unreachableEndpoints include all seeds or (2) if liveEndpoints includes at least one non-seed. However, putting these checks on the normal execution path only for the sake of one special case does not appeal to me, so decided to add this simple check instead.&lt;/p&gt;

&lt;p&gt;Now that I think of it, there is one extremely special case that still could cause a partition: cluster of 4 seeds and 2 non-seeds. First 2 seeds and 2 non-seeds come online -&amp;gt; everybody is happy as cluster size is the same as number of seeds. Now both seeds go down, and then the other two seeds come up. Again everybody is happy. Now suppose the two non-seeds go down, and after that the two original seeds come up simultaneously, and happen to choose each other from the list of random seeds. In this case all seeds will send gossip only to the other seed, as they have 2 nodes in unreachableEndpoint, which makes the total number of seen nodes equal number of seeds. To avoid this, we might relax the condition a bit and send gossip to a seed if number of liveEndpoints is less than seeds (that is, ignore unreachableEndpoints). This modification would take care of the scenario above, but don&apos;t know if it is worth the trouble. If either of the non-seeds recovers (or one of the seeds goes down), this deadlock will be broken.&lt;/p&gt;</comment>
                            <comment id="12782718" author="jaakko" created="Thu, 26 Nov 2009 01:50:24 +0000"  >&lt;p&gt;It might indeed be better to check only if liveEndpoints.size &amp;lt; seeds.size (do not count unreachableEndpoints). This will cause a bit more unnecessary gossip to seeds in some special cases, but is perhaps better approach. Have to think about this a bit still.&lt;/p&gt;</comment>
                            <comment id="12782726" author="jbellis" created="Thu, 26 Nov 2009 02:12:13 +0000"  >&lt;p&gt;I see, so to make the less-strong check be enough when seeds.size &amp;lt;= live node count we reason that:&lt;/p&gt;

&lt;p&gt;either all the live nodes are seeds, in which case non-seeds that come online will introduce themselves to a member of the ring by definition, and become known in turn,&lt;/p&gt;

&lt;p&gt;or there is at least one non-seed node in the list, in which case eventually someone will gossip to it, and then do a gossip to a random seed from the existing clause in the if statement.&lt;/p&gt;

&lt;p&gt;&amp;gt; It might indeed be better to check only if liveEndpoints.size &amp;lt; seeds.size&lt;/p&gt;

&lt;p&gt;yes, let&apos;s go with this.  better to do a little extra gossiping in corner cases than risk indefinite partitions.&lt;/p&gt;</comment>
                            <comment id="12782727" author="jbellis" created="Thu, 26 Nov 2009 02:16:53 +0000"  >&lt;p&gt;committed as described above&lt;/p&gt;</comment>
                            <comment id="12782875" author="hudson" created="Thu, 26 Nov 2009 12:34:02 +0000"  >&lt;p&gt;Integrated in Cassandra #269 (See &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Cassandra/269/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Cassandra/269/&lt;/a&gt;)&lt;br/&gt;
    send extra gossip to random seed as long as there are less nodes alive than seed nodes configured&lt;br/&gt;
patch by Jaakko Laine; reviewed by jbellis for &lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12426096" name="150.patch" size="1345" author="jaakko" created="Wed, 25 Nov 2009 12:58:37 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[jaakko]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>19569</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            16 years, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0fx6f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>90973</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12961"><![CDATA[Low]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>