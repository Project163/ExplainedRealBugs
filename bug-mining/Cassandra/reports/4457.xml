<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:04:48 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-11363] High Blocked NTR When Connecting</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-11363</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;When upgrading from 2.1.9 to 2.1.13, we are witnessing an issue where the machine load increases to very high levels (&amp;gt; 120 on an 8 core machine) and native transport requests get blocked in tpstats.&lt;/p&gt;

&lt;p&gt;I was able to reproduce this in both CMS and G1GC as well as on JVM 7 and 8.&lt;/p&gt;

&lt;p&gt;The issue does not seem to affect the nodes running 2.1.9.&lt;/p&gt;

&lt;p&gt;The issue seems to coincide with the number of connections OR the number of total requests being processed at a given time (as the latter increases with the former in our system)&lt;/p&gt;

&lt;p&gt;Currently there is between 600 and 800 client connections on each machine and each machine is handling roughly 2000-3000 client requests per second.&lt;/p&gt;

&lt;p&gt;Disabling the binary protocol fixes the issue for this node but isn&apos;t a viable option cluster-wide.&lt;/p&gt;

&lt;p&gt;Here is the output from tpstats:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Pool Name                    Active   Pending      Completed   Blocked  All time blocked
MutationStage                     0         8        8387821         0                 0
ReadStage                         0         0         355860         0                 0
RequestResponseStage              0         7        2532457         0                 0
ReadRepairStage                   0         0            150         0                 0
CounterMutationStage             32       104         897560         0                 0
MiscStage                         0         0              0         0                 0
HintedHandoff                     0         0             65         0                 0
GossipStage                       0         0           2338         0                 0
CacheCleanupExecutor              0         0              0         0                 0
InternalResponseStage             0         0              0         0                 0
CommitLogArchiver                 0         0              0         0                 0
CompactionExecutor                2       190            474         0                 0
ValidationExecutor                0         0              0         0                 0
MigrationStage                    0         0             10         0                 0
AntiEntropyStage                  0         0              0         0                 0
PendingRangeCalculator            0         0            310         0                 0
Sampler                           0         0              0         0                 0
MemtableFlushWriter               1        10             94         0                 0
MemtablePostFlush                 1        34            257         0                 0
MemtableReclaimMemory             0         0             94         0                 0
Native-Transport-Requests       128       156         387957        16            278451

Message type           Dropped
READ                         0
RANGE_SLICE                  0
_TRACE                       0
MUTATION                     0
COUNTER_MUTATION             0
BINARY                       0
REQUEST_RESPONSE             0
PAGED_RANGE                  0
READ_REPAIR                  0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Attached is the jstack output for both CMS and G1GC.&lt;/p&gt;

&lt;p&gt;Flight recordings are here:&lt;br/&gt;
&lt;a href=&quot;https://s3.amazonaws.com/simple-logs/cassandra-102-cms.jfr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3.amazonaws.com/simple-logs/cassandra-102-cms.jfr&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://s3.amazonaws.com/simple-logs/cassandra-102-g1gc.jfr&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3.amazonaws.com/simple-logs/cassandra-102-g1gc.jfr&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It is interesting to note that while the flight recording was taking place, the load on the machine went back to healthy, and when the flight recording finished the load went back to &amp;gt; 100.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12950933">CASSANDRA-11363</key>
            <summary>High Blocked NTR When Connecting</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tjake">T Jake Luciani</assignee>
                                    <reporter username="devdazed">Russell Bradberry</reporter>
                        <labels>
                    </labels>
                <created>Wed, 16 Mar 2016 18:11:49 +0000</created>
                <updated>Tue, 14 Oct 2025 12:14:00 +0000</updated>
                            <resolved>Tue, 20 Sep 2016 02:52:02 +0000</resolved>
                                        <fixVersion>2.1.16</fixVersion>
                    <fixVersion>2.2.8</fixVersion>
                    <fixVersion>3.0.10</fixVersion>
                    <fixVersion>3.10</fixVersion>
                                    <component>Legacy/Coordination</component>
                        <due></due>
                            <votes>6</votes>
                                    <watches>31</watches>
                                                                                                                <comments>
                            <comment id="15197912" author="crolo" created="Wed, 16 Mar 2016 18:45:55 +0000"  >&lt;p&gt;Also reproducible in 3.0.3.&lt;/p&gt;

&lt;p&gt;I have 2 clusters showing exactly the same problem, one running 2.1.13 (5 nodes, 4 cores, 32GB Ram, CMS Java 7) and another 3 nodes, 4cores, 16GB Ram running 3.0.3 (CMS Java 8). &lt;/p&gt;

&lt;p&gt;Both get the problems described: &quot;The issue seems to coincide with the number of connections OR the number of total requests being processed at a given time (as the latter increases with the former in our system)&quot;&lt;/p&gt;

&lt;p&gt;It is normal for OS Load to shoot to 33+ with around 40connected clients.&lt;/p&gt;</comment>
                            <comment id="15199929" author="devdazed" created="Thu, 17 Mar 2016 17:00:23 +0000"  >&lt;p&gt;For troubleshooting we set up a coordinator-only node and pointed one app server at it.  This resulted in roughly 90 connections to the node.  We witnessed many timeouts of requests from the app server&apos;s perspective. We downgraded the coordinator-only node to 2.1.9 and upgraded point-release by point-release (DSE point releases) until we saw the same behavior in DSE 4.8.4 (Cassandra 2.1.12).&lt;/p&gt;

&lt;p&gt;I&apos;m not certain this has to do with connection count anymore. &lt;/p&gt;

&lt;p&gt;We have several different types of workloads going on, but we found that only the workloads that use batches were timing out. Additionally this is only happening when the node is used as a coordinator.  We are not seeing this issue when we disable the binary protocol, effectively making the node no-longer a coordinator.&lt;/p&gt;</comment>
                            <comment id="15199944" author="crolo" created="Thu, 17 Mar 2016 17:09:40 +0000"  >&lt;p&gt;I can confirm that in both my clusters batches are in use.&lt;/p&gt;</comment>
                            <comment id="15230325" author="arodrime" created="Thu, 7 Apr 2016 14:35:39 +0000"  >&lt;p&gt;I also observed in C*2.1.12 that a certain percentage of the Native-Transport-Requests are blocked, yet no major CPU or resources issue on my side though, it might then not be related.&lt;/p&gt;

&lt;p&gt;For what it is worth here is something I observed about Native-Transport-Requests: increasing the &apos;native_transport_max_threads&apos; value help mitigating this, as expected, but Native-Transport-Requests number is still a non zero value.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[alain~]$ knife ssh &quot;role:cassandra&quot; &quot;nodetool tpstats | grep Native-Transport-Requests&quot; | grep -e server1 -e server2 -e server3 -e server4 | sort | awk &apos;BEGIN { printf &quot;%50s %10s&quot;,&quot;Server |&quot;,&quot; Blocked ratio:\n&quot; } { printf &quot;%50s %10f%\n&quot;, $1, (($7/$5)*100) }&apos;
       Server |  Blocked ratio:
       server3   0.044902%
       server4   0.030127%
       server2   0.045759%
       server1   0.082763%
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I waited long enough between the change and the result capture, many days, probably a few weeks. As all the nodes are in the same datacenter, under a (fairly) balanced load, this is probably relevant.&lt;/p&gt;

&lt;p&gt;Here are the result for those nodes, in our use case.&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Server&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;native_transport_max_threads&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Percentage of blocked threads&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;server1&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;128&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0.082763%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;server2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;384&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0.044902%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;server3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;512&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0.045759%&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;server4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1024&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;0.030127%&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;Also from the mailing list outputs, it looks like it is quite common to have some Native-Transport-Requests blocked, probably unavoidable depending on the network and use cases (spiky workloads?). &lt;/p&gt;</comment>
                            <comment id="15230546" author="zznate" created="Thu, 7 Apr 2016 16:51:03 +0000"  >&lt;p&gt;Raised this to critical. &lt;/p&gt;

&lt;p&gt;You have four long-time users with multiple large deployments who are seeing a quantifiable percentage of client errors on non-resource constrained clusters across multiple versions. &lt;/p&gt;</comment>
                            <comment id="15230670" author="cnlwsu" created="Thu, 7 Apr 2016 17:35:19 +0000"  >&lt;p&gt;fixed by &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-10200&quot; title=&quot;NetworkTopologyStrategy.calculateNaturalEndpoints is rather inefficient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-10200&quot;&gt;&lt;del&gt;CASSANDRA-10200&lt;/del&gt;&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="15230924" author="pauloricardomg" created="Thu, 7 Apr 2016 19:50:26 +0000"  >&lt;p&gt;I went through 2.1.12 changes and didn&apos;t find anything suspicious. On 2.1.13 and 3.0.3 though, we changed the &lt;tt&gt;ServerConnection&lt;/tt&gt; query state map from a &lt;tt&gt;NonBlockingHashMap&lt;/tt&gt; to a &lt;tt&gt;ConcurrentHashMap&lt;/tt&gt; on &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-10938&quot; title=&quot;test_bulk_round_trip_blogposts is failing occasionally&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-10938&quot;&gt;&lt;del&gt;CASSANDRA-10938&lt;/del&gt;&lt;/a&gt;, which might be misbehaving for some reason.&lt;/p&gt;

&lt;p&gt;Is anyone willing to try the revert patch below on 2.1.13 or 3.0.3 and check if that changes anything?&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;diff --git a/src/java/org/apache/cassandra/transport/ServerConnection.java b/src/java/org/apache/cassandra/transport/ServerConnection.java
index ce4d164..5991b33 100644
--- a/src/java/org/apache/cassandra/transport/ServerConnection.java
+++ b/src/java/org/apache/cassandra/transport/ServerConnection.java
@@ -17,7 +17,6 @@
  */
 package org.apache.cassandra.transport;
 
-import java.util.concurrent.ConcurrentHashMap;
 import java.util.concurrent.ConcurrentMap;
 
 import io.netty.channel.Channel;
@@ -29,6 +28,8 @@ import org.apache.cassandra.config.DatabaseDescriptor;
 import org.apache.cassandra.service.ClientState;
 import org.apache.cassandra.service.QueryState;
 
+import org.cliffc.high_scale_lib.NonBlockingHashMap;
+
 public class ServerConnection extends Connection
 {
     private enum State { UNINITIALIZED, AUTHENTICATION, READY }
@@ -37,7 +38,7 @@ public class ServerConnection extends Connection
     private final ClientState clientState;
     private volatile State state;
 
-    private final ConcurrentMap&amp;lt;Integer, QueryState&amp;gt; queryStates = new ConcurrentHashMap&amp;lt;&amp;gt;();
+    private final ConcurrentMap&amp;lt;Integer, QueryState&amp;gt; queryStates = new NonBlockingHashMap&amp;lt;&amp;gt;();
 
     public ServerConnection(Channel channel, int version, Connection.Tracker tracker)
     {
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15230926" author="devdazed" created="Thu, 7 Apr 2016 19:52:51 +0000"  >&lt;p&gt;Unfortunately we are on DSE, so I can&apos;t run the revert&lt;/p&gt;</comment>
                            <comment id="15230947" author="zznate" created="Thu, 7 Apr 2016 19:58:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt; unfortunately, this may be been latent for some time. &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-10044&quot; title=&quot;Native-Transport-Requests is missing from the nodetool tpstats output in Cassandra 2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-10044&quot;&gt;&lt;del&gt;CASSANDRA-10044&lt;/del&gt;&lt;/a&gt; re-introduced the tpstats counters as they had been missing (which is mostly likely why this has not been noticed until recently). &lt;/p&gt;</comment>
                            <comment id="15231025" author="pauloricardomg" created="Thu, 7 Apr 2016 20:44:01 +0000"  >&lt;p&gt;that&apos;s right, that was just a wild guess. I focused my quick investigation on 2.1.12 and 2.1.13 commits, but some deeper investigation is probably needed.&lt;/p&gt;

&lt;p&gt;probably it&apos;s a good idea to backport &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-10044&quot; title=&quot;Native-Transport-Requests is missing from the nodetool tpstats output in Cassandra 2.1&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-10044&quot;&gt;&lt;del&gt;CASSANDRA-10044&lt;/del&gt;&lt;/a&gt; to versions &amp;lt; 2.1.12 and setup a coordinator-only node and check where it started happening to narrow down the scope.&lt;/p&gt;</comment>
                            <comment id="15231045" author="devdazed" created="Thu, 7 Apr 2016 20:55:59 +0000"  >&lt;p&gt;we may have two separate issues here, in mine, the issue is 100% CPU utilization and ultra high load when using batches.  According to the jfr all of hot threads are spinning on &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.apache.cassandra.locator.NetworkTopologyStrategy.hasSufficientReplicas(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;, Map, Multimap)
    -&amp;gt; org.apache.cassandra.locator.NetworkTopologyStrategy.hasSufficientReplicas(Map, Multimap)
    -&amp;gt; org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(Token, TokenMetadata)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15231063" author="zznate" created="Thu, 7 Apr 2016 21:06:08 +0000"  >&lt;p&gt;Yeah, that is quite different that what &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=arodrime&quot; class=&quot;user-hover&quot; rel=&quot;arodrime&quot;&gt;arodrime&lt;/a&gt; and I have seen recently. Nodes in our case were otherwise well within utilization thresholds.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devdazed&quot; class=&quot;user-hover&quot; rel=&quot;devdazed&quot;&gt;devdazed&lt;/a&gt; Your issue looks like it would be addressed by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=cnlwsu&quot; class=&quot;user-hover&quot; rel=&quot;cnlwsu&quot;&gt;cnlwsu&lt;/a&gt;&apos;s reference to &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-10200&quot; title=&quot;NetworkTopologyStrategy.calculateNaturalEndpoints is rather inefficient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-10200&quot;&gt;&lt;del&gt;CASSANDRA-10200&lt;/del&gt;&lt;/a&gt; (lean on DSE folks for a patch). &lt;/p&gt;</comment>
                            <comment id="15231066" author="devdazed" created="Thu, 7 Apr 2016 21:07:24 +0000"  >&lt;p&gt;well, it may be possible that this is the same issue, just very much exacerbated by the batches&lt;/p&gt;</comment>
                            <comment id="15231095" author="pauloricardomg" created="Thu, 7 Apr 2016 21:19:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devdazed&quot; class=&quot;user-hover&quot; rel=&quot;devdazed&quot;&gt;devdazed&lt;/a&gt; are you using unlogged batches by any chance?&lt;/p&gt;</comment>
                            <comment id="15231108" author="devdazed" created="Thu, 7 Apr 2016 21:31:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt; yes, we are using unlogged batches that cross partitions&lt;/p&gt;</comment>
                            <comment id="15231113" author="pauloricardomg" created="Thu, 7 Apr 2016 21:35:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devdazed&quot; class=&quot;user-hover&quot; rel=&quot;devdazed&quot;&gt;devdazed&lt;/a&gt; Ok, then it&apos;s very likely that you&apos;re hit by &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11529&quot; title=&quot;Checking if an unlogged batch is local is inefficient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11529&quot;&gt;&lt;del&gt;CASSANDRA-11529&lt;/del&gt;&lt;/a&gt;. I created another ticket in case this one is a different issue.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zznate&quot; class=&quot;user-hover&quot; rel=&quot;zznate&quot;&gt;zznate&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=CRolo&quot; class=&quot;user-hover&quot; rel=&quot;CRolo&quot;&gt;CRolo&lt;/a&gt; can you double check you&apos;re not hitting &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11529&quot; title=&quot;Checking if an unlogged batch is local is inefficient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11529&quot;&gt;&lt;del&gt;CASSANDRA-11529&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="15231115" author="devdazed" created="Thu, 7 Apr 2016 21:37:00 +0000"  >&lt;p&gt;any chance the number of vnodes in a cluster affects how bad this issue is?&lt;/p&gt;</comment>
                            <comment id="15231123" author="pauloricardomg" created="Thu, 7 Apr 2016 21:40:50 +0000"  >&lt;p&gt;yes, cluster with more vnodes will be more affected by that.&lt;/p&gt;</comment>
                            <comment id="15231130" author="devdazed" created="Thu, 7 Apr 2016 21:42:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11529&quot; title=&quot;Checking if an unlogged batch is local is inefficient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11529&quot;&gt;&lt;del&gt;CASSANDRA-11529&lt;/del&gt;&lt;/a&gt; makes sense because &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9303&quot; title=&quot;Match cassandra-loader options in COPY FROM&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9303&quot;&gt;&lt;del&gt;CASSANDRA-9303&lt;/del&gt;&lt;/a&gt; was backported to 2.1.12 in DSE 4.8.4. Hence why we see it in that version vs only in 2.1.13.&lt;/p&gt;</comment>
                            <comment id="15231171" author="zznate" created="Thu, 7 Apr 2016 21:59:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11529&quot; title=&quot;Checking if an unlogged batch is local is inefficient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11529&quot;&gt;&lt;del&gt;CASSANDRA-11529&lt;/del&gt;&lt;/a&gt; makes sense for &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=devdazed&quot; class=&quot;user-hover&quot; rel=&quot;devdazed&quot;&gt;devdazed&lt;/a&gt;, but the numbers from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=arodrime&quot; class=&quot;user-hover&quot; rel=&quot;arodrime&quot;&gt;arodrime&lt;/a&gt; are on a &lt;b&gt;non-vnode&lt;/b&gt; cluster. &lt;/p&gt;

&lt;p&gt;Keep in mind: &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;we actually get this metric to go down a bit by increasing native transport threads&lt;/li&gt;
	&lt;li&gt;we are not CPU bound or spiking (noticeably at least)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If this was an inefficiency in a hot code path per &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11529&quot; title=&quot;Checking if an unlogged batch is local is inefficient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11529&quot;&gt;&lt;del&gt;CASSANDRA-11529&lt;/del&gt;&lt;/a&gt;, I feel like increasing parallelism would exacerbate the issue. &lt;/p&gt;

&lt;p&gt;Good thoughts though - thanks for digging in!&lt;/p&gt;</comment>
                            <comment id="15231802" author="arodrime" created="Fri, 8 Apr 2016 07:47:10 +0000"  >&lt;p&gt;On my description above we where using physical nodes (initial token set, num_token commented, no vnodes enable), fwiw.&lt;/p&gt;</comment>
                            <comment id="15231924" author="crolo" created="Fri, 8 Apr 2016 09:27:30 +0000"  >&lt;p&gt;On my systems: 3.0.3 -&amp;gt; Logged Batches, &lt;br/&gt;
                          2.1.13 -&amp;gt; Unlogged Batches.&lt;/p&gt;

&lt;p&gt;But on 2.1.13 batches where taken out at some point to see if it would improve, but not much improvement was seen. So it might not be related to batches. The test was also small (With batches disabled), so would not make that count as solid test. &lt;/p&gt;</comment>
                            <comment id="15232837" author="pauloricardomg" created="Fri, 8 Apr 2016 20:08:23 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=CRolo&quot; class=&quot;user-hover&quot; rel=&quot;CRolo&quot;&gt;CRolo&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=arodrime&quot; class=&quot;user-hover&quot; rel=&quot;arodrime&quot;&gt;arodrime&lt;/a&gt; if you could record and attach JFR files of servers with high numbers of blocked NTR threads that would be of great help in investigating this issue. (I will also have a look on the already attached files, but they could be affected by &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11529&quot; title=&quot;Checking if an unlogged batch is local is inefficient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11529&quot;&gt;&lt;del&gt;CASSANDRA-11529&lt;/del&gt;&lt;/a&gt;). Thanks in advance!&lt;/p&gt;</comment>
                            <comment id="15234872" author="crolo" created="Mon, 11 Apr 2016 10:56:23 +0000"  >&lt;p&gt;I will try to get it today, I might able to get those for the 3.0.3 cluster, the 2.1.13 might prove more difficult. I will update this ticket later.&lt;/p&gt;</comment>
                            <comment id="15260641" author="pauloricardomg" created="Wed, 27 Apr 2016 18:12:00 +0000"  >&lt;p&gt;I wasn&apos;t able to reproduce this condition so far in a 2.1 &lt;a href=&quot;http://cstar.datastax.com/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;cstar_perf&lt;/a&gt; cluster with the following spec: 1 stress, 3 Cassandra, each node 2x Intel(R) Xeon(R) CPU E5-2620 v2 @ 2.10GHz (12 cores total), 64G, 3 Samsung SSD 845DC EVO 240GB, mdadm RAID 0.&lt;/p&gt;

&lt;p&gt;The test consistent in the following sequence of stress steps followed by &lt;tt&gt;nodetool tpstats&lt;/tt&gt;:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;user profile=&lt;a href=&quot;https://raw.githubusercontent.com/mesosphere/cassandra-mesos/master/driver-extensions/cluster-loadtest/cqlstress-example.yaml&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://raw.githubusercontent.com/mesosphere/cassandra-mesos/master/driver-extensions/cluster-loadtest/cqlstress-example.yaml&lt;/a&gt; ops&amp;#40;insert=1&amp;#41; n=1M -rate threads=300&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;user profile=&lt;a href=&quot;https://raw.githubusercontent.com/mesosphere/cassandra-mesos/master/driver-extensions/cluster-loadtest/cqlstress-example.yaml&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://raw.githubusercontent.com/mesosphere/cassandra-mesos/master/driver-extensions/cluster-loadtest/cqlstress-example.yaml&lt;/a&gt; ops&amp;#40;simple1=1&amp;#41; n=1M -rate threads=300&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;user profile=&lt;a href=&quot;https://raw.githubusercontent.com/mesosphere/cassandra-mesos/master/driver-extensions/cluster-loadtest/cqlstress-example.yaml&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://raw.githubusercontent.com/mesosphere/cassandra-mesos/master/driver-extensions/cluster-loadtest/cqlstress-example.yaml&lt;/a&gt; ops&amp;#40;range1=1&amp;#41; n=1M -rate threads=300&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;At the end of 5 runs, the total number of blocked NTR threads was negligible (0 for all runs, except one with 0.004% blocked). I will try running on a larger mixed workload, ramping up the number of stress threads and also try it on 3.0.&lt;/p&gt;

&lt;p&gt;Meanwhile, some JFR files, reproduction steps or at least more detailed description on the environment/workload to reproduce this would be greatly appreciated.&lt;/p&gt;</comment>
                            <comment id="15262397" author="zznate" created="Thu, 28 Apr 2016 16:03:58 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt; Thanks for continuing to dig into this. However, it looks like you have RF=1 in that stress script:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;keyspace_definition: |
  CREATE KEYSPACE stresscql WITH replication = {&apos;class&apos;: &apos;SimpleStrategy&apos;, &apos;replication_factor&apos;: 1};
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;



</comment>
                            <comment id="15271519" author="pauloricardomg" created="Wed, 4 May 2016 22:04:37 +0000"  >&lt;p&gt;Tried reproducing this in a variety of workloads without success in the following environment:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;3 nodes m3.xlarge (m3.xlarge, 4 vcpu, 15GB RAM, 2 x 40)&lt;/li&gt;
	&lt;li&gt;8GB Heap, CMS&lt;/li&gt;
	&lt;li&gt;100M keys (24GB per node)&lt;/li&gt;
	&lt;li&gt;C* 3.0.3 with default settings and a variation with native_transport_max_threads=10&lt;/li&gt;
	&lt;li&gt;no-vnodes&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The workloads were based a variation of &lt;a href=&quot;https://raw.githubusercontent.com/mesosphere/cassandra-mesos/master/driver-extensions/cluster-loadtest/cqlstress-example.yaml&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://raw.githubusercontent.com/mesosphere/cassandra-mesos/master/driver-extensions/cluster-loadtest/cqlstress-example.yaml&lt;/a&gt; with 100M keys and RF=3 executed from 2 m3.xlarge stress nodes for 1, 2 and 6 hours with 10, 20 and 30 threads without exhausting the cluster CPU/IO capacity. I tried several combinations of the following workloads:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;read-only&lt;/li&gt;
	&lt;li&gt;write-only&lt;/li&gt;
	&lt;li&gt;range-only&lt;/li&gt;
	&lt;li&gt;triggering repairs during the execution&lt;/li&gt;
	&lt;li&gt;unthrottling compaction&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I recorded and analyzed flight recordings during the tests but didn&apos;t find anything suspicious. No blocked native transport threads were verified during tests with above scenarios, so this might indicate that this condition is not a widespread bug like &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11529&quot; title=&quot;Checking if an unlogged batch is local is inefficient&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11529&quot;&gt;&lt;del&gt;CASSANDRA-11529&lt;/del&gt;&lt;/a&gt; but probably some edgy combination of workload, environment and bad scheduling that happens in production but is harder to reproduce with synthetic workloads.&lt;/p&gt;

&lt;p&gt;A thread dump of when this condition happens would probably help us detect where is the bottleneck or contention, so I created &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11713&quot; title=&quot;Add ability to log thread dump when NTR pool is blocked&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11713&quot;&gt;&lt;del&gt;CASSANDRA-11713&lt;/del&gt;&lt;/a&gt; to add ability of logging a thread dump when the thread pool queue is full. If someone could install that patch and enable it in production to capture a thread dump when the blockage happens that would probably help us elucidate what&apos;s going on here.&lt;/p&gt;</comment>
                            <comment id="15308549" author="iamaleksey" created="Tue, 31 May 2016 20:19:24 +0000"  >&lt;p&gt;Closing as Cannot Reproduce (and not for lack of effort). Feel free to reopen with concrete reproducible steps, if you are able to come up with them.&lt;/p&gt;</comment>
                            <comment id="15334604" author="tjake" created="Thu, 16 Jun 2016 20:17:58 +0000"  >&lt;p&gt;The Native Transport Request pool is the only thread pool that has a bounded limit (128)&lt;/p&gt;

&lt;p&gt;The NTR uses the SEPExecutor which effectively blocks till the queue has room.&lt;/p&gt;

&lt;p&gt;However if I&apos;m reading it correctly the SEPWorker goes into a spin loop for some scenarios when there us no work so perhaps we are hitting some edge case when the tasks are blocked.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt; perhaps try setting this queue to something small like 4 to force blocking?&lt;/p&gt;</comment>
                            <comment id="15375615" author="rha" created="Wed, 13 Jul 2016 19:46:06 +0000"  >&lt;p&gt;I also see this behavior after upgrading to 2.1.14 (from 2.0.17). The JMX counter was at 0 before the upgrade. &lt;br/&gt;
I increased the native max threads to 256 but the NTR blocked were still at 0.6%.&lt;/p&gt;

&lt;p&gt;As for now, the better change was to move memtable offheap (less GC time =&amp;gt; less load =&amp;gt; less all time blocked NTR). But I still see up to 0.3% of all time blocked...&lt;/p&gt;

&lt;p&gt;Note: I see this behavior on two different clusters (hosted on AWS). The load after the upgrade in 2.1 is higher than in 2.0 on the cluster using LCS. This cluster has the worst blocked NTR ratio. &lt;/p&gt;</comment>
                            <comment id="15382976" author="tjake" created="Mon, 18 Jul 2016 20:15:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rha&quot; class=&quot;user-hover&quot; rel=&quot;rha&quot;&gt;rha&lt;/a&gt; would you be able to try the attached patch &lt;tt&gt;thread-queue-2.1.txt&lt;/tt&gt; and see if that helps?&lt;/p&gt;</comment>
                            <comment id="15409734" author="rha" created="Fri, 5 Aug 2016 17:10:08 +0000"  >&lt;p&gt;I see a lower NTR blocked percentage with 1024 max queued requests.&lt;/p&gt;

&lt;p&gt;I attached &lt;tt&gt;max_queued_ntr_property.txt&lt;/tt&gt; to set this value in &lt;tt&gt;cassandra-env.sh&lt;/tt&gt; and it turns out that 1536 was a good value in my case. I don&apos;t see any blocked NTR so far. That said it&apos;s just a workaround because the root cause might be elsewhere. Anyway I think it&apos;s better to have a property to set this value instead of a hard coded number. WDYT?&lt;/p&gt;

&lt;p&gt;UPDATE: I had to increase up to &lt;tt&gt;-Dcassandra.max_queued_native_transport_requests=3072&lt;/tt&gt; on the other DC (same cluster) in order to see 0 blocked NTR.&lt;/p&gt;</comment>
                            <comment id="15414131" author="zznate" created="Tue, 9 Aug 2016 19:53:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt; We are preparing this patch for a deploy to two different cluster: 2.2.6 and 2.1.16. Both clusters can exhibit high-burst workloads (which is why I think you can&apos;t reproduce this with the stress tool, and I agree with your suspicions on the cause). We&apos;ll let you know how it goes. &lt;/p&gt;</comment>
                            <comment id="15414132" author="zznate" created="Tue, 9 Aug 2016 19:54:37 +0000"  >&lt;p&gt;Sorry, but I am re-opening this. Given we have a potential explanation and workaround available and that I still see blocked NTR (to a varying degree) on &lt;b&gt;every&lt;/b&gt; 2.x+ cluster I have come in contact with recently. &lt;/p&gt;</comment>
                            <comment id="15414134" author="tjake" created="Tue, 9 Aug 2016 19:55:02 +0000"  >&lt;p&gt;Great, looks like this is the issue.  I guess the question is what is a reasonable default for this? Should we set it high? what&apos;s too high?  &lt;/p&gt;

&lt;p&gt;Any suggestions &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="15414913" author="benedict" created="Wed, 10 Aug 2016 08:36:05 +0000"  >&lt;p&gt;This blocking behaviour and default queue limit was carried forward from the prior code, so I&apos;m afraid I don&apos;t have any insights.  It may be that the increased baseline performance of 2.1 permits worse outlier states to accumulate if the user exploits it.  &lt;/p&gt;

&lt;p&gt;The old code was using the jboss MemoryAwareExecutorService, but estimated the size of each request as 1.  A value of 128 does seem very small for users performing very small operations, but conversely a few large reads could destroy the box, so we will have complaints whatever we pick.  Perhaps configuring this parameter should be explicitly called out in whatever best practices docs we have.  &lt;/p&gt;

&lt;p&gt;Ideally, this limit would be removed entirely and better dynamic constraints applied - I think we have some tickets already for keeping the number of requests at a coordinator constrained.  If that were dealt with (for all request types), this limit could be removed entirely.&lt;/p&gt;</comment>
                            <comment id="15415890" author="tjake" created="Wed, 10 Aug 2016 19:44:25 +0000"  >&lt;p&gt;Ok thanks,  I think we are best off using the System.property with a larger default like 4096.  I&apos;ll wait for &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zznate&quot; class=&quot;user-hover&quot; rel=&quot;zznate&quot;&gt;zznate&lt;/a&gt; to confirm this fixes his issue as well...&lt;/p&gt;</comment>
                            <comment id="15417327" author="mark curtis" created="Thu, 11 Aug 2016 14:28:59 +0000"  >&lt;p&gt;Is the following setting in the &lt;tt&gt;cassandra.yaml&lt;/tt&gt; not the same as &lt;tt&gt;-Dcassandra.max_queued_native_transport_requests&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;native_transport_max_threads: 128&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="15417344" author="tjake" created="Thu, 11 Aug 2016 14:38:03 +0000"  >&lt;p&gt;These are two different things.&lt;/p&gt;

&lt;p&gt;The latter is the number of threads. The former is the max length of the queue&apos;d requests which are allowed to build up before being rejected for those threads.&lt;/p&gt;</comment>
                            <comment id="15417680" author="rha" created="Thu, 11 Aug 2016 18:00:57 +0000"  >&lt;p&gt;4096 seems a safe default value. Over time I see some blocked NTR on one DC even with 3072.&lt;/p&gt;</comment>
                            <comment id="15419050" author="arodrime" created="Fri, 12 Aug 2016 15:58:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt; I am probably going to give this a try as well early next week, I will keep you posted about results over here using 4096 as it seems people tend to converge to this value.&lt;/p&gt;</comment>
                            <comment id="15424176" author="arodrime" created="Wed, 17 Aug 2016 09:09:16 +0000"  >&lt;p&gt;So after giving this a try it looks like 4096 was not high enough here, but it is already a way better:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[alain@ip-172-17-xx-xx ~]$ nodetool tpstats | grep -e Native-Transport-Requests -e Blocked
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
Native-Transport-Requests         1         2         199191         0                27
[alain@ip-172-17-xx-xx ~]$ nodetool tpstats | grep -e Native-Transport-Requests -e Blocked
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
Native-Transport-Requests         1         0         507360         0                36
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I would say:&lt;/p&gt;

&lt;p&gt;1 - This definitely solve the issue on my side as well&lt;br/&gt;
2 - 4096 is not enough on my specific case but is probably still a good default value. We probably need to smooth operations on our side as well, this workload is very spiky.&lt;/p&gt;

&lt;p&gt;I know &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zznate&quot; class=&quot;user-hover&quot; rel=&quot;zznate&quot;&gt;zznate&lt;/a&gt; is giving it a try as well currently, and it looks great so far.&lt;/p&gt;

&lt;p&gt;Thanks for the patch &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt; !&lt;/p&gt;</comment>
                            <comment id="15424622" author="JIRAUSER308715" created="Wed, 17 Aug 2016 14:36:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=arodrime&quot; class=&quot;user-hover&quot; rel=&quot;arodrime&quot;&gt;arodrime&lt;/a&gt; do you see an improvement in client observed latencies with the value increased?  I just want to make sure we have an observable improvement changing this, not just &quot;the metric that sounds bad went down&quot;.  As less things blocked up front could actually make latency worse if it bogs down other things in the system or causes more GC churn than your box can handle.&lt;/p&gt;</comment>
                            <comment id="15424632" author="arodrime" created="Wed, 17 Aug 2016 14:43:11 +0000"  >&lt;p&gt;I will see what I can find. This environment I am working on is not very well monitored. I should be able to give you a latency trend though. Let me check that.&lt;/p&gt;</comment>
                            <comment id="15425867" author="zznate" created="Thu, 18 Aug 2016 03:48:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt; &lt;br/&gt;
The patch was deployed running with the following setting:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;-Dcassandra.max_queued_native_transport_requests=1024
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;(EDIT: changed 4096 to 1024 as I pasted the wrong value initially).&lt;/p&gt;

&lt;p&gt;After an hour of soaking in, it looks good so far:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Native-Transport-Requests         5         0       17738639         0                 0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;On this particular cluster, the NTR all-time would start ticking up immediately (but sporadically) after a restart so the above is an excellent sign. The workload contains a large number of very small rows mixed read and write. &lt;/p&gt;

&lt;p&gt;There is no discernable impact on client latencies, nor with GC churn. If anything, both are marginally smaller. &lt;/p&gt;</comment>
                            <comment id="15505395" author="zznate" created="Tue, 20 Sep 2016 02:52:02 +0000"  >&lt;p&gt;Committed to: 2.1.16, 2.2.8, 3.0.9, 3.10, trunk.&lt;/p&gt;</comment>
                            <comment id="15527840" author="JIRAUSER308715" created="Tue, 27 Sep 2016 23:59:46 +0000"  >&lt;p&gt;For future reference this went into 3.0.10 not 3.0.9&lt;/p&gt;</comment>
                            <comment id="16147323" author="sadagopan88" created="Wed, 30 Aug 2017 14:34:44 +0000"  >&lt;p&gt;I&apos;m using Apache Cassandra&#8482; 3.0.12.1656 which packages with DSE 5.0.8. I&apos;m still seeing high numbers native request blocked. Should i add the &lt;b&gt;-Dcassandra.max_queued_native_transport_requests=1024&lt;/b&gt; or its already in the patch because ,im seeing this issue.&lt;/p&gt;</comment>
                            <comment id="16147483" author="rha" created="Wed, 30 Aug 2017 15:46:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sadagopan88&quot; class=&quot;user-hover&quot; rel=&quot;sadagopan88&quot;&gt;sadagopan88&lt;/a&gt; When using Open Source Apache Cassandra you have to specify it in &lt;tt&gt;cassandra-env.sh&lt;/tt&gt;:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;JVM_OPTS=&lt;span class=&quot;code-quote&quot;&gt;&quot;$JVM_OPTS -Dcassandra.max_queued_native_transport_requests=1024&quot;&lt;/span&gt;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I don&apos;t know if DSE set this setting to something different than default value (128). You can ask to DataStax.&lt;/p&gt;</comment>
                            <comment id="16155610" author="sadagopan88" created="Wed, 6 Sep 2017 16:23:09 +0000"  >&lt;p&gt;Thanks it worked!. &lt;/p&gt;</comment>
                            <comment id="16503765" author="dattem" created="Wed, 6 Jun 2018 19:14:12 +0000"  >&lt;p&gt;I have same issue in open source cassandra 3.8, I have high blocked Native-transport-requests in nodetool tpstats. I also see higher client latencies on co-ordinator when the local latencies are much low and no network issues. I am not sure if the client latencies are related to this. I am currently using default of 128 threads.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12926775/12926775_tablestats.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12926776/12926776_tpstats.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12926774/12926774_tablehistograms.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12926773/12926773_proxyhistograms.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12793819" name="cassandra-102-cms.stack" size="1103058" author="devdazed" created="Wed, 16 Mar 2016 18:11:49 +0000"/>
                            <attachment id="12793820" name="cassandra-102-g1gc.stack" size="1057707" author="devdazed" created="Wed, 16 Mar 2016 18:11:49 +0000"/>
                            <attachment id="12822330" name="max_queued_ntr_property.txt" size="1004" author="rha" created="Fri, 5 Aug 2016 17:05:33 +0000"/>
                            <attachment id="12926773" name="proxyhistograms.png" size="55996" author="dattem" created="Wed, 6 Jun 2018 19:14:03 +0000"/>
                            <attachment id="12926774" name="tablehistograms.png" size="53105" author="dattem" created="Wed, 6 Jun 2018 19:14:03 +0000"/>
                            <attachment id="12926775" name="tablestats.png" size="38394" author="dattem" created="Wed, 6 Jun 2018 19:14:03 +0000"/>
                            <attachment id="12818636" name="thread-queue-2.1.txt" size="936" author="tjake" created="Mon, 18 Jul 2016 20:14:28 +0000"/>
                            <attachment id="12926776" name="tpstats.png" size="131345" author="dattem" created="Wed, 6 Jun 2018 19:14:03 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[tjake]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 23 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2us4n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311421" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Reproduced In</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12333872">2.1.12</customfieldvalue>
    <customfieldvalue id="12334273">2.1.13</customfieldvalue>
    <customfieldvalue id="12334361">3.0.3</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>zznate</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[zznate]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>