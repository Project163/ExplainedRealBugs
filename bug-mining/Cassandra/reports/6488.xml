<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:30:19 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-19534] Unbounded queues in native transport requests lead to node instability</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-19534</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;When a node is under pressure, hundreds of thousands of requests can show up in the native transport queue, and it looks like it can take way longer to timeout than is configured.&#160; We should be shedding load much more aggressively and use a bounded queue for incoming work.&#160; This is extremely evident when we combine a resource consuming workload with a smaller one:&lt;/p&gt;

&lt;p&gt;Running 5.0 HEAD on a single node as of today:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;# populate only
easy-cass-stress run RandomPartitionAccess -p 100  -r 1 --workload.rows=100000 --workload.select=partition --maxrlat 100 --populate 10m --rate 50k -n 1

# workload 1 - larger reads
easy-cass-stress run RandomPartitionAccess -p 100  -r 1 --workload.rows=100000 --workload.select=partition --rate 200 -d 1d

# second workload - small reads
easy-cass-stress run KeyValue -p 1m --rate 20k -r .5 -d 24h&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It appears our results don&apos;t time out at the requested server time either:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;&#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;Writes &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;Reads &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160;Deletes &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; &#160; Errors
&#160; Count &#160;Latency (p99) &#160;1min (req/s) | &#160; Count &#160;Latency (p99) &#160;1min (req/s) | &#160; Count &#160;Latency (p99) &#160;1min (req/s) | &#160; Count &#160;1min (errors/s)
&#160;950286 &#160; &#160; &#160; 70403.93 &#160; &#160; &#160; &#160;634.77 | &#160;789524 &#160; &#160; &#160; 70442.07 &#160; &#160; &#160; &#160;426.02 | &#160; &#160; &#160; 0 &#160; &#160; &#160; &#160; &#160; &#160; &#160;0 &#160; &#160; &#160; &#160; &#160; &#160; 0 | 9580484 &#160; &#160; &#160; &#160; 18980.45
&#160;952304 &#160; &#160; &#160; 70567.62 &#160; &#160; &#160; &#160; 640.1 | &#160;791072 &#160; &#160; &#160; 70634.34 &#160; &#160; &#160; &#160;428.36 | &#160; &#160; &#160; 0 &#160; &#160; &#160; &#160; &#160; &#160; &#160;0 &#160; &#160; &#160; &#160; &#160; &#160; 0 | 9636658 &#160; &#160; &#160; &#160; 18969.54
&#160;953146 &#160; &#160; &#160; 70767.34 &#160; &#160; &#160; &#160; 640.1 | &#160;791400 &#160; &#160; &#160; 70767.76 &#160; &#160; &#160; &#160;428.36 | &#160; &#160; &#160; 0 &#160; &#160; &#160; &#160; &#160; &#160; &#160;0 &#160; &#160; &#160; &#160; &#160; &#160; 0 | 9695272 &#160; &#160; &#160; &#160; 18969.54
&#160;956833 &#160; &#160; &#160; 71171.28 &#160; &#160; &#160; &#160;623.14 | &#160;794009 &#160; &#160; &#160; &#160;71175.6 &#160; &#160; &#160; &#160;412.79 | &#160; &#160; &#160; 0 &#160; &#160; &#160; &#160; &#160; &#160; &#160;0 &#160; &#160; &#160; &#160; &#160; &#160; 0 | 9749377 &#160; &#160; &#160; &#160; 19002.44
&#160;959627 &#160; &#160; &#160; 71312.58 &#160; &#160; &#160; &#160;656.93 | &#160;795703 &#160; &#160; &#160; 71349.87 &#160; &#160; &#160; &#160;435.56 | &#160; &#160; &#160; 0 &#160; &#160; &#160; &#160; &#160; &#160; &#160;0 &#160; &#160; &#160; &#160; &#160; &#160; 0 | 9804907 &#160; &#160; &#160; &#160; 18943.11&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;After stopping the load test altogether, it took nearly a minute before the requests were no longer queued.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13574821">CASSANDRA-19534</key>
            <summary>Unbounded queues in native transport requests lead to node instability</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10000" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Urgent</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ifesdjeen">Alex Petrov</assignee>
                                    <reporter username="rustyrazorblade">Jon Haddad</reporter>
                        <labels>
                    </labels>
                <created>Fri, 5 Apr 2024 20:11:51 +0000</created>
                <updated>Fri, 10 Oct 2025 08:47:53 +0000</updated>
                            <resolved>Fri, 31 May 2024 09:36:16 +0000</resolved>
                                        <fixVersion>4.1.6</fixVersion>
                    <fixVersion>5.0-rc1</fixVersion>
                    <fixVersion>5.0</fixVersion>
                    <fixVersion>5.1</fixVersion>
                                    <component>Legacy/Local Write-Read Paths</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>14</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="37800">10.5h</timespent>
                                <comments>
                            <comment id="17834560" author="ifesdjeen" created="Sat, 6 Apr 2024 20:23:09 +0000"  >&lt;p&gt;I believe I know the root cause of this and have a nuanced solution. I believe replica side read and write queues are also overflowing with pending requests in this case.  I will do my best to post the patch early next week, was intending to do so but other things got in the way.&lt;/p&gt;</comment>
                            <comment id="17837406" author="rustyrazorblade" created="Mon, 15 Apr 2024 19:00:35 +0000"  >&lt;p&gt;In the example I listed, this was a single node, so there were no replicas to take into account.&#160; However, unbounded queues for the other queues are also a problem, if that&apos;s what you meant then I agree.&lt;/p&gt;</comment>
                            <comment id="17837416" author="ifesdjeen" created="Mon, 15 Apr 2024 19:31:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rustyrazorblade&quot; class=&quot;user-hover&quot; rel=&quot;rustyrazorblade&quot;&gt;rustyrazorblade&lt;/a&gt; oh yes, that would exist in a single node as well. Think of a single node as a case of RF1 and coordinator and replica are colocated. I have just finished the last wrinkle in my patch, now just need to rebase and hope to post it ASAP. Hope it&apos;s not pressing, but wanted to indicate that unless you already have a patch for this, probably the quickest way is to check out what I got as what you describe should be well covered.&lt;/p&gt;</comment>
                            <comment id="17837417" author="rustyrazorblade" created="Mon, 15 Apr 2024 19:34:41 +0000"  >&lt;p&gt;I don&apos;t have a patch for this, will wait for yours.&#160;&#160;&lt;/p&gt;

&lt;p&gt;Happy to give a perf test when it&apos;s done, but we should probably get someone else to review for correctness.&lt;/p&gt;</comment>
                            <comment id="17837419" author="ifesdjeen" created="Mon, 15 Apr 2024 19:36:30 +0000"  >&lt;p&gt;Sounds good, I&apos;ll tag you as soon as I have it up. Thank you &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rustyrazorblade&quot; class=&quot;user-hover&quot; rel=&quot;rustyrazorblade&quot;&gt;rustyrazorblade&lt;/a&gt;!&lt;/p&gt;</comment>
                            <comment id="17837427" author="brandon.williams" created="Mon, 15 Apr 2024 20:16:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rustyrazorblade&quot; class=&quot;user-hover&quot; rel=&quot;rustyrazorblade&quot;&gt;rustyrazorblade&lt;/a&gt; do you have more details on how you caused this?  I&apos;m not sure if I need to wait the full 24h but so far I have not seen the same results using your easy-cass-stress commands on a modest machine with 8 cores and 8G of ram.&lt;/p&gt;</comment>
                            <comment id="17837437" author="rustyrazorblade" created="Mon, 15 Apr 2024 20:55:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brandon.williams&quot; class=&quot;user-hover&quot; rel=&quot;brandon.williams&quot;&gt;brandon.williams&lt;/a&gt; not offhand.&#160; Are you running the first command to populate the DB before running both of the other two workloads?&lt;/p&gt;

&lt;p&gt;I was able to trigger this very quickly IIRC.&lt;/p&gt;

&lt;p&gt;Maybe you need to increase the --rate?&lt;/p&gt;

&lt;p&gt;If you can&apos;t repro, I&apos;ll try it again on my end and I&apos;ll give you a full script to reproduce using easy-cass-lab.&lt;/p&gt;</comment>
                            <comment id="17837443" author="brandon.williams" created="Mon, 15 Apr 2024 21:04:17 +0000"  >&lt;p&gt;I did run the first workload to populate, and after many minutes cancelled the larger reads.  The last workload has been going now for about an hour without issue at a steady 10k req/s.&lt;/p&gt;</comment>
                            <comment id="17837445" author="rustyrazorblade" created="Mon, 15 Apr 2024 21:08:32 +0000"  >&lt;p&gt;OK.&#160; The issue is how C* recovers from being overloaded, so try doubling the throughput from easy-cass-stress.&#160; Having two workloads running concurrently helps make the problem more obvious, because this one is fairly resource consuming:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;easy-cass-stress run RandomPartitionAccess -p 100 -r 1 --workload.rows=100000 --workload.select=partition --rate 200 -d 1d
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By mixing one workload that has long queries with another workload that has short ones, it creates a bottleneck and the small queries create a massive backlog.&lt;/p&gt;</comment>
                            <comment id="17837458" author="brandon.williams" created="Mon, 15 Apr 2024 23:00:49 +0000"  >&lt;p&gt;I do have to use a higher rate, but running both workloads concurrently reproduces the runaway buildup and latency.  On 4.1 this seems to stabilize once errors begin, though higher than any timeouts are set, so this is new behavior.&lt;/p&gt;</comment>
                            <comment id="17838355" author="ifesdjeen" created="Wed, 17 Apr 2024 18:56:50 +0000"  >&lt;p&gt;I am a bit surprised to see that on 4.1 we seem to stabilize when errors begin. In essence, the problem is that request lifetime is unbounded. There are several contributing factors, such as lifetimes of local runnables, hints being re-submitted on the local mutation queue, and mutations on the replica side not respecting message expiration deadlines. I think most of these should have been present in 4.1, too. Unless, of course, there is more than one problem. I have initially discovered it pre-5.0 though. &lt;/p&gt;</comment>
                            <comment id="17838359" author="brandon.williams" created="Wed, 17 Apr 2024 19:01:53 +0000"  >&lt;p&gt;The p99 from easy-cass-stress does creep up on 4.1 as well but at a much slower rate so it&apos;s not as easily observable as 5.0.&lt;/p&gt;</comment>
                            <comment id="17838360" author="ifesdjeen" created="Wed, 17 Apr 2024 19:03:38 +0000"  >&lt;p&gt;Do you have observability data from the cluster per chance? Would you be able to maybe check out the Native, Read, and Write stages pending request counts?&lt;/p&gt;</comment>
                            <comment id="17838888" author="ifesdjeen" created="Fri, 19 Apr 2024 07:17:25 +0000"  >&lt;p&gt;Talked to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brandon.williams&quot; class=&quot;user-hover&quot; rel=&quot;brandon.williams&quot;&gt;brandon.williams&lt;/a&gt; and checked the remains of the cluster in the bad state to observe that at least the symptoms match my own observations and the issue I have seen. 180+K of tasks in the Native queue. I am a bit surprised that read and write queues are almost empty (under 100 items in both), but depending on which node was coordinating this can be ok.&lt;/p&gt;</comment>
                            <comment id="17838957" author="brandon.williams" created="Fri, 19 Apr 2024 11:12:32 +0000"  >&lt;p&gt;FWIW, that was a single node.&lt;/p&gt;</comment>
                            <comment id="17838960" author="ifesdjeen" created="Fri, 19 Apr 2024 11:20:10 +0000"  >&lt;p&gt;I guess this can explain it. We have 32 read, 32 write threads, and 128 native threads, so 2:1 relation. Read queue is slightly deeper (about 80) requests, which is clear since latency there is probably higher (however depends on the request), and write queue is almost empty. We easily can have all 128 requests blocked in this case, so they can not really overload the downstream stages. Besides, there&apos;s no hints, so at least a part of the issue we may have in a distributed environment is not applicable. &lt;/p&gt;</comment>
                            <comment id="17840058" author="ifesdjeen" created="Tue, 23 Apr 2024 10:36:25 +0000"  >&lt;p&gt;The main change is the introduction of (currently implicit) configurable &lt;em&gt;native request deadline&lt;/em&gt;. No request, read or write, will be allowed to prolong its execution beyond this deadline. Some of the hidden places that would allow requests to stay overdue were local executor runnables, replica-side writes, and hints. Default is 12 seconds, since this is how much time 3.x driver (which I believe is still the most used version in the community) waits until removing its handlers after which any response from the server will just be ignored. Now, there is an &lt;em&gt;option&lt;/em&gt; to enable expiration based on the queue time, which will be &lt;em&gt;disabled&lt;/em&gt; by default to preserve existing semantics, but my tests have shown enabling it only has positive effects. We will try it out cautiously in different clusters over the next months and will see if tests match up with real loads before we change any of the defaults.&lt;/p&gt;

&lt;p&gt;So by default behaviour will be as follows:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;If request has spent more than 12 seconds in the NATIVE queue, we throw Overloaded exception back to the client. This timeout used to be max of read/write/range/counter rpc timeout.&lt;/li&gt;
	&lt;li&gt;If requests has spent less than 12 seconds, it is allowed to execute; any request issued by the coordinator can live:
	&lt;ol&gt;
		&lt;li&gt;&lt;em&gt;either&lt;/em&gt; &lt;tt&gt;Verb.timeout&lt;/tt&gt; number of milliseconds,&lt;/li&gt;
		&lt;li&gt;&lt;em&gt;or&lt;/em&gt; up to the up to the native request deadline, as measured from the time when the request was admitted to the coordinator&apos;s NATIVE queue, whichever one of these is happening earlier.&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Example 1, read timeout is 5 seconds:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Client sends a request; request spends 6 seconds in the NATIVE queue&lt;/li&gt;
	&lt;li&gt;Coordinator issues requests to replicas; two replicas respond within 3 seconds&lt;/li&gt;
	&lt;li&gt;Coordinator responds to the client with success&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Example 2, read timeout is 5 seconds:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Client sends a request; request spends 6 seconds in the NATIVE queue&lt;/li&gt;
	&lt;li&gt;Coordinator issues requests to replicas; one replica responds within 3 seconds; other replicas fail to respond within 5 seconds of read timeout&lt;/li&gt;
	&lt;li&gt;Coordinator responds to the client with read timeout (preserves current behaviour)&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Example 3, read timeout is 5 seconds:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Client sends a request; request spends 10 seconds in the NATIVE queue&lt;/li&gt;
	&lt;li&gt;Coordinator issues requests to replicas; all replicas fail to respond within 2 seconds&lt;/li&gt;
	&lt;li&gt;Coordinator responds to the client with read timeout; if messages are still in queue on replicas, they will get dropped before processing&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;There will be a &lt;em&gt;new&lt;/em&gt; metric that shows how many of the timeouts would have been &#8220;blind timeouts&#8221; previously. I.e. client &lt;em&gt;would&lt;/em&gt; register them as timeouts, but we as server-side operators would be oblivious to them. This metric will keep us collectively motivated even if we see there is a slight uptick in timeouts after committing the patch.&lt;/p&gt;

&lt;p&gt;Lastly, there is an option to say how much of the 12 seconds client requests are allowed to spend in the native queue. You can say that if there is a client request that has spent 80% of its max 12 seconds in the native queue, we start applying backpressure to the client socket (or throwing overloaded exception, depending on the value of &lt;tt&gt;native_transport_throw_on_overload&lt;/tt&gt;). We have to be careful with enabling this one, since my tests have shown that while we see fewer timeouts server side, clients see more timeouts, because part of the time they consider &#8220;request time&#8221; is now spent somewhere in TCP queues, which we can not account for.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;NewConfigurationParams&quot;&gt;&lt;/a&gt;New Configuration Params&lt;/h3&gt;
&lt;h3&gt;&lt;a name=&quot;cqlstarttime&quot;&gt;&lt;/a&gt;cql_start_time&lt;/h3&gt;

&lt;p&gt;Configures what is considered to be a base for the replica-side timeout. This has actually existed before, it is now actually safe to enable. It still defaults to &lt;tt&gt;REQUEST&lt;/tt&gt;&#160;(processing start time is taken as a timeout base), and an alternative is &lt;tt&gt;QUEUE&lt;/tt&gt; (queue admission time is taken as a timeout base). Unfortunately, there is no consistent view of the timeout base in the community: some people think that server-side read/write timeouts are how much time &lt;em&gt;replicas&lt;/em&gt; have to respond to coordinator. Some believe they mean how much time &lt;em&gt;coordinator&lt;/em&gt; has to respond to the client. This patch is agnostic to these beliefs.&#160;&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;nativetransportthrowonoverload&quot;&gt;&lt;/a&gt;native_transport_throw_on_overload&lt;/h3&gt;

&lt;p&gt;Whether we should apply backpressure to client (i.e. stop reading from the socket), or throw Overloaded exception. Default is socket backpressure, and this is probably fine for now. In principle, this can also be set by the client on per-connection basis via protocol options. However, 3.x series of the driver do not have this addition implemented, so in practice this is not really used. If used, setting from the client takes precedence.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;nativetransporttimeoutinms&quot;&gt;&lt;/a&gt;native_transport_timeout_in_ms&lt;/h3&gt;

&lt;p&gt;The absolute maximum amount of time the server has to respond to client. No work related to client-side request will be done after that period elapses. Default is 100 seconds, which is unreasonably high, but not unbounded. In practice, we should use at most 12 seconds.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;nativetransportqueuemaxitemagethreshold&quot;&gt;&lt;/a&gt;native_transport_queue_max_item_age_threshold&lt;/h3&gt;

&lt;p&gt;Works in conjunction with &lt;tt&gt;native_transport_timeout_in_ms&lt;/tt&gt;. How much of the maximum time the oldest request in the native queue is allowed to spend in the queue before we start applying backpressure (or throwing overloaded exception, depending on what &lt;tt&gt;native_transport_throw_on_overload&lt;/tt&gt; is set to). Default is &#8220;all the time needed&#8221;. We should set it to 0.5-0.8 (50-80% of 12 seconds). But I would leave it disabled at least until there&apos;s more testing done, since it hides the time request spent in TCP queues before it got to us.&lt;/p&gt;
&lt;h3&gt;&lt;a name=&quot;nativetransport%5Cbackoffonqueueoverloadms&quot;&gt;&lt;/a&gt;native_transport_{min|max}_backoff_on_queue_overload_ms&lt;/h3&gt;

&lt;p&gt;If we start applying backpressure (see &lt;tt&gt;native_transport_timeout_in_ms&lt;/tt&gt;), what is the minimum value. If the queue fills up above &lt;tt&gt;native_transport_queue_max_item_age_threshold&lt;/tt&gt;, in other words, request has been sitting more than a % of &lt;tt&gt;native_transport_timeout_in_ms&lt;/tt&gt; in the queue, we start an incident.&lt;/p&gt;

&lt;p&gt;Incident starts by marking the incident time and raising severity level to 1. Each time we observe an old item in the head of the queue, we first bump then number of times we have applied the backpressure. After bumping it 10 times, we increase severity level by&lt;/p&gt;

&lt;p&gt;Backpressure delay applied to the client socket is computed by multiplying the severity level by the minimum delay.&lt;/p&gt;

&lt;p&gt;If we have not seen old requests in the head of the queue for 1 second, we close the incident.&lt;/p&gt;

&lt;p&gt;If the queue remains saturated for a prolonged period, the amount of delay will increase in proportion to the request rate as appliedTimes &amp;amp; severityLevel are incremented. If no new requests are considered overloaded in this way for a second, the incident will be reset and so the delay will drop back down to minimum delay.&lt;/p&gt;
&lt;h2&gt;&lt;a name=&quot;NewlyIntroducedMetrics&quot;&gt;&lt;/a&gt;Newly Introduced Metrics&lt;/h2&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;ClientMetrics/&lt;/tt&gt;&lt;tt&gt;ConnectionPaused&lt;/tt&gt; number of times client connection was paused due to backpressure.&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;ThreadPoolMetrics/OldestTaskQueueTime&lt;/tt&gt; the age (in milliseconds) of the oldest task in the given queue.&lt;/li&gt;
	&lt;li&gt;&lt;tt&gt;ClientMetrics/&lt;/tt&gt;&lt;tt&gt;Queued&lt;/tt&gt; - for how long (in nanoseconds) the item was queued before processing.&lt;/li&gt;
&lt;/ul&gt;


&lt;h2&gt;&lt;a name=&quot;TestingandExamples&quot;&gt;&lt;/a&gt;Testing and Examples&lt;/h2&gt;

&lt;p&gt;Scenario 1: constant overload&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
  &#160; &#160; &#160; &#160; &#160; &#160;Stock | QUEUE | QUEUE + backpressure
success &#160; &#160;| 595 &#160; | 56179 | 21540
timedOut &#160; | 101180| 45647 | 80310
overloaded | 66 &#160; &#160;| 0 &#160; &#160; | 0&#160;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Scenario 2: sudden burst, followed by the drop&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&#160; &#160; &#160; &#160; &#160; &#160; &#160;Stock | QUEUE &#160;| QUEUE+backpressure
success &#160; &#160;| 46305 | 41087 &#160;| 46021
timedOut &#160; | 5016 &#160;| 10121 &#160;| 5204
overloaded | 0 &#160; &#160; | 5204 &#160; | 0 &lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;How to interpret the results: overloaded cluster eventually falls into the state where it does not serve any good traffic at all (in real life continue serving more traffic, but clients would experience a substantial number of timeouts). It is very busy and spinning on all gears, all queues are full. However, because timeout base is &#8220;when we began processing the request&#8221; (disregarding the potentially long queue time), we serve no (almost) good traffic. Above, you can see that the number of successes in the &#8220;old&#8221; row is very little. This is a number of requests that we are able to complete successfully after increasing 50ms of artificial latency in the READ verb handler.&lt;/p&gt;

&lt;p&gt;During overload, we now instead shed bad (timed out) requests and do not attempt to serve them. In addition to this, we apply backpressure to the clients to attempt to push them back a little.&lt;/p&gt;

&lt;p&gt;While in the cluster under constant overload pressure this is just &#8220;the best we can do&#8221;, in real life such events are rare and short, so all we need to do is to not allow a sudden short-time burst to stay around for long enough for the cluster to serve less good traffic.&lt;/p&gt;

&lt;p&gt;You can see that backpressure does not add much on top of the QUEUE; server-side metrics will be slightly better, but client-side, we will keep timing out since client does not know if the request was paused because of the server-side queue delay, or request was buffered in the TCP queue. We still believe that we do need client backpressure (or load shedding), since in case of a sudden burst in requests that eventually tapers off, backpressure does have a positive effect.&lt;/p&gt;

&lt;p&gt;There may be a &lt;em&gt;slight&lt;/em&gt; change in metrics after that change, but added time will be constant and very insignificant, maybe even not significant enough for the operators to notice. Previously, &lt;tt&gt;nanoTime&lt;/tt&gt; for metric calculation was grabbed &lt;em&gt;in the middle of the request&lt;/em&gt; on the coordinator side. Now, we will track entire coordinated path. But this is just a few method calls both on the read and write paths.&lt;/p&gt;</comment>
                            <comment id="17840171" author="brandon.williams" created="Tue, 23 Apr 2024 17:52:30 +0000"  >&lt;p&gt;I think this all sounds good, though there may be a bit of a learning curve for users. Native request deadline is easy enough to understand, but things get a bit nuanced past that.&lt;/p&gt;

&lt;p&gt;Regarding native_transport_timeout_in_ms:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Default is 100 seconds, which is unreasonably high, but not unbounded. In practice, we should use at most 12 seconds.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you mean this currently exists at 100? If not, what is the rationale for that default?&lt;/p&gt;</comment>
                            <comment id="17840173" author="ifesdjeen" created="Tue, 23 Apr 2024 17:56:18 +0000"  >&lt;p&gt;Sorry for the lack of clarity; before this patch, there was no deadline at all. Tasks will live in the system essentially forever clogging queues doing busy work. I was intending to post a patch but it is currently in my CI queue; however otherwise ready to go.&#160;&lt;/p&gt;

&lt;p&gt;I believe with 12 seconds default, users will only see an improvement and there will be no learning curve at all. All configuration options are for the people who understand their request lifetimes and want to get an even better profile.&#160;&lt;/p&gt;</comment>
                            <comment id="17842112" author="ifesdjeen" created="Mon, 29 Apr 2024 17:24:34 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brandon.williams&quot; class=&quot;user-hover&quot; rel=&quot;brandon.williams&quot;&gt;brandon.williams&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rustyrazorblade&quot; class=&quot;user-hover&quot; rel=&quot;rustyrazorblade&quot;&gt;rustyrazorblade&lt;/a&gt; would you be so kind to try running your tests against the branch posted above? I suggest setting  &lt;tt&gt;native_transport_timeout_in_ms&lt;/tt&gt; to about 10 (or 12 max) seconds, and &lt;tt&gt;internode_timeout&lt;/tt&gt; to &lt;tt&gt;true&lt;/tt&gt; for starters. If you really want to push the limits, I&apos;d suggest setting &lt;tt&gt;cql_start_time&lt;/tt&gt; to &lt;tt&gt;REQUEST&lt;/tt&gt;, but this is optional, as we will not roll it out with this setting enabled.&lt;/p&gt;</comment>
                            <comment id="17842114" author="rustyrazorblade" created="Mon, 29 Apr 2024 17:31:25 +0000"  >&lt;p&gt;I can fire it up this week.  &lt;/p&gt;</comment>
                            <comment id="17842164" author="brandon.williams" created="Mon, 29 Apr 2024 21:05:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;d suggest setting cql_start_time to REQUEST&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This appears to be the default in the patch, so first I ran with no config changes. Here are the KeyValue ECS numbers while the random workload is also running with an increased rate of 300:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;                 Writes                                  Reads                                  Deletes                       Errors
  Count  Latency (p99)  1min (req/s) |   Count  Latency (p99)  1min (req/s) |   Count  Latency (p99)  1min (req/s) |      Count  1min (errors/s)
1035374      100254.13             0 |  828223       100252.6             0 |       0              0             0 |   91260303          20005.4
1035374      100254.13             0 |  828223       100252.6             0 |       0              0             0 |   91320989         20007.02
1035374      100254.13             0 |  828223       100252.6             0 |       0              0             0 |   91380356         20007.02
1035374      100254.13             0 |  828223       100252.6             0 |       0              0             0 |   91441015         19976.79
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see the 100ms native transport timeout default which is stable, and with the ECS rate set to 20k/s it is doing nothing but throwing errors at this point.  There was also a good amount of GC pressure.&lt;/p&gt;

&lt;p&gt;With the native transport timeout adjusted to 12s:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;                 Writes                                  Reads                                  Deletes                       Errors
  Count  Latency (p99)  1min (req/s) |   Count  Latency (p99)  1min (req/s) |   Count  Latency (p99)  1min (req/s) |   Count  1min (errors/s)
6362953       12019.36       7602.56 | 6346212       12016.37       7581.98 |       0              0             0 | 1639458          4976.36
6384650       12016.84        7566.8 | 6367878       12023.32       7553.07 |       0              0             0 | 1655989          5033.01
6405461       12016.84        7566.8 | 6388707       12023.32       7553.07 |       0              0             0 | 1674127          5033.01
6426641       12016.84       7510.02 | 6409624       12021.76        7493.9 |       0              0             0 | 1693822          5158.58
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can see the timeout reflected again, but this time without heap pressure it continues to serve many requests.&lt;/p&gt;

&lt;p&gt;Finally, here is cql_start_time set to QUEUE and the native transport timeout at 12s:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;                 Writes                                  Reads                                  Deletes                       Errors
  Count  Latency (p99)  1min (req/s) |   Count  Latency (p99)  1min (req/s) |   Count  Latency (p99)  1min (req/s) |   Count  1min (errors/s)
 505121       11983.81         53.36 |  794926        6334.45        113.39 |       0              0             0 | 5350041          19782.8
 505123       11983.81         49.13 |  794926        6334.45        104.33 |       0              0             0 | 5410428         19815.76
 505137       11983.81         49.13 |  794926        6334.45        104.33 |       0              0             0 | 5468740         19815.76
 505145       11983.81         45.53 |  794926        6334.45         95.99 |       0              0             0 | 5528104         19848.02
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This also ended up throwing errors but still respected the timeout.&lt;/p&gt;

&lt;p&gt;This patch appears to solve the runaway latency growth as requests never last beyond the native transport timeout.  I still think the 100s default is too high; it&apos;s the closest to the unbounded behavior from before but still detrimental and probably not what most people actually want especially since it may exert additional GC pressure.&lt;/p&gt;</comment>
                            <comment id="17842171" author="ifesdjeen" created="Mon, 29 Apr 2024 21:22:22 +0000"  >&lt;p&gt;This is great, thank you for testing!&lt;/p&gt;

&lt;p&gt;My 100s timeout was erring (probably too far) on the side of sticking to the old behaviour. I was slightly concerned that people will see timeouts and conclude this is not something they want. But unfortunately there&#8217;s no way for us to produce reasonable workload balance without shedding some load and timing out lagging requests. I will update a default to 12s.&lt;/p&gt;</comment>
                            <comment id="17843356" author="rustyrazorblade" created="Fri, 3 May 2024 20:47:56 +0000"  >&lt;p&gt;I&apos;m juggling a couple things right now, so I haven&apos;t had time to debug this.  I created a build off the git link here but when I tried to fire it up with these in cassandra.yaml:&lt;/p&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;native_transport_timeout_in_ms: 10000
internode_timeout: true
cql_start_time: &quot;REQUEST&quot;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I got this on startup:&lt;/p&gt;


&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;ERROR [main] 2024-05-03 20:33:19,330 CassandraDaemon.java:887 - Exception encountered during startup: Invalid yaml. Please remove properties [native_transport_timeout_in_ms] from your cassandra.yaml
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This was built using the following:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;url: https://github.com/ifesdjeen/cassandra.git
branch: CASSANDRA-19534
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I can circle back to this next week, hopefully it&apos;s something minor that I missed.&lt;/p&gt;</comment>
                            <comment id="17843358" author="brandon.williams" created="Fri, 3 May 2024 20:49:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rustyrazorblade&quot; class=&quot;user-hover&quot; rel=&quot;rustyrazorblade&quot;&gt;rustyrazorblade&lt;/a&gt; the parameter is actually &apos;native_transport_timeout&apos; (with a unit in your value) and the others already default to what you set.&lt;/p&gt;</comment>
                            <comment id="17843366" author="rustyrazorblade" created="Fri, 3 May 2024 21:13:15 +0000"  >&lt;p&gt;Gotcha.  Updated here and it&apos;s fired up.  Currently throwing a ton of work at a 3 node cluster.  1 node is using the patch, the other two are using original.  &lt;/p&gt;</comment>
                            <comment id="17843375" author="ifesdjeen" created="Fri, 3 May 2024 22:18:05 +0000"  >&lt;p&gt;Thank you for looking into this. Just to make sure, patch works both on the coordinator and replica side, so it would make most sense to compare two clusters: one with a patch and one without. &lt;/p&gt;

&lt;p&gt;There might be some improvement if we only have one node using deadlines but then all three nodes will benefit from replica side shedding while coordinator side shedding will work just for one of them. I think having all nodes with a patch will have a more pronounced effect.&lt;/p&gt;</comment>
                            <comment id="17843376" author="rustyrazorblade" created="Fri, 3 May 2024 22:36:30 +0000"  >&lt;p&gt;I&apos;m running these two workloads concurrently:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;easy-cass-stress run RandomPartitionAccess --rate 50k -r .5 -d 10h  --workload.rows=100000 --workload.select=partition
easy-cass-stress run KeyValue --rate 50k -r .5 -d 24h -p 10m --populate 100m
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this screenshot, the top node is running the branch, the other two are running 5.0-HEAD.&lt;/p&gt;

&lt;p&gt;The first node has more completed native transport requests with a significantly less backed up queue:&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13068635/13068635_screenshot-1.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;The cluster has reached a point where it&apos;s failing a ton, so I&apos;ve stopped the workload to see how fast it recovers.  The cassandra0 node with the branch recovered almost immediately.  The other nodes took approximately 10 seconds.&lt;/p&gt;

&lt;p&gt;I restarted the above two workloads and added a third:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;easy-cass-stress run KeyValue --keyspace test1 --field.keyvalue.value=&apos;random(1024,2048)&apos; -p 1m -r .5 --populate 1m
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The mixed nature of expensive and cheap reads is an easy way to create a deep queue for NTR.  It wasn&apos;t long before I got to this:&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13068636/13068636_screenshot-2.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;It looks like load is being shed much faster off cassandra0:&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13068637/13068637_screenshot-3.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;Within 10 seconds the first node has fully recovered, it took about 10 additional for the other two nodes to recover as well.&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13068638/13068638_screenshot-4.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;I&apos;ve rerun this several times now and am finding &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ifesdjeen&quot; class=&quot;user-hover&quot; rel=&quot;ifesdjeen&quot;&gt;ifesdjeen&lt;/a&gt;&apos;s patched version to recover quicker and than the other two.  The boxes are all running at 99+% CPU, and cassandra0 each time continues to get more completed requests as well as maintain a more shallow queue and recover first.&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13068639/13068639_screenshot-5.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;Starting my test with all 3 nodes running the patch.&lt;/p&gt;


</comment>
                            <comment id="17843377" author="rustyrazorblade" created="Fri, 3 May 2024 22:48:22 +0000"  >&lt;p&gt;Switched the entire cluster to use the patch, started the workloads back up again.  I&apos;ve also added this to the mix to be particularly brutal:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;easy-cass-stress run KeyValue -d 2h --rate 25k -r 0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13068640/13068640_screenshot-6.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;Within seconds the NTR queue is empty and the cluster has fully recovered.&lt;/p&gt;

&lt;p&gt;Switching back to 5.0-HEAD, I&apos;ve restarting the test.&lt;/p&gt;

&lt;p&gt;After just a few minutes the queue looks like this:&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13068641/13068641_screenshot-7.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;Took about 20 seconds to recover after stopping the test.&lt;/p&gt;</comment>
                            <comment id="17843379" author="rustyrazorblade" created="Fri, 3 May 2024 22:55:43 +0000"  >&lt;p&gt;For one last test, I&apos;ve updated cassandra2 to use the patch and restarted the test.&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13068642/13068642_screenshot-8.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;Recovery is in just a few seconds while the other nodes took 10-15 seconds.&lt;/p&gt;

&lt;p&gt; &lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13068643/13068643_screenshot-9.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt; &lt;/p&gt;</comment>
                            <comment id="17843382" author="rustyrazorblade" created="Fri, 3 May 2024 23:11:27 +0000"  >&lt;p&gt;Last one.&#160; I set the timeout for 2 seconds and started the test.&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;watch &quot;nodetool tpstats | egrep &#160;&apos;Native|Read|Mutation&apos;&quot;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It&apos;s clearly very aggressively shedding requests:&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13068644/13068644_image-2024-05-03-16-08-10-101.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;So far, from the perspective of node recovery, this looks like a great improvement.&lt;/p&gt;</comment>
                            <comment id="17843389" author="ifesdjeen" created="Sat, 4 May 2024 05:31:01 +0000"  >&lt;p&gt;These tests look really good! I haven&#8217;t expected the one patched node scenario to work that well but glad that it helps even in that case.&lt;/p&gt;

&lt;p&gt;Thank you for checking!&lt;/p&gt;</comment>
                            <comment id="17844448" author="maedhroz" created="Tue, 7 May 2024 21:24:41 +0000"  >&lt;p&gt;Finished my first pass and left a bunch of comments in the PR. Overall, things look pretty good...just lots of small details to confirm. The only thing that confused me a little was &lt;a href=&quot;https://github.com/apache/cassandra/pull/3274/files#r1591605637&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/cassandra/pull/3274/files#r1591605637&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17845930" author="ifesdjeen" created="Mon, 13 May 2024 13:47:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=maedhroz&quot; class=&quot;user-hover&quot; rel=&quot;maedhroz&quot;&gt;maedhroz&lt;/a&gt; thank you for the review! &lt;br/&gt;
Pushed a new commit that should address your comments.&lt;/p&gt;</comment>
                            <comment id="17846031" author="maedhroz" created="Mon, 13 May 2024 17:46:30 +0000"  >&lt;p&gt;+1 LGTM&lt;/p&gt;

&lt;p&gt;(dropped a couple more little cleanup nits in the PR)&lt;/p&gt;</comment>
                            <comment id="17848777" author="edimitrova" created="Wed, 22 May 2024 23:56:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;+1 LGTM&lt;/p&gt;

&lt;p&gt;(dropped a couple more little cleanup nits in the PR)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Does this mean this is ready to commit? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/biggrin.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I am excited as this is one of the last two 5.0 rc blockers&#160;&lt;/p&gt;</comment>
                            <comment id="17849812" author="ifesdjeen" created="Mon, 27 May 2024 19:16:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=e.dimitrova&quot; class=&quot;user-hover&quot; rel=&quot;e.dimitrova&quot;&gt;e.dimitrova&lt;/a&gt; I believe it does. I was just finishing up the trunk and 4.1 commits, and getting clean CI runs. I think it looks mostly good now.&lt;/p&gt;</comment>
                            <comment id="17851017" author="ifesdjeen" created="Fri, 31 May 2024 09:36:16 +0000"  >&lt;p&gt;Committed to 4.1 with &lt;a href=&quot;https://github.com/apache/cassandra/commit/dc17c29724d86547538cc8116ff1a90d36a0bf3a&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dc17c29724d86547538cc8116ff1a90d36a0bf3a&lt;/a&gt; and merged up to &lt;a href=&quot;https://github.com/apache/cassandra/commit/617a75843c9bfaf241249514f9604466f6c8ccab&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;5.0&lt;/a&gt; and &lt;a href=&quot;https://github.com/apache/cassandra/commit/d10008d54bfb301ba12d022037b1caf78f18418b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;trunk&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="17871760" author="JIRAUSER306480" created="Wed, 7 Aug 2024 18:40:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ifesdjeen&quot; class=&quot;user-hover&quot; rel=&quot;ifesdjeen&quot;&gt;ifesdjeen&lt;/a&gt; : When I tried to checkout&#160; and test the change-&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
git checkout dc17c29724d86547538cc8116ff1a90d36a0bf3a&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;, one dtest (ReadRepairTest.readRepairRTRangeMovementTest) is failing complaining about timeout from a node while trying to retrieve read results. Below are the details:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13070743/13070743_image-2024-08-07-11-37-58-417.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Am I missing something here?&lt;/p&gt;</comment>
                            <comment id="17871880" author="ifesdjeen" created="Thu, 8 Aug 2024 06:48:13 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gauravapiscean&quot; class=&quot;user-hover&quot; rel=&quot;gauravapiscean&quot;&gt;gauravapiscean&lt;/a&gt; is there any reason you believe this SHA has caused it? CI on my side came clean (report attached). You can try reproducing on SHA~1 and/or check if it is flaky.&lt;/p&gt;</comment>
                            <comment id="17872141" author="JIRAUSER306480" created="Thu, 8 Aug 2024 21:28:57 +0000"  >&lt;p&gt;I am not sure of the reason. I tried running dtests multiple times (to rule out flakiness), however the same test (ReadRepairTest.readRepairRTRangeMovementTest) is failing on this SHA and passing on SHA~1.&#160;&#160;&lt;/p&gt;

&lt;p&gt;Additionally, I noticed one pytest (largecolumn_test.py::TestLargeColumn::test_cleanup) failing too&lt;/p&gt;


&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13070777/13070777_image-2024-08-08-14-25-12-915.png&quot; height=&quot;298&quot; width=&quot;1029&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;I see its also failing in the attached ci_summary*.html.&lt;/p&gt;

&lt;p&gt;I am trying to find out the reason for the 2 failures mentioned above and will post my findings here.&lt;/p&gt;</comment>
                            <comment id="17872420" author="ifesdjeen" created="Fri, 9 Aug 2024 17:57:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gauravapiscean&quot; class=&quot;user-hover&quot; rel=&quot;gauravapiscean&quot;&gt;gauravapiscean&lt;/a&gt; RR test turned out to be a consequence of a bad merge, you can check out the fix here: &lt;a href=&quot;https://github.com/ifesdjeen/cassandra/pull/new/cassandra-4.1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/ifesdjeen/cassandra/pull/new/cassandra-4.1&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="17872784" author="JIRAUSER306480" created="Mon, 12 Aug 2024 06:46:02 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ifesdjeen&quot; class=&quot;user-hover&quot; rel=&quot;ifesdjeen&quot;&gt;ifesdjeen&lt;/a&gt;, &lt;a href=&quot;https://github.com/ifesdjeen/cassandra/pull/new/cassandra-4.1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/ifesdjeen/cassandra/pull/new/cassandra-4.1&lt;/a&gt; fixed the issue. Do you think this config (native_transport_timeout) needs to be set in &lt;a href=&quot;https://github.com/apache/cassandra-dtest/blob/trunk/largecolumn_test.py&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/cassandra-dtest/blob/trunk/largecolumn_test.py&lt;/a&gt; as well?&lt;/p&gt;</comment>
                            <comment id="17873433" author="ifesdjeen" created="Wed, 14 Aug 2024 07:48:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gauravapiscean&quot; class=&quot;user-hover&quot; rel=&quot;gauravapiscean&quot;&gt;gauravapiscean&lt;/a&gt; I do not think largecolumn test failure is caused by this commit, or at least not at the first glance. &lt;/p&gt;

&lt;p&gt;I &lt;a href=&quot;https://github.com/apache/cassandra/commit/ff3e5ab76d7ad1386e216eadaf8a1a7e8ea9e0b8&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;ninja&apos;d&lt;/a&gt; 4.1 test fix because it already was reviewed for other branches, and tested by Gaurav to confirm. &lt;/p&gt;</comment>
                            <comment id="17922350" author="JIRAUSER304687" created="Thu, 30 Jan 2025 12:06:48 +0000"  >&lt;p&gt;Hello! &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ifesdjeen&quot; class=&quot;user-hover&quot; rel=&quot;ifesdjeen&quot;&gt;ifesdjeen&lt;/a&gt; can you please add this new configuration parameters to config file cassandra.yaml and to web documentation &lt;a href=&quot;https://cassandra.apache.org/doc/stable/cassandra/configuration/cass_yaml_file.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;page&lt;/a&gt;? This changes are non trivial and little bit hard to understand, thanks! The most important question - Is default values of this parameters are not be broken existing highload clusters behavior?&lt;/p&gt;</comment>
                            <comment id="17922354" author="ifesdjeen" created="Thu, 30 Jan 2025 12:12:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=m.sherstyuk&quot; class=&quot;user-hover&quot; rel=&quot;m.sherstyuk&quot;&gt;m.sherstyuk&lt;/a&gt; I would recommend sticking with defaults, which I designed specifically for handling situations described in this ticket.&#160;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13582575">CASSANDRA-19702</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13619605">CASSANDRA-20691</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13617223">CASSANDRA-20625</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310051">
                    <name>Supercedes</name>
                                            <outwardlinks description="supercedes">
                                        <issuelink>
            <issuekey id="13547569">CASSANDRA-18766</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13562518">CASSANDRA-19215</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="13068375" name="Scenario 1 - QUEUE + Backpressure.jpg" size="597859" author="ifesdjeen" created="Tue, 23 Apr 2024 10:39:59 +0000"/>
                            <attachment id="13068374" name="Scenario 1 - QUEUE.jpg" size="511227" author="ifesdjeen" created="Tue, 23 Apr 2024 10:39:59 +0000"/>
                            <attachment id="13068376" name="Scenario 1 - Stock.jpg" size="615707" author="ifesdjeen" created="Tue, 23 Apr 2024 10:39:59 +0000"/>
                            <attachment id="13068377" name="Scenario 2 - QUEUE + Backpressure.jpg" size="2067805" author="ifesdjeen" created="Tue, 23 Apr 2024 10:41:23 +0000"/>
                            <attachment id="13068378" name="Scenario 2 - QUEUE.jpg" size="1890361" author="ifesdjeen" created="Tue, 23 Apr 2024 10:41:23 +0000"/>
                            <attachment id="13068379" name="Scenario 2 - Stock.jpg" size="864595" author="ifesdjeen" created="Tue, 23 Apr 2024 10:41:23 +0000"/>
                            <attachment id="13069152" name="ci_summary-4.1.html" size="34951" author="ifesdjeen" created="Tue, 28 May 2024 12:48:34 +0000"/>
                            <attachment id="13069133" name="ci_summary-5.0.html" size="30067" author="ifesdjeen" created="Mon, 27 May 2024 19:14:20 +0000"/>
                            <attachment id="13069134" name="ci_summary-trunk.html" size="110703" author="ifesdjeen" created="Mon, 27 May 2024 19:15:20 +0000"/>
                            <attachment id="13068539" name="ci_summary.html" size="55906" author="ifesdjeen" created="Mon, 29 Apr 2024 17:19:10 +0000"/>
                            <attachment id="13068644" name="image-2024-05-03-16-08-10-101.png" size="73838" author="rustyrazorblade" created="Fri, 3 May 2024 23:08:11 +0000"/>
                            <attachment id="13070743" name="image-2024-08-07-11-37-58-417.png" size="41094" author="gauravapiscean" created="Wed, 7 Aug 2024 18:37:59 +0000"/>
                            <attachment id="13070777" name="image-2024-08-08-14-25-12-915.png" size="251720" author="gauravapiscean" created="Thu, 8 Aug 2024 21:25:14 +0000"/>
                            <attachment id="13068635" name="screenshot-1.png" size="52871" author="rustyrazorblade" created="Fri, 3 May 2024 22:05:06 +0000"/>
                            <attachment id="13068636" name="screenshot-2.png" size="51793" author="rustyrazorblade" created="Fri, 3 May 2024 22:12:33 +0000"/>
                            <attachment id="13068637" name="screenshot-3.png" size="49517" author="rustyrazorblade" created="Fri, 3 May 2024 22:13:02 +0000"/>
                            <attachment id="13068638" name="screenshot-4.png" size="50376" author="rustyrazorblade" created="Fri, 3 May 2024 22:13:38 +0000"/>
                            <attachment id="13068639" name="screenshot-5.png" size="48505" author="rustyrazorblade" created="Fri, 3 May 2024 22:22:53 +0000"/>
                            <attachment id="13068640" name="screenshot-6.png" size="49502" author="rustyrazorblade" created="Fri, 3 May 2024 22:42:14 +0000"/>
                            <attachment id="13068641" name="screenshot-7.png" size="46860" author="rustyrazorblade" created="Fri, 3 May 2024 22:47:01 +0000"/>
                            <attachment id="13068642" name="screenshot-8.png" size="45218" author="rustyrazorblade" created="Fri, 3 May 2024 22:54:06 +0000"/>
                            <attachment id="13068643" name="screenshot-9.png" size="48921" author="rustyrazorblade" created="Fri, 3 May 2024 22:54:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>22.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[ifesdjeen]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12313825" key="com.atlassian.jira.plugin.system.customfieldtypes:cascadingselect">
                        <customfieldname>Bug Category</customfieldname>
                        <customfieldvalues>
                                                    <customfieldvalue key="12983" cascade-level=""><![CDATA[Availability]]></customfieldvalue>
            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12313821" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Complexity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12966"><![CDATA[Challenging]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313822" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Discovered By</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12969"><![CDATA[User Report]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12313922" key="jira.plugin.projectspecificselectfield.jpssf:multicftype">
                        <customfieldname>Impacts</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="13100"><![CDATA[None]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            40 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12313921" key="jira.plugin.projectspecificselectfield.jpssf:multicftype">
                        <customfieldname>Platform</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="13076"><![CDATA[All]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1oh0o:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[maedhroz]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12963"><![CDATA[Critical]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311420" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12333891">3.0.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313924" key="com.atlassian.jira.plugin.system.customfieldtypes:textfield">
                        <customfieldname>Source Control Link</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;p&gt;&lt;a href=&quot;https://github.com/apache/cassandra/commit/dc17c29724d86547538cc8116ff1a90d36a0bf3a&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/cassandra/commit/dc17c29724d86547538cc8116ff1a90d36a0bf3a&lt;/a&gt;&lt;/p&gt;</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313823" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Test and Documentation Plan</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;p&gt;Includes tests, also was tested separately; screenshots and description attached&lt;/p&gt;</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>