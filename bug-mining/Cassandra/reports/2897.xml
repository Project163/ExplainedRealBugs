<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:45:52 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-7307] New nodes mark dead nodes as up for 10 minutes</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-7307</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;When doing a node replacement when other nodes are down we see the down nodes marked as up for about 10 minutes. This means requests are routed to the dead nodes causing timeouts. It also means replacing a node when multiple nodes from a replica set is extremely difficult - the node usually tries to stream from a dead node and the replacement fails.&lt;/p&gt;

&lt;p&gt;This isn&apos;t limited to host replacement. I did a simple test:&lt;/p&gt;

&lt;p&gt;1. Create a 2 node cluster&lt;br/&gt;
2. Kill node 2&lt;br/&gt;
3. Start a 3rd node with a unique token (I used auto_bootstrap=false but I don&apos;t think this is significant)&lt;/p&gt;

&lt;p&gt;The 3rd node lists node 2 (127.0.0.2) as up for almost 10 minutes:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;INFO [main] 2014-05-27 14:28:24,753 CassandraDaemon.java (line 119) Logging initialized
INFO [GossipStage:1] 2014-05-27 14:28:31,492 Gossiper.java (line 843) Node /127.0.0.2 is now part of the cluster
INFO [GossipStage:1] 2014-05-27 14:28:31,495 Gossiper.java (line 809) InetAddress /127.0.0.2 is now UP
INFO [GossipTasks:1] 2014-05-27 14:37:44,526 Gossiper.java (line 823) InetAddress /127.0.0.2 is now DOWN
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I reproduced on 1.2.15 and 1.2.16.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12716920">CASSANDRA-7307</key>
            <summary>New nodes mark dead nodes as up for 10 minutes</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="brandon.williams">Brandon Williams</assignee>
                                    <reporter username="rlow">Richard Low</reporter>
                        <labels>
                    </labels>
                <created>Tue, 27 May 2014 21:59:54 +0000</created>
                <updated>Tue, 16 Apr 2019 09:31:44 +0000</updated>
                            <resolved>Wed, 18 Jun 2014 20:27:03 +0000</resolved>
                                        <fixVersion>1.2.17</fixVersion>
                    <fixVersion>2.0.9</fixVersion>
                    <fixVersion>2.1 rc2</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                                                                                <comments>
                            <comment id="14010403" author="brandon.williams" created="Tue, 27 May 2014 22:16:20 +0000"  >&lt;p&gt;I was just looking at this.. here are my results:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; INFO 21:58:45,820 Node /10.208.8.63 state jump to normal
 INFO 22:07:58,880 InetAddress /10.208.8.63 is now DOWN
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&apos;m 100% sure the cause of this is &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6385&quot; title=&quot;FD phi estimator initial conditions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6385&quot;&gt;&lt;del&gt;CASSANDRA-6385&lt;/del&gt;&lt;/a&gt;.  Luckily, after &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-4375&quot; title=&quot;FD incorrectly using RPC timeout to ignore gossip heartbeats&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-4375&quot;&gt;&lt;del&gt;CASSANDRA-4375&lt;/del&gt;&lt;/a&gt; you can override this.  Unluckily, you have to apply a two liner from &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6751&quot; title=&quot;Setting -Dcassandra.fd_initial_value_ms Results in NPE&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6751&quot;&gt;&lt;del&gt;CASSANDRA-6751&lt;/del&gt;&lt;/a&gt; because of stupid static initialization.  We should, perhaps, revisit our motivation for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6385&quot; title=&quot;FD phi estimator initial conditions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6385&quot;&gt;&lt;del&gt;CASSANDRA-6385&lt;/del&gt;&lt;/a&gt; and make the default much lower (for the common case) and let people who are using 1000 node clusters with vnodes perform the override instead.  WDYT &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbellis&quot; class=&quot;user-hover&quot; rel=&quot;jbellis&quot;&gt;jbellis&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="14010428" author="brandon.williams" created="Tue, 27 May 2014 22:27:11 +0000"  >&lt;p&gt;I&apos;ll note this is especially pernicious for replacement, since the time to mark the node down will always be longer than RING_DELAY, and overriding RING_DELAY to be long enough is annoying.  Here&apos;s how long it takes with the initial value set to 3000ms:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; INFO 22:22:06,270 InetAddress /10.208.8.63 is now UP
 INFO 22:23:01,978 InetAddress /10.208.8.63 is now DOWN
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which is better, but still exceeds RING_DELAY, though overriding that to one minute or so is much more reasonable.&lt;/p&gt;</comment>
                            <comment id="14010462" author="rlow" created="Tue, 27 May 2014 22:55:15 +0000"  >&lt;p&gt;For host replacement, I tried increasing RING_DELAY to 5 minutes and got &apos;java.lang.UnsupportedOperationException: Cannnot replace a live node... &apos;. So I don&apos;t think increasing RING_DELAY is a valid workaround, at least for host replacement.&lt;/p&gt;</comment>
                            <comment id="14010466" author="brandon.williams" created="Tue, 27 May 2014 22:58:48 +0000"  >&lt;p&gt;You&apos;d have to set it to 10 minutes or so, since that&apos;s how long failure detection is taking &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14010478" author="rlow" created="Tue, 27 May 2014 23:09:05 +0000"  >&lt;p&gt;The &apos;Cannnot (sic) replace a live node&apos; error came about 1 minute after boot, even with a 5 minute RING_DELAY. So I don&apos;t think a higher RING_DELAY will work:&lt;/p&gt;

&lt;p&gt;INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2014-05-23 19:51:16,934 CassandraDaemon.java (line 119) Logging initialized&lt;br/&gt;
INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2014-05-23 19:51:20,038 StorageService.java (line 105) Overriding RING_DELAY to 300000ms&lt;br/&gt;
ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2014-05-23 19:52:25,189 CassandraDaemon.java (line 464) Exception encountered during startup&lt;br/&gt;
java.lang.UnsupportedOperationException: Cannnot replace a live node... &lt;/p&gt;

&lt;p&gt;I was surprised by this, I expected it to wait for RING_DELAY before getting host replacement info. Is this expected behaviour?&lt;/p&gt;

&lt;p&gt;(These logs are from 1.2.15)&lt;/p&gt;</comment>
                            <comment id="14010483" author="brandon.williams" created="Tue, 27 May 2014 23:14:09 +0000"  >&lt;p&gt;You&apos;re right, RING_DELAY has no effect when replacing because we only sleep for load dissemination, which is 60s.  So the only workaround left is to override the FD initial value.  Fixed the typo in 4e67631ef8.&lt;/p&gt;</comment>
                            <comment id="14017991" author="jbellis" created="Wed, 4 Jun 2014 18:43:01 +0000"  >&lt;p&gt;Agreed, we should make it a bit less forgiving.  I&apos;m not really sure how the math works, what does 10s work out to in practice compared to the current 30s?&lt;/p&gt;

&lt;p&gt;Edit: I remember where 30 came from; it was Quentin&apos;s claim that we&apos;re not statistically valid until we have 30 rounds.  Clearly the effect is not linear!&lt;/p&gt;</comment>
                            <comment id="14018011" author="jbellis" created="Wed, 4 Jun 2014 18:56:46 +0000"  >&lt;p&gt;Actually I think changing the initial padding (which we may want to do anyway) is only a band-aid.  The real problem is that when a new node processes the state that is gossiped to it, it treats the heartbeats it gets as if they just happened, vs (potentially) a long time in the past for a dead node.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure it&apos;s possible to have it both ways without changing the design: we want to assume nodes we just learned about are good, to avoid false-positive failure events a la 6385, but we also want to identify dead nodes quickly.&lt;/p&gt;

&lt;p&gt;I think you&apos;d need gossip arrivalwindows for every peer so that you could pass a &quot;history&quot; to new nodes to fix this.  Otherwise, heuristics like &quot;don&apos;t count this node I just learned about if the gossip event is older than X&quot; are going to be fragile and worse than phi-based FD overall.&lt;/p&gt;</comment>
                            <comment id="14018032" author="jbellis" created="Wed, 4 Jun 2014 19:13:07 +0000"  >&lt;p&gt;In short I&apos;m happy to say that &quot;new nodes mark dead nodes up&quot; is an artifact of the design, working as intended.  Happy to tweak the initial value though to get the time span down, though.  What do you get for 10s / 5s, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rlow&quot; class=&quot;user-hover&quot; rel=&quot;rlow&quot;&gt;rlow&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="14018106" author="brandon.williams" created="Wed, 4 Jun 2014 20:22:06 +0000"  >&lt;p&gt;Going to 3s is still slightly dangerous for replace, taking ~54s in my test above.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m not sure it&apos;s possible to have it both ways without changing the design&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree with that, but I think we should err on the common side.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I think you&apos;d need gossip arrivalwindows for every peer so that you could pass a &quot;history&quot; to new nodes to fix this.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that goes against the FD&apos;s principle, since it&apos;d be passing what is essentially invalid latency history, especially cross-dc.&lt;/p&gt;</comment>
                            <comment id="14018109" author="jbellis" created="Wed, 4 Jun 2014 20:27:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think we should err on the common side &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Me too, but isn&apos;t adding capacity when things are up, more common than when things are down?&lt;/p&gt;</comment>
                            <comment id="14018136" author="brandon.williams" created="Wed, 4 Jun 2014 20:39:15 +0000"  >&lt;p&gt;I was thinking more like, which is more common, moderately sized clusters, or huge ones where the large value becomes needed? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14018189" author="jbellis" created="Wed, 4 Jun 2014 21:11:29 +0000"  >&lt;p&gt;It&apos;s still wrong for small clusters, you just have lower probability of it biting you since there are less dice being rolled.&lt;/p&gt;</comment>
                            <comment id="14018190" author="brandon.williams" created="Wed, 4 Jun 2014 21:14:18 +0000"  >&lt;p&gt;So much less dice though that it took us 13 minor releases and an exceptional test env to encounter it.&lt;/p&gt;</comment>
                            <comment id="14018191" author="brandon.williams" created="Wed, 4 Jun 2014 21:14:48 +0000"  >&lt;p&gt;Actually, more than 13.  Every C* revision before 1.2.13.&lt;/p&gt;</comment>
                            <comment id="14018575" author="rlow" created="Thu, 5 Jun 2014 08:25:59 +0000"  >&lt;p&gt;I don&apos;t know about the FD params so can&apos;t comment on that, but my test case shows it when growing a 2 node cluster to 3 nodes. We also saw it in prod in a larger cluster when doing node replacement. So it shows in both small and large clusters.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;but isn&apos;t adding capacity when things are up, more common than when things are down?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;True, but replacing nodes when nodes are down is common. That&apos;s why we really care about it - currently if there are 2 nodes down from a replica set you can&apos;t use -Dcassandra.replace_address because the replacement node tries to stream from the dead one.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;What do you get for 10s / 5s, Richard Low&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I will test locally but harder to test on a large cluster. Brandon suggested 1 second when doing replacement.&lt;/p&gt;

&lt;p&gt;NB on 1.1 we had no problems with this so the initial conditions used to be better, at least when doing replacement.&lt;/p&gt;</comment>
                            <comment id="14018851" author="jbellis" created="Thu, 5 Jun 2014 15:15:43 +0000"  >&lt;p&gt;Yeah, for 1.1 it used 0.5x instead of 30x. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14036215" author="jbellis" created="Wed, 18 Jun 2014 19:19:43 +0000"  >&lt;p&gt;So, while 30x is probably too aggressive, it sounds to me like the real fix is to not crash and burn if FD is out of date when we try to stream.  Can we turn that into a retry loop instead?&lt;/p&gt;</comment>
                            <comment id="14036226" author="brandon.williams" created="Wed, 18 Jun 2014 19:23:40 +0000"  >&lt;p&gt;30x also greatly complicates troubleshooting gossip connectivity problems, where a node is seen as down even though you know it&apos;s not.  Every time you attempt a fix, you have to wait 10 minutes to see if it really takes.&lt;/p&gt;</comment>
                            <comment id="14036229" author="jbellis" created="Wed, 18 Jun 2014 19:27:32 +0000"  >&lt;p&gt;So let&apos;s drop it to 5x or so.  But I don&apos;t think there&apos;s going to be a happy medium where we avoid both false positives and false negatives, so I&apos;m saying we should update replace to be false-positive-tolerant like the rest of the system.&lt;/p&gt;</comment>
                            <comment id="14036251" author="brandon.williams" created="Wed, 18 Jun 2014 19:39:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t think there&apos;s going to be a happy medium where we avoid both false positives and false negatives&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, but I&apos;d rather err on the side of the more common case, and let the more exotic cases override the initial value if they need it.  5s still puts us outside of the range for the replace_address check to make sure the node it&apos;s replacing is dead, before it even begins streaming (though you&apos;d possibly have streaming problems during bootstrap in this scenario as well) and retry looping would result in an endless loop for the scenario that check is designed to catch.  3s puts us just inside the brink.&lt;/p&gt;</comment>
                            <comment id="14036259" author="jbellis" created="Wed, 18 Jun 2014 19:47:59 +0000"  >&lt;p&gt;If we make code changes to replace to be able to retry if there is a node it thinks is alive that is actually dead, we can make that use case happy while not re-breaking large clusters &amp;#8211; as well as making it more robust all around.&lt;/p&gt;

&lt;p&gt;Changing the multiplier to 3x or 2x is a quicker fix but it re-breaks large clusters so I&apos;d rather take the first approach unless that&apos;s not feasible for some reason.&lt;/p&gt;</comment>
                            <comment id="14036263" author="brandon.williams" created="Wed, 18 Jun 2014 19:50:49 +0000"  >&lt;p&gt;It&apos;s going to take replace 10 minutes of looping before it starts, and will still break bootstraps unless your override ring_delay.  I don&apos;t think we ever actually encountered large clusters breaking in the wild, but obviously from this ticket, problems with replace are being discovered.&lt;/p&gt;</comment>
                            <comment id="14036284" author="jbellis" created="Wed, 18 Jun 2014 20:05:53 +0000"  >&lt;p&gt;Why wouldn&apos;t you try the next-best replica after the first guess fails?&lt;/p&gt;</comment>
                            <comment id="14036288" author="brandon.williams" created="Wed, 18 Jun 2014 20:07:48 +0000"  >&lt;p&gt;For bootstrap?  Let me be clear, the problem with replace is not related to streaming.  It&apos;s refusing to replace a live node, because the FD takes so long to report it as down upon first discovery.&lt;/p&gt;</comment>
                            <comment id="14036305" author="jbellis" created="Wed, 18 Jun 2014 20:17:27 +0000"  >&lt;p&gt;Crap.  I thought it was &quot;because the replacement node tries to stream from the dead one.&quot;&lt;/p&gt;

&lt;p&gt;All right, set it to 3, or 2, and ship it.&lt;/p&gt;</comment>
                            <comment id="14036323" author="brandon.williams" created="Wed, 18 Jun 2014 20:27:03 +0000"  >&lt;p&gt;Went ahead and set it to 2s (which clocks in at around 26s) since that&apos;s safer for bootstrap to hit before ring_delay, and also since 3s put us dangerously close to having the worst of both worlds.  Anyone who has the large cluster problem is going to have to override this either way.&lt;/p&gt;</comment>
                            <comment id="14036328" author="rlow" created="Wed, 18 Jun 2014 20:30:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;For bootstrap? Let me be clear, the problem with replace is not related to streaming. It&apos;s refusing to replace a live node, because the FD takes so long to report it as down upon first discovery.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually, most of the time the problem is streaming. It is happy during replacement (which surprises me, since it clearly lists it as UP), but then requests to stream from the dead node which fails. We&apos;ve seen this where it happily streams from other nodes, but then ultimately fails because the stream from the dead node fails.&lt;/p&gt;

&lt;p&gt;However, we also see a problem where it fails to replace because it thinks the node is live. This happens less often but I expect has the same root cause.&lt;/p&gt;</comment>
                            <comment id="14036345" author="brandon.williams" created="Wed, 18 Jun 2014 20:39:33 +0000"  >&lt;p&gt;There can be two problems here, the bigger one being a) replace will always refuse to replace a live a node, and b) nodes that think they need to stream from that node won&apos;t realize it&apos;s dead soon enough.  If overriding the initial value to 1s worked for you, I&apos;m fairly certain the problem was more a) than b), since as we discovered earlier in the ticket there&apos;s a maximum 60s window the old node can be alive before that check is triggered and bails out.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[brandon.williams]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>395128</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 22 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1w14v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>395262</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>jbellis</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[jbellis]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311420" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12325602">1.2.13</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>