<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:06:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-13038] 33% of compaction time spent in StreamingHistogram.update()</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-13038</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;With the following table, that contains a &lt;b&gt;lot&lt;/b&gt; of cells: &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;CREATE TABLE biggraphite.datapoints_11520p_60s (
    metric uuid,
    time_start_ms bigint,
    offset smallint,
    count &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,
    value &lt;span class=&quot;code-object&quot;&gt;double&lt;/span&gt;,
    PRIMARY KEY ((metric, time_start_ms), offset)
) WITH CLUSTERING ORDER BY (offset DESC);
AND compaction = {&lt;span class=&quot;code-quote&quot;&gt;&apos;class&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.cassandra.db.compaction.TimeWindowCompactionStrategy&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;compaction_window_size&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;6&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;compaction_window_unit&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;HOURS&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;max_threshold&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;32&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;min_threshold&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;6&apos;&lt;/span&gt;}

Keyspace : biggraphite
        Read Count: 1822
        Read Latency: 1.8870054884742042 ms.
        Write Count: 2212271647
        Write Latency: 0.027705127678653473 ms.
        Pending Flushes: 0
                Table: datapoints_11520p_60s
                SSTable count: 47
                Space used (live): 300417555945
                Space used (total): 303147395017
                Space used by snapshots (total): 0
                Off heap memory used (total): 207453042
                SSTable Compression Ratio: 0.4955200053039823
                &lt;span class=&quot;code-object&quot;&gt;Number&lt;/span&gt; of keys (estimate): 16343723
                Memtable cell count: 220576
                Memtable data size: 17115128
                Memtable off heap memory used: 0
                Memtable &lt;span class=&quot;code-keyword&quot;&gt;switch&lt;/span&gt; count: 2872
                Local read count: 0
                Local read latency: NaN ms
                Local write count: 1103167888
                Local write latency: 0.025 ms
                Pending flushes: 0
                Percent repaired: 0.0
                Bloom filter &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; positives: 0
                Bloom filter &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; ratio: 0.00000
                Bloom filter space used: 105118296
                Bloom filter off heap memory used: 106547192
                Index summary off heap memory used: 27730962
                Compression metadata off heap memory used: 73174888
                Compacted partition minimum bytes: 61
                Compacted partition maximum bytes: 51012
                Compacted partition mean bytes: 7899
                Average live cells per slice (last five minutes): NaN
                Maximum live cells per slice (last five minutes): 0
                Average tombstones per slice (last five minutes): NaN
                Maximum tombstones per slice (last five minutes): 0
                Dropped Mutations: 0
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It looks like a good chunk of the compaction time is lost in StreamingHistogram.update() (which is used to store the estimated tombstone drop times).&lt;/p&gt;

&lt;p&gt;This could be caused by a huge number of different deletion times which would makes the bin huge but it this histogram should be capped to 100 keys. It&apos;s more likely caused by the huge number of cells.&lt;/p&gt;

&lt;p&gt;A simple solutions could be to only take into accounts part of the cells, the fact the this table has a TWCS also gives us an additional hint that sampling deletion times would be fine.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13027729">CASSANDRA-13038</key>
            <summary>33% of compaction time spent in StreamingHistogram.update()</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jjirsa">Jeff Jirsa</assignee>
                                    <reporter username="iksaif">Corentin Chary</reporter>
                        <labels>
                    </labels>
                <created>Tue, 13 Dec 2016 09:21:19 +0000</created>
                <updated>Tue, 16 Apr 2019 09:30:13 +0000</updated>
                            <resolved>Thu, 2 Mar 2017 17:46:42 +0000</resolved>
                                        <fixVersion>3.0.12</fixVersion>
                    <fixVersion>3.11.0</fixVersion>
                                    <component>Local/Compaction</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>13</watches>
                                                                                                                <comments>
                            <comment id="15745263" author="tjake" created="Tue, 13 Dec 2016 14:16:14 +0000"  >&lt;p&gt;Which version? This should be fixed in &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9766&quot; title=&quot;Faster Streaming&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9766&quot;&gt;&lt;del&gt;CASSANDRA-9766&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15745291" author="iksaif" created="Tue, 13 Dec 2016 14:28:12 +0000"  >&lt;p&gt;Tested with 3.9 and HEAD from two weeks ago. AFAIK the current code still runs StreamingHistogram for &lt;b&gt;every&lt;/b&gt; cell. A cf with a default ttl and a lot of cells is probably the worst case.&lt;/p&gt;</comment>
                            <comment id="15745347" author="spodxx@gmail.com" created="Tue, 13 Dec 2016 14:56:47 +0000"  >&lt;p&gt;It would be great if you could do some testing again with &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13040&quot; title=&quot;Estimated TS drop-time histogram updated with Cell.NO_DELETION_TIME&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13040&quot;&gt;&lt;del&gt;CASSANDRA-13040&lt;/del&gt;&lt;/a&gt;. There still might be performance issues during compaction of sstables with many distinct deletion times, but it&apos;s hard to tell by just looking at the code or just by doing some quick local testing.&lt;/p&gt;</comment>
                            <comment id="15745550" author="iksaif" created="Tue, 13 Dec 2016 16:26:23 +0000"  >&lt;p&gt;I&apos;ll run a patched version, but according to @pcmanus &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13040&quot; title=&quot;Estimated TS drop-time histogram updated with Cell.NO_DELETION_TIME&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13040&quot;&gt;&lt;del&gt;CASSANDRA-13040&lt;/del&gt;&lt;/a&gt; probably doesn&apos;t change much to that.&lt;/p&gt;</comment>
                            <comment id="15745581" author="iksaif" created="Tue, 13 Dec 2016 16:41:11 +0000"  >&lt;p&gt;I can confirm that the patched version still spend 30% of the compaction time in this code.&lt;/p&gt;</comment>
                            <comment id="15746505" author="iksaif" created="Tue, 13 Dec 2016 22:41:47 +0000"  >&lt;p&gt;What about this patch ? Basically it also doesn&apos;t update the tombstone histogram for expiring cells. Another approach could simply be to update it if isTombstone() is true.&lt;/p&gt;

&lt;p&gt;Still not perfect because it doesn&apos;t handle update(DeletionTime dt).&lt;/p&gt;</comment>
                            <comment id="15747730" author="iksaif" created="Wed, 14 Dec 2016 08:58:12 +0000"  >&lt;p&gt;looking at the compaction code, this would probably not work for strategies that aren&apos;t TWCS or DTCS because they relay on worthDroppingTombstones(). For TWCS and DTCS this means that only fully expired SSTables would be deleted (which probably is a good thing but is a pretty big behavior change).&lt;/p&gt;

&lt;p&gt;any opinion ? &lt;/p&gt;</comment>
                            <comment id="15747786" author="slebresne" created="Wed, 14 Dec 2016 09:23:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;What about this patch ? Basically it also doesn&apos;t update the tombstone histogram for expiring cells.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I admit I don&apos;t follow the reasoning here. You can&apos;t remove functionality just because it&apos;s inconvenient to you performance wise. We &lt;b&gt;want&lt;/b&gt; to include expiring columns in &lt;tt&gt;estimatedTombstoneDropTime&lt;/tt&gt; because that&apos;s what allows us to pro-actively compact sstables that mostly consists of expired (and gc_able) data. Expiring columns is in fact the main reason for &lt;tt&gt;estimatedTombstoneDropTime&lt;/tt&gt; to exist, as the optimization it&apos;s used for kicks in much more frequently in the case of lots of expired data than in the case of pure tombstones (you need some pretty special workload to end up regularly with sstable almost entirely comprised of pure tombstones).&lt;/p&gt;

&lt;p&gt;I think the right way to do something about this is to improve &lt;tt&gt;StreamingHistogram&lt;/tt&gt; (or at least the usage of it), and looking at it&apos;s code, it does look pretty inefficient for what we use it for here.&lt;/p&gt;

&lt;p&gt;In practice, the &lt;tt&gt;localDeletionTime&lt;/tt&gt; for cells in a sstable is likely to vary a lot: if you have a default TTL on your table, the &lt;tt&gt;localDeletionTime&lt;/tt&gt; for a cell is &lt;tt&gt;&amp;lt;insertion time&amp;gt; + ttl + gc_grace&lt;/tt&gt;, which means every cell could well have a different value, which means we can assume most calls to &lt;tt&gt;StreamingHistogram.update()&lt;/tt&gt; will see a different value, and that means we&apos;ll constantly hit &lt;tt&gt;maxBinSize&lt;/tt&gt; and take the slow path of that method.&lt;/p&gt;

&lt;p&gt;But in practice, we use it to estimate if a sstable has more than a certain ratio of droppable data to trigger a special tombstone-removal compaction, so we can likely afford some imprecision. So a dead simple idea could be to round the &lt;tt&gt;localDeletionTime&lt;/tt&gt; to say the closest hour before adding it to the histogram. Losing some precision to the hour is unlikely to matter for the optimization we&apos;re talking about, but this would most likely reduce a lot the amount of time we hit the slow path on &lt;tt&gt;update&lt;/tt&gt; in most cases.&lt;/p&gt;

&lt;p&gt;Or we can probably do a lot better with a little bit more involvment. For isntance, it&apos;s not very hard to know upfront (at the time we create the &lt;tt&gt;MetadataCollector&lt;/tt&gt;) the min/max &lt;tt&gt;localDeletionTime&lt;/tt&gt; we&apos;re gonna see: we can track that easily at the memtable level (we already track the min in fact in the &lt;tt&gt;EncodingStats&lt;/tt&gt;) for flushes, and we can use the stats of the compacted sstables on compaction. So we could take that amplitude, divide it by how many bucket we&apos;re willing to afford (equivalent to the current &lt;tt&gt;maxBinSize&lt;/tt&gt;) and have a simple array of buckets. Updating would be some trivial math and an array access and that&apos;s it. Of course, it would be less precise than the current approach, especially when the concrete deletion times are not well distributed over the amplitude we consider, but I don&apos;t think that matter for this use case.&lt;/p&gt;</comment>
                            <comment id="15747804" author="iksaif" created="Wed, 14 Dec 2016 09:31:31 +0000"  >
&lt;p&gt;The original patch was sent assuming `estimatedTombstoneDropTime` was only used for tombstones and not also for expiring cells, which is wrong.&lt;/p&gt;

&lt;p&gt;I also found out that for TWCS and DTCS tables without `tombstone_threshold` or `tombstone_compaction_interval` (which is the default) the tombstone histogram doesn&apos;t seem to be used at all.&lt;/p&gt;

&lt;p&gt;I guess rounding to the hour or pre-allocating the histogram would both work. The main advantage of rounding to the our is that it&apos;s a simple fix that doesn&apos;t change the behavior too much, we probably want to take &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13024&quot; title=&quot;Droppable Tombstone Ratio Calculation&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13024&quot;&gt;CASSANDRA-13024&lt;/a&gt; into account before writing something smarter.&lt;/p&gt;</comment>
                            <comment id="15748276" author="iksaif" created="Wed, 14 Dec 2016 12:53:53 +0000"  >&lt;p&gt;I can confirm that something as simple as truncating to the hour fixes the issue:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;diff --git a/src/java/org/apache/cassandra/io/sstable/metadata/MetadataCollector.java b/src/java/org/apache/cassandra/io/sstable/metadata/MetadataCollector.java
index 3b32ae2..6c39ce1 100644
--- a/src/java/org/apache/cassandra/io/sstable/metadata/MetadataCollector.java
+++ b/src/java/org/apache/cassandra/io/sstable/metadata/MetadataCollector.java
@@ -205,8 +205,10 @@ &lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;MetadataCollector &lt;span class=&quot;code-keyword&quot;&gt;implements&lt;/span&gt; PartitionStatisticsCollector
     &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; void updateLocalDeletionTime(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; newLocalDeletionTime)
     {
         localDeletionTimeTracker.update(newLocalDeletionTime);
-        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (newLocalDeletionTime != Cell.NO_DELETION_TIME)
-            estimatedTombstoneDropTime.update(newLocalDeletionTime);
+        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (newLocalDeletionTime != Cell.NO_DELETION_TIME) {
+            &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; roundedDeletionTime = newLocalDeletionTime / 3600 * 3600;
+            estimatedTombstoneDropTime.update(roundedDeletionTime);
+        }
     }
 
     &lt;span class=&quot;code-keyword&quot;&gt;private&lt;/span&gt; void updateTTL(&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; newTTL)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(It fixes the performance issue, but I&apos;m not sure of the actual implication for compactions)&lt;/p&gt;</comment>
                            <comment id="15748332" author="benedict" created="Wed, 14 Dec 2016 13:22:02 +0000"  >&lt;p&gt;The StreamingHistogram is a &lt;b&gt;really&lt;/b&gt; inefficient data structure; back when I was active on the project it was one of the data structures I was itching to fix, but there was never a good reason.  I&apos;m kind of glad somebody has an excuse to fix it.&lt;/p&gt;

&lt;p&gt;If anyone does intend to, I recommend exploiting the fact that the answer is only ever consulted at the end, so maintaining a valid answer at all times is a waste.  The whole thing can be done in a single primitive array, with better constant factors and worst-case bounds, without changing the output.  Definitely avoid the worst-case polymorphic behaviour of the present algorithm.&lt;/p&gt;</comment>
                            <comment id="15760592" author="iksaif" created="Mon, 19 Dec 2016 08:47:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt;: wouldn&apos;t using a simple array use a huge amount of memory when compressing big SSTables ? That was probably one of the rationales behind the current implementation. &lt;/p&gt;</comment>
                            <comment id="15760790" author="benedict" created="Mon, 19 Dec 2016 10:20:43 +0000"  >&lt;p&gt;Why do you suppose that?  Array-backed storage is pretty much always more compact, as there are no per-item storage overheads.&lt;/p&gt;</comment>
                            <comment id="15763534" author="iksaif" created="Tue, 20 Dec 2016 07:39:31 +0000"  >&lt;p&gt;I understood the Array-based storage as &quot;storing all the deletion times and sorting them at the end to get the percentiles/buckets&quot;, but I think got that wrong.&lt;/p&gt;</comment>
                            <comment id="15795005" author="iksaif" created="Tue, 3 Jan 2017 12:54:59 +0000"  >&lt;p&gt;What about this ? Simply truncating to the next hour.&lt;/p&gt;</comment>
                            <comment id="15797119" author="jjirsa" created="Wed, 4 Jan 2017 04:18:55 +0000"  >&lt;p&gt;Rounding up to the next hour may be suitable for special cases (including your use case), and you may be willing to tolerate that until the streaming histogram is more efficient, but is not reasonable for the general population. &lt;/p&gt;</comment>
                            <comment id="15797941" author="iksaif" created="Wed, 4 Jan 2017 10:58:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jjirsa&quot; class=&quot;user-hover&quot; rel=&quot;jjirsa&quot;&gt;jjirsa&lt;/a&gt; I honestly did not expect the general population to rely on the deletion of expired SSTables when using TWCS/DTCS in less that one hour.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;DTCS has a default base period of one day.&lt;/li&gt;
	&lt;li&gt;TWCS has a default window size of one day.&lt;/li&gt;
	&lt;li&gt;STCS also has a default tombstone_compaction_interval of one day.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Having that in mind, is it really reasonable to slow down compactions to have estimatedTombstoneDropTime precise to the second ?&lt;/p&gt;

&lt;p&gt;Any suggestion ?&lt;/p&gt;</comment>
                            <comment id="15798631" author="jjirsa" created="Wed, 4 Jan 2017 16:17:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt; offered one suggestion - we can make &lt;tt&gt;StreamingHistogram&lt;/tt&gt; far more efficient.&lt;/p&gt;

&lt;p&gt;As written, it creates 100 buckets, and each time it sees a tombstone drop time outside of one of those buckets, it merges two adjacent buckets to create a new bucket.&lt;/p&gt;

&lt;p&gt;You&apos;re correct in noting that compacting on high throughput, TTL-only systems will probably see that bucket merging happen on nearly every cell once we get past 100 cells - so you need an algorithmic change that avoids the constant merges.  For example, &lt;a href=&quot;https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/utils/StreamingHistogram.java#L98-L99&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;rather than merging every time we overflow the max size&lt;/a&gt;, allow &lt;tt&gt;StreamingHistogram&lt;/tt&gt; to grow to more than  &lt;tt&gt;maxBinSize&lt;/tt&gt; and only reduce/shrink/merge it at a later time (like right before we use/serialize it, perhaps with an additional upper threshold, to avoid it from growing unbounded prior to the final trim).&lt;/p&gt;
</comment>
                            <comment id="15804317" author="benedict" created="Fri, 6 Jan 2017 11:15:42 +0000"  >&lt;p&gt;So, to clarify, I was suggesting using a primitive sorted array for the histogram, and another primitive array buffer of some size &amp;gt;= the histogram (perhaps many multiples of).  When the buffer overflows (or the end is reached), sort the buffer, perform a linear merge with the existing histogram to select the &quot;largest&quot; N buckets (or just most evenly distributed ones, which is slightly different in approach, but probably better), then linear merge again to re-populate the histogram.  &lt;/p&gt;

&lt;p&gt;There&apos;s also the possibility of simply maintaining a priority queue of pairs of delta and their histogram bucket value (sorted on the delta only).  If we calculate the delta for each item inserted (with its neighbours) and it is smaller than the minimum delta in the queue, we perform an O(1) merge instead of an insertion; otherwise we remove the head of the queue, and remove its matching item in the histogram.  This is algorithmically less efficient, and maybe has worse constant factors (hard to say, as fewer rounds, but worse memory behaviour), but is a much simpler change.&lt;/p&gt;</comment>
                            <comment id="15804437" author="iksaif" created="Fri, 6 Jan 2017 12:22:49 +0000"  >&lt;p&gt;For anybody working on this: simple changes to have microbenchmarks for this: &lt;a href=&quot;https://github.com/iksaif/cassandra/commit/9f9886b767de39d9357033aa421197b9bb81bfbd&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/iksaif/cassandra/commit/9f9886b767de39d9357033aa421197b9bb81bfbd&lt;/a&gt;&lt;br/&gt;
Note that the compaction microbenchmark should probably add explicit ttls because the 50k items are probably added in less than one second.&lt;/p&gt;
</comment>
                            <comment id="15850011" author="iksaif" created="Thu, 2 Feb 2017 14:55:36 +0000"  >&lt;p&gt;Hello, did anybody manage to make progress on that (I think &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jjirsa&quot; class=&quot;user-hover&quot; rel=&quot;jjirsa&quot;&gt;jjirsa&lt;/a&gt; said he would try on IRC but I&apos;m not sure anymore).&lt;/p&gt;

&lt;p&gt;I&apos;m still not convinced that it is strictly necessary to have such a fine grained resolution for this histogram by default though given the defaults for the other compaction settings.&lt;/p&gt;</comment>
                            <comment id="15857854" author="slebresne" created="Wed, 8 Feb 2017 11:30:15 +0000"  >&lt;p&gt;Fwiw, I agree that making &lt;tt&gt;StreamingHistogram&lt;/tt&gt; more efficient is needed and should be done, but I also have &lt;em&gt;strong&lt;/em&gt; doubt about &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jjirsa&quot; class=&quot;user-hover&quot; rel=&quot;jjirsa&quot;&gt;jjirsa&lt;/a&gt;&apos;s affirmation that rounding up to the next hour &quot;is not reasonable for the general population&quot; &lt;em&gt;given&lt;/em&gt; where and why it is used, and I&apos;d be really curious to see a non-very-special use case where doing so would have a noticeably bad impact.&lt;/p&gt;

&lt;p&gt;In fact, &lt;em&gt;even&lt;/em&gt; when we do make &lt;tt&gt;StreamingHistogram&lt;/tt&gt; more efficient, it&apos;ll still have more work to do if we keep the second precision than if we round it up a bit (doesn&apos;t have to be to the hour, could be even just to 5-10 minutes) and since I really doubt we need second precision here, I feel not doing at least a bit of rounding up is just a waste of perfectly good CPU cycles.&lt;/p&gt;

&lt;p&gt;Again, let me stress that with a default table TTL and for and a decently loaded time series workload (both of which are not uncommon), you are fairly likely to see a &lt;tt&gt;localDeletionTime&lt;/tt&gt; for almost every second of whatever time span your sstable covers. But in what world do user cares about an sstable being dropped at the exact second at which the last data it contains gets gcable (versus being perfectly happy with it happening within some reasonably short time window)? Keeping in mind that if users do care about such crazy precision, we&apos;re actually failing them already since despite keeping such precision as input of &lt;tt&gt;StreamingHistogram&lt;/tt&gt;, it remains that 1) &lt;tt&gt;StreamingHistogram&lt;/tt&gt; has a relatively small bucket size so it loses precision on its own and 2) the current compaction code isn&apos;t even &lt;em&gt;checking&lt;/em&gt; for sstable to drop every seconds in the first place (in all fairness, we could be smarter here but...).&lt;/p&gt;

&lt;p&gt;Anyway, all this to say that I have personnally no time in the near future to rewrite &lt;tt&gt;StreamingHistogram&lt;/tt&gt; more efficiently, Corentin said he doesn&apos;t either and no-one else has stepped up in the last month, so I&apos;d personally be fine (and in favor really) with doing some rounding up of the time we pass as input to &lt;tt&gt;StreamingHistogram&lt;/tt&gt;. And rounding up to the hour is arguably a bit of a big hammer, so I&apos;d actually suggest something along the line of 5-10 minutes (which should still improve the situation described on this ticket substantially).&lt;/p&gt;

&lt;p&gt;We&apos;ll obviously still want to have a ticket to improve the implementation (though as said above, I&apos;d be in favor of &lt;em&gt;keeping&lt;/em&gt; the rounding up even then) but that would at least make it less urgent.&lt;/p&gt;

&lt;p&gt;With all that said, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jjirsa&quot; class=&quot;user-hover&quot; rel=&quot;jjirsa&quot;&gt;jjirsa&lt;/a&gt;, you have expressed some strong opposition to rounding up above (though maybe a smaller 10 minute rouding up is more acceptable?), and maybe that opposition was shared by other, so I&apos;ll just leave that to &quot;here&apos;s what I would do and why&quot; for now.&lt;/p&gt;</comment>
                            <comment id="15858245" author="jjirsa" created="Wed, 8 Feb 2017 17:07:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=slebresne&quot; class=&quot;user-hover&quot; rel=&quot;slebresne&quot;&gt;slebresne&lt;/a&gt; - my main objection is that I know people (not me) are using 5 minute TWCS windows, and presumably, they&apos;re doing so because they need to expire in 5 minute chunks. Rounding up to 1 hour TTL histograms would eliminate that use case, and it&apos;s not a hypothetical use case. There is a lower bound where it becomes much less useful (maybe it&apos;s 5 minutes, or 1 minute), but it&apos;s not 1 hour. &lt;/p&gt;

&lt;p&gt;In any case, I&apos;ll take the ticket. I had implemented one method using a larger temporary spool of bins that we merge/compact down into the other set on use, and it&apos;s about 3-5x faster without further loss of resolution. I&apos;ll extend that to add 60 second rounding and see what that does - I suspect it&apos;ll be significant, especially in the cases where we&apos;re dealing with a day (or week, or month) worth of data inserted with a default TTL.  Depending on how that looks, we can decide if 60s or 300s is the right rounding point. &lt;/p&gt;</comment>
                            <comment id="15858568" author="jjirsa" created="Wed, 8 Feb 2017 21:34:32 +0000"  >&lt;p&gt;Here&apos;s the new microbench results. Code coming shortly. In each test, we create an array of 10,000,000 integers. &lt;/p&gt;

&lt;p&gt;By default, it creates them in the range of 0-86400 (TTLs for every second in a day)&lt;br/&gt;
In the narrow test, they range from 0-14400 (TTLs for every second in 3 hours). &lt;br/&gt;
In the sparse test, they range from 0-60 (TTLs for every second in a minute). &lt;/p&gt;

&lt;p&gt;The first set of results shows the existing streaming histogram behavior (~8s/run)&lt;br/&gt;
The second set of results is the existing streaming histogram with 60s rounding (~300ms/run)&lt;br/&gt;
The third set buffers updates into a spool 1000x larger than the bins (so 100 bins and 100000 spool), faster than stock by ~2-4x.&lt;br/&gt;
The fourth set buffers updates into a spool 1000x larger than the bins (100 bins, 100000 spool), and rounds to 60s: ~170ms/run&lt;/p&gt;

&lt;p&gt;The rest can probably be ignored - they were testing other bin/spool sizes.&lt;/p&gt;

&lt;p&gt;Based on this output, I think trimming to 60s is worthwhile - I propose we round to 60 second TTL resolution by default, and then make it tunable via system property to either go higher ( ~3600 if hourly buckets work ), or as low as 1 for existing behavior.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;     [java] # Run complete. Total time: 00:15:01
     [java]
     [java] Benchmark                                                               Mode  Samples      Score      Error  Units
     [java] o.a.c.t.m.StreamingHistogramBench.exitingSH                             avgt        5   8299.160 &#177;  819.791  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.narrowexistingSH                       avgt        5   8754.663 &#177; 1026.628  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.sparseexistingSH                       avgt        5    755.780 &#177;   43.003  ms/op

     [java] o.a.c.t.m.StreamingHistogramBench.sparsestreaminghistogram60s           avgt        5    278.455 &#177;   22.318  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.streaminghistogram60s                 avgt        5    280.750 &#177;   38.739  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.narrowstreaminghistogram60s            avgt        5    291.652 &#177;   24.164  ms/op

     [java] o.a.c.t.m.StreamingHistogramBench.newSH1000x                            avgt        5   2909.856 &#177;  245.517  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.narrownewSH1000x                      avgt        5   1491.073 &#177;   96.055  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.sparsenewSH1000x                      avgt        5    467.546 &#177;   41.027  ms/op

     [java] o.a.c.t.m.StreamingHistogramBench.newstreaminghistogram1000x60s         avgt        5    174.485 &#177;    7.145  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.narrownewstreaminghistogram1000x60s    avgt        5    163.713 &#177;   16.302  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.sparsenewstreaminghistogram1000x60    avgt        5    162.857 &#177;   19.290  ms/op

     [java] o.a.c.t.m.StreamingHistogramBench.newSH10x                              avgt        5   8148.194 &#177; 1053.900  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.narrownewSH10x                        avgt        5   8843.801 &#177;  824.742  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.sparsenewSH10x                        avgt        5    457.407 &#177;   22.189  ms/op

     [java] o.a.c.t.m.StreamingHistogramBench.newSH100x                             avgt        5  11140.663 &#177; 1116.051  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.narrownewSH100x                       avgt        5   7654.534 &#177;  379.143  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.sparsenewSH100x                       avgt        5    445.953 &#177;    5.981  ms/op

     [java] o.a.c.t.m.StreamingHistogramBench.newSH10000x                           avgt        5   3016.663 &#177;  605.904  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.narrownewSH10000x                     avgt        5   2773.356 &#177;  292.641  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.sparsenewSH10000x                     avgt        5   3090.090 &#177;  361.765  ms/op

     [java] o.a.c.t.m.StreamingHistogramBench.newSH50and100x                        avgt        5   7305.847 &#177;  619.186  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.narrownewSH50and100x                  avgt        5   5015.139 &#177;  611.160  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.sparsenewSH50and100x                  avgt        5    477.743 &#177;   12.814  ms/op

     [java] o.a.c.t.m.StreamingHistogramBench.newSH50and1000x                        avgt        5   3304.479 &#177;  342.117  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.narrownewSH50and1000x                 avgt        5   1683.944 &#177;  167.946  ms/op
     [java] o.a.c.t.m.StreamingHistogramBench.sparsenewSH50and1000x                 avgt        5    461.928 &#177;    5.799  ms/op
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15859243" author="slebresne" created="Thu, 9 Feb 2017 09:16:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;and then make it tunable via system property&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;d rather we don&apos;t. There is imo such a thing as configuration creep and I really don&apos;t think it&apos;s useful here.&lt;/p&gt;

&lt;p&gt;Frankly, I&apos;d personally just go with 5 minutes round-up and call it a day. I genuinely don&apos;t think it would be reasonable for any user to rely on a high precision if only, again, because the there is other factor that make it impossible to rely on this.&lt;/p&gt;

&lt;p&gt;But whatever, I don&apos;t care that much, and if going with 1 minute make anyone sleep better for some reason, let&apos;s go with that. But please let&apos;s not add more configuration for this.&lt;/p&gt;</comment>
                            <comment id="15860080" author="jjirsa" created="Thu, 9 Feb 2017 19:50:45 +0000"  >&lt;p&gt;Pushing my branches from yesterday here, with CI runs just starting. These have some extra system properties (3, in fact - one for the histogram bin size, one for the spool size, and one for the rounding) - I&apos;ll push a change in a moment to remove the system properties to minimize config bloat. &lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Branch &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; Unit Tests &lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt; DTest &lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/jeffjirsa/cassandra/tree/cassandra-3.0-13038&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.0&lt;/a&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13038-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13038-testall/&lt;/a&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13038-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cassci.datastax.com/job/jeffjirsa-cassandra-3.0-13038-dtest/&lt;/a&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/jeffjirsa/cassandra/tree/cassandra-3.11-13038&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.11&lt;/a&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13038-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13038-testall/&lt;/a&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13038-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cassci.datastax.com/job/jeffjirsa-cassandra-3.11-13038-dtest/&lt;/a&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/jeffjirsa/cassandra/tree/cassandra-13038&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;trunk&lt;/a&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;http://cassci.datastax.com/job/jeffjirsa-cassandra-13038-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cassci.datastax.com/job/jeffjirsa-cassandra-13038-testall/&lt;/a&gt; &lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt; &lt;a href=&quot;http://cassci.datastax.com/job/jeffjirsa-cassandra-13205-13038/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://cassci.datastax.com/job/jeffjirsa-cassandra-13205-13038/&lt;/a&gt; &lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
</comment>
                            <comment id="15864782" author="zznate" created="Tue, 14 Feb 2017 00:51:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jjirsa&quot; class=&quot;user-hover&quot; rel=&quot;jjirsa&quot;&gt;jjirsa&lt;/a&gt; Let&apos;s remove the &lt;tt&gt;System.setProperty&lt;/tt&gt; invocations in the tests since you&apos;ve removed those props and the subsequent commit. &lt;/p&gt;

&lt;p&gt;Should add &lt;tt&gt;spool.hashCode()&lt;/tt&gt; to &lt;tt&gt;StreamingHistogram.hashCode()&lt;/tt&gt; since we are using it in the &lt;tt&gt;equals()&lt;/tt&gt; method. &lt;/p&gt;

&lt;p&gt;Did you intend to override &lt;tt&gt;Object.finalize()&lt;/tt&gt;? We can&apos;t really rely on that for lifecycle. &lt;/p&gt;</comment>
                            <comment id="15865012" author="jjirsa" created="Tue, 14 Feb 2017 04:05:27 +0000"  >&lt;p&gt;No, I didn&apos;t mean to override Object.finalize, and I&apos;ve renamed that to avoid doing so. That was just carelessness on my part, and I can&apos;t believe I did it.&lt;/p&gt;

&lt;p&gt;I&apos;ve added spool.hashCode() to StreamingHistogram.hashCode().&lt;/p&gt;

&lt;p&gt;I removed 2 of the 3 system properties. The third I left, primarily because the compaction tests assume the old behavior. I don&apos;t disagree with Sylvain about config bloat, though. I&apos;ve force pushed fixes to the other two things while I think about how I want to handle this last system property.&lt;/p&gt;</comment>
                            <comment id="15865188" author="iksaif" created="Tue, 14 Feb 2017 06:40:49 +0000"  >&lt;p&gt;The code and the remaining property looks good to me.&lt;br/&gt;
The code of the benchmark could probably be slightly refactored but that&apos;s not really a big deal.&lt;br/&gt;
Thanks for doing it !&lt;/p&gt;</comment>
                            <comment id="15886572" author="zznate" created="Mon, 27 Feb 2017 21:39:47 +0000"  >&lt;p&gt;Thanks for the fixes &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jjirsa&quot; class=&quot;user-hover&quot; rel=&quot;jjirsa&quot;&gt;jjirsa&lt;/a&gt; +1 on the current patch. &lt;/p&gt;</comment>
                            <comment id="15887009" author="jjirsa" created="Tue, 28 Feb 2017 01:27:30 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=iksaif&quot; class=&quot;user-hover&quot; rel=&quot;iksaif&quot;&gt;iksaif&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zznate&quot; class=&quot;user-hover&quot; rel=&quot;zznate&quot;&gt;zznate&lt;/a&gt; - committed as &lt;tt&gt;a5ce963117acf5e4cf0a31057551f2f42385c398&lt;/tt&gt; to 3.0.12 and merged to 3.11.0 and trunk. &lt;/p&gt;</comment>
                            <comment id="15890941" author="jkni" created="Wed, 1 Mar 2017 20:08:59 +0000"  >&lt;p&gt;It looks like this ticket introduced a few test failures. &lt;tt&gt;org.apache.cassandra.io.sstable.metadata.MetadataSerializerTest.testSerialization&lt;/tt&gt; is consistently failing on 3.11 and trunk after this commit, and &lt;tt&gt;org.apache.cassandra.db.compaction.CompactionsTest.testSingleSSTableCompactionWithSizeTieredCompaction&lt;/tt&gt; is failing nearly 100% of the time after this commit on trunk.&lt;/p&gt;

&lt;p&gt;In both cases, these tests are failing on the linked CI above and appear to have no historical failures. I don&apos;t see any discussion of these CI failures for the linked branches on the ticket - are they being resolved elsewhere?&lt;/p&gt;

&lt;p&gt;EDIT: In addition, reverting this commit fixes these test failures.&lt;/p&gt;</comment>
                            <comment id="15890982" author="jjirsa" created="Wed, 1 Mar 2017 20:35:25 +0000"  >&lt;p&gt;Ack. I&apos;ve pushed a fix for &lt;tt&gt;org.apache.cassandra.io.sstable.metadata.MetadataSerializerTest.testSerialization&lt;/tt&gt; and kicked off CI again.&lt;/p&gt;

&lt;p&gt;So far I havent been able to get &lt;tt&gt;org.apache.cassandra.db.compaction.CompactionsTest.testSingleSSTableCompactionWithSizeTieredCompaction&lt;/tt&gt; to fail locally, but I do see it in the previous trunk testall, so I&apos;ll try to get that by EOD.&lt;/p&gt;</comment>
                            <comment id="15891013" author="jkni" created="Wed, 1 Mar 2017 20:50:32 +0000"  >&lt;p&gt;Thanks! I can reproduce the &lt;tt&gt;CompactionsTest&lt;/tt&gt; failure locally, so feel free to ping me if I can help diagnose.&lt;/p&gt;</comment>
                            <comment id="15891018" author="jjirsa" created="Wed, 1 Mar 2017 20:53:43 +0000"  >&lt;p&gt;Sorry &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jkni&quot; class=&quot;user-hover&quot; rel=&quot;jkni&quot;&gt;jkni&lt;/a&gt; - I&apos;ve repro&apos;d and pushed a fix. Will restart CI.&lt;/p&gt;</comment>
                            <comment id="15891175" author="jkni" created="Wed, 1 Mar 2017 22:15:56 +0000"  >&lt;p&gt;Thanks - on a first skim, both of those look good and fix the tests locally for me. One minor nit - if removing the maxSpoolSize from &lt;tt&gt;equals&lt;/tt&gt; on &lt;tt&gt;StreamingHistogram&lt;/tt&gt;, it seems we should remove it from &lt;tt&gt;hashCode&lt;/tt&gt; as well to respect the method contract.&lt;/p&gt;</comment>
                            <comment id="15891209" author="jjirsa" created="Wed, 1 Mar 2017 22:31:34 +0000"  >&lt;p&gt;Addressed your nit, also pushed related changes for 3.0 - care to glance at those as well (CI should be starting in a second)&lt;/p&gt;</comment>
                            <comment id="15891229" author="jkni" created="Wed, 1 Mar 2017 22:43:18 +0000"  >&lt;p&gt;Nit + 3.0 changes look good. If CI doesn&apos;t have any problems, +1.&lt;/p&gt;</comment>
                            <comment id="15892671" author="jjirsa" created="Thu, 2 Mar 2017 17:46:42 +0000"  >&lt;p&gt;CI looks good (there&apos;s one failure, but you and I chatted on IRC, and it looks unrelated).&lt;/p&gt;

&lt;p&gt;Committed to 3.0 as &lt;tt&gt;adbe2cc4df0134955a2c83ae4ebd0086ea5e9164&lt;/tt&gt; and merged up through 3.11 and trunk.&lt;/p&gt;

&lt;p&gt;Thanks again, and apologies for the test breakage.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13027016">CASSANDRA-13024</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13093731">CASSANDRA-13752</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13027758">CASSANDRA-13040</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13046990">CASSANDRA-13281</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13093973">CASSANDRA-13756</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12845382" name="compaction-speedup.patch" size="2019" author="iksaif" created="Tue, 3 Jan 2017 12:54:59 +0000"/>
                            <attachment id="12842962" name="compaction-streaminghistrogram.png" size="116333" author="iksaif" created="Tue, 13 Dec 2016 09:21:21 +0000"/>
                            <attachment id="12842961" name="profiler-snapshot.nps" size="40913" author="iksaif" created="Tue, 13 Dec 2016 09:21:21 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[jjirsa]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 37 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i37j27:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>zznate</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[zznate]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>