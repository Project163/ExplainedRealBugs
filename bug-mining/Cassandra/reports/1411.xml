<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:28:57 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-3483] Support bringing up a new datacenter to existing cluster without repair</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-3483</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;Was talking to Brandon in irc, and we ran into a case where we want to bring up a new DC to an existing cluster. He suggested from jbellis the way to do it currently was set strategy options of dc2:0, then add the nodes. After the nodes are up, change the RF of dc2, and run repair. &lt;/p&gt;

&lt;p&gt;I&apos;d like to avoid a repair as it runs AES and is a bit more intense than how bootstrap works currently by just streaming ranges from the SSTables. Would it be possible to improve this functionality (adding a new DC to existing cluster) than the proposed method? We&apos;d be happy to do a patch if we got some input on the best way to go about it.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12531095">CASSANDRA-3483</key>
            <summary>Support bringing up a new datacenter to existing cluster without repair</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="scode">Peter Schuller</assignee>
                                    <reporter username="lenn0x">Chris Goffinet</reporter>
                        <labels>
                    </labels>
                <created>Fri, 11 Nov 2011 04:59:06 +0000</created>
                <updated>Tue, 14 Oct 2025 12:13:55 +0000</updated>
                            <resolved>Mon, 30 Jan 2012 15:24:36 +0000</resolved>
                                        <fixVersion>1.1.0</fixVersion>
                                        <due></due>
                            <votes>2</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="13148280" author="lenn0x" created="Fri, 11 Nov 2011 05:43:51 +0000"  >&lt;p&gt;Some discussion from irc:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;23:43 &amp;lt; goffinet&amp;gt; has datastax ever had a customer add a new datacenter to an existing cluster? No docs or info on web suggest anyone has done this before
23:44 &amp;lt; driftx&amp;gt; yeah
23:44 &amp;lt; goffinet&amp;gt; how is it done? we are running a case where if i modify strategy options before adding nodes, writes will fail since no endpoints for DC have been added
23:44 &amp;lt; goffinet&amp;gt; we were expecting this might work because we want to bootstrap the new DC to the existing cluster
23:44 &amp;lt; goffinet&amp;gt; take on writes + stream data with RF factor
23:45 &amp;lt; driftx&amp;gt; general best practice is (jbellis can correct if I&apos;m outdated) add the dc at rf:0, add the nodes/update snitch, repair
23:45 &amp;lt; driftx&amp;gt; err, update rf, repair
23:46 &amp;lt; goffinet&amp;gt; yeah mind if i open up a jira? that seems extreme to make the cluster do that .. ?
23:46 &amp;lt; goffinet&amp;gt; or is repair smart enough to just stream ranges instead of AES?
23:46 &amp;lt; driftx&amp;gt; &apos;instead of AES?&apos; that&apos;s what repair is, but if just streams ranges
23:46 &amp;lt; driftx&amp;gt; s/if/it/
23:47 &amp;lt; goffinet&amp;gt; right but AES builds merkle tree, scans through all data ?
23:47 &amp;lt; goffinet&amp;gt; isn&apos;t bootstrap a different operation?
23:47 &amp;lt; goffinet&amp;gt; when streaming just sstables
23:47 &amp;lt; driftx&amp;gt; yeah, it is
23:47 &amp;lt; goffinet&amp;gt; yeah thats more heavy. dont understand why we couldnt use that instead
23:47 &amp;lt; goffinet&amp;gt; like bootstrap
23:48 &amp;lt; stuhood&amp;gt; now that i think about it, it doesn&apos;t really make sense that a CL.ONE write fails if a DC isn&apos;t available
23:48 &amp;lt; stuhood&amp;gt; independent of the bootstrap case, that sounds like the real issue
23:49 &amp;lt; stuhood&amp;gt; goffinet: ^
23:50 &amp;lt; driftx&amp;gt; hmm, yeah that doesn&apos;t
23:50 &amp;lt; driftx&amp;gt; but the problem with bootstrapping a dc is the first node you bootstrap gets everything
23:50 &amp;lt; goffinet&amp;gt; stuhood: yeah. it was complaining about not enough endpoints 
23:50 &amp;lt; goffinet&amp;gt; driftx: why is that? if you are doubling the cluster, and assign the tokens manually ?
23:51 &amp;lt; driftx&amp;gt; still have to do them 2 mins apart, and they&apos;re probably going to be part of the same replica set which I think is troublesome too
23:51 &amp;lt; goffinet&amp;gt; driftx: maybe we can make repair a bit more intelligent? if no data exists on the node .. just stream the ranges instead of using AES
23:52 &amp;lt; driftx&amp;gt; problem is we&apos;re pushing AES to do the entire replica set (which is nearly does now)
23:52 &amp;lt; stuhood&amp;gt; goffinet: it shouldn&apos;t be as heavyweight as you&apos;re thinking
23:53 &amp;lt; goffinet&amp;gt; stuhood: but we have a way currently that is less heavy
23:53 &amp;lt; goffinet&amp;gt; i dont understand why we couldnt use that method
23:53 &amp;lt; stuhood&amp;gt; not implemented =)
23:53 &amp;lt; goffinet&amp;gt; don&apos;t cut corners :)
23:53 &amp;lt; stuhood&amp;gt; human time vs cpu time =P
23:54 &amp;lt; driftx&amp;gt; you could almost do something like #3452 and then have a jmx call to say &apos;ok, finish&apos;
23:54 &amp;lt; CassBotJr&amp;gt; https://issues.apache.org/jira/browse/CASSANDRA-3452 : Create an &apos;infinite bootstrap&apos; mode for sampling live traffic
23:54 &amp;lt; driftx&amp;gt; except the first one that tries is going to have every node pound it with all the writes
23:54 &amp;lt; goffinet&amp;gt; driftx: ill make a jira ticket so we can discuss there, it doesn&apos;t seem like it would be too much trouble to support this use case
23:54 &amp;lt; goffinet&amp;gt; we&apos;d be happy to write the patch after some input
23:55 &amp;lt; driftx&amp;gt; trickier than it sounds I&apos;ll bet, but sgtm
23:57 &amp;lt; stuhood&amp;gt; alternatively, is now the right time to add back group bootstrap?
23:58 &amp;lt; stuhood&amp;gt; so you&apos;d 1) add the dc to the strategy, 2) do a group bootstrap of the entire dc
23:58 &amp;lt; stuhood&amp;gt; would also have to fix the CL.ONE problem though.
23:59 &amp;lt; goffinet&amp;gt; how did group bootstrap work again?
23:59 &amp;lt; driftx&amp;gt; #2434 is relevant
23:59 &amp;lt; CassBotJr&amp;gt; https://issues.apache.org/jira/browse/CASSANDRA-2434 : range movements can violate consistency
--- Day changed Fri Nov 11 2011
00:00 &amp;lt; stuhood&amp;gt; goffinet: bootstrapping many nodes at once without the 2 minute wait
00:01 &amp;lt; goffinet&amp;gt; why was it removed?
00:01 &amp;lt; stuhood&amp;gt; used zookeeper
00:01 &amp;lt; goffinet&amp;gt; oh.
00:01 &amp;lt; stuhood&amp;gt; but come to think of it, removing the 2 minute wait would seem to be relatively easy
00:02 &amp;lt; goffinet&amp;gt; stuhood, i thought the 2 minute wait was just waiting for ring state to settle?
00:02 &amp;lt; goffinet&amp;gt; before it streamed from nodes
00:02 &amp;lt; stuhood&amp;gt; goffinet: yea: you could form a &quot;group&quot; bootstrap by inverting things and waiting until you -hadn&apos;t- seen a new node in 2-10 minutes before you chose a token and started bootstrapping
00:03 &amp;lt; stuhood&amp;gt; so, not terribly simple, but.
00:04 &amp;lt; stuhood&amp;gt; you&apos;d basically have a bunch of nodes sitting around waiting until no new nodes started, and then they have to deterministically choose tokens.
00:05 &amp;lt; goffinet&amp;gt; yes
00:05 &amp;lt; stuhood&amp;gt; well, alternatively, you wouldn&apos;t need a new way to deterministically choose tokens
00:05 &amp;lt; stuhood&amp;gt; (easier)
00:05 &amp;lt; stuhood&amp;gt; no&#8230; scratch that. you would need a way
00:05 &amp;lt; stuhood&amp;gt; for this DC case, all of the nodes are entering an empty ring
00:06 &amp;lt; stuhood&amp;gt; so the group would need to choose something balanced
00:06 &amp;lt; goffinet&amp;gt; empty ring?
00:06 &amp;lt; stuhood&amp;gt; yea, essentially&#8230; there are no tokens in that dc
00:06 &amp;lt; goffinet&amp;gt; but we were going to provide the tokens manually?
00:06 &amp;lt; goffinet&amp;gt; were you thinking of making it automatic?
00:07 &amp;lt; stuhood&amp;gt; yea. fixing bootstrapping groups of nodes would make automatic safe again
00:08 &amp;lt; stuhood&amp;gt; so&#8230; whatever state a node is in when it is sitting and waiting for enough information to choose a token, it should just stay that way and watch what other nodes enter that state
00:08 &amp;lt; goffinet&amp;gt; so i have a question about the 120 second window you have to wait..
00:09 &amp;lt; stuhood&amp;gt; mm
00:09 &amp;lt; driftx&amp;gt; hmm, what if they started up at rf:0 but stayed in some dead state (hibernate might work) without doing anything until you changed the rf, then actually bootstrapped?
00:09 &amp;lt; goffinet&amp;gt; so imagine i startup all the nodes in DC2 at same time, does join_ring=false not grab gossip info at all? I was thinking it would be good if we could just start gossip on all nodes, but until operator says &apos;go&apos; then i could bootstrap them all at same time
00:09 &amp;lt; goffinet&amp;gt; since i would only have to wait at most 120 seconds before kicking them all off
00:10 &amp;lt; stuhood&amp;gt; driftx: yea, that could work too&#8230; but you&apos;d still need to choose tokens. (also, the rf=0 thing shouldn&apos;t be necessary, right? that&apos;s the CL.ONE bug)
00:11 &amp;lt; driftx&amp;gt; well, you really want to choose tokens anyway
00:11 &amp;lt; stuhood&amp;gt; goffinet: it does get gossip&#8230; i think that&apos;s basically equivalent to the pre-join state
00:11 &amp;lt; driftx&amp;gt; I guess you don&apos;t need rf=0 if all the nodes are in hibernate
00:12 &amp;lt; goffinet&amp;gt; yeah i think you do need hibernate in this case, because if i set tokens upfront, i want all nodes to know about ATL ones too
00:12 &amp;lt; goffinet&amp;gt; before i kick off bootstrap
00:12 &amp;lt; stuhood&amp;gt; driftx: i&apos;m confused&#8230; what is the difference between rf=0 and not being there?
00:12 &amp;lt; stuhood&amp;gt; is that a workaround for the CL.ONE bug?
00:13 &amp;lt; driftx&amp;gt; you know there&apos;s a dc with rf:0, can add one with impacting anything
00:13 &amp;lt; driftx&amp;gt; err, without
00:14 ?? boaz__ (0819c319@gateway/web/freenode/ip.8.25.195.25) has joined #cassandra-dev
00:14 &amp;lt; stuhood&amp;gt; so what was the point of adding it? that&apos;s why i&apos;m confused...
00:14 &amp;lt; goffinet&amp;gt; im fine with rf:0, its so you can add the nodes to the cluster before calling repair
00:14 &amp;lt; goffinet&amp;gt; before you add nodes
00:15 &amp;lt; driftx&amp;gt; because the dc is in the schema
00:15 &amp;lt; driftx&amp;gt; so you need it there to have nodes be in it
00:15 &amp;lt; stuhood&amp;gt; ah
00:16 &amp;lt; goffinet&amp;gt; driftx: any reason why we couldnt just fix that? so dc2:3 wont throw an error if nodes are down?
00:16 &amp;lt; goffinet&amp;gt; that way you would needed to do two steps
00:16 &amp;lt; goffinet&amp;gt; dc2:0, add nodes, dc2:3
00:16 &amp;lt; goffinet&amp;gt; wouldn&apos;t*
00:16 &amp;lt; driftx&amp;gt; I don&apos;t understand, you can already do that
00:17 &amp;lt; driftx&amp;gt; you just have to repair afterwards
00:17 &amp;lt; goffinet&amp;gt; it throws an error currently? if you set dc2:3 and no nodes exist for dc2
00:17 &amp;lt; goffinet&amp;gt; we&apos;ll double check on that
00:18 &amp;lt; goffinet&amp;gt; for writes
00:18 &amp;lt; driftx&amp;gt; oh, it does
00:19 &amp;lt; driftx&amp;gt; but only for writes
00:19 &amp;lt; goffinet&amp;gt; yeah
00:19 &amp;lt; goffinet&amp;gt; so thats fine, thats fixable
00:19 &amp;lt; goffinet&amp;gt; im just curious about a) how can we bootstrap nodes without 120s delays between N nodes b) stream from DC1 without AES
00:21 &amp;lt; stuhood&amp;gt; goffinet: if you figure out a, i don&apos;t think b is necessary?
00:22 &amp;lt; stuhood&amp;gt; assuming they are aware of the other joining nodes, and can all join the same range
00:22 &amp;lt; stuhood&amp;gt; that would be the keystone for some kind of group bootstrap
00:23 &amp;lt; goffinet&amp;gt; let me test out join_ring, because im curious. if join_ring=false still gossips but doesnt offically join.. it would be nice if node 2 in DC2 knew about that node too somehow?
00:23 &amp;lt; driftx&amp;gt; that&apos;s why I proposed cheating, add them all as non-members, then ask them to bootstrap
00:23 &amp;lt; goffinet&amp;gt; because then .. i could just run a command on each node at same time
00:23 &amp;lt; goffinet&amp;gt; since they all know about each other in a hibernate state
00:23 &amp;lt; goffinet&amp;gt; driftx: yes i like that
00:24 &amp;lt; driftx&amp;gt;     private void joinTokenRing(int delay) throws IOException, org.apache.cassandra.config.ConfigurationException
00:24 &amp;lt; driftx&amp;gt;     {
00:24 &amp;lt; driftx&amp;gt;         logger_.info(&quot;Starting up server gossip&quot;);
00:24 &amp;lt; driftx&amp;gt; they don&apos;t use gossip with join_ring off
00:24 &amp;lt; stuhood&amp;gt; but will that actually allow them to all join the same range?
00:24 &amp;lt; goffinet&amp;gt; okay cool, yeah we would need to make it join in that special state then
00:25 &amp;lt; stuhood&amp;gt; i think there is an edgecase here&#8230; if multiple nodes are joining the same range, and one of them fails, then should they all fail?
00:25 &amp;lt; driftx&amp;gt; no, it basically saves you server startup time that is not ring-related :)
00:25 &amp;lt; goffinet&amp;gt; stuhood, they all know the tokens ahead of time?
00:25 &amp;lt; goffinet&amp;gt; they just need to know the current global state of things
00:25 &amp;lt; stuhood&amp;gt; goffinet: right, but if they are streaming the range that they will be responsible for...
00:26 ?? mw1 (~Adium@8.25.195.29) has quit (Quit: Leaving.)
00:26 &amp;lt; stuhood&amp;gt; Joining nodes don&apos;t stick around if they fail
00:26 &amp;lt; goffinet&amp;gt; they shouldnt be allowed to do that until they joined ?
00:26 &amp;lt; stuhood&amp;gt; nah, you stream while you are joining&#8230; unless you are talking about repair
00:26 &amp;lt; goffinet&amp;gt; stuhood: was that removed? i thought u had to still remove the node
00:26 &amp;lt; goffinet&amp;gt; using the new options in 1.0
00:26 &amp;lt; stuhood&amp;gt; don&apos;t know about 1.0
00:27 &amp;lt; driftx&amp;gt; no, a failed non-member is just a fat client and disappears
00:27 &amp;lt; goffinet&amp;gt; but i thought there was a timeout for fat client ?
00:27 &amp;lt; goffinet&amp;gt; is it 30s or something?
00:27 &amp;lt; driftx&amp;gt; yes
00:28 &amp;lt; goffinet&amp;gt; so nodes that arent fat clients, why might we remove them ? if we didnt..
00:28 &amp;lt; goffinet&amp;gt; and let the operator do it
00:28 &amp;lt; goffinet&amp;gt; or have a larger timeout
00:28 &amp;lt; goffinet&amp;gt; might make this a non-issue
00:28 &amp;lt; driftx&amp;gt; what does a larger timeout/keeping them around buy you?
00:29 &amp;lt; goffinet&amp;gt; because if they go away, and i bootstrap after they failed, wont my view of ring be skewed?
00:29 &amp;lt; stuhood&amp;gt; driftx: i guess in this case, the node would resume bootstrapping from where it left off
00:29 &amp;lt; driftx&amp;gt; it would&apos;ve missed writes in the meantime and require a repair afterwards anyway
00:29 &amp;lt; stuhood&amp;gt; sorry&#8230; &quot;resume&quot; in the sense of &quot;start over&quot;, but yea
00:31 &amp;lt; stuhood&amp;gt; that would be a pretty big change, but it might make sense
00:31 &amp;lt; goffinet&amp;gt; stuhood: what would you change
00:31 &amp;lt; stuhood&amp;gt; what you said, about nodes in joining staying in joining
00:31 &amp;lt; stuhood&amp;gt; so if the machine restarts, it begins joining at the same position again
00:33 &amp;lt; goffinet&amp;gt; if we supported that + letting nodes gossip in hibernate, would allow us to add capacity at operator control
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13151499" author="slebresne" created="Wed, 16 Nov 2011 20:49:20 +0000"  >&lt;p&gt;I think it wouldn&apos;t be crazy and actually very (very) simple to add a new nodetool command (rebuild?) that would basically have the node asks the other replicas to stream all there data to him (for the correct ranges obviously). In other words, a command that force the streaming part of bootstrap without all the join ring part. Or another way to say is to have the streaming part of a repair but without the validation part.&lt;/p&gt;

&lt;p&gt;The method to add a new DC would be the same as today except that repair would be replaced by this new operation.&lt;/p&gt;</comment>
                            <comment id="13151503" author="brandon.williams" created="Wed, 16 Nov 2011 20:52:07 +0000"  >&lt;p&gt;We would still need to put the nodes into a &apos;bootstrap&apos; state to get incoming writes forwarded to them, otherwise you have to repair in the end anyway.&lt;/p&gt;</comment>
                            <comment id="13151784" author="lenn0x" created="Thu, 17 Nov 2011 04:27:03 +0000"  >&lt;p&gt;Yeah as Brandon mentioned, we would still want to go into the bootstrap state to get those writes. This would also allow us to add capacity in the same way, if we manually pick tokens (auto bootstrap is kinda worthless IMO) to existing DC as well. We can just fire off the bootstrap command from nodetool as needed.&lt;/p&gt;</comment>
                            <comment id="13151854" author="lenn0x" created="Thu, 17 Nov 2011 06:30:04 +0000"  >&lt;p&gt;Sorry, I had to re-read what Sylvain said for it to &apos;click&apos;. So the process he proposes is as follows with RF of 3:&lt;/p&gt;

&lt;p&gt;1. strategy options dc2:0&lt;br/&gt;
2. bring up new nodes in dc2 with auto_bootstrap off and token set&lt;br/&gt;
3. set strategy options dc2:3&lt;br/&gt;
4. run &apos;rebuild&apos; on each node in dc2&lt;/p&gt;

&lt;p&gt;this would handle the writes part.&lt;/p&gt;

&lt;p&gt;i was kinda hoping though that we could modify the gossip state because I could very well see this playing into the case where you weren&apos;t adding DCs but wanted to add lots of nodes (60-100 like we do currently) ... and wanted to have them all added to existing DC.. having the bootstrap defined that way, would allow us to bootstrap nodes as we please in existing DCs, bring them all to the ring at once to have a consistent state without taking on traffic until they transitioned states (joining/normal). Where as this proposal wouldn&apos;t be able to satisfy that use case.&lt;/p&gt;
</comment>
                            <comment id="13151933" author="slebresne" created="Thu, 17 Nov 2011 09:18:43 +0000"  >&lt;p&gt;So you&apos;re proposing to add support for bootstrapping multiple nodes together. I&apos;m not against that, it would be nice and that would give you 90% of what this ticket is about (you&apos;d have to add the ability to multi-boostrap &lt;b&gt;and&lt;/b&gt; add a DC/augment the replication faction at the same time). But it is orders of magnitude more complicated than what I&apos;m suggesting. Which is not a problem in itself given it&apos;s a broader solution, but it means we&apos;ll have multi-node boostrap at best for 1.1, while I&apos;m pretty sure I can get the &apos;rebuild&apos; command wrote in like an hour (and I see no reason why it couldn&apos;t be put in the 1.0 series).&lt;/p&gt;
</comment>
                            <comment id="13152496" author="lenn0x" created="Fri, 18 Nov 2011 00:22:00 +0000"  >&lt;p&gt;Sylvain, it looks like a state &apos;HIBERNATE&apos; exists in GOSSIP already based on recovering a dead node in 1.0. Do you have a preference on the name of the new state if we attempted adding multiple nodes patch? WAITING? STANDBY?&lt;/p&gt;</comment>
                            <comment id="13152634" author="scode" created="Fri, 18 Nov 2011 04:22:10 +0000"  >&lt;p&gt;I was thinking of GHOST too after I realized we&apos;ll need a state where the node receives neither writes nor reads (more details here later).&lt;/p&gt;</comment>
                            <comment id="13154502" author="jbellis" created="Mon, 21 Nov 2011 21:03:49 +0000"  >&lt;p&gt;So, am I understanding correctly that we&apos;re talking about two different scenarios?&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Add new DC without repair&lt;/li&gt;
	&lt;li&gt;Add many nodes to existing DC without RING_DELAY in between&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think Sylvain&apos;s proposal addresses the first nicely.  So what I need help with is understanding what problem you&apos;re trying to solve with the second part.  Dealing with overlapping ranges in node movement basically requires a rewrite of that subsystem (&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-2434&quot; title=&quot;range movements can violate consistency&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-2434&quot;&gt;&lt;del&gt;CASSANDRA-2434&lt;/del&gt;&lt;/a&gt;).  But I suspect there is a &quot;good enough&quot; solution that we could find if I understood better what your pain point is here.&lt;/p&gt;</comment>
                            <comment id="13154898" author="lenn0x" created="Tue, 22 Nov 2011 05:57:02 +0000"  >&lt;p&gt;Jonathan,&lt;/p&gt;

&lt;p&gt;You are correct. Sylvain&apos;s proposal does satisfy this ticket. It doesn&apos;t solve the case of (2) where if you want to add lots of nodes in an existing DC, and you know the tokens they should be at, and want to join them all at once.&lt;/p&gt;

&lt;p&gt;Our use case is, we actually add 60-100 nodes in one big capacity add. We would like to avoid the 120 second per node time frame. It&apos;s not a deal breaker though. We actually realized though if we are adding that many nodes to our cluster, with a large cluster already, we need to rebalance heavily anyway.  Peter is almost done with the &apos;rebuild&apos; patch, I&apos;m assigning him to this ticket. &lt;/p&gt;

&lt;p&gt;Our next big focus is improving the rebalancing of a cluster. We have a very large cluster and after adding 100 nodes every month or so, this becomes painful. Almost all of our nodes have over 600GB+ each. We have an application that will require us to be rebalancing at all times to reduce our hot spots.&lt;/p&gt;
</comment>
                            <comment id="13155592" author="scode" created="Wed, 23 Nov 2011 00:51:51 +0000"  >&lt;p&gt;We ended up going for the simpler rebuild patch as Chris hinted at. I&apos;ll quote myself:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I&apos;ve been looking at this some more.&lt;/p&gt;

&lt;p&gt;Here&apos;s the proposal so far:&lt;/p&gt;

&lt;p&gt;We introduce a GHOST state, in which a node receives neither reads nor writes. This allows us to bring in a group of nodes in the ring without suffering any ill effects. It is completely invisible to reads and writes, and will never count towards e.g. consistency level.&lt;/p&gt;

&lt;p&gt;Once all ghosts are ready, we can start bootstrapping, taking ghost nodes into accounts for purposes of determining which range we are responsible for, but streaming only from non-ghost nodes. This accomplishes the goal of not transferring more data than necessary.&lt;/p&gt;

&lt;p&gt;In order to avoid a bootstrapping node from taking writes for more than it&apos;s eventual share, we&apos;d have to make the write endpoints be aware of ghost nodes. This is dosable, but not critical since we&apos;re bisecting a range that was previously handled by a single node anyway so the traffic would be managable. It would just be cleaner to not have to cleanup afterwards.&lt;/p&gt;

&lt;p&gt;Once we transition from bootstrapping to being &quot;up&quot;, we have a bigger problem however. If the read paths and write paths are only aware of non-ghost nodes, the read/write paths would think that these nodes had more ownership than they really do.&lt;/p&gt;

&lt;p&gt;So we must really be taking into account the other nodes in the read/write path as well - but only when determining ownership of a completely bootstrapped node that was previously a ghost. That means we must distinguish between a &quot;normally up&quot; node and one that&apos;s been bootstraped from ghost state (call it &quot;ghost strapped&quot;).&lt;/p&gt;

&lt;p&gt;This suddenly gets complex. The process of group bootstrap would then be:&lt;/p&gt;

&lt;p&gt;Add a bunch of nodes in GHOST state&lt;br/&gt;
Bootstrap all of them, each of them going into GHOSTSTRAPPED state&lt;br/&gt;
Once all are GHOSTSTRAPPED, we can safely transntion from GHOSTSTRAPPED to normal/up&lt;br/&gt;
Is there a simpler solution?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;After some additional discussion we felt this was adding to much complexity and potential edge cases/bugs that it became more cost-effective to just go with the simple rebuild for our immediate needs, hoping to address the problem of adding lots of nodes to a DC separately in some other way.&lt;/p&gt;

&lt;p&gt;A patch is forthcoming soon.&lt;/p&gt;</comment>
                            <comment id="13160214" author="scode" created="Wed, 30 Nov 2011 18:29:06 +0000"  >&lt;p&gt;Here is a patch rebased against 0.8 for cursory review. I do not expect this to go into 0.8, and in fact I have not tested this patch other than build against vanilla 0.8 (the original patch is tested, but against our internal 0.8).&lt;/p&gt;

&lt;p&gt;If there are no concerns with the overall implementation, I&apos;ll submit a rebased version for 1.0/trunk.&lt;/p&gt;

&lt;p&gt;There are two components of the change:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Breaking out the streaming part of BootStrapper into a separate RangeStreamer. Change BootStrapper to use that.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Implement the rebuild command on top of RangeStreamer.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;There are two ways to invoke rebuild:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;nodetool rebuild
nodetool rebuild nameofdc
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The first form streams from nearest endpoints, while the latter streams from nearest endpoints in the specified data center.&lt;/p&gt;
</comment>
                            <comment id="13160223" author="brandon.williams" created="Wed, 30 Nov 2011 18:40:44 +0000"  >&lt;blockquote&gt;&lt;p&gt;If there are no concerns with the overall implementation, I&apos;ll submit a rebased version for 1.0/trunk.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This looks good to me, I like the RangeStreamer abstraction.&lt;/p&gt;</comment>
                            <comment id="13162555" author="scode" created="Mon, 5 Dec 2011 01:40:03 +0000"  >&lt;p&gt;Attached is a version rebased against 1.0 (and tested).&lt;/p&gt;</comment>
                            <comment id="13162857" author="JIRAUSER308715" created="Mon, 5 Dec 2011 16:09:23 +0000"  >&lt;p&gt;Once this option is in, is this the procedure for running rebuild (with 4 changed to &apos;rebuild dc1&apos;)?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;1. strategy options dc2:0&lt;br/&gt;
2. bring up new nodes in dc2 with auto_bootstrap off and token set&lt;br/&gt;
3. set strategy options dc2:3&lt;br/&gt;
4. run &apos;rebuild&apos; on each node in dc2&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do we need to stagger issuing the rebuild commands or can they be run all at once?&lt;/p&gt;</comment>
                            <comment id="13162971" author="scode" created="Mon, 5 Dec 2011 19:28:45 +0000"  >&lt;p&gt;Yes, that looks good.&lt;/p&gt;

&lt;p&gt;And yes, you can run rebuilds concurrently as long as you&apos;re comfortable with the amount of bandwidth you&apos;ll be pushing and the load you&apos;ll be putting on the source nodes.&lt;/p&gt;

&lt;p&gt;However, if you expect to see reasonable performance and streaming at full speed to all nodes, you also need &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-3494&quot; title=&quot;Streaming is mono-threaded (the bulk loader too by extension)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-3494&quot;&gt;&lt;del&gt;CASSANDRA-3494&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Regardless: I strongly recommend testing this with your exact version of Cassandra before trying it for real.&lt;/p&gt;</comment>
                            <comment id="13168924" author="slebresne" created="Wed, 14 Dec 2011 00:37:22 +0000"  >&lt;p&gt;I haven&apos;t applied the patch yet, it needs rebase and preferably against trunk since that is the likely target for this, but a few comments.&lt;/p&gt;

&lt;p&gt;We could have more reuse of code between Boostrapper ant the rebuild command.  Typically:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;RangeStreamer.getAllRangeWithSourcesFor does essentially the same thing that Boostrapper.getRangesWithSources, so it would be nice to do some reuse.&lt;/li&gt;
	&lt;li&gt;In rebuild, we essentially have the code of Boostrapper.getWorkMap, again would be nice to do some code reuse.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think we should move all of those in RangeStreamer and ultimately Boostrapper.boostrap() should be just one call to rebuild with the right arguments (mostly the correct tokenMetada instance and the &quot;myRange&quot; collection).&lt;/p&gt;

&lt;p&gt;A few nits:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;rebuild code could be simplified slightly by using StorageService.getLocalRanges()&lt;/li&gt;
	&lt;li&gt;rebuild doesn&apos;t fully respect the code style.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13168975" author="scode" created="Wed, 14 Dec 2011 01:52:49 +0000"  >&lt;p&gt;I&apos;ll get it rebased once it&apos;s otherwise okay.&lt;/p&gt;

&lt;p&gt;As for re-use: I had intermediate versions that tried to do this, but ever time I ended up realizing that it was exploding in verbosity at the point where I was using the abstraction so it didn&apos;t actually help. However, I think there were a few changes towards the end after which I didn&apos;t re-evaluate.&lt;/p&gt;

&lt;p&gt;I&apos;ll look at it again and see what I can do.&lt;/p&gt;</comment>
                            <comment id="13168976" author="scode" created="Wed, 14 Dec 2011 01:52:49 +0000"  >&lt;p&gt;I&apos;ll get it rebased once it&apos;s otherwise okay.&lt;/p&gt;

&lt;p&gt;As for re-use: I had intermediate versions that tried to do this, but ever time I ended up realizing that it was exploding in verbosity at the point where I was using the abstraction so it didn&apos;t actually help. However, I think there were a few changes towards the end after which I didn&apos;t re-evaluate.&lt;/p&gt;

&lt;p&gt;I&apos;ll look at it again and see what I can do.&lt;/p&gt;</comment>
                            <comment id="13175651" author="scode" created="Sat, 24 Dec 2011 04:08:43 +0000"  >&lt;p&gt;Attaching version rebased to trunk but not yet re-factored.&lt;/p&gt;</comment>
                            <comment id="13193339" author="jbellis" created="Wed, 25 Jan 2012 21:24:09 +0000"  >&lt;p&gt;Peter, are you planning to follow up on Sylvain&apos;s comments still?&lt;/p&gt;</comment>
                            <comment id="13195434" author="scode" created="Sat, 28 Jan 2012 05:31:28 +0000"  >&lt;p&gt;I do. I&apos;m sorry for the delay, this has been nagging me for quite some time. It&apos;s not forgotten, I have just been inundated with urgent stuff to do.&lt;/p&gt;

&lt;p&gt;I&apos;m attaching a fresh rebase against current trunk and I hope to submit an improved version later tonight (keyword being &quot;hope&quot;).&lt;/p&gt;</comment>
                            <comment id="13195447" author="scode" created="Sat, 28 Jan 2012 07:06:18 +0000"  >&lt;p&gt;&lt;tt&gt;CASSANDRA&amp;#45;3483&amp;#45;trunk&amp;#45;refactored&amp;#45;v1.txt&lt;/tt&gt; addresses the duplication between BootStrapper and RangeStreamer.&lt;/p&gt;

&lt;p&gt;Next patch will address rebuild/getworkmap duplication.&lt;/p&gt;</comment>
                            <comment id="13195448" author="scode" created="Sat, 28 Jan 2012 07:07:04 +0000"  >&lt;p&gt;(It also contains the addition of a brace from &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-3806&quot; title=&quot;merge from 1.0 (aa20c7206cdc1efc1983466de05c224eccac1084) breaks build&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-3806&quot;&gt;&lt;del&gt;CASSANDRA-3806&lt;/del&gt;&lt;/a&gt;; this is intentional to avoid pain.) &lt;/p&gt;</comment>
                            <comment id="13195449" author="scode" created="Sat, 28 Jan 2012 07:12:34 +0000"  >&lt;p&gt;I borked the unit test, will address that too.&lt;/p&gt;</comment>
                            <comment id="13195473" author="scode" created="Sat, 28 Jan 2012 10:00:16 +0000"  >&lt;p&gt;&lt;tt&gt;CASSANDRA&amp;#45;3483&amp;#45;trunk&amp;#45;refactored&amp;#45;v2.txt&lt;/tt&gt; I believe addresses the concerns, plus makes other improvements. I&apos;m much more happy with this one.&lt;/p&gt;

&lt;p&gt;It addresses &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-3807&quot; title=&quot;bootstrap can silently fail if sources are down for one or more ranges&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-3807&quot;&gt;&lt;del&gt;CASSANDRA-3807&lt;/del&gt;&lt;/a&gt; by supporting fetch &quot;consistency levels&quot; (though only ONE is currently usable without patching), and the filtering of hosts is abstracted out.&lt;/p&gt;

&lt;p&gt;There is still some duplication between &lt;tt&gt;Bootstrapper.bootstrap()&lt;/tt&gt; and &lt;tt&gt;StorageService.rebuild()&lt;/tt&gt; in that both do the dance of iteration over tables to construct the final map. I am not really feeling that abstracting away that is a good idea to include in this ticket, though I think it&apos;s worthwhile doing at some point separately.&lt;/p&gt;

&lt;p&gt;The unit test is fixed; my adjustment of it was wrong because I wasn&apos;t picking pending ranges (in the test).&lt;/p&gt;

&lt;p&gt;I&apos;ve tested both rebuild and bootstrap in a 3 node cluster.&lt;/p&gt;

&lt;p&gt;I&apos;ve added some more logging than what is typically the case; there have been several cases where I wished streaming was logged in more detail at INFO, particularly when bootstrapping or rebuilding. I think it&apos;s worthwhile to get that in while at it.&lt;/p&gt;</comment>
                            <comment id="13196090" author="slebresne" created="Mon, 30 Jan 2012 12:47:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;It addresses &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-3807&quot; title=&quot;bootstrap can silently fail if sources are down for one or more ranges&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-3807&quot;&gt;&lt;del&gt;CASSANDRA-3807&lt;/del&gt;&lt;/a&gt; by supporting fetch &quot;consistency levels&quot; (though only ONE is currently usable without patching)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think a first issue is that current bootstrap does not fail if no node is alive for a given range, which arguably it should. I&apos;m good with doing that, though it would be worth backporting to 1.0 too so it may be worth splitting that to a separate patch (or rather just create one for the fix in 1.0).&lt;/p&gt;

&lt;p&gt;However, that does not solve the problem of bootstrap possibly breaking the consistency contract. The problem being that if we transfer a range from a node that happens to be lacking behind in term of consistency, and we end up replacing a node that was not lacking behind, we could break some consistency contracts. To fix that, I really only see only one solution right off the bat (which doesn&apos;t mean there isn&apos;t other): it is to ensure that for each range, we transfer it from (at least) the node we will replace for this range.&lt;/p&gt;

&lt;p&gt;I believe the FetchConsistencyLevel of this patch is making an attempt to fix this by allowing to fetch from more than one node. While it does make it less likely to break consistency, unless we fetch from all nodes (and thus the one we&apos;ll replace), we cannot be sure we won&apos;t break the consistency level for people that say write at CL.ALL and read at CL.ONE. Overall, I fully agree this is a problem that we should fix someone, but I&apos;m not sure the FetchConsistencyLevel is the right solution and even if it is it&apos;s a complicated enough problem that it&apos;s worth it&apos;s own ticket. I would agree that the problem with rebuild is a little bit different, but since anyway the patch introduce FCL without using it, let&apos;s keep that for later if that&apos;s ok.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;There is still some duplication between Bootstrapper.bootstrap() and StorageService.rebuild() in that both do the dance of iteration over tables to construct the final map. I am not really feeling that abstracting away that is a good idea to include in this ticket&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it is, at least for a good chunk of it. It&apos;s not very complicated, it clearly improves code readability and since the patch already refactor that code I don&apos;t see a good reason to push that to later, especially if we agree it&apos;s worthwhile.&lt;/p&gt;

&lt;p&gt;Attaching a v3 that 1) remove FetchConsistencyLevel for the reasons above and 2) move most of the details of creating the multimaps in RangeStreamer.&lt;/p&gt;</comment>
                            <comment id="13196127" author="jbellis" created="Mon, 30 Jan 2012 14:04:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;Overall, I fully agree this is a problem that we should fix someone, but I&apos;m not sure the FetchConsistencyLevel is the right solution and even if it is it&apos;s a complicated enough problem that it&apos;s worth it&apos;s own &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-2434&quot; title=&quot;range movements can violate consistency&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-2434&quot;&gt;&lt;del&gt;CASSANDRA-2434&lt;/del&gt;&lt;/a&gt; isn&apos;t it?&lt;/p&gt;</comment>
                            <comment id="13196128" author="slebresne" created="Mon, 30 Jan 2012 14:09:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;This is &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-2434&quot; title=&quot;range movements can violate consistency&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-2434&quot;&gt;&lt;del&gt;CASSANDRA-2434&lt;/del&gt;&lt;/a&gt; isn&apos;t it?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;it is.&lt;/p&gt;</comment>
                            <comment id="13196149" author="jbellis" created="Mon, 30 Jan 2012 14:49:18 +0000"  >&lt;p&gt;cleanup patch addressing mostly typos and style.  only substantial code change was to RangeStreamer.getRangeFetchMap.  Also, moved OperationType.REBUILD to the end of the enum to make sure we don&apos;t break anything depending on ordinal.&lt;/p&gt;

&lt;p&gt;+1 from me otherwise.&lt;/p&gt;</comment>
                            <comment id="13196170" author="slebresne" created="Mon, 30 Jan 2012 15:24:36 +0000"  >&lt;p&gt;Committed v3 + Jonathan&apos;s cleanups (and a fix to the unit test).&lt;/p&gt;</comment>
                            <comment id="13196264" author="scode" created="Mon, 30 Jan 2012 18:12:02 +0000"  >&lt;p&gt;For the record I never intended to fix the general problem of bootstrapping never violating consistency. But in retrospect it&apos;s obvious how my choice of naming would make it sound like I did &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I agree it&apos;s a problem for its own ticket.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12540250">CASSANDRA-3807</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12540843">CASSANDRA-3833</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12512421" name="3483-cleanup.txt" size="12928" author="jbellis" created="Mon, 30 Jan 2012 14:49:18 +0000"/>
                            <attachment id="12512409" name="3483-v3.patch" size="27925" author="slebresne" created="Mon, 30 Jan 2012 12:47:15 +0000"/>
                            <attachment id="12505645" name="CASSANDRA-3483-0.8-prelim.txt" size="18643" author="scode" created="Wed, 30 Nov 2011 18:29:06 +0000"/>
                            <attachment id="12506076" name="CASSANDRA-3483-1.0.txt" size="17273" author="scode" created="Mon, 5 Dec 2011 01:40:03 +0000"/>
                            <attachment id="12508583" name="CASSANDRA-3483-trunk-noredesign.txt" size="17591" author="scode" created="Sat, 24 Dec 2011 04:08:43 +0000"/>
                            <attachment id="12512290" name="CASSANDRA-3483-trunk-rebase2.txt" size="17000" author="scode" created="Sat, 28 Jan 2012 05:31:28 +0000"/>
                            <attachment id="12512294" name="CASSANDRA-3483-trunk-refactored-v1.txt" size="21079" author="scode" created="Sat, 28 Jan 2012 07:06:18 +0000"/>
                            <attachment id="12512299" name="CASSANDRA-3483-trunk-refactored-v2.txt" size="27344" author="scode" created="Sat, 28 Jan 2012 10:00:15 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>8.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[scode]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>216833</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            13 years, 43 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0gkgn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>94745</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>slebresne</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[slebresne]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>