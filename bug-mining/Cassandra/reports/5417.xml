<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:16:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-15700] Performance regression on internode messaging</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-15700</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;Me and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jasonstack&quot; class=&quot;user-hover&quot; rel=&quot;jasonstack&quot;&gt;jasonstack&lt;/a&gt; have been investigating a performance regression affecting 4.0 during a 3 nodes, RF 3 write throughput test with a timeseries like workload, as shown in this plot, where blue is 3.11 and orange is 4.0:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12999226/12999226_Oss40vsOss311.png&quot; height=&quot;214&quot; width=&quot;389&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&#160;It&apos;s been a bit of a long investigation, but two clues ended up standing out:&lt;br/&gt;
1) An abnormal number of expired messages on 4.0 (as shown in the attached  system log), while 3.11 has almost none.&lt;br/&gt;
2) An abnormal GC activity (as shown in the attached gc log).&lt;/p&gt;

&lt;p&gt;Turns out the two are related, as the &lt;a href=&quot;https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/OutboundConnection.java#L462&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;on expired callback&lt;/a&gt; creates a huge amount of strings in the &lt;tt&gt;id()&lt;/tt&gt; call. The next question is what causes all those message expirations; we thoroughly reviewed the internode messaging code and the only issue we could find so far is related to the &quot;batch pruning&quot; calls &lt;a href=&quot;https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/OutboundMessageQueue.java#L81&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/OutboundMessageQueue.java#L188&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;: it &lt;em&gt;seems&lt;/em&gt; too much time is spent on those, causing the event loop to fall behind in processing the rest of the messages, which will end up being expired. This is supported by the analysis of the collapsed stacks (after fixing the GC issue):&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;(tprint (top-aggregated-calls oss40nogc &quot;EventLoopDelivery:doRun&quot; 5))
org/apache/cassandra/net/OutboundConnection$EventLoopDelivery:doRun 3456
org/apache/cassandra/net/OutboundMessageQueue:access$600 1621
org/apache/cassandra/net/PrunableArrayQueue:prune 1621
org/apache/cassandra/net/OutboundMessageQueue$WithLock:close 1621
org/apache/cassandra/net/OutboundMessageQueue:pruneInternalQueueWithLock 1620
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Those are the top 5 sampled calls from &lt;tt&gt;EventLoopDelivery#doRun()&lt;/tt&gt; which spends half of its time pruning. But only a tiny portion of such pruning time is spent actually expiring:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;(tprint (top-aggregated-calls oss40nogc &quot;OutboundMessageQueue:pruneInternalQueueWithLock&quot; 5))
org/apache/cassandra/net/OutboundMessageQueue:pruneInternalQueueWithLock 1900
org/apache/cassandra/net/PrunableArrayQueue:prune 1894
org/apache/cassandra/net/OutboundMessageQueue$1Pruner:onPruned 147
org/apache/cassandra/net/OutboundConnection$$Lambda$444/740904487:accept 147
org/apache/cassandra/net/OutboundConnection:onExpired 147
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And indeed, the &lt;tt&gt;PrunableArrayQueue:prune()&lt;/tt&gt; self time is dominant:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;(tprint (top-self-calls oss40nogc &quot;PrunableArrayQueue:prune&quot; 5))
org/apache/cassandra/net/PrunableArrayQueue:prune 1718
org/apache/cassandra/net/OutboundConnection:releaseCapacity 27
java/util/concurrent/ConcurrentHashMap:replaceNode 19
java/util/concurrent/ConcurrentLinkedQueue:offer 16
java/util/concurrent/LinkedBlockingQueue:offer 15
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That said, before proceeding with a PR to fix those issues, I&apos;d like to understand: what&apos;s the reason to prune so often, rather than just when polling the message during delivery? If there&apos;s a reason I&apos;m missing, let&apos;s talk about how to optimize pruning, otherwise let&apos;s get rid of that.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13296801">CASSANDRA-15700</key>
            <summary>Performance regression on internode messaging</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sbtourist">Sergio Bossa</assignee>
                                    <reporter username="sbtourist">Sergio Bossa</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Tue, 7 Apr 2020 11:00:11 +0000</created>
                <updated>Wed, 16 Mar 2022 13:21:18 +0000</updated>
                            <resolved>Mon, 29 Jun 2020 16:57:16 +0000</resolved>
                                        <fixVersion>4.0-beta1</fixVersion>
                    <fixVersion>4.0</fixVersion>
                                    <component>Messaging/Internode</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="1200">20m</timespent>
                                <comments>
                            <comment id="17077168" author="benedict" created="Tue, 7 Apr 2020 12:18:05 +0000"  >&lt;p&gt;Thanks for the bug report, this is very valuable.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;what&apos;s the reason to prune so often, rather than just when polling the message during delivery?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is the only way (presently) to know what deadline the eventLoop needs to wake up on, to ensure we do not let expiring messages accumulate indefinitely, without maintaining an auxiliary structure of expirations or introducing a more sophisticated queue for maintaining QoS.  It is not an astonishing surprise to see this report, though we had been reassured by this path&apos;s absence so far in other testing, as we consciously implemented a suboptimal approach to keep the patch tractable.  We expected that this work would be self limiting - a cluster &lt;em&gt;shouldn&apos;t&lt;/em&gt; have a backlog that dominates unless there are network problems, or if the receiving node is getting backed up, in which case you would expect expiry itself to dominate since the cost of pruning &lt;em&gt;should&lt;/em&gt; be quite low.  Clearly that isn&apos;t the case here; fortunately there are numerous solutions to this problem, none of which are super onerous.&lt;/p&gt;</comment>
                            <comment id="17077179" author="sbtourist" created="Tue, 7 Apr 2020 12:35:44 +0000"  >&lt;blockquote&gt;&lt;p&gt;This is the only way (presently) to know what deadline the eventLoop needs to wake up on, to ensure we do not let expiring messages accumulate indefinitely&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;m not sure I follow, it would be great if you could elaborate more, maybe with some pointers on the code. AFAIU, the &quot;event loop&quot; (assuming you mean the &lt;tt&gt;EventLoopDelivery&lt;/tt&gt; class) is potentially scheduled at every new message, then rescheduling itself based on the number of pending bytes and the writable state; this means, as long as there are pending bytes, and even if no new messages arrive, it will keep rescheduling itself and getting rid of expired messages along the way, so I don&apos;t see how they could accumulate indefinitely, although I might very well missing something. Or you just mean such &quot;rescheduling to expire&quot; is simply inefficient?&lt;/p&gt;</comment>
                            <comment id="17077246" author="benedict" created="Tue, 7 Apr 2020 13:40:59 +0000"  >&lt;p&gt;None of the above factors &lt;em&gt;guarantee&lt;/em&gt; that work is scheduled promptly.  Delivery will only be scheduled at enqueue if it is not already processing outstanding work, and though iirc the enqueue operation will itself forcefully prune in some circumstances, there are anyway no guarantees that new messages will be arriving to this connection.  Delivery will only reschedule itself once the outbound TCP channel has room, and so in a network outage that does not terminate the connection or stalled recipient process (for example), where it is making no (or very slow) progress, it will not be woken up to process further pending messages.&lt;/p&gt;</comment>
                            <comment id="17077309" author="sbtourist" created="Tue, 7 Apr 2020 14:52:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;Delivery will only be scheduled at enqueue if it is not already processing outstanding work, and though iirc the enqueue operation will itself forcefully prune in some circumstances, there are anyway no guarantees that new messages will be arriving to this connection.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Correct.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Delivery will only reschedule itself once the outbound TCP channel has room, and so in a network outage that does not terminate the connection or stalled recipient process (for example), where it is making no (or very slow) progress, it will not be woken up to process further pending messages.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Isn&apos;t this a problem regardless, given the current implementation? That is, we currently prune upon:&lt;/p&gt;

&lt;p&gt;1) Receiving a new message.&lt;/p&gt;

&lt;p&gt;2) Finishing processing messages in an event loop run.&lt;/p&gt;

&lt;p&gt;As we said, we have no guarantee we&apos;ll keep receiving messages, which rules #1 out, and if the event loop is stalling (i.e. due to network issues or whatever) we&apos;ll not run the message processing loop anyway, which rules #2 out and leaves us with the same problem of leaving expired messages in the queue.&lt;/p&gt;

&lt;p&gt;If this really is the problem we want to solve, which is a bit of an edge case but fine, something as easy as scheduling a background pruning at the earliest expiry date before &quot;stalling&quot; (i.e. before setting the connection as unwritable) would do it (possibly using a Hashed Wheel Timer because the Netty scheduler sucks a bit at scheduled tasks).&lt;/p&gt;</comment>
                            <comment id="17077336" author="benedict" created="Tue, 7 Apr 2020 15:22:09 +0000"  >&lt;p&gt;So, I&apos;m actually misremembering: though I had thought we scheduled exactly what you suggest (for which we must use something like &lt;tt&gt;earliestExpiresAt&lt;/tt&gt;, and for which it would need to be maintained) we actually only use it to avoid wasted work performing a prune on enqueue when it is unnecessary to avoid head-of-line blocking.&lt;/p&gt;

&lt;p&gt;Either way, it&apos;s the same basic problem (i.e. that we only maintain the minimum value gating prune work), and the solution is probably to maintain some kind of minimal histogram of the expiration times, or otherwise pick a number that isn&apos;t the minimum but amortises the cost of refreshing the value over multiple operations, that permits us to prune at worst a little later than we might while bounding the number of expired messages we leave in place.&lt;/p&gt;

&lt;p&gt;It looks like we only schedule regular prunes when disconnected; we should probably fix that too.&lt;/p&gt;</comment>
                            <comment id="17077356" author="benedict" created="Tue, 7 Apr 2020 15:51:13 +0000"  >&lt;p&gt;Sorry, I realise I never made clear in either of my messages a critical fact: the point of these prunes is not primarily to prune, but to determine when we must next prune by.  It determines this precisely today, and only to the next precise point, which is of course invalidated whenever we deliver the soonest-to-expire message.  However it should probably be approximate, and potentially also determine multiple such points at once, so that the linear cost of deciding this can be amortised effectively.&lt;/p&gt;</comment>
                            <comment id="17077469" author="sbtourist" created="Tue, 7 Apr 2020 17:50:03 +0000"  >&lt;blockquote&gt;&lt;p&gt;So, I&apos;m actually misremembering: though I had thought we scheduled exactly what you suggest&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It&apos;s ok, I do also forget most stuff past the 24hrs cut-off&#160;&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;On a more serious note, I get your point about the need to be precise VS amortising costs, but wondering what data we have to help with such decision, if any.&lt;/p&gt;

&lt;p&gt;We definitely have evidence that the &lt;em&gt;current&lt;/em&gt;&#160;implementation massively affects performance, and given the need to stabilise the codebase upon the 4.0 release, I would personally opt for a fix which doesn&apos;t increase overall complexity, and rather tries to strike a balance between said complexity and desired accuracy and performance, along the lines of:&lt;/p&gt;

&lt;p&gt;1) Remove pruning and expire time tracking from enqueue.&lt;/p&gt;

&lt;p&gt;2) Remove pruning from the event loop delivery run.&lt;/p&gt;

&lt;p&gt;3) During event loop delivery:&lt;/p&gt;

&lt;p&gt;a) Compute a weighted average of each message expire time.&lt;/p&gt;

&lt;p&gt;b) If potentially stalling (flushing over the high water mark), schedule a background pruning using the computed expire time.&lt;/p&gt;

&lt;p&gt;c) If resuming from stalling before the pruning deadline, cancel it (because we&apos;ll keep pruning as we process).&#160;&lt;/p&gt;

&lt;p&gt;This should be reasonably easy to implement, have decent accuracy, and perform well, as the expire deadline would be computed on the single threaded event loop (no contention) and the pruner would only run if actually stalling (which should &lt;b&gt;not&lt;/b&gt; be the case most of the time).&lt;/p&gt;

&lt;p&gt;How does that sound? Am I missing anything?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17077512" author="benedict" created="Tue, 7 Apr 2020 19:12:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;Remove pruning and expire time tracking from enqueue.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This would be a regression from 3.0 unfortunately, though the semantic for 3.0 was a to use the head of the list, which we could potentially restore (not necessarily as trivially as might be desired, but it is achievable).&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Compute a weighted average of each message expire time.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I assume you mean to maintain a guess of the number of items we &lt;em&gt;on average&lt;/em&gt; have in the queue? In which case I am fairly strongly -1 on this category approach. One of the features running through this work is that we make strong guarantees, and this would weaken that. &#160;I would prefer to be able to stipulate that (e.g.) we never reject an &lt;tt&gt;enqueue&lt;/tt&gt;&#160;if more than 20% of the queue is already expired.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I would personally opt for a fix which doesn&apos;t increase overall complexity&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Complexity is in the eye of the beholder &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&#160; I think the main goal should be minimising risk to delivery. The stability of this patch has been tested &lt;em&gt;fairly&lt;/em&gt; extensively and so far held up, so messing with important control flow semantics is something I would prefer to avoid.&lt;/p&gt;

&lt;p&gt;I think the least risky approach is to change how we compute and select the expiration time we use for triggering an expiration. This also has the benefit of maintaining well-define guarantees, is simple, and modifies no behaviours besides the selection of this value. For instance, picking an expiration time that we expect to contain a certain amount of data, then simply tracking &lt;em&gt;accurately&lt;/em&gt; on the delivery thread that it really does so. Once it doesn&apos;t, we prune to pick a better value.&lt;/p&gt;

&lt;p&gt;More specifically (e.g.), you pick a time &lt;tt&gt;t&lt;/tt&gt;, such that you expect the set of messages with expiration less than &lt;tt&gt;t&lt;/tt&gt;, say &lt;tt&gt;M(&amp;lt;t)&lt;/tt&gt;, should collectively occupy no more than 20% of the connection&apos;s data or item limit. As you introduce new messages, you simply record the total amount of data represented by &lt;tt&gt;M(&amp;lt;t)&lt;/tt&gt;, and if it crosses the threshold, you recompute picking a new &lt;tt&gt;t&lt;/tt&gt;. Assuming a random distribution of expiration this should be amortised constant time, as whatever &lt;tt&gt;t&lt;/tt&gt; we pick first, our new &lt;tt&gt;t&lt;/tt&gt; will be recomputed only &lt;tt&gt;O(1/n)&lt;/tt&gt; times, at &lt;tt&gt;O&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/thumbs_down.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/tt&gt; cost.&lt;/p&gt;

&lt;p&gt;This has the advantage of changing none of the important behaviours, so requires only isolated analysis of this particular calculation, and the calculation is simple.&lt;/p&gt;

&lt;p&gt;FWIW, we should also exclude the possibility that there really are more legitimate expirations occurring. The new patch properly enforces timeouts for an operation &lt;em&gt;and its responses&lt;/em&gt;, so we may also be seeing a different behaviour as we enforce these timeouts accurately. &#160;3.0 was much more lax in this regard, and there is a real chance that this is involved given the presence of real expirations. &#160;It &lt;em&gt;might&lt;/em&gt;&#160;be that the time spent pruning is immaterial given the impact on progress of this behavioural change. Previously we may have been happily serving operations that had actually had their timeout elapse (for instance). Unfortunately I don&apos;t know enough about the workload you are running to say much with confidence. Either way, this is a good opportunity to patch this known weakness, but further investigation might be warranted to isolate possibly multiple causes of the aggregate behaviour you are seeing.&lt;/p&gt;

&lt;p&gt;I won&apos;t be able to participate in this discussion for a week or two, but I expect &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aleksey&quot; class=&quot;user-hover&quot; rel=&quot;aleksey&quot;&gt;aleksey&lt;/a&gt; will get involved soon. Either way, there is no need to solve this on such a short time horizon I don&apos;t think.&lt;/p&gt;</comment>
                            <comment id="17077675" author="benedict" created="Tue, 7 Apr 2020 23:58:02 +0000"  >&lt;p&gt;I think anyway I&apos;ve let us get ahead of ourselves, in my race to respond before disappearing on leave for a period. &#160;I hadn&apos;t had time to really look at the code. &#160;Aleksey will be better placed to talk about this, as his memory and understanding of the pruner is likely superior to mine. &#160;The semantics of pruning are actually more intrinsically linked to the queuing than I had remembered, so all of the above suggestions by both of us probably miss the mark. &#160;Perhaps Aleksey and I can discuss it when I return from leave, and we can make a concrete proposal.&lt;/p&gt;

&lt;p&gt;It remains a relatively isolated problem, and nothing that I would be concerned about causing a delay to GA.&lt;/p&gt;

&lt;p&gt;In the meantime, it would be great if you could investigate the possibility that the modified timeout semantics for internode message delivery could also be involved.&lt;/p&gt;</comment>
                            <comment id="17078042" author="sbtourist" created="Wed, 8 Apr 2020 10:38:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;This would be a regression from 3.0 unfortunately&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Oh that&apos;s a good point, forgot we do that in 3.0+ too. We can keep the &lt;tt&gt;enqueue()&lt;/tt&gt; pruning, as that&apos;s not the worst offender (see collapsed stacks).&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I assume you mean to maintain a guess of the number of items we&#160;&lt;em&gt;on average&lt;/em&gt;&#160;have in the queue?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;No, I meant to compute the &quot;next expire time&quot; as an approximation of the expire time of the processed messages, rather than relying on exactly computing it via the pruner at every event loop delivery run.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;One of the features running through this work is that we make strong guarantees, and this would weaken that. &#160;I would prefer to be able to stipulate that (e.g.) we never reject an&#160;&lt;tt&gt;enqueue&lt;/tt&gt;&#160;if more than 20% of the queue is already expired.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;How is that related to what we&apos;re discussing? Enqueuing new messages is regulated by memory limiting, so that&apos;s what gives us strong guarantees; also, we do already prune the backlog if memory limits are met.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;messing with important control flow semantics is something I would prefer to avoid.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;d still like to understand how the pruning approach we&apos;re discussing here is important to the control flow semantics at all, as I don&apos;t think I&apos;ve got a clear answer yet, although it might be me missing the point. What I&apos;ve heard/understood is:&lt;/p&gt;

&lt;p&gt;1) It protects us against saturating memory upon network stalls.&lt;/p&gt;

&lt;p&gt;2) It protects us against saturating memory upon too many expired messages.&lt;/p&gt;

&lt;p&gt;AFAIU, none of those is accurate, as the current implementation doesn&apos;t satisfy #1, and #2 is covered by the memory limits implementation.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I think the least risky approach is to change how we compute and select the expiration time we use for triggering an expiration. This also has the benefit of maintaining well-define guarantees, is simple, and modifies no behaviours besides the selection of this value.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is something that can definitely be tried first, to reduce the amount of pruner runs. I will test this next.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;More specifically (e.g.), you pick a time&#160;&lt;tt&gt;t&lt;/tt&gt;, such that you expect the set of messages with expiration less than&#160;&lt;tt&gt;t&lt;/tt&gt;, say&#160;&lt;tt&gt;M(&amp;lt;t)&lt;/tt&gt;, should collectively occupy no more than 20% of the connection&apos;s data or item limit.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This is an example of borderline unnecessary complexity in my book &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&#160;The reason is, memory limits are already implemented via a separate mechanism and controlled by 3 different config properties (&lt;tt&gt;internode_application_send_queue_*&lt;/tt&gt;), so why adding an additional mechanism and 4th config property (your 20% threshold)? In other words, my definition of complexity is whatever additional piece of code or configuration which is not justified by an actual use case or doesn&apos;t solve an actual problem, and I can hardly see what problem we&apos;re trying to solve with that. If I&apos;m missing anything, happy to hear further explanations.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;FWIW, we should also exclude the possibility that there really are more legitimate expirations occurring. The new patch properly enforces timeouts for an operation&#160;&lt;em&gt;and its responses&lt;/em&gt;, so we may also be seeing a different behaviour as we enforce these timeouts accurately.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Good point, I wouldn&apos;t exclude that, but after a thorough review (as thorough as a 2 days review of a large piece of unknown code can be) we leaned against that being the case because:&lt;/p&gt;

&lt;p&gt;1) We disabled cross node timeouts, to make 4.0 behave as similarly as possible to 3.11, and to remove some of the complexity around computing the expire time based on the creation time (which being done via cross-node time translations could be suffering from subtle bugs).&lt;/p&gt;

&lt;p&gt;2) The collapsed stacks clearly show most time is spent by pruning itself, that is by iterating the queue, rather than by expiring messages: if time were spent by actually expiring messages, it would have shown in the stacks.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;It remains a relatively isolated problem, and nothing that I would be concerned about causing a delay to GA.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;This honestly worries me quite a bit: on what ground you say this is an isolated problem? And on what ground isn&apos;t a massive performance regression a concern for our GA date?&lt;/p&gt;</comment>
                            <comment id="17078265" author="benedict" created="Wed, 8 Apr 2020 13:15:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;on what ground you say this is an isolated problem?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This appears to be an issue in communication: the word isolated refers to the problem &lt;em&gt;code&lt;/em&gt;.  Please be assured I consider this a &lt;em&gt;serious&lt;/em&gt; problem, I am just unconcerned about (any difficulty) resolving it.  There is no need to panic and rush a fix.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&#8217;d still like to understand how the pruning approach we&apos;re discussing here is important to the control flow semantics at all&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In the word of Dirk Gently: everything&#8217;s connected &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  Your prior alternative proposal to replace the existing semantics involved two distinct changes to control flow, namely introducing a hash timer wheel (something I&#8217;m in favour of generally, but demonstrably a control flow change, and preferable to defer until 5.0) and eliminating the expiry on enqueue.  I just consider these kinds of change to be riskier at this stage.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The collapsed stacks clearly show most time is spent by pruning itself, that is by iterating the queue, rather than by expiring messages&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I believe it shows as much as 10% of time in real expirations?  That is not insignificant, and given how relatively cheap evaluating an expiration is, it &lt;em&gt;may&lt;/em&gt; well be the case that the algorithmic inefficiency we are discussing is incidental to the behaviour.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;to make 4.0 behave as similarly as possible to 3.11&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This leaves a significant gap still: 4.0 will use the local node message arrival to determine the timeout for its response, so there is still plenty of scope for messages to expire ahead of 3.x&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;No, I meant to compute the &quot;next expire time&quot; as an approximation of the expire time of the processed messages, rather than relying on exactly computing it via the pruner at every event loop delivery run.&lt;/p&gt;&lt;/blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;We can keep the enqueue() pruning, as that&apos;s not the worst offender (see collapsed stacks).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The enqueue pruning is cheap because we compute the minimum expiration time, so it is infrequently called; if we only guess this number now, we offer no guarantees the balance of new messages dropped in favour of expired messages.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;so why adding an additional mechanism and 4th config property (your 20% threshold)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is this an additional mechanism? We already have a mechanism, we just pick our number differently.  Guessing at a number is also a mechanism, surely?  How would we configure the guess algorithm, and why wouldn&#8217;t we expose its parameters?  I had assumed we would not make this configurable, in the same way we would not make the assumptions of any guess algorithm configurable, since its purpose is just to guarantee algorithmic complexity and bound how far from our memory limits we permit expired messages to be preferred over unexpired messages.&lt;/p&gt;

&lt;p&gt;Honestly though it sounds like there is not much between our proposals now, which is reassuring, so perhaps we should focus on the common ground we have.  However, I am now on leave so I would appreciate it if you can be patient until I return to continue this discussion.  I would also like to look more closely again at the existing behaviour, as the pruning is closely related to the migration of new records from the MPSC queue to the internal queue.  I have been trying to respond promptly to your queries, but I feel that in doing so my responses have not been sufficiently well considered, and I would prefer to take time to produce a complete and coherent view and proposal.  Is that acceptable to you?&lt;/p&gt;</comment>
                            <comment id="17078510" author="sbtourist" created="Wed, 8 Apr 2020 17:29:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;Honestly though it sounds like there is not much between our proposals now, which is reassuring, so perhaps we should focus on the common ground we have.&#160;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Indeed, I think we agree on the necessity to fix this in the least risky way, which it seems to mean keeping the current pruning implementation and accuracy, but avoiding it to be run &lt;b&gt;at every single delivery&lt;/b&gt;&#160;(although maybe you dispute this being the problem, but the collapsed stacks speak soundly about that).&lt;/p&gt;

&lt;p&gt;In the spirit of that, I&apos;ve fixed the message queue algorithm to compute the expiration deadline in a way that can be used to actually run the pruning task &lt;em&gt;only after such deadline&lt;/em&gt;. I&apos;ll give it another review on my own and possibly add more unit tests (please note the current implementation seems to had none at all), but performance tests now look much better (orange is patched 4.0):&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12999352/12999352_Oss40patchedvsOss311.png&quot; height=&quot;299&quot; width=&quot;556&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here&apos;s the branch:&#160;&lt;a href=&quot;https://github.com/sbtourist/cassandra/commits/CASSANDRA-15700&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/sbtourist/cassandra/commits/CASSANDRA-15700&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I understand there&apos;s no &lt;em&gt;panic&lt;/em&gt; to fix it, and you&apos;ll be away the next couple weeks, but this means realistically postponing this issue for at least 3 weeks, which will add to the delay we&apos;re already accumulating in 4.0 for other reasons, so maybe you could delegate this to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aleksey&quot; class=&quot;user-hover&quot; rel=&quot;aleksey&quot;&gt;aleksey&lt;/a&gt;&#160;as you mentioned?&lt;/p&gt;</comment>
                            <comment id="17078557" author="iamaleksey" created="Wed, 8 Apr 2020 18:10:12 +0000"  >&lt;p&gt;I&apos;ll take a look soonish.&lt;/p&gt;</comment>
                            <comment id="17078610" author="benedict" created="Wed, 8 Apr 2020 19:07:21 +0000"  >&lt;p&gt;Your proposed solution sounds great, so there&apos;s not much use for me here.  Aleksey is the more natural reviewer anyway.&lt;/p&gt;</comment>
                            <comment id="17079102" author="sbtourist" created="Thu, 9 Apr 2020 09:26:39 +0000"  >&lt;p&gt;Thank you both.&lt;/p&gt;</comment>
                            <comment id="17079208" author="sbtourist" created="Thu, 9 Apr 2020 11:17:56 +0000"  >&lt;p&gt;Added some more thorough unit tests in the meantime.&lt;/p&gt;</comment>
                            <comment id="17112255" author="iamaleksey" created="Wed, 20 May 2020 14:14:25 +0000"  >&lt;p&gt;On &lt;tt&gt;id()&lt;/tt&gt; change: the only frequent callers of it are &lt;tt&gt;onOverloaded()&lt;/tt&gt; and &lt;tt&gt;onExpired()&lt;/tt&gt; callbacks, via &lt;tt&gt;noSpamLogger.warn()&lt;/tt&gt;. To get rid of these we could pass &lt;tt&gt;OutboundConnection&lt;/tt&gt; instance as argument itself instead of explicitly invoking &lt;tt&gt;id()&lt;/tt&gt;, as &lt;tt&gt;toString()&lt;/tt&gt; is overloaded to return &lt;tt&gt;id()&lt;/tt&gt;, and will only be invoked by formatter if we should log - no more than once in every 30 seconds. I think the code is a bit cleaner without that change, though don&apos;t mind it too strongly.&lt;/p&gt;</comment>
                            <comment id="17112333" author="benedict" created="Wed, 20 May 2020 15:11:20 +0000"  >&lt;p&gt;That part could also be left to &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-15766&quot; title=&quot;NoSpamLogger arguments building objects on hot paths&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-15766&quot;&gt;&lt;del&gt;CASSANDRA-15766&lt;/del&gt;&lt;/a&gt;, which I think is also addressing this and will do so in a consistent manner across various NoSpamLogger call sites.&lt;/p&gt;</comment>
                            <comment id="17124959" author="iamaleksey" created="Wed, 3 Jun 2020 13:32:48 +0000"  >&lt;p&gt;I &lt;b&gt;think&lt;/b&gt; the way we update times at the end of pruning in this patch is potentially problematic, but I might be getting this wrong, so pardon me for any obvious foolishness. Was thinking about this particular scenario:&lt;/p&gt;

&lt;p&gt;1. After partial consumption/delivery, thread &lt;tt&gt;t1&lt;/tt&gt; updates &lt;tt&gt;earliestExpiryTime&lt;/tt&gt; with some high value (say, all messages up to now have been with long timeouts); gets descheduled for a period of time.&lt;br/&gt;
2. A different thread, &lt;tt&gt;t2&lt;/tt&gt;, invokes &lt;tt&gt;add()&lt;/tt&gt; with an lots of short-expiration messages, all skipping purging, as the lock is being held by &lt;tt&gt;t1&lt;/tt&gt;. Repeatedly updates both &lt;tt&gt;earliestExpiryTime&lt;/tt&gt; and &lt;tt&gt;nextExpirationDeadline&lt;/tt&gt; with a small value&lt;br/&gt;
3. Thread &lt;tt&gt;t1&lt;/tt&gt; is now scheduled again, sets &lt;tt&gt;nextExpirationDeadline&lt;/tt&gt; to the high value from step (1)&lt;/p&gt;

&lt;p&gt;If there are no &lt;tt&gt;add()&lt;/tt&gt; incoming and delivery has stalled, we can now hold an arbitrary # of messages added at step 2, as we use &lt;tt&gt;nextExpirationDeadline&lt;/tt&gt; value to determine whether or not pruning is necessary.&lt;/p&gt;</comment>
                            <comment id="17125211" author="sbtourist" created="Wed, 3 Jun 2020 18:24:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aleksey&quot; class=&quot;user-hover&quot; rel=&quot;aleksey&quot;&gt;aleksey&lt;/a&gt; thanks for reviewing: getting back to this after two months and having to rebuild the whole thing in my mind, made me realize how complex this is, and that I should have put a couple more comments, so apologies and kudos for getting through it all alone &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;That said, to answer your question straight: the race you point out is absolutely correct, and was unfortunately overlooked by myself, but there should be an easy fix. Before going with that, I think I have to clarify first why I introduced the new &lt;tt&gt;nextExpirationDeadline&lt;/tt&gt; variable along &lt;tt&gt;earliestExpiryTime&lt;/tt&gt;. Simply put, if we only track time via &lt;tt&gt;earliestExpiryTime&lt;/tt&gt; during both &lt;tt&gt;add&lt;/tt&gt; and &lt;tt&gt;prune&lt;/tt&gt;, we risk getting into a race where we accumulate an unbounded number of messages until the next expiration, as shown in the following scenario:&lt;br/&gt;
1. &lt;tt&gt;add&lt;/tt&gt; is called N times with &lt;tt&gt;earliestExpiryTime&lt;/tt&gt; set as the minimum time among the added messages; you can&apos;t adjust in this case by the current time otherwise you would never expire (as the time would always shift).&lt;br/&gt;
2. &lt;tt&gt;prune&lt;/tt&gt; is called and &lt;tt&gt;earliestExpiryTime&lt;/tt&gt; is set as the minimum between the minimum time among the pruned messages and the current expiry time.&lt;br/&gt;
3. This means that any messages arrived between the start and end of the pruning whose expiry time was not the minimum, but still less than the minimum expiry time among pruned messages, would be &quot;ignored&quot; and remain in the queue.&lt;/p&gt;

&lt;p&gt;At this point you might say it really sounds like the race you discovered and in a way it is &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Although in the above case the race window would be much larger (the whole &lt;tt&gt;prune&lt;/tt&gt;).&lt;/p&gt;

&lt;p&gt;Anyway, as I said there should be an easy fix: the deadline should be updated only if it&apos;s actually the minimum value (adjusted by current time), and I&apos;ve sent a new commit with such fix. I don&apos;t see a way to fix the same kind of races by just keeping a single variable, but let me know if you find any.&lt;/p&gt;</comment>
                            <comment id="17126005" author="iamaleksey" created="Thu, 4 Jun 2020 15:23:25 +0000"  >&lt;p&gt;Thanks for your patience overall. Just have a few nits now.&lt;/p&gt;

&lt;p&gt;1. Can you change &lt;tt&gt;maybeUpdateEarliestExpiryTime()&lt;/tt&gt; to have the same structure as &lt;tt&gt;maybeUpdateNextExpirationDeadline()&lt;/tt&gt;?&lt;br/&gt;
2. In &lt;tt&gt;add()&lt;/tt&gt;, use the return value of &lt;tt&gt;maybeUpdateExpiryTime()&lt;/tt&gt; instead of loading the current value (should be equivalent)?&lt;br/&gt;
3. Fix up whitespace in the last commit&lt;br/&gt;
4. Check &lt;tt&gt;Remover&lt;/tt&gt; logic? In particular I don&apos;t think &lt;tt&gt;earliestExpiresAt = Long.MAX_VALUE;&lt;/tt&gt; belongs there anymore.&lt;/p&gt;

&lt;p&gt;And, so long as you don&apos;t mind omitting the &lt;tt&gt;id()&lt;/tt&gt; commit, we can push.&lt;/p&gt;</comment>
                            <comment id="17145048" author="sbtourist" created="Thu, 25 Jun 2020 15:57:52 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aleksey&quot; class=&quot;user-hover&quot; rel=&quot;aleksey&quot;&gt;aleksey&lt;/a&gt; apologies for this late reply. I&apos;ve pushed the recommended changes, please have a look when you have a moment.&lt;/p&gt;</comment>
                            <comment id="17147963" author="iamaleksey" created="Mon, 29 Jun 2020 16:57:16 +0000"  >&lt;p&gt;Cheers, committed as &lt;a href=&quot;https://github.com/apache/cassandra/commit/56f24f78f62c9945fae40790e3ed09893fa1ed18&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;56f24f78f62c9945fae40790e3ed09893fa1ed18&lt;/a&gt; to trunk.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13301578">CASSANDRA-15766</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10001">
                    <name>dependent</name>
                                                                <inwardlinks description="is depended upon by">
                                        <issuelink>
            <issuekey id="13184748">CASSANDRA-14747</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12999352" name="Oss40patchedvsOss311.png" size="43217" author="sbtourist" created="Wed, 8 Apr 2020 17:19:07 +0000"/>
                            <attachment id="12999226" name="Oss40vsOss311.png" size="57362" author="sbtourist" created="Mon, 6 Apr 2020 22:59:38 +0000"/>
                            <attachment id="12999225" name="oss40.gc" size="607552" author="sbtourist" created="Mon, 6 Apr 2020 23:02:23 +0000"/>
                            <attachment id="12999223" name="oss40_nogc.tar.xz" size="696048" author="sbtourist" created="Tue, 7 Apr 2020 10:53:28 +0000"/>
                            <attachment id="12999224" name="oss40_system.log" size="1478173" author="sbtourist" created="Mon, 6 Apr 2020 23:02:24 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[sbtourist]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12313825" key="com.atlassian.jira.plugin.system.customfieldtypes:cascadingselect">
                        <customfieldname>Bug Category</customfieldname>
                        <customfieldvalues>
                                                    <customfieldvalue key="12984" cascade-level=""><![CDATA[Degradation]]></customfieldvalue>
                                <customfieldvalue key="12997" cascade-level="1"><![CDATA[Performance Bug/Regression]]></customfieldvalue>
            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12313821" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Complexity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12965"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313822" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Discovered By</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12973"><![CDATA[Performance Regression Test]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12313922" key="jira.plugin.projectspecificselectfield.jpssf:multicftype">
                        <customfieldname>Impacts</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="13100"><![CDATA[None]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 20 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12313921" key="jira.plugin.projectspecificselectfield.jpssf:multicftype">
                        <customfieldname>Platform</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="13076"><![CDATA[All]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0dd1k:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[aleksey]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311420" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12346093">4.0-alpha</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12314136" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12348281">4.0-alpha1</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313924" key="com.atlassian.jira.plugin.system.customfieldtypes:textfield">
                        <customfieldname>Source Control Link</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;p&gt;&lt;a href=&quot;https://github.com/apache/cassandra/commit/56f24f78f62c9945fae40790e3ed09893fa1ed18&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;56f24f78f62c9945fae40790e3ed09893fa1ed18&lt;/a&gt;&lt;/p&gt;</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313823" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Test and Documentation Plan</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;p&gt;See attached performance test plots. Also see unit tests.&lt;/p&gt;</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>