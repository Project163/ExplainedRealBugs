<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:36:53 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-4446] nodetool drain sometimes doesn&apos;t mark commitlog fully flushed</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-4446</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;I recently wiped a customer&apos;s QA cluster. I drained each node and verified that they were drained. When I restarted the nodes, I saw the commitlog replay create a memtable and then flush it. I have attached a sanitized log snippet from a representative node at the time. &lt;/p&gt;

&lt;p&gt;It appears to show the following :&lt;br/&gt;
1) Drain begins&lt;br/&gt;
2) Drain triggers flush&lt;br/&gt;
3) Flush triggers compaction&lt;br/&gt;
4) StorageService logs DRAINED message&lt;br/&gt;
5) compaction thread excepts&lt;br/&gt;
6) on restart, same CF creates a memtable&lt;br/&gt;
7) and then flushes it &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The columnfamily involved in the replay in 7) is the CF for which the compaction thread excepted in 5). This seems to suggest a timing issue whereby the exception in 5) prevents the flush in 3) from marking all the segments flushed, causing them to replay after restart.&lt;/p&gt;

&lt;p&gt;In case it might be relevant, I did an online change of compaction strategy from Leveled to SizeTiered during the uptime period preceding this drain.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; Isn&apos;t commitlog replay not supposed to automatically trigger a flush in modern cassandra?&lt;/p&gt;</description>
                <environment>&lt;p&gt;ubuntu 10.04 64bit&lt;br/&gt;
Linux HOSTNAME 2.6.32-345-ec2 #48-Ubuntu SMP Wed May 2 19:29:55 UTC 2012 x86_64 GNU/Linux&lt;br/&gt;
sun JVM&lt;br/&gt;
cassandra 1.0.10 installed from apache deb&lt;/p&gt;</environment>
        <key id="12599347">CASSANDRA-4446</key>
            <summary>nodetool drain sometimes doesn&apos;t mark commitlog fully flushed</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10003" iconUrl="https://issues.apache.org/jira/images/icons/priorities/trivial.svg">Low</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jbellis">Jonathan Ellis</assignee>
                                    <reporter username="rcoli">Robert Coli</reporter>
                        <labels>
                    </labels>
                <created>Wed, 18 Jul 2012 21:30:43 +0000</created>
                <updated>Tue, 16 Apr 2019 09:32:30 +0000</updated>
                            <resolved>Tue, 15 Jan 2013 15:14:04 +0000</resolved>
                                        <fixVersion>1.2.1</fixVersion>
                                    <component>Tool/nodetool</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>13</watches>
                                                                                                                <comments>
                            <comment id="13451241" author="scode" created="Sat, 8 Sep 2012 04:02:32 +0000"  >&lt;p&gt;In general, nodetool drain never seems to completely eliminate on-startup log replay. I observe this all the time on all clusters. It certainly cuts down the amount of replay done, but either never or fairly seldom eliminates it completely - at least not based on log messages indicating replay.&lt;/p&gt;

&lt;p&gt;Never had time to investigate.&lt;/p&gt;</comment>
                            <comment id="13471960" author="kmueller" created="Mon, 8 Oct 2012 23:40:42 +0000"  >&lt;p&gt;Also seeing this in an upgrade from 1.0.xx to 1.1.15:&lt;/p&gt;

&lt;p&gt; INFO 16:29:17,486 completed pre-loading (3 keys) key cache.&lt;br/&gt;
 INFO 16:29:17,495 Replaying /data2/commit-cassandra/CommitLog-1349727956484.log&lt;br/&gt;
 INFO 16:29:17,503 Replaying /data2/commit-cassandra/CommitLog-1349727956484.log&lt;br/&gt;
 INFO 16:29:18,495 GC for ParNew: 3506 ms for 4 collections, 1963062320 used; max is 17095983104&lt;br/&gt;
 INFO 16:29:18,498 Finished reading /data2/commit-cassandra/CommitLog-1349727956484.log&lt;br/&gt;
 INFO 16:29:18,499 Log replay complete, 0 replayed mutations&lt;/p&gt;


&lt;p&gt;This is a standard upgrade process which includes a drain&lt;/p&gt;</comment>
                            <comment id="13476213" author="omid" created="Mon, 15 Oct 2012 15:41:13 +0000"  >&lt;p&gt;I also experience this every time I drain / restart (up until latest 1.1.6 but not on 1.1.6 itself any more) and getting this message in log:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;2012-10-12_15:50:36.92191  INFO 15:50:36,921 Log replay complete, N replayed mutations   &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;with N being non-zero. I wonder if this is a cause of double-counts for Counter mutations.&lt;/p&gt;</comment>
                            <comment id="13501722" author="tamarfraenkel" created="Wed, 21 Nov 2012 06:36:37 +0000"  >&lt;p&gt;I had the same experience, when I upgraded my cluster from 1.0.9 to 1.0.11. I ran drain before the upgrade, upgrade on the node finished and node restarted at 2012-11-20 10:20:58, but then I see in the logs reply of commit log:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2012-11-20 09:41:13,918 CommitLog.java (line 172) Replaying /raid0/cassandra/commitlog/CommitLog-1353402218337.log&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2012-11-20 09:41:20,360 CommitLog.java (line 179) Log replay complete, 0 replayed mutations&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2012-11-20 10:11:35,635 CommitLog.java (line 167) No commitlog files found; skipping replay&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2012-11-20 10:21:11,631 CommitLog.java (line 172) Replaying /raid0/cassandra/commitlog/CommitLog-1353404473899.log&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2012-11-20 10:21:18,119 CommitLog.java (line 179) Log replay complete, 6413 replayed mutations&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2012-11-20 10:55:46,435 CommitLog.java (line 172) Replaying /raid0/cassandra/commitlog/CommitLog-1353406871619.log&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2012-11-20 10:55:54,139 CommitLog.java (line 179) Log replay complete, 3 replayed mutations&lt;/p&gt;&lt;/blockquote&gt; 
&lt;p&gt;This caused over increment of counters&lt;/p&gt;</comment>
                            <comment id="13501895" author="jbellis" created="Wed, 21 Nov 2012 12:00:06 +0000"  >&lt;p&gt;This is going to stand as a known limitation with 1.0.x; so far it looks like it is fixed in latest 1.1.&lt;/p&gt;</comment>
                            <comment id="13543446" author="mkjellman" created="Fri, 4 Jan 2013 00:16:26 +0000"  >&lt;p&gt;did a nodetool drain before 1.1.7 -&amp;gt; 1.2.0. upon starting 1.2.0 every node in my cluster still replayed the commit logs and created mutations.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt; INFO 13:17:06,529 DRAINING: starting drain process
 INFO 13:17:06,529 Stop listening to thrift clients
 INFO 13:17:06,532 Announcing shutdown
 INFO 13:17:07,536 Waiting &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; messaging service to quiesce
 INFO 13:17:07,537 MessagingService shutting down server thread.

... normal startup stuff..
 INFO 13:20:20,182 Replaying /ssd/commitlog/CommitLog-1355265349912.log
 INFO 13:20:24,166 Finished reading /ssd/commitlog/CommitLog-1355265349912.log
 INFO 13:20:24,166 Replaying /ssd/commitlog/CommitLog-1355265349914.log
 INFO 13:20:26,700 Finished reading /ssd/commitlog/CommitLog-1355265349914.log
 INFO 13:20:26,701 Replaying /ssd/commitlog/CommitLog-1355265349915.log
 INFO 13:20:28,118 Finished reading /ssd/commitlog/CommitLog-1355265349915.log
... more replay lines ...
 INFO 13:22:00,061 Log replay complete, 8052 replayed mutations
 INFO 13:22:00,358 Possible old-format hints found. Truncating
 INFO 13:22:00,370 Enqueuing flush of Memtable-local@1908923620(402/402 serialized/live bytes, 13 ops)
 INFO 13:22:00,372 Writing Memtable-local@1908923620(402/402 serialized/live bytes, 13 ops)
 INFO 13:22:00,494 Cassandra version: 1.2.0
 INFO 13:22:00,495 Thrift API version: 19.35.0
 INFO 13:22:00,495 CQL supported versions: 2.0.0,3.0.0 (&lt;span class=&quot;code-keyword&quot;&gt;default&lt;/span&gt;: 3.0.0)
 INFO 13:22:00,534 Loading persisted ring state
 INFO 13:22:00,537 Starting up server gossip
 WARN 13:22:00,557 No host ID found, created dd3a40e2-fef1-4574-87b8-e2929fd80235 (Note: This should happen exactly once per node).
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13543753" author="slebresne" created="Fri, 4 Jan 2013 10:11:04 +0000"  >&lt;p&gt;Let&apos;s reopen then if it doesn&apos;t sound like it&apos;s fixed in recent releases.&lt;/p&gt;</comment>
                            <comment id="13543789" author="arodrime" created="Fri, 4 Jan 2013 10:59:36 +0000"  >&lt;p&gt;+1. Good to see this ticket reopen. Drain didn&apos;t work for a while. I remove all the commit logs files before a restart to avoid counters operations to replay.&lt;/p&gt;</comment>
                            <comment id="13544210" author="tpatterson" created="Fri, 4 Jan 2013 20:11:21 +0000"  >&lt;p&gt;It always replays 3 mutations when I follow these steps:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;bin/cassandra
tools/bin/cassandra-stress --operation=INSERT --num-keys=100000
bin/nodetool drain
bin/cassandra
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This is on trunk, commit acf30622&lt;/p&gt;</comment>
                            <comment id="13551655" author="jbellis" created="Fri, 11 Jan 2013 23:46:43 +0000"  >&lt;p&gt;System tables were not getting flushed.  This is the source of the extra replaying.  Patch attached to fix this, and also parallelize flushing.&lt;/p&gt;</comment>
                            <comment id="13553083" author="yukim" created="Mon, 14 Jan 2013 20:28:02 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="13553181" author="jbellis" created="Mon, 14 Jan 2013 22:07:38 +0000"  >&lt;p&gt;Committed.&lt;/p&gt;

&lt;p&gt;Note that earlier releases can workaround by manually running flush against system KS before drain.&lt;/p&gt;</comment>
                            <comment id="13556790" author="rcoli" created="Fri, 18 Jan 2013 00:23:07 +0000"  >&lt;p&gt;While I&apos;m sure that this does fix one real cause of drain not working in trunk (yay!), one of the symptoms I&apos;ve heard reported in the 1.0.x - 1.1.5 timeframe is that &quot;my counters over-counted on upgrade, despite drain&quot;. Most recent report was 1.0.12-&amp;gt;1.1.8 with drain being run as part of the upgrade process.&lt;/p&gt;

&lt;p&gt;NEWS.txt says :&lt;/p&gt;

&lt;p&gt;&quot;If you using counters and upgrading from a version prior to 1.1.6, you should drain existing Cassandra nodes prior to the upgrade to prevent overcount during commitlog replay (see &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-4782&quot; title=&quot;Commitlog not replayed after restart&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-4782&quot;&gt;&lt;del&gt;CASSANDRA-4782&lt;/del&gt;&lt;/a&gt;).  For non-counter uses, drain is not required but is a good practice to minimize restart time.&quot;&lt;/p&gt;

&lt;p&gt;If drain in these versions can&apos;t be counted on (heh) to actually work for this purpose (which reports suggest it cannot), then I propose changing this line to read &quot;drain existing nodes and remove their commitlog&quot;.&lt;/p&gt;</comment>
                            <comment id="13556803" author="jbellis" created="Fri, 18 Jan 2013 00:34:30 +0000"  >&lt;p&gt;No counter mutations are double-counted, only the unflushed system changes are replayed.&lt;/p&gt;</comment>
                            <comment id="13556837" author="rcoli" created="Fri, 18 Jan 2013 01:04:28 +0000"  >&lt;p&gt;If only unflushed system changes are replayed, how do you account for :&lt;/p&gt;

&lt;p&gt;&quot;I upgraded from 1.0.12 to 1.1.8, using drain, and I noticed overcounting counters&quot; ?&lt;/p&gt;

&lt;p&gt;It&apos;s quite possible that upgrading from 1.1.x to 1.1.y&amp;gt;x does not in fact replay anything other than system keyspace and does not incur double counting of counters. I am however pretty confident based on multiple reports of the above quoted issue that counter increments may be over-replayed if one uses drain (as NEWS.txt suggests) while upgrading from 1.0.x to &amp;gt;1.1.6.&lt;/p&gt;

&lt;p&gt;If this is being dealt with as &quot;known limitation of 1.0.x&quot;, then I continue to suggest the above change to NEWS.txt, as otherwise people using counters in 1.0.x WILL incur double-increment while upgrading per the instructions in NEWS.txt.&lt;/p&gt;</comment>
                            <comment id="13556847" author="jbellis" created="Fri, 18 Jan 2013 01:11:07 +0000"  >&lt;p&gt;Show me how to reproduce it and I will re-evaluate my position, but as near as I can tell the advice in NEWS is still best practice.&lt;/p&gt;</comment>
                            <comment id="13557767" author="rcoli" created="Fri, 18 Jan 2013 23:45:47 +0000"  >&lt;p&gt;How to reproduce it, from the multiple reports :&lt;/p&gt;

&lt;p&gt;1) Drain and stop cluster with counters on 1.0.x&lt;br/&gt;
2) Start same cluster on 1.1.x&lt;br/&gt;
3) Notice commitlog replay of the counter columnfamily and that your counters have over-counted&lt;/p&gt;

&lt;p&gt;Attached is a log from the latest reporter, &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-4446&quot; title=&quot;nodetool drain sometimes doesn&amp;#39;t mark commitlog fully flushed&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-4446&quot;&gt;&lt;del&gt;CASSANDRA-4446&lt;/del&gt;&lt;/a&gt;--1.0.12_to_1.1.8.txt. It shows the following.&lt;/p&gt;

&lt;p&gt;1) Drain starts and completes on 1.0.12&lt;br/&gt;
2) Cluster then starts on 1.1.8, and replays the commit log&lt;br/&gt;
3) As part of commitlog replay, it flushes various CFs including titan3/RMEntityCount/, which is a counter columnfamily; machine has 4gb of heap and the flush is while thrift is down and the node has not jumped state to normal, so it seems reasonable to conjecture this flush is part of commitlog replay&lt;br/&gt;
4) It then logs &quot;10698 replayed mutations&quot;, which adds further support to the idea that these Counts are part of replay&lt;br/&gt;
5) Operator then noticed a significant percentage of records had overcounted in this columnfamily&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12664851">CASSANDRA-5911</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12564518" name="4446.txt" size="3598" author="jbellis" created="Fri, 11 Jan 2013 23:46:43 +0000"/>
                            <attachment id="12565581" name="CASSANDRA-4446--1.0.12_to_1.1.8.txt" size="10322" author="rcoli" created="Fri, 18 Jan 2013 23:46:09 +0000"/>
                            <attachment id="12537076" name="cassandra.1.0.10.replaying.log.after.exception.during.drain.txt" size="34350" author="rcoli" created="Wed, 18 Jul 2012 21:35:34 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[jbellis]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>245708</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 44 weeks, 3 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i06ctz:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>35013</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>yukim</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[yukim]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12961"><![CDATA[Low]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>