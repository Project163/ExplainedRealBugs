<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:47:35 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-7743] Possible C* OOM issue during long running test</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-7743</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;During a long running test, we ended up with a lot of &quot;java.lang.OutOfMemoryError: Direct buffer memory&quot; errors on the Cassandra instances.&lt;/p&gt;

&lt;p&gt;Here is an example of stacktrace from system.log :&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;ERROR [SharedPool-Worker-1] 2014-08-11 11:09:34,610 ErrorMessage.java:218 - Unexpected exception during request
java.lang.OutOfMemoryError: Direct buffer memory
        at java.nio.Bits.reserveMemory(Bits.java:658) ~[na:1.7.0_25]
        at java.nio.DirectByteBuffer.&amp;lt;init&amp;gt;(DirectByteBuffer.java:123) ~[na:1.7.0_25]
        at java.nio.ByteBuffer.allocateDirect(ByteBuffer.java:306) ~[na:1.7.0_25]
        at io.netty.buffer.PoolArena$DirectArena.newChunk(PoolArena.java:434) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.PoolArena.allocateNormal(PoolArena.java:179) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.PoolArena.allocate(PoolArena.java:168) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.PoolArena.allocate(PoolArena.java:98) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.PooledByteBufAllocator.newDirectBuffer(PooledByteBufAllocator.java:251) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:155) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.AbstractByteBufAllocator.directBuffer(AbstractByteBufAllocator.java:146) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.buffer.AbstractByteBufAllocator.ioBuffer(AbstractByteBufAllocator.java:107) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.AdaptiveRecvByteBufAllocator$HandleImpl.allocate(AdaptiveRecvByteBufAllocator.java:104) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:112) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:507) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:464) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:378) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:350) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:116) ~[netty-all-4.0.20.Final.jar:4.0.20.Final]
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:724) ~[na:1.7.0_25]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The test consisted of a 3-nodes cluster of n1-standard-1 GCE instances (1 vCPU, 3.75 GB RAM) running cassandra-2.1.0-rc5, and a n1-standard-2 instance running the test.&lt;/p&gt;

&lt;p&gt;After ~2.5 days, several requests start to fail and we see the previous stacktraces in the system.log file.&lt;/p&gt;

&lt;p&gt;The output from linux &#8216;free&#8217; and &#8216;meminfo&#8217; suggest that there is still memory available.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ free -m
total              used       free     shared    buffers     cached
Mem:          3702       3532        169          0        161        854
-/+ buffers/cache:       2516       1185
Swap:            0          0          0

$ head -n 4 /proc/meminfo
MemTotal:        3791292 kB
MemFree:          173568 kB
Buffers:          165608 kB
Cached:           874752 kB
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;These errors do not affect all the queries we run. The cluster is still responsive but is unable to display tracing information using cqlsh :&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;$ ./bin/nodetool --host 10.240.137.253 status duration_test
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.240.98.27    925.17 KB  256     100.0%            41314169-eff5-465f-85ea-d501fd8f9c5e  RAC1
UN  10.240.137.253  1.1 MB     256     100.0%            c706f5f9-c5f3-4d5e-95e9-a8903823827e  RAC1
UN  10.240.72.183   896.57 KB  256     100.0%            15735c4d-98d4-4ea4-a305-7ab2d92f65fc  RAC1


$ echo &lt;span class=&quot;code-quote&quot;&gt;&apos;tracing on; select count(*) from duration_test.ints;&apos;&lt;/span&gt; | ./bin/cqlsh 10.240.137.253
Now tracing requests.

 count
-------
  9486

(1 rows)

Statement trace did not complete within 10 seconds
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</description>
                <environment>&lt;p&gt;Google Compute Engine, n1-standard-1&lt;/p&gt;</environment>
        <key id="12733263">CASSANDRA-7743</key>
            <summary>Possible C* OOM issue during long running test</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="benedict">Benedict Elliott Smith</assignee>
                                    <reporter username="pingtimeout">Pierre Laporte</reporter>
                        <labels>
                    </labels>
                <created>Mon, 11 Aug 2014 15:25:05 +0000</created>
                <updated>Tue, 16 Apr 2019 09:31:37 +0000</updated>
                            <resolved>Thu, 21 Aug 2014 08:38:54 +0000</resolved>
                                        <fixVersion>2.1 rc6</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="14092892" author="enigmacurry" created="Mon, 11 Aug 2014 15:34:11 +0000"  >&lt;p&gt;I&apos;d recommend running &lt;a href=&quot;http://www.eclipse.org/mat/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;MAT&lt;/a&gt; on of the core files to be able to examine what exactly is eating up the ram. Although, I&apos;m not sure if this helps with &quot;Direct buffer memory&quot; as I&apos;ve only used it to debug things before we went off-heap.&lt;/p&gt;</comment>
                            <comment id="14092904" author="jbellis" created="Mon, 11 Aug 2014 15:43:45 +0000"  >&lt;p&gt;This means you need a larger MaxDirectMemorySize, but we&apos;ve avoided allocateDirect in favor of Unsafe in the past, in part because of this problem. /cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14093010" author="pingtimeout" created="Mon, 11 Aug 2014 17:22:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enigmacurry&quot; class=&quot;user-hover&quot; rel=&quot;enigmacurry&quot;&gt;enigmacurry&lt;/a&gt; Eclipse MAT shows 300k instances of java.nio.ByteBuffer[] but retaining only ~26MB. It only accounts for in-heap data.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbellis&quot; class=&quot;user-hover&quot; rel=&quot;jbellis&quot;&gt;jbellis&lt;/a&gt; Ok I am going to start two new tests: one on n1-standard-1 with -XX:MaxDirectMemorySize=-1 and another one on n1-standard-2 without this setting&lt;/p&gt;</comment>
                            <comment id="14093206" author="benedict" created="Mon, 11 Aug 2014 19:50:47 +0000"  >&lt;p&gt;Are you running with memtable_allocation_type: offheap_buffers? If so, switch to the offheap_objects. &lt;/p&gt;

&lt;p&gt;If not, it&apos;s surprising to be hitting that limit with netty buffers, as we don&apos;t allocate them anywhere else. Either way, the fact that this is failing inside netty is surprising, since this is prior to the fix for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-7695&quot; title=&quot;Inserting the same row in parallel causes bad data to be returned to the client&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-7695&quot;&gt;&lt;del&gt;CASSANDRA-7695&lt;/del&gt;&lt;/a&gt;, so we shouldn&apos;t in principle be allocating direct buffers with netty.&lt;/p&gt;</comment>
                            <comment id="14093965" author="pingtimeout" created="Tue, 12 Aug 2014 11:10:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt; Actually, the nodes are running with memtable_allocation_type: heap_buffers.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbellis&quot; class=&quot;user-hover&quot; rel=&quot;jbellis&quot;&gt;jbellis&lt;/a&gt; The test failed on bigger instance too.  I just realized that setting -XX:MaxDirectMemorySize=-1 is useless since it is the default value.  Now I am doubting -1 really means &quot;unlimited&quot;...  Restarting a new one with -XX:MaxDirectMemorySize=1G to see if things change.&lt;/p&gt;</comment>
                            <comment id="14093976" author="benedict" created="Tue, 12 Aug 2014 11:25:38 +0000"  >&lt;p&gt;Could we get some heap dumps? Sounds to me like it&apos;s possibly a netty bug, or a ref counting bug coupled with a leaked/held reference somewhere. We need to see where these ByteBuffer references are being retained and why.&lt;/p&gt;</comment>
                            <comment id="14094030" author="pingtimeout" created="Tue, 12 Aug 2014 12:52:16 +0000"  >&lt;p&gt;Sure, I have uploaded one here : &lt;a href=&quot;https://drive.google.com/file/d/0BxvGkaXP3ayeMDlRTWJ2MVhvT0E/edit?usp=sharing&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://drive.google.com/file/d/0BxvGkaXP3ayeMDlRTWJ2MVhvT0E/edit?usp=sharing&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14095406" author="slebresne" created="Wed, 13 Aug 2014 12:10:37 +0000"  >&lt;p&gt;Has this been tried/reproduced on the current  2.1 branch, notably post &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-7735&quot; title=&quot;Remove ref-counting of netty buffers&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-7735&quot;&gt;&lt;del&gt;CASSANDRA-7735&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="14095411" author="benedict" created="Wed, 13 Aug 2014 12:16:07 +0000"  >&lt;p&gt;No, but I don&apos;t think it&apos;s likely to be related, since they would still be collected when unreferenced, so we&apos;d likely see LEAK DETECTOR warnings from netty at which time the associated resources would also be freed, so we&apos;d be somwhat unlikely to see the bug.&lt;/p&gt;

&lt;p&gt;No harm in trying, of course, but it sounds like it takes a few days to reproduce.&lt;/p&gt;</comment>
                            <comment id="14095524" author="tjake" created="Wed, 13 Aug 2014 14:23:02 +0000"  >&lt;p&gt;It sounds like the safest bet may be to not use the pooled allocator at all&lt;/p&gt;</comment>
                            <comment id="14095535" author="tjake" created="Wed, 13 Aug 2014 14:33:51 +0000"  >&lt;p&gt;Can we run this with  -Dio.netty.leakDetectionLevel=PARANOID ?&lt;/p&gt;</comment>
                            <comment id="14095698" author="pingtimeout" created="Wed, 13 Aug 2014 16:53:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt; Sure, I just started a new test with this option&lt;/p&gt;</comment>
                            <comment id="14095780" author="benedict" created="Wed, 13 Aug 2014 17:40:46 +0000"  >&lt;p&gt;It looks like the problem is caused by a number of changes in 2.1 composing to yield especially bad behaviour. We use pooled buffers in netty, but we also introduced an SEPWorker pool that has many threads (more than the number that actually service any single pool), and all threads may eventually service work on the netty executor side. This gives us ~130 threads periodically performing this work, and each of them apparently allocates a buffer at some point. These buffers are unfortunately allocated from a threadlocal pool, which starts at 16Mb, so each thread retains at least 16Mb of largely useless memory.&lt;/p&gt;

&lt;p&gt;The best fix will be to stop the SEPWorker tasks from allocating any buffers, but &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt; has pointed out we can also tweak some settings to mitigate the negative impact of this kind of problem as well.&lt;/p&gt;

&lt;p&gt;I&apos;ll look into a patch tomorrow.&lt;/p&gt;</comment>
                            <comment id="14096619" author="benedict" created="Thu, 14 Aug 2014 06:11:47 +0000"  >&lt;p&gt;Hmm. So, looking at this a little more closely, I think this may effectively be a netty bug after all. It looks like no matter what pool/thread a pooled bytebuf is allocated on, it gets returned to the pool of the thread that &lt;em&gt;releases&lt;/em&gt; it. This means it simply accumulates indefinitely (up to the pool limit, which defaults to 32Mb) in the SEPWorkers, since they never themselves &lt;em&gt;allocate&lt;/em&gt;, only release.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=norman&quot; class=&quot;user-hover&quot; rel=&quot;norman&quot;&gt;norman&lt;/a&gt; is that analysis correct? If so, it looks like this behaviour is somewhat unexpected and not ideal. However we can work around it for now.&lt;/p&gt;</comment>
                            <comment id="14096625" author="norman" created="Thu, 14 Aug 2014 06:16:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt; hmm.. it should always get returned to the pool that it was allocated from. Could you provide me with an easy way to reproduce ?&lt;/p&gt;</comment>
                            <comment id="14096631" author="benedict" created="Thu, 14 Aug 2014 06:22:17 +0000"  >&lt;p&gt;I haven&apos;t got to that stage yet, I&apos;m just analysing the code right now. It&apos;s why I asked for your input, was hoping you could disabuse me if I&apos;m completely wrong. I don&apos;t 100% understand the control flow, as it doesn&apos;t make much sense (to me) to be adding it to a different cache. However if you look in PooledByteBuf.deallocate(), it calls PoolArena.free() to release the memory, which in turn calls parent.threadCache.get().add() to cache its memory; obviously the threadCache.get() is grabbing the threadlocal cache for the thread releasing, not the source PoolThreadCache.&lt;/p&gt;

&lt;p&gt;Also worth noting I&apos;m not convinced that, even if I&apos;m correct, this fully explains the behaviour. We should only release on a different thread if an exception occurs during processing anyway, so I&apos;m still digging for a more satisfactory full explanation of the behaviour.&lt;/p&gt;</comment>
                            <comment id="14096634" author="norman" created="Thu, 14 Aug 2014 06:24:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt; Yeah it add to the cache of the &quot;releasing&quot; thread that is right.. I thought you talk about return to pool.&lt;/p&gt;</comment>
                            <comment id="14096638" author="benedict" created="Thu, 14 Aug 2014 06:27:13 +0000"  >&lt;p&gt;We&apos;re conflating two pools maybe &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;I mean the &quot;pool&quot; of memory the thread can allocate from. So, to confirm I have this right, if you have two threads A and B, A only allocating and B only releasing, you would get memory accumulating up to max pool size in B, and A always allocating new memory?&lt;/p&gt;</comment>
                            <comment id="14096647" author="norman" created="Thu, 14 Aug 2014 06:33:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt; well it will be released after a while if not used. But I think for your use-case it would be best to disable the cache which can be done via the PooledByteBufAllocator constructor just pass in 0 for &quot;int tinyCacheSize, int smallCacheSize, int normalCacheSize&quot;.&lt;/p&gt;</comment>
                            <comment id="14096651" author="benedict" created="Thu, 14 Aug 2014 06:37:30 +0000"  >&lt;blockquote&gt;&lt;p&gt;well it will be released after a while if not used.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;how long? it shouldn&apos;t ever be used, and it looks like it accumulates gigabytes in total over the course of a few days (around 16-32Mb per thread)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;just pass in 0 for &quot;int tinyCacheSize, int smallCacheSize, int normalCacheSize&quot;.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Won&apos;t that obviate most of the benefit of the pooled buffers? &lt;/p&gt;

&lt;p&gt;I plan to simply prevent our deallocating on the other threads.&lt;/p&gt;</comment>
                            <comment id="14096661" author="benedict" created="Thu, 14 Aug 2014 06:46:16 +0000"  >&lt;p&gt;Ok, so I suspect that the exceptions being thrown that cause us to deallocate frequently enough are presumably timeouts. I can&apos;t see another place we&apos;re either deallocating or allocating bytebufs.&lt;/p&gt;

&lt;p&gt;So a simple patch that should fix this problem is available &lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/7743-nettyoom&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</comment>
                            <comment id="14098977" author="tjake" created="Fri, 15 Aug 2014 19:32:51 +0000"  >&lt;p&gt;Looks good +1&lt;/p&gt;</comment>
                            <comment id="14099486" author="benedict" created="Sat, 16 Aug 2014 03:34:41 +0000"  >&lt;p&gt;I&apos;d like to get confirmation this bug is fixed before resolving it, but no reason to hold up rc6 for that.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pingtimeout&quot; class=&quot;user-hover&quot; rel=&quot;pingtimeout&quot;&gt;pingtimeout&lt;/a&gt; do you think you&apos;ll be able to try this out?&lt;/p&gt;</comment>
                            <comment id="14100080" author="kishkaru" created="Sun, 17 Aug 2014 19:37:55 +0000"  >&lt;p&gt;I&apos;m running rc5 + the patch, and the issue still shows up. &lt;br/&gt;
I patched rc5 with the one file, and ran &quot;ant realclean jar&quot; to compile. I hope this command didn&apos;t re-pull from git.&lt;/p&gt;

&lt;p&gt;$ free -m&lt;br/&gt;
             total       used       free     shared    buffers     cached&lt;br/&gt;
Mem:          3702       2667       1035          0          1        144&lt;br/&gt;
-/+ buffers/cache:       2520       1181&lt;br/&gt;
Swap:            0          0          0&lt;/p&gt;

&lt;p&gt;$ head -n 4 /proc/meminfo&lt;br/&gt;
MemTotal:        3791292 kB&lt;br/&gt;
MemFree:         1060548 kB&lt;br/&gt;
Buffers:            1280 kB&lt;br/&gt;
Cached:           148968 kB&lt;/p&gt;</comment>
                            <comment id="14100197" author="benedict" created="Mon, 18 Aug 2014 00:44:28 +0000"  >&lt;p&gt;Did you see the actual error, or have more info than meminfo? Because that is not at all conclusive by itself.&lt;/p&gt;</comment>
                            <comment id="14105197" author="benedict" created="Thu, 21 Aug 2014 08:38:54 +0000"  >&lt;p&gt;I&apos;ve had off-JIRA confirmation that this has been fixed (by both inspecting heap dumps and leaving the test to run for its extended period to confirm). Kishan&apos;s negative result was apparently a mistake.&lt;/p&gt;</comment>
                            <comment id="14105211" author="norman" created="Thu, 21 Aug 2014 08:49:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt; so no netty issue at all ?&lt;/p&gt;</comment>
                            <comment id="14105217" author="benedict" created="Thu, 21 Aug 2014 08:57:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=normanm&quot; class=&quot;user-hover&quot; rel=&quot;normanm&quot;&gt;normanm&lt;/a&gt; IMO the netty behaviour is surprising and likely to bite other projects as well, however it can be worked around if you realise it&apos;s there - but only with careful code analysis, it&apos;s hard to be certain you aren&apos;t allocating/releasing on other threads. It would be useful I think to have some warnings logged by netty if you initialise a new threadlocal memory pool on &lt;em&gt;returning&lt;/em&gt; a bytebuf, as this might well be indicative of pathological behaviour (you&apos;d expect a thread to have allocated at least once before releasing if it is likely to allocate again). It might even be nice to explicitly define which threads are permitted to pool memory, so that you cannot accidentally build up pools on worker threads without noticing through accidental allocations as well. This wasn&apos;t a problem for us here, but I could see us accidentally introducing a bug like that pretty easily in future.&lt;/p&gt;

&lt;p&gt;It would be great if the guides and javadocs highlighted this as well, especially when discussing handlers with associated executor pools, which is where this is most likely to bite.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[benedict]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>411291</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 13 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1yrcn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>411283</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>tjake</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[tjake]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311420" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12327541">2.1 rc5</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12311124" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Tester</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>pingtimeout</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>