<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:13:24 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-1130] Row iteration can stomp start-of-row mark</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-1130</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am trying to use TTL (timeToLive) feature in SuperColumns.&lt;br/&gt;
My usecase is:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I have a SuperColumn and 3 subcolumns.&lt;/li&gt;
	&lt;li&gt;I try to expire data after 60 seconds.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;While Cassandra is up and running, I am successfully able to push and read data without any problems. Data compaction and all occurs fine. After inserting say about 100000 records, I stop Cassandra while data is still coming.&lt;/p&gt;

&lt;p&gt;On startup Cassandra throws an exception and won&apos;t start up. (This happens 1 in every 3 times). Exception varies like:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;EOFException while reading data&lt;/li&gt;
	&lt;li&gt;negative value encountered exception&lt;/li&gt;
	&lt;li&gt;Heap Space Exception&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Cassandra simply won&apos;t start up.&lt;/p&gt;

&lt;p&gt;Again I get this problem only when I use TTL with SuperColumns. There are no issues with using TTL with regular Columns.&lt;/p&gt;

&lt;p&gt;I tried to diagnose the problem and it seems to happen on startup when it sees a Column that is marked Deleted and its trying to read data. Its off by some bytes and hence all these exceptions.&lt;/p&gt;

&lt;p&gt;Caused by: java.io.IOException: Corrupt (negative) value length encountered&lt;br/&gt;
        at org.apache.cassandra.utils.FBUtilities.readByteArray(FBUtilities.java:317)&lt;br/&gt;
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:84)&lt;br/&gt;
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:336)&lt;br/&gt;
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:285)&lt;br/&gt;
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.getNextBlock(SSTableSliceIterator.java:235)&lt;br/&gt;
        at org.apache.cassandra.db.filter.SSTableSliceIterator$ColumnGroupReader.pollColumn(SSTableSliceIterator.java:195)&lt;br/&gt;
        ... 18 more&lt;/p&gt;


&lt;p&gt;Let me know if you need more information.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Jignesh&lt;/p&gt;</description>
                <environment></environment>
        <key id="12465398">CASSANDRA-1130</key>
            <summary>Row iteration can stomp start-of-row mark</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="slebresne">Sylvain Lebresne</assignee>
                                    <reporter username="jigneshdhruv">Jignesh Dhruv</reporter>
                        <labels>
                    </labels>
                <created>Tue, 25 May 2010 18:13:16 +0000</created>
                <updated>Tue, 16 Apr 2019 09:33:23 +0000</updated>
                            <resolved>Mon, 14 Jun 2010 18:06:50 +0000</resolved>
                                        <fixVersion>0.7 beta 1</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="12871289" author="slebresne" created="Tue, 25 May 2010 18:51:10 +0000"  >&lt;p&gt;If you have some script that allows to reproduce, that would be awesome. Alternatively, if you&apos;re able to locate the culprit data file and if the infos are not sensible, providing the file could help.&lt;/p&gt;

&lt;p&gt;Are you using latest trunk ?&lt;/p&gt;</comment>
                            <comment id="12871293" author="jigneshdhruv" created="Tue, 25 May 2010 19:06:31 +0000"  >&lt;p&gt;Yes I am using the latest source code from trunk.&lt;/p&gt;

&lt;p&gt;I have a small java application that deals with creating schema and populating data.&lt;/p&gt;

&lt;p&gt;This is what I was able to debug till now:&lt;br/&gt;
The error occurs during deserialization in ColumnSerializer.&lt;/p&gt;

&lt;p&gt;There is an extra int byte that needs to be read before ColumnSerializer.java:84. Value of this extra int byte is &quot;4&quot;. Not sure what it stands for.&lt;/p&gt;

&lt;p&gt;I am not sure from where that byte is set. After reading that byte, I get the localDeletionTime value.&lt;/p&gt;

&lt;p&gt;Also this happens when the DELETION_MASK is set on a record. It works fine for records with EXPIRATION_MASK. I am thinking that converting a record from EXPIRY to DELETED is causing this error at startup or something like that.&lt;/p&gt;

&lt;p&gt;Let me know if you need more information.&lt;/p&gt;

&lt;p&gt;But you should be able to reproduce this if you have a SuperColumn and their subColumns with TTL. Stop and start cassandra after loading some data. It consistently fails to startup 1 out of every 3 times.&lt;/p&gt;

&lt;p&gt;Jignesh&lt;/p&gt;</comment>
                            <comment id="12871345" author="jigneshdhruv" created="Tue, 25 May 2010 20:37:49 +0000"  >&lt;p&gt;OK. I think I narrowed it down further..&lt;/p&gt;

&lt;p&gt;The bug may be in &quot;org/apache/cassandra/db/filter/SSTableSliceIterator.java:getNextBlock() method.&lt;/p&gt;

&lt;p&gt;See the while loop on line 233.&lt;br/&gt;
Out here, its reading 1 column at a time.&lt;/p&gt;

&lt;p&gt;As I said before, the problem is when an Column of Type ExpiringColumn becomes DeletedColumn when time has expired.&lt;/p&gt;

&lt;p&gt;In that case, once the Supercolumn whose subcolumns are of type &quot;DELETED&quot; are read in this while loop, there are some extra bytes that needs to be skipped but instead it goes in the second iteration in the while loop and tries to read the next column and thats where all the problem starts.&lt;/p&gt;

&lt;p&gt;Shouldn&apos;t the while loop just read one Column at a time and then exit. That is what it does when it reads all the bytes. If I put a &quot;break statement&quot; in the end of while loop after reading a column all works fine as the extra bytes are skipped during the next read of a Column.&lt;/p&gt;

&lt;p&gt;I am not sure what is the purpose of this while loop? but if we break after reading  1 column at a time, all works fine and cassandra starts up smoothly.&lt;/p&gt;

&lt;p&gt;This looks similar to issue&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-1073&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/CASSANDRA-1073&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Jignesh&lt;/p&gt;</comment>
                            <comment id="12871363" author="jigneshdhruv" created="Tue, 25 May 2010 21:19:34 +0000"  >&lt;p&gt;I did some more testing with the patch that I suggested i.e. skipping extra bytes after reading the column and exiting the while loop worked fine. I am now able to load and start cassandra with SuperColumn data with TTL without any problems.&lt;/p&gt;</comment>
                            <comment id="12871367" author="jbellis" created="Tue, 25 May 2010 21:30:06 +0000"  >&lt;p&gt;this sounds like something you could build a unit test for?&lt;/p&gt;</comment>
                            <comment id="12871375" author="slebresne" created="Tue, 25 May 2010 21:40:39 +0000"  >&lt;p&gt;But they shouldn&apos;t be extra bytes to skip. An expired column becomes a deletedColumn only &lt;br/&gt;
after everything is deserialized. It is expected that we read all and everything that is written.&lt;/p&gt;

&lt;p&gt;The while loop you&apos;re referring to is here to read all the column (or super columns) that falls &lt;br/&gt;
into one given index range (the index is sparse). If you exit the while loop, you will just potentially &lt;br/&gt;
skip some columns (super columns in your example). That it makes cassandra start may just be &lt;br/&gt;
that it skips the problematic parts. &lt;/p&gt;

&lt;p&gt;I&apos;ll try to reproduce this tomorrow (but if you want and can share your test code to make that &lt;br/&gt;
easier, feel free to &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;). &lt;/p&gt;</comment>
                            <comment id="12871583" author="slebresne" created="Wed, 26 May 2010 09:22:43 +0000"  >&lt;p&gt;Sorry but I don&apos;t seem able to reproduce this.&lt;br/&gt;
I&apos;ve tried a simple test, inserting supercolumns with 100 colums in them, &lt;br/&gt;
each having a TTL (that I varied from 10 seconds to like 3 minutes).&lt;br/&gt;
I typically let it insert over 10000 super columns  (so around 1 millions ttled &lt;br/&gt;
columns) and kill it. I run cassandra again, let it compact, kill it again, run again, &lt;br/&gt;
start insertion again, etc... I tried like 20 times, no crashes whatsoever.&lt;/p&gt;

&lt;p&gt;I&apos;ve tried with a trunk of a week or so ago and then with trunk from 1 hour ago. &lt;br/&gt;
Sounds like you have no problem reproducing on your side so .. I don&apos;t know.&lt;/p&gt;

&lt;p&gt;If you could somehow come up with a unit test that make it crashes or a small &lt;br/&gt;
script test that triggers it, that would be amazing.&lt;/p&gt;</comment>
                            <comment id="12871836" author="jigneshdhruv" created="Wed, 26 May 2010 17:29:27 +0000"  >&lt;p&gt;I checked out the latest source code this morning and I am still able to reproduce it.&lt;/p&gt;

&lt;p&gt;My usecase is:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Start Cassandra&lt;/li&gt;
	&lt;li&gt;Keep adding SuperColumns with 3 subcolumns within each SuperColumn. Each subcolumn expires in 35 seconds.&lt;/li&gt;
	&lt;li&gt;Let cassandra run until you see statements  like &quot;Deleted  files&quot;&lt;/li&gt;
	&lt;li&gt;Stop cassandra and try to start and it will give you all the exceptions that I am talking about.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Also I believe ExpiringColumn contains some more data compared to DeletedColumn. Correct? In my testing I found that length of each DeletedColumn was similar to ExpiringColumn and once a complete DeletedColumn record was read there were some more extra bytes at the end of the record which is causing all this issue?&lt;/p&gt;

&lt;p&gt;When you convert a ExpiringColumn to DeletedColumn, is it in place replacement or the old record is marked for deletion by just changing the EXPIRING_MASK to DELETED_MASK.&lt;/p&gt;

&lt;p&gt;I will try to produce a junit test case. But one needs to still stop cassandra when one sees some files being deleted. At that point you will see the error that I am talking about.&lt;/p&gt;

&lt;p&gt;Jignesh&lt;/p&gt;</comment>
                            <comment id="12872349" author="slebresne" created="Thu, 27 May 2010 20:22:05 +0000"  >&lt;p&gt;Sorry but I&apos;m still unable to reproduce.&lt;/p&gt;

&lt;p&gt;I&apos;m attaching a small insert script (in java) that, as far as I can see,&lt;br/&gt;
seems to do what you say triggers the bug. Could you look if this fails &lt;br/&gt;
on your side. If I haven&apos;t understand the steps correctly, would you mind &lt;br/&gt;
updating this test so that it reproduce the bug you see ?&lt;br/&gt;
(the script uses raw thrift and requires a very recent trunk)&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Also I believe ExpiringColumn contains some more data compared to DeletedColumn. Correct? In my testing I found that length of each&lt;br/&gt;
DeletedColumn was similar to ExpiringColumn and once a complete DeletedColumn record was read there were some more extra bytes at the &lt;br/&gt;
end of the record which is causing all this issue?&lt;br/&gt;
When you convert a ExpiringColumn to DeletedColumn, is it in place replacement or the old record is marked for deletion by just &lt;br/&gt;
changing the EXPIRING_MASK to DELETED_MASK.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;File on disk are not updated in place. So we never change on disk an&lt;br/&gt;
ExpiringColumn to a DeletedColumn. There only is a small optimisation in the&lt;br/&gt;
deserialization code that, after having fully deserialize an ExpiringColumn,&lt;br/&gt;
will return an equivalent DeletedColumn if the column is expired. But we always&lt;br/&gt;
read exactly what we have written (or it&apos;s a bug).&lt;/p&gt;</comment>
                            <comment id="12872375" author="jigneshdhruv" created="Thu, 27 May 2010 21:21:22 +0000"  >&lt;p&gt;Hello,&lt;/p&gt;

&lt;p&gt;I am still able to reproduce the problem consistently. I have attached my JUNIT Test case which reproduces it 1 out of 3 times.&lt;/p&gt;

&lt;p&gt;Streps to Reproduce:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Run it until it inserts atleaset 200000 records or until you see a &quot;Deleted files&quot; message on your console.&lt;/li&gt;
	&lt;li&gt;Stop cassandra while the data is still coming in.&lt;/li&gt;
	&lt;li&gt;Start Cassandra and you should get exceptions.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;If you do not get exceptions, without deleting the index repeat the above steps.&lt;/p&gt;

&lt;p&gt;Let me know if you are able to reproduce the problem.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Jignesh&lt;/p&gt;</comment>
                            <comment id="12872570" author="slebresne" created="Thu, 27 May 2010 22:08:03 +0000"  >&lt;p&gt;Ok, I&apos;m able to reproduce with your test.&lt;br/&gt;
I&apos;ll have a look at it, thanks&lt;/p&gt;</comment>
                            <comment id="12873026" author="slebresne" created="Fri, 28 May 2010 16:01:46 +0000"  >&lt;p&gt;Attached fiie should fix the problem. &lt;/p&gt;

&lt;p&gt;The problem is unrelated to TTL per se but a problem in row iterations so the &lt;br/&gt;
ticket title can be a bit misleading.&lt;br/&gt;
Citing irc: &lt;br/&gt;
  &quot;a columnGroupReader mark the file when it is created. Then it reset() when getting a next block.&lt;br/&gt;
   but with the way the row iteration works, a new columnGroupReader is created (and mark the file) before &lt;br/&gt;
   the previous one has retrieved it&apos;s block&lt;br/&gt;
  (it&apos;s because computeNext() create the next SSTableSliceIterator before getReduced() had retrieved the actual &lt;br/&gt;
   column of the previous one)&quot;&lt;br/&gt;
The patch allow for each columnGroupReader to have it&apos;s own mark on the file&lt;/p&gt;

&lt;p&gt;Btw, I was unable to reproduce previously because it&apos;s the cache preloading that &lt;br/&gt;
trigged the error and I did not use one in my first tests. &lt;/p&gt;

&lt;p&gt;Thanks Jignesh for helping find this one.&lt;/p&gt;</comment>
                            <comment id="12873056" author="slebresne" created="Fri, 28 May 2010 17:00:49 +0000"  >&lt;p&gt;Attached version of the patch rebased against 0.6&lt;/p&gt;</comment>
                            <comment id="12873069" author="slebresne" created="Fri, 28 May 2010 17:32:11 +0000"  >&lt;p&gt;Attaching a unit test for trunk that fails without the patch and passes with it.&lt;/p&gt;

&lt;p&gt;It doesn&apos;t fail in 0.6 though. I suspect this is because getRangeSlice doesn&apos;t &lt;br/&gt;
work in the same way in 0.6. So I&apos;m not sure how to reproduce in 0.6. But I&apos;ll &lt;br/&gt;
have a better at how it works in 0.6 and see if I can have a unit test for it too.&lt;/p&gt;</comment>
                            <comment id="12873077" author="jigneshdhruv" created="Fri, 28 May 2010 17:54:14 +0000"  >&lt;p&gt;Excellent. I guess the changes are in trunk. I will check it out.&lt;/p&gt;

&lt;p&gt;Thanks,&lt;br/&gt;
Jignesh&lt;/p&gt;</comment>
                            <comment id="12873357" author="jbellis" created="Sun, 30 May 2010 01:41:43 +0000"  >&lt;p&gt;This bug is only present in 0.7.&lt;/p&gt;</comment>
                            <comment id="12878624" author="jbellis" created="Mon, 14 Jun 2010 16:08:03 +0000"  >&lt;p&gt;Sorry for the delay applying.  Can you rebase to trunk please?&lt;/p&gt;</comment>
                            <comment id="12878633" author="slebresne" created="Mon, 14 Jun 2010 16:53:28 +0000"  >&lt;p&gt;No problem, attaching rebased patches&lt;/p&gt;</comment>
                            <comment id="12878673" author="jbellis" created="Mon, 14 Jun 2010 18:06:50 +0000"  >&lt;p&gt;committed, thanks!&lt;/p&gt;</comment>
                            <comment id="12881050" author="jbellis" created="Tue, 22 Jun 2010 04:06:28 +0000"  >&lt;p&gt;weird, I totally remember committing this (and editing SSTNI to make it apply) but it didn&apos;t make it to svn.  Probably I forgot git-svn dcommit somehow.&lt;/p&gt;

&lt;p&gt;really committed this time.&lt;/p&gt;</comment>
                            <comment id="12881208" author="hudson" created="Tue, 22 Jun 2010 14:08:08 +0000"  >&lt;p&gt;Integrated in Cassandra #473 (See &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Cassandra/473/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Cassandra/473/&lt;/a&gt;)&lt;br/&gt;
    fix race condition in SSTable*Iterator.&lt;br/&gt;
patch by Sylvain Lebresne; reviewed by jbellis for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-1130&quot; title=&quot;Row iteration can stomp start-of-row mark&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-1130&quot;&gt;&lt;del&gt;CASSANDRA-1130&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12447039" name="0001-Allow-for-multiple-mark-on-a-file.patch" size="11942" author="slebresne" created="Mon, 14 Jun 2010 16:53:27 +0000"/>
                            <attachment id="12447040" name="0002-Unit-test-for-row-iteration.patch" size="3152" author="slebresne" created="Mon, 14 Jun 2010 16:53:27 +0000"/>
                            <attachment id="12445706" name="TestSuperColumnTTL.java" size="3278" author="jigneshdhruv" created="Thu, 27 May 2010 21:21:22 +0000"/>
                            <attachment id="12445696" name="TestSuperColumnTTL.java" size="1824" author="slebresne" created="Thu, 27 May 2010 20:22:05 +0000"/>
                            <attachment id="12445795" name="cassandra_0.6-Allow_multiple_mark_on_file.diff" size="10319" author="slebresne" created="Fri, 28 May 2010 17:00:49 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[slebresne]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>20002</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            15 years, 23 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0g36f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>91945</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>