<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:43:18 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-6285] 2.0 HSHA server introduces corrupt data</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-6285</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;After altering everything to LCS the table OpsCenter.rollups60 amd one other none OpsCenter-Table got stuck with everything hanging around in L0.&lt;br/&gt;
The compaction started and ran until the logs showed this:&lt;br/&gt;
ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:111&amp;#93;&lt;/span&gt; 2013-11-01 19:14:53,865 CassandraDaemon.java (line 187) Exception in thread Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:111,1,RMI Runtime&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.RuntimeException: Last written key DecoratedKey(1326283851463420237, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f696e6465785f323031335f31305f30382d63616368655f646f63756d656e74736c6f6f6b75702d676574426c6f6f6d46696c746572537061636555736564) &amp;gt;= current key DecoratedKey(954210699457429663, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f696e6465785f323031335f31305f30382d63616368655f646f63756d656e74736c6f6f6b75702d676574546f74616c4469736b5370616365557365640b0f) writing into /var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-jb-58656-Data.db&lt;br/&gt;
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)&lt;br/&gt;
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)&lt;br/&gt;
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)&lt;br/&gt;
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)&lt;br/&gt;
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)&lt;br/&gt;
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)&lt;br/&gt;
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)&lt;br/&gt;
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:296)&lt;br/&gt;
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)&lt;br/&gt;
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)&lt;br/&gt;
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:724)&lt;/p&gt;

&lt;p&gt;Moving back to STC worked to keep the compactions running.&lt;br/&gt;
Especialy my own Table i would like to move to LCS.&lt;br/&gt;
After a major compaction with STC the move to LCS fails with the same Exception.&lt;/p&gt;</description>
                <environment>&lt;p&gt;4 nodes, shortly updated from 1.2.11 to 2.0.2&lt;/p&gt;</environment>
        <key id="12677127">CASSANDRA-6285</key>
            <summary>2.0 HSHA server introduces corrupt data</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10000" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Urgent</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="xedin">Pavel Yaskevich</assignee>
                                    <reporter username="davedamoon">David Sauer</reporter>
                        <labels>
                    </labels>
                <created>Fri, 1 Nov 2013 18:45:21 +0000</created>
                <updated>Tue, 16 Apr 2019 09:32:01 +0000</updated>
                            <resolved>Mon, 19 May 2014 21:39:04 +0000</resolved>
                                        <fixVersion>2.0.8</fixVersion>
                                        <due></due>
                            <votes>4</votes>
                                    <watches>34</watches>
                                                                                                                <comments>
                            <comment id="13811919" author="davedamoon" created="Sat, 2 Nov 2013 09:15:36 +0000"  >&lt;p&gt;After removing all the Data from the OpsCenters Keyspace (and using LCS) and collectiong new Data for a night, the command nodetool compact OpsCenter rollups60 failed with this Exception:&lt;/p&gt;

&lt;p&gt;Error occurred during compaction&lt;br/&gt;
java.util.concurrent.ExecutionException: java.lang.RuntimeException: Last written key DecoratedKey(-6663228376520744598, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f6c6f6767696e672d706572666f726d616e63655f67726f757065642d67657452656164436f756e740b0f0000000100000009726f6c6c75707336) &amp;gt;= current key DecoratedKey(-6896470603826733036, 37382e34362e3132382e3139382d6d65646970726569735f7365617263685f696e6465785f323031335f31305f30382d6d756c7469776f7264735f70686f6e656d732d6765744c69766553535461626c65436f756e74) writing into /var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-jb-14-Data.db&lt;br/&gt;
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)&lt;br/&gt;
	at java.util.concurrent.FutureTask.get(FutureTask.java:188)&lt;br/&gt;
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:281)&lt;br/&gt;
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1845)&lt;br/&gt;
	at org.apache.cassandra.service.StorageService.forceKeyspaceCompaction(StorageService.java:2167)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)&lt;br/&gt;
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)&lt;br/&gt;
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)&lt;br/&gt;
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)&lt;br/&gt;
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)&lt;br/&gt;
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)&lt;br/&gt;
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)&lt;br/&gt;
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)&lt;br/&gt;
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)&lt;br/&gt;
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)&lt;br/&gt;
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)&lt;br/&gt;
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)&lt;br/&gt;
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)&lt;br/&gt;
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)&lt;br/&gt;
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)&lt;br/&gt;
	at sun.reflect.GeneratedMethodAccessor35.invoke(Unknown Source)&lt;br/&gt;
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)&lt;br/&gt;
	at java.lang.reflect.Method.invoke(Method.java:606)&lt;br/&gt;
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)&lt;br/&gt;
	at sun.rmi.transport.Transport$1.run(Transport.java:177)&lt;br/&gt;
	at sun.rmi.transport.Transport$1.run(Transport.java:174)&lt;br/&gt;
	at java.security.AccessController.doPrivileged(Native Method)&lt;br/&gt;
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)&lt;br/&gt;
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)&lt;br/&gt;
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)&lt;br/&gt;
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
	at java.lang.Thread.run(Thread.java:724)&lt;br/&gt;
Caused by: java.lang.RuntimeException: Last written key DecoratedKey(-6663228376520744598, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f6c6f6767696e672d706572666f726d616e63655f67726f757065642d67657452656164436f756e740b0f0000000100000009726f6c6c75707336) &amp;gt;= current key DecoratedKey(-6896470603826733036, 37382e34362e3132382e3139382d6d65646970726569735f7365617263685f696e6465785f323031335f31305f30382d6d756c7469776f7264735f70686f6e656d732d6765744c69766553535461626c65436f756e74) writing into /var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-jb-14-Data.db&lt;br/&gt;
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)&lt;br/&gt;
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)&lt;br/&gt;
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)&lt;br/&gt;
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)&lt;br/&gt;
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)&lt;br/&gt;
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)&lt;br/&gt;
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)&lt;br/&gt;
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:296)&lt;br/&gt;
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)&lt;br/&gt;
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)&lt;br/&gt;
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)&lt;br/&gt;
	... 3 more&lt;/p&gt;</comment>
                            <comment id="13835403" author="brevilo" created="Fri, 29 Nov 2013 14:47:43 +0000"  >&lt;p&gt;Just in case it helps: I&apos;m getting almost identical exceptions while running a single node stress test using &lt;tt&gt;cassandra-stress&lt;/tt&gt; with Cassandra 2.0.2 (DSC) on Debian Wheezy with 2 GB RAM. I&apos;m running 10e7 write ops on a single HDD, using (more or less) Cassandra&apos;s default configuration, specifically STC.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;ERROR [CompactionExecutor:14] 2013-11-29 15:33:39,978 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:14,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-3658992336117051287, 3033353732383438) &amp;gt;= current key DecoratedKey(-4078405136366838408, 3033353634323236) writing into /srv3/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-tmp-jb-106-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13872338" author="brandon.kearby" created="Wed, 15 Jan 2014 18:06:06 +0000"  >&lt;p&gt;I&apos;m getting it as well on 2.0.4. I&apos;m testing a new cluster. I don&apos;t get the error with one node, but when I add two or more I get the same error.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;ERROR [CompactionExecutor:6] 2014-01-15 17:13:13,395 CassandraDaemon.java (line 187) Exception in thread &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[CompactionExecutor:6,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-1983406872803353678, 545749545445523a333535383030333439353835353830303334) &amp;gt;= current key DecoratedKey(-7683510718755081698, 545749545445523a333639333235363931383339333238323537) writing into /BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-tmp-jb-121-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here&apos;s the schema I&apos;m testing with:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;create column family signal
  with column_type = &lt;span class=&quot;code-quote&quot;&gt;&apos;Standard&apos;&lt;/span&gt;
  and comparator = &lt;span class=&quot;code-quote&quot;&gt;&apos;UTF8Type&apos;&lt;/span&gt;
  and default_validation_class = &lt;span class=&quot;code-quote&quot;&gt;&apos;BytesType&apos;&lt;/span&gt;
  and key_validation_class = &lt;span class=&quot;code-quote&quot;&gt;&apos;UTF8Type&apos;&lt;/span&gt;
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 432000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = &lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;
  and compaction_strategy = &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.cassandra.db.compaction.LeveledCompactionStrategy&apos;&lt;/span&gt;
  and caching = &lt;span class=&quot;code-quote&quot;&gt;&apos;ALL&apos;&lt;/span&gt;
  and compaction_strategy_options = {&lt;span class=&quot;code-quote&quot;&gt;&apos;sstable_size_in_mb&apos;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&apos;160&apos;&lt;/span&gt;}
  and comment = &lt;span class=&quot;code-quote&quot;&gt;&apos;A store of information about each individual signal.&apos;&lt;/span&gt;
  and column_metadata = [
    {column_name : &lt;span class=&quot;code-quote&quot;&gt;&apos;type&apos;&lt;/span&gt;,
    validation_class : UTF8Type},
    {column_name : &lt;span class=&quot;code-quote&quot;&gt;&apos;foo_id&apos;&lt;/span&gt;,
    validation_class : LongType},
    validation_class : UTF8Type}]
  and compression_options = {&lt;span class=&quot;code-quote&quot;&gt;&apos;sstable_compression&apos;&lt;/span&gt; : &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;&lt;/span&gt;};
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13872351" author="jbellis" created="Wed, 15 Jan 2014 18:16:08 +0000"  >&lt;p&gt;Can you enable snapshot_before_compaction and post the sstables that it&apos;s trying to compact?  I can get you a private upload place if necessary.&lt;/p&gt;</comment>
                            <comment id="13872371" author="brandon.kearby" created="Wed, 15 Jan 2014 18:24:15 +0000"  >&lt;p&gt;Sure,&lt;/p&gt;

&lt;p&gt;BTW, I tried changing to SizeTieredCompactionStrategy and got the same error. I&apos;ll enable snapshot_before_compaction.&lt;/p&gt;</comment>
                            <comment id="13872409" author="brandon.kearby" created="Wed, 15 Jan 2014 18:47:08 +0000"  >&lt;p&gt;Snapshot of compaction before failing&lt;br/&gt;
Added attachment: system-compactions_in_progress-jb-25-Data.db&lt;/p&gt;

&lt;p&gt;Logs before failing&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;INFO [CompactionExecutor:6] 2014-01-15 18:38:47,690 ColumnFamilyStore.java (line 740) Enqueuing flush of Memtable-compactions_in_progress@856586691(847/8470 serialized/live bytes, 35 ops)
 INFO [FlushWriter:3] 2014-01-15 18:38:47,691 Memtable.java (line 333) Writing Memtable-compactions_in_progress@856586691(847/8470 serialized/live bytes, 35 ops)
 INFO [FlushWriter:3] 2014-01-15 18:38:47,700 Memtable.java (line 373) Completed flushing /BigData/lib/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-jb-24-Data.db (304 bytes) &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; commitlog position ReplayPosition(segmentId=1389810508756, position=33429429)
 INFO [CompactionExecutor:6] 2014-01-15 18:38:47,703 CompactionTask.java (line 115) Compacting [SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-45-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-46-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-67-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-72-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-78-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-75-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-56-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-62-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-66-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-49-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-47-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-57-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-61-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-79-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-65-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-50-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-58-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-77-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-54-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-53-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-48-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-64-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-68-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-76-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-55-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-74-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-60-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-52-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-69-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-71-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-73-Data.db&apos;&lt;/span&gt;), SSTableReader(path=&lt;span class=&quot;code-quote&quot;&gt;&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-63-Data.db&apos;&lt;/span&gt;)]
 INFO [CompactionExecutor:6] 2014-01-15 18:39:05,208 ColumnFamilyStore.java (line 740) Enqueuing flush of Memtable-compactions_in_progress@2046507929(0/0 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2014-01-15 18:39:05,208 Memtable.java (line 333) Writing Memtable-compactions_in_progress@2046507929(0/0 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2014-01-15 18:39:05,218 Memtable.java (line 373) Completed flushing /BigData/lib/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-jb-25-Data.db (42 bytes) &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; commitlog position ReplayPosition(segmentId=1389810508756, position=33430117)
ERROR [CompactionExecutor:6] 2014-01-15 18:39:05,220 CassandraDaemon.java (line 187) Exception in thread &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[CompactionExecutor:6,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-5705444534806265577, 0000000000000000000000000000000000000000000000000000) &amp;gt;= current key DecoratedKey(-7490754936938484492, 00ab1b0000000000000000000000000000000000000000000000) writing into /BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-tmp-jb-80-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:744)
 INFO [MemoryMeter:1] 2014-01-15 18:39:08,865 Memtable.java (line 451) CFS(Keyspace=&lt;span class=&quot;code-quote&quot;&gt;&apos;system&apos;&lt;/span&gt;, ColumnFamily=&lt;span class=&quot;code-quote&quot;&gt;&apos;sstable_activity&apos;&lt;/span&gt;) liveRatio is 14.596825396825396 (just-counted was 14.596825396825396).  calculation took 1ms &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 84 cells
 INFO [MemoryMeter:1] 2014-01-15 18:43:44,575 Memtable.java (line 451) CFS(Keyspace=&lt;span class=&quot;code-quote&quot;&gt;&apos;system&apos;&lt;/span&gt;, ColumnFamily=&lt;span class=&quot;code-quote&quot;&gt;&apos;sstable_activity&apos;&lt;/span&gt;) liveRatio is 14.591111111111111 (just-counted was 14.585396825396826).  calculation took 4ms &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 210 cells
(END) 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13872416" author="jbellis" created="Wed, 15 Jan 2014 18:50:10 +0000"  >&lt;p&gt;can you tar up all the components, not just .db?&lt;/p&gt;</comment>
                            <comment id="13872422" author="jbellis" created="Wed, 15 Jan 2014 18:54:18 +0000"  >&lt;p&gt;... for all the sstables in the &quot;Compacting&quot; list&lt;/p&gt;</comment>
                            <comment id="13872516" author="brandon.kearby" created="Wed, 15 Jan 2014 19:50:45 +0000"  >&lt;p&gt;Hi Jonathan,&lt;/p&gt;

&lt;p&gt;Here&apos;s a link to what you need: &lt;a href=&quot;http://bhorne.test.s3.amazonaws.com/cassandra.tar.gz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://bhorne.test.s3.amazonaws.com/cassandra.tar.gz&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13872602" author="brandon.kearby" created="Wed, 15 Jan 2014 21:11:51 +0000"  >&lt;p&gt;So it seems like it might be related to the hsa server. BTW, I was getting &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6373&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/CASSANDRA-6373&lt;/a&gt; where it would hang describing the ring. So I upgraded to thrift-server-0.3.3.jar. When running with the sync server, I don&apos;t get the error above.&lt;/p&gt;

&lt;p&gt;A little more context, I&apos;m using pig and the CassandraStorage class to drive the writes. Running as a map task with 12 concurrent mappers creates 2773 connections!&lt;/p&gt;

&lt;p&gt; lsof -i tcp:9160 | wc -l&lt;br/&gt;
2773&lt;/p&gt;</comment>
                            <comment id="13872734" author="jbellis" created="Wed, 15 Jan 2014 22:47:36 +0000"  >&lt;p&gt;The  .tar.gz does not contain the sstables mentioned in the error message&lt;/p&gt;</comment>
                            <comment id="13872739" author="brandon.kearby" created="Wed, 15 Jan 2014 22:53:56 +0000"  >&lt;p&gt;Correct. The tar contains a full log file with another example of the error.&lt;/p&gt;</comment>
                            <comment id="13872744" author="jbellis" created="Wed, 15 Jan 2014 22:57:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=enigmacurry&quot; class=&quot;user-hover&quot; rel=&quot;enigmacurry&quot;&gt;enigmacurry&lt;/a&gt; Can your team reproduce w/ the schema above and the sstables from the tarball?&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Compacting [SSTableReader(path=&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-2-Data.db&apos;), STableReader(path=&apos;/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-1-Data.db&apos;)]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13872801" author="brandon.kearby" created="Wed, 15 Jan 2014 23:34:29 +0000"  >&lt;p&gt;BTW, It happens when we use hsha. The schema above is abbreviated as I&apos;ve left out a lot of the payload from the signal table. Ping me if you need the full signal table definition.&lt;/p&gt;</comment>
                            <comment id="13872809" author="brandon.kearby" created="Wed, 15 Jan 2014 23:40:28 +0000"  >&lt;p&gt;When I cranked up the  number of rpc_max_threads and changed to sync, it stopped happening.&lt;/p&gt;</comment>
                            <comment id="13883112" author="brandon.kearby" created="Mon, 27 Jan 2014 18:55:36 +0000"  >&lt;p&gt;After doing some more digging, looks like my issue is the same as &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-4687&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/CASSANDRA-4687&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13883129" author="jbellis" created="Mon, 27 Jan 2014 19:10:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rhatch&quot; class=&quot;user-hover&quot; rel=&quot;rhatch&quot;&gt;rhatch&lt;/a&gt; would still be useful to try to repro w/ Brandon&apos;s instructions since we don&apos;t have a way to repro 4687 yet.&lt;/p&gt;</comment>
                            <comment id="13883253" author="rhatch" created="Mon, 27 Jan 2014 20:39:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbellis&quot; class=&quot;user-hover&quot; rel=&quot;jbellis&quot;&gt;jbellis&lt;/a&gt; &amp;#8211; I was able to get the exception to occur by doing the following:&lt;/p&gt;

&lt;p&gt;create a new cluster with ccm, and populate with 3 nodes&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;create keyspace SocialData with placement_strategy=&apos;org.apache.cassandra.locator.SimpleStrategy&apos; and strategy_options = {replication_factor:3};
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;create signal Column Family (I had to modify schema above a little bit to make it work):&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;create column family signal
  with column_type = &apos;Standard&apos;
  and comparator = &apos;UTF8Type&apos;
  and default_validation_class = &apos;BytesType&apos;
  and key_validation_class = &apos;UTF8Type&apos;
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 432000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = &apos;org.apache.cassandra.db.compaction.LeveledCompactionStrategy&apos;
  and caching = &apos;ALL&apos;
  and compaction_strategy_options = {&apos;sstable_size_in_mb&apos; : &apos;160&apos;}
  and comment = &apos;A store of information about each individual signal.&apos;
  and column_metadata = [
    {column_name : &apos;type&apos;, validation_class : UTF8Type},
    {column_name : &apos;foo_id&apos;, validation_class : LongType}]
  and compression_options = {&apos;sstable_compression&apos; : &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}; 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;stopped the nodes&lt;br/&gt;
copied all the files from the provided tar&apos;s /data/SocialData/ directory to one of my nodes&lt;br/&gt;
started the nodes up again&lt;br/&gt;
At this point I didn&apos;t find any data in the signal column family (using &apos;list signal;&apos;)&lt;br/&gt;
The exception appeared in the node&apos;s log&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;ERROR [CompactionExecutor:10] 2014-01-27 12:45:26,734 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:10,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(4322717900587903123, 706f737431353834373031323038270903ae0022076d9f) &amp;gt;= current key DecoratedKey(-7009815163526224622, 545749545445523a333533343836323333393032363439333437) writing into /home/rhatch/.ccm/test_cluster_1390845354/node1/data/SocialData/signal/SocialData-signal-tmp-jb-7-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I was curious if repair would have any bearing, so I ran repair on one node (after which I can see data in the signal table), then I stopped and started the nodes again &amp;#8211; a similar exception appears in the log for all 3 nodes (&apos;Last written key DecoratedKey ...&apos;).&lt;/p&gt;

&lt;p&gt;I&apos;m not 100% certain if my procedure for using the provided tar&apos;s test data was correct, so let me know if there&apos;s anything obvious I missed and I&apos;ll run through it again.&lt;/p&gt;</comment>
                            <comment id="13883265" author="jbellis" created="Mon, 27 Jan 2014 20:45:53 +0000"  >&lt;p&gt;Was that 2.0 HEAD or 2.0.4?&lt;/p&gt;</comment>
                            <comment id="13883267" author="rhatch" created="Mon, 27 Jan 2014 20:50:26 +0000"  >&lt;p&gt;oh sorry, forgot that detail. I reproduced from the cassandra-2.0.2 tag.&lt;/p&gt;</comment>
                            <comment id="13883274" author="jbellis" created="Mon, 27 Jan 2014 20:52:16 +0000"  >&lt;p&gt;Can you try 2.0 HEAD as well just to be sure?&lt;/p&gt;</comment>
                            <comment id="13883313" author="rhatch" created="Mon, 27 Jan 2014 21:11:48 +0000"  >&lt;p&gt;OK, appears we have the same issue on 2.0 HEAD as well (8bbb6e...) &amp;#8211; exception appears on startup using the procedure I included earlier.&lt;/p&gt;</comment>
                            <comment id="13883319" author="rhatch" created="Mon, 27 Jan 2014 21:17:59 +0000"  >&lt;p&gt;I&apos;m going to attempt to condense this down to a simple dtest as well.&lt;/p&gt;</comment>
                            <comment id="13883375" author="brandon.kearby" created="Mon, 27 Jan 2014 21:45:20 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rhatch&quot; class=&quot;user-hover&quot; rel=&quot;rhatch&quot;&gt;rhatch&lt;/a&gt;, &lt;/p&gt;

&lt;p&gt;Here&apos;s the full schema I&apos;m using to test with:&lt;/p&gt;

&lt;p&gt;create keyspace SocialData&lt;br/&gt;
  with placement_strategy = &apos;NetworkTopologyStrategy&apos;&lt;br/&gt;
  and strategy_options = &lt;/p&gt;
{DC-Analytics : 3}
&lt;p&gt;  and durable_writes = true;&lt;/p&gt;

&lt;p&gt;use SocialData;&lt;/p&gt;

&lt;p&gt;create column family signal&lt;br/&gt;
  with column_type = &apos;Standard&apos;&lt;br/&gt;
  and comparator = &apos;UTF8Type&apos;&lt;br/&gt;
  and default_validation_class = &apos;BytesType&apos;&lt;br/&gt;
  and key_validation_class = &apos;UTF8Type&apos;&lt;br/&gt;
  and read_repair_chance = 0.1&lt;br/&gt;
  and dclocal_read_repair_chance = 0.0&lt;br/&gt;
  and gc_grace = 432000&lt;br/&gt;
  and min_compaction_threshold = 4&lt;br/&gt;
  and max_compaction_threshold = 32&lt;br/&gt;
  and replicate_on_write = true&lt;br/&gt;
  and compaction_strategy = &apos;org.apache.cassandra.db.compaction.LeveledCompactionStrategy&apos;&lt;br/&gt;
  and caching = &apos;NONE&apos;&lt;br/&gt;
  and compaction_strategy_options = &lt;/p&gt;
{&apos;sstable_size_in_mb&apos; : &apos;160&apos;}
&lt;p&gt;  and comment = &apos;A store of information about each individual signal.&apos;&lt;br/&gt;
  and column_metadata = [&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;type&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;department_id&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;ecosystem_account_id&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;content_type&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;rating_count&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;service_account_id&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;time&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;organization_id&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;conversation_id&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;favorites_count&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;dislike_count&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;url&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;impressions&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;network_strength&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;parent_signal_id&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;account_snapshot_id&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;region_id&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;time_bucket&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;enriched_on&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;dachis_account_id&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;text&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;sentiment&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;like_count&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;industry_id&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;service&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;cloned_from&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;constituent_type&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;listings_count&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;network_size&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;analyzed&apos;,
    validation_class : Int32Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;username&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;service_signal_id&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;language&apos;,
    validation_class : UTF8Type}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;brand_id&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;rating&apos;,
    validation_class : LongType}
&lt;p&gt;,&lt;br/&gt;
    &lt;/p&gt;
{column_name : &apos;relationship_id&apos;,
    validation_class : UTF8Type}
&lt;p&gt;]&lt;br/&gt;
  and compression_options = &lt;/p&gt;
{&apos;sstable_compression&apos; : &apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;}
&lt;p&gt;;&lt;/p&gt;
</comment>
                            <comment id="13884381" author="rhatch" created="Tue, 28 Jan 2014 18:22:58 +0000"  >&lt;p&gt;I was able to repro with the data provided in the tar (as noted above). Unfortunately my attempts to use dtest to reproduce the issue &quot;from scratch&quot; haven&apos;t been successful. First I tried testing with dtest w/CQL, but had no luck, though I may have missed something when trying to translate everything into CQL.&lt;/p&gt;

&lt;p&gt;Next I tried dtest w/thrift but similarly wasn&apos;t able to trigger the issue in this way. I tried using the hsha rpc_server and was getting what appeared to be an unrelated error:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;ERROR [Thrift-Selector_0] 2014-01-28 11:09:16,762 Message.java (line 153) Read an invalid frame size of 0. Are you using TFramedTransport on the client side?
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&apos;ll attach my basic dtest here in case it&apos;s useful later, but as for now I can&apos;t repro the issue without the provided db.&lt;/p&gt;</comment>
                            <comment id="13888227" author="thobbs" created="Fri, 31 Jan 2014 22:06:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brandon.kearby&quot; class=&quot;user-hover&quot; rel=&quot;brandon.kearby&quot;&gt;brandon.kearby&lt;/a&gt; It looks like there may be a few things going on here.&lt;/p&gt;

&lt;p&gt;The first is that some of your column names are not valid UTF-8.  I&apos;m not terribly familiar with the UTF-8 specs, but they seem to fail validation in different ways, and Python seems to agree that they are not valid UTF-8, so I don&apos;t think it&apos;s a problem with our validation code.  Did you change the comparator from BytesType to UTF8Type at some point?  It might not be relevant to this ticket, but you may want to check on that on your end.&lt;/p&gt;

&lt;p&gt;The second problem is that SocialData-signal-jb-2-Data.db has some out-of-order rows.  It looks like about 9 rows are randomly out of place in the sstable.  Running scrub would fix this, but I think it&apos;s erroring on UTF8 validation.  If I change the comparator to BytesType, the scrub completes and the rows are written in order.  So the problem is not necessarily with compaction itself but with out-of-order rows being written to sstables.&lt;/p&gt;

&lt;p&gt;Given that switching from hsha to sync seemed to fix the problem, I wonder if that&apos;s part of the original cause.&lt;/p&gt;</comment>
                            <comment id="13893563" author="brandon.kearby" created="Thu, 6 Feb 2014 17:32:35 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thobbs&quot; class=&quot;user-hover&quot; rel=&quot;thobbs&quot;&gt;thobbs&lt;/a&gt;, The odd thing is that with hsha, it works with one node. When we have two or more nodes in the cluster, it starts getting these errors.&lt;/p&gt;</comment>
                            <comment id="13893660" author="ravilr" created="Thu, 6 Feb 2014 18:59:11 +0000"  >&lt;p&gt;cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt;&lt;br/&gt;
we were also seeing such random out of place partitions/rows in sstables (rows not hashing to the node) while using disruptor based hsha thrift server, causing compaction to fail with out of order keys. this used to happen on freshly flushed sstables in L0.  We also used to see thrift validation failing on some columns while reading back.  We don&apos;t see these after switching back to sync server.&lt;/p&gt;


</comment>
                            <comment id="13893751" author="xedin" created="Thu, 6 Feb 2014 20:29:14 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ravilr&quot; class=&quot;user-hover&quot; rel=&quot;ravilr&quot;&gt;ravilr&lt;/a&gt; Can you try with the most recent release of hsha, version 0.3.3? Just remove the old jar and drop in new one, that should be sufficient.&lt;/p&gt;</comment>
                            <comment id="13893800" author="brandon.kearby" created="Thu, 6 Feb 2014 20:58:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt;, I was running with 0.3.3. The previous version would hang on describe ring for me.&lt;/p&gt;</comment>
                            <comment id="13893894" author="xedin" created="Thu, 6 Feb 2014 22:19:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rhatch&quot; class=&quot;user-hover&quot; rel=&quot;rhatch&quot;&gt;rhatch&lt;/a&gt; The 0 frame size you are seeing is a known Thrift problem which happens even with stock server implementations, they are working on it but it shouldn&apos;t cause any problems as such frames are ignored (it could also happen if something does e.g. telnet to the thrift port). I&apos;m not sure that this is a problem with HsHa directly but might be unveiled by the increased throughput you can get with HsHa comparing to sync, it looks exactly like &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-4687&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/CASSANDRA-4687&lt;/a&gt; (as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brandon.kearby&quot; class=&quot;user-hover&quot; rel=&quot;brandon.kearby&quot;&gt;brandon.kearby&lt;/a&gt; mentioned) so can you try disabling key_cache and try uploading again with hsha?&lt;/p&gt;</comment>
                            <comment id="13893956" author="brandon.kearby" created="Thu, 6 Feb 2014 23:12:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt;, I&apos;ve tried disabling the key_cache and it didn&apos;t help. That was the last thing I tried.&lt;/p&gt;</comment>
                            <comment id="13893967" author="xedin" created="Thu, 6 Feb 2014 23:26:22 +0000"  >&lt;p&gt;I see that you have it set to NONE in CF schema but have you also tried disabling it all together in yaml? I&apos;m not saying that it would help but trying to eliminate all possibilities. It&apos;s just not obvious to me if it&apos;s a hsha problem how Thrift could actually be correctly interpreting erroneous data from the socket, dispatching it the right Thrift handler and deserializing whole mutation (and meta information) to insert it into storage...&lt;/p&gt;</comment>
                            <comment id="13894144" author="ravilr" created="Fri, 7 Feb 2014 02:56:54 +0000"  >&lt;p&gt;Also, one more factor with disruptor based hsha is direct memory/Unsafe versus heap-based message buffers. When we encountered this issue, we were running with jna,  hence was using direct memory buffers. I didn&apos;t test with heap-based message buffers. &lt;/p&gt;</comment>
                            <comment id="13903449" author="ngrigoriev" created="Mon, 17 Feb 2014 19:00:18 +0000"  >&lt;p&gt;I have started seeing these too. Surprisingly...after adding OpsCenter CE to my cluster. I do not see these associated with my own data.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.RuntimeException: Last written key DecoratedKey(3542937286762954312, 31302e332e34352e3135382d676574466c757368657350656e64696e67) &amp;gt;= current
key DecoratedKey(-2152912038130700738, 31302e332e34352e3135362d77696e7465726d7574655f6a6d657465722d776d5f6170706c69636174696f6e732d676574526563656e744
26c6f6f6d46) writing into /hadoop/disk1/cassandra/data/OpsCenter/rollups300/OpsCenter-rollups300-tmp-jb-5055-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:724)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13903655" author="kohlisankalp" created="Tue, 18 Feb 2014 00:48:28 +0000"  >&lt;p&gt;This issue is also in logs attached in &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6716&quot; title=&quot;snapshots constantly fail with &amp;quot;Tried to hard link to file that does not exist&amp;quot;&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6716&quot;&gt;&lt;del&gt;CASSANDRA-6716&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="13907434" author="ngrigoriev" created="Thu, 20 Feb 2014 20:06:38 +0000"  >&lt;p&gt;Can confirm on my side. I have switched to &quot;sync&quot; RPC server and after few scrubs/restarts I am running my load tests on a 6-node 2.0.5 cluster without a single exception in last ~8 hours.&lt;/p&gt;

&lt;p&gt;I tried to correlate the moment I started getting large number of FileNotFoundException&apos;s with other events in my cluster....realized that it was not exactly 2.0.5 upgrade. It seems to correlate mostly with a moment when my jmeter server went out of free space and a bunch of tests crashed. Obviously, these crashes have terminated a few hundreds of client connections to Cassandra.&lt;/p&gt;

&lt;p&gt;Not sure if it is related but it seems that from that moment it was some sort of snowball effect.&lt;/p&gt;</comment>
                            <comment id="13917874" author="krummas" created="Mon, 3 Mar 2014 09:37:18 +0000"  >&lt;p&gt;I think we can conclude that this only happens with HSHA&lt;/p&gt;

&lt;p&gt;I brought back TThreadedSelectorServer instead of the Disruptor based server and have been running it for a few hours without the bug happening.&lt;/p&gt;

&lt;p&gt;Could someone (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kvaster&quot; class=&quot;user-hover&quot; rel=&quot;kvaster&quot;&gt;kvaster&lt;/a&gt; ?) try out &lt;a href=&quot;https://github.com/krummas/cassandra/commits/marcuse/hsha&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/krummas/cassandra/commits/marcuse/hsha&lt;/a&gt; and see if you can break it?&lt;/p&gt;

&lt;p&gt;Note that i had to make a change in thrift 0.9.1 to get it to build and work, I&apos;ll follow up on that if this seems to solve the issue.&lt;/p&gt;</comment>
                            <comment id="13917950" author="kvaster" created="Mon, 3 Mar 2014 11:06:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://www.dropbox.com/s/3ldg10zh7qvva27/cassandra-attack.jar&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/3ldg10zh7qvva27/cassandra-attack.jar&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Schema is located inside jar file - cassandra.txt&lt;/p&gt;

&lt;p&gt;1. Start cassandra&lt;br/&gt;
2. java -jar cassandra-attack.jar&lt;br/&gt;
3. Stop cassandra&lt;br/&gt;
4. Start cassandra - commit logs will be corrupted.&lt;/p&gt;</comment>
                            <comment id="13918048" author="ngrigoriev" created="Mon, 3 Mar 2014 13:38:43 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I think using HSHA makes it easier to reproduce but...I am running SYNC for over a week now and recently I have experienced the same issue again.&lt;/p&gt;

&lt;p&gt;We had another unclean shutdown (hrrr...some people are smarter than the UPSes &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; ) and after bringing the nodes back I have found that  on one node my compactions constantly fail with FileNotFoundException. Even worse, I can&apos;t scrub the keyspace/CF in question because &quot;scrub&quot; fails instantly with &quot;RuntimeException: Tried to hard link to file that does not exist...&quot;. I have reported that one too. It is impossible to scrub. The only way to fix that issue I have found so far is to restart Cassandra on that node, stop compactions as soon as it starts (well, I could disable them differently, I assume) and then scrub. Sometimes I have to do it in several iterations to complete the process. Once I scrub all problematic KS/CFs I see no more exceptions.&lt;/p&gt;</comment>
                            <comment id="13918053" author="kvaster" created="Mon, 3 Mar 2014 13:42:11 +0000"  >&lt;p&gt;Bug with FileNotFoundException is not related to HsHa problem.&lt;br/&gt;
And about several iterations for scrub:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6791&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/CASSANDRA-6791&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13918157" author="jbellis" created="Mon, 3 Mar 2014 15:31:32 +0000"  >&lt;p&gt;According to &lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/cassandra-user/201402.mbox/%3C038601cf28ea$a2e504d0$e8af0e70$@struq.com%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mail-archives.apache.org/mod_mbox/cassandra-user/201402.mbox/%3C038601cf28ea$a2e504d0$e8af0e70$@struq.com%3E&lt;/a&gt; the 0.9 TThreadSelectorServer works well, although I&apos;m not sure if he means that it performs better than 0.8 TTSS or just that it doesn&apos;t cause corruption. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13918212" author="kvaster" created="Mon, 3 Mar 2014 16:19:23 +0000"  >&lt;p&gt;I&apos;ve tried to investigate problem with HsHaDistruptorServer, but with no luck. Telling the truth, I see no reason for that server to corrupt data.&lt;br/&gt;
Also HsHaDistruptor do not corrupt data in case useHeapBasedAllocation is turned on.&lt;br/&gt;
More over if you look at disruptor-thrift-server code - Message.reallocateDataBuffer and turn on heap based allocation only for dataBuffer then you will not see corruption.&lt;/p&gt;</comment>
                            <comment id="13918343" author="xedin" created="Mon, 3 Mar 2014 18:14:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ngrigoriev&quot; class=&quot;user-hover&quot; rel=&quot;ngrigoriev&quot;&gt;ngrigoriev&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brandon.kearby&quot; class=&quot;user-hover&quot; rel=&quot;brandon.kearby&quot;&gt;brandon.kearby&lt;/a&gt; can you try setting useHeapBasedAllocation to &quot;true&quot; ? I&apos;m fine with switch back to TThreadedSelectorServer if that helps.&lt;/p&gt;</comment>
                            <comment id="13918463" author="ngrigoriev" created="Mon, 3 Mar 2014 19:22:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;That seems to be a parameter of the Thrift server...How do I control this parameter? Or I should just disable JNA?&lt;/p&gt;</comment>
                            <comment id="13918496" author="xedin" created="Mon, 3 Mar 2014 19:45:02 +0000"  >&lt;p&gt;You can do it via JMX or disable JNA, I can also make a patch with would set it explicitly in Cassandra code.&lt;/p&gt;</comment>
                            <comment id="13919643" author="chris.wirt" created="Tue, 4 Mar 2014 17:18:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Jonathan+Ellis&quot; class=&quot;user-hover&quot; rel=&quot;Jonathan Ellis&quot;&gt;Jonathan Ellis&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Marcus+Eriksson&quot; class=&quot;user-hover&quot; rel=&quot;Marcus Eriksson&quot;&gt;Marcus Eriksson&lt;/a&gt; That was my post from the user mail list.&lt;br/&gt;
After our 1.2.14 -&amp;gt; 2.0.5 upgrade and failure to get the new HsHa stable in our system, we moved to using the thrift 0.9.1 TTSS with reasonable success. We&apos;ve now been running for two weeks under a relatively high read load.&lt;br/&gt;
We haven&apos;t seen any &quot;DecoratedKey != ...&quot; errors.&lt;br/&gt;
We have seen some warnings on start up about SSTable rows being out of order.&lt;br/&gt;
We have seen commit logs starting to build up with &quot;All time blocked&quot; on the FlushWriter incrementing.&lt;/p&gt;

&lt;p&gt;Performance comparisons might be a little unfair, but certainly our p95, p99 have overall improved.&lt;/p&gt;

&lt;p&gt;Obviously very keen to not be running a custom build of C*.&lt;/p&gt;</comment>
                            <comment id="13919656" author="krummas" created="Tue, 4 Mar 2014 17:26:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chris.wirt&quot; class=&quot;user-hover&quot; rel=&quot;chris.wirt&quot;&gt;chris.wirt&lt;/a&gt; could you paste those startup log lines?&lt;/p&gt;</comment>
                            <comment id="13919679" author="chris.wirt" created="Tue, 4 Mar 2014 17:45:26 +0000"  >&lt;p&gt;These have since disappeared. I just restarted this node just now to check. We haven&apos;t run a scrub.&lt;/p&gt;

&lt;p&gt; WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2014-02-16 23:23:02,032 LeveledManifest.java (line 171) At level 1, SSTableReader(path=&apos;/disk2/cassandra/data/struqrealtime/impressionstorev2/struqrealtime-impressionstorev2-jb-118905-Data.db&apos;) &lt;span class=&quot;error&quot;&gt;&amp;#91;DecoratedKey(-1513272878957942943, c41c955b40274acfa466ccb6079a21e5), DecoratedKey(6301362410765453237, 43ae61aacbc446be92c8bdea1d43e342)&amp;#93;&lt;/span&gt; overlaps SSTableReader(path=&apos;/disk2/cassandra/data/struqrealtime/impressionstorev2/struqrealtime-impressionstorev2-jb-116400-Data.db&apos;) &lt;span class=&quot;error&quot;&gt;&amp;#91;DecoratedKey(3953001739649874864, 5811ce41b7014917ab82eb32e8861ca5), DecoratedKey(9190609424240623933, 4e5b00a5a7594289924674974f44a995)&amp;#93;&lt;/span&gt;.  This could be caused by a bug in Cassandra 1.1.0 .. 1.1.3 or due to the fact that you have dropped sstables from another node into the data directory. Sending back to L0.  If you didn&apos;t drop in sstables, and have not yet run scrub, you should do so since you may also have rows out-of-order within an sstable&lt;/p&gt;</comment>
                            <comment id="13919748" author="krummas" created="Tue, 4 Mar 2014 18:41:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chris.wirt&quot; class=&quot;user-hover&quot; rel=&quot;chris.wirt&quot;&gt;chris.wirt&lt;/a&gt; those entries are unrelated (and fixed in &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6688&quot; title=&quot;Avoid possible sstable overlaps with leveled compaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6688&quot;&gt;&lt;del&gt;CASSANDRA-6688&lt;/del&gt;&lt;/a&gt;)&lt;/p&gt;</comment>
                            <comment id="13919835" author="xedin" created="Tue, 4 Mar 2014 19:40:46 +0000"  >&lt;p&gt;The patch sets heap based allocation by default in disruptor server, should make it easier for people to test that scenario...&lt;/p&gt;</comment>
                            <comment id="13922897" author="jbellis" created="Thu, 6 Mar 2014 19:05:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mshuler&quot; class=&quot;user-hover&quot; rel=&quot;mshuler&quot;&gt;mshuler&lt;/a&gt; Can you test hsha with Viktor&apos;s jar above?  (&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6285?focusedCommentId=13917950&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13917950&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/CASSANDRA-6285?focusedCommentId=13917950&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13917950&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;I want to know if&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;you can reproduce with a single node&lt;/li&gt;
	&lt;li&gt;if not, if you can reproduce with multiple nodes&lt;/li&gt;
	&lt;li&gt;assuming either 1 or 2, if you can still reproduce after applying Pavel&apos;s heap allocation path&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13922903" author="mshuler" created="Thu, 6 Mar 2014 19:08:51 +0000"  >&lt;p&gt;Sure - let me see what I can find out.&lt;/p&gt;</comment>
                            <comment id="13923012" author="mshuler" created="Thu, 6 Mar 2014 20:29:38 +0000"  >&lt;p&gt;6285_testnotes1.txt attached.&lt;/p&gt;

&lt;p&gt;Neither a single node with hsha, nor a 3 node ccm cluster with hsha gave me any interesting errors with the attack jar.  Should I go back and try some of the previous repro steps and check yay/nay on the patch fixing this for those?&lt;/p&gt;</comment>
                            <comment id="13923022" author="xedin" created="Thu, 6 Mar 2014 20:37:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mshuler&quot; class=&quot;user-hover&quot; rel=&quot;mshuler&quot;&gt;mshuler&lt;/a&gt; Can you try the same on the machine running Linux (if you haven&apos;t done that yet)? &lt;/p&gt;

&lt;p&gt;Edit: from the log it looks like Disruptor wasn&apos;t using the off-heap memory because JNA is disabled, &quot;Off-heap allocation couldn&apos;t be used as JNA is not present in classpath or broken, using on-heap instead.&quot; So it would be great if you could test this on Linux with jna enabled.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;</comment>
                            <comment id="13923026" author="mshuler" created="Thu, 6 Mar 2014 20:38:53 +0000"  >&lt;p&gt;I&apos;m using a linux machine  &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;  - and will link in JNA - good suggestion.&lt;/p&gt;</comment>
                            <comment id="13923043" author="mshuler" created="Thu, 6 Mar 2014 20:51:51 +0000"  >&lt;p&gt;With jna enabled, yes, on a single node, after running the attack jar and restarting c*, I get:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; INFO [main] 2014-03-06 14:46:51,272 ColumnFamilyStore.java (line 254) Initializing tmp.CF
 INFO [main] 2014-03-06 14:46:51,277 ColumnFamilyStore.java (line 254) Initializing system_traces.sessions
 INFO [main] 2014-03-06 14:46:51,280 ColumnFamilyStore.java (line 254) Initializing system_traces.events
 INFO [main] 2014-03-06 14:46:51,281 CassandraDaemon.java (line 291) completed pre-loading (5 keys) key cache.
 INFO [main] 2014-03-06 14:46:51,288 CommitLog.java (line 130) Replaying /var/lib/cassandra/commitlog/CommitLog-3-1394138577628.log, /var/lib/
cassandra/commitlog/CommitLog-3-1394138577629.log
 INFO [main] 2014-03-06 14:46:51,311 CommitLogReplayer.java (line 184) Replaying /var/lib/cassandra/commitlog/CommitLog-3-1394138577628.log (C
L version 3, messaging version 7)
ERROR [main] 2014-03-06 14:46:51,432 CommitLogReplayer.java (line 306) Unexpected error deserializing mutation; saved to /tmp/mutation77387084
28696995512dat and ignored.  This may be caused by replaying a mutation against a table with the same name but incompatible schema.  Exception
 follows: 
org.apache.cassandra.serializers.MarshalException: Invalid version for TimeUUID type.
        at org.apache.cassandra.serializers.TimeUUIDSerializer.validate(TimeUUIDSerializer.java:39)
        at org.apache.cassandra.db.marshal.AbstractType.validate(AbstractType.java:172)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:276)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:97)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:151)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:131)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:312)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:471)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:560)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&apos;ll double-check a 3 node cluster, then patch and see where I get.&lt;/p&gt;

&lt;p&gt;(edit) this looks quite different than the previously posted errors - not sure if I&apos;m on the right track, here..&lt;/p&gt;</comment>
                            <comment id="13923104" author="mshuler" created="Thu, 6 Mar 2014 21:35:37 +0000"  >&lt;p&gt;Both a single local hsha node and 3x node ccm cluster with hsha (jna on both) throw the above errors after attack.jar run and restart.  The patch does appear to fix both single and ccm cluster.  My pre-patch ccm cluster never fully restarted, but do we need logs or anything from before/after?&lt;/p&gt;</comment>
                            <comment id="13923112" author="xedin" created="Thu, 6 Mar 2014 21:39:31 +0000"  >&lt;p&gt;I don&apos;t think we need logs, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbellis&quot; class=&quot;user-hover&quot; rel=&quot;jbellis&quot;&gt;jbellis&lt;/a&gt; I&apos;m thinking of commiting attached patch which should help meanwhile I working on resolving off-heap problem, WDYT?&lt;/p&gt;</comment>
                            <comment id="13923125" author="benedict" created="Thu, 6 Mar 2014 21:56:12 +0000"  >&lt;p&gt;So, I think there may potentially be at least two races in the off heap deallocation. I suspect that may not be everything, though, as these two races probably won&apos;t cause the problem often. These are predicated on the assumption that thrift doesn&apos;t copy data from the DirectByteBuffer that the hsha server provides to it, so could be wrong, but anyway:&lt;/p&gt;

&lt;p&gt;1) CL appends can be lagged behind the memtable update and, as a result, the acknowledgment to the client of success writing. If the CL record contains the ByteBuffer when it is freed, and that address is then reused in another allocation, it will write incorrect data to the commit log.&lt;br/&gt;
2) I believe thrift calls are two stage. If this is the case, and the client disconnects in between sending the first stage and receiving the result in the second stage, the buffer could be freed whilst still in flight to the memtable/CL&lt;/p&gt;

&lt;p&gt;These are just quick ideas for where it might be, I haven&apos;t familiarised myself fully with thrift, the disruptor etc. to be certain if these are plausible, but it may turn out to be useful so thought I&apos;d share.&lt;/p&gt;</comment>
                            <comment id="13923245" author="rbranson" created="Thu, 6 Mar 2014 23:21:49 +0000"  >&lt;p&gt;Unfortunately this hit us during our 2.0.5 upgrade and &apos;sync&apos; is not an option for the # of connections we have per node (tried this). We&apos;ve been running Marcus&apos; patch in prod and limping along on it, but it looks like the requestInvoke() override is causing the requests to get executed on the selector pool (which is limited by CPU #) instead of the executor service so our response times are pretty bad. The lack of anything showing up in the JMX for the executor service definitely points towards this. &lt;/p&gt;</comment>
                            <comment id="13923275" author="xedin" created="Thu, 6 Mar 2014 23:57:32 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rbranson&quot; class=&quot;user-hover&quot; rel=&quot;rbranson&quot;&gt;rbranson&lt;/a&gt; Which Marcus&apos; patch are you talking about? Also I want to clarify one thing - disruptor server doesn&apos;t use requestInvoke(FrameBuffer) but dispatchInvoke(Message) which schedules message to executor pool based on ring buffer (WorkerPool) so actual execution is done in the separate thread. I can attach a patch which would switch back to TThreadedSelectorServer which is packed with Thrift (the only different between it and disruptor is that it schedules to classic thread pool), maybe disruptor server wasn&apos;t as good an idea for all of the real world use cases...&lt;/p&gt;</comment>
                            <comment id="13923318" author="benedict" created="Fri, 7 Mar 2014 00:34:32 +0000"  >&lt;p&gt;It looks like thrift doesn&apos;t retain the DirectByteBuffer, just reads straight from them. The only possible window for corruption is during the construction of the thrift args object, which is a fairly narrow window.&lt;/p&gt;</comment>
                            <comment id="13923441" author="rbranson" created="Fri, 7 Mar 2014 02:27:26 +0000"  >&lt;p&gt;We put in the TThreadedSelectorServer patch from Marcus. On top of that, to get our response times down from the 10x what they should be, I rolled out a larger hard-coded selector thread pool size of 256 (instead of the # of processors &amp;#8211; a measly 16). This is shaping up nicely.&lt;/p&gt;</comment>
                            <comment id="13923453" author="xedin" created="Fri, 7 Mar 2014 02:38:55 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rbranson&quot; class=&quot;user-hover&quot; rel=&quot;rbranson&quot;&gt;rbranson&lt;/a&gt; So what you are saying is that after problem with disruptor you never tried it again with on-heap buffers but switched to TThreadedSelectorServer and increased selector pool size and the requestInvoke() is the problem with TThreadedSelectorServer?&lt;/p&gt;</comment>
                            <comment id="13923466" author="rbranson" created="Fri, 7 Mar 2014 02:53:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt;: We had perf issues with the disruptor as well (sudden spikes of CPU to 100%) + this so I just wanted to get production away from it ASAP.&lt;/p&gt;</comment>
                            <comment id="13923484" author="xedin" created="Fri, 7 Mar 2014 03:22:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rbranson&quot; class=&quot;user-hover&quot; rel=&quot;rbranson&quot;&gt;rbranson&lt;/a&gt; But the most important question for this ticket at least is - did you run with on or off heap buffers? I can bring TThreadedSelectorServer back in this ticket or just go with on-heap buffers and disruptor. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mshuler&quot; class=&quot;user-hover&quot; rel=&quot;mshuler&quot;&gt;mshuler&lt;/a&gt; do you have any performance tests related to Thrift server? Maybe there is a low hanging fruit in there to fix up the spikes that Rick mentioned if we can reproduce.&lt;/p&gt;</comment>
                            <comment id="13923494" author="jbellis" created="Fri, 7 Mar 2014 03:36:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m thinking of commiting attached patch which should help meanwhile I working on resolving off-heap problem&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yeah, let&apos;s do this for now and roll 2.0.6 so we can stop the bleeding, then figure out whether doing more work on disruptor or TTSS is better for 2.0.7.&lt;/p&gt;</comment>
                            <comment id="13923514" author="rbranson" created="Fri, 7 Mar 2014 03:59:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt;: off-heap for disruptor.&lt;/p&gt;

&lt;p&gt;I think that we should really consider bringing back the old HSHA implementation from 1.2 as the &quot;hsha&quot; and allow switching to the disruptor implementation as another rpc_server_type for those that want to try it out.&lt;/p&gt;</comment>
                            <comment id="13923522" author="brandon.williams" created="Fri, 7 Mar 2014 04:10:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think that we should really consider bringing back the old HSHA implementation from 1.2 as the &quot;hsha&quot; and allow switching to the disruptor implementation as another rpc_server_type for those that want to try it out.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think I&apos;m inclined to agree with this, aside from creating yaml creation problems in a minor.  If we just give up and effectively revert, what have we lost?  We need 2.0 stabilization sooner rather than later now, in all aspects.  If we can&apos;t trust disruptor except with a small change, let&apos;s just not trust it yet and worry about that in a future release.  With 2.1 beta already out, we can&apos;t tolerate much instability in the 2.0 branch.&lt;/p&gt;</comment>
                            <comment id="13923541" author="xedin" created="Fri, 7 Mar 2014 04:37:21 +0000"  >&lt;p&gt;I&apos;m not sure if there is a point of going all the way back to original HsHa when there is TThrededSelectorServer, but I&apos;m fine with going with disruptor as a separate option, something like &quot;disruptor_hsha&quot; and making &quot;hsha&quot; - TThrededSelectorServer from Thrift, that&apos;s how I wanted it originally. Also I just want to mention that people have reported that disruptor works for them with on-heap buffers, so I am not sure if we need to go all paranoid about this...&lt;/p&gt;</comment>
                            <comment id="13923640" author="krummas" created="Fri, 7 Mar 2014 07:02:21 +0000"  >&lt;p&gt;My branch from above needed a tiny hack to thrift (&lt;a href=&quot;https://github.com/krummas/thrift/commit/01ba2a3f3d386d0981371aab2494470e2a78e596&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/krummas/thrift/commit/01ba2a3f3d386d0981371aab2494470e2a78e596&lt;/a&gt;), so if we want to roll with TTSS we should refactor our thrift usage a bit to avoid that hack&lt;/p&gt;</comment>
                            <comment id="13923716" author="xedin" created="Fri, 7 Mar 2014 09:35:01 +0000"  >&lt;p&gt;Ah, so they have finally made transport a protected field in FrameBuffer... Well, that considerably complicates things with switching back TThreadedSelectorServer.&lt;/p&gt;</comment>
                            <comment id="13923745" author="slebresne" created="Fri, 7 Mar 2014 10:21:03 +0000"  >&lt;p&gt;Alright, I&apos;ve committed Pavel&apos;s patch above as a stopgap solution as discussed above because I want to start a vote on 2.0.6 asap (the changelog is getting pretty big). I&apos;ve created &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6815&quot; title=&quot;Decided if we want to bring back thrift HSHA in 2.0.7&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6815&quot;&gt;&lt;del&gt;CASSANDRA-6815&lt;/del&gt;&lt;/a&gt; to decide what we want the followup for that to be for 2.0.7.&lt;/p&gt;</comment>
                            <comment id="13924330" author="mshang" created="Fri, 7 Mar 2014 20:53:48 +0000"  >&lt;p&gt;To add to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rbranson&quot; class=&quot;user-hover&quot; rel=&quot;rbranson&quot;&gt;rbranson&lt;/a&gt;&apos;s input, we&apos;re also seeing the same stacktrace as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mshuler&quot; class=&quot;user-hover&quot; rel=&quot;mshuler&quot;&gt;mshuler&lt;/a&gt; (TimeUUID MarshalException). I inspected the row mutations that caused it. Three ranges were nonsensical: the key, the column name, and the value. By nonsensical, I mean that they don&apos;t match my expectation of what we are inserting in production data. All other ranges seemed fine (timestamps, masks, sizes, cfid). The key, column name, and value were read successfully, so their length metadata was good. For our data, the column comparator is TimeUUID. Our client library is pycassa. Whereas pycassa generates tuuids like this: 913d7fea-a631-11e3-8080-808080808080, the nonsensical column names look like this: 22050aa4-de11-e380-8080-80808080800b and this: 10c326eb-86a4-e211-e380-808080808080. Most are of the first form. By shifting these nonsensical tuuids to the left or right by an octet, you get a reasonable tuuid. I don&apos;t have a similar insight into the nonsensical keys and values, but they could also be left or right shifted.&lt;/p&gt;</comment>
                            <comment id="13925433" author="enigmacurry" created="Mon, 10 Mar 2014 03:55:07 +0000"  >&lt;p&gt;I&apos;d like to be able to reproduce this in dtests to track this bug. Seeing as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rhatch&quot; class=&quot;user-hover&quot; rel=&quot;rhatch&quot;&gt;rhatch&lt;/a&gt;&apos;s python test wasn&apos;t able to repro this issue, and a &lt;a href=&quot;https://github.com/riptano/cassandra-dtest/blob/cassandra-6285/test_6285.py&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;quick test I wrote&lt;/a&gt; doesn&apos;t either, does anyone have a simple way to reproduce this issue? &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=kvaster&quot; class=&quot;user-hover&quot; rel=&quot;kvaster&quot;&gt;kvaster&lt;/a&gt; would you mind sharing the source code for your attack jar?&lt;/p&gt;</comment>
                            <comment id="13925591" author="kvaster" created="Mon, 10 Mar 2014 09:33:05 +0000"  >&lt;p&gt;Attached patches for on-heap disruptor.&lt;/p&gt;

&lt;p&gt;First pacth (disruptor-high-cpu.patch) turns off any key interests in case we&apos;re waiting for message to be processed. We need that cause processing may be delayed in case of high load and there may be something available to read from stream. In that case we&apos;ll have 100% cpu core usage.&lt;/p&gt;

&lt;p&gt;Second patch (disruptor-memory-corruption.patch) makes copy from off-heap ByteBuffer when reading binary data. This binary data may be stored inside cassandra as is even after message processing. And binary data can be corrupted - cause it&apos;s memory may be already deallocated.&lt;/p&gt;</comment>
                            <comment id="13925599" author="kvaster" created="Mon, 10 Mar 2014 09:42:10 +0000"  >&lt;p&gt;Attached cassandra-attack-src.zip - eclipse project for making high load test on cassandra.&lt;br/&gt;
This attack uses 100 threads to make writes, reads and deletes.&lt;/p&gt;</comment>
                            <comment id="13925605" author="benedict" created="Mon, 10 Mar 2014 09:51:27 +0000"  >&lt;p&gt;Hmm. Just taking a look at Viktor&apos;s patch, I realised that my initial conclusions were actually quite plausible and probably (one of) the causes of the problem. When I dismissed them, I didn&apos;t realise we were using a custom TBinaryProtocol implementation. In particular (1) is definitely possible, and probably the cause of the issue, although the attack jar source would be helpful to figure out of there are any other potential causes. We should be able to force the problem to occur by artificially delaying the commit log write to prove this.&lt;/p&gt;

&lt;p&gt;Either way, I don&apos;t think Viktor&apos;s patch is the best way to deal with this problem, as it leaves cleaning up the direct buffers to GC. Since we could be creating a lot of these, we could create an awful lot of artificial memory pressure. Honestly, I think the best solution is to simply avoid using direct buffers with thrift, at least until 2.1, which should fix this problem by ensuring the CL &lt;em&gt;write&lt;/em&gt; (if not commit) has happened before performing the memtable insertion.&lt;/p&gt;</comment>
                            <comment id="13925609" author="kvaster" created="Mon, 10 Mar 2014 09:55:32 +0000"  >&lt;p&gt;My patch is defenetly NOT GOOD. Also for me that patch means anothe thing: it seems that we have &apos;success&apos; answer  before data is passed to commitlog... I don&apos;t think that this is good.&lt;/p&gt;</comment>
                            <comment id="13925614" author="benedict" created="Mon, 10 Mar 2014 09:59:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;we have &apos;success&apos; answer before data is passed to commitlog&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, see my comment from a few days ago:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;1) CL appends can be lagged behind the memtable update and, as a result, the acknowledgment to the client of success writing. If the CL record contains the ByteBuffer when it is freed, and that address is then reused in another allocation, it will write incorrect data to the commit log.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is an absolutely plausible scenario since we do actually slice directly from the DirectByteBuffer, which I previously thought we did not.&lt;/p&gt;</comment>
                            <comment id="13925617" author="benedict" created="Mon, 10 Mar 2014 10:05:04 +0000"  >&lt;p&gt;Has anybody tested this problem against 2.1? As if this is the only issue, it should be fixed there.&lt;/p&gt;</comment>
                            <comment id="13926067" author="enigmacurry" created="Mon, 10 Mar 2014 19:17:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=benedict&quot; class=&quot;user-hover&quot; rel=&quot;benedict&quot;&gt;benedict&lt;/a&gt; I haven&apos;t yet been able to reproduce this with anything other than Viktor&apos;s attack jar. I&apos;m thinking Java&apos;s threading is beating Python&apos;s threading here, so I &lt;a href=&quot;https://github.com/riptano/cassandra-dtest/blob/master/thrift_hsha_test.py&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;created a dtest&lt;/a&gt; that just run&apos;s his jar directly. This test is currently passing on cassandra-2.0 and cassandra-2.1 HEAD. &lt;/p&gt;</comment>
                            <comment id="13926077" author="jbellis" created="Mon, 10 Mar 2014 19:23:06 +0000"  >&lt;p&gt;You&apos;d want to revert Pavel&apos;s patch from 2.1 to test Benedict&apos;s theory.&lt;/p&gt;</comment>
                            <comment id="13926233" author="kvaster" created="Mon, 10 Mar 2014 21:17:45 +0000"  >&lt;p&gt;You may set threads count to only one in cass-atack jar and you will be still able to reproduce error.&lt;/p&gt;</comment>
                            <comment id="13929762" author="benedict" created="Tue, 11 Mar 2014 00:05:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;You&apos;d want to revert Pavel&apos;s patch from 2.1&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;beta1 should be fine to test against for this&lt;/p&gt;</comment>
                            <comment id="13931627" author="kvaster" created="Wed, 12 Mar 2014 10:46:37 +0000"  >&lt;p&gt;I&apos;ve tried my test with beta1 and I can confirm that I was not able to reproduce bug.&lt;br/&gt;
I think that it will be better to not use disruptor on 2.0.x even with on-heap allocation (we can still reuse buffer in case message will be of equal size when previous).&lt;br/&gt;
And it should be safe to use disruptor on 2.1 branch.&lt;/p&gt;

&lt;p&gt;We&apos;ll be waiting for 2.1 release, cause it really impressed me over 2.0&lt;/p&gt;</comment>
                            <comment id="13934952" author="kvaster" created="Fri, 14 Mar 2014 12:45:53 +0000"  >&lt;p&gt;Telling the truth, I don&apos;t think that this is really fixed in 2.0.6.&lt;br/&gt;
It&apos;s not easy to reproduce bug right now, but I think it can be. thrift-disruptor server does not allocate new Buffer for new message in case new message is of equal size with previous. In that case bug can be reproduced even with on-heap allocation.&lt;/p&gt;</comment>
                            <comment id="13934956" author="benedict" created="Fri, 14 Mar 2014 12:54:58 +0000"  >&lt;p&gt;+1. That needs to be fixed as well.&lt;/p&gt;</comment>
                            <comment id="13939363" author="rcoli" created="Tue, 18 Mar 2014 15:17:51 +0000"  >&lt;blockquote&gt;
&lt;p&gt;... Telling the truth, I don&apos;t think that this is really fixed in 2.0.6.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If hsha is irrevocably broken with data corruption risk in 2.0 line, could we either get it wired off in the next point release, or some messaging in NEWS.txt that instructs people not to use it? My preference is the former to cover upgraders who are foolish enough to not read NEWS.txt; I am unable to see the benefit of leaving it usable if it is known broken.&lt;/p&gt;</comment>
                            <comment id="13942195" author="appodictic" created="Thu, 20 Mar 2014 19:49:29 +0000"  >&lt;p&gt;I read thought this. Does it make sense to call this HSHA2 and restore the old code and call it HSHA? I&lt;/p&gt;</comment>
                            <comment id="13993761" author="rbranson" created="Fri, 9 May 2014 18:01:16 +0000"  >&lt;p&gt;This is not fixed. Still seeing the same exception running 2.0.6.&lt;/p&gt;

&lt;p&gt;ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:7&amp;#93;&lt;/span&gt; 2014-05-09 17:59:58,640 CassandraDaemon.java (line 196) Exception in thread Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:7,1,main&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.RuntimeException: Last written key DecoratedKey(132126721345628486111245439753727165857, 0f3b67f2) &amp;gt;= current key DecoratedKey(37424530135488872684523334498941679307, 196b70ab) writing into /data/cassandra/data/redacted/Redacted/redacted-Redacted-tmp-jb-156533-Data.db&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)&lt;br/&gt;
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)&lt;br/&gt;
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)&lt;br/&gt;
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:744)&lt;/p&gt;</comment>
                            <comment id="13993857" author="brandon.williams" created="Fri, 9 May 2014 19:31:17 +0000"  >&lt;p&gt;The line in question: &lt;a href=&quot;https://github.com/xedin/disruptor_thrift_server/commit/77d6715af0eeba4c52f42fa6ba6549c8ae52ffa7#diff-18c889f19dc9fbeb73af99dcff152b6eR421&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/xedin/disruptor_thrift_server/commit/77d6715af0eeba4c52f42fa6ba6549c8ae52ffa7#diff-18c889f19dc9fbeb73af99dcff152b6eR421&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13993868" author="brandon.williams" created="Fri, 9 May 2014 19:50:36 +0000"  >&lt;p&gt;Patch to enable buffer reallocation.&lt;/p&gt;</comment>
                            <comment id="13994028" author="jbellis" created="Fri, 9 May 2014 23:05:09 +0000"  >&lt;p&gt;I thought we were reallocating by default but I must have gotten that confused with on-heap buffers above.  If Viktor is right, reusing buffers is always potentially dangerous and should just be removed.  Can you comment, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="13994068" author="xedin" created="Fri, 9 May 2014 23:45:05 +0000"  >&lt;p&gt;No, by default it&apos;s turned off, because Thrift side expectation is that once the invocation is complete nobody else holds the buffers, but it seems like the problem is that on Cassandra side we actually never copy the buffer for the commit log (or was it something else?). So we need to set thrift server to alwayReallocate explicitly.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rbranson&quot; class=&quot;user-hover&quot; rel=&quot;rbranson&quot;&gt;rbranson&lt;/a&gt; I can give you updated jar so you don&apos;t have to wait for the release of Cassandra which would have alwaysReallocate set to true by default.&lt;/p&gt;</comment>
                            <comment id="13994075" author="xedin" created="Fri, 9 May 2014 23:55:20 +0000"  >&lt;p&gt;So I can do two things, a). set alwaysReuse to true by default and release 0.3.5 today b). you can just switch to alwaysReallocate(true) in the configuration for 2.0.8, either works for me.&lt;/p&gt;</comment>
                            <comment id="13994137" author="jbellis" created="Sat, 10 May 2014 04:23:19 +0000"  >&lt;p&gt;Yeah, we do treat BB as immutable so CL would understandably not expect Thrift to pull the rug out from under it.&lt;/p&gt;

&lt;p&gt;I&apos;m fine with calling alwaysReallocate on the Cassandra side in the interest of not changing things out from under any other users.&lt;/p&gt;</comment>
                            <comment id="13994249" author="brandon.williams" created="Sat, 10 May 2014 16:10:41 +0000"  >&lt;p&gt;I have no issue with doing a) &lt;em&gt;AND&lt;/em&gt; b), just to be extra safe, if we know this puts the nail in this ticket&apos;s coffin.&lt;/p&gt;</comment>
                            <comment id="13994597" author="appodictic" created="Sun, 11 May 2014 16:12:31 +0000"  >&lt;p&gt;I was poking around the dependency a bit&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Running com.thinkaurelius.thrift.OffHeapMultiRequestTest&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.515 sec&lt;br/&gt;
Running com.thinkaurelius.thrift.OnHeapMultiConnectionWithReallocateTest&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.481 sec&lt;br/&gt;
Running com.thinkaurelius.thrift.OffheapMultiConnectionWithRellocateTest&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.478 sec&lt;br/&gt;
Running com.thinkaurelius.thrift.OnHeapMultiConnectionTest&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.417 sec&lt;br/&gt;
Running com.thinkaurelius.thrift.OnHeapMultiRequestWithReallocateTest&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.735 sec&lt;br/&gt;
Running com.thinkaurelius.thrift.OffHeapMultiRequestWithReallocateTest&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.437 sec&lt;br/&gt;
Running com.thinkaurelius.thrift.OffHeapMultiConnectionTest&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.282 sec&lt;br/&gt;
Running com.thinkaurelius.thrift.OnHeapMultiRequestTest&lt;br/&gt;
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.491 sec&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Q. Are a few tests that run in roughly 20 seconds enough to prove that this component fit for production? These highly concurrent off heap systems can have very subtle bugs as this ticket shows.&lt;/p&gt;

&lt;p&gt;If I understand correct HSHA is not the default, the artifact has good test coverage, and only a handful of findbugs issues. Is there any piece that is going to run end-to-end or attempt to load/concurrently test these classes and be more rigorous then the previous system? Can that be made?&lt;/p&gt;</comment>
                            <comment id="13994677" author="xedin" created="Sun, 11 May 2014 20:57:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brandon.williams&quot; class=&quot;user-hover&quot; rel=&quot;brandon.williams&quot;&gt;brandon.williams&lt;/a&gt; I have released 0.3.5 just now, with reallocation and on-heap buffers turned on by default. +1 on the change so it&apos;s either we commit 0.3.5 or your patch.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=appodictic&quot; class=&quot;user-hover&quot; rel=&quot;appodictic&quot;&gt;appodictic&lt;/a&gt; Here is a definition of &lt;a href=&quot;http://en.wikipedia.org/wiki/Unit_testing&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;unit testing&lt;/a&gt;, in my tests I cover single/multi connection on-heap/off-heap +/- reallocation scenarios, basically everything to prove that server functions properly in all of the modes and returns correct results based on the operation being used. end-to-end tests are what systems which integrate project are supposed to do and that is done by stress and bpdlab testing. If you have been following discussion in this ticket you must have already realized that the problem is not caused by HsHa server directly but rather by the fact that Cassandra holds Thrift buffers even after blocking evaluation is finished.&lt;/p&gt;</comment>
                            <comment id="13994796" author="xedin" created="Mon, 12 May 2014 04:36:24 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brandon.williams&quot; class=&quot;user-hover&quot; rel=&quot;brandon.williams&quot;&gt;brandon.williams&lt;/a&gt; 0.3.5 is already available on &lt;a href=&quot;http://search.maven.org/remotecontent?filepath=com/thinkaurelius/thrift/thrift-server/0.3.5/thrift-server-0.3.5.jar&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;maven central&lt;/a&gt; so act as you think appropriate.&lt;/p&gt;</comment>
                            <comment id="13995325" author="appodictic" created="Mon, 12 May 2014 17:52:50 +0000"  >&lt;p&gt;@Pavel I understand what you are saying. I understand what unit test is. Whenever I submit patches to the ASF they come with tests. ::cough:: ::cough::. In any case, what I was saying is the external dependency does not do load testing. Cassandra does not default to hsha. I DO NOT see any reference in this conversation into how exactly the HSHA server is now load/correctness tested. If such a test exists great, if not potentially should be added.&lt;/p&gt;</comment>
                            <comment id="13995380" author="appodictic" created="Mon, 12 May 2014 18:17:57 +0000"  >&lt;p&gt;Also while we are on the topic.&lt;/p&gt;

&lt;blockquote&gt;
&lt;ol&gt;
	&lt;li&gt;The default is sync because on Windows hsha is about 30% slower.  On Linux,&lt;/li&gt;
	&lt;li&gt;sync/hsha performance is about the same, with hsha of course using less memory.&lt;br/&gt;
#&lt;/li&gt;
	&lt;li&gt;Alternatively,  can provide your own RPC server by providing the fully-qualified class name&lt;/li&gt;
	&lt;li&gt;of an o.a.c.t.TServerFactory that can create an instance of it.&lt;br/&gt;
rpc_server_type: sync&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;The logic behind this default confuses me. The the vast majority of the cassandra user base is linux. We chose &apos;sync&apos; so the uncommon case is not slowed down. Clearly anyone using linux should switch to hsha because it uses less memory and is wiched fast according to github tests. But not being the default does it really get performance/correctness evaluated in any meaningful way?&lt;/p&gt;
</comment>
                            <comment id="13995574" author="benedict" created="Mon, 12 May 2014 20:07:12 +0000"  >&lt;p&gt;For many workloads sync is faster than async on linux also (by a significant margin), so perhaps the docs should be updated.&lt;/p&gt;</comment>
                            <comment id="13996712" author="rbranson" created="Tue, 13 May 2014 18:06:02 +0000"  >&lt;p&gt;I think what might help this specific quality issue out is just moving to the new HSHA implementation entirely in a later version and removing the choice. The new HSHA supposedly eliminates the performance issues that made it not a good default choice, so it appears as if there&apos;s no advantage to having the other choices.&lt;/p&gt;</comment>
                            <comment id="13996760" author="brandon.williams" created="Tue, 13 May 2014 18:41:12 +0000"  >&lt;p&gt;For 2.1, I can get behind that I think, especially calling it &apos;disruptor&apos; or pretty much anything besides &apos;HSHA.&apos;  For 2.0 though it&apos;s hard to swallow in a minor.&lt;/p&gt;</comment>
                            <comment id="13996899" author="rbranson" created="Tue, 13 May 2014 21:11:03 +0000"  >&lt;p&gt;I did some more digging around on our cluster that was running 2.0.6 when it saw the corruption: it took anywhere from a few hours to 48 hours for the first compaction with the out of order key exception to throw. These nodes are receiving thousands of writes per second, so it&apos;s not going to be trivially reproducible. We&apos;ve been running one of the nodes with 2.0.8-tenative + enable_reallocate_buffers.txt and will report back once we&apos;ve reached 72 hours and are comfortable rolling this out wide to our own clusters.&lt;/p&gt;</comment>
                            <comment id="14002142" author="rbranson" created="Mon, 19 May 2014 18:36:48 +0000"  >&lt;p&gt;Haven&apos;t been able to repro in over 5 days. We&apos;re considering the enable_reallocate_buffers.txt patch fixed and production-ready.&lt;/p&gt;</comment>
                            <comment id="14002181" author="brandon.williams" created="Mon, 19 May 2014 19:04:11 +0000"  >&lt;p&gt;I committed this patch to 2.0, but did not update the disruptor jar for fear of any further regressions, so the patch Rick tested is in there.  For 2.1, I committed both this patch and disruptor 0.3.5.&lt;/p&gt;</comment>
                            <comment id="14002197" author="brandon.williams" created="Mon, 19 May 2014 19:13:32 +0000"  >&lt;p&gt;Oops, wait, I only changed the maven dependency.  &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mishail&quot; class=&quot;user-hover&quot; rel=&quot;mishail&quot;&gt;mishail&lt;/a&gt; could you clean up the 2.1+ side of things?&lt;/p&gt;</comment>
                            <comment id="14002440" author="mishail" created="Mon, 19 May 2014 21:35:51 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brandon.williams&quot; class=&quot;user-hover&quot; rel=&quot;brandon.williams&quot;&gt;brandon.williams&lt;/a&gt; done.&lt;/p&gt;</comment>
                            <comment id="14002448" author="brandon.williams" created="Mon, 19 May 2014 21:39:04 +0000"  >&lt;p&gt;Thanks.&lt;/p&gt;</comment>
                            <comment id="14178814" author="sterligovak" created="Tue, 21 Oct 2014 18:41:56 +0000"  >&lt;p&gt;It looks like this is not fixed in 2.1.0. We have cassandra under heavy load through binary interface and only OpsCenter by thrift. OpsCenter rollups are corrupted in about an hour after scrub.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:71&amp;#93;&lt;/span&gt; 2014-10-21 22:16:39,950 CassandraDaemon.java:166 - Exception in thread Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:71,1,main&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.RuntimeException: Last written key DecoratedKey(-7581200918995348250, 39352e3130382e3234322e32302d6973732d73686172645f696e666f2d676574426c6f6f6d46696c74657246616c7365506f73697469766573) &amp;gt;= current key DecoratedKey(-8301289422298317140, 800100010000000c62617463685f6d75746174650006d04a0d00010b0d0000000100000025) writing into /ssd/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-ka-9128-Data.db&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:177) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:235) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:744) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We&apos;ll try to switch to sync and see what will happen.&lt;/p&gt;

&lt;p&gt;Is it possible that streaming hangs because of that exception? Is it possible that this exception affect minor compactions of other keyspaces?&lt;/p&gt;</comment>
                            <comment id="14178828" author="ngrigoriev@gmail.com" created="Tue, 21 Oct 2014 18:51:02 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sterligovak&quot; class=&quot;user-hover&quot; rel=&quot;sterligovak&quot;&gt;sterligovak&lt;/a&gt; I was always wondering why did I always see these problems appearing for OpsCenter keyspace. My keyspace had much more traffic but when I had this problem - it always manifested itself with OpsCenter keyspace. Even when I was also using Thrift (we use native protocol now).&lt;/p&gt;

&lt;p&gt;I even remember disabling OpsCenter to prove the point &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; &lt;/p&gt;
</comment>
                            <comment id="14179321" author="sterligovak" created="Tue, 21 Oct 2014 23:37:54 +0000"  >&lt;p&gt;Have you proven that it&apos;s really related to OpsCenter?&lt;/p&gt;

&lt;p&gt;We&apos;ve switched to &quot;sync&quot;, but still get corrupted sstables. Now we get exception not during compaction, but at start:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;SSTableBatchOpen:10&amp;#93;&lt;/span&gt; 2014-10-22 02:47:48,762 CassandraDaemon.java:166 - Exception in thread Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;SSTableBatchOpen:10,5,main&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.IllegalStateException: SSTable first key DecoratedKey(4206305143314087741, 800100010000000c62617463685f6d7574617465000010250d00010b0d000000010000004e33372e3134302e3134312e3231322d6973732d736c6f745f636f6e66696775726174696f6e5f746172) &amp;gt; last key DecoratedKey(-4632241097675266745, 800100010000000c62617463685f6d7574617465000010260d00010b0d000000010000005133372e3134302e3134312e3231322d6973732d736c6f745f636f6e66696775726174696f6e5f746172676574)&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableReader.validate(SSTableReader.java:1083) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:398) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:294) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableReader$4.run(SSTableReader.java:430) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:744) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;And nodetools scrub doesn&apos;t help. It finds no errors and after restart we get same exceptions.&lt;/p&gt;</comment>
                            <comment id="14179431" author="ngrigoriev@gmail.com" created="Wed, 22 Oct 2014 00:56:51 +0000"  >&lt;p&gt;I think this is the error that you cannot fix by scrubbing. Corrupted sstable. I was fixing those by deleting the sstables and doing repairs. Unfortunately, if that happens on many nodes there is a risk of data loss.&lt;/p&gt;

&lt;p&gt;As for the OpsCenter - do not get me wrong &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; I did not want to say that OpsCenter was directly responsible for these troubles. But I do believe that OpsCenter does something particular that reveals the bug in hsha server. At least this was my impression. After disabling OpsCenter and fixing the outstanding problems I do not recall seeing those errors anymore. And I was also using Thrift and I was writing and reading 100x more data than OpsCenter.&lt;/p&gt;
</comment>
                            <comment id="14179658" author="xedin" created="Wed, 22 Oct 2014 06:57:25 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sterligovak&quot; class=&quot;user-hover&quot; rel=&quot;sterligovak&quot;&gt;sterligovak&lt;/a&gt; Did you get any WARN messages like this &quot;N out of order rows found while scrubbing &amp;lt;file&amp;gt;; Those have been written (in order) to a new sstable &amp;lt;new-file&amp;gt;&quot; while running scrub? Anyhow, you will have to delete affected files and repair from the neighbors, I&apos;m also not sure how much of an involvement Thrift has in this because the only thing that could go wrong (shared buffers) was already fixed to be copied for every request and everything is allocated on-heap....&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rbranson&quot; class=&quot;user-hover&quot; rel=&quot;rbranson&quot;&gt;rbranson&lt;/a&gt; Are you running HsHa with 2.1 or still on 2.0 ? &lt;/p&gt;</comment>
                            <comment id="14179687" author="sterligovak" created="Wed, 22 Oct 2014 07:37:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt; No, I&apos;ve not seen such messages. sstablescrub failed with NPE. sstables were corrupted on all 17 nodes. I removed them manually and there was no errors overnight. It seems sync really impacted the problem. Maybe there are some another problem which hides with sync server. I still have problems - validation hangs on one table on all nodes &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                            <comment id="14181742" author="ngrigoriev@gmail.com" created="Thu, 23 Oct 2014 18:38:22 +0000"  >&lt;p&gt;By the way, I am getting &lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;ERROR [CompactionExecutor:2333] 2014-10-23 18:29:53,590 CassandraDaemon.java (line 199) Exception in thread &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[Compactio
nExecutor:2333,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(1156541975678546868, 001000000000111100000000000003bc510f000010000
0000003bc510f00000000111100000000100000000000004000000000000000000100) &amp;gt;= current key DecoratedKey(36735936098318717, 001000
0000001111000000000000015feb8a00001000000000015feb8a00000000111100000000100000000000004000000000000000000100) writing into /
cassandra-data/disk2/myks/mytable/myks-mytable-tmp-jb-94445-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:745)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;with 2.0.10 release. I am using native protocol. I believe native protocol handler is based on HSHA, am I right? Anyway, I am getting those too.&lt;/p&gt;</comment>
                            <comment id="14181789" author="xedin" created="Thu, 23 Oct 2014 19:12:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ngrigoriev&quot; class=&quot;user-hover&quot; rel=&quot;ngrigoriev&quot;&gt;ngrigoriev&lt;/a&gt; No, native protocol is not using Thrift, which further confirms that it&apos;s cross transport problem, I think we should create a separate ticket to handle it.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sterligovak&quot; class=&quot;user-hover&quot; rel=&quot;sterligovak&quot;&gt;sterligovak&lt;/a&gt; Do you still have stacktrace for the NPE you&apos;ve got while scrubbing?&lt;/p&gt;</comment>
                            <comment id="14182626" author="sterligovak" created="Fri, 24 Oct 2014 09:25:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt; That NPE happend once and unfortunatelly I have not saved it. If I&apos;ll get it once more I&apos;ll save this sstable.&lt;br/&gt;
I totally removed OpsCenter keyspace (with sstables) and recreated them. I don&apos;t get &quot;Last written key DecoratedKey&quot; any more. By the way, this error definetely causees streams to hang on 100%.&lt;/p&gt;

&lt;p&gt;I have several strange things happening now:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;I&apos;ve noticed that it takes about 30 minutes between &quot;nodetool repair&quot; and first pending AntiEntropySession. Is that ok?&lt;/li&gt;
	&lt;li&gt;Repair is already running for 24 hours (~13GB per node, 17 nodes). What&apos;s the number of AntiEntropySessions to finish single repair? Number of key ranges?
&lt;blockquote&gt;
&lt;p&gt;Pool Name                    Active   Pending      Completed   Blocked  All time blocked&lt;br/&gt;
CounterMutationStage              0         0              0         0                 0&lt;br/&gt;
ReadStage                         0         0         392196         0                 0&lt;br/&gt;
RequestResponseStage              0         0        5271906         0                 0&lt;br/&gt;
MutationStage                     0         0       19832506         0                 0&lt;br/&gt;
ReadRepairStage                   0         0           2280         0                 0&lt;br/&gt;
GossipStage                       0         0         453830         0                 0&lt;br/&gt;
CacheCleanupExecutor              0         0              0         0                 0&lt;br/&gt;
MigrationStage                    0         0              0         0                 0&lt;br/&gt;
ValidationExecutor                0         0          39446         0                 0&lt;br/&gt;
MemtableReclaimMemory             0         0          29927         0                 0&lt;br/&gt;
InternalResponseStage             0         0         588279         0                 0&lt;br/&gt;
AntiEntropyStage                  0         0        5325285         0                 0&lt;br/&gt;
MiscStage                         0         0              0         0                 0&lt;br/&gt;
CommitLogArchiver                 0         0              0         0                 0&lt;br/&gt;
MemtableFlushWriter               0         0          29927         0                 0&lt;br/&gt;
PendingRangeCalculator            0         0             30         0                 0&lt;br/&gt;
MemtablePostFlush                 0         0         135734         0                 0&lt;br/&gt;
CompactionExecutor               31        31         502175         0                 0&lt;br/&gt;
AntiEntropySessions               3         3           3446         0                 0&lt;br/&gt;
HintedHandoff                     0         0             44         0                 0&lt;/p&gt;

&lt;p&gt;Message type           Dropped&lt;br/&gt;
RANGE_SLICE                  0&lt;br/&gt;
READ_REPAIR                  0&lt;br/&gt;
PAGED_RANGE                  0&lt;br/&gt;
BINARY                       0&lt;br/&gt;
READ                         0&lt;br/&gt;
MUTATION                     2&lt;br/&gt;
_TRACE                       0&lt;br/&gt;
REQUEST_RESPONSE             0&lt;br/&gt;
COUNTER_MUTATION             0&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;
	&lt;li&gt;Some validation compactions run for more than 100% (1923%). I thinks that it&apos;s &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-7239&quot; title=&quot;Nodetool Status Reports Negative Load With VNodes Disabled&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-7239&quot;&gt;&lt;del&gt;CASSANDRA-7239&lt;/del&gt;&lt;/a&gt;, right?&lt;/li&gt;
	&lt;li&gt;the amount of sstables for some CFs is about 15 000 and continues to grow during repair.&lt;/li&gt;
	&lt;li&gt;There are several following exceptions during repair
&lt;blockquote&gt;
&lt;p&gt;ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;RepairJobTask:80&amp;#93;&lt;/span&gt; 2014-10-24 13:27:31,717 RepairJob.java:127 - Error occurred during snapshot phase&lt;br/&gt;
java.lang.RuntimeException: Could not create snapshot at /37.140.189.163&lt;br/&gt;
        at org.apache.cassandra.repair.SnapshotTask$SnapshotCallback.onFailure(SnapshotTask.java:77) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.net.MessagingService$5$1.run(MessagingService.java:347) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:744) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;AntiEntropySessions:141&amp;#93;&lt;/span&gt; 2014-10-24 13:27:31,724 RepairSession.java:303 - &lt;a href=&quot;#da2cb020-5b5f-11e4-a45e-d9cec1206f33&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;repair #da2cb020-5b5f-11e4-a45e-d9cec1206f33&lt;/a&gt; session completed with the following error&lt;br/&gt;
java.io.IOException: Failed during snapshot creation.&lt;br/&gt;
        at org.apache.cassandra.repair.RepairSession.failedSnapshot(RepairSession.java:344) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.repair.RepairJob$2.onFailure(RepairJob.java:128) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1172) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;guava-16.0.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:744) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;AntiEntropySessions:141&amp;#93;&lt;/span&gt; 2014-10-24 13:27:31,724 CassandraDaemon.java:166 - Exception in thread Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;AntiEntropySessions:141,5,RMI Runtime&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.RuntimeException: java.io.IOException: Failed during snapshot creation.&lt;br/&gt;
        at com.google.common.base.Throwables.propagate(Throwables.java:160) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;guava-16.0.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:744) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_51&amp;#93;&lt;/span&gt;&lt;br/&gt;
Caused by: java.io.IOException: Failed during snapshot creation.&lt;br/&gt;
        at org.apache.cassandra.repair.RepairSession.failedSnapshot(RepairSession.java:344) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.repair.RepairJob$2.onFailure(RepairJob.java:128) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1172) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;guava-16.0.jar:na&amp;#93;&lt;/span&gt;&lt;br/&gt;
        ... 3 common frames omitted&lt;/p&gt;&lt;/blockquote&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14188471" author="krummas" created="Wed, 29 Oct 2014 15:43:30 +0000"  >&lt;p&gt;I think the cause of the latest exceptions in this ticket is &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8211&quot; title=&quot;Overlapping sstables in L1+&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8211&quot;&gt;&lt;del&gt;CASSANDRA-8211&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14292451" author="rbfblk" created="Mon, 26 Jan 2015 21:39:11 +0000"  >&lt;p&gt;I am getting this exception using Thrift HSHA in 2.1.0:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:8&amp;#93;&lt;/span&gt; 2015-01-26 13:32:51,818 CompactionTask.java (line 138) Compacting &lt;span class=&quot;error&quot;&gt;&amp;#91;SSTableReader(path=&amp;#39;/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-2-Data.db&amp;#39;), SSTableReader(path=&amp;#39;/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-1-Data.db&amp;#39;)&amp;#93;&lt;/span&gt;&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:8&amp;#93;&lt;/span&gt; 2015-01-26 13:32:51,890 ColumnFamilyStore.java (line 856) Enqueuing flush of compactions_in_progress: 212 (0%) on-heap, 20 (0%) off-heap&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;MemtableFlushWriter:8&amp;#93;&lt;/span&gt; 2015-01-26 13:32:51,892 Memtable.java (line 326) Writing Memtable-compactions_in_progress@1155018639(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;MemtableFlushWriter:8&amp;#93;&lt;/span&gt; 2015-01-26 13:32:51,896 Memtable.java (line 360) Completed flushing /tmp/cass_test/cassandra/TestCassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-2-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1422296630707, position=430226)&lt;br/&gt;
ERROR &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:8&amp;#93;&lt;/span&gt; 2015-01-26 13:32:51,906 CassandraDaemon.java (line 166) Exception in thread Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:8,1,RMI Runtime&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.RuntimeException: Last written key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) &amp;gt;= current key DecoratedKey(14775611966645399672119169777260659240, 726f776b65793030385f31343232323937313537353835) writing into /tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-tmp-ka-3-Data.db&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:177) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:235) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_40&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_40&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_40&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_40&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:724) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_40&amp;#93;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think it&apos;s caused by &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8211&quot; title=&quot;Overlapping sstables in L1+&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8211&quot;&gt;&lt;del&gt;CASSANDRA-8211&lt;/del&gt;&lt;/a&gt;, because it happens during the first compaction that takes place between the first 2 SSTables to get flushed from an initially empty column family.&lt;/p&gt;

&lt;p&gt;Also, I&apos;ve only been able to reproduce it when using both &lt;b&gt;hsha&lt;/b&gt; for the rpc server and &lt;b&gt;offheap_objects&lt;/b&gt; for memtable allocation. If I switch either to sync or to offheap_buffers or heap_buffers then I cannot reproduce the problem. Also under the same circumstances I&apos;m pretty sure I&apos;ve seen incorrect data being returned to a client multiget_slice request before any SSTables had been flushed yet, so I presume this is corruption that happens before any flush/compaction takes place.&lt;/p&gt;

&lt;p&gt;nodetool scrub yielded these errors:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt; INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:9&amp;#93;&lt;/span&gt; 2015-01-26 13:48:01,512 OutputHandler.java (line 42) Scrubbing SSTableReader(path=&apos;/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-2-Data.db&apos;) (168780 bytes)&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:10&amp;#93;&lt;/span&gt; 2015-01-26 13:48:01,512 OutputHandler.java (line 42) Scrubbing SSTableReader(path=&apos;/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-1-Data.db&apos;) (135024 bytes)&lt;br/&gt;
 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:9&amp;#93;&lt;/span&gt; 2015-01-26 13:48:01,531 OutputHandler.java (line 52) Out of order row detected (DecoratedKey(14775611966645399672119169777260659240, 726f776b65793030385f31343232323937313537353835) found after DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000))&lt;br/&gt;
 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:9&amp;#93;&lt;/span&gt; 2015-01-26 13:48:01,534 OutputHandler.java (line 57) Error reading row (stacktrace follows):&lt;br/&gt;
java.lang.RuntimeException: Last written key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) &amp;gt;= current key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) writing into /tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-tmp-ka-4-Data.db&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.io.sstable.SSTableRewriter.tryAppend(SSTableRewriter.java:141) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:186) ~&lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:592) &lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:100) &lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionManager$3.execute(CompactionManager.java:315) &lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:270) &lt;span class=&quot;error&quot;&gt;&amp;#91;apache-cassandra-2.1.0.jar:2.1.0&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_40&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_40&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_40&amp;#93;&lt;/span&gt;&lt;br/&gt;
        at java.lang.Thread.run(Thread.java:724) &lt;span class=&quot;error&quot;&gt;&amp;#91;na:1.7.0_40&amp;#93;&lt;/span&gt;&lt;br/&gt;
 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:9&amp;#93;&lt;/span&gt; 2015-01-26 13:48:01,534 OutputHandler.java (line 52) Row starting at position 25342 is unreadable; skipping to next&lt;br/&gt;
 WARN &lt;span class=&quot;error&quot;&gt;&amp;#91;CompactionExecutor:10&amp;#93;&lt;/span&gt; 2015-01-26 13:48:01,534 OutputHandler.java (line 52) Out of order row detected (DecoratedKey(29459452031265566667651334397450214244, 726f776b65793030355f31343232323936393033323837) found after DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000))&lt;/p&gt;

&lt;p&gt;etc...&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;EDIT: I copied my comment to a new issue (&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8719&quot; title=&quot;Using thrift HSHA with offheap_objects appears to corrupt data&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8719&quot;&gt;&lt;del&gt;CASSANDRA-8719&lt;/del&gt;&lt;/a&gt;) since this issue one is long closed&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12633219" name="6285_testnotes1.txt" size="117650" author="mshuler" created="Thu, 6 Mar 2014 20:29:38 +0000"/>
                            <attachment id="12632583" name="CASSANDRA-6285-disruptor-heap.patch" size="1341" author="xedin" created="Tue, 4 Mar 2014 19:40:46 +0000"/>
                            <attachment id="12633659" name="cassandra-attack-src.zip" size="8972958" author="kvaster" created="Mon, 10 Mar 2014 09:42:10 +0000"/>
                            <attachment id="12625617" name="compaction_test.py" size="3000" author="rhatch" created="Tue, 28 Jan 2014 18:24:14 +0000"/>
                            <attachment id="12633656" name="disruptor-high-cpu.patch" size="2509" author="kvaster" created="Mon, 10 Mar 2014 09:33:05 +0000"/>
                            <attachment id="12633657" name="disruptor-memory-corruption.patch" size="617" author="kvaster" created="Mon, 10 Mar 2014 09:33:05 +0000"/>
                            <attachment id="12644174" name="enable_reallocate_buffers.txt" size="1280" author="brandon.williams" created="Fri, 9 May 2014 19:50:36 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>7.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[xedin]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>356503</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 43 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1pgiv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>356791</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311421" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Reproduced In</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12326170">2.0.6</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>brandon.williams</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[brandon.williams]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12963"><![CDATA[Critical]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311420" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12324629">2.0.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12311124" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Tester</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>rbranson</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>