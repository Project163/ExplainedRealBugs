<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:15:15 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-15367] Memtable memory allocations may deadlock</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-15367</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;ul&gt;
	&lt;li&gt;Under heavy contention, we guard modifications to a partition with a mutex, for the lifetime of the memtable.&lt;/li&gt;
	&lt;li&gt;Memtables block for the completion of all &lt;tt&gt;OpOrder.Group&lt;/tt&gt; started before their flush began&lt;/li&gt;
	&lt;li&gt;Memtables permit operations from this cohort to fall-through to the following Memtable, in order to guarantee a precise commitLogUpperBound&lt;/li&gt;
	&lt;li&gt;Memtable memory limits may be lifted for operations in the first cohort, since they block flush (and hence block future memory allocation)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;With very unfortunate scheduling&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A contended partition may rapidly escalate to a mutex&lt;/li&gt;
	&lt;li&gt;The system may reach memory limits that prevent allocations for the new Memtable&#8217;s cohort (C2)&lt;/li&gt;
	&lt;li&gt;An operation from C2 may hold the mutex when this occurs&lt;/li&gt;
	&lt;li&gt;Operations from a prior Memtable&#8217;s cohort (C1), for a contended partition, may fall-through to the next Memtable&lt;/li&gt;
	&lt;li&gt;The operations from C1 may execute after the above is encountered by those from C2&lt;/li&gt;
&lt;/ul&gt;
</description>
                <environment></environment>
        <key id="13263782">CASSANDRA-15367</key>
            <summary>Memtable memory allocations may deadlock</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="benedict">Benedict Elliott Smith</assignee>
                                    <reporter username="benedict">Benedict Elliott Smith</reporter>
                        <labels>
                    </labels>
                <created>Tue, 22 Oct 2019 11:32:07 +0000</created>
                <updated>Fri, 15 May 2020 08:41:37 +0000</updated>
                            <resolved>Wed, 8 Apr 2020 00:03:02 +0000</resolved>
                                        <fixVersion>3.0.21</fixVersion>
                    <fixVersion>3.11.7</fixVersion>
                    <fixVersion>4.0-alpha4</fixVersion>
                    <fixVersion>4.0</fixVersion>
                                    <component>Local/Commit Log</component>
                    <component>Local/Memtable</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="16970245" author="benedict" created="Fri, 8 Nov 2019 14:48:09 +0000"  >&lt;p&gt;patches available:&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Branch&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Tests&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/15367-3.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.0&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://circleci.com/workflow-run/7bab1194-44d7-49e4-bb04-6fe809657065&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;circleci&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/15367-3.11&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.11&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://circleci.com/workflow-run/85e153ff-fda0-4d12-929a-b56ffa67ba16&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;circleci&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/15367-4.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;4.0&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://circleci.com/workflow-run/3bda14eb-76f0-45c3-99c4-4abe2ea9c34a&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;circleci&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
</comment>
                            <comment id="17016393" author="bdeggleston" created="Wed, 15 Jan 2020 23:14:08 +0000"  >&lt;p&gt;I&apos;ve been trying to work out exactly how this deadlock can occur, based on your description. Could the deadlock be restated like this?&lt;/p&gt;

&lt;p&gt;&#160;&lt;br/&gt;
 For a given partition key:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;a write is part of an OpGroup before a barrier set on Memtable1 (M1), but with a replay position after the final replay position set on M1 before it flushes.&lt;/li&gt;
	&lt;li&gt;So it&#8217;s forwarded to M2, while still blocking flushes on M1&lt;/li&gt;
	&lt;li&gt;M2 has another in flight write for this partition, it&#8217;s contended, so it&#8217;s holding the lock
	&lt;ul&gt;
		&lt;li&gt;It can&#8217;t progress because it can&#8217;t allocate memory (in part because M1 can&#8217;t flush)&lt;/li&gt;
		&lt;li&gt;It doesn&#8217;t degrade to allocating on heap it&#8217;s oporder isn&#8217;t blocking anything.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;The write stage becomes saturated with deadlocked writes like these, no more writes&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17016398" author="benedict" created="Wed, 15 Jan 2020 23:30:58 +0000"  >&lt;p&gt;Correct, except perhaps the last part.  There&apos;s no need to collect more than one of these deadlocks to bring down the node.  If there are no memtable flushes already in progress, then no more flushes will ever occur, because they must wait for all earlier operations to complete, including the deadlock.  So from this point on no Memtable memory will ever be released.&lt;/p&gt;</comment>
                            <comment id="17017426" author="bdeggleston" created="Thu, 16 Jan 2020 19:25:58 +0000"  >&lt;p&gt;What if AtomicBTreePartition never attempted to acquire a lock if was applying an update that had fallen through from a previous memtable? That would prevent the deadlock, and wouldn&#8217;t alter memtable behavior in the common case.&#160;&lt;/p&gt;

&lt;p&gt;I suppose this could create a problem if a bunch of large contending writes overflowed. I&#8217;m not sure if this would actually be an issue in practice, but I suppose you could synchronize on something other than the partition, like the allocator or something, to prevent that.&lt;/p&gt;

&lt;p&gt;I have an rough sketch of how this might work &lt;a href=&quot;https://github.com/bdeggleston/cassandra/commit/da6e709d6c5792de71e4686c6578110f48e6dd06&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17017431" author="benedict" created="Thu, 16 Jan 2020 19:36:31 +0000"  >&lt;p&gt;You&apos;re right.  It&apos;s structurally ugly, but it would do the job.&lt;/p&gt;

&lt;p&gt;I&apos;m not particularly keen on this as a long term solution, as it&apos;s an extra layer of obfuscation around behaviour, and I&apos;m not sure how to better materialise to a reader that this property is being set, or why.  But it&apos;s a lot simpler than all of the approaches I&apos;ve taken so far, and probably has a minimal impact on the performance characteristics of the system.&lt;/p&gt;</comment>
                            <comment id="17017436" author="benedict" created="Thu, 16 Jan 2020 19:59:55 +0000"  >&lt;p&gt;For comparison, &lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/15367-a2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this patch&lt;/a&gt; addresses this ticket by ensuring allocations only happen whilst the lock is not held.  It aims to reduce the necessity of locking, not just for this use case, without removing it altogether (but laying the groundwork for removing it). &lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;So that the fast path is unaffected, we perform our first attempt to insert as normal&lt;br/&gt;
Unlike before, we disable &lt;tt&gt;abortEarly&lt;/tt&gt; for this first attempt, so that we always construct a complete new tree
	&lt;ul&gt;
		&lt;li&gt;If we fail, we walk this new tree, looking for any remnants of the insert&lt;/li&gt;
		&lt;li&gt;These remnants are collected into a new insert containing only the parts that were retained after resolving&lt;/li&gt;
		&lt;li&gt;This new insert contains only Memtable-allocated data, so we do not need to copy anything next attempt&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Future attempts to insert operate on this minimal copied version of the data, this preventing the worst case scenario the lock was introduced for, namely Memtable exhaustion&lt;/li&gt;
	&lt;li&gt;However, to minimise any performance regression, we retain the lock and continue to perform the same waste tracking as before&lt;/li&gt;
	&lt;li&gt;If locking has been enabled for the partition, step 1 is skipped, and we immediately copy the entire insert into the Memtable before obtaining the lock&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The performance impact of this patch is still being comprehensively validated, and the results will be posted in a few days. It is reasonable to expect that there will be some slight performance penalty in some cases, and some improvements in others.&lt;/p&gt;</comment>
                            <comment id="17018429" author="benedict" created="Sat, 18 Jan 2020 00:09:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=bdeggleston&quot; class=&quot;user-hover&quot; rel=&quot;bdeggleston&quot;&gt;bdeggleston&lt;/a&gt; do you want to polish your patch at least for 3.0, 3.x, and I guess 4.0 for now?  It&apos;s definitely the least invasive approach.  I&apos;ll file a separate ticket for moving towards removing the lock entirely, and we can consider if/when we want to address it.  I would prefer 4.0 had a &quot;better&quot; (conceptually) solution to the problem, but it&apos;s very much up for debate (one that can be had on another ticket)&lt;/p&gt;</comment>
                            <comment id="17022906" author="benedict" created="Fri, 24 Jan 2020 12:34:29 +0000"  >&lt;p&gt;So, I decided to start writing a version of your approach with slightly more explicit control flow.  However, I realised that this bug is not fixed by this approach, or my original approach.&lt;/p&gt;

&lt;p&gt;The issue is that we have all been assuming there is only one table on the system.  In fact, the flushing &lt;tt&gt;Memtable&lt;/tt&gt; that&apos;s waiting for the operation to complete may be in an altogether different table.  It might be that the operation holding the lock and the operation that needs to obtain the lock are both members are the same logical cohort for this &lt;tt&gt;Memtable&lt;/tt&gt;. &lt;/p&gt;

&lt;p&gt;We &lt;em&gt;could&lt;/em&gt; try to introduce a separate &lt;tt&gt;OpOrder&lt;/tt&gt; per table, but this causes its own issues, since we can have multiple tables in a single operation, each one with its own different blocking behaviour.  I don&apos;t want to think about what bugs we might introduce there.&lt;/p&gt;

&lt;p&gt;We could explicitly order operations by their &lt;tt&gt;OpOrder.Group&lt;/tt&gt; when acquiring a lock - if pessimistic locking is required, we wait for all earlier operations to complete before we acquire the lock.  I&apos;m not sure what impact this might have on the system, as this might introduce delays for these operations.&lt;/p&gt;

&lt;p&gt;Alternatively, we really do need the follow-up work I&apos;ve done recently to remove the lock entirely.  This is a significant amount of work, but has no real caveats.&lt;/p&gt;</comment>
                            <comment id="17022930" author="benedict" created="Fri, 24 Jan 2020 13:13:22 +0000"  >&lt;p&gt;I have force-pushed a version that takes this simple approach.&lt;/p&gt;

&lt;p&gt;As stated, I&apos;m nervous about the impact this might have, though in practice it should &lt;em&gt;hopefully&lt;/em&gt; be limited.  I don&apos;t see another realistic option besides eliminating the lock entirely, as I had previously hoped to.  It does increase my desire to land that change in 4.0 though.&lt;/p&gt;</comment>
                            <comment id="17023133" author="bdeggleston" created="Fri, 24 Jan 2020 17:33:12 +0000"  >&lt;p&gt;Just so I understand, the scenario you&#8217;re describing could be described like this?&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;there are 2 tables, T1 &amp;amp; T2, writing against OpGroup 1 (OP&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;a write comes in for T1 and is assigned to op group 1 (W1&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;T2 starts to flush, OpGroup is bumped to OP&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;. T2 is now waiting on W1&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;a write comes in for T1 and is assigned to op group 2 (W2&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;)&lt;/li&gt;
	&lt;li&gt;W2&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt; acquires a lock, but is unable to allocate memory until T2 flushes&lt;/li&gt;
	&lt;li&gt;W1&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; is blocking the T2 flush, and is unable to acquire the lock&lt;/li&gt;
	&lt;li&gt;deadlock&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17023136" author="bdeggleston" created="Fri, 24 Jan 2020 17:34:53 +0000"  >&lt;p&gt;also, is &lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/15367-a2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this&lt;/a&gt; the branch you pushed to?&#160;&lt;/p&gt;</comment>
                            <comment id="17023303" author="benedict" created="Fri, 24 Jan 2020 22:26:01 +0000"  >&lt;p&gt;Sorry, I meant &lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/15367&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this branch&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I think you&apos;re right, except with the following modification (presumably a typo):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;W1&lt;span class=&quot;error&quot;&gt;&amp;#91;*1*&amp;#93;&lt;/span&gt; is blocking the T2 flush, and is unable to acquire the lock&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17023358" author="bdeggleston" created="Sat, 25 Jan 2020 00:41:13 +0000"  >&lt;p&gt;Yep, I think that would fix the problem. Another approach that wouldn&#8217;t have the potential to introduce delays would be to skip locking if we have (or are about to) set the final replay position on a memtable waiting on an op group. Like setting blocking, but it won&#8217;t bypass the allocator in case the flush queue is long. That would fix the deadlock without delaying later writes, although it could increase contention.&lt;/p&gt;

&lt;p&gt;Rough example with lazy naming &lt;a href=&quot;https://github.com/bdeggleston/cassandra/tree/15367-alternative-2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;It would be nice if a write waiting for a lock could unblock itself as soon as it&apos;s op group becomes blocking&lt;/p&gt;

&lt;p&gt;Random thoughts about longer term fixes:&lt;/p&gt;

&lt;p&gt;I didn&#8217;t have a chance to get my head around how you&#8217;d intended to remove the lock completely, but I don&#8217;t understand how that could be done without reintroducing the contention gc problem.&lt;/p&gt;

&lt;p&gt;It seems to me that the root cause of all this is that we have 2 mechanisms for ordering events (OpOrder and ReplayPosition) which are mostly independent, but have to interact in non-deterministic ways during memtable flush, which creates these edge cases. I think the right fix (or one of them) is to either merge these two classes, or make one control the other.&lt;/p&gt;</comment>
                            <comment id="17023361" author="benedict" created="Sat, 25 Jan 2020 00:58:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&#8217;t understand how that could be done without reintroducing the contention gc problem.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Simply by making it fast enough.  &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-15511&quot; title=&quot;Utilising BTree Improvements&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-15511&quot;&gt;&lt;del&gt;CASSANDRA-15511&lt;/del&gt;&lt;/a&gt; manages to get the costs significantly lower than today, even with 16-threads actively spinning on a dual-socket 24-core machine.  It manages to compete fairly well even against itself with locking enabled (although the lock variant does manage lower garbage, it isn&apos;t dramatic overall).  &lt;/p&gt;

&lt;p&gt;The spreadsheet I posted compares a number of possible approaches, settings and workloads.  &lt;/p&gt;

&lt;p&gt;Of course, there would still be the potential for some wasted work that could be usefully spent elsewhere, but I think the gains are minimal.  There are also other options available to us for minimising contention besides a lock, that I&apos;ve broached before (e.g. on failure to update, tag the write onto a linked-list and merge lazily on read, potentially attempting to apply the merge to the tree each time to reduce duplicated work).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;either merge these two classes, or make one control the other&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It might well be possible to merge the management of these things, it&apos;s an interesting idea and something to consider.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Rough example with lazy naming here&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks.  I&apos;ll have to dig into that next week, failing to reckon with it right now.&lt;/p&gt;</comment>
                            <comment id="17023363" author="benedict" created="Sat, 25 Jan 2020 01:04:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;Rough example with lazy naming here&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think that patch works, actually, as &lt;tt&gt;orderOnOrBefore&lt;/tt&gt; is null until &lt;tt&gt;issue()&lt;/tt&gt; is invoked&lt;/p&gt;</comment>
                            <comment id="17023367" author="bdeggleston" created="Sat, 25 Jan 2020 01:08:44 +0000"  >&lt;p&gt;Sorry, it&apos;s really just intended as an illustration of the idea in the preceding paragraph.&lt;/p&gt;</comment>
                            <comment id="17023370" author="benedict" created="Sat, 25 Jan 2020 01:13:09 +0000"  >&lt;p&gt;Right, but ordering here is pretty essential, as the &lt;tt&gt;issue&lt;/tt&gt; has to happen first (so that we know what to mark), but as soon as it happens these operations can begin to route to the new &lt;tt&gt;Memtable&lt;/tt&gt; and bypass our don&apos;t-lock check.  But I won&apos;t try to make any stronger claims about what can or cannot happen until next week.  I suppose it might be possible to &lt;tt&gt;markSomething&lt;/tt&gt; in the &lt;tt&gt;issue&lt;/tt&gt; critical section, but before making &lt;tt&gt;orderOrOnBefore&lt;/tt&gt; visible.&lt;/p&gt;</comment>
                            <comment id="17023512" author="benedict" created="Sat, 25 Jan 2020 11:06:39 +0000"  >&lt;p&gt;So, I think anyway it&apos;s unclear to me what the best approach is between these two, as one permits competition amongst operations permitted to grow memtable memory usage unboundedly, and the other potentially reduces parallelism briefly.  The number of parallelism reductions is limited to the number of memtable flushes on the system, which can actually be quite frequent, but probably not frequent enough to matter.  However the number of competing operations to create memory pressure on memtables is also relatively few.  But neither are desirable eventualities, and we cannot predict their true incidence.&lt;/p&gt;

&lt;p&gt;I wonder if a mixture of approach would be a good idea:  try to introduce your suggestion of ignoring the lock for operations that could deadlock (by potentially marking during the critical section, which is likely an imperceptible cost given the rate of barrier issue), so that we do not harm parallelism.  But also prevent operations from re-allocating memory into memtable space, as this memory cannot be reclaimed until a (slow) flush occurs, potentially harming node stability when we&apos;re past our memtable limit.&lt;/p&gt;</comment>
                            <comment id="17025850" author="benedict" created="Wed, 29 Jan 2020 12:59:38 +0000"  >&lt;p&gt;I&apos;ve pushed a branch that implements your suggested approach.  I wanted to get your feedback on whether or not you think we should try to mitigate the issue of competition in these overflow scenario.&lt;/p&gt;

&lt;p&gt;One approach would be to ignore most of the work I did initially wrt &lt;tt&gt;extractUnshadowed&lt;/tt&gt;, and &lt;em&gt;only in the event we will refuse to lock&lt;/em&gt;:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Do not &lt;tt&gt;abortEarly&lt;/tt&gt; for our first update attempt&lt;/li&gt;
	&lt;li&gt;For remaining attempts, apply the whole new partition result to the current head, until we succeed&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;This may mean increased heap allocations for each failed attempt, and each individual update may be slightly slower, but there will be:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;No extra heap allocations for successful attempts&lt;/li&gt;
	&lt;li&gt;We can avoid re-cloning values into the memtable space (which is not reclaimable until flush)&lt;/li&gt;
	&lt;li&gt;Majority of operations are entirely unaffected, so we don&apos;t have to worry much about performance impact&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;WDYT?  I&apos;m &lt;em&gt;relatively&lt;/em&gt; comfortable doing nothing to mitigate this scenario, too.  But it &lt;em&gt;might&lt;/em&gt; be preferable to do this.&lt;/p&gt;</comment>
                            <comment id="17026922" author="bdeggleston" created="Thu, 30 Jan 2020 18:53:16 +0000"  >&lt;p&gt;Nice. I&#8217;m also fairly comfortable not addressing competition in this scenario. Ideally, we would, but I&#8217;m not sure it would be worth adding a secondary synchronization path. Although I guess we could synchronize on the writeOp.&lt;/p&gt;

&lt;p&gt;There still is (technically) a brief window for deadlock between &lt;tt&gt;setCommitLogUpperBound&lt;/tt&gt; and &lt;tt&gt;writeBarrier.issue()&lt;/tt&gt; in &lt;tt&gt;org.apache.cassandra.db.ColumnFamilyStore.Flush#Flush&lt;/tt&gt;, but I&#8217;m not sure if it&#8217;s worth addressing, since we&#8217;d need to immediately waste 10MB on a partition as soon as a memtable is created, and it&#8217;s not exacerbated by flush queue length. Anyway, I think this qualifies as good enough. I&#8217;d also prefer it over waiting on the previous op group because it limits the window of potential bad behavior to a narrower set of circumstances. What do you think?&lt;/p&gt;

&lt;p&gt;About removing the lock, I&#8217;m sure 15511 will help with contention, and we should commit it, however I think there will still be pathological cases where faster updates won&#8217;t be enough. For instance, if there were 20 small updates and one much larger one contending with each other, I can imagine the large one would have a tough time making progress and end up wasting a lot of memory.&lt;/p&gt;

&lt;p&gt;&amp;lt;random-idea&amp;gt;&lt;br/&gt;
 This might be better illustrated with code, and would be a trunk-only follow on ticket, but instead of synchronizing writes on the partition object whenever there&#8217;s contention, what if we queued up contended writes on the partition? If a write comes in and there&#8217;s no longer contention, or the size of queued writes is too high, it could merge the updates and synchronize on applying them. By merging the updates, I think we&#8217;d end up allocating less memory in the contended case than the uncontended case.&lt;br/&gt;
 &amp;lt;/random-idea&amp;gt;&lt;/p&gt;</comment>
                            <comment id="17027080" author="benedict" created="Thu, 30 Jan 2020 23:51:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;but I&#8217;m not sure if it&#8217;s worth addressing&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think any deadlock is acceptable to ignore.  Hmm.  If we don&apos;t go with one of the other approaches I&apos;ve suggested, I&apos;ll have to find some time in a week to see if there&apos;s a variant of this suggested approach that works in this respect.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;&amp;lt;random-idea&amp;gt;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think this is something I have proposed before, but it&apos;s not trivial.  I had planned to implement something like this as part of my work addressing this problem, but decided not to given the complexity.  The idea would be to introduce a linked-list of deferred updates, and merge them either on future reads or writes, but ensuring everyone sees a consistent view with this approach, while minimising duplicated work and ensuring progress, is less trivial than I imagined when I proposed it a while ago.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;About removing the lock, I&#8217;m sure 15511 will help with contention, and we should commit it, however I think there will still be pathological cases where faster updates won&#8217;t be enough&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We can benchmark this specific scenario, but all we really care about is if the aggregate behaviour for all 21 operations is good enough to warrant removal of the lock, and the commensurate reduction in complexity when reasoning about the system (that has been &lt;em&gt;amply&lt;/em&gt; demonstrated by this ticket).  IMO, the performance numbers from 15511 more than cross this threshold, but we can certainly explore further verification work to be certain.&lt;/p&gt;</comment>
                            <comment id="17027653" author="bdeggleston" created="Fri, 31 Jan 2020 16:41:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;I don&apos;t think any deadlock is acceptable to ignore.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was assuming this couldn&#8217;t be modified to eliminate that window for deadlock. It would be great if it could be. If not, I agree this needs to be fixed eventually, but given the extent that this mitigates the issue, I do think it should be committed, at least as a stop-gap for trunk, but possibly as the fix for 3.x.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;lock stuff&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;maybe we should pick this conversation up when we get back to removing the lock. I&#8217;m pretty unfamiliar with the specifics, and it&#8217;s probably not very useful for me to be arguing about stuff based on a bunch of assumptions.&lt;/p&gt;</comment>
                            <comment id="17036235" author="benedict" created="Thu, 13 Feb 2020 13:50:34 +0000"  >&lt;p&gt;So, I&apos;m looking at this more closely now I have some time, and I wonder if you could outline how you think the deadlock occurs between &lt;tt&gt;setCommitLogUpperBound&lt;/tt&gt; and &lt;tt&gt;writeBarrier.issue()&lt;/tt&gt;?  Because the deadlock requires a new cohort to exist, that does not get instantiated until &lt;tt&gt;writeBarrier.issue()&lt;/tt&gt; so the deadlock cannot occur until then?&lt;/p&gt;

&lt;p&gt;However there &lt;em&gt;is&lt;/em&gt; a window &lt;em&gt;after&lt;/em&gt; &lt;tt&gt;!writeOp.isBehindBarrier()&lt;/tt&gt;, which cannot be avoided because there are no timed wait mechanisms for obtaining a monitor, and &lt;tt&gt;tryMonitorEnter&lt;/tt&gt; anyway isn&apos;t possible in later versions of Java.&lt;/p&gt;

&lt;p&gt;So, I propose a variant of my earlier approach that definitely worked, that waited for all earlier operations to complete, to instead essentially invert the behaviour of your suggestion: if there are any running older operations, refuse to lock until they all complete (and invoke &lt;tt&gt;Thread.yield()&lt;/tt&gt; once to give them an opportunity with the CPU).  So locking is essentially disabled for all newer operations until the older ones expire, and we try to give them dibs on the CPU if the scheduler lets us, so that this window is as narrow as possible.&lt;/p&gt;</comment>
                            <comment id="17036255" author="benedict" created="Thu, 13 Feb 2020 14:14:29 +0000"  >&lt;p&gt;I&apos;ve pushed an update that does this, but I need to look at it again tomorrow to be 100% certain everything is fine.&lt;/p&gt;</comment>
                            <comment id="17039466" author="bdeggleston" created="Tue, 18 Feb 2020 21:15:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;how you think the deadlock occurs&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It may be the same one you&apos;re referring to. Basically if a write blocks trying to acquire the lock on the new memtable after the final commit log position is set and before the write barrier is issued, there&apos;s a risk of deadlock.&lt;/p&gt;

&lt;p&gt;Given op groups O1 &amp;amp; O2, replay positions R1 &amp;amp; R2, and memtables M1 &amp;amp; M2. M1 is flushing with a barrier on O1, and final commit log upper bound R1 is set on it.&#160;A new write (W1) is assigned to O1, and writes to the commit log at position R2. It will overflow to M2, the new memtable. Before the barrier is set on it, it blocks on a locked partition. Write W2 against O2 then acquires the lock ahead of W1, blocks on the allocator, and we deadlock.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;So, I propose a variant of my earlier approach that definitely worked&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Great, as far as I can tell, this fixes the deadlock 100% and with minimal risk of adding new issues or disrupting system behavior. +1&lt;/p&gt;</comment>
                            <comment id="17051130" author="benedict" created="Wed, 4 Mar 2020 11:45:04 +0000"  >&lt;p&gt;Sorry for the delay on this, I&apos;ve been trying to figure out what&apos;s wrong with CircleCI for 3.0 and 3.11, but have given up.  It appears the issues occur on HEAD for both as well.&lt;/p&gt;

&lt;p&gt;patch:&lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/15367-3.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.0&lt;/a&gt; ci:&lt;a href=&quot;https://circleci.com/workflow-run/b6396d79-d2bb-42a1-a76f-805715e896de&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.0&lt;/a&gt; head ci:&lt;a href=&quot;https://circleci.com/workflow-run/759d8331-7e2f-431f-8161-847fe46af4d3&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.0&lt;/a&gt;&lt;br/&gt;
patch:&lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/15367-311&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.11&lt;/a&gt; ci:&lt;a href=&quot;https://circleci.com/workflow-run/7c74fea5-3730-44bf-bd62-5d112cf974d4&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.11&lt;/a&gt; head ci:&lt;a href=&quot;https://circleci.com/workflow-run/72a173e0-713e-479d-b5db-9fd7e94eb3d6&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.11&lt;/a&gt;&lt;br/&gt;
patch:&lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/15367-trunk&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;trunk&lt;/a&gt; ci:&lt;a href=&quot;https://circleci.com/workflow-run/b2b24c56-d191-4923-9f66-b05036d62ea1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;trunk&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I will commit tomorrow after double checking the test failures are consistent and assuming you don&apos;t say otherwise.&lt;/p&gt;
</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[benedict]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12313825" key="com.atlassian.jira.plugin.system.customfieldtypes:cascadingselect">
                        <customfieldname>Bug Category</customfieldname>
                        <customfieldvalues>
                                                    <customfieldvalue key="12983" cascade-level=""><![CDATA[Availability]]></customfieldvalue>
                                <customfieldvalue key="12992" cascade-level="1"><![CDATA[Process Crash]]></customfieldvalue>
            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12313821" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Complexity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12966"><![CDATA[Challenging]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313822" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Discovered By</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12969"><![CDATA[User Report]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12313922" key="jira.plugin.projectspecificselectfield.jpssf:multicftype">
                        <customfieldname>Impacts</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="13100"><![CDATA[None]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 36 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12313921" key="jira.plugin.projectspecificselectfield.jpssf:multicftype">
                        <customfieldname>Platform</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="13076"><![CDATA[All]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z07u34:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[benedict]]></customfieldvalue>
        <customfieldvalue><![CDATA[bdeggleston]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12963"><![CDATA[Critical]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311420" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12326275">2.1 beta1</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313924" key="com.atlassian.jira.plugin.system.customfieldtypes:textfield">
                        <customfieldname>Source Control Link</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;p&gt;&lt;a href=&quot;https://github.com/apache/cassandra/commit/e3f54d4a0c3403141db24f86714c3900eb9f212e&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;e3f54d4a0c3403141db24f86714c3900eb9f212e&lt;/a&gt;&lt;/p&gt;</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313823" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Test and Documentation Plan</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>&lt;p&gt;unit test included&lt;/p&gt;</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>