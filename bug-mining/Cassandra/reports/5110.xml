<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:12:19 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-14358] Partitioned outbound internode TCP connections can occur when nodes restart</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-14358</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;edit summary: This primarily impacts networks with stateful firewalls such as AWS. I&apos;m working on a proper patch for trunk but unfortunately it relies on the Netty refactor in 4.0 so it will be hard to backport to previous versions.&#160;A workaround for earlier versions is to set the &lt;tt&gt;net.ipv4.tcp_retries2&lt;/tt&gt;&#160;sysctl to ~5. This can be done with the following:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
$ cat /etc/sysctl.d/20-cassandra-tuning.conf
net.ipv4.tcp_retries2=5
$ # Reload all sysctls
$ sysctl --system&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Original Bug Report:&lt;/p&gt;

&lt;p&gt;I&apos;ve been trying to debug nodes not being able to see each other during longer (~5 minute+) Cassandra restarts in 3.0.x and 2.1.x which can contribute to &lt;tt&gt;UnavailableExceptions&lt;/tt&gt; during rolling restarts of 3.0.x and 2.1.x clusters for us. I think I finally have a lead. It appears that prior to trunk (with the awesome Netty refactor) we do not set socket connect timeouts on SSL connections (in 2.1.x, 3.0.x, or 3.11.x) nor do we set &lt;tt&gt;SO_TIMEOUT&lt;/tt&gt; as far as I can tell on outbound connections either. I believe that this means that we could potentially block forever on &lt;tt&gt;connect&lt;/tt&gt; or &lt;tt&gt;recv&lt;/tt&gt;&#160;syscalls, and we could block forever on the SSL Handshake as well. I think that the OS will protect us somewhat (and that may be what&apos;s causing the eventual timeout) but I think that given the right network conditions our &lt;tt&gt;OutboundTCPConnection&lt;/tt&gt; threads can just be stuck never making any progress until the OS intervenes.&lt;/p&gt;

&lt;p&gt;I have attached&#160;some logs of such a network partition&#160;during a rolling restart where an old node in the cluster has a completely foobarred &lt;tt&gt;OutboundTcpConnection&lt;/tt&gt; for ~10 minutes before finally getting a &lt;tt&gt;java.net.SocketException: Connection timed out (Write failed)&lt;/tt&gt; and immediately successfully reconnecting. I conclude that the old node is the problem because the new node (the one that restarted) is sending ECHOs to the old node, and the old node is sending ECHOs and REQUEST_RESPONSES to the new node&apos;s ECHOs, but the new node is never getting the ECHO&apos;s. This appears, to me, to indicate that the old node&apos;s &lt;tt&gt;OutboundTcpConnection&lt;/tt&gt;&#160;thread is just stuck and can&apos;t make any forward progress. By the time we could notice this and slap TRACE logging on, the only thing we see is ~10 minutes later a &lt;tt&gt;SocketException&lt;/tt&gt;&#160;inside &lt;tt&gt;writeConnected&lt;/tt&gt;&apos;s flush and an immediate recovery. It is interesting to me that the exception happens in &lt;tt&gt;writeConnected&lt;/tt&gt; and it&apos;s a &lt;em&gt;connection timeout&lt;/em&gt; (and since we see &lt;tt&gt;Write failure&lt;/tt&gt;&#160;I believe&#160;that this can&apos;t be a connection reset), because my understanding is that we should have a fully handshaked SSL connection at that point in the code.&lt;/p&gt;

&lt;p&gt;Current theory:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;&quot;New&quot; node restarts,&#160; &quot;Old&quot; node calls &lt;a href=&quot;https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L433&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;newSocket&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;Old node starts &lt;a href=&quot;https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnectionPool.java#L141&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;creating a new&lt;/a&gt;&#160;SSL socket&#160;&lt;/li&gt;
	&lt;li&gt;SSLSocket calls &lt;a href=&quot;https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/security/SSLFactory.java#L98&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;createSocket&lt;/a&gt;, which conveniently calls connect with a default timeout of &quot;forever&quot;. We could hang here forever until the OS kills us.&lt;/li&gt;
	&lt;li&gt;If we continue, we get to &lt;a href=&quot;https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L263&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;writeConnected&lt;/a&gt; which eventually calls &lt;a href=&quot;https://github.com/apache/cassandra/blob/6f30677b28dcbf82bcd0a291f3294ddf87dafaac/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L341&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;flush&lt;/a&gt; on the output stream and also can hang forever. I think the probability is especially high when a node is restarting and is overwhelmed with SSL handshakes and such.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I don&apos;t fully understand the attached traceback as it appears we are getting a &lt;tt&gt;Connection Timeout&lt;/tt&gt; from a &lt;tt&gt;send&lt;/tt&gt; failure (my understanding is you can only get a connection timeout prior to a send), but I think it&apos;s reasonable that we have a timeout configuration issue.&#160;I&apos;d like to try to make Cassandra robust to networking issues like this via maybe:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Change the &lt;tt&gt;SSLSocket&lt;/tt&gt; &lt;tt&gt;getSocket&lt;/tt&gt; methods to provide connection timeouts of 2s (equivalent to trunk&apos;s &lt;a href=&quot;https://github.com/apache/cassandra/blob/11496039fb18bb45407246602e31740c56d28157/src/java/org/apache/cassandra/net/async/NettyFactory.java#L329&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;timeout&lt;/a&gt;)&lt;/li&gt;
	&lt;li&gt;Appropriately set recv timeouts via&#160;&lt;tt&gt;SO_TIMEOUT&lt;/tt&gt;, maybe something like 2 minutes (in old versions via &lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html#setSoTimeout-int-&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;setSoTimeout&lt;/a&gt;, in trunk via &lt;a href=&quot;http://netty.io/4.0/api/io/netty/channel/ChannelOption.html#SO_TIMEOUT&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;SO_TIMEOUT&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;Since we can&apos;t set send timeouts afaik (thanks java) maybe we can have some kind of watchdog that ensures OutboundTcpConnection is making progress in its queue and if it doesn&apos;t make any progress for ~30s-1m, forces a disconnect.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;If anyone has insight or suggestions, I&apos;d be grateful. I am going to rule out if this is keepalive duration by setting tcp_keepalive_probes to like 1 and maybe tcp_retries2 to like 8 get more information about the state of the tcp connections the next time this happens. It&apos;s a very rare bug and when it does happen I only have 10 minutes to jump on the nodes and fix it before it fixes itself so I&apos;ll do my best.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Cassandra 2.1.19 (also reproduced on 3.0.15), running with &lt;tt&gt;internode_encryption: all&lt;/tt&gt;&#160;and the EC2 multi region snitch on Linux 4.13 within the same AWS region. Smallest cluster I&apos;ve seen the problem on is 12 nodes, reproduces more reliably on 40+ and 300 node clusters consistently reproduce on at least one node in the cluster.&lt;/p&gt;

&lt;p&gt;So all the connections are SSL and we&apos;re connecting on the internal ip addresses (not the public endpoint ones).&lt;/p&gt;

&lt;p&gt;Potentially relevant sysctls:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;/proc/sys/net/ipv4/tcp_syn_retries = 2
/proc/sys/net/ipv4/tcp_synack_retries = 5
/proc/sys/net/ipv4/tcp_keepalive_time = 7200
/proc/sys/net/ipv4/tcp_keepalive_probes = 9
/proc/sys/net/ipv4/tcp_keepalive_intvl = 75
/proc/sys/net/ipv4/tcp_retries2 = 15
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</environment>
        <key id="13149263">CASSANDRA-14358</key>
            <summary>Partitioned outbound internode TCP connections can occur when nodes restart</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jolynch">Joey Lynch</assignee>
                                    <reporter username="jolynch">Joey Lynch</reporter>
                        <labels>
                            <label>4.0-feature-freeze-review-requested</label>
                    </labels>
                <created>Fri, 30 Mar 2018 23:51:34 +0000</created>
                <updated>Fri, 15 May 2020 08:05:27 +0000</updated>
                            <resolved>Thu, 1 Nov 2018 22:37:46 +0000</resolved>
                                        <fixVersion>4.0-alpha1</fixVersion>
                    <fixVersion>4.0</fixVersion>
                                    <component>Legacy/Streaming and Messaging</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>21</watches>
                                                                                                                <comments>
                            <comment id="16421087" author="jolynch" created="Fri, 30 Mar 2018 23:59:05 +0000"  >&lt;p&gt;It&apos;s also worth noting that the non ssl connections have the same problem, it&apos;s just unlikely I think that the destination server get&apos;s as overloaded and drops a handshake.&lt;/p&gt;</comment>
                            <comment id="16422073" author="djoshi3" created="Mon, 2 Apr 2018 09:49:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jolynch&quot; class=&quot;user-hover&quot; rel=&quot;jolynch&quot;&gt;jolynch&lt;/a&gt;, I think what you&apos;re experiencing is a classic slow consumer issue. The OS will likely buffer the socket but it looks like once this queue is full, you will see it stuck&#160;on &lt;tt&gt;java.net.SocketOutputStream.socketWrite0&lt;/tt&gt;. On Linux, TCP will continue trying to retransmit your packets for roughly 15 minutes before it finally gives up. This is controlled via &lt;tt&gt;tcp_retries2 (retransmission timeout)&lt;/tt&gt;. You can tune it down to be closer to 100s if that is more desirable to you (See:&#160;RFC 1122).&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16422783" author="jolynch" created="Mon, 2 Apr 2018 17:07:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=djoshi3&quot; class=&quot;user-hover&quot; rel=&quot;djoshi3&quot;&gt;djoshi3&lt;/a&gt; yea I think that you&apos;re probably right, but I think that this might not be a slow consumer so much as a never consumer. Specifically if there is a stateful firewall (e.g. security groups or vpc) in the way, the network could absolutely blackhole packets on a connection that&apos;s been reset. My plan for this week is I&apos;m going to try to catch this happening and run the following analysis:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Get a netstat view of established connections and their send buffers on both sides&lt;/li&gt;
	&lt;li&gt;Slap a tcpdump on both sides to see if &lt;tt&gt;RESETS&lt;/tt&gt; from the restarted node are even getting to the old node (VPC might be swallowing them)&lt;/li&gt;
	&lt;li&gt;If #1 or #2 confirm the theory that we&apos;re just in a stuck tcp connection, I will try tuning &lt;tt&gt;tcp_retries2&lt;/tt&gt; down to ~5, setting &lt;tt&gt;SO_SNDBUF&lt;/tt&gt; or &lt;tt&gt;wmem_default&lt;/tt&gt; down to a more reasonable number (I think right now it&apos;s like 16 megs) to see if that fixes the issue&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;For what it&apos;s worth, Cassandra is &lt;em&gt;really&lt;/em&gt; intolerant of these kinds of network partitions. For example if I tell a stateful firewall to just drop all packets on one of Cassandra&apos;s internode connections with &lt;tt&gt;iptables -A OUPTUT -p tcp -d &amp;lt;ip1&amp;gt; --sport &amp;lt;ip1 local port&amp;gt; --dport &amp;lt;ssl port&amp;gt; -j DROP&lt;/tt&gt;, Cassandra continues sending packets out on that &lt;tt&gt;OutboundTcpConnection&lt;/tt&gt; until the operating system says &quot;hey that&apos;s probably not going to work&quot; 15 minutes later. Unfortunately we have two TCP connections so I can&apos;t just do a simple heartbeat mechanism.&lt;/p&gt;</comment>
                            <comment id="16422794" author="jjirsa" created="Mon, 2 Apr 2018 17:15:03 +0000"  >&lt;p&gt;Suspect you&apos;ll see a single byte in the sendq on the instance that didnt bounce.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16422896" author="djoshi3" created="Mon, 2 Apr 2018 18:14:40 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jolynch&quot; class=&quot;user-hover&quot; rel=&quot;jolynch&quot;&gt;jolynch&lt;/a&gt; I understand the problem that you&apos;re describing but this is TCP specific and not Cassandra specific. If there is packet loss or slow consumer or no consumer but the connection is established, all applications will see this issue. The problem here is that the thread is stuck on making progress due to Java Sockets implementation specifically the &lt;tt&gt;write0&lt;/tt&gt; call. In case of Netty or Java NIO, you simply would ignore the socket and continue processing other &quot;ready&quot; sockets allowing Cassandra&apos;s threads to make progress. I suspect you will not see this issue in trunk. Have you given that a try?&lt;/p&gt;</comment>
                            <comment id="16422969" author="jolynch" created="Mon, 2 Apr 2018 18:52:41 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=djoshi3&quot; class=&quot;user-hover&quot; rel=&quot;djoshi3&quot;&gt;djoshi3&lt;/a&gt; I agree that the issue is probably a bad TCP level connection, but also I think it&apos;s a&#160; bug that Cassandra blocks forever never making any progress and potentially causing an outage for users (until the OS kills the connection); we&apos;re supposed to be highly available right &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;. In non JVM languages this would be as simple (I think) as setting &lt;tt&gt;SO_SNDTIMEO&lt;/tt&gt; to a reasonable number like 30s, but I don&apos;t see how to do it without JNI in Cassandra 2.1.x/3.0.x/3.11.x. If this is the issue (I think we still need more data), I also think recommending users tune &lt;tt&gt;tcp_retries2&lt;/tt&gt; to 8, 5 or even 3 would be a reasonable workaround if it&apos;s fixed in trunk.&lt;/p&gt;

&lt;p&gt;I haven&apos;t tried in trunk (tbh we can&apos;t even build/deploy trunk right now), but unfortunately I think it would be vulnerable to same kind of issue since afaict &lt;tt&gt;&lt;a href=&quot;https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/NettyFactory.java#L358-L377&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;NettyFactory&lt;/a&gt;&lt;/tt&gt; doesn&apos;t add any kind of &lt;tt&gt;&lt;a href=&quot;https://netty.io/4.0/api/io/netty/handler/timeout/WriteTimeoutHandler.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;WriteTimeoutHandler&lt;/a&gt;&lt;/tt&gt; or &lt;tt&gt;&lt;a href=&quot;https://netty.io/4.0/api/io/netty/handler/timeout/ReadTimeoutHandler.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;ReadTimeoutHandler&lt;/a&gt;&lt;/tt&gt; which will throw an exception (I believe) if the write/read on the socket doesn&apos;t complete in a reasonable amount of time. Admittedly I&apos;m very new to the netty refactor in 4.0 so I might be missing the part where we set a bunch of timeouts somewhere.&lt;/p&gt;

&lt;p&gt;I don&apos;t want to invest too much time testing trunk until I confirm the type of partition that is actually happening, but if our theory is correct it should be easy to reproduce on trunk with the above iptables rule (if trunk handles it then the tcp connection should get thrown out after ~30s or something reasonable and we re-connect).&lt;/p&gt;</comment>
                            <comment id="16423155" author="aweisberg" created="Mon, 2 Apr 2018 21:18:34 +0000"  >&lt;p&gt;We can implement timeouts with a watchdog and wake up the threads by closing the socket. This will work even if they are blocked on the various socket methods.&lt;/p&gt;

&lt;p&gt;This sounds like a real implementation hole to me. It means MTTR for any instance where the kernel doesn&apos;t gracefully clean up connections is as long as whatever the timeout is.&lt;/p&gt;

&lt;p&gt;Even the Netty implementation is vulnerable here because it won&apos;t attempt the reconnect for a long time. The thread won&apos;t be blocked, but that isn&apos;t the issue here the issue is that communication itself is blocked until the faulty connection is replaced.&lt;/p&gt;</comment>
                            <comment id="16423167" author="pauloricardomg" created="Mon, 2 Apr 2018 21:33:02 +0000"  >&lt;p&gt;Did you try reproducing this issue with &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9630&quot; title=&quot;Killing cassandra process results in unclosed connections&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9630&quot;&gt;&lt;del&gt;CASSANDRA-9630&lt;/del&gt;&lt;/a&gt; in? This has been committed relatively recently and it used to cause hanging connections on 2.1/3.X.&lt;/p&gt;</comment>
                            <comment id="16423205" author="jolynch" created="Mon, 2 Apr 2018 22:12:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aweisberg&quot; class=&quot;user-hover&quot; rel=&quot;aweisberg&quot;&gt;aweisberg&lt;/a&gt; if the issue is what I think it is (and I definitely need more supporting evidence), I think we can fix it in trunk with a netty &lt;tt&gt;WriteTimeoutHandler&lt;/tt&gt;/ &lt;tt&gt;ReadTimeoutHandler&lt;/tt&gt; which will throw away the tcp connection if recv/write take too long. I think it&apos;s reasonable to say in earlier versions we are not going to patch it past connection and recv timeouts and just recommend &lt;tt&gt;/proc/sys/net/ipv4/tcp_retries2 = 5&lt;/tt&gt;. I tested the sysctl and it leads to a much shorter partition with the iptables test, like 20s instead of 10 minutes. I&apos;m testing it out on a large cluster with a rolling restart soon.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt; I believe that &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9630&quot; title=&quot;Killing cassandra process results in unclosed connections&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9630&quot;&gt;&lt;del&gt;CASSANDRA-9630&lt;/del&gt;&lt;/a&gt; is a separate issue (although +1 to a 2.1.x backport there since it&apos;s so simple and 2.1.x is actually still used places) because in this case I believe we have an established connection that is blackholing. I&apos;ll double check this next time I run the test and ensure that the tcp connection state is &lt;tt&gt;ESTABLISHED&lt;/tt&gt;.&lt;/p&gt;</comment>
                            <comment id="16423406" author="djoshi3" created="Tue, 3 Apr 2018 03:05:08 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aweisberg&quot; class=&quot;user-hover&quot; rel=&quot;aweisberg&quot;&gt;aweisberg&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jolynch&quot; class=&quot;user-hover&quot; rel=&quot;jolynch&quot;&gt;jolynch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The underlying issue is that when a node restarts and begins listening, the peers in the cluster cause a thundering herd overwhelming the node. In 3.0 / 2.1 we accept the incoming connection and read some bytes off the socket on the same thread. This probably causes the connections to build up in the listen queue. Using a threadpool and separating the accept from rest of the code would make this better. &lt;/p&gt;

&lt;p&gt;If the peers kill their connections and restart them with a shorter timeout, the restarting node will get further overwhelmed. I looked at trunk and I don&apos;t think we will see the same issue crop up as we use a separate event loop group for accepting connections. Netty internally uses a separate IO thread for the channel pipeline and the request is dispatched using a separate threadpool. Unless you block the Netty accept thread or the IO thread, these symptoms won&apos;t surface.&lt;/p&gt;</comment>
                            <comment id="16424033" author="jasobrown" created="Tue, 3 Apr 2018 13:38:38 +0000"  >&lt;p&gt;For trunk, we already use &lt;tt&gt;IdleStateHandler&lt;/tt&gt; &lt;a href=&quot;https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/net/async/OutboundHandshakeHandler.java#L196&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;in the outbound pipeline&lt;/a&gt; for the &quot;not making progress sending data&quot; case. Take a look at&#160;&lt;tt&gt;MessageOutHandler#userEventTriggered&lt;/tt&gt;. iirc, &lt;tt&gt;WriteTimeoutHandler&lt;/tt&gt; / &lt;tt&gt;ReadTimeoutHandler&lt;/tt&gt; does not fit our use case for messaging; I&apos;d have to look again to page it back in.&lt;/p&gt;

&lt;p&gt;Further, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jolynch&quot; class=&quot;user-hover&quot; rel=&quot;jolynch&quot;&gt;jolynch&lt;/a&gt;, I thought your problem was in pre-4.0? There is a problem (or, at least, non-optimization) with &lt;tt&gt;MessagingService.SocketThread&lt;/tt&gt; where the single-threaded accept thread handles receiving the first message pf the c* internode messaging protocol, which may require executing the TLS handshake before that can be received (if using internode TLS). Thus a single new establishing connection may block the accept thread for 500-1000 ms for a cross-datacenter connection. Even worse, if you are using the &lt;tt&gt;Ec2MultiregionSnitch&lt;/tt&gt;, all connections for the local region will be torn down and built anew as we switch from the public IP to the region-local private IP - more traffic on the accept thread. So one slow connection interaction screws the whole accept thread.&lt;/p&gt;

&lt;p&gt;I have an internal patch for 3.0 which, when a new socket is accepted, moves all processing (internode protocol handshake, TLS handshake) to a separate thread (via an executor group). So, if one connection runs into a problem, the other incoming connections are not held up. I&apos;ve been reticent about OSS&apos;ing this late in the 3.0/3.11 branches, even though it&apos;s not that clever or invasive. Let me know if you&apos;d be interested in giving it shot and I can post it. If it&apos;s useful/helpful we can consider committing it, as well.&lt;/p&gt;</comment>
                            <comment id="16424110" author="aweisberg" created="Tue, 3 Apr 2018 14:42:51 +0000"  >&lt;p&gt;A node restart that is graceful (kernel closes connections) is different from power failure or network black holing packets to a host. In the latter case what in Cassandra causes the server to notice a TCP connection is no longer responding and attempt to recreate it so it can connect to the new version of the host?&lt;/p&gt;</comment>
                            <comment id="16424369" author="jolynch" created="Tue, 3 Apr 2018 18:00:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=djoshi3&quot; class=&quot;user-hover&quot; rel=&quot;djoshi3&quot;&gt;djoshi3&lt;/a&gt;&#160;We send RSTs on listen overflows via the &lt;tt&gt;tcp_abort_on_overflow&lt;/tt&gt; sysctl, so I don&apos;t think that this is a simple listen overflow. You&apos;re absolutely right that nodes coming up can get overwhelmed in 2.1/3.0/3.11, and I think it&apos;s really great that trunk fixes that; but I don&apos;t think this is the bug I&apos;ve identified here. In this case a Cassandra node that &lt;em&gt;didn&apos;t&lt;/em&gt; restart continues trying to send on a dead connection forever until the kernel steps in and cuts off the connection.&lt;/p&gt;

&lt;p&gt;&#160;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jasobrown&quot; class=&quot;user-hover&quot; rel=&quot;jasobrown&quot;&gt;jasobrown&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;For trunk, we already use IdleStateHandler in the outbound pipeline for the &quot;not making progress sending data&quot; case. Take a look at MessageOutHandler#userEventTriggered. iirc, WriteTimeoutHandler / ReadTimeoutHandler does not fit our use case for messaging; I&apos;d have to look again to page it back in.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I am definitely not a netty expert, but I think that &lt;tt&gt;IdleStateHandler&lt;/tt&gt; needs to be paired with a &lt;tt&gt;WriteTimeoutHandler&lt;/tt&gt; or &lt;tt&gt;ReadTimeoutHandler&lt;/tt&gt; in order to effectively identify such blackhole partitions. The app is sending data on the socket, it&apos;s just never going anywhere. My understanding is that &lt;tt&gt;IdleStateHandler&lt;/tt&gt; just sends data if there is none.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Further, Joseph Lynch, I thought your problem was in pre-4.0? There is a problem (or, at least, non-optimization) with MessagingService.SocketThread where the single-threaded accept thread handles receiving the first message pf the c* internode messaging protocol, which may require executing the TLS handshake before that can be received (if using internode TLS). Thus a single new establishing connection may block the accept thread for 500-1000 ms for a cross-datacenter connection. Even worse, if you are using the Ec2MultiregionSnitch, all connections for the local region will be torn down and built anew as we switch from the public IP to the region-local private IP - more traffic on the accept thread. So one slow connection interaction screws the whole accept thread.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes, our problem is with pre-4.0, but I&apos;m 99% sure we can mitigate this by setting &lt;tt&gt;tcp_retries2&lt;/tt&gt; to 3 or 5 to tell the OS to cover over the app bug. If Cassandra trunk is resilient to the (rare) bug I think&#160;the sysctl is a reasonable workaround for earlier versions, what do you think? It&apos;s also worth noting that this kind of partition probably only occurs frequently in stateful networks that swallow RSTs to unknown flows such as AWS VPC (although as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aweisberg&quot; class=&quot;user-hover&quot; rel=&quot;aweisberg&quot;&gt;aweisberg&lt;/a&gt; points out, power failure could manifest similarly).&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I have an internal patch for 3.0 which, when a new socket is accepted, moves all processing (internode protocol handshake, TLS handshake) to a separate thread (via an executor group). So, if one connection runs into a problem, the other incoming connections are not held up. I&apos;ve been reticent about OSS&apos;ing this late in the 3.0/3.11 branches, even though it&apos;s not that clever or invasive. Let me know if you&apos;d be interested in giving it shot and I can post it. If it&apos;s useful/helpful we can consider committing it, as well.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I don&apos;t think that would fix this particular bug, but it is awesome and fixes other issues. I volunteer to test it and review it if that helps get it merged to 3.0/3.11.&lt;/p&gt;</comment>
                            <comment id="16424370" author="djoshi3" created="Tue, 3 Apr 2018 18:01:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aweisberg&quot; class=&quot;user-hover&quot; rel=&quot;aweisberg&quot;&gt;aweisberg&lt;/a&gt; For cases where you have power failure (or process crashes or even restarts without properly closing its connections), the Socket will be left in a &quot;half-open&quot; state. Per TCP&apos;s spec, it will retry transmitting the packet a number of times. Each unsuccessful attempt will cause an exponential back off. Ultimately it will give up and send a RST packet. All this while your application will continue writing to the Kernel&apos;s socket buffer and will not know that the other end is dead / unresponsive. This is per TCP&apos;s design. If you want your applications to timeout faster or detect dead peers quicker you can tune TCP&apos;s parameters per your requirements.&lt;/p&gt;</comment>
                            <comment id="16424802" author="jolynch" created="Wed, 4 Apr 2018 00:24:13 +0000"  >&lt;p&gt;For what it&apos;s worth we&apos;ve opened an AWS ticket to find out if this is expected behaviour in VPCs (that the network blackholes packets on unknown flows rather than resetting them). I have a feeling that&#160;answer will be &quot;yea that&apos;s how stateful firewalls work we can&apos;t keep flows forever&quot;, in which&#160;case Cassandra (probably)&#160;should be resilient to it I think.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=djoshi3&quot; class=&quot;user-hover&quot; rel=&quot;djoshi3&quot;&gt;djoshi3&lt;/a&gt; I think you&apos;ve got it now. I&apos;m proposing that we tune Cassandra&apos;s socket options (opt-in of course like most socket options we set) to tune TCP&apos;s parameters per Cassandra&apos;s requirement of high availability. In the case of blocking socket the way that I&apos;m familiar with are connect timeouts (&lt;a href=&quot;http://man7.org/linux/man-pages/man2/connect.2.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;unix&lt;/a&gt;, &lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html#connect-java.net.SocketAddress-int-&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;java&lt;/a&gt;), receive timeouts (&lt;a href=&quot;https://linux.die.net/man/7/socket&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;SO_RCVTIMEO&lt;/a&gt;, &lt;a href=&quot;https://docs.oracle.com/javase/8/docs/api/java/net/Socket.html#setSoTimeout-int-&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;java&lt;/a&gt;) and send timeouts which Java doesn&apos;t expose afaict (&lt;a href=&quot;https://linux.die.net/man/7/socket&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;SO_SNDTIMEO&lt;/a&gt;). For &lt;tt&gt;O_NONBLOCK&lt;/tt&gt; sockets I think typically the timeout is handled at the event loop level (e.g. I think that&apos;s what &lt;tt&gt;WriteTimeoutHandler&lt;/tt&gt; and &lt;tt&gt;ReadTimeoutHandler&lt;/tt&gt; do in Netty).&lt;/p&gt;

&lt;p&gt;I still need to test this thoroughly, but I believe asking users to set &lt;tt&gt;/proc/sys/net/ipv4/tcp_retries2 = 5&lt;/tt&gt; is a reasonable workaround. I was able to simulate pretty easily the exact error in the production logs by doing the following:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$ sudo lsof -p $(pgrep -f Cassandra) -n | grep OTHER_IP | grep STORAGE_PORT
java    5277 cassandra  64u     IPv4             113860      0t0        TCP LOCAL_IP:38665-&amp;gt;OTHER_IP:STORAGE_PORT (ESTABLISHED)
java    5277 cassandra  69u     IPv4             114797      0t0        TCP LOCAL_IP:STORAGE_PORT-&amp;gt;OTHER_IP:54875 (ESTABLISHED)

$ sudo iptables -A OUTPUT -p tcp -d OTHER_IP --sport 38665 --dport STORAGE_PORT -j DROP

$ cqlsh
cqlsh&amp;gt; CONSISTENCY ALL
Consistency level set to ALL.
cqlsh&amp;gt; select * from test_ks.test_cf WHERE key = &apos;key_that_lives_on_OTHER_IP&apos;;
OperationTimedOut: errors={}, last_host=&amp;lt;something&amp;gt;
... timeouts happen for next ~15 minutes

$ tail -20 system.log 
TRACE [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:05,009 OutboundTcpConnection.java:365 - Socket to /OTHER_IP closed
DEBUG [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:05,010 OutboundTcpConnection.java:303 - error writing to /OTHER_IP
java.net.SocketException: Connection timed out (Write failed)
	at java.net.SocketOutputStream.socketWrite0(Native Method) ~[na:1.8.0_152]
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111) ~[na:1.8.0_152]
	at java.net.SocketOutputStream.write(SocketOutputStream.java:155) ~[na:1.8.0_152]
	at sun.security.ssl.OutputRecord.writeBuffer(OutputRecord.java:431) ~[na:1.8.0_152]
	at sun.security.ssl.OutputRecord.write(OutputRecord.java:417) ~[na:1.8.0_152]
	at sun.security.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:886) ~[na:1.8.0_152]
	at sun.security.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:857) ~[na:1.8.0_152]
	at sun.security.ssl.AppOutputStream.write(AppOutputStream.java:123) ~[na:1.8.0_152]
	at net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:205) ~[lz4-1.2.0.jar:na]
	at net.jpountz.lz4.LZ4BlockOutputStream.flush(LZ4BlockOutputStream.java:222) ~[lz4-1.2.0.jar:na]
	at org.apache.cassandra.io.util.DataOutputStreamPlus.flush(DataOutputStreamPlus.java:55) ~[cassandra-2.1.19.jar:2.1.19]
	at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:294) [cassandra-2.1.19.jar:2.1.19]
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:222) [cassandra-2.1.19.jar:2.1.19]
DEBUG [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:49,867 OutboundTcpConnection.java:380 - attempting to connect to /OTHER_IP
INFO  [HANDSHAKE-/OTHER_IP] 2018-04-04 00:13:49,916 OutboundTcpConnection.java:496 - Handshaking version with /OTHER_IP
TRACE [MessagingService-Outgoing-/OTHER_IP] 2018-04-04 00:13:49,960 OutboundTcpConnection.java:453 - Upgrading OutputStream to be compressed

$ cqlsh
cqlsh&amp;gt; CONSISTENCY ALL
Consistency level set to ALL.
cqlsh&amp;gt; select * from test_ks.test_cf WHERE key = &apos;key_that_lives_on_OTHER_IP&apos;;
... results come back and everything is happy

$ sudo lsof -p $(pgrep -f Cassandra) -n | grep OTHER_IP | grep STORAGE_PORT
java    5277 cassandra  134u     IPv4             113860      0t0        TCP LOCAL_IP:33417-&amp;gt;OTHER_IP:STORAGE_PORT (ESTABLISHED)
java    5277 cassandra   69u     IPv4             114797      0t0        TCP LOCAL_IP:STORAGE_PORT-&amp;gt;OTHER_IP:54875 (ESTABLISHED)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This simulates &quot;blackholing&quot; of the &lt;tt&gt;OutboundTcpConnection&lt;/tt&gt;. With the default linux value of &lt;tt&gt;/proc/sys/net/ipv4/tcp_retries2 = 15&lt;/tt&gt; this takes ~15 minutes to resolve (the operating system eventually saves Cassandra). With the changed value of ~3, the partition only lasts ~15 seconds. If Cassandra trunk set (configurable) read and write timeouts we could control how long it takes to recover from such a partition rather than relying on the OS.&lt;/p&gt;</comment>
                            <comment id="16427531" author="jolynch" created="Thu, 5 Apr 2018 20:19:40 +0000"  >&lt;p&gt;A quick update, AWS has confirmed that this type of half open partition is very much possible when using VPC/security groups as they do their own &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-network-security.html#security-group-connection-tracking&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;connection tracking&lt;/a&gt;. If the sg doesn&apos;t know about the flow because the &quot;timeout&quot; (which is very non-specific as to how long it is) occurs after close, the sg will blackhole packets. We&apos;re following up to see if they could have the more useful behavior of sending resets instead of dropping, but I imagine they&apos;ll answer that it&apos;s for security reasons.&lt;/p&gt;

&lt;p&gt;I&apos;m going to spend time seeing if the proposed workaround of setting &lt;tt&gt;/proc/sys/net/ipv4/tcp_retries2 = 5&#160;&lt;/tt&gt; fixes the issue satisfactorily, and if so I think that&apos;s a reasonable workaround for pre-netty, and post-netty we can use the reproduction steps above to see if Cassandra is now resilient to this kind of half-open partition. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aweisberg&quot; class=&quot;user-hover&quot; rel=&quot;aweisberg&quot;&gt;aweisberg&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jasobrown&quot; class=&quot;user-hover&quot; rel=&quot;jasobrown&quot;&gt;jasobrown&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=djoshi3&quot; class=&quot;user-hover&quot; rel=&quot;djoshi3&quot;&gt;djoshi3&lt;/a&gt; what do you guys think?&lt;/p&gt;</comment>
                            <comment id="16431480" author="jolynch" created="Mon, 9 Apr 2018 23:15:26 +0000"  >&lt;p&gt;Good news, Linux has support for a &lt;a href=&quot;https://patchwork.ozlabs.org/patch/62889/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;socket option&lt;/a&gt; since 2.6.37 called &lt;tt&gt;TCP_USER_TIMEOUT&lt;/tt&gt; which implements &lt;a href=&quot;https://tools.ietf.org/html/rfc5482&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;RFC5482&lt;/a&gt; and does exactly what we want (ensures HA on a kept alive TCP connection). Netty also supports &lt;a href=&quot;https://github.com/netty/netty/issues/4174&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;setting this option&lt;/a&gt;&#160;for epoll sockets as of 4.0.31, so we can set that on our internode TCP connections and get really high availability networking without requiring OS tuning on Linux. The &lt;tt&gt;man tcp&lt;/tt&gt; entry is educational:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;       TCP_USER_TIMEOUT (since Linux 2.6.37)
              This  option takes an unsigned int as an argument.  When the value is greater than 0, it specifies the maximum amount of time in milliseconds that trans&#8208;
              mitted data may remain unacknowledged before TCP will forcibly close the corresponding connection and return ETIMEDOUT to the application.  If the option
              value is specified as 0, TCP will to use the system default.

              Increasing  user  timeouts allows a TCP connection to survive extended periods without end-to-end connectivity.  Decreasing user timeouts allows applica&#8208;
              tions to &quot;fail fast&quot;, if so desired.  Otherwise, failure may take up to 20 minutes with the current system defaults in a normal WAN environment.

              This option can be set during any state of a TCP connection, but is effective only during the synchronized states  of  a  connection  (ESTABLISHED,  FIN-
              WAIT-1, FIN-WAIT-2, CLOSE-WAIT, CLOSING, and LAST-ACK).  Moreover, when used with the TCP keepalive (SO_KEEPALIVE) option, TCP_USER_TIMEOUT will override
              keepalive to determine when to close a connection due to keepalive failure.

              The option has no effect on when TCP retransmits a packet, nor when a keepalive probe is sent.

              This option, like many others, will be inherited by the socket returned by accept(2), if it was set on the listening socket.

              Further details on the user timeout feature can be found in RFC 793 and RFC 5482 (&quot;TCP User Timeout Option&quot;).
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I&apos;ve started working on a &lt;a href=&quot;https://github.com/jolynch/cassandra/tree/CASSANDRA-14358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;trunk patch&lt;/a&gt; which will fix this on any Linux based deployment via the &lt;tt&gt;TCP_USER_TIMEOUT&lt;/tt&gt; socket option.&#160;&lt;/p&gt;

&lt;p&gt;I&apos;ve been testing with the reproduction steps I listed above using iptables and without my patch Cassandra waits until the OS kills the connection in ~15-20 minutes, and with my patch it kills it after ~10s.&lt;/p&gt;</comment>
                            <comment id="16431491" author="aweisberg" created="Mon, 9 Apr 2018 23:27:39 +0000"  >&lt;p&gt;Nice find! The numbers you picked seem pretty aggressive. I was expecting in the 30-60 second range before giving up. If the network conditions are unreliable but working you might drop connections and then not be able to recreate them quickly due to packet loss. Granted things are probably mostly unavailable anyways in that scenario, but you don&apos;t want them looping too tightly trying to recreate connections.&lt;/p&gt;

&lt;p&gt;The last question is if this needs to be a hot property or not.&lt;/p&gt;</comment>
                            <comment id="16432971" author="jolynch" created="Tue, 10 Apr 2018 20:54:46 +0000"  >&lt;p&gt;Yea, I am interested as to what&apos;s the right default for this setting.&#160;For normal TCP connections I think it would be reasonable to put this very low (like close to the TCP connect timeout of 2s we use right now), but for SSL ... losing those SSL handshakes is somewhat of a bummer if it&apos;s just a temporary switch failure + OSPF convergence or something. What do you think about being conservative (maybe like 30s) for SSL and we&apos;ll make it a hot property in addition to the yaml configuration?&lt;/p&gt;

&lt;p&gt;I&apos;ve been testing this option on Linux 4.4 and 4.13 with&#160;&lt;a href=&quot;https://gist.github.com/jolynch/90033c2b10ab8280859c8cfe352503cd&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;a minimal repro&lt;/a&gt; and it appears that the option works well if set to a small number (e.g. 5, 10, 20s), but it seems to take about 2x as long for large settings and (I need to do further testing) on Linux 4.4 any setting greater than 30s appears to just default to the system behavior (on 4.13 it is 2x the timeout, but not the system default). So if we set it to 30s we&apos;d get a 60s timeout in most modern Linux&apos;s, and if it&apos;s not effective we&apos;ll just get the system default.&lt;/p&gt;</comment>
                            <comment id="16433171" author="aweisberg" created="Tue, 10 Apr 2018 23:20:24 +0000"  >&lt;p&gt;30 seconds (effectively 60) and a hot prop sounds excellent.&lt;/p&gt;</comment>
                            <comment id="16463328" author="alienth" created="Fri, 4 May 2018 03:34:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jolynch&quot; class=&quot;user-hover&quot; rel=&quot;jolynch&quot;&gt;jolynch&lt;/a&gt; One thing slightly interesting here that I found when reproducing. I can confirm that the traffic is retransmitting from the non-restarted node on the blackholed connection, yet the socket is in `CLOSE_WAIT` rather than `ESTABLISHED`. This would indicate that the non-restarted node got the FIN, but AWS blackholed the FIN,ACK. To me that suggests that the flow tracking was only lost in one direction. I saw this both times I was able to reproduce it, so I doubt it&apos;s a fluke of timing.&lt;/p&gt;</comment>
                            <comment id="16464368" author="alienth" created="Fri, 4 May 2018 20:40:00 +0000"  >&lt;p&gt;Reproduced this behaviour several times now,and in all cases the socket that cassandra is trying to Outbound on has been in stuck in `CLOSE_WAIT`.&lt;/p&gt;

&lt;p&gt;I think that suggests that the more common case might be where AWS stops tracking the flow for a FINd connection, and an ACK being dropped somewhere resulting in a node not fully closing the connection?&lt;/p&gt;

&lt;p&gt;Note that I&apos;m seeing this on cassandra 3.11.2, which has the patches from &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9630&quot; title=&quot;Killing cassandra process results in unclosed connections&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9630&quot;&gt;&lt;del&gt;CASSANDRA-9630&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;tcp    CLOSE-WAIT 1      322683 10.0.161.40:18254              10.0.109.39:7000                timer:(on,1min54sec,13) uid:115 ino:5837280 sk:1a1 --&amp;gt;&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="16464495" author="alienth" created="Fri, 4 May 2018 23:22:21 +0000"  >&lt;p&gt;Captured the socket state for before, during, and after the restart, from the POV of a node which the restarted node sees as down:&lt;/p&gt;

&lt;p&gt;Before, when nothing has been done:&lt;br/&gt;
 &lt;tt&gt;Netid State Recv-Q Send-Q Local Address:Port Peer Address:Port&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;tcp&#160; &#160;ESTAB 0&#160; &#160; &#160; 0&#160; &#160; &#160; 10.0.161.40:59739&#160; 10.0.107.88:7000 timer:(keepalive,4min59sec,0) uid:115 ino:5913893 sk:216e &amp;lt;-&amp;gt;&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;After 10.0.107.88 has been restarted. Note the &lt;tt&gt;1&lt;/tt&gt; in in the recv-q:&lt;br/&gt;
 &lt;tt&gt;tcp CLOSE-WAIT 1 0 10.0.161.40:59739 10.0.107.88:7000 timer:(keepalive,4min14sec,0) uid:115 ino:5913893 sk:216e --&amp;gt;&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;When 10.0.107.88 comes back up and 10.0.161.40 tries to respond to the EchoMessage, using the previous socket which has been in CLOSE-WAIT. You can see the outbounds piling up in the send-q:&lt;br/&gt;
 &lt;tt&gt;tcp CLOSE-WAIT 1 36527 10.0.161.40:59739 10.0.107.88:7000 timer:(on,1.932ms,4) uid:115 ino:5913893 sk:216e --&amp;gt;&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="16464566" author="jasobrown" created="Sat, 5 May 2018 02:03:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=alienth&quot; class=&quot;user-hover&quot; rel=&quot;alienth&quot;&gt;alienth&lt;/a&gt; nice detective work. it looks like &lt;tt&gt;10.0.161.40&lt;/tt&gt; hasn&apos;t closed the previous sockets/connections. If you have the logs of &lt;tt&gt;10.0.161.40&lt;/tt&gt; still handy, can you see if, during and after the bounce, there are log statements about  &lt;tt&gt;10.0.107.88&lt;/tt&gt; being down or alive (or restarted). Not looking at the code to see the exact log messages, but typically they are emitted by &lt;tt&gt;Gossiper&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Also, how long did it take  &lt;tt&gt;10.0.107.88&lt;/tt&gt; to bounce? If it went down and came back up fast enough, it&apos;s possible the failure detector on &lt;tt&gt;10.0.161.40&lt;/tt&gt; didn&apos;t mark it as down.&lt;/p&gt;

&lt;p&gt;How did you terminate &lt;tt&gt;10.0.107.88&lt;/tt&gt;? A normal shutdown, or &lt;tt&gt;kill -9&lt;/tt&gt;? &lt;/p&gt;</comment>
                            <comment id="16464596" author="alienth" created="Sat, 5 May 2018 03:25:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jasobrown&quot; class=&quot;user-hover&quot; rel=&quot;jasobrown&quot;&gt;jasobrown&lt;/a&gt; &lt;tt&gt;.88&lt;/tt&gt; was down for about 10 minutes, and was shutdown via `nodetool drain`, followed by stopping the service gracefully.&lt;/p&gt;


&lt;p&gt;&lt;tt&gt;.40&lt;/tt&gt; has no gossiper logs showing &lt;tt&gt;.88&lt;/tt&gt; going down, until the socket to &lt;tt&gt;.88&lt;/tt&gt; is killed about 20 minutes later. &lt;tt&gt;.88&lt;/tt&gt; was shut down at 1602 and came back up at 1612:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;RMI TCP Connection(22)-127.0.0.1&amp;#93;&lt;/span&gt; 2018-05-04 16:02:14,055 StorageService.java:1449 - DRAINING: starting drain process&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;RMI TCP Connection(22)-127.0.0.1&amp;#93;&lt;/span&gt; 2018-05-04 16:02:14,057 HintsService.java:220 - Paused hints dispatch&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;RMI TCP Connection(22)-127.0.0.1&amp;#93;&lt;/span&gt; 2018-05-04 16:02:14,058 ThriftServer.java:139 - Stop listening to thrift clients&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;RMI TCP Connection(22)-127.0.0.1&amp;#93;&lt;/span&gt; 2018-05-04 16:02:14,077 Server.java:176 - Stop listening for CQL clients&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;RMI TCP Connection(22)-127.0.0.1&amp;#93;&lt;/span&gt; 2018-05-04 16:02:14,078 Gossiper.java:1540 - Announcing shutdown&lt;/tt&gt;&lt;br/&gt;
&amp;lt;snip&amp;gt;&lt;br/&gt;
&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;main&amp;#93;&lt;/span&gt; 2018-05-04 16:12:39,746 StorageService.java:1449 - JOINING: Finish joining ring&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;And &lt;tt&gt;.40&lt;/tt&gt; has no logs of this happening, and seemingly doesn&apos;t see it as down until ~20 minutes later:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;GossipStage:1&amp;#93;&lt;/span&gt; 2018-05-01 16:20:36,680 Gossiper.java:1034 - InetAddress /10.0.107.88 is now DOWN&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;HANDSHAKE-/10.0.107.88&amp;#93;&lt;/span&gt; 2018-05-01 16:20:36,693 OutboundTcpConnection.java:560 - Handshaking version with /10.0.107.88&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;HANDSHAKE-/10.0.107.88&amp;#93;&lt;/span&gt; 2018-05-01 16:20:36,822 OutboundTcpConnection.java:560 - Handshaking version with /10.0.107.88&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;GossipStage:1&amp;#93;&lt;/span&gt; 2018-05-01 16:20:56,323 Gossiper.java:1053 - Node /10.0.107.88 has restarted, now UP&lt;/tt&gt;&lt;br/&gt;
&lt;tt&gt;INFO  &lt;span class=&quot;error&quot;&gt;&amp;#91;GossipStage:1&amp;#93;&lt;/span&gt; 2018-05-01 16:20:56,325 StorageService.java:2292 - Node /10.0.107.88 state jump to NORMAL&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;That is.. bizarre.&lt;/p&gt;</comment>
                            <comment id="16464598" author="alienth" created="Sat, 5 May 2018 03:28:42 +0000"  >&lt;p&gt;One extra thing to note, as expected turning down `tcp_retries2` does greatly alleviate this issue. The `CLOSE-WAIT` socket is nuked by the kernel much more quickly, resulting in the partition resolving quickly.&lt;/p&gt;</comment>
                            <comment id="16464604" author="alienth" created="Sat, 5 May 2018 03:34:44 +0000"  >&lt;p&gt;Extra note: At the same second that &lt;tt&gt;.88&lt;/tt&gt;&apos;s gossiper announced shutdown, there was a newly established socket&#160;from&#160;&lt;tt&gt;.40&lt;/tt&gt; to &lt;tt&gt;.88&lt;/tt&gt;:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;Fri May 4 16:02:15 PDT 2018&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;tcp ESTAB 0 0 10.0.161.40:59739 10.0.107.88:7000 timer:(keepalive,4min59sec,0) uid:115 ino:5913893 sk:216e &amp;lt;-&amp;gt;&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;This is the same socket that would become CLOSE-WAIT only seconds later, and remain that way for the following 18 minutes:&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;Fri May 4 16:02:20 PDT 2018&lt;/tt&gt;&lt;br/&gt;
 &lt;tt&gt;tcp CLOSE-WAIT 1 0 10.0.161.40:59739 10.0.107.88:7000 timer:(keepalive,4min54sec,0) uid:115 ino:5913893 sk:216e --&amp;gt;&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="16464622" author="alienth" created="Sat, 5 May 2018 04:39:50 +0000"  >&lt;p&gt;Why on earth didn&apos;t the failure detector on &lt;tt&gt;.40&lt;/tt&gt; see anything for 20 minutes? Possibly due to blocking on an Outbound forever?&lt;/p&gt;</comment>
                            <comment id="16482919" author="jolynch" created="Mon, 21 May 2018 19:06:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=alienth&quot; class=&quot;user-hover&quot; rel=&quot;alienth&quot;&gt;alienth&lt;/a&gt; that is interesting and thank you for digging so deeply! If I understand correctly during a &lt;tt&gt;drain&lt;/tt&gt; the other servers are responsible for noticing the change and closing their connections within the &lt;tt&gt;&lt;a href=&quot;https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/gms/Gossiper.java#L1497&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;shutdown_announce_in_ms&lt;/a&gt;&lt;/tt&gt; period in &lt;a href=&quot;https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/gms/GossipShutdownVerbHandler.java#L37&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;response&lt;/a&gt; to the &lt;tt&gt;GOSSIP_SHUTDOWN&lt;/tt&gt; gossip state, and then the &lt;tt&gt;&lt;a href=&quot;https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/gms/Gossiper.java#L363-L373&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;markAsShutdown&lt;/a&gt;&lt;/tt&gt; method marks it down and forcibly convicts it. I believe that the TCP connections get closed via the &lt;tt&gt;StorageService&lt;/tt&gt;&apos;s &lt;tt&gt;onDead&lt;/tt&gt; method which calls &lt;tt&gt;&lt;a href=&quot;https://github.com/apache/cassandra/blob/cassandra-3.0/src/java/org/apache/cassandra/service/StorageService.java#L2514&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;onDead&lt;/a&gt;&lt;/tt&gt; which calls &lt;tt&gt;&lt;a href=&quot;https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/MessagingService.java#L505&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;MessagingService::reset&lt;/a&gt;&lt;/tt&gt; which calls &lt;tt&gt;&lt;a href=&quot;https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/OutboundTcpConnectionPool.java#L80&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;OutboundTcpConnection::closeSocket&lt;/a&gt;, which &lt;a href=&quot;https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L210&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;enqueues a sentinel&lt;/a&gt;&lt;/tt&gt; into the backlog and then the &lt;tt&gt;&lt;a href=&quot;https://github.com/apache/cassandra/blob/06b3521acdb21dd3d85902d59146b9d08ad7d752/src/java/org/apache/cassandra/net/OutboundTcpConnection.java#L253&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;OutboundTcpConnection::run&lt;/a&gt;&lt;/tt&gt; method is actually supposed to close it. The &lt;tt&gt;drainedMessages&lt;/tt&gt; queue is a local reference though so backlog could get something&#160;that was&#160;enqueued before the &lt;tt&gt;CLOSE_SENTINEL&lt;/tt&gt; and after it as well.&#160;This seems very racey to me, in particular the reconnection logic might race with the closing logic from what I can tell as we have a 2 second window between when the clients start closing and when the server will actually stop accepting new connections (because it closes&#160;the listeners).&lt;/p&gt;

&lt;p&gt;Non stateful&#160;networks would surface the RST in the &lt;tt&gt;writeConnected&lt;/tt&gt; method, but AWS is like &quot;yea that machine isn&apos;t allowed to talk to that one&quot; and just blackholes the RSTs... I wonder if I can reproduce this by increasing that window significantly and just sending lots of traffic.&lt;/p&gt;</comment>
                            <comment id="16506404" author="khuizhang" created="Fri, 8 Jun 2018 18:18:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jolynch&quot; class=&quot;user-hover&quot; rel=&quot;jolynch&quot;&gt;jolynch&lt;/a&gt; currently we have an issue with gossip going one way after a node had sudden loss of storage controller. After the offending node comes back online, all the rest nodes show TCP connections on gossip, but those connections&#160; (in bold) are not seen on the offending node. On offending node, nodetool gossipinfo shows generation 0 for all other nodes and nodetool status show DN for all other nodes. On other nodes, nodetool gossipinfo seems fine but nodetool status shows offending node down. This can be resolved by restarting cassandra on all nodes except the offending node, or wait for 2 hours after crash event (tcp_keepalive is set to 7200s on debian?). I don&apos;t know if this is related, but wondering if there is any way to verify (like TRACE logging on&#160;org.apache.cassandra.gms and/or&#160;org.apache.cassandra.net, or maybe packet capture). So far we can reproduce it at 30% chance.&#160; Thanks in advance.&#160;&lt;/p&gt;

&lt;p&gt;node 10.96.105.4&lt;br/&gt;
&lt;b&gt;tcp 0 0 10.96.105.4:7001 10.96.105.6:55629 ESTABLISHED keepalive (729.79/0/0)&lt;/b&gt;&lt;br/&gt;
&lt;b&gt;tcp 0 0 10.96.105.4:39219 10.96.105.6:7001 ESTABLISHED keepalive (783.04/0/0)&lt;/b&gt;&lt;br/&gt;
&lt;b&gt;tcp 0 0 10.96.105.4:7001 10.96.105.6:60007 ESTABLISHED keepalive (729.79/0/0)&lt;/b&gt;&lt;br/&gt;
tcp 0 0 10.96.105.4:7001 10.96.105.6:45318 ESTABLISHED keepalive (1471.16/0/0)&lt;br/&gt;
node 10.96.105.6&lt;br/&gt;
tcp 0 0 10.96.105.6:7001 0.0.0.0:* LISTEN off (0.00/0/0)&lt;br/&gt;
tcp 0 0 10.96.105.6:45318 10.96.105.4:7001 ESTABLISHED keepalive (1477.00/0/0)&lt;/p&gt;</comment>
                            <comment id="16589562" author="jolynch" created="Thu, 23 Aug 2018 01:42:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=khuizhang&quot; class=&quot;user-hover&quot; rel=&quot;khuizhang&quot;&gt;khuizhang&lt;/a&gt; yea that looks similar with the half open keepalive connections. Did you try the kernel workaround and did it help?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aweisberg&quot; class=&quot;user-hover&quot; rel=&quot;aweisberg&quot;&gt;aweisberg&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jasobrown&quot; class=&quot;user-hover&quot; rel=&quot;jasobrown&quot;&gt;jasobrown&lt;/a&gt; I&apos;ve got a mitigation patch so that Cassandra trunk at least heals the half open partitions faster. Please take a look if you&#160;have bandwidth for&#160;review. While testing the re-connection behavior I ran into&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-14503&quot; title=&quot;Internode connection management is race-prone&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-14503&quot;&gt;&lt;del&gt;CASSANDRA-14503&lt;/del&gt;&lt;/a&gt; because the retry future was just getting clobbered by another message, so I couldn&apos;t test that we don&apos;t keep retrying after just connections are killed (as right now they just retry every message).&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;trunk&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;patch&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://circleci.com/gh/jolynch/workflows/cassandra/tree/CASSANDRA-14358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;unit tests&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;This patch makes the timeout configurable for internode connection (2s) and internode tcp user timeout (30s). The timeouts are settable via JMX (and nodetool).&lt;/p&gt;

&lt;p&gt;I&apos;m marking this as patch available as I think the operating system workaround is&#160;probably ok for previous releases? If it&apos;s not just let me know and I can try to figure out how to fix it for those ones as well.&lt;/p&gt;

&lt;p&gt;I didn&apos;t have any new tests because the only way I&apos;m aware to reproduce this behavior is by using iptables to blackhole traffic. I&apos;ve been testing with a ccm cluster&#160;by trying to block just the small message channel (if gossip is blocked then the failure detector convicts it) and doing a read at ALL:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;$ netstat -on | grep 7000 | grep 127.0.0.3:7000 | grep -v &quot;0 127.0.0.3&quot;
tcp        0      0 127.0.0.1:55604         127.0.0.3:7000          ESTABLISHED keepalive (4093.37/0/0)
tcp        0      0 127.0.0.1:55610         127.0.0.3:7000          ESTABLISHED keepalive (4093.53/0/0)
tcp        0      0 127.0.0.1:58080         127.0.0.3:7000          ESTABLISHED keepalive (6601.96/0/0)

# Try to drop just the small message channel
$ sudo iptables -A OUTPUT -p tcp -d 127.0.0.3 --dport 7000 --sport 58080 -j DROP
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Then I just check that we properly reconnect faster than 15 minutes. On an unpatched trunk I can watch the small message channel just sit there probing for 15 retries (the default tcp_retries2 value, node that this netstat is a different run than the previous one) :&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;netstat -on | grep 7000 | grep 127.0.0.3:7000 | grep -v &quot;0 127.0.0.3&quot; &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;
tcp &#160;&#160;&#160;&#160;&#160;&#160;&#160;0 &#160;&#160;&#160;&#160;&#160;0 127.0.0.1:34532 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;127.0.0.3:7000 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ESTABLISHED keepalive (6823.99/0/0) 
tcp &#160;&#160;&#160;&#160;&#160;&#160;&#160;0 &#160;&#160;&#160;&#160;&#160;0 127.0.0.1:34564 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;127.0.0.3:7000 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ESTABLISHED keepalive (6840.36/0/0) 
tcp &#160;&#160;&#160;&#160;&#160;&#160;&#160;0 &#160;&#160;&#160;808 127.0.0.1:34544 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;127.0.0.3:7000 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ESTABLISHED probe (108.32/0/10) 
tcp &#160;&#160;&#160;&#160;&#160;&#160;&#160;0 &#160;&#160;&#160;&#160;&#160;0 127.0.0.1:34554 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;127.0.0.3:7000 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ESTABLISHED keepalive (6840.26/0/0) 
tcp &#160;&#160;&#160;&#160;&#160;&#160;&#160;0 &#160;&#160;&#160;&#160;&#160;0 127.0.0.1:34540 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;127.0.0.3:7000 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ESTABLISHED keepalive (6831.27/0/0) 
tcp &#160;&#160;&#160;&#160;&#160;&#160;&#160;0 &#160;&#160;&#160;&#160;&#160;0 127.0.0.1:34534 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;127.0.0.3:7000 &#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;ESTABLISHED keepalive (6828.96/0/0)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And then with my patch it get&apos;s killed&#160;after like 5 retries instead of waiting the full 15.&lt;/p&gt;

&lt;p&gt;If you have ideas for how to&#160;unit test it I&apos;m open to&#160;suggestions of course.&lt;/p&gt;</comment>
                            <comment id="16630690" author="khuizhang" created="Thu, 27 Sep 2018 16:23:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jolynch&quot; class=&quot;user-hover&quot; rel=&quot;jolynch&quot;&gt;jolynch&lt;/a&gt;: thanks for the answer. I did the kernel workaround and it worked.&#160;&lt;/p&gt;</comment>
                            <comment id="16647004" author="aweisberg" created="Thu, 11 Oct 2018 20:34:31 +0000"  >&lt;p&gt;One argument against this approach is that this is Linux specific and maybe we should go with a solution that works on other platforms (timers and external threads timing stuff out), but to an extent that is the tail wagging the dog, and I like how simple this is.&lt;/p&gt;

&lt;p&gt;For testing for dead connections one good reason to heartbeat at the application level is that it can detect bugs at the application level that prevent the networking code from processing messages. This won&apos;t catch a connection being wedged like that. And once you have gone to the trouble of heartbeating having the kernel check is superfluous.&lt;/p&gt;

&lt;p&gt;I think a heartbeat mechanism is a bit much for 4.0 during the code freeze and this is a small enough change I would rather back it out later (or leave it, it&apos;s harmless) if we add heartbeating down the road.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;#diff-f5e4f8d3a95c98844b371ba1d1e98285R38&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Move these defaults to Config &lt;/a&gt; and use them there so they can&apos;t mismatch.&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-b66584c9ce7b64019b5db5a531deeda1R147&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Can you add these config options to the yaml with these comments, but commented out?&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;It would be nice if the unit of the timeout were in the name in more places. Basically add _ms everywhere you can. I would actually leave nodetool the way it is just to satisfy my OCD since units are not in the name of the other options and we don&apos;t want to change them. Although adding synonyms might be nice.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;My only concern is whether we should ratchet TCP_USER_TIMEOUT timeouts down as a new default. Not because I have a concrete issue with it, but because I lack the operational experience say yeah this is a good idea in practice.&lt;/p&gt;

&lt;p&gt;I really want to give people a good default and not cop out and leave it at 10 minutes where it was just because that is how it was. I brought it up in IRC and maybe some people will chime in.&lt;/p&gt;</comment>
                            <comment id="16647022" author="aweisberg" created="Thu, 11 Oct 2018 20:50:49 +0000"  >&lt;p&gt;Talked about it in IRC and people think I am overthinking.&lt;/p&gt;

&lt;p&gt;I think 30 seconds should be fine. Each server would only see a few hundred incoming connections at once. A very nice benchmark number to have for C* would be the SSL accept rate. Apparently SSL accept is single threaded right now. One thing to watch out for when measuring that kind of thing is sockets being stuck in TIMED_WAIT causing you to run out of ephemeral ports. Setting &quot;TCP_TW_REUSE&quot; fixes that issue for benchmarks.&lt;/p&gt;

&lt;p&gt;I don&apos;t think you need to do that for this issue though.&lt;/p&gt;</comment>
                            <comment id="16663028" author="jolynch" created="Thu, 25 Oct 2018 00:10:03 +0000"  >&lt;p&gt;Great! I pushed a patch incorporating your feedback and am running dtests against it. I am slightly concerned that &lt;tt&gt;OutboundConnectionParams&lt;/tt&gt; now requires the &lt;tt&gt;DatabaseDescriptor&lt;/tt&gt; to be loaded, but if tests pass I think we&apos;re ok?&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;trunk&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;patch&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;dtests:&lt;a href=&quot;https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;I agree that active application level monitoring is a great idea, but to be honest I think this plus the latency probes we added as part of &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-14459&quot; title=&quot;DynamicEndpointSnitch should never prefer latent nodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-14459&quot;&gt;CASSANDRA-14459&lt;/a&gt; should do a pretty good job of detecting and evicting bad connections. There is of course room for improvement but I think it&apos;s actually a pretty good start.&lt;/p&gt;</comment>
                            <comment id="16667851" author="aweisberg" created="Mon, 29 Oct 2018 22:44:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-1560ed3bf5675f8ec0b1b35198debe15R41&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Why add protocol version here?&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-f5e4f8d3a95c98844b371ba1d1e98285R224&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;The acceptable range of values differs, but the tests are for both are -1&lt;/a&gt;. I just want to confirm what the underlying tunables actually support.&lt;/p&gt;</comment>
                            <comment id="16667881" author="jolynch" created="Mon, 29 Oct 2018 23:35:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-1560ed3bf5675f8ec0b1b35198debe15R41&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Why add protocol version here?&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Because otherwise the test fails the &lt;tt&gt;protocolVersion&lt;/tt&gt; check instead of the &lt;tt&gt;sendBufferSize&lt;/tt&gt; check which the test was (I believe) trying to test. Before this patch I believe that the unit test was testing the wrong thing.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358#diff-f5e4f8d3a95c98844b371ba1d1e98285R224&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;The acceptable range of values differs, but the tests are for both are -1&lt;/a&gt;. I just want to confirm what the underlying tunables actually support.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Correct, the &lt;tt&gt;TCP_USER_TIMEOUT&lt;/tt&gt; can be set to zero (which will pick up the OS level setting).&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;       TCP_USER_TIMEOUT (since Linux 2.6.37)
              This option takes an unsigned int as an argument.  When the value is greater than 0, it specifies the maximum amount of time in milliseconds that transmitted data may remain unacknowledged before TCP will forcibly close the corresponding connection and return ETIMEDOUT to the applica&#8208;
              tion.  If the option value is specified as 0, TCP will to use the system default.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The connection timeout setting also &lt;a href=&quot;https://netty.io/4.0/api/io/netty/channel/ChannelConfig.html#setConnectTimeoutMillis-int-&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;supports zero&lt;/a&gt; (meaning disable) but I don&apos;t (imo) think that users should ever disable connection timeouts. A user could plausibly set it to the same value as their RPC timeout, or even higher, but I don&apos;t think turning it off ever makes sense.&lt;/p&gt;</comment>
                            <comment id="16671116" author="jolynch" created="Thu, 1 Nov 2018 04:34:57 +0000"  >&lt;p&gt;I have a branch over on &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-14862&quot; title=&quot;Fix incorrect sorting of replicas in SimpleStrategy.calculateNaturalReplicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-14862&quot;&gt;&lt;del&gt;CASSANDRA-14862&lt;/del&gt;&lt;/a&gt; which fixes the broken dtests. Is there any more testing or changes you want me to do on this patch?&lt;/p&gt;</comment>
                            <comment id="16671709" author="aweisberg" created="Thu, 1 Nov 2018 14:58:12 +0000"  >&lt;p&gt;Can you update CHANGES.txt and NEW.txt and then I will commit?&lt;/p&gt;

&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="16671999" author="jolynch" created="Thu, 1 Nov 2018 18:50:03 +0000"  >&lt;p&gt;I&apos;ve added the CHANGES and NEWS updates, rebased and squashed down the changes into&#160;&lt;a href=&quot;https://github.com/apache/cassandra/commit/cb82946b48100b06a342a02093dd3bb2c489e25b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;cb82946&lt;/a&gt;. Tests are re-running on my branch.&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;trunk&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...jolynch:CASSANDRA-14358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;patch&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;dtests:&lt;a href=&quot;https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://circleci.com/gh/jolynch/cassandra/tree/CASSANDRA-14358.png?circle-token= 1102a59698d04899ec971dd36e925928f7b521f5&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
</comment>
                            <comment id="16672014" author="aweisberg" created="Thu, 1 Nov 2018 18:58:03 +0000"  >&lt;p&gt;Hey, so feedback on NEWS.txt, linking to the ticket is helpful, but saying there are two new tunables in the YAML is important.&lt;/p&gt;

&lt;p&gt;For CHANGES.txt I think we always use the title of the JIRA. On commit I frequently change the title of the JIRA to make it more useful in CHANGES.txt. People should know either what changed, or what problem was fixed, or both.&lt;/p&gt;</comment>
                            <comment id="16672160" author="jolynch" created="Thu, 1 Nov 2018 20:36:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aweisberg&quot; class=&quot;user-hover&quot; rel=&quot;aweisberg&quot;&gt;aweisberg&lt;/a&gt; Ok I think I&apos;ve re-factored the jira ticket name and CHANGES entry appropriately (since we&apos;re not fixing the underlying problem in this ticket, just mitigating). I&apos;ve also updated the NEWS entry to list the two tunables.&lt;/p&gt;

&lt;p&gt;Rebased patch at &lt;a href=&quot;https://github.com/apache/cassandra/commit/ac83f0def640ab89d0a1c911e82d867f6588de4c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;ac83f0de&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="16672162" author="aweisberg" created="Thu, 1 Nov 2018 20:41:18 +0000"  >&lt;p&gt;OK, +1 I&apos;ll commit. Just a heads up you can write long beautiful commit messages, but they get replaced with a one line subject of the JIRA and a standard Patch by XYZ; Reviews by ZYX for CASSANDRA1234. We don&apos;t deviate from that format for commit messages.&lt;/p&gt;</comment>
                            <comment id="16672201" author="jolynch" created="Thu, 1 Nov 2018 21:05:57 +0000"  >&lt;p&gt;Ok, sounds reasonable &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="16672302" author="aweisberg" created="Thu, 1 Nov 2018 22:37:46 +0000"  >&lt;p&gt;Committed as &lt;a href=&quot;https://github.com/apache/cassandra/commit/bfbc5274f2b3a5af2cbbe9679f0e78f1066ef638&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;bfbc5274f2b3a5af2cbbe9679f0e78f1066ef638&lt;/a&gt;. Thanks!&lt;/p&gt;</comment>
                            <comment id="16673445" author="jolynch" created="Fri, 2 Nov 2018 17:20:41 +0000"  >&lt;p&gt;For those looking for a fix for earlier versions, unfortunately I was not able to determine the root cause of the close issues, but we have a follow up ticket &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-14818&quot; title=&quot;Cycling of outbound connections on node restart is suspect and racy&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-14818&quot;&gt;CASSANDRA-14818&lt;/a&gt; for trying to figure that out (if someone can figure that out I think it&apos;s worth fixing that).&lt;/p&gt;

&lt;p&gt;That being said these partitions are now mitigated in 4.0 with the TCP_USER_TIMEOUT socket option.&lt;/p&gt;

&lt;p&gt;A workaround for earlier versions is to set the &lt;tt&gt;net.ipv4.tcp_retries2&lt;/tt&gt;&#160;sysctl to ~5. This can be done with the following:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-bash&quot;&gt;
$ cat /etc/sysctl.d/20-cassandra-tuning.conf
net.ipv4.tcp_retries2=5
$ &lt;span class=&quot;code-comment&quot;&gt;# Reload all sysctls
&lt;/span&gt;$ sysctl --system&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16675048" author="benedict" created="Mon, 5 Nov 2018 12:15:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;We don&apos;t deviate from that format for commit messages&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is this&#160;true? &#160;I recall our standardising on the last line following the format &quot;Patch by X,Y,Z; reviewed by A,B,C for CASSANDRA-XXXXX&quot;, and the first line approximating the JIRA title. &#160;I don&apos;t recall any proposal to prohibit more information between those two lines?&lt;/p&gt;

&lt;p&gt;I certainly put more information in sometimes, and should probably do it more.  I&apos;d hate to explicitly discourage it.&lt;/p&gt;</comment>
                            <comment id="16677198" author="aweisberg" created="Tue, 6 Nov 2018 19:16:36 +0000"  >&lt;p&gt;I guess I misinterpreted the scolding I got long ago WRT to commit messages.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="13155836">CASSANDRA-14424</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13191068">CASSANDRA-14818</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13116853">CASSANDRA-14001</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12917074" name="10 Minute Partition.pdf" size="129855" author="jolynch" created="Fri, 30 Mar 2018 23:44:23 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[jolynch]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 2 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3s0cn:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311421" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Reproduced In</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12341240">2.1.19</customfieldvalue>
    <customfieldvalue id="12341001">3.0.15</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>aweisberg</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[aweisberg]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>