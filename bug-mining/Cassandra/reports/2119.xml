<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:37:59 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-5371] Perform size-tiered compactions in L0 (&quot;hybrid compaction&quot;)</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-5371</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;If LCS gets behind, read performance deteriorates as we have to check bloom filters on man sstables in L0.  For wide rows, this can mean having to seek for each one since the BF doesn&apos;t help us reject much.&lt;/p&gt;

&lt;p&gt;Performing size-tiered compaction in L0 will mitigate this until we can catch up on merging it into higher levels.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12638256">CASSANDRA-5371</key>
            <summary>Perform size-tiered compactions in L0 (&quot;hybrid compaction&quot;)</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="jbellis">Jonathan Ellis</assignee>
                                    <reporter username="jbellis">Jonathan Ellis</reporter>
                        <labels>
                            <label>lcs</label>
                    </labels>
                <created>Thu, 21 Mar 2013 15:22:30 +0000</created>
                <updated>Tue, 14 Oct 2025 12:13:55 +0000</updated>
                            <resolved>Mon, 8 Apr 2013 18:24:16 +0000</resolved>
                                        <fixVersion>2.0 beta 1</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>15</watches>
                                                                                                                <comments>
                            <comment id="13609358" author="JIRAUSER308715" created="Thu, 21 Mar 2013 19:32:33 +0000"  >&lt;p&gt;I could see this possibly helping if you left the non L0 alone and went to STCS for L0 until the write load stopped. But you have to come up with a heuristic to know when to shut off LCS.  So a cluster with a periodic write load, which was too high for LCS&apos;s increased IO needs, would revert to STCS of L0 only until the load dropped.  You would then have to play catchup shoving all that L0 data into the other levels. I could see use cases where this would be useful, such as periodic large data dumps into a cluster. You would have to be careful that there was enough down time between dumps for LCS to catchup.&lt;/p&gt;</comment>
                            <comment id="13609455" author="tjake" created="Thu, 21 Mar 2013 20:52:32 +0000"  >&lt;p&gt;This initial version puts the newly flushed memtables into a queue and when there are 4 it size tiers them.  So you get 1/4 the sstables in L0.&lt;/p&gt;</comment>
                            <comment id="13621414" author="jbellis" created="Wed, 3 Apr 2013 22:17:59 +0000"  >&lt;p&gt;Alternate implementation pushed to &lt;a href=&quot;http://github.com/jbellis/cassandra/commits/5371&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://github.com/jbellis/cassandra/commits/5371&lt;/a&gt; with the following improvements:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Only applies STCS to L0 if L0 gets behind (defined as &quot;accumulates more than MAX_COMPACTING_L0 sstables&quot;)&lt;/li&gt;
	&lt;li&gt;Performs true STCS, rather than &quot;compact in sets of four and then never again&quot;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13621653" author="tjake" created="Thu, 4 Apr 2013 02:02:07 +0000"  >&lt;p&gt;Oh good, this is what I wanted the implementation to end up being.&lt;/p&gt;

&lt;p&gt;In LeveledManifest.getCompactionCandidates:&lt;/p&gt;

&lt;p&gt;I think there is a bug in the size tier candidate checks.  You seem to be size tiering across all the non-compacting sstables and not the level0 ones.  I think you mean&apos;t to intersect the level0 sstables with the non-compacting ones.  You should also add a check after that to make sure the non-compacting level0 sstables are still &amp;gt; MAX_COMPACTING_L0&lt;/p&gt;

&lt;p&gt;Also, the code only checks for STCS when a higher level is ready to be compacted.  Maybe move this to the top before the higher level checks. We know the higher levels are seek bounded but the code should try to keep up with level 0 flushes as much as possible.&lt;/p&gt;</comment>
                            <comment id="13621679" author="jbellis" created="Thu, 4 Apr 2013 02:33:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;I think you mean&apos;t to intersect the level0 sstables with the non-compacting ones&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.  Fix pushed.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;the code only checks for STCS when a higher level is ready to be compacted&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The idea is, we&apos;d prefer to do normal LCS compaction on L0.  So if the higher levels are okay, we&apos;ll treat L0 the same as before.  But if we do need to compact a higher level, we&apos;ll first check and see if L0 is far enough behind that we should do an STCS round there as a stop-gap.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;You should also add a check after that to make sure the non-compacting level0 sstables are still &amp;gt; MAX_COMPACTING_L0&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think it&apos;s more correct as written &amp;#8211; basically, we&apos;re doing L0 out-of-turn, since for max throughput we&apos;d do the higher level next.  So, we&apos;ll do L0 STCS until it&apos;s under MCL0, then we&apos;ll go back to the higher levels until we catch up and can actually apply leveling to L0.&lt;/p&gt;</comment>
                            <comment id="13622038" author="slebresne" created="Thu, 4 Apr 2013 11:07:31 +0000"  >&lt;p&gt;For my own curiosity, do we have performance numbers for this (including, not only on SSD tests)?&lt;/p&gt;

&lt;p&gt;A priori, I&apos;m not fully sold on this being always a win (or even most of the time of &quot;L0 is being&quot;). That is, I understand the reasoning that lots of SSTables in L0 is bad for reads, but at the same time, if you compact STCS things in L0, a lot of the work you&apos;ve done you will redo when you compact your now bigger L0 sstable against L1. I.e. those STCS compactions don&apos;t help you make progress as far as leveling is concerned, so it seems like it waste work overall. Besides, in theory, our LCS is supposed to be able to compact large amount of L0 sstables into L1 to help with the &quot;I&apos;m behind on L0 but it&apos;s just a pike in load&quot;. Now I guess if you&apos;ve pushed a lot of data in L1 and get behind again in L0, then it&apos;s not fun because all of L1 need to be including in L0 compaction. But if you are constantly behind, doesn&apos;t that mean you have bigger problems (and/or that you should just use STSC)?&lt;/p&gt;

&lt;p&gt;Basically I wonder if there won&apos;t be a number of scenario where because you get a bit behind on L0 once, then the I/O you &quot;waste&quot; doing STSC in L0 will help you get even more and more behind on your leveling and you&apos;d end up doing mostly STSC, while letting LCS do its job would have been fine overall.&lt;/p&gt;

&lt;p&gt;That is, I&apos;m happy with this if that makes things clearly better in practice more often than not, it&apos;s just that intellectually it&apos;s not obvious to me that it&apos;s the case (note that I&apos;m not saying that it&apos;s obvious it&apos;s a bad idea either).&lt;/p&gt;
</comment>
                            <comment id="13622068" author="tjake" created="Thu, 4 Apr 2013 12:02:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbellis&quot; class=&quot;user-hover&quot; rel=&quot;jbellis&quot;&gt;jbellis&lt;/a&gt; let me run some tests but the code looks good now.&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=slebresne&quot; class=&quot;user-hover&quot; rel=&quot;slebresne&quot;&gt;slebresne&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;do we have performance numbers for this (including, not only on SSD tests)?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We only have SSD and I think LCS only ever makes sense on SSD.  If we want to support HDD then I agree this is def more IO overall.  The performance numbers for our use case went from all read timeouts using LCS to reads rarely timing out with the original patch.  The stress tool doesn&apos;t have a wide row scenario so it&apos;s hard to simulate out of the box.  &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;doesn&apos;t that mean you have bigger problems (and/or that you should just use STSC)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You are right, this does require writes die down at somepoint otherwise you end up with STCS. ellis mentions this in the comments.&lt;/p&gt;

&lt;p&gt;STCS isn&apos;t viable for the LCS use cases.  I don&apos;t see how having this (on SSD) would not help all LCS use cases since LCS is for wide row or heavy updates. The point of this is to avoid the situation where all sstables in L0 contain a portion of the row which requires reading them all.   One thing to keep in mind is if you do have a wide row and you end up with a STCS compacted row of 10MB and LCS has a 5MB limit you still end up with a 10MB sstable with a single row in it so the higher levels do benefit from STCS in this case.&lt;/p&gt;</comment>
                            <comment id="13622109" author="slebresne" created="Thu, 4 Apr 2013 12:51:26 +0000"  >&lt;blockquote&gt;&lt;p&gt;The stress tool doesn&apos;t have a wide row scenario so it&apos;s hard to simulate out of the box&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed, and that&apos;s definitively lacking. I believe there is a few knobs that allow to do wideish rows, but that&apos;s probably not very realistic.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;I think LCS only ever makes sense on SSD&lt;br/&gt;
since LCS is for wide row or heavy updates&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure I agree. Maybe there is some truth to it in our current implementation, but that would then be more of a quirk of the implementation that the goal. Typically, I&apos;m not really sure why only wide rows would benefit it. There is certainly nothing in theory that makes it so. As for &quot;it&apos;s for heavy updates only&quot;, I think that LCS has a number of nice properties (like avoiding huge files that require half of you disk in free space) that are nice even if you have a moderate to low update rate (and in that case you can definitively afford LCS on HDD). More concretely, I&apos;m pretty sure we have tons on users on LCS on HDD.&lt;/p&gt;

&lt;p&gt;Anyway, all this to say that I don&apos;t necessary agree on optimizing LCS for heavy writes + wide rows + SSD &lt;b&gt;if&lt;/b&gt; that&apos;s done at the expense of all other type of workload (and I&apos;m not saying that&apos;s what this patch is doing, just that discarding other type of workload as unimportant is not ok imo).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;LCS has a 5MB limit you still end up with a 10MB sstable with a single row&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If having 10MB sstables being split due to row too wide is a problem, then you should either not use LCS or pick a 10MB limit for LCS, not 5MB.&lt;/p&gt;

&lt;p&gt;Anyway, I&apos;m not vetoing this or anything like this. Just trying to get a better understanding of why this is a good thing to do in general.&lt;/p&gt;</comment>
                            <comment id="13622118" author="jbellis" created="Thu, 4 Apr 2013 13:05:00 +0000"  >&lt;blockquote&gt;&lt;p&gt;Basically I wonder if there won&apos;t be a number of scenario where because you get a bit behind on L0 once, then the I/O you &quot;waste&quot; doing STSC in L0 will help you get even more and more behind on your leveling&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That&apos;s exact;y the case, which is why we only apply STCS to L0 when it&apos;s fairly badly behind, i.e., we can conclude two things:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;if the current workload continues, it&apos;s not going to magically catch up any time soon&lt;/li&gt;
	&lt;li&gt;reads are starting to get into trouble&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Note that #2 will cause a vicious cycle, slowing down compaction in turn.&lt;/p&gt;

&lt;p&gt;So while I can hypothesize workloads that burst just long enough to cause STCS to kick in before stopping, thus &quot;wasting&quot; iops, I think for the vast majority this is a good &quot;safety valve,&quot; and specifically not worth adding a config option to disable.&lt;/p&gt;

&lt;p&gt;However, I do think it&apos;s worth creating a ticket to allow STCS config options to be applied to the size-tiering done by LCS, and specifically allow configuring MAX_COMPACTION_L0 via the max sstables threshold, which I think may adequately address your concern.&lt;/p&gt;</comment>
                            <comment id="13625443" author="tjake" created="Mon, 8 Apr 2013 15:05:35 +0000"  >&lt;p&gt;I&apos;m going to test this out to show how it helps our workload.  &lt;/p&gt;

&lt;p&gt;In the meantime I think this is fine to commit for 2.0 if you&apos;d like to get it in now.&lt;/p&gt;</comment>
                            <comment id="13625634" author="jbellis" created="Mon, 8 Apr 2013 18:24:16 +0000"  >&lt;p&gt;All right.  Rebased and committed, and created &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-5439&quot; title=&quot;allow STCS options to apply to the L0 compaction performed by LCS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-5439&quot;&gt;&lt;del&gt;CASSANDRA-5439&lt;/del&gt;&lt;/a&gt; for the options application.&lt;/p&gt;</comment>
                            <comment id="13648500" author="rbranson" created="Fri, 3 May 2013 15:36:45 +0000"  >&lt;p&gt;Is this just waiting on &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt;&apos;s test to backport to 1.2? &lt;/p&gt;

&lt;p&gt;Yesterday we bootstrapped our first new node on our first LCS cluster where each node only had ~50GB of data, and it took 6 hours to complete the bootstrap, even after running the CPUs hot by bumping compaction throughput up to 64MB. We probably could have stood to raise this to 128MB/sec and pegged them, but I dread to think of what this would be like if we moved some larger, read-heavy data sets to Cassandra under LCS. Jake seems to think this patch will help with that.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://i.imgur.com/LpdAKyc.png&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://i.imgur.com/LpdAKyc.png&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://i.imgur.com/ZsgEB9G.png&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://i.imgur.com/ZsgEB9G.png&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This is on an EC2 hi1.4xlarge, which is a 16-core box w/60GB RAM, 2TB of SSD storage, and 10GigE. &lt;/p&gt;

&lt;p&gt;We also have a cluster of m1.xlarges (4-core, 15G, 2TB rust) each with ~300GB of relatively cold data under STCS. Considering the spinning rust cluster w/1GigE and 16MB/s compaction throughput can bootstrap a new node in &amp;lt; 2 hours with 6x as much data we will definitely be trying this HCS on the SSD cluster running LCS at the moment.&lt;/p&gt;</comment>
                            <comment id="13648508" author="jbellis" created="Fri, 3 May 2013 15:46:26 +0000"  >&lt;p&gt;I&apos;m not backporting this to a stable release.  It&apos;s a lot more involved than the 4KB proof of concept.  You can probably collaborate w/ Jake on a 1.2-appropriate alternative though.&lt;/p&gt;

&lt;p&gt;That said, the main benefit of this is not that it magically makes LCS faster (it doesn&apos;t), but that when it does get behind your reads don&apos;t suffer so much.&lt;/p&gt;</comment>
                            <comment id="13648520" author="tjake" created="Fri, 3 May 2013 15:58:48 +0000"  >&lt;p&gt;Right, it takes a long time but it will keep reads happier.&lt;/p&gt;

&lt;p&gt;Why do you need LCS on your dataset Rick? is it a wide row?&lt;/p&gt;</comment>
                            <comment id="13856888" author="br1985" created="Thu, 26 Dec 2013 14:21:15 +0000"  >&lt;p&gt;Hi,&lt;/p&gt;

&lt;p&gt;We hit the same bug in production recently. We walked around it by switching to STCS for a few days, letting it stabilize and then going back to LCS. Quite long, but fully successful trip.&lt;/p&gt;

&lt;p&gt;In our case we have a lot of sstables at L0 as a result of migration. Because of another bug in sstableloader (&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6527&quot; title=&quot;Random tombstones after adding a CF with sstableloader&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6527&quot;&gt;&lt;del&gt;CASSANDRA-6527&lt;/del&gt;&lt;/a&gt;), we finally ended up simply copying all sstable files from the old cluster to the new one.&lt;/p&gt;

&lt;p&gt;After the migration we had over 10k sstables (160MB per file) on each node. Of course, STCS-fallback activates automatically in that case.&lt;/p&gt;

&lt;p&gt;I wonder if similar situation will happen after the classic bootstrap? Will streaming during bootstrapping put sstables at L0 or at the original level?&lt;/p&gt;

&lt;p&gt;If it will put them all at L0 then I&apos;m not sure if falling back to STCS is the best way to handle the situation. I&apos;ve read the comment in the code and I&apos;m aware why it is a good thing to do if we have to many sstables at L0 as a result of too many random inserts. We have a lot of sstables, each of them covers the whole ring, there&apos;s simply no better option. &lt;/p&gt;

&lt;p&gt;However, after the bootstrap situation looks a bit different. The loaded sstables already have vary small ranges! We just have to tidy up a bit and everything should be OK. STCS ignores that completely and after a while we have a bit less sstables but each of them covers the whole ring instead of just a small part. I believe that in that case letting LCS do the job is a better option that allowing STCS mix everything up before.&lt;/p&gt;

&lt;p&gt;Is there a way to disable STCS fallback? I&apos;ll be glad to test this option the next time we do similar operation.&lt;/p&gt;</comment>
                            <comment id="13881634" author="ravilr" created="Sat, 25 Jan 2014 03:20:21 +0000"  >&lt;p&gt;+1 on &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=br1985&quot; class=&quot;user-hover&quot; rel=&quot;br1985&quot;&gt;br1985&lt;/a&gt; comment. &lt;br/&gt;
Even during dead node replace (using replace_address), streaming puts all sstables in L0. 2.0.x switches to STCS, in doing so, also creates larger sstables, which means more free disk space to be left, in order for them to be compacted later into higher levels. LCS is known to lower the amount of free disk space (headroom) needed for compaction. this is no more true with LCS in above scenarios.&lt;br/&gt;
Is there a way to disable STCS fallback, please?&lt;/p&gt;</comment>
                            <comment id="13881824" author="br1985" created="Sat, 25 Jan 2014 11:46:53 +0000"  >&lt;p&gt;Ravi, could you confirm that streaming puts all tables in L0? In that case I think we should open a separate issue instead of commenting on a closed one.&lt;/p&gt;</comment>
                            <comment id="13881947" author="brandon.williams" created="Sat, 25 Jan 2014 16:54:21 +0000"  >&lt;p&gt;They have to go to L0 since preserving the level across machines doesn&apos;t make any sense.  Please do open a new issue.&lt;/p&gt;</comment>
                            <comment id="13882336" author="br1985" created="Sun, 26 Jan 2014 16:24:33 +0000"  >&lt;p&gt;I&apos;ve just created &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6621&quot; title=&quot;Add flag to disable STCS in L0&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6621&quot;&gt;&lt;del&gt;CASSANDRA-6621&lt;/del&gt;&lt;/a&gt; describing the issue from the last comments.&lt;/p&gt;</comment>
                            <comment id="14721586" author="deag" created="Sun, 30 Aug 2015 16:23:25 +0000"  >&lt;p&gt;I&apos;m benchmarking our solution that uses LCS with C* 2.1.8 and we have the scenario here by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jjordan&quot; class=&quot;user-hover&quot; rel=&quot;jjordan&quot;&gt;jjordan&lt;/a&gt; every week we need to burst C* with a batch job that takes 18-20h. The system behaves very well dureing the whole week, but during that time degrades considerably on the highest percentiles (&amp;gt;p99) were we get reads rocketing to latencies &amp;gt; 200ms. &lt;/p&gt;

&lt;p&gt;I&apos;m working on understanding what is happening with the system since I don&apos;t see IO or CPU exhausted. Wonder what is the usual rate of compaction with LCS. In my system log I see that compaction tasks are only doing 2-3MB/s&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;INFO  [CompactionExecutor:93] 2015-08-30 17:06:22,114  CompactionTask.java:274 - Compacted 9 sstables to [/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20266,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20274,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20280,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20286,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20291,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20297,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20302,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20307,].  1,383,544,305 bytes to 1,335,157,959 (~96% of original) in 450,225ms = 2.828154MB/s.  153,450 total partitions merged to 130,214.  Partition merge counts were {1:106978, 2:23236, }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Is this normal? &lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12541339">CASSANDRA-3854</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12574880" name="HybridCompactionStrategy.java" size="4334" author="tjake" created="Thu, 21 Mar 2013 20:52:32 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[jbellis]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>318732</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 12 weeks, 2 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1izvj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>319073</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>tjake</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[tjake]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>