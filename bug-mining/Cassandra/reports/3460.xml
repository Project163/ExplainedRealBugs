<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:51:49 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-8670] Large columns + NIO memory pooling causes excessive direct memory usage</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-8670</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;If you provide a large byte array to NIO and ask it to populate the byte array from a socket it will allocate a thread local byte buffer that is the size of the requested read no matter how large it is. Old IO wraps new IO for sockets (but not files) so old IO is effected as well.&lt;/p&gt;

&lt;p&gt;Even If you are using Buffered&lt;/p&gt;
{Input | Output}
&lt;p&gt;Stream you can end up passing a large byte array to NIO. The byte array read method will pass the array to NIO directly if it is larger than the internal buffer.  &lt;/p&gt;

&lt;p&gt;Passing large cells between nodes as part of intra-cluster messaging can cause the NIO pooled buffers to quickly reach a high watermark and stay there. This ends up costing 2x the largest cell size because there is a buffer for input and output since they are different threads. This is further multiplied by the number of nodes in the cluster - 1 since each has a dedicated thread pair with separate thread locals.&lt;/p&gt;

&lt;p&gt;Anecdotally it appears that the cost is doubled beyond that although it isn&apos;t clear why. Possibly the control connections or possibly there is some way in which multiple &lt;/p&gt;

&lt;p&gt;Need a workload in CI that tests the advertised limits of cells on a cluster. It would be reasonable to ratchet down the max direct memory for the test to trigger failures if a memory pooling issue is introduced. I don&apos;t think we need to test concurrently pulling in a lot of them, but it should at least work serially.&lt;/p&gt;

&lt;p&gt;The obvious fix to address this issue would be to read in smaller chunks when dealing with large values. I think small should still be relatively large (4 megabytes) so that code that is reading from a disk can amortize the cost of a seek. It can be hard to tell what the underlying thing being read from is going to be in some of the contexts where we might choose to implement switching to reading chunks.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12769480">CASSANDRA-8670</key>
            <summary>Large columns + NIO memory pooling causes excessive direct memory usage</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="aweisberg">Ariel Weisberg</assignee>
                                    <reporter username="aweisberg">Ariel Weisberg</reporter>
                        <labels>
                    </labels>
                <created>Thu, 22 Jan 2015 23:36:37 +0000</created>
                <updated>Wed, 2 Oct 2024 21:55:42 +0000</updated>
                            <resolved>Fri, 3 Apr 2015 21:44:47 +0000</resolved>
                                        <fixVersion>2.2.0 beta 1</fixVersion>
                                    <component>Legacy/Local Write-Read Paths</component>
                    <component>Legacy/Streaming and Messaging</component>
                    <component>Local/Compaction</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>7</watches>
                                                                                                                <comments>
                            <comment id="14351742" author="evin.callahan" created="Sat, 7 Mar 2015 18:57:12 +0000"  >&lt;p&gt;What&apos;s the path forward on this?&lt;/p&gt;</comment>
                            <comment id="14358866" author="aweisberg" created="Thu, 12 Mar 2015 16:02:16 +0000"  >&lt;p&gt;I am getting to this now. Should be fixed in 3.0. Once I have it fixed for 3.0 we can decide about back porting to 2.1.&lt;/p&gt;</comment>
                            <comment id="14364086" author="aweisberg" created="Mon, 16 Mar 2015 22:21:30 +0000"  >&lt;p&gt;I have a dtest that can reproduce the issue. I added to JMX gcstats the current amount of in flight  direct bytebuffer memory (this includes buffers that haven&apos;t been GCed). This is the value from java.nio.Bits.&lt;/p&gt;

&lt;p&gt;When I took a heap dump the issue was with Netty pooling memory. Netty pooled 600 megabytes of memory after I serially (and single threaded) wrote/read 5 rows with a single 35 megabyte column each. Each pooled bit of memory was 16 megabytes. I don&apos;t know yet where Netty steady states.&lt;/p&gt;

&lt;p&gt;This doesn&apos;t match what I recall from the original user report where the memory was being pooled as part of intracluster networking. There may be another factor like the setting for intracluster compression that influences it. It may even be that enabling intracluster compression is a work around.&lt;/p&gt;</comment>
                            <comment id="14366114" author="aweisberg" created="Tue, 17 Mar 2015 21:21:51 +0000"  >&lt;p&gt;I can confirm that compression determines whether you see an issue with memory used by intracluster messaging.&lt;/p&gt;

&lt;p&gt;The memory used by Netty shouldn&apos;t scale with cluster size the same way. &lt;/p&gt;

&lt;p&gt;Still figuring out how to test for the issue when Netty is dominating direct bytebuffer usage. I don&apos;t see a way to get Netty to report on it&apos;s memory usage nor a way to get the allocations done by NIO tracked.&lt;/p&gt;

&lt;p&gt;Monkey patching where art thou?&lt;/p&gt;</comment>
                            <comment id="14376787" author="aweisberg" created="Mon, 23 Mar 2015 22:30:16 +0000"  >&lt;p&gt;Took way longer than I would have liked, but here is an alternative implementation of DataInputStream and DataOutputStreamPlus that wraps a WritableByteChannel and does any necessary buffering.  &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...aweisberg:C-8670?expand=1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Implementation available on  github.&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I am also attaching a dtest validates that almost no direct byte buffer memory is allocated even when using large columns. To check how much is allocated I used reflection on java.nio.Bits and have GCInspector supply it along with the other metrics it supplies.&lt;/p&gt;

&lt;p&gt;To make it easy to test for I added a -D flag for testing that has Netty not pool memory and prefer non-direct byte buffers.&lt;/p&gt;

&lt;p&gt;The only other place where I think we might run into this issue is streaming. That operates on the input/output streams from sockets. With streaming you don&apos;t connect to as many nodes, and if the thread that is used for streaming is released once streaming completes is shouldn&apos;t be a problem.&lt;/p&gt;</comment>
                            <comment id="14378807" author="benedict" created="Tue, 24 Mar 2015 22:35:25 +0000"  >&lt;p&gt;Could we get the patch rebased so that all of the commits are adjacent, and not interspersed with merges in from 2.1/trunk? It makes it difficult to follow exactly what&apos;s been changed (I&apos;ve been guilty of this approach in the past, but I think it helps clean review to always ensure every commit for a patch occurs at the end of the git log)&lt;/p&gt;

&lt;p&gt;It&apos;s also worth discussing a potential simpler approach to this: couldn&apos;t we wrap DataInputStream, and proxy read(byte[]) to a loop over read(byte[], int, int)? For DataOutputPlus we can just change the behaviour of our DataOutputStreamPlus for byte[], which would fall through to DataOutputStreamAndChannel which could use the code you have for write(ByteBuffer) only to duplicate the behaviour here. We could (and probably should, when compression is disabled) use that in OTC to remove the indirection when filling a ByteBuffer. I&apos;m not saying for sure this is better, but since it is much simpler it seems we should refute this approach before attempting something more involved?&lt;/p&gt;</comment>
                            <comment id="14378902" author="aweisberg" created="Tue, 24 Mar 2015 23:18:13 +0000"  >&lt;p&gt;Do you want to see it commit by commit or should I squash it?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It&apos;s also worth discussing a potential simpler approach to this: couldn&apos;t we wrap DataInputStream, and proxy read(byte[]) to a loop over read(byte[], int, int)? For DataOutputPlus we can just change the behaviour of our DataOutputStreamPlus for byte[], which would fall through to DataOutputStreamAndChannel which could use the code you have for write(ByteBuffer) only to duplicate the behaviour here. We could (and probably should, when compression is disabled) use that in OTC to remove the indirection when filling a ByteBuffer. I&apos;m not saying for sure this is better, but since it is much simpler it seems we should refute this approach before attempting something more involved?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I was generally trying improve the situation by reading/writing to direct buffers and in the read case not adding another wrapper class and indirection. I have no idea what kind of output the JVM has for Streams that wrap other streams.&lt;/p&gt;

&lt;p&gt;DataOutputStreamAndChannel doesn&apos;t work with BufferedOutputStream. It doesn&apos;t flush the output stream before writing to the channel. We could always change that though.&lt;/p&gt;

&lt;p&gt;I am in favor of doing whatever doesn&apos;t have me benchmarking.&lt;/p&gt;</comment>
                            <comment id="14378930" author="benedict" created="Tue, 24 Mar 2015 23:36:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;I am in favor of doing whatever doesn&apos;t have me benchmarking.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Let&apos;s not get bogged down in microbenchmarking this stuff. IMO a little analysis of the options is sufficient, since either is likely an improvement. I don&apos;t have a preconceived answer to the question I raised, I just think it&apos;s worth assessing both options in contrast to each other. It&apos;s a bit late for me to assess that myself this evening, so I&apos;ll aim to collect my thoughts on that tomorrow. Feel free to fill in yours if you have time.&lt;/p&gt;</comment>
                            <comment id="14379647" author="benedict" created="Wed, 25 Mar 2015 10:17:16 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do you want to see it commit by commit or should I squash it?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t mind, so long as it&apos;s easy to squash myself (it looks to be interleaved with commits from elsewhere right now, is the difficulty)&lt;/p&gt;</comment>
                            <comment id="14379897" author="aweisberg" created="Wed, 25 Mar 2015 13:59:05 +0000"  >&lt;p&gt;What tool are you using to review? github does a good job handling merge commits and not show them as part of the diff and it doesn&apos;t show them as individual commits.&lt;/p&gt;

&lt;p&gt;You can also convert any github comparison to a single diff by adding .diff to the URL. That&apos;s what I used to create a &lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...aweisberg:C-8670-2?expand=1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;new branch&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14381722" author="benedict" created="Thu, 26 Mar 2015 11:20:19 +0000"  >&lt;blockquote&gt;&lt;p&gt;What tool are you using to review?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I like to navigate in IntelliJ, and on the command line, so having a clean run of commits helps a lot.&lt;/p&gt;

&lt;p&gt;After a bit of consideration, I think there&apos;s a good justification for introducing a whole new class if we intend to fully replace DataStreamOutputAndChannel, largely because the two write paths are not at all clear, and appear to be different (the old versions of the write paths being hard to actually pin down the location of in the VM source). So having a solid handle on how it behaves, and ensuring fewer code paths are executed, seems a good thing. As such, I think this patch should replace DSOaC entirely, and remove it from the codebase. I also think this is a good opportunity to share its code with DataOutputByteBuffer, and in doing hopefully make that faster, potentially improving performance of CL append (it doesn&apos;t need to extend AbstractDataOutput, and would share most of its implementation with NIODataOutputStream if it did not).&lt;/p&gt;

&lt;p&gt;A few comments in NIODataInputStream:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;readNext() should assert it is never shuffling more than 7 bytes; in fact ideally this would be done by readMinimum() to make it clearer&lt;/li&gt;
	&lt;li&gt;readNext() should IMO never shuffle unless it&apos;s at the end of its capacity; if it hasRemaining() and limit() != capacity() it should read on from its current limit (readMinimum can ensure there is room to fully meet its requirements)&lt;/li&gt;
	&lt;li&gt;readUnsignedShort() could simply be: {{ return readShort() &amp;amp; 0xFFFF;}}&lt;/li&gt;
	&lt;li&gt;available() should return the bytes in the buffer at least&lt;/li&gt;
	&lt;li&gt;ensureMinimum() isn&apos;t clearly named, since it is more intrinsically linked to primitive reads than it suggests, consuming the bytes and throwing EOF if it cannot read. Something like preparePrimitiveRead() (no fixed idea myself, just think it is more than &quot;ensureMinimum&quot;)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;A few comments in NIODataOutputStreamPlus:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;close() should flush&lt;/li&gt;
	&lt;li&gt;close() should clean the buffer&lt;/li&gt;
	&lt;li&gt;why the use of hollowBuffer? For clarity in case of restoring the cursor position during exceptions? Would be helpful to clarify with a comment. It seems like perhaps this should only be used for the first branch, though, since the second should have no risk of throwing an exception, so we can safely restore the position. It seems like it might be best to make hollowBuffer default to null, and instantiate it only if it is larger than our buffer size, otherwise first flushing our internal buffer if we haven&apos;t got enough room. This way we should rarely need the hollowBuffer.&lt;/li&gt;
	&lt;li&gt;We should either extend our AbstractDataOutput, or make our writeUTF method public static, so we can share it&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Finally, it would be nice if we didn&apos;t need to stash the OutputStream version separately. Perhaps we can reorganise the class hierarchy, so that DataOutputStreamPlus doesn&apos;t wrap an internal OutputStream, it just is a light abstract class merge of the types OutputStream and DataOutputPlus. We can introduce a WrappedDataOutputStreamPlus in its place, and AbstractDataOutput could extend our new DataOutputStreamPlus instead of the other way around (with Wrapped... extending &lt;em&gt;it&lt;/em&gt;). Then we can just stash a DataOutputStreamPlus in all cases. Sound reasonable?&lt;/p&gt;</comment>
                            <comment id="14382524" author="aweisberg" created="Thu, 26 Mar 2015 19:50:34 +0000"  >&lt;p&gt;NIODataInputStream&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;readNext() should assert it is never shuffling more than 7 bytes; in fact ideally this would be done by readMinimum() to make it clearer&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;By assert you mean an assert that compiles out or a precondition?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;readNext() should IMO never shuffle unless it&apos;s at the end of its capacity; if it hasRemaining() and limit() != capacity() it should read on from its current limit (readMinimum can ensure there is room to fully meet its requirements)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I guess I don&apos;t get when this optimization will help. I could see it hurting. You could stream through the buffer not returning to the beginning on a regular basis and end up issuing smaller then desired reads.&lt;/p&gt;

&lt;p&gt;Users of buffered input stream get this behavior and I didn&apos;t want to change it. DataInput and company pull bytes out one at a time even for multi-byte types.&lt;/p&gt;

&lt;p&gt;NIODataOutputStreamPlus&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;available() should return the bytes in the buffer at least&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I duplicated the JDK behavior for NIO. DataInputStream for a socket returns 0, for a file it returns the bytes remaining to read from the file. I think it makes sense for the API when you don&apos;t have a real answer.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;why the use of hollowBuffer? For clarity in case of restoring the cursor position during exceptions? Would be helpful to clarify with a comment. It seems like perhaps this should only be used for the first branch, though, since the second should have no risk of throwing an exception, so we can safely restore the position. It seems like it might be best to make hollowBuffer default to null, and instantiate it only if it is larger than our buffer size, otherwise first flushing our internal buffer if we haven&apos;t got enough room. This way we should rarely need the hollowBuffer.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The contract of the API requires that the incoming buffer not be modified. For thread safety reasons I don&apos;t modify the original buffer&apos;s position and then reset it in a finally block.&lt;/p&gt;

&lt;p&gt;I am not sure what you mean by hollow buffer larger than our buffer. It&apos;s hollow so it has no size. We also use it copy things into our buffer while preserving the original position.&lt;/p&gt;

&lt;p&gt;The rest is reasonable.&lt;/p&gt;


</comment>
                            <comment id="14382646" author="benedict" created="Thu, 26 Mar 2015 20:52:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;By assert you mean an assert that compiles out or a precondition?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t mind, really. It&apos;s just for clarity&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I guess I don&apos;t get when this optimization will help. I could see it hurting. You could stream through the buffer not returning to the beginning on a regular basis and end up issuing smaller then desired reads.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It was more a suggestion for clarity - at least, in my opinion. Assuming we typically fill the buffer, it isn&apos;t really a problem, and if we don&apos;t we usually have room to fill after it (although if we were to try to fill an almost full buffer it would be a problem; but so is repeatedly shuffling a buffer that is regularly very underfilled). But perhaps some more comments explaining the behaviour of (and reasoning behind) each branch is a better solution, along with the assertions to make clear this is not a costly or common operation.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;DataInputStream for a socket returns 0&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;DataInputStream isn&apos;t buffered. BufferedInputStream, and the API spec in InputStream#available suggest we should return number of bytes we have buffered&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We also use it copy things into our buffer while preserving the original position.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In this scenario it is likely cheaper to simply restore the position once done, and this approach also means we can likely typically avoid ever allocating a hollow buffer. There is also no (typical) risk of exception, so no reason to use the hollow buffer, since we can guarantee we will be able to restore its position.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I am not sure what you mean by hollow buffer larger then our buffer&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I meant parameter provided buffer&lt;/p&gt;

&lt;p&gt;There are also some formatting issues I forgot to mention (braces on the wrong line, and lots of extra linebreaks between methods)&lt;/p&gt;</comment>
                            <comment id="14382709" author="aweisberg" created="Thu, 26 Mar 2015 21:26:42 +0000"  >&lt;blockquote&gt;&lt;p&gt;In this scenario it is likely cheaper to simply restore the position once done, and this approach also means we can likely typically avoid ever allocating a hollow buffer. There is also no (typical) risk of exception, so no reason to use the hollow buffer, since we can guarantee we will be able to restore its position.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I&apos;ve been bitten by very hard to find threading bugs from having multiple threads concurrently reading from the same ByteBuffer using relative methods. It&apos;s a small amount of code to not have to worry about it ever. It&apos;s common that you will send a message referencing an &quot;immutable&quot; object graph across multiple connections and it ends up not being so immutable because serialization for some object uses a ByteBuffer without duplicating first.&lt;/p&gt;

&lt;p&gt;I don&apos;t think allocating an extra object per stream should enter into the decision making. It&apos;s tiny in the big picture.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;but so is repeatedly shuffling a buffer that is regularly very underfilled&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;True, but you only have to shuffle up to 7 bytes, and if the buffer is empty the cost of filling it is going to dominate. JNI calls, multiple allocations, context switch to the kernel. &lt;/p&gt;

&lt;p&gt;In the case of socket IO we are hoping to buffer entire message, potentially all available ones so the common case is that the buffer will be empty at the end and it doesn&apos;t matter much.&lt;/p&gt;

&lt;p&gt;For file IO doing a sequential read the common case will be that there are handful of bytes left because we are reading a multi-byte value. If we don&apos;t shuffle in that case we will do all that work to read a few bytes and then go back to fill the rest of the buffer a second time.&lt;/p&gt;

&lt;p&gt;I&apos;ll update available() to return the buffered data.&lt;/p&gt;</comment>
                            <comment id="14382732" author="benedict" created="Thu, 26 Mar 2015 21:40:27 +0000"  >&lt;blockquote&gt;&lt;p&gt;to not have to worry about it ever&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure you can ever not worry about it, since the default behaviour is that methods &lt;em&gt;do&lt;/em&gt; modify the position of a BB, so you have to assume when it&apos;s a risk that you need to ensure each thread has its own version via duplicate(). But if that&apos;s your rationale, just comment it to explain and I&apos;m cool with it.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;For file IO&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;this should always be fully populated, but like I said: comments to explain (and assertions) will solve everything &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14384556" author="aweisberg" created="Fri, 27 Mar 2015 20:29:13 +0000"  >&lt;p&gt;I think I covered what we talked about. I followed quite a few things and this is where it lead me. I don&apos;t feel like I made a dent in terms of having less code in wide use.&lt;/p&gt;

&lt;p&gt;AbstractDataOutputStreamAndChannelPlus (formerly AbstractDataOutput) is still pretty firmly entrenched.&lt;/p&gt;</comment>
                            <comment id="14386007" author="benedict" created="Sun, 29 Mar 2015 23:14:03 +0000"  >&lt;p&gt;I&apos;ve pushed some suggestions for further refactoring &lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/8670-suggestions&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;. I&apos;ve only looked at the overall class hierarchy, I haven&apos;t focused yet on reviewing the method implementation changes.&lt;/p&gt;

&lt;p&gt;Mostly these changes flatten the class hierarchy; it&apos;s gotten deep enough I don&apos;t think there&apos;s a good reason to maintain the distinction between DataStreamOutputPlus and DataStreamOutputPlusAndChannel, especially since we often just mock up a Channel based off the OutputStream. I&apos;ve also flattened NIODataOutputStream and DataOutputStreamByteBufferPlus into BufferedDataOutputStreamPlus, since we only write to the buffer if we don&apos;t exceed its size. At the same time, since we are now refactoring this whole hierarchy, I made DataOutputBuffer extend BufferedDataOutputStreamPlus, and just ensures the buffer grows as necessary, and have removed FastByteArrayOutputStream since we no longer need it.&lt;/p&gt;

&lt;p&gt;I&apos;ve also stopped SequentialWriter implementing WritableByteChannel, and now pass in its internal Channel, since that&apos;s the only way the operations will benefit. As a follow up ticket, we should probably move SequentialWriter to utilising BufferedDataOutputStreamPlus directly, so that it can benefit from faster encoding of primitives&lt;/p&gt;

&lt;p&gt;Let me know what you think of the changes to the hierarchy, and once we&apos;ve ironed that out we can move on to the home stretch and confirm the code changes. One other thing we could consider is dropping the &quot;Plus&quot; from everything except the interface, since it seems superfluous, and it&apos;s all fairly verbose.&lt;/p&gt;</comment>
                            <comment id="14386852" author="benedict" created="Mon, 30 Mar 2015 15:19:32 +0000"  >&lt;p&gt;I&apos;ve just pushed another update, which undoes part of the SequentialWriter changes, since all writes (esp. for compression) should go through the SW itself. It also makes one further minor change to stop using Channels.newChannel() everywhere, since that introduces an extra layer of byte shuffling unnecessarily, when DataOutputPlus implements a compatible method that can be called directly.&lt;/p&gt;</comment>
                            <comment id="14388515" author="benedict" created="Tue, 31 Mar 2015 13:26:44 +0000"  >&lt;p&gt;I&apos;ve pushed one more round of changes &lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/8670-2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;, after your follow up round (which I mention for posterity). I&apos;ve made the following changes; let me know your thoughts on them:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;Merged writeUTF into one method, with a fast path &lt;em&gt;only&lt;/em&gt; for ASCII characters, since this is likely to benefit most from unrolling, and the instruction cache pollution effect is small. The two separate but near identical and very large methods look almost certain to be worse due to icache misses than a single branch that is mostly predicted correctly, especially when we had multiple branches inside the loop, which were each more likely to be mispredicted. As a follow-up commit, in case you&apos;re worried by this, I&apos;ve introduced a no-conditional version of sizeOfChar (which we may be able to optimise further), but I haven&apos;t performed any benchmarks to measure the difference in effect.&lt;/li&gt;
	&lt;li&gt;Reverted the new hollowBuffer approach for array backed buffers - I couldn&apos;t see a reason for not just directly invoking the write(byte[]) methods?&lt;/li&gt;
	&lt;li&gt;Based SafeMemoryWriter on DataOutputBuffer&lt;/li&gt;
	&lt;li&gt;Shared the UBDOSP.utfBytes and DOSP.WBC.buf in the same ThreadLocal&lt;/li&gt;
	&lt;li&gt;Preferred bb.hasArray() to bb.isDirect(), since it is a concrete method, so can be inlined&lt;/li&gt;
	&lt;li&gt;Moved writeUTFLegacy into the test case, since it&apos;s only for test purposes now&lt;/li&gt;
	&lt;li&gt;Fixed formatting in UnbufferedDataOutputStreamPlus (seems a good opportunity to standardise it)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="14388757" author="aweisberg" created="Tue, 31 Mar 2015 16:08:45 +0000"  >&lt;p&gt;Unit tets pass and I am +1 on your changes.&lt;/p&gt;</comment>
                            <comment id="14388792" author="benedict" created="Tue, 31 Mar 2015 16:29:17 +0000"  >&lt;p&gt;Committed&lt;/p&gt;</comment>
                            <comment id="14390360" author="benedict" created="Wed, 1 Apr 2015 10:41:45 +0000"  >&lt;p&gt;A little niggle was bugging me, and I decided to check it out, and I think it warrants further consideration:&lt;/p&gt;

&lt;p&gt;The &lt;a href=&quot;http://hg.openjdk.java.net/jdk7/jdk7/jdk/file/00cd9dc3c2b5/src/windows/native/java/net/SocketOutputStream.c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;windows&lt;/a&gt; and &lt;a href=&quot;http://hg.openjdk.java.net/jdk6/jdk6/jdk/annotate/1a53516ce032/src/solaris/native/java/net/SocketOutputStream.c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;solaris&lt;/a&gt; implementations of SocketOutputStream allocate &lt;em&gt;temporary&lt;/em&gt; (not cached) heap (not Java heap) memory of the total size of the data being written to them, if they&apos;re above the size of the stack memory they allocate by default (according to the source, this ranges from 8K to 64K on &lt;a href=&quot;http://hg.openjdk.java.net/jdk7/jdk7/jdk/file/4dbd83eb0250/src/solaris/native/java/net/net_util_md.h#l123&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Solaris&lt;/a&gt;, and 2K on &lt;a href=&quot;http://hg.openjdk.java.net/jdk7/jdk7/jdk/file/0c27202d66c1/src/windows/native/java/net/net_util_md.h#l236&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Windows&lt;/a&gt;, for the stack buffer, with the heap buffer being a little larger)&lt;/p&gt;

&lt;p&gt;This raises a few questions: &lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;how much of our thread stack frame ends up being used by these methods? Seems we have some headroom with even the minimum stack frame, so maybe not worth worrying about; but larger writes will incur some non-java-heap allocations without much benefit&lt;/li&gt;
	&lt;li&gt;should we simply copy our byte[] data into our direct buffer, and call write on this instead? this has the added advantage of avoiding the OutputStream entirely in situations where we have access to the Channel, which removes an entire codepath from the common execution path. There seems to be no downside, especially if our buffer is large enough, since the copying is exactly what the native code does anyway.&lt;/li&gt;
	&lt;li&gt;what is behaviour on Linux? I can&apos;t for the life of me find the linux implementation; best I can assume is it is very similar to Solaris&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="14390400" author="snazy" created="Wed, 1 Apr 2015 11:14:41 +0000"  >&lt;p&gt;Your assumption is correct &lt;tt&gt;jdk/src/solaris/native/java/net/SocketOutputStream.c&lt;/tt&gt; is the Linux source.&lt;br/&gt;
What I don&apos;t understand is why &lt;tt&gt;Java_java_net_SocketOutputStream_socketWrite0&lt;/tt&gt; not just uses the given &lt;tt&gt;byte[]&lt;/tt&gt; but copies to a stack/heap buffer.&lt;/p&gt;</comment>
                            <comment id="14390405" author="benedict" created="Wed, 1 Apr 2015 11:18:32 +0000"  >&lt;blockquote&gt;&lt;p&gt;What I don&apos;t understand is why Java_java_net_SocketOutputStream_socketWrite0 not just uses the given byte[] but copies to a stack/heap buffer.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Time to safe point. Using it directly would require pausing GC until the socket write had completed, which could be lengthy (since it&apos;s blocking, it&apos;s unbounded, in fact)&lt;/p&gt;</comment>
                            <comment id="14390541" author="aweisberg" created="Wed, 1 Apr 2015 13:28:16 +0000"  >&lt;p&gt;In what scenarios do we end up actually writing to the native methods right now? In most cases we opt to use the channel when available. The only time we should be using WrappedDataOutputStreamPlus is when we aren&apos;t actually writing to the channel of a file or socket.&lt;/p&gt;

&lt;p&gt;Where we could really do better is when using compression, we could have the compression wrap a direct buffer which wraps the channel and avoid relying on the built in mechanisms for getting data off heap.&lt;/p&gt;

&lt;p&gt;I had some other refrigerator moments overnight as well.&lt;/p&gt;

&lt;p&gt;BufferedDataOutputStreamPlus.close() will clean the buffer even if it might not own the buffer such as when it is provided to the constructor. It also doesn&apos;t check if the channel is null.&lt;/p&gt;

&lt;p&gt;When used from the commit log without a channel it will throw an NPE if it exceeds the capacity of the buffer when it goes to flush. I suppose one runtime exception is as good as another. There is also the extra bounds check in ensureRemaining() which always seemed a little useless to me and the fact that it will not drop through and do efficient copies for direct byte buffers. It almost seems like there is a case for a version just for wrapping fixed size bytebuffers. &lt;/p&gt;</comment>
                            <comment id="14390555" author="aweisberg" created="Wed, 1 Apr 2015 13:37:52 +0000"  >&lt;p&gt;I updated the microbenchmark to do what I think is the right think WRT to dead code elimination and constant folding. Results don&apos;t change, but I it looks more like I know what we I am doing.&lt;/p&gt;</comment>
                            <comment id="14390561" author="benedict" created="Wed, 1 Apr 2015 13:45:54 +0000"  >&lt;blockquote&gt;&lt;p&gt;In most cases we opt to use the channel when available. ... Where we could really do better is when using compression&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You&apos;re right. I thought we had more paths leftover, but it is just compression really. That&apos;s a much cleaner state of affairs!&lt;/p&gt;

&lt;p&gt;It looks like compression will typically be under the stack buffer size, so we don&apos;t have such a problem, but it would still be nice to move to a DirectByteBuffer compressed output stream. It should be quite viable since there are now compression methods for working over these directly, but that should probably be a follow up ticket. Do you want to file, or shall I?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;BufferedDataOutputStreamPlus.close() will clean the buffer even if it might not own the buffer such as when it is provided to the constructor. It also doesn&apos;t check if the channel is null.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We don&apos;t call close in situations where this is a problem&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;When used from the commit log without a channel it will throw an NPE if it exceeds the capacity of the buffer when it goes to flush&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We presize the buffer correctly&lt;/p&gt;

&lt;p&gt;Still, on both these counts we could offer better safety if we wanted, without much cost. If we used a DataOutputBuffer it would solve these problems, and we could assert that the final buffer is == the provided buffer...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;It almost seems like there is a case for a version just for wrapping fixed size bytebuffers.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Possibly. I&apos;m on the fence, since it&apos;s extra code cache and class hierarchy pollution, with limited positive impact. We would need to duplicate the whole of BufferedDataOutputStreamPlus. You could benchmark to see if there&apos;s an appreciable difference? If the difference is small (which I expect it is, given branch prediction), I would prefer to avoid the pollution.&lt;/p&gt;</comment>
                            <comment id="14390570" author="aweisberg" created="Wed, 1 Apr 2015 13:52:28 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8887&quot; title=&quot;Direct (de)compression of internode communication&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8887&quot;&gt;&lt;del&gt;CASSANDRA-8887&lt;/del&gt;&lt;/a&gt; would be part of it. Maybe an umbrella ticket and do that first to figure out how hook the compression libraries up. Can you file?&lt;/p&gt;

&lt;p&gt;Also on trying to understand code cache pollution. How much do you really know about how many instructions are emitted when the JVM can inline and duplicate stuff out the yin yang?&lt;/p&gt;</comment>
                            <comment id="14390595" author="aweisberg" created="Wed, 1 Apr 2015 14:10:53 +0000"  >&lt;p&gt;On more thinking I don&apos;t think having close clean the buffer if we allow people to supply one is great. It could be double free or use after free if someone makes a mistake.&lt;/p&gt;

&lt;p&gt;&lt;del&gt;I think we should had least have FileUtil.clean null out the pointer to the buffer before/after cleaning (whichever is possible) so we get as immediate a failure as possible.&lt;/del&gt; &lt;del&gt;(This is done by the cleaner)&lt;/del&gt; Except not all buffers we create will have cleaners... Duplicates or slices of the buffer won&apos;t pick up that the pointer was nulled, but it&apos;s better then nothing. We should also assert that the buffer wasn&apos;t provided in the constructor and throw an exception if someone did that and then called close.&lt;/p&gt;</comment>
                            <comment id="14390598" author="benedict" created="Wed, 1 Apr 2015 14:18:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;Also on trying to understand code cache pollution. How much do you really know about how many instructions are emitted when the JVM can inline and duplicate stuff out the yin yang?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, there are a lot of heuristics to apply, that are admittedly limited and imperfect. But in general: hotspot won&apos;t inline megamorphic call sites, and even bimorphic callsites are unlikely to be (i think probably never) inlined, only given a static despatch fast path. These heuristics are enough to guide decisions around this sufficiently well in my experience. If there are multiple implementations viable at any moment, then the callsite will not be inlined, at most its location will be, and even if it &lt;em&gt;is&lt;/em&gt; inlined, this doesn&apos;t necessarily pollute the code cache, since inlined methods are small (as are most methods) and adjacent occupancy of the cache is essentially free, unless the overall method size exceeds the cache line boundary.&lt;/p&gt;

&lt;p&gt;In general my view, the simplest heuristic is: if the benefit is small, and it increases the number of active call sites, then let&apos;s not. This works from a code management as well as a cache pollution perspective at once.&lt;/p&gt;</comment>
                            <comment id="14390630" author="benedict" created="Wed, 1 Apr 2015 14:34:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;We should also assert that the buffer wasn&apos;t provided in the constructor and throw an exception if someone did that and then called close.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If we forbid this entirely, and only expose it via DataOutputBuffer, then the close method is harmless (as it&apos;s a no-op)&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13594113">CASSANDRA-19976</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12708687" name="OutputStreamBench.java" size="7317" author="aweisberg" created="Wed, 1 Apr 2015 13:37:52 +0000"/>
                            <attachment id="12706739" name="largecolumn_test.py" size="1724" author="aweisberg" created="Mon, 23 Mar 2015 22:30:16 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[aweisberg]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 33 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i24p4f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>benedict</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[benedict]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>