<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:40:22 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-5661] Discard pooled readers for cold data</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-5661</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;Reader pooling was introduced in &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-4942&quot; title=&quot;Pool [Compressed]RandomAccessReader on the partitioned read path&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-4942&quot;&gt;&lt;del&gt;CASSANDRA-4942&lt;/del&gt;&lt;/a&gt; but pooled RandomAccessReaders are never cleaned up until the SSTableReader is closed.  So memory use is &quot;the worst case simultaneous RAR we had open for this file, forever.&quot;&lt;/p&gt;

&lt;p&gt;We should introduce a global limit on how much memory to use for RAR, and evict old ones.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12653702">CASSANDRA-5661</key>
            <summary>Discard pooled readers for cold data</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="xedin">Pavel Yaskevich</assignee>
                                    <reporter username="jbellis">Jonathan Ellis</reporter>
                        <labels>
                    </labels>
                <created>Wed, 19 Jun 2013 13:31:17 +0000</created>
                <updated>Tue, 14 Oct 2025 12:13:57 +0000</updated>
                            <resolved>Tue, 17 Sep 2013 21:58:11 +0000</resolved>
                                        <fixVersion>2.0.1</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>14</watches>
                                                                                                                <comments>
                            <comment id="13687985" author="jbellis" created="Wed, 19 Jun 2013 13:32:51 +0000"  >&lt;p&gt;One possibility would be to have a &lt;tt&gt;CLHM&amp;lt;SSTableReader, Queue&amp;lt;RandomAccessReader&amp;gt;&amp;gt;&lt;/tt&gt; that PooledSegmentedFile looks itself up in.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt;, do you have time to take a stab at this?&lt;/p&gt;</comment>
                            <comment id="13693254" author="jbellis" created="Tue, 25 Jun 2013 18:39:13 +0000"  >&lt;p&gt;This has bitten someone in production now...  LCS w/ 4000 sstables, using 2GB of heap (16k CRAR buffers).&lt;/p&gt;</comment>
                            <comment id="13693270" author="xedin" created="Tue, 25 Jun 2013 18:54:42 +0000"  >&lt;p&gt;I&apos;m starting to think that it would be just easier for everybody to just start using mmap buffers for compressed buffers all the time instead before we make caching too complex. I will start working in that direction to see if there is any promise in that.&lt;/p&gt;</comment>
                            <comment id="13693275" author="JIRAUSER308715" created="Tue, 25 Jun 2013 18:57:23 +0000"  >&lt;p&gt;Saw some issues caused by these never getting cleared out.  On a cluster using LCS CompressedRandomAccessReader objects are using 2 GB of heap space per node.&lt;/p&gt;</comment>
                            <comment id="13693406" author="jbellis" created="Tue, 25 Jun 2013 21:46:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m starting to think that it would be just easier for everybody to just start using mmap buffers for compressed buffers all the time&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good idea, but probably too big a change for 1.2.7?&lt;/p&gt;</comment>
                            <comment id="13693419" author="xedin" created="Tue, 25 Jun 2013 21:55:59 +0000"  >&lt;p&gt;As a brain dump, I see 3 ways to resolve this:&lt;/p&gt;

&lt;p&gt;1. Introduce queue total memory limit and evict based on that, but there is no guarantee that we won&apos;t be evicting incorrect instances.&lt;/p&gt;

&lt;p&gt;2. Introduce liveness per instance (e.g. 20-30 seconds) before evictor considers it as &quot;old&quot;, that solves the problem with #1 but are relying on eviction thread to be robust and run constantly, any delays in such manual GC could result in the same memory bloat as described in the issue.&lt;/p&gt;

&lt;p&gt;3. Remove caching and go with mmap&apos;ed segments instead, the problem with that is that we need to create direct byte buffer every time we decompress data which I&apos;m not sure if could be GC&apos;ed reliably, so for example, if particular JVM implementation only cleans up such buffers only on CMS or full GC process can effectively OOM because we are actually trying to avoid any significant GC activity as much as possible.&lt;/p&gt;

&lt;p&gt;I like #2 the most. Thoughts?&lt;/p&gt;</comment>
                            <comment id="13693478" author="jbellis" created="Tue, 25 Jun 2013 23:05:34 +0000"  >&lt;blockquote&gt;&lt;p&gt;Introduce queue total memory limit and evict based on that, but there is no guarantee that we won&apos;t be evicting incorrect instances.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t follow &amp;#8211; surely LRU is a better heuristic than &quot;idle for N seconds&quot; as in #2?&lt;/p&gt;</comment>
                            <comment id="13693556" author="xedin" created="Wed, 26 Jun 2013 01:08:06 +0000"  >&lt;p&gt;LRU is more of a replacement strategy when you have distinct objects and fixed upper limit (num items, memory), but in here we deal with mostly duplicate objects and the states where num of items hardly overgrows queue so the problem is not handling replacement but rather expiring items after some period of idling. I was thinking of doing something like ExpiringQueue with the one (or set of timers) similar to Guava Cache. Which solves the problem of &lt;span class=&quot;error&quot;&gt;&amp;#91;C&amp;#93;&lt;/span&gt;RAR instances being stuck in the cache for long periods of time even when read pattern have changed and they are not useful anymore.&lt;/p&gt;</comment>
                            <comment id="13693600" author="jbellis" created="Wed, 26 Jun 2013 03:28:44 +0000"  >&lt;p&gt;It still looks like an LRU problem to me: I have room for N objects, when I try to allocate N+1 I throw away the LRU.&lt;/p&gt;

&lt;p&gt;This means we will use more memory than a timer approach if N is actually more than we need, but it will do better than timers if we are crunched for space, which seems like the more important scenario to optimize.&lt;/p&gt;

&lt;p&gt;It also means that we have a very clear upper bound on memory use, rather than depending on workload, which history shows is tough for users to tune.&lt;/p&gt;</comment>
                            <comment id="13693612" author="xedin" created="Wed, 26 Jun 2013 03:55:19 +0000"  >&lt;p&gt;It seems like we are trying to address different problems of what is in description to the ticket and what Jeremia pointed out. Let me describe what I&apos;m trying to solve: When reading from multiple SSTables for a while and then pattern changes and load is switched to the different subset of SSTables, previous &lt;span class=&quot;error&quot;&gt;&amp;#91;C&amp;#93;&lt;/span&gt;RAR instances are returned to the appropriate queues and stuck there until each SSTable is deallocated (by compaction) which creates memory pressure on stale workloads or when compaction is running behind.&lt;/p&gt;

&lt;p&gt;LRU could solve that problem when we have limit on total amount of memory that we can use so it would start kicking in only after we reach that limit and create a jitter in the queue and processing latencies. &lt;/p&gt;

&lt;p&gt;What I propose adds minimal booking overhead per queue and expires items quicker than LRU and more precise, also I&apos;m not really worried about max number of items in the queue per SSTable as it&apos;s organically limited to the number of concurrent readers.&lt;/p&gt;</comment>
                            <comment id="13693631" author="jbellis" created="Wed, 26 Jun 2013 04:45:48 +0000"  >&lt;p&gt;LCS makes number of sstables x concurrent readers a problem.&lt;/p&gt;</comment>
                            <comment id="13693680" author="xedin" created="Wed, 26 Jun 2013 06:13:01 +0000"  >&lt;p&gt;What is LCS?&lt;/p&gt;</comment>
                            <comment id="13693876" author="JIRAUSER308715" created="Wed, 26 Jun 2013 09:46:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt; LCS == LeveledCompactionStrategy.  While in theory I like the idea of the searchers in most cases expiring quicker with timers, in practice since these are on the heap, and LCS defaults to only 5 MB files per levels, you can have A LOT of sstables, 200 per GB...  Which as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbellis&quot; class=&quot;user-hover&quot; rel=&quot;jbellis&quot;&gt;jbellis&lt;/a&gt; mentioned makes # of sstables * concurrent readers a big problem, which was the problem hit above.  So we really need to bound memory usage with a hard cap, not the fuzzy cap of &quot;how many can I open in X seconds&quot;.&lt;/p&gt;

&lt;p&gt;1k reqs/sec hitting a 4 level deep LCS CF could mean 4k Reader&apos;s (~500 MB) created per second.  Now that I say that out loud, I almost think we should go back to not caching these at all so they always just get recycled in young gen and never have a chance to hit old gen.  I guess it could be a config parameter which defaults on for STCS and off for LCS.  Since STCS work loads have no where near as many sstables.&lt;/p&gt;</comment>
                            <comment id="13693990" author="jbellis" created="Wed, 26 Jun 2013 14:20:25 +0000"  >&lt;p&gt;I&apos;m not as pessimistic &amp;#8211; I think pooling will still be useful for most workloads &amp;#8211; but if we add a configurable ceiling on memory use, we could certainly add that zero = no pooling at all.&lt;/p&gt;

&lt;p&gt;Would also be useful to track the pool &quot;hit rate.&quot;&lt;/p&gt;</comment>
                            <comment id="13694164" author="xedin" created="Wed, 26 Jun 2013 19:00:29 +0000"  >&lt;blockquote&gt;&lt;p&gt;While in theory I like the idea of the searchers in most cases expiring quicker with timers, in practice since these are on the heap, and LCS defaults to only 5 MB files per levels, you can have A LOT of sstables, 200 per GB... Which as Jonathan Ellis mentioned makes # of sstables * concurrent readers a big problem, which was the problem hit above. So we really need to bound memory usage with a hard cap, not the fuzzy cap of &quot;how many can I open in X seconds&quot;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;When I was talking about concurrent readers I actually meant the max number of threads in READ stage (concurrent_reads in yaml, default 32) that could be running in the same time, that is the worst case limit of cache queue per SSTable.&lt;/p&gt;

&lt;p&gt;I&apos;m not convinced that LRU on SSTable would work better then &lt;span class=&quot;error&quot;&gt;&amp;#91;C&amp;#93;&lt;/span&gt;RAR instance expiration for our use case because due to replacement mechanism it creates jitter in latencies grater than p75 once global threshold is reached, which in production systems create situation when people can&apos;t explain why latencies suddenly degraded without increasing amount of data.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure that I follow what you mean by last part, we are not trying to limit number of open for period of time but rather expire items that been returned to the queue after time period. I agree that we can add upper limit on memory usage just like we did for key/row caches, which in combination with expiring would yield predictable behavior.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;1k reqs/sec hitting a 4 level deep LCS CF could mean 4k Reader&apos;s (~500 MB) created per second. Now that I say that out loud, I almost think we should go back to not caching these at all so they always just get recycled in young gen and never have a chance to hit old gen. I guess it could be a config parameter which defaults on for STCS and off for LCS. Since STCS work loads have no where near as many sstables.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It sounds like memory overhead caused by compression for your use-case is too big with or without caching (it&apos;s around 79KB = 64KB default chunk + snappy overhead) per file, even if you drop caching your allocation rate would cause &lt;span class=&quot;error&quot;&gt;&amp;#91;C&amp;#93;&lt;/span&gt;RAR buffers to be promoted and cause fragmentation in the old gen which could cause long &quot;remark&quot; phrase of CMS (can be battled with CMSScavengeBeforeRemark option) and as CMS is not compacting you can suffer from FullGC more frequently. That is what we are trying to battle with caching &lt;span class=&quot;error&quot;&gt;&amp;#91;C&amp;#93;&lt;/span&gt;RAR instances (and their buffers) as if we allocate those close in time and reuse, it&apos;s more luckily that they are going to be laid out closely plus removes portion of work from young gen collector.&lt;/p&gt;</comment>
                            <comment id="13694202" author="JIRAUSER308715" created="Wed, 26 Jun 2013 19:56:48 +0000"  >&lt;p&gt;Screen shots attached to show the issue.&lt;br/&gt;
Histogram.png show my histogram.  Over 2 GB is being retained by CRAR objects on a node with only 80 GB of data, most of that in LCS CF&apos;s with 20 MB sstable size, and some ine LCS CF&apos;s with 10 MB sstable size.&lt;br/&gt;
&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12589787/12589787_Histogram.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Here is the dominator tree for that same heap dump.&lt;br/&gt;
&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/12589788/12589788_DominatorTree.png&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;The problem is that most of those sstables have 1 MB or more of CRAR buffers sitting there waiting to be used.  If we do a timer with an absolute cap, even better, but we need some kind of cap.&lt;/p&gt;

&lt;p&gt;The compression overhead itself isn&apos;t really an issue, the issue is the cached CRAR objects/buffers.&lt;/p&gt;</comment>
                            <comment id="13694292" author="xedin" created="Wed, 26 Jun 2013 22:05:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;The problem is that most of those sstables have 1 MB or more of CRAR buffers sitting there waiting to be used. If we do a timer with an absolute cap, even better, but we need some kind of cap.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;1MB actually means that it&apos;s around 12-14 caches reads which is not even half of default concurrency limit, we can start with expiring for 1.2.7 as it doesn&apos;t require adding options (and maybe disable caching with LCS), then for 2.0 we can add cap limit per CF or even global into yaml.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The compression overhead itself isn&apos;t really an issue, the issue is the cached CRAR objects/buffers.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It is for such a small files because it&apos;s not just buffers but also compression metadata per SSTable although you can&apos;t see it in histogram/tree as it was moved off heap, I guess it&apos;s an artifact of LCS that we can&apos;t do anything about.&lt;/p&gt;

&lt;p&gt;This all got me wondering what is BF space overhead LCS vs. STCS, can you please check?&lt;/p&gt;</comment>
                            <comment id="13694299" author="jbellis" created="Wed, 26 Jun 2013 22:12:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;it&apos;s not just buffers but also compression metadata per SSTable although you can&apos;t see it in histogram/tree as it was moved off heap&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The on-heap part is bad enough &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13694344" author="xedin" created="Wed, 26 Jun 2013 23:17:00 +0000"  >&lt;p&gt;After thinking about this more as LCS produces similar (same) sized files there is actually no benefit of caching &lt;span class=&quot;error&quot;&gt;&amp;#91;C&amp;#93;&lt;/span&gt;RAR instances. I think we should do caching only with STCS + expiring as it introduces less memory overhead per file for &lt;span class=&quot;error&quot;&gt;&amp;#91;C&amp;#93;&lt;/span&gt;RAR.&lt;/p&gt;

&lt;p&gt;I also want to share my worries about LCS using 5MB file size with mmap and no compression: default vm.max_map_count is 65536 (maximum number of memory mappings per process) which is around 322GB of data, we need to mention somewhere that if somebody wants to run with mmap + LCS they need to adjust that setting. Another thing that worries me is that performance of mmap, munmap and similar + minor/major page faults would degrade logarithmically as memory mappings are handled by red-black tree and with constant compaction we would be burning a lot more cpu on tree balancing as dataset grows.&lt;/p&gt;</comment>
                            <comment id="13694410" author="jbellis" created="Thu, 27 Jun 2013 01:36:29 +0000"  >&lt;p&gt;I don&apos;t see what similar-sized files has to do with it.  CRAR buffer size is independent of file size, and that&apos;s what causes the fragmentation.  (Zeroing out large buffers isn&apos;t free, either.)&lt;/p&gt;</comment>
                            <comment id="13694422" author="xedin" created="Thu, 27 Jun 2013 01:54:44 +0000"  >&lt;p&gt;I&apos;m not sure what you mean. I was trying to say that caching doesn&apos;t deliver good trade-off in terms of memory usage on small files. On the other hand caching makes more sense with STCS especially on higher tiers as number of blocks handled by one buffer would be few orders of magnitude higher.&lt;/p&gt;</comment>
                            <comment id="13694466" author="xedin" created="Thu, 27 Jun 2013 04:03:16 +0000"  >&lt;p&gt;In other words, my initial idea didn&apos;t take into account leveled compaction, it was simple - by paying overall small memory price (one chunk size or less depending on size of file) per 1GB we can minimize GC work, memory allocation and number of syscalls per read. With leveled compaction that strategy doesn&apos;t work as price per 1 GB is pretty big as most of the files are very small which increases allocation rate to cache and GC activity by very frequent compactions.&lt;/p&gt;</comment>
                            <comment id="13694719" author="tjake" created="Thu, 27 Jun 2013 13:43:36 +0000"  >&lt;p&gt;We use LCS and haven&apos;t seen any heap pressure here, though we set our files to 128M.  Would it make more sense to change this default from 5mb.  No one successfully using LCS has it set to 5mb.&lt;/p&gt;</comment>
                            <comment id="13694743" author="jbellis" created="Thu, 27 Jun 2013 14:16:23 +0000"  >&lt;p&gt;Agreed that we should evaluate LCS defaults (and we have Daniel Meyer working on this now), but that just kicks the can down the road; if you&apos;re in trouble with 50GB of data and 5MB sstables, you&apos;ll be in equal trouble at 1TB of data and 100MB sstables.&lt;/p&gt;</comment>
                            <comment id="13694977" author="xedin" created="Thu, 27 Jun 2013 19:28:46 +0000"  >&lt;p&gt;Well it depends on how do you define equal, having 1TB of data would definitely require bigger heap and physical memory configuration. &lt;/p&gt;

&lt;p&gt;Let&apos;s calculate (where each file have one buffer in memory at all times):&lt;/p&gt;

&lt;p&gt;5MB   files (each 79KB decompression buffer) for 1GB of such files in memory would be: 204 (num files in 1GB) * 79KB = &lt;b&gt;16MB&lt;/b&gt; buffers&lt;br/&gt;
128MB files (&lt;del&gt;//&lt;/del&gt;) require 25.5 times less buffers per 1GB than 5MB files: 16MB (buffers per 1GB in case of 5MB files) / 25.5 = &lt;b&gt;643KB&lt;/b&gt; buffers&lt;/p&gt;

&lt;p&gt;So for 1TB with 5MB files we need 1024 * 16MB = &lt;b&gt;16GB&lt;/b&gt; of heap and for 128MB files it&apos;s 25.5 times less = &lt;b&gt;643MB&lt;/b&gt;, if each of the files is going to have at least 8 caches items in the same time with 128MB files we are going to have around 5GB of heap but I do think this scenario is a worst case, normal mode would be 2-3GB. If you go with 14-16GB heap and 1TB of data, 2GB of cache is the least of your problems as it&apos;s around 10% of total heap size which is still good trade-off to allocation rate if those buffers are allocated per call.&lt;/p&gt;</comment>
                            <comment id="13696161" author="jbellis" created="Sat, 29 Jun 2013 17:32:19 +0000"  >&lt;p&gt;It occurs to me that we may be approaching the problem the wrong way.  If we just pool the buffers rather than the CRAR objects, we would only need (concurrent readers x max sstables for a given partition) which is going to be much lower than the total sstable count.&lt;/p&gt;</comment>
                            <comment id="13696188" author="xedin" created="Sat, 29 Jun 2013 20:18:34 +0000"  >&lt;p&gt;I tried that before going with cached instances which is per CF map&amp;lt;int, queue&amp;lt;ByteBuffer&amp;gt;&amp;gt; as one CF could have different chunk sizes, it actually performs worse because of queue contention and we would still have to pay the price of &quot;open&quot; call on each read of file.&lt;/p&gt;</comment>
                            <comment id="13696192" author="jbellis" created="Sat, 29 Jun 2013 20:31:01 +0000"  >&lt;p&gt;We don&apos;t need exact chunk size matches though &amp;#8211; i.e., we can use a larger buffer.  So if we just pool max chunk size buffers we&apos;ll probably come out ahead.&lt;/p&gt;</comment>
                            <comment id="13696197" author="xedin" created="Sat, 29 Jun 2013 20:58:07 +0000"  >&lt;p&gt;It&apos;s waste of memory and doesn&apos;t solve contention problem.&lt;/p&gt;</comment>
                            <comment id="13696219" author="jbellis" created="Sat, 29 Jun 2013 23:59:05 +0000"  >&lt;p&gt;It&apos;s a lot less memory used than the status quo.  I&apos;d take a little contention over OOMing people.&lt;/p&gt;</comment>
                            <comment id="13696225" author="xedin" created="Sun, 30 Jun 2013 00:42:59 +0000"  >&lt;p&gt;I think we are trying solve the consequence instead of actual problem of adjusting max sstable size as jake pointed out, I think &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=vijay2win%40yahoo.com&quot; class=&quot;user-hover&quot; rel=&quot;vijay2win@yahoo.com&quot;&gt;vijay2win@yahoo.com&lt;/a&gt; was also doing so in production. Caching in the current state does the job for STCS and LCS with bigger files, expiring would be a good addition tho.&lt;/p&gt;</comment>
                            <comment id="13696233" author="jbellis" created="Sun, 30 Jun 2013 01:32:28 +0000"  >&lt;p&gt;As I explained, increasing sstable size helps but does not solve the problem; we&apos;re supposed to be supporting up to 5-10TB of data in 1.2.&lt;/p&gt;</comment>
                            <comment id="13696239" author="xedin" created="Sun, 30 Jun 2013 02:00:17 +0000"  >&lt;p&gt;What I am just trying to say is that expiring with global limit as good enough even for LCS with bigger files, but useless with 5MB besides all other problems. And as I pointed in on of the comment for 5-10 terabytes even with 128MB files are too small and affect system performance without taking into account indexing/bf overhead.&lt;/p&gt;</comment>
                            <comment id="13696259" author="jbellis" created="Sun, 30 Jun 2013 03:33:33 +0000"  >&lt;blockquote&gt;&lt;p&gt;What I am just trying to say is that expiring with global limit as good enough even for LCS with bigger files&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t see how that follows at all.  The expiring approach is broken at any dataset size where memory pressure is a problem, since in the worst case it will not evict quickly enough.&lt;/p&gt;</comment>
                            <comment id="13696270" author="xedin" created="Sun, 30 Jun 2013 04:44:43 +0000"  >&lt;p&gt;Right, that&apos;s what max memory size cap is for, to make eviction more intelligent in times of memory pressure, and concurrency as limited so as dataset grows there wouldn&apos;t be a lot if items in each the queue anyway.&lt;/p&gt;</comment>
                            <comment id="13701083" author="xedin" created="Fri, 5 Jul 2013 19:22:36 +0000"  >&lt;p&gt;Attached patch adds FileCacheService (+ metrics) which can expire instances after they are not accessed for time period and has a global memory usage limit (set to 512MB).&lt;/p&gt;</comment>
                            <comment id="13701480" author="jbellis" created="Sun, 7 Jul 2013 02:25:41 +0000"  >&lt;p&gt;I talked this over a bit with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ben.manes&quot; class=&quot;user-hover&quot; rel=&quot;ben.manes&quot;&gt;ben.manes&lt;/a&gt; (author of CLHM).  Here&apos;s his take:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;This&amp;#93;&lt;/span&gt; sounds less like a multimap cache than a multi-way object pool. To me a multimap cache would be like any other cache where entries are read frequently, rarely explicitly invalidated, and evicted by a boundary condition. An object pool has instances checked in and out, so care needs be taken to make sure the transfer overhead is cheap. I think you want a more advanced pool with global boundary conditions and is multi-way, so more complex than a traditional database connection pool. For that, actually, a few years ago I advised the author of BoneCP to use a LinkedTransferQueue to leverage elimination to avoid contention which provided the performance improvements to make his library the fastest available.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ben put together an implementation at &lt;a href=&quot;https://github.com/ben-manes/multiway-pool:&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/ben-manes/multiway-pool:&lt;/a&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;ul&gt;
	&lt;li&gt;A concurrent object pool that supports pooling multiple resources that are associated with a&lt;/li&gt;
	&lt;li&gt;single key. A resource is borrowed from the pool, used exclusively, and released back for reuse&lt;/li&gt;
	&lt;li&gt;by another caller. This implementation can optionally be bounded by a maximum size, time-to-live,&lt;/li&gt;
	&lt;li&gt;or time-to-idle policies.&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;A traditional object pool is homogeneous; all of the resources are identical in the data and&lt;/li&gt;
	&lt;li&gt;capabilities offered. For example a database connection pool to a shared database instance. A&lt;/li&gt;
	&lt;li&gt;multiway object pool is heterogeneous; resources may differ in the data and capabilities offered.&lt;/li&gt;
	&lt;li&gt;For example a flat file database may pool random access readers to the database table files. The&lt;/li&gt;
	&lt;li&gt;relationship of a single-way to a multi-way object pool is similar to that of a map to a&lt;/li&gt;
	&lt;li&gt;multimap.&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;When this pool is bounded any resource is eligible for eviction regardless of the key that it is&lt;/li&gt;
	&lt;li&gt;associated with. A size based bound will evict resources by a best-effort LRU policy and a time&lt;/li&gt;
	&lt;li&gt;based policy will evict by either a time-to-idle and/or time-to-live policy. The resource&apos;s life&lt;/li&gt;
	&lt;li&gt;cycle can be instrumented, such as when cleaning up after eviction, by using the appropriate&lt;/li&gt;
	&lt;li&gt;ResourceLifecycle method.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;</comment>
                            <comment id="13701490" author="xedin" created="Sun, 7 Jul 2013 03:15:00 +0000"  >&lt;p&gt;The only point I disagree with is &quot;rarely explicitly invalidated&quot;, this is not true especially with LCS and small files, that work already done for us by compaction, things we need to care are - expiry after access + total memory limit (as concurrency is also limited by read stage size so all of the queues are implicitly bounded). My implementation is fairly simple and takes advantage of all services provided by upper levels (compaction for eviction, read stage for concurrently limiting) as well as build-in guava cache compatibilities of expiring items and handling removals.&lt;/p&gt;</comment>
                            <comment id="13701495" author="ben.manes" created="Sun, 7 Jul 2013 03:35:30 +0000"  >&lt;p&gt;&quot;rarely explicitly invalidated&quot; is in regards to a cache, as Jonathan originally described the problem as a multimap cache instead of as an object pool. He also expressed concern with evicting a block of buffers at once when he conceived of the same model that you implemented.&lt;/p&gt;

&lt;p&gt;I am intimately familiar with Guava&apos;s cache as I designed the algorithms, ported and wrote code for it, and advised on the api. Unfortunately I am not familiar with Cassandra&apos;s needs and its code, so the pool was implemented based on a brief description of the problem and ideal behavior.&lt;/p&gt;

&lt;p&gt;It was a fun exercise for a long weekend. I&apos;d recommend writing tests and benchmarks, which unfortunately appears to be missing with the patch in its current form. Of couse use whatever makes the most sense.&lt;/p&gt;</comment>
                            <comment id="13701497" author="xedin" created="Sun, 7 Jul 2013 03:46:21 +0000"  >&lt;p&gt;It&apos;s more of object pool, where each key has limited number of &quot;equal&quot; objects, so if caller wants to read any portion of the file it would get an instance which represents whole file, seek to appropriate position and do reading, returning that instance to the pull when done. evicting block of buffers is still required when files are compacted out which would be more frequent than eviction by timer because use-cases usually don&apos;t drift in pattern. I also raised question about what I think is a major problem here - max size of a file being 5MB by default for CLS.&lt;/p&gt;</comment>
                            <comment id="13701501" author="jbellis" created="Sun, 7 Jul 2013 04:21:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;It&apos;s more of object pool, where each key has limited number of &quot;equal&quot; objects&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes.  This is what the javadoc means by &quot;supports pooling multiple resources that are associated with a single key.&quot;&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;evicting block of buffers is still required when files are compacted out&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Sure, and this is supported, but this is not a case of contention which was the concern I raised with Ben.&lt;/p&gt;</comment>
                            <comment id="13701503" author="jbellis" created="Sun, 7 Jul 2013 04:24:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;I also raised question about what I think is a major problem here - max size of a file being 5MB by default for CLS&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I mentioned that Daniel Meyer is working on this, but I&apos;ve formally opened &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-5727&quot; title=&quot;Evaluate default LCS sstable size&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-5727&quot;&gt;&lt;del&gt;CASSANDRA-5727&lt;/del&gt;&lt;/a&gt; in the hopes that that helps restrict our scope of discussion here. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13701505" author="xedin" created="Sun, 7 Jul 2013 05:00:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;Sure, and this is supported, but this is not a case of contention which was the concern I raised with Ben.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;What contention are we talking? If that is memory pressure, then expiring items in bulk is good and leaves room for new files. In case of LCS instances mostly are going to be invalidated by compaction vs access time expiry, STCS observes the same effect but better in terms of memory usage as smaller files are going to be compacted into bigger one so each compaction reduces number of instances and buffers. &lt;/p&gt;

&lt;p&gt;I think the original problem in here was that with default settings LCS eats a lot of memory because files are too small and even if minor portion of dataset is read memory, overhead for caching is still unacceptable, this is why I was talking about adding global memory cap and as a good bonus - expiry of unused instances, because if file is expired by timer, it means that reads have turned away from it or it&apos;s read in bursts so deallocating all of the cached instances is the way to go, where with LRU we would only replace when new instances are returned to the pull (as we don&apos;t pre-allocate) which could create a problem when there is a burst of requests to the same file after long internal of inactivity. But even after so many comments that we had here and people (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt;) reporting that even current setup works for them on bigger files, it still looks like the everybody is trying to solve different issues.&lt;/p&gt;</comment>
                            <comment id="13701511" author="jbellis" created="Sun, 7 Jul 2013 05:39:24 +0000"  >&lt;p&gt;Perhaps it will help if I recap:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;CRAR pooling + LCS OOMs us at relatively small amounts of data (20GB in Jeremiah&apos;s example)&lt;/li&gt;
	&lt;li&gt;worst case pool size proportional to sstables * concurrent readers means that memory usage increases linearly with disk size; increasing sstable size decreases it linearly, so we expect 100MB sstables &lt;span class=&quot;error&quot;&gt;&amp;#91;which is on the high side of what is reasonable IMO&amp;#93;&lt;/span&gt; to get us to ~4TB of space managed which is good but not &quot;problem solved&quot; territory.&lt;/li&gt;
	&lt;li&gt;thus, we need to bound CRAR pooling and not just say &quot;it&apos;s working as designed, go use larger sstables or STCS&quot; &lt;span class=&quot;error&quot;&gt;&amp;#91;although that is probably adequate for 1.2, so I am tagging this for 2.0&amp;#93;&lt;/span&gt;&lt;/li&gt;
	&lt;li&gt;The most common operations in an object pool are the borrow/return.  Ben&apos;s multiway pool optimizes for this, with particular attention to possible contention from multiple reader threads.&lt;/li&gt;
	&lt;li&gt;The multiway pool supports expiring objects after they have been idle for a given time, as well as a total pool size.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13701512" author="xedin" created="Sun, 7 Jul 2013 05:59:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;worst case pool size proportional to sstables * concurrent readers means that memory usage increases linearly with disk size; increasing sstable size decreases it linearly, so we expect 100MB sstables &lt;span class=&quot;error&quot;&gt;&amp;#91;which is on the high side of what is reasonable IMO&amp;#93;&lt;/span&gt; to get us to ~4TB of space managed which is good but not &quot;problem solved&quot; territory.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Let&apos;s clarify that it&apos;s a really-really worse case which means that your reads touch whole dataset all the time and it&apos;s a good show case for side affect of having fixed size files of small size so even if you don&apos;t do caching it would create allocation rate proportional to the dataset size as well as increased syscall rate to open/seek/close those files so it&apos;s frequent GC/FullGC vs OOM in here (when run without memory cap).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The most common operations in an object pool are the borrow/return. Ben&apos;s multiway pool optimizes for this, with particular attention to possible contention from multiple reader threads.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If I understand correctly by borrow you mean allocate object in the pool and give it way to the caller, this is not how I would like it to behave instead we can allocate object without assigning to the queue when it runs short (as all of the objects are short lived) and then decide if we want it back when caller it done with it, disadvantages of allocating to the pool I have already described in my previous comments.&lt;/p&gt;

&lt;p&gt;Bottom line for me being, I&apos;m tired of arguing about this so I will let mighty people to decide what they see fit as in all other cases (e.g. fadvic&apos;ing whole file on reads, preheating page cache etc.). &lt;/p&gt;
</comment>
                            <comment id="13701523" author="xedin" created="Sun, 7 Jul 2013 07:33:01 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ben.manes&quot; class=&quot;user-hover&quot; rel=&quot;ben.manes&quot;&gt;ben.manes&lt;/a&gt; I briefly looked through the code and I think there is an important component missing - we need to weight size of the cache based on internal structure of each object allocated into it (borrowed) as the biggest part would be in memory buffer that each instance holds, so simple number of entries wouldn&apos;t do...&lt;/p&gt;

&lt;p&gt;I&apos;m worried that getResourceHandler has to go through few queues and allocation, I wanted to avoid polling + atomic CAS on read path, which is most critical, as much as possible because it adds additional undesired latency especially to high cardinality requests.&lt;/p&gt;

&lt;p&gt;Also I don&apos;t think using just weak references to remove unused queues is a good idea (if I interpreted comment at the top correctly), we need something more agressive, because ParNew+CMS only processes those in FullGC phrase even when objects effectively die in young gen (sun/open jdk) and G1 doesn&apos;t even have any guarantees on when they are going to be processed, not mentioning that it requires double pass.&lt;/p&gt;</comment>
                            <comment id="13701530" author="ben.manes" created="Sun, 7 Jul 2013 08:38:52 +0000"  >&lt;p&gt;Weights are trivial to add and I wanted to avoid adding non-critical features without more details. In your patch, it appears assumed that every queue as a single entry with the same size buffer and privately Jonathan&apos;s description of the problem stated 128KB per CRAR. If the weight is constant than they are merely a convenience mapping as it really is the number of entries.&lt;/p&gt;

&lt;p&gt;Uncontended CAS is cheap, short lived allocations are trivial, and Doug Lea describes LTQ as lower overhead than CLQ (especially under load).&lt;/p&gt;

&lt;p&gt;The use of weak references was an easy way to avoid race conditions when flushing out the primary structure. It could be replaced with lazy clean-up passes, which is what I originally started with. At this point it seemed unwise to complicate things without more information so I simplified it. The number of queues is probably going to be quite small, on the order of dozens, so the reference cost in this case is quite small.&lt;/p&gt;

&lt;p&gt;You&apos;re trying to compare approaches, which is valid but better oriented towards discussing with Jonathan. The challenge presented to me is as described in the class&apos;s JavaDoc: an multiway object pool bounded by the total number of entries. I took a more general approach due to not knowing the trade-offs one could make with context to Cassandra&apos;s behavior.&lt;/p&gt;</comment>
                            <comment id="13701532" author="xedin" created="Sun, 7 Jul 2013 09:03:42 +0000"  >&lt;p&gt;I understand that you didn&apos;t account for any specific nuances, I just wanted to point those things out for subscribers of that ticket...&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In your patch, it appears assumed that every queue as a single entry with the same size buffer and privately Jonathan&apos;s description of the problem stated 128KB per CRAR. If the weight is constant than they are merely a convenience mapping as it really is the number of entries.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure if Jonathan wants to use it per CF or globally but different column families are going to have different buffer settings especially with CRAR, my patch accounts each queue buffer as being more or less a constant size but doesn&apos;t assume very key introduces the same overhead.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The number of queues is probably going to be quite small, on the order of dozens, so the reference cost in this case is quite small.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s actually going to be pretty high especially with LCS (default size of 5 MB), it creates 204 files for 1GB, so it&apos;s order of thousands without taking into account compaction process.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Uncontended CAS is cheap&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well if 8-32 threads (default number of threads) are going to request the same row from the same number of files, there could be contention especially it we throw LRU in the mix. But I&apos;m more concerned about 1 ms polling timeout in there tho... This is why I&apos;m concerned with tries to make cache first class citizen instead of lucky shot especially when we don&apos;t really need that.&lt;/p&gt;</comment>
                            <comment id="13701544" author="ben.manes" created="Sun, 7 Jul 2013 10:28:33 +0000"  >&lt;p&gt;I solved the LRU problem years ago, which gave you CLHM and Guava&apos;s Cache. It scales very well without degrading due to LRU management under higher thread count, limited primarily by the hash table usage. Previous approaches didn&apos;t scale to 4-8 threads, but 32+ is limited by the chosen hash table design.&lt;/p&gt;

&lt;p&gt;In neither approaches will there be significant contention or overhead. The difference is about the level of granularity to bound the resources by and how to evict them.&lt;/p&gt;

&lt;p&gt;You seem to be focusing on tuning parameters, minute details, etc. for a class written in a few evenings as a favor, knowing that those things are trivial to change. There&apos;s not much of a point debating it with me as I don&apos;t care and have no stake or interest in what is decided. Especially when you&apos;re comparing it against a simplistic usage relying on another class I wrote much of, Guava&apos;s. In the end something I wrote will be used to solve this bug. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;
</comment>
                            <comment id="13701637" author="xedin" created="Sun, 7 Jul 2013 19:17:52 +0000"  >&lt;p&gt;You got me wrong, I&apos;m not trying to debate anything with you and minute details are proven to be the hardest sometimes... I&apos;m just trying so say, to Jonathan most of all, if we go with borrowing approach and put cache in front of each row read we would suffer additional latency on every read (even if it&apos;s 1-3 ms, that adds to every file read on each request) of the row where people already report order of magnitude worse latencies on &amp;gt;= p99, am I right &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;P.S. am I sorry I forgot to bow before I darred to speak up to such master who solved all your problems and who&apos;s modesty doesn&apos;t know it&apos;s limits...&lt;/p&gt;</comment>
                            <comment id="13701644" author="jbellis" created="Sun, 7 Jul 2013 19:36:28 +0000"  >&lt;p&gt;I&apos;m confused.  We already have a borrowing approach, what part of the proposal do you see adding 1-3ms?&lt;/p&gt;</comment>
                            <comment id="13701652" author="xedin" created="Sun, 7 Jul 2013 19:55:08 +0000"  >&lt;p&gt;We don&apos;t borrow if there is no items in the cache, we just allocate new instance and then decide if we want to add it on recycle. 1-3 ms I&apos;m talking is in Ben&apos;s multiway pool in getResourceHandler has to poll few times (once of those from blocking queue with 1 ms timeout) + atomic CAS.&lt;/p&gt;</comment>
                            <comment id="13703051" author="xedin" created="Tue, 9 Jul 2013 08:28:43 +0000"  >&lt;p&gt;I think this discussion already outgrown proportions of the original problem, I want to suggest we try currently implemented queueing approach with expiry and maximum memory size cap as fix for problem related to memory usage (with LCS in particular) without introducing any additional complexity on getSegment (e.g. borrowing) at least for next 1.2 release. And in the meantime I&apos;m open for discussion of other ways of handling caching which could be borrowing and gradual control over each instance in the queue, like proposed multiway pool, without adding any significant overhead on such critical path as reading file segments.  &lt;/p&gt;</comment>
                            <comment id="13703478" author="jbellis" created="Tue, 9 Jul 2013 16:44:19 +0000"  >&lt;p&gt;As mentioned above (but apparently I didn&apos;t actually touch fixver, oops) I&apos;d rather just tweak default sstable size for 1.2.x and do a deeper fix in 2.0.  1.2 is 6 months old at this point and it seems like we have a pretty good workaround available without making deep code changes.  WDYT?&lt;/p&gt;</comment>
                            <comment id="13703656" author="xedin" created="Tue, 9 Jul 2013 19:04:26 +0000"  >&lt;p&gt;I&apos;m fine with that, I can work on a patch that would use multiway pool, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dmeyer&quot; class=&quot;user-hover&quot; rel=&quot;dmeyer&quot;&gt;dmeyer&lt;/a&gt; Will you be able to test and compare expiry + mem limit to multiway borrowing, once I&apos;m done with second patch, to check performance/latenties of reads and memory usage?&lt;/p&gt;</comment>
                            <comment id="13704045" author="xedin" created="Wed, 10 Jul 2013 00:38:47 +0000"  >&lt;p&gt;I have started working on integrating multiway pool and it looks like we have two problems:&lt;/p&gt;

&lt;p&gt;#1. As each SegmentedFile has to return unique instance using &quot;createReader(String)&quot; LoadingCache won&apos;t do for us, as we need get(K, Callable) per SSTableReader.&lt;br/&gt;
#2. as MultiWay returns a handle I changed RAR to have a setHandle method instead of passing SegmentedFile into constructor, which seems a bit hacky to me as we need to be careful in maintaining that relationship...&lt;/p&gt;

&lt;p&gt;I did some performance testing (with attached patch) where MultiwayPool allocated per instance because we can&apos;t specify loader in borrow(...) yet, which should be a best case for it, not in terms of memory usage but contention. I loaded 5,000,000 keys with following stress command (./tools/bin/cassandra-stress -n 5000000 -S 512 -C 20 -Z LeveledCompactionStrategy) for initial data and then I made it run in a loop and was doing reads in parallel.&lt;/p&gt;

&lt;p&gt;With writes:&lt;/p&gt;

&lt;p&gt;Average read performance for MultiwayPool: median 6.2, 95th 11.4, 99.9th 78.8 &lt;br/&gt;
Average read performance for FileCacheService: median: 5.3, 95th 9.6, 99.9th 73.1&lt;/p&gt;

&lt;p&gt;No writes, no compaction:&lt;/p&gt;

&lt;p&gt;Average read performance for MultiwayPool: median 2.3, 95th 3.2, 99.9th 21.3 &lt;br/&gt;
Average read performance for FileCacheService: median: 1.7, 95th 2.9, 99.9th 19.2&lt;/p&gt;

&lt;p&gt;I tried doing range_slice but due to timeouts I couldn&apos;t really complete test on any of the implementations, median latenties on average different by 3-4 ms.&lt;/p&gt;

&lt;p&gt;Edit: I forgot to mention that I hardcoded maxSize per MultiwayPool instance which was fine for that test, but we really need a way to weight items if we are going to use it globally.&lt;/p&gt;</comment>
                            <comment id="13704086" author="ben.manes" created="Wed, 10 Jul 2013 01:29:11 +0000"  >&lt;p&gt;I think part of the problem is that idle caching is not overly efficient in this version. That can be improved upon, but maximum size might be better to verify as a baseline with first. &lt;/p&gt;

&lt;p&gt;Weights are supported as of the 7th.&lt;/p&gt;</comment>
                            <comment id="13704109" author="xedin" created="Wed, 10 Jul 2013 01:51:40 +0000"  >&lt;p&gt;Good to know that weight is supported now, i must have overlooked it... Anyhow since we can&apos;t make pool global yet maxSize was good baseline, as you mentioned. &lt;/p&gt;</comment>
                            <comment id="13707423" author="jbellis" created="Fri, 12 Jul 2013 21:50:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;As each SegmentedFile has to return unique instance using &quot;createReader(String)&quot; LoadingCache won&apos;t do for us, as we need get(K, Callable) per SSTableReader.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Ben has added a borrow(K, Callable) method.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;as MultiWay returns a handle I changed RAR to have a setHandle method instead of passing SegmentedFile into constructor, which seems a bit hacky to me as we need to be careful in maintaining that relationship&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Can you elaborate?&lt;/p&gt;</comment>
                            <comment id="13707652" author="ben.manes" created="Sat, 13 Jul 2013 04:16:26 +0000"  >&lt;p&gt;I rewrote the time-to-idle policy, so it should be faster when enabled. &lt;/p&gt;

&lt;p&gt;Details (if interested)&lt;br/&gt;
------------------------&lt;br/&gt;
For prototyping purposes, I previously used a secondary Guava Cache to track idle resources. Unlike a cache&apos;s time-to-idle, which is reset when an entry is read, an object pool&apos;s concept of idle time is when a resource resides unused and ready to be borrowed. The use of a secondary Guava Cache meant that the resource had to be added and removed frequently, resulting in locking on the hashtable segments and incurring other maintenance overhead.&lt;/p&gt;

&lt;p&gt;In Guava&apos;s cache we observed that expiration policies mirrored maximum size policies, but time based. Thus time-to-live is a FIFO queue and time-to-idle is an LRU queue. That let us leverage the amortization technique in CLHM to be used for expiration with O(1) reorder costs.&lt;/p&gt;

&lt;p&gt;The new implementation strips off the unnecessary work by maintaining a time ordered queue that only supports adds and removals. For our definition of idle there is no need to reorder so it is effectively a FIFO. A tryLock guards the policy operations, draining a queue of pending operations if acquired. I decided to allow this to be proactively drained whenever possible, though if we see a need then we can buffer the operations for longer like the caches do.&lt;/p&gt;</comment>
                            <comment id="13707659" author="xedin" created="Sat, 13 Jul 2013 05:27:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;Can you elaborate?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Please that a look to v2 patch PoolingSegmentFile.getSegment() method, I have added comment about that.&lt;/p&gt;

&lt;p&gt;v2 patch makes cache global and adds per borrow loading of missing items. The problem I encountered is that system is unable to start because of the following error:&lt;/p&gt;

&lt;p&gt;I&apos;m not sure if I&apos;m doing something wrong or it&apos;s a bug in multiway pool&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; INFO 22:19:07,254 Opening /var/lib/cassandra/data/system/local/system-local-ja-21 (520 bytes)
ERROR 22:19:07,298 Exception encountered during startup
com.google.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2258)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3990)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4793)
	at com.github.benmanes.multiway.TransferPool.newResourceHandle(TransferPool.java:206)
	at com.github.benmanes.multiway.TransferPool.tryToGetResourceHandle(TransferPool.java:186)
	at com.github.benmanes.multiway.TransferPool.getResourceHandle(TransferPool.java:167)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:152)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:143)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:64)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1040)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.createFileDataInput(SSTableNamesIterator.java:96)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:109)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.&amp;lt;init&amp;gt;(SSTableNamesIterator.java:62)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:87)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:124)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1458)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1284)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:55)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:227)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:245)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:154)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:456)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:237)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Caused by: java.lang.NullPointerException
	at com.github.benmanes.multiway.TransferPool$2.call(TransferPool.java:209)
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4796)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3589)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2374)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2337)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2252)
	... 28 more
com.google.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2258)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3990)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4793)
	at com.github.benmanes.multiway.TransferPool.newResourceHandle(TransferPool.java:206)
	at com.github.benmanes.multiway.TransferPool.tryToGetResourceHandle(TransferPool.java:186)
	at com.github.benmanes.multiway.TransferPool.getResourceHandle(TransferPool.java:167)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:152)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:143)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:64)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1040)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.createFileDataInput(SSTableNamesIterator.java:96)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:109)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.&amp;lt;init&amp;gt;(SSTableNamesIterator.java:62)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:87)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:124)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1458)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1284)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:55)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:227)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:245)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:154)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:456)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:237)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Caused by: java.lang.NullPointerException
	at com.github.benmanes.multiway.TransferPool$2.call(TransferPool.java:209)
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4796)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3589)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2374)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2337)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2252)
	... 28 more
Exception encountered during startup: java.lang.NullPointerException
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13707660" author="xedin" created="Sat, 13 Jul 2013 05:33:42 +0000"  >&lt;p&gt;v2 updated with a small thing - added recordStats() to the builder so we have statistics information available on demand. Exception listed above still accours.&lt;/p&gt;</comment>
                            <comment id="13707667" author="ben.manes" created="Sat, 13 Jul 2013 06:12:30 +0000"  >&lt;p&gt;I think I just fixed this issue in my last push. Sorry I didn&apos;t check my email earlier, as I found it when writing more test cases. The problem is that I forgot to default the lifecycle to a discarding instance if not used, after I made it an optional setting.&lt;/p&gt;</comment>
                            <comment id="13707669" author="xedin" created="Sat, 13 Jul 2013 06:26:35 +0000"  >&lt;p&gt;I have pulled/rebuild your code and now there is another error:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt; INFO 23:24:33,367 Opening /var/lib/cassandra/data/system/local/system-local-ja-21 (520 bytes)
ERROR 23:24:33,412 Exception encountered during startup
java.lang.ClassCastException: com.github.benmanes.multiway.ResourceKey$LinkedResourceKey cannot be cast to java.lang.String
	at org.apache.cassandra.io.util.PoolingSegmentedFile$1.weigh(PoolingSegmentedFile.java:35)
	at com.google.common.cache.LocalCache$Segment.setValue(LocalCache.java:2219)
	at com.google.common.cache.LocalCache$Segment.storeLoadedValue(LocalCache.java:3196)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2375)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2337)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2252)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3990)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4793)
	at com.github.benmanes.multiway.TransferPool.newResourceHandle(TransferPool.java:215)
	at com.github.benmanes.multiway.TransferPool.tryToGetResourceHandle(TransferPool.java:195)
	at com.github.benmanes.multiway.TransferPool.getResourceHandle(TransferPool.java:176)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:156)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:147)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:65)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1040)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.createFileDataInput(SSTableNamesIterator.java:96)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:109)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.&amp;lt;init&amp;gt;(SSTableNamesIterator.java:62)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:87)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:124)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1458)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1284)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:55)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:227)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:245)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:154)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:456)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:237)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13707672" author="ben.manes" created="Sat, 13 Jul 2013 06:44:18 +0000"  >&lt;p&gt;okay, fixed. thanks for catching this. The tests no longer use raw keys, which should catch this from occurring again.&lt;/p&gt;</comment>
                            <comment id="13707684" author="xedin" created="Sat, 13 Jul 2013 07:31:17 +0000"  >&lt;p&gt;Thanks, it worked that time, I did a quick test on local machine with global cache backed by MultiwayPool.&lt;/p&gt;

&lt;p&gt;MultiwayPool average run: median 3.1, 95th 4.7, 99.9th 21.9&lt;br/&gt;
FileCacheService (+ LinkedTranferQueue) average run: median 2.4, 95th 3.0, 99.9th 17.1&lt;/p&gt;

&lt;p&gt;It&apos;s the same setup that I used in my previous test with no writes and no compaction.&lt;/p&gt;

&lt;p&gt;I will try to experiment with ArrayBlockingQueue as we know upper bound on concurrency and update my FileCacheService patch with either of them (LTQ vs. ABQ) soon.&lt;/p&gt;</comment>
                            <comment id="13707689" author="ben.manes" created="Sat, 13 Jul 2013 07:49:30 +0000"  >&lt;p&gt;LTQ is best when you allow there to be some spin between producers and consumers, as its optimized for message passing scenarios. In your usage you don&apos;t allow any delay, so the likelihood of a successful transfer is low. When transfers are common, the overhead is less due to fewer contented CAS operations.&lt;/p&gt;

&lt;p&gt;If desired, I can make the pool parameterized to take a supplier of queues to produce so you can parameterize that as well.&lt;/p&gt;

&lt;p&gt;The pool will always be slower than the FileCacheService patch, since it does more. The decision is whether the performance degradation is acceptable and if the rational for the pool is to provide a finer grained eviction policy is still desired.&lt;/p&gt;</comment>
                            <comment id="13707694" author="xedin" created="Sat, 13 Jul 2013 08:07:59 +0000"  >&lt;p&gt;I understand what is ideal use-case for LTQ is, I wanted to try it out since it was mentioned couple of times to have better results than CLQ under load.&lt;/p&gt;

&lt;p&gt;I expected FileCacheService to be faster, I was just trying to check how much latency it actually adds even on such synthetic scenario as stress with no writes nor compaction. I strongly think (and I explained why multiple times) that we can&apos;t allow any degradation more than 0.5 ms in percentile on such critical path and why expiring items in bulk is okey for us since, in steady state, eviction would be driven by compactions cleaning all of open file descriptors per compacted sstable, where timed expiry would be very infrequent as read pattern doesn&apos;t change frequently in production systems.&lt;/p&gt;</comment>
                            <comment id="13707974" author="ben.manes" created="Sun, 14 Jul 2013 06:58:45 +0000"  >&lt;p&gt;In a simple single-threaded benchmark, LTQ is relatively on par within the object pool.&lt;/p&gt;

&lt;p&gt;Currently I have a finalizer on the handle as a safety net, both to catch my bugs and usage mistakes. This includes a note on the performance impact, which appears to have add 2.5x overhead. I had intended to replace this with phantom references instead, though now I&apos;m wondering if I should not put any safety net in whatsoever.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Finalizer&lt;br/&gt;
queueType  ns linear runtime&lt;br/&gt;
      ABQ 489 =========================&lt;br/&gt;
      SAQ 545 ============================&lt;br/&gt;
      CLQ 535 ===========================&lt;br/&gt;
      LBQ 578 ==============================&lt;br/&gt;
      LTQ 555 ============================&lt;br/&gt;
      LBD 490 =========================&lt;/li&gt;
&lt;/ol&gt;


&lt;ol&gt;
	&lt;li&gt;No finalizer&lt;br/&gt;
queueType  ns linear runtime&lt;br/&gt;
      ABQ 176 =========================&lt;br/&gt;
      SAQ 159 ======================&lt;br/&gt;
      CLQ 166 =======================&lt;br/&gt;
      LBQ 210 ==============================&lt;br/&gt;
      LTQ 183 ==========================&lt;br/&gt;
      LBD 181 =========================&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="13707988" author="xedin" created="Sun, 14 Jul 2013 08:48:50 +0000"  >&lt;p&gt;Finalizers are never a good idea, because it requires double pass though GC and secondary actual finalization processing is single threaded with global queue, all of which creates additional CPU overhead as well as memory pressure. Also as as far as I remember CMS/G1 both process all types of references on FullGC phase even if refs to underlying objects where lost in young gen they are still promoted to old gen.&lt;/p&gt;</comment>
                            <comment id="13707990" author="ben.manes" created="Sun, 14 Jul 2013 08:59:17 +0000"  >&lt;p&gt;yes, I understand that and that was documented. It was correct to add it early on, due to prototyping to help catch my bugs if there were race conditions. When looking at performance then it became appropriate to remove it as tests have baked the code.&lt;/p&gt;

&lt;p&gt;The only aspect I&apos;m grudgingly punting on is that I prefer warning developers when they have resource leaks, when possible without overhead, instead of silently letting production environments crash. This can be done with phantom references, but I dislike having libraries spawn its own threads (e.g. MapMaker did) and prefer amortizing it (e.g. CacheBuilder). There&apos;s no free hook in my pool to tie into, so I&apos;m not providing that warning given you don&apos;t need it atm.&lt;/p&gt;</comment>
                            <comment id="13707991" author="ben.manes" created="Sun, 14 Jul 2013 09:01:55 +0000"  >&lt;p&gt;anyways, this is now removed so hopefully your performance tests will see a favorable impact like mine do.&lt;/p&gt;</comment>
                            <comment id="13707992" author="xedin" created="Sun, 14 Jul 2013 09:52:09 +0000"  >&lt;p&gt;As default queue size is set to 512MB the tests I did weren&apos;t actually using expiry on replacement (as there were no compaction nor dataset was been enough) so I shouldn&apos;t have touched the finalization path. I re-run with updated MultiwayPool code and saw the same results as previously stated in the steady state (which is expected), but when I add compaction in the mix (by overwriting data), latencies become shaky with MultiwayPool most notably on 99.9th percentile where random spikes by 10-15 ms are observed, FileCacheService 99.9th stays almost constant.&lt;/p&gt;</comment>
                            <comment id="13708114" author="ben.manes" created="Sun, 14 Jul 2013 20:11:40 +0000"  >&lt;p&gt;Can you test without time-to-idle? Most likely there are bursts of expirations and the penalty is now better spread out. &lt;/p&gt;</comment>
                            <comment id="13708209" author="ben.manes" created="Mon, 15 Jul 2013 02:39:01 +0000"  >&lt;p&gt;Profiled to reduce allocations, cutting out about 10ms in a caliper benchmark. The dominating factor in a profile are reads from the cache.&lt;/p&gt;</comment>
                            <comment id="13714389" author="ben.manes" created="Sat, 20 Jul 2013 09:00:42 +0000"  >&lt;p&gt;I replaced the external handle with directly providing the resource and maintaining the association in a threadlocal. This should better match your usage and resolve your concern above.&lt;/p&gt;

&lt;p&gt;The primary motivation was to reduce object churn, as a handle was created per borrow. This reduced the hot spot time from an average invocation time of 1001us to 704us, when summing up the worst offenders.&lt;/p&gt;

&lt;p&gt;This may remove the random spiked that you observed if they were caused by garbage collection.&lt;/p&gt;

&lt;p&gt;98% of the overhead is now due to usage of other collections (Guava&apos;s Cache, LTQ, CLQ).&lt;/p&gt;</comment>
                            <comment id="13714403" author="ben.manes" created="Sat, 20 Jul 2013 10:41:24 +0000"  >&lt;p&gt;Switching from LTQ to a custom elimination backoff stack appears to have dropped the 98% to 179us. The single threaded benchmark improves by 30ns. A significant gain was also observed when using an EBS instead of an array of CLQs in the time-to-idle policy.&lt;/p&gt;

&lt;p&gt;I&apos;m surprised by how much of a gain occurs, so I&apos;ll have to experiment further to understand if its factual. LTQ/CLQ are hindered by having to honor FIFO with j.u.c. interfaces, and LIFO elimination is the ideal strategy for an object pool. The more frequently successful exchanges may reduce down to eden-space GC, resulting in major net wins. That, or I&apos;m prematurely believing that its working correctly.&lt;/p&gt;</comment>
                            <comment id="13714539" author="xedin" created="Sat, 20 Jul 2013 20:51:54 +0000"  >&lt;p&gt;attaching patch with supports the latest version of multiway pool (and latest trunk) and adds (C)RAR deallocation on object removal. Sorry I didn&apos;t run the test sooner, I was quiet busy with other things... So I see that with this version 95-99.9 have degraded comparing to previous by ~3 ms but they don&apos;t shake as they used too - it&apos;s around ~5 ms now (I&apos;m testing with expireAfterAccess). &lt;/p&gt;</comment>
                            <comment id="13714543" author="ben.manes" created="Sat, 20 Jul 2013 21:03:34 +0000"  >&lt;p&gt;Thanks Pavel.&lt;/p&gt;

&lt;p&gt;I&apos;m not sure why it got worse recently, except that you did turn on recordStats() in the last few runs. That can incur significant overhead by maintaining multiple LongAdders. Since you did not turn it on in the FileCache patch, which would provide similar stats, it may be an unfair comparison.&lt;/p&gt;

&lt;p&gt;I&apos;ll try to wrap up my EBS prototype and push those changes soon. Those aren&apos;t on github yet.&lt;/p&gt;</comment>
                            <comment id="13714550" author="xedin" created="Sat, 20 Jul 2013 22:14:40 +0000"  >&lt;p&gt;Maybe that&apos;s side effect of theadlocals, i am not sure. We need recordStats() to expose some via JMX, FileCache uses separate metrics service as we track explicitly every get/release which are exposed via JMX the same way as the rest of Cassandra, i didn&apos;t do that for multiway yet but that would be a final step.&lt;/p&gt;

&lt;p&gt;Edit: I have also added onRemoval listener with the latest patch which does syscall to close the file among other things, which could have affected borrow/release latencies somehow.&lt;/p&gt;</comment>
                            <comment id="13714610" author="ben.manes" created="Sun, 21 Jul 2013 02:31:14 +0000"  >&lt;p&gt;I was able to reduce the EBS version down to 120us. I probably won&apos;t have it on github until tomorrow, though.&lt;/p&gt;</comment>
                            <comment id="13714636" author="xedin" created="Sun, 21 Jul 2013 04:42:42 +0000"  >&lt;p&gt;Ok, let me know and I will try to test it tomorrow.&lt;/p&gt;</comment>
                            <comment id="13714640" author="xedin" created="Sun, 21 Jul 2013 04:50:21 +0000"  >&lt;p&gt;this is updated patch which actually includes onRemoval callback.&lt;/p&gt;</comment>
                            <comment id="13714921" author="ben.manes" created="Mon, 22 Jul 2013 05:28:48 +0000"  >&lt;p&gt;Since the EBS version is still in progress, the code is shared below. It uses a treiber stack with backoff to an elimination array, mixing in optimizations borrowed from j.u.c.Exchanger. It performs superior to the queues by not to honor FIFO ordering, making cancellation easy to achieve.&lt;/p&gt;

&lt;p&gt;While all tests pass, I think that the time-to-idle policy is corrupted as it assumed fifo ordering. I can make it tolerant of running out-of-order. I may try writing an elimination queue (LTQ uses a dual queue design).&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/ben-manes/multiway-pool/tree/elimination&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/ben-manes/multiway-pool/tree/elimination&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13733587" author="jbellis" created="Thu, 8 Aug 2013 15:17:45 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=xedin&quot; class=&quot;user-hover&quot; rel=&quot;xedin&quot;&gt;xedin&lt;/a&gt;, have you had a chance to get back to this?&lt;/p&gt;</comment>
                            <comment id="13733815" author="xedin" created="Thu, 8 Aug 2013 18:34:51 +0000"  >&lt;p&gt;I was actually destructed with other things and kind of waiting until code is done so I can test the version of multipool with all of the changes.&lt;/p&gt;</comment>
                            <comment id="13733830" author="ben.manes" created="Thu, 8 Aug 2013 18:50:26 +0000"  >&lt;p&gt;My benchmark had a bug and EBS may only be on par with LTQ performance wise. I need to investigate that again, though.&lt;/p&gt;

&lt;p&gt;I shifted focus to fixing the performance bottleneck in Guava&apos;s cache. The way we tracked usage history (e.g. LRU) was focused on common usage, but is a bottleneck on synthetic benchmarks. I made the fixes to CLHM (v1.4) and offered them upstream (issue 1487). I&apos;ll experiment with using CLHM instead to see if that removes the hotspot.&lt;/p&gt;</comment>
                            <comment id="13735727" author="ben.manes" created="Sat, 10 Aug 2013 04:02:07 +0000"  >&lt;p&gt;EBS is 28% faster than LTQ. There might be opportunities to make it slightly faster with some tuning. I could probably add blocking methods if there was interest in experimenting with it for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-4718&quot; title=&quot;More-efficient ExecutorService for improved throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-4718&quot;&gt;&lt;del&gt;CASSANDRA-4718&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="13735753" author="xedin" created="Sat, 10 Aug 2013 05:01:24 +0000"  >&lt;p&gt;Sounds good, let me know when it&apos;s available so I can run stress tests again.&lt;/p&gt;</comment>
                            <comment id="13753423" author="ben.manes" created="Thu, 29 Aug 2013 08:27:48 +0000"  >&lt;p&gt;Sorry that I haven&apos;t had the time to work on this for a while. I&apos;ve been playing with writing a queue using a combining arena, similar to how the stack has an elimination arena, and how to incorporate thread locals to reduce contention. That made me think about the flat combining technique, so after a little digging I uncovered a conversation I had with Chris Vest. At the time he was starting on an object pool, which he&apos;s released as Stormpot (&lt;a href=&quot;http://chrisvest.github.io/stormpot&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://chrisvest.github.io/stormpot&lt;/a&gt;). The implementation has some excellent ideas that are worth borrowing and mixing in. While it is not multi-way, he might be inclined to add that capability after playing with my prototype.&lt;/p&gt;</comment>
                            <comment id="13770046" author="jbellis" created="Tue, 17 Sep 2013 21:58:11 +0000"  >&lt;p&gt;We&apos;re now beginning 2.0&apos;s &quot;stabilization&quot; period, so even if Ben&apos;s multiway cache were available tomorrow I&apos;d be uneasy about pushing it out to our &quot;stable&quot; branch.&lt;/p&gt;

&lt;p&gt;So I&apos;ve committed Pavel&apos;s original queue-based cache with light revisions (primarily adding a conf setting to allow overriding the size).&lt;/p&gt;

&lt;p&gt;Longer term I think &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6045&quot; title=&quot;Compressed block cache&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6045&quot;&gt;&lt;del&gt;CASSANDRA-6045&lt;/del&gt;&lt;/a&gt; may be a better approach overall.&lt;/p&gt;</comment>
                            <comment id="13770072" author="ben.manes" created="Tue, 17 Sep 2013 22:19:51 +0000"  >&lt;p&gt;sounds good to me.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                                                <inwardlinks description="is duplicated by">
                                        <issuelink>
            <issuekey id="12673613">CASSANDRA-6191</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12593385" name="CASSANDRA-5661-global-multiway-cache.patch" size="4685" author="xedin" created="Sun, 21 Jul 2013 04:50:21 +0000"/>
                            <attachment id="12591034" name="CASSANDRA-5661.patch" size="10158" author="xedin" created="Fri, 5 Jul 2013 19:22:36 +0000"/>
                            <attachment id="12589788" name="DominatorTree.png" size="277427" author="jjordan" created="Wed, 26 Jun 2013 19:50:37 +0000"/>
                            <attachment id="12589787" name="Histogram.png" size="44898" author="jjordan" created="Wed, 26 Jun 2013 19:50:37 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[xedin]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>333979</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 10 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1lm2f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>334305</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>jbellis</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[jbellis]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311124" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Tester</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>dmeyer</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                        </customfields>
    </item>
</channel>
</rss>