<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:59:30 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-7217] Native transport performance (with cassandra-stress) drops precipitously past around 1000 threads</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-7217</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;This is obviously bad. Let&apos;s figure out why it&apos;s happening and put a stop to it.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12713898">CASSANDRA-7217</key>
            <summary>Native transport performance (with cassandra-stress) drops precipitously past around 1000 threads</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="aweisberg">Ariel Weisberg</assignee>
                                    <reporter username="benedict">Benedict Elliott Smith</reporter>
                        <labels>
                            <label>performance</label>
                            <label>stress</label>
                            <label>triaged</label>
                    </labels>
                <created>Tue, 13 May 2014 13:37:20 +0000</created>
                <updated>Wed, 15 Oct 2025 09:49:00 +0000</updated>
                            <resolved>Wed, 25 Nov 2015 14:05:24 +0000</resolved>
                                        <fixVersion>3.0.1</fixVersion>
                    <fixVersion>3.1</fixVersion>
                                    <component>Legacy/Tools</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="13996595" author="jasobrown" created="Tue, 13 May 2014 16:42:32 +0000"  >&lt;p&gt;Do you think this is a problem on the stress side, or on the server side? Do you see a problem with thrift? Lastly, should I assume this arose due to testing your changes on &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-4718&quot; title=&quot;More-efficient ExecutorService for improved throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-4718&quot;&gt;&lt;del&gt;CASSANDRA-4718&lt;/del&gt;&lt;/a&gt;?&lt;/p&gt;</comment>
                            <comment id="13997409" author="benedict" created="Wed, 14 May 2014 09:03:31 +0000"  >&lt;p&gt;I haven&apos;t investigated much at all, so I don&apos;t know the answer to any of these questions yet (except that it did indeed come about off the back of &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-4718&quot; title=&quot;More-efficient ExecutorService for improved throughput&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-4718&quot;&gt;&lt;del&gt;CASSANDRA-4718&lt;/del&gt;&lt;/a&gt;). The only thing I can say for sure is that it is unrelated to MaxRPC (i.e. nothing to do with native transport threads blocking on adding to the work queue).&lt;/p&gt;</comment>
                            <comment id="14192917" author="jbellis" created="Sat, 1 Nov 2014 02:19:03 +0000"  >&lt;p&gt;Ryan, can your team see how reproducible this is?&lt;/p&gt;</comment>
                            <comment id="14200604" author="shawn.kumar" created="Thu, 6 Nov 2014 18:30:50 +0000"  >&lt;p&gt;I&apos;ll be continuing testing on a more cpu-perfomant instance but thought I would briefly try the cstar_perf on bdplab. &lt;a href=&quot;http://cstar.datastax.com/graph?stats=dd73c4a6-65d9-11e4-9413-bc764e04482c&amp;amp;metric=op_rate&amp;amp;operation=1_write&amp;amp;smoothing=1&amp;amp;show_aggregates=true&amp;amp;xmin=0&amp;amp;xmax=279.07&amp;amp;ymin=0&amp;amp;ymax=120665.6&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;Here&lt;/a&gt; are the results - I increase the threads from 500 - 1500 in 250 thread increments from the first operation to the last (ie. 1_write to 5_write) and it seems like there is a noticeable drop in performance especially around 1250 threads.&lt;/p&gt;</comment>
                            <comment id="15002303" author="aweisberg" created="Thu, 12 Nov 2015 16:05:09 +0000"  >&lt;p&gt;I was able to reproduce this running the server on my OS X laptop and the client on my quad-core i5 Sandy Bridge Linux desktop.&lt;/p&gt;

&lt;p&gt;With 500 threads I was getting 80k op/sec and with 2000 I was getting 30k op/sec.&lt;/p&gt;

&lt;p&gt;I took flight recordings, but they are too big to attach and don&apos;t look at that interesting. There is more contention detected with a 1 millisecond threshold at 500 threads then at 2000 threads presumably because with 500 threads so much more work is getting done.&lt;/p&gt;

&lt;p&gt;CPU utilization at the client is pretty high at 500 threads, above 300%. 18k interrupts/second and 140k context switches/second.&lt;/p&gt;

&lt;p&gt;With 2000 threads utilization is lower more towards 250% with closer to 10k interrupts/second, but 250-300k context switches/second.&lt;/p&gt;

&lt;p&gt;My hypothesis is that having so many client threads is a problem for the Netty threads because there are more client threads than event threads by a large margin. With only one server there would really only be one since there is a single connection.&lt;/p&gt;

&lt;p&gt;In cstar on bdplab I see a sharp drop between 1000 and 1250 threads. I would have expected a graceful slope as the overhead of context switching threads (and their cache footprint) increases so there is still more to be explained.&lt;/p&gt;</comment>
                            <comment id="15003017" author="aweisberg" created="Thu, 12 Nov 2015 21:58:14 +0000"  >&lt;p&gt;Performance counters&lt;/p&gt;

&lt;p&gt;2000 threads&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Results:
op rate                   : 20576 [WRITE:20576]
partition rate            : 20576 [WRITE:20576]
row rate                  : 20576 [WRITE:20576]
latency mean              : 97.2 [WRITE:97.2]
latency median            : 91.0 [WRITE:91.0]
latency 95th percentile   : 179.1 [WRITE:179.1]
latency 99th percentile   : 268.3 [WRITE:268.3]
latency 99.9th percentile : 499.0 [WRITE:499.0]
latency max               : 1123.2 [WRITE:1123.2]
Total partitions          : 19000000 [WRITE:19000000]
Total errors              : 0 [WRITE:0]
total gc count            : 0
total gc mb               : 0
total gc time (s)         : 0
avg gc time(ms)           : NaN
stdev gc time(ms)         : 0
Total operation time      : 00:15:23
END

 Performance counter stats &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&apos;./cassandra-stress write n=19000000 -rate threads=2000 -mode &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt; cql3 -node 192.168.1.9&apos;&lt;/span&gt;:

 3,236,123,141,155      cycles                    #    2.115 GHz                     [16.14%]
 2,580,132,815,701      instructions              #    0.80  insns per cycle        
                                                  #    0.89  stalled cycles per insn [21.45%]
    63,994,020,523      cache-references          #   41.828 M/sec                   [26.72%]
    12,523,946,172      cache-misses              #   19.570 % of all cache refs     [32.00%]
 2,294,356,584,027      idle-cycles-frontend      #   70.90% frontend cycles idle    [37.28%]
 1,636,932,476,246      idle-cycles-backend       #   50.58% backend  cycles idle    [42.54%]
    1529337.521837      cpu-clock (msec)                                            
    1529938.883184      task-clock (msec)         #    1.635 CPUs utilized          
           129,217      page-faults               #    0.084 K/sec                  
        87,687,956      cs                        #    0.057 M/sec                  
        36,591,482      migrations                #    0.024 M/sec                  
           129,132      minor-faults              #    0.084 K/sec                  
   360,467,544,173      branch-instructions       #  235.609 M/sec                   [47.81%]
     5,205,849,494      branch-misses             #    1.44% of all branches         [47.76%]
    67,636,847,959      L1-dcache-load-misses     #   44.209 M/sec                   [47.83%]
    24,113,350,939      L1-dcache-store-misses    #   15.761 M/sec                   [47.94%]
    18,928,905,359      L1-dcache-prefetch-misses #   12.372 M/sec                   [42.84%]
    56,721,903,854      L1-icache-load-misses     #   37.075 M/sec                   [42.94%]
     3,977,754,938      dTLB-load-misses          #    2.600 M/sec                   [42.96%]
       748,817,996      dTLB-store-misses         #    0.489 M/sec                   [42.93%]
       791,352,271      iTLB-load-misses          #    0.517 M/sec                   [42.86%]
     5,414,521,445      branch-load-misses        #    3.539 M/sec                   [42.80%]
    37,275,666,810      LLC-loads                 #   24.364 M/sec                   [42.83%]
    10,226,436,059      LLC-stores                #    6.684 M/sec                   [42.80%]
    16,548,689,552      LLC-prefetches            #   10.817 M/sec                   [10.57%]

     935.835191719 seconds time elapsed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;500 threads&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Results:
op rate                   : 63563 [WRITE:63563]
partition rate            : 63563 [WRITE:63563]
row rate                  : 63563 [WRITE:63563]
latency mean              : 7.9 [WRITE:7.9]
latency median            : 5.8 [WRITE:5.8]
latency 95th percentile   : 16.2 [WRITE:16.2]
latency 99th percentile   : 36.3 [WRITE:36.3]
latency 99.9th percentile : 74.0 [WRITE:74.0]
latency max               : 422.0 [WRITE:422.0]
Total partitions          : 19000000 [WRITE:19000000]
Total errors              : 0 [WRITE:0]
total gc count            : 0
total gc mb               : 0
total gc time (s)         : 0
avg gc time(ms)           : NaN
stdev gc time(ms)         : 0
Total operation time      : 00:04:58
END

 Performance counter stats &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&apos;./cassandra-stress write n=19000000 -rate threads=500 -mode &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt; cql3 -node 192.168.1.9&apos;&lt;/span&gt;:

 1,967,800,644,333      cycles                    #    2.424 GHz                     [16.23%]
 1,939,192,725,937      instructions              #    0.99  insns per cycle        
                                                  #    0.67  stalled cycles per insn [21.56%]
    29,961,702,909      cache-references          #   36.915 M/sec                   [26.87%]
     7,138,097,546      cache-misses              #   23.824 % of all cache refs     [32.16%]
 1,290,923,581,701      idle-cycles-frontend      #   65.60% frontend cycles idle    [37.44%]
   827,710,334,443      idle-cycles-backend       #   42.06% backend  cycles idle    [42.67%]
     811637.475308      cpu-clock (msec)                                            
     811646.201981      task-clock (msec)         #    2.618 CPUs utilized          
            79,867      page-faults               #    0.098 K/sec                  
        34,954,827      cs                        #    0.043 M/sec                  
         1,803,328      migrations                #    0.002 M/sec                  
            79,531      minor-faults              #    0.098 K/sec                  
   216,302,396,604      branch-instructions       #  266.498 M/sec                   [47.89%]
     2,293,191,606      branch-misses             #    1.06% of all branches         [47.75%]
    36,684,160,264      L1-dcache-load-misses     #   45.197 M/sec                   [47.69%]
    15,585,249,129      L1-dcache-store-misses    #   19.202 M/sec                   [47.62%]
    14,137,121,831      L1-dcache-prefetch-misses #   17.418 M/sec                   [42.28%]
    33,608,185,424      L1-icache-load-misses     #   41.407 M/sec                   [42.28%]
     2,489,611,820      dTLB-load-misses          #    3.067 M/sec                   [42.26%]
       371,870,411      dTLB-store-misses         #    0.458 M/sec                   [42.27%]
       512,108,974      iTLB-load-misses          #    0.631 M/sec                   [42.28%]
     2,280,308,348      branch-load-misses        #    2.809 M/sec                   [42.31%]
    16,344,737,798      LLC-loads                 #   20.138 M/sec                   [42.38%]
     3,477,812,875      LLC-stores                #    4.285 M/sec                   [42.43%]
     9,526,173,996      LLC-prefetches            #   11.737 M/sec                   [10.69%]

     310.036724914 seconds time elapsed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15003199" author="aweisberg" created="Thu, 12 Nov 2015 23:33:53 +0000"  >&lt;p&gt;My takeaway from the counters is that with 2000 threads working through 19 million writes took more instructions, almost double the number of cache references,  more than double the number of context switches, and double the number of dcache misses. So there was a big drop in efficiency that could explain how this occurs even without contention or starvation.&lt;/p&gt;

&lt;p&gt;Now if there is a way to have 2000 threads do this work more efficiently is a good question. There are a lot more performance counters that might give insight into what having more threads changed as well as profiling. I&apos;ll look into it tomorrow.&lt;/p&gt;</comment>
                            <comment id="15004731" author="aweisberg" created="Fri, 13 Nov 2015 21:13:23 +0000"  >&lt;p&gt;Attempt at flame graphs. &lt;tt&gt;perf record&lt;/tt&gt; was kind of unhappy with 2k thread and demolished a core and throughput went down a nice chunk.&lt;/p&gt;

&lt;p&gt;Also have more counters. Interestingly there are 3x as many system calls with 2k threads, but I wasn&apos;t counting individual system calls so I don&apos;t know what that time was spent doing.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Results:
op rate                   : 51730 [WRITE:51730]
partition rate            : 51730 [WRITE:51730]
row rate                  : 51730 [WRITE:51730]
latency mean              : 9.6 [WRITE:9.6]
latency median            : 7.3 [WRITE:7.3]
latency 95th percentile   : 19.7 [WRITE:19.7]
latency 99th percentile   : 39.2 [WRITE:39.2]
latency 99.9th percentile : 79.0 [WRITE:79.0]
latency max               : 1119.0 [WRITE:1119.0]
Total partitions          : 19000000 [WRITE:19000000]
Total errors              : 0 [WRITE:0]
total gc count            : 0
total gc mb               : 0
total gc time (s)         : 0
avg gc time(ms)           : NaN
stdev gc time(ms)         : 0
Total operation time      : 00:06:07
END

 Performance counter stats &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&apos;./cassandra-stress write n=19000000 -rate threads=500 -mode &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt; cql3 -node 192.168.1.9&apos;&lt;/span&gt;:

 2,515,431,304,667      cycles                    #    2.441 GHz                     [16.05%]
 2,187,585,695,677      instructions              #    0.87  insns per cycle        
                                                  #    0.79  stalled cycles per insn [21.35%]
    41,594,524,794      cache-references          #   40.359 M/sec                   [26.62%]
    10,229,632,567      cache-misses              #   24.594 % of all cache refs     [31.87%]
 1,731,248,021,308      idle-cycles-frontend      #   68.83% frontend cycles idle    [37.10%]
 1,171,017,181,584      idle-cycles-backend       #   46.55% backend  cycles idle    [42.33%]
    1030610.942568      cpu-clock (msec)                                            
    1030611.990849      task-clock (msec)         #    2.721 CPUs utilized          
            91,892      page-faults               #    0.089 K/sec                  
        32,039,387      cs                        #    0.031 M/sec                  
         1,584,549      migrations                #    0.002 M/sec                  
            91,537      minor-faults              #    0.089 K/sec                  
   271,307,683,357      branch-instructions       #  263.249 M/sec                   [47.56%]
     2,777,454,782      branch-misses             #    1.02% of all branches         [47.48%]
    52,738,483,134      L1-dcache-load-misses     #   51.172 M/sec                   [47.47%]
    18,283,685,960      L1-dcache-store-misses    #   17.741 M/sec                   [47.47%]
    15,205,838,312      L1-dcache-prefetch-misses #   14.754 M/sec                   [42.21%]
    39,417,616,920      L1-icache-load-misses     #   38.247 M/sec                   [42.28%]
     2,630,018,507      dTLB-load-misses          #    2.552 M/sec                   [42.33%]
       480,675,134      dTLB-store-misses         #    0.466 M/sec                   [42.38%]
       567,242,554      iTLB-load-misses          #    0.550 M/sec                   [42.43%]
     2,779,803,804      branch-load-misses        #    2.697 M/sec                   [42.48%]
    23,639,238,796      LLC-loads                 #   22.937 M/sec                   [42.51%]
     4,117,222,926      LLC-stores                #    3.995 M/sec                   [42.51%]
    11,311,004,882      LLC-prefetches            #   10.975 M/sec                   [10.60%]
        46,798,741      sched:sched_wakeup        #    0.045 M/sec                  
             1,284      sched:sched_wakeup_new    #    0.001 K/sec                  
        32,039,387      sched:sched_switch        #    0.031 M/sec                  
         1,585,242      sched:sched_migrate_task  #    0.002 M/sec                  
               643      sched:sched_process_exit  #    0.001 K/sec                  
                 0      sched:sched_wait_task     #    0.000 K/sec                  
                 4      sched:sched_process_wait  #    0.000 K/sec                  
               642      sched:sched_process_fork  #    0.001 K/sec                  
                 5      sched:sched_process_exec  #    0.000 K/sec                  
 9,609,960,131,198      sched:sched_stat_wait     # 9324.518 M/sec                  
388,048,782,246,881      sched:sched_stat_sleep    # 376522.674 M/sec                  
        40,039,320      sched:sched_stat_iowait   #    0.039 M/sec                  
     8,678,041,742      sched:sched_stat_blocked  #    8.420 M/sec                  
 1,032,643,169,845      sched:sched_stat_runtime  # 1001.971 M/sec                  
           272,903      sched:sched_wake_idle_without_ipi #    0.265 K/sec                  
             2,269      workqueue:workqueue_queue_work #    0.002 K/sec                  
             2,269      workqueue:workqueue_activate_work #    0.002 K/sec                  
         2,185,692      irq:irq_handler_entry     #    0.002 M/sec                  
         2,185,692      irq:irq_handler_exit      #    0.002 M/sec                  
         2,809,612      irq:softirq_entry         #    0.003 M/sec                  
         2,809,612      irq:softirq_exit          #    0.003 M/sec                  
         2,834,592      irq:softirq_raise         #    0.003 M/sec                  
               642      task:task_newtask         #    0.001 K/sec                  
                 5      task:task_rename          #    0.000 K/sec                  
            84,774      exceptions:page_fault_user #    0.082 K/sec                  
             7,118      exceptions:page_fault_kernel #    0.007 K/sec                  
        84,530,358      raw_syscalls:sys_enter    #    0.082 M/sec                  
        84,530,372      raw_syscalls:sys_exit     #    0.082 M/sec                  

     378.751202925 seconds time elapsed


Results:
op rate                   : 12919 [WRITE:12919]
partition rate            : 12919 [WRITE:12919]
row rate                  : 12919 [WRITE:12919]
latency mean              : 154.9 [WRITE:154.9]
latency median            : 83.2 [WRITE:83.2]
latency 95th percentile   : 366.2 [WRITE:366.2]
latency 99th percentile   : 579.3 [WRITE:579.3]
latency 99.9th percentile : 872.4 [WRITE:872.4]
latency max               : 2396.9 [WRITE:2396.9]
Total partitions          : 19000000 [WRITE:19000000]
Total errors              : 0 [WRITE:0]
total gc count            : 0
total gc mb               : 0
total gc time (s)         : 0
avg gc time(ms)           : NaN
stdev gc time(ms)         : 0
Total operation time      : 00:24:30
END

 Performance counter stats &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-quote&quot;&gt;&apos;./cassandra-stress write n=19000000 -rate threads=2000 -mode &lt;span class=&quot;code-keyword&quot;&gt;native&lt;/span&gt; cql3 -node 192.168.1.9&apos;&lt;/span&gt;:

 5,181,129,425,325      cycles                    #    2.169 GHz                     [16.10%]
 3,612,561,477,198      instructions              #    0.70  insns per cycle        
                                                  #    1.06  stalled cycles per insn [21.43%]
   125,303,440,058      cache-references          #   52.445 M/sec                   [26.76%]
    31,162,640,970      cache-misses              #   24.870 % of all cache refs     [32.03%]
 3,846,338,739,881      idle-cycles-frontend      #   74.24% frontend cycles idle    [37.28%]
 2,862,782,268,966      idle-cycles-backend       #   55.25% backend  cycles idle    [42.49%]
    2387975.425875      cpu-clock (msec)                                            
    2389248.103454      task-clock (msec)         #    1.610 CPUs utilized          
           159,955      page-faults               #    0.067 K/sec                  
        79,702,888      cs                        #    0.033 M/sec                  
        44,298,673      migrations                #    0.019 M/sec                  
           159,869      minor-faults              #    0.067 K/sec                  
   641,996,673,767      branch-instructions       #  268.702 M/sec                   [47.76%]
     6,698,884,506      branch-misses             #    1.04% of all branches         [47.72%]
   140,604,450,350      L1-dcache-load-misses     #   58.849 M/sec                   [47.69%]
    37,276,664,029      L1-dcache-store-misses    #   15.602 M/sec                   [47.72%]
    30,237,145,992      L1-dcache-prefetch-misses #   12.656 M/sec                   [42.39%]
    69,945,237,261      L1-icache-load-misses     #   29.275 M/sec                   [42.47%]
     5,669,464,361      dTLB-load-misses          #    2.373 M/sec                   [42.51%]
     1,086,499,138      dTLB-store-misses         #    0.455 M/sec                   [42.55%]
     1,042,804,417      iTLB-load-misses          #    0.436 M/sec                   [42.52%]
     6,685,073,730      branch-load-misses        #    2.798 M/sec                   [42.53%]
    87,031,289,561      LLC-loads                 #   36.426 M/sec                   [42.55%]
    11,988,446,940      LLC-stores                #    5.018 M/sec                   [42.46%]
    29,826,259,945      LLC-prefetches            #   12.484 M/sec                   [10.58%]
       154,542,169      sched:sched_wakeup        #    0.065 M/sec                  
             4,284      sched:sched_wakeup_new    #    0.002 K/sec                  
        79,702,888      sched:sched_switch        #    0.033 M/sec                  
        44,301,003      sched:sched_migrate_task  #    0.019 M/sec                  
             2,143      sched:sched_process_exit  #    0.001 K/sec                  
                 0      sched:sched_wait_task     #    0.000 K/sec                  
                 4      sched:sched_process_wait  #    0.000 K/sec                  
             2,142      sched:sched_process_fork  #    0.001 K/sec                  
                 5      sched:sched_process_exec  #    0.000 K/sec                  
   448,770,334,535      sched:sched_stat_wait     #  187.829 M/sec                  
5,998,250,681,675,102      sched:sched_stat_sleep    # 2510518.130 M/sec                  
       154,752,649      sched:sched_stat_iowait   #    0.065 M/sec                  
    49,347,196,951      sched:sched_stat_blocked  #   20.654 M/sec                  
 3,155,194,145,918      sched:sched_stat_runtime  # 1320.580 M/sec                  
        71,084,628      sched:sched_wake_idle_without_ipi #    0.030 M/sec                  
             6,089      workqueue:workqueue_queue_work #    0.003 K/sec                  
             6,089      workqueue:workqueue_activate_work #    0.003 K/sec                  
           981,847      irq:irq_handler_entry     #    0.411 K/sec                  
           981,847      irq:irq_handler_exit      #    0.411 K/sec                  
         2,690,966      irq:softirq_entry         #    0.001 M/sec                  
         2,690,966      irq:softirq_exit          #    0.001 M/sec                  
         2,701,243      irq:softirq_raise         #    0.001 M/sec                  
             2,142      task:task_newtask         #    0.001 K/sec                  
                 5      task:task_rename          #    0.000 K/sec                  
           152,187      exceptions:page_fault_user #    0.064 K/sec                  
             7,768      exceptions:page_fault_kernel #    0.003 K/sec                  
       232,031,226      raw_syscalls:sys_enter    #    0.097 M/sec                  
       232,031,239      raw_syscalls:sys_exit     #    0.097 M/sec                  

    1483.891153703 seconds time elapsed
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15005575" author="aweisberg" created="Sat, 14 Nov 2015 19:50:07 +0000"  >&lt;p&gt;To test stress and threading in general I mocked out interactions between stress and the client library. There is no performance regression up to 4000 threads if you remove the server and client library from the picture.&lt;/p&gt;

&lt;p&gt;Attached is what I used to fake the queries. It&apos;s a thread pulling queries off a delay queue and the delay is set to be a uniform distribution between some minimum and maximum latency. I tried 3-9 milliseconds with a server side throughput of 100k. The threads issuing the queries are woken up via &lt;tt&gt;java.util.concurrent.FutureTask&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;I&apos;ll mock out the server in the client library next.&lt;/p&gt;</comment>
                            <comment id="15006987" author="aweisberg" created="Mon, 16 Nov 2015 17:47:52 +0000"  >&lt;p&gt;It was easier to stub out the server first than to stub out the client library so I did that first. See attached diff. I have ExecuteMessage return a void result immediately.&lt;/p&gt;

&lt;p&gt;On my setup with the server stubbed out the client node maxes out CPU at 400% (4 cores) and does 100k operations/second with 500 threads. I increased to 2000 threads and utilization reported by top decreased to 270% (don&apos;t believe top, it&apos;s saturated) and throughput decreased to 30k.&lt;/p&gt;

&lt;p&gt;At 1000 threads I still get 100k. I do see the drop at 1250 threads. So yes it exists, but it&apos;s might be an issue with the client library or how the client library chooses to present load to the server. I&apos;ll dig a bit into how the client library works to see if I can explain it.&lt;/p&gt;

&lt;p&gt;I personally don&apos;t necessarily see this as a bug. If you want to concurrently execute more than 1000 requests you should not use thread per request on one node. That said we do have an interest in having it work as well as possible since people are going to do it anyways and we might as well pave the way modulo how much time we want to invest.&lt;/p&gt;

&lt;p&gt;I am going to experiment with having stress use two (or N) instances of the client library to see if reduced contention in the client will ameliorate the drop off at 1250 threads. If that helps it may just be a matter of making sure the client library can operate as shared nothing shards internally so it can be made to have locality and scale up.&lt;/p&gt;

&lt;p&gt;In the past I have found that a single global client instance with global locks doesn&apos;t scale, but I also had limited success with running multiple instances. It helps, but not to the point you get linear scale up.&lt;/p&gt;</comment>
                            <comment id="15007258" author="aweisberg" created="Mon, 16 Nov 2015 20:28:17 +0000"  >&lt;p&gt;I was able to narrow this down to a configuration issue with the driver combined with less than perfect behavior if you don&apos;t run with this configuration. If I increase the maximum number of pending requests per connection from 128 to 256 then the performance at 1250 threads goes back to normal.&lt;/p&gt;

&lt;p&gt;For stress we can do something smarter when setting this tunable to reflect the number of available threads. Generally if we have a thread submitting requests we would want it to default to having a pending request against the server otherwise all you are really benchmarking is the driver&apos;s ability to deal with pending requests.&lt;/p&gt;

&lt;p&gt;Then there is the separate driver issue of the degradation in performance when the number of pending requests is not high enough. I wouldn&apos;t expect that kind of drop off. Whether the request is pending at the client or languishing in a TCP buffer in the server shouldn&apos;t really matter. I haven&apos;t looked, but my guess is that when the driver reaches the limit the thread submitting a request goes to sleep, and then it is woken up again. This means that every request has to flow through some extra scheduling points per request to account for this.&lt;/p&gt;

&lt;p&gt;A better way is to always flatten the serialized request to a shared buffer and when the connection is ready to accept more work the network thread can wake up and write multiple requests to the server at once.&lt;/p&gt;</comment>
                            <comment id="15007275" author="aweisberg" created="Mon, 16 Nov 2015 20:41:05 +0000"  >&lt;p&gt;Created &lt;a href=&quot;https://datastax-oss.atlassian.net/browse/JAVA-992&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://datastax-oss.atlassian.net/browse/JAVA-992&lt;/a&gt; for the suspected Java client driver issue.&lt;/p&gt;</comment>
                            <comment id="15009316" author="aweisberg" created="Tue, 17 Nov 2015 19:24:04 +0000"  >&lt;p&gt;Here is a proposed change to stress to make it always set a max pending requests that is high enough that all threads will be able to have outstanding requests.&lt;/p&gt;

&lt;p&gt;It also makes it possible to manually specify a number of connections per host as well the maximum number of pending requests per connection.&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...aweisberg:CASSANDRA-7217?expand=1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;code&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-CASSANDRA-7217-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;utests&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/aweisberg/job/aweisberg-CASSANDRA-7217-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtests&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;This addresses the performance drop off between 1000 and 1250 threads in &lt;a href=&quot;http://cstar.datastax.com/graph?command=one_job&amp;amp;stats=d77e88c8-8d5c-11e5-9bfa-0256e416528f&amp;amp;metric=op_rate&amp;amp;operation=2_write&amp;amp;smoothing=1&amp;amp;show_aggregates=true&amp;amp;xmin=0&amp;amp;xmax=183.04&amp;amp;ymin=0&amp;amp;ymax=152938.5&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this workload&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="15025012" author="jmckenzie" created="Tue, 24 Nov 2015 18:13:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt; to review.&lt;/p&gt;</comment>
                            <comment id="15025016" author="tjake" created="Tue, 24 Nov 2015 18:16:02 +0000"  >&lt;p&gt;LGTM +1&lt;/p&gt;</comment>
                            <comment id="15026813" author="tjake" created="Wed, 25 Nov 2015 14:05:24 +0000"  >&lt;p&gt;committed thx&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12772296" name="2000-threads.svg" size="642968" author="aweisberg" created="Fri, 13 Nov 2015 21:13:23 +0000"/>
                            <attachment id="12772295" name="500-threads.svg" size="1009062" author="aweisberg" created="Fri, 13 Nov 2015 21:13:23 +0000"/>
                            <attachment id="12772382" name="FakeQuerySystem.java" size="1913" author="aweisberg" created="Sat, 14 Nov 2015 19:50:07 +0000"/>
                            <attachment id="12772536" name="stub_server.diff" size="8061" author="aweisberg" created="Mon, 16 Nov 2015 17:47:52 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>4.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[aweisberg]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>392211</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 51 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1vjmf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>392404</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>tjake</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[tjake]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>