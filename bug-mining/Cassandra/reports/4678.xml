<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:06:50 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-13153] Reappeared Data when Mixing Incremental and Full Repairs</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-13153</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;This happens for both LeveledCompactionStrategy and SizeTieredCompactionStrategy.  I&apos;ve only tested it on Cassandra version 2.2 but it most likely also affects all Cassandra versions after 2.2, if they have anticompaction with full repair.&lt;/p&gt;

&lt;p&gt;When mixing incremental and full repairs, there are a few scenarios where the Data SSTable is marked as unrepaired and the Tombstone SSTable is marked as repaired.  Then if it is past gc_grace, and the tombstone and data has been compacted out on other replicas, the next incremental repair will push the Data to other replicas without the tombstone.&lt;/p&gt;

&lt;p&gt;Simplified scenario:&lt;br/&gt;
3 node cluster with RF=3&lt;br/&gt;
Intial config:&lt;br/&gt;
	Node 1 has data and tombstone in separate SSTables.&lt;br/&gt;
	Node 2 has data and no tombstone.&lt;br/&gt;
	Node 3 has data and tombstone in separate SSTables.&lt;/p&gt;

&lt;p&gt;Incremental repair (nodetool repair -pr) is run every day so now we have tombstone on each node.&lt;br/&gt;
Some minor compactions have happened since so data and tombstone get merged to 1 SSTable on Nodes 1 and 3.&lt;br/&gt;
	Node 1 had a minor compaction that merged data with tombstone. 1 SSTable with tombstone.&lt;br/&gt;
	Node 2 has data and tombstone in separate SSTables.&lt;br/&gt;
	Node 3 had a minor compaction that merged data with tombstone. 1 SSTable with tombstone.&lt;/p&gt;

&lt;p&gt;Incremental repairs keep running every day.&lt;br/&gt;
Full repairs run weekly (nodetool repair -full -pr). &lt;br/&gt;
Now there are 2 scenarios where the Data SSTable will get marked as &quot;Unrepaired&quot; while Tombstone SSTable will get marked as &quot;Repaired&quot;.&lt;/p&gt;

&lt;p&gt;Scenario 1:&lt;br/&gt;
        Since the Data and Tombstone SSTable have been marked as &quot;Repaired&quot; and anticompacted, they have had minor compactions with other SSTables containing keys from other ranges.  During full repair, if the last node to run it doesn&apos;t own this particular key in it&apos;s partitioner range, the Data and Tombstone SSTable will get anticompacted and marked as &quot;Unrepaired&quot;.  Now in the next incremental repair, if the Data SSTable is involved in a minor compaction during the repair but the Tombstone SSTable is not, the resulting compacted SSTable will be marked &quot;Unrepaired&quot; and Tombstone SSTable is marked &quot;Repaired&quot;.&lt;/p&gt;

&lt;p&gt;Scenario 2:&lt;br/&gt;
        Only the Data SSTable had minor compaction with other SSTables containing keys from other ranges after being marked as &quot;Repaired&quot;.  The Tombstone SSTable was never involved in a minor compaction so therefore all keys in that SSTable belong to 1 particular partitioner range. During full repair, if the last node to run it doesn&apos;t own this particular key in it&apos;s partitioner range, the Data SSTable will get anticompacted and marked as &quot;Unrepaired&quot;.   The Tombstone SSTable stays marked as Repaired.&lt;/p&gt;

&lt;p&gt;Then it&#8217;s past gc_grace.  Since Node&#8217;s #1 and #3 only have 1 SSTable for that key, the tombstone will get compacted out.&lt;br/&gt;
	Node 1 has nothing.&lt;br/&gt;
	Node 2 has data (in unrepaired SSTable) and tombstone (in repaired SSTable) in separate SSTables.&lt;br/&gt;
	Node 3 has nothing.&lt;/p&gt;

&lt;p&gt;Now when the next incremental repair runs, it will only use the Data SSTable to build the merkle tree since the tombstone SSTable is flagged as repaired and data SSTable is marked as unrepaired.  And the data will get repaired against the other two nodes.&lt;br/&gt;
	Node 1 has data.&lt;br/&gt;
	Node 2 has data and tombstone in separate SSTables.&lt;br/&gt;
	Node 3 has data.&lt;br/&gt;
If a read request hits Node 1 and 3, it will return data.  If it hits 1 and 2, or 2 and 3, however, it would return no data.&lt;/p&gt;

&lt;p&gt;Tested this with single range tokens for simplicity.&lt;/p&gt;</description>
                <environment>&lt;p&gt;Apache Cassandra 2.2&lt;/p&gt;</environment>
        <key id="13037722">CASSANDRA-13153</key>
            <summary>Reappeared Data when Mixing Incremental and Full Repairs</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10000" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Urgent</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="spod">Stefan Podkowinski</assignee>
                                    <reporter username="Amanda.Debrot">Amanda Debrot</reporter>
                        <labels>
                            <label>Cassandra</label>
                    </labels>
                <created>Wed, 25 Jan 2017 15:13:50 +0000</created>
                <updated>Fri, 15 May 2020 08:03:34 +0000</updated>
                            <resolved>Mon, 20 Mar 2017 19:29:52 +0000</resolved>
                                        <fixVersion>2.2.10</fixVersion>
                    <fixVersion>3.0.13</fixVersion>
                    <fixVersion>3.11.0</fixVersion>
                    <fixVersion>4.0-alpha1</fixVersion>
                    <fixVersion>4.0</fixVersion>
                                    <component>Legacy/Tools</component>
                    <component>Local/Compaction</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>14</watches>
                                                                                                                <comments>
                            <comment id="15839622" author="spodxx@gmail.com" created="Thu, 26 Jan 2017 12:06:44 +0000"  >&lt;p&gt;Thanks reporting this, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Amanda.Debrot&quot; class=&quot;user-hover&quot; rel=&quot;Amanda.Debrot&quot;&gt;Amanda.Debrot&lt;/a&gt;! Let me try to wrap-up again what&apos;s happending here..&lt;/p&gt;

&lt;p&gt;I think the assumption was that anti-compaction will isolate repaired ranges into the repaired set of sstables, while parts of sstables not covered by the repair will stay in the unrepaired set. As described by Amanda, trouble starts when anti-compaction is taking place exclusively on already repaired sstables. Once we&apos;ve finished repairing a certain range using full repair, anti-compaction will move unaffected ranges in overlapping sstables from the repaired into unrepaired set again, even if ranges have actually already been repaired before. As the overlap between ranges and sstables is non-deterministic, we could either see regular cells, tombstones or both being move to unrepaired, based on whether the sstable happens to overlap or not. &lt;/p&gt;

&lt;p&gt;Unfortunately this is not the only way that this could happen. As described in &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9143&quot; title=&quot;Fix consistency of incrementally repaired data across replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9143&quot;&gt;&lt;del&gt;CASSANDRA-9143&lt;/del&gt;&lt;/a&gt;, compactions during the repairs can prevent anti-compaction for individual sstables and tombstones and data could end up in different sets in this case as well. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;ve only tested it on Cassandra version 2.2 but it most likely also affects all Cassandra versions with incremental repair - like 2.1 and 3.0.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think 2.1 should not be affected, as we started doing anti-compactions for full repairs in 2.2.&lt;/p&gt;</comment>
                            <comment id="15839768" author="amanda.debrot" created="Thu, 26 Jan 2017 14:42:14 +0000"  >&lt;p&gt;Hi Stefan,&lt;/p&gt;

&lt;p&gt;Yes true, it should just affect Cassandra 2.2+ versions.  I forgot about that point with 2.1.  I&apos;ll update the &quot;since version&quot;.  Thanks!&lt;/p&gt;</comment>
                            <comment id="15846617" author="spodxx@gmail.com" created="Tue, 31 Jan 2017 10:09:26 +0000"  >&lt;p&gt;I can think of two options here. First of all we could simply prevent anti-compaction for full primary range repairs just as we did for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9142&quot; title=&quot;DC Local repair or -hosts should only be allowed with -full repair&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9142&quot;&gt;&lt;del&gt;CASSANDRA-9142&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-10422&quot; title=&quot;Avoid anticompaction when doing subrange repair&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-10422&quot;&gt;&lt;del&gt;CASSANDRA-10422&lt;/del&gt;&lt;/a&gt;, see &lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...spodkowinski:WIP-13153-global&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;. We could also try to adopt the anti-compaction process to fall back to the repairedAt min value of all compacting sstables, instead of using UNREPAIRED_SSTABLE, see &lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...spodkowinski:WIP-13153-repairedAt&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;. But that&apos;s a bit tricky as even a single unrepaired sstables can prevent this and we&apos;d have to group sstables differently for that.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt;, any other ideas or comments?&lt;/p&gt;</comment>
                            <comment id="15865792" author="spodxx@gmail.com" created="Tue, 14 Feb 2017 13:50:22 +0000"  >&lt;p&gt;Getting back to this ticket and giving it some thoughts again, I&apos;m pretty sure that it&apos;s not enough to disable anti-compaction for full PK repairs. This will only prevent the described issue for the repair initiator node, but not the involved other replicas. I&apos;m afraid there&apos;s no way around disabling anti-compaction for full repairs completely to prevent this issue from happening.&lt;/p&gt;</comment>
                            <comment id="15866448" author="bdeggleston" created="Tue, 14 Feb 2017 19:30:25 +0000"  >&lt;p&gt;I think this can also happen just by running incremental repair only, because of the way it leaks data into the unrepaired sstable bucket. This has been fixed in &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9143&quot; title=&quot;Fix consistency of incrementally repaired data across replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9143&quot;&gt;&lt;del&gt;CASSANDRA-9143&lt;/del&gt;&lt;/a&gt;&#8230; but that was only committed to trunk, since it&#8217;s not a trivial change. Unfortunately, the only way to avoid this in pre-4.0 clusters is to just not run incremental repair.&lt;/p&gt;

&lt;p&gt;This may not be as bad as it sounds though, since what pre &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9143&quot; title=&quot;Fix consistency of incrementally repaired data across replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9143&quot;&gt;&lt;del&gt;CASSANDRA-9143&lt;/del&gt;&lt;/a&gt; incremental repair gained in validation time, it likely lost in redundant re-streaming of otherwise repaired data. If you compacted a large sstable that was also involved in a repair, the entire contents of that sstable would end up getting streamed to every other replica on the next incremental repair.&lt;/p&gt;</comment>
                            <comment id="15866518" author="spodxx@gmail.com" created="Tue, 14 Feb 2017 19:57:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13153&quot; title=&quot;Reappeared Data when Mixing Incremental and Full Repairs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13153&quot;&gt;&lt;del&gt;CASSANDRA-13153&lt;/del&gt;&lt;/a&gt; is not just about redundant re-streaming. It&apos;s about streaming only &lt;em&gt;partial&lt;/em&gt; data for partitions or cells based on the circumstance if an individual sstable has been affected or not. If it did, you may end up leaking data that is covered by a tombstone back to unrepaired, while the tombstone in the unaffected sstable stays in repaired, and have the data streamed from there to all other nodes (which may already compacted the data and tombstone away). Or am I missing something here?&lt;/p&gt;

&lt;p&gt;With &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9143&quot; title=&quot;Fix consistency of incrementally repaired data across replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9143&quot;&gt;&lt;del&gt;CASSANDRA-9143&lt;/del&gt;&lt;/a&gt; it&apos;s not &lt;em&gt;that&lt;/em&gt; bad, since you start on unrepaired, recent data and the next incremental run will indeed fix the data that has been left in unrepaired before, given it&apos;s run within gc_grace. But with &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13153&quot; title=&quot;Reappeared Data when Mixing Incremental and Full Repairs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13153&quot;&gt;&lt;del&gt;CASSANDRA-13153&lt;/del&gt;&lt;/a&gt; you might leak arbitrary old data into unrepaired, which should never happen.&lt;/p&gt;</comment>
                            <comment id="15866811" author="bdeggleston" created="Tue, 14 Feb 2017 22:15:05 +0000"  >&lt;blockquote&gt;&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13153&quot; title=&quot;Reappeared Data when Mixing Incremental and Full Repairs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13153&quot;&gt;&lt;del&gt;CASSANDRA-13153&lt;/del&gt;&lt;/a&gt; is not just about redundant re-streaming. It&apos;s about streaming only partial data for partitions or cells &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, agreed. My point was that not using incremental repair should fix &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=Amanda.Debrot&quot; class=&quot;user-hover&quot; rel=&quot;Amanda.Debrot&quot;&gt;Amanda.Debrot&lt;/a&gt;&apos;s problem. The part about redundant streaming just meant that as a workaround, it might not actually be as bad as it sounds.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;With &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9143&quot; title=&quot;Fix consistency of incrementally repaired data across replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9143&quot;&gt;&lt;del&gt;CASSANDRA-9143&lt;/del&gt;&lt;/a&gt; it&apos;s not that bad, since you start on unrepaired, recent data and the next incremental run will indeed fix the data that has been left in unrepaired before, given it&apos;s run within gc_grace. But with &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13153&quot; title=&quot;Reappeared Data when Mixing Incremental and Full Repairs&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13153&quot;&gt;&lt;del&gt;CASSANDRA-13153&lt;/del&gt;&lt;/a&gt; you might leak arbitrary old data into unrepaired, which should never happen.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure what you mean here. The goal of &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9143&quot; title=&quot;Fix consistency of incrementally repaired data across replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9143&quot;&gt;&lt;del&gt;CASSANDRA-9143&lt;/del&gt;&lt;/a&gt; was to prevent repaired data from ever leaking back into unrepaired, for both correctness and performance reasons. Do you mean that leaking data is still possible after &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9143&quot; title=&quot;Fix consistency of incrementally repaired data across replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9143&quot;&gt;&lt;del&gt;CASSANDRA-9143&lt;/del&gt;&lt;/a&gt;, or that the point of this ticket is different?&lt;/p&gt;</comment>
                            <comment id="15867542" author="spodxx@gmail.com" created="Wed, 15 Feb 2017 09:39:44 +0000"  >&lt;p&gt;Taking a closer look at &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9143&quot; title=&quot;Fix consistency of incrementally repaired data across replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9143&quot;&gt;&lt;del&gt;CASSANDRA-9143&lt;/del&gt;&lt;/a&gt; again, I&apos;m certain that your work there would indeed fix the issue described in this ticket, as we no longer do anti-compaction for full repairs. So that brings me back to the question: shouldn&apos;t we get rid of anti-compactions for full repairs in 2.2+ as well?&lt;/p&gt;</comment>
                            <comment id="15867566" author="krummas" created="Wed, 15 Feb 2017 09:54:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spodxx%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;spodxx@gmail.com&quot;&gt;spodxx@gmail.com&lt;/a&gt; are you saying that sstables marked as repaired are getting moved to unrepaired? I don&apos;t see how that could happen, with -full repairs, if a repaired sstable gets compacted away, it will (should?) stay in repaired, not get moved to unrepaired. The already repaired sstables will just not get anticompacted with the new repairedAt time&lt;/p&gt;</comment>
                            <comment id="15867821" author="krummas" created="Wed, 15 Feb 2017 13:20:37 +0000"  >&lt;p&gt;Ok, so the problem is actually if we run a -full repair and some of the ranges fail, we might anticompact an sstable containing the data to unrepaired, but the repaired tombstone stays in repaired because that sstable was compacted away. The fix would be that we anticompact to the previous value of repairedAt.&lt;/p&gt;

&lt;p&gt;Or that we, as suggested, don&apos;t anticompact on full repairs at all.&lt;/p&gt;</comment>
                            <comment id="15868143" author="bdeggleston" created="Wed, 15 Feb 2017 16:44:04 +0000"  >&lt;blockquote&gt;&lt;p&gt;shouldn&apos;t we get rid of anti-compactions for full repairs in 2.2+ as well?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think we&apos;re ok leaving them as is. Using pre 4.0 incremental repairs is root cause of this. If operators stop using incremental repairs, there&apos;s no harm in doing an anticompaction after a full repair. The only scenario it would cause problems is when using incremental repair for the first time after upgrading to 4.0, when the repaired datasets are very likely inconsistent. This could be addressed by just running a final full repair on the upgraded cluster. As part of &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9143&quot; title=&quot;Fix consistency of incrementally repaired data across replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9143&quot;&gt;&lt;del&gt;CASSANDRA-9143&lt;/del&gt;&lt;/a&gt;, full repairs no longer perform anticompaction, and streamed sstables include the repairedAt time, which would bring the repaired and unrepaired datasets in sync.&lt;/p&gt;

&lt;p&gt;So having said all that, it seems like we should recommend that users who delete data:&lt;br/&gt;
1. Stop using incremental repair (pre-4.0)&lt;br/&gt;
2. Run a full repair after upgrading to 4.0 before using incremental repair again&lt;/p&gt;

&lt;p&gt;We should also recommend that even if users don&apos;t delete data, they should take a look at the amount of streaming their incremental repair is doing, and decide if it might be less expensive to just do full repairs instead.&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;</comment>
                            <comment id="15869826" author="spodxx@gmail.com" created="Thu, 16 Feb 2017 12:28:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;If operators stop using incremental repairs, there&apos;s no harm in doing an anticompaction after a full repair. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Even if there&apos;s no harm when it comes to consistency, it&apos;s still causing fragmentation of existing sstables. All repaired ranges will cause all replicas to go through all local, intersecting sstables and rewrite them segregated by affected and unaffected token ranges. This will cause unnecessary load and is probably pretty bad for LCS or STCS, as we constantly break up bigger sstables by doing so.&lt;/p&gt;

&lt;p&gt;One option to avoid this would be just to never run anti-compaction on repaired sstables. See &lt;a href=&quot;https://github.com/spodkowinski/cassandra/commit/684d1c72cda58fecea15b46f928a451df38d87cb&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt; for a simple approach. I don&apos;t think anti-compaction was ever meant to work on already repaired sstables, so that&apos;s probably the most non-intrusive fix to avoid most of the known issues around incremental repairs discussed here.&lt;/p&gt;

&lt;p&gt;Btw, I&apos;m also a bit confused by looking at &lt;a href=&quot;https://github.com/apache/cassandra/blob/98d74ed998706e9e047dc0f7886a1e9b18df3ce9/src/java/org/apache/cassandra/db/compaction/CompactionManager.java#L1285&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;createWriterForAntiCompaction&lt;/a&gt;. Each sstable&apos;s level will be streamed as well, doesn&apos;t it? Can we really throw them into the same level locally, just because they have been at level X on other nodes? Won&apos;t this potentially break the &quot;non-overlapping sstables&quot; guarantee by dropping them blindly to level X?&lt;/p&gt;</comment>
                            <comment id="15869836" author="krummas" created="Thu, 16 Feb 2017 12:35:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;Can we really throw them into the same level locally, just because they have been at level X on other nodes?&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;no, we check if it would create overlap before adding it to the manifest:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/cassandra/blob/98d74ed998706e9e047dc0f7886a1e9b18df3ce9/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java#L149&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/cassandra/blob/98d74ed998706e9e047dc0f7886a1e9b18df3ce9/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java#L149&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15871984" author="krummas" created="Fri, 17 Feb 2017 15:29:31 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spodxx%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;spodxx@gmail.com&quot;&gt;spodxx@gmail.com&lt;/a&gt; the patch LGTM, could you run CI on it?&lt;/p&gt;</comment>
                            <comment id="15878122" author="spodxx@gmail.com" created="Wed, 22 Feb 2017 12:46:16 +0000"  >&lt;p&gt;Patch has now been finished based on my last suggestion to simply skip already repaired sstables during anti-compaction. I&apos;ve also made the repairedAt timestamps for both the containing and not containing ranges an explicit parameter. This should help to avoid overlooking the fact that we have to deal with repairedAt for the not containing part as well.&lt;/p&gt;

&lt;p&gt;PR for corresponding dtest can be found &lt;a href=&quot;https://github.com/riptano/cassandra-dtest/pull/1447&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Test results (just started new dtest run with PR branch):&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;2.2&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;3.0&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;3.11&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13153-2.2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13153-3.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/spodkowinski/cassandra/tree/CASSANDRA-13153-3.11&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-2.2-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-3.0-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-3.11-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-2.2-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-3.0-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-13153-3.11-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;



</comment>
                            <comment id="15888245" author="krummas" created="Tue, 28 Feb 2017 15:35:03 +0000"  >&lt;p&gt;Not sure I agree that adding the parameter helps, could we just add an assert in &lt;tt&gt;anticompactGroup&lt;/tt&gt; that all sstables are unrepaired instead?&lt;/p&gt;</comment>
                            <comment id="15888289" author="spodxx@gmail.com" created="Tue, 28 Feb 2017 15:59:14 +0000"  >&lt;p&gt;I&apos;m not sure I really understand what the additional &lt;tt&gt;repairedAtNotContainedInRange&lt;/tt&gt; parameter has to do with adding an assert for making sure &quot;all sstables are unrepaired&quot;. Even if all sstables are, we still need to apply a repairedAt value for those ranges not successfully repaired.&lt;/p&gt;</comment>
                            <comment id="15888348" author="krummas" created="Tue, 28 Feb 2017 16:15:44 +0000"  >&lt;p&gt;My thinking was that the only time anyone could expect the sstable with the non-repaired ranges to be something other than UNREPAIRED would be if they passed in repaired sstables, so having the assert shows that this is not expected&lt;/p&gt;</comment>
                            <comment id="15888552" author="spodxx@gmail.com" created="Tue, 28 Feb 2017 17:52:30 +0000"  >&lt;p&gt;Yes, that could be an option as well. But as we already discussed the possibility of actually doing anti-compaction on repaired sstables for the sake of tracking repairedAt more accurately, I was hoping someone someday would be able to make use of the method as is by providing a reasonable repairedAt value for both anti-compaction outputs. But I&apos;m open to add an assert instead, if you think I&apos;m a bit to optimistic here.&lt;/p&gt;</comment>
                            <comment id="15889664" author="krummas" created="Wed, 1 Mar 2017 07:12:37 +0000"  >&lt;p&gt;Makes sense, but lets add this if/when we do that change?&lt;/p&gt;</comment>
                            <comment id="15892486" author="spodxx@gmail.com" created="Thu, 2 Mar 2017 16:13:01 +0000"  >&lt;p&gt;I&apos;ve changed my branches to the bare minimum of what needs to be done for filtering already repaired sstables and re-run tests. See above for links.&lt;/p&gt;</comment>
                            <comment id="15902779" author="spodxx@gmail.com" created="Thu, 9 Mar 2017 09:38:00 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt;, any feedback on the latest, simplified patch version?&lt;/p&gt;</comment>
                            <comment id="15902783" author="krummas" created="Thu, 9 Mar 2017 09:41:06 +0000"  >&lt;p&gt;oops, sorry for the delay, +1&lt;/p&gt;</comment>
                            <comment id="15933359" author="spodxx@gmail.com" created="Mon, 20 Mar 2017 19:29:52 +0000"  >&lt;p&gt;Merged as 06316df549c0096bd774893a405d1d32512e97bf&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13060826">CASSANDRA-13398</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12849305" name="Step-by-Step-Simulate-Reappeared-Data.txt" size="845" author="Amanda.Debrot" created="Wed, 25 Jan 2017 15:13:50 +0000"/>
                            <attachment id="12849306" name="log-Reappeared-Data.txt" size="68886" author="Amanda.Debrot" created="Wed, 25 Jan 2017 15:13:50 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[spod]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                    <customfield id="customfield_12313825" key="com.atlassian.jira.plugin.system.customfieldtypes:cascadingselect">
                        <customfieldname>Bug Category</customfieldname>
                        <customfieldvalues>
                                                    <customfieldvalue key="12982" cascade-level=""><![CDATA[Correctness]]></customfieldvalue>
                                <customfieldvalue key="13161" cascade-level="1"><![CDATA[Unrecoverable Corruption / Loss]]></customfieldvalue>
            
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 35 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i396wf:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311421" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Reproduced In</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12335581">2.2.7</customfieldvalue>
    <customfieldvalue id="12337440">2.2.8</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>marcuse</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[marcuse]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12963"><![CDATA[Critical]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12311420" key="com.atlassian.jira.plugin.system.customfieldtypes:version">
                        <customfieldname>Since Version</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12324945">2.2.0 beta 1</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>