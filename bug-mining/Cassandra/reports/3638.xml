<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:54:59 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-9681] Memtable heap size grows and many long GC pauses are triggered</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-9681</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;C* 2.1.7 cluster is behaving really bad after 1-2 days. &lt;tt&gt;gauges.cassandra.jmx.org.apache.cassandra.metrics.ColumnFamily.AllMemtablesHeapSize.Value&lt;/tt&gt; jumps to 7 GB (&lt;a href=&quot;https://www.dropbox.com/s/vraggy292erkzd2/Screenshot%202015-06-29%2019.12.53.png?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/vraggy292erkzd2/Screenshot%202015-06-29%2019.12.53.png?dl=0&lt;/a&gt;) on 3/6 nodes in each data center and then there are many long GC pauses. Cluster is using default heap size values (&lt;tt&gt;-Xms8192M -Xmx8192M -Xmn2048M&lt;/tt&gt;)&lt;/p&gt;

&lt;p&gt;Before C* 2.1.5 memtables heap size was basically constant ~500MB (&lt;a href=&quot;https://www.dropbox.com/s/fjdywik5lojstvn/Screenshot%202015-06-29%2019.30.00.png?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/fjdywik5lojstvn/Screenshot%202015-06-29%2019.30.00.png?dl=0&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;After restarting all nodes is behaves stable for 1-2days. Today I&apos;ve done that and long GC pauses are gone (~18:00 &lt;a href=&quot;https://www.dropbox.com/s/7vo3ynz505rsfq3/Screenshot%202015-06-29%2019.28.37.png?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/7vo3ynz505rsfq3/Screenshot%202015-06-29%2019.28.37.png?dl=0&lt;/a&gt;). The only pattern we&apos;ve found so far is that long GC  pauses are happening basically at the same time on all nodes in the same data center - even on the ones where memtables heap size is not growing.&lt;/p&gt;

&lt;p&gt;Cliffs on the graphs are nodes restarts.&lt;/p&gt;

&lt;p&gt;Used memory on boxes where &lt;tt&gt;AllMemtabelesHeapSize&lt;/tt&gt; grows, stays at the same level - &lt;a href=&quot;https://www.dropbox.com/s/tes9abykixs86rf/Screenshot%202015-06-29%2019.37.52.png?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/tes9abykixs86rf/Screenshot%202015-06-29%2019.37.52.png?dl=0&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Replication factor is set to 3.&lt;/p&gt;</description>
                <environment>&lt;p&gt;C* 2.1.7, Debian Wheezy&lt;/p&gt;</environment>
        <key id="12841392">CASSANDRA-9681</key>
            <summary>Memtable heap size grows and many long GC pauses are triggered</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10000" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Urgent</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="benedict">Benedict Elliott Smith</assignee>
                                    <reporter username="mlowicki">mlowicki</reporter>
                        <labels>
                    </labels>
                <created>Mon, 29 Jun 2015 17:32:44 +0000</created>
                <updated>Tue, 16 Apr 2019 09:31:06 +0000</updated>
                            <resolved>Thu, 2 Jul 2015 09:37:41 +0000</resolved>
                                        <fixVersion>2.1.8</fixVersion>
                    <fixVersion>2.2.0 rc2</fixVersion>
                    <fixVersion>3.0 alpha 1</fixVersion>
                                        <due></due>
                            <votes>1</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14606026" author="benedict" created="Mon, 29 Jun 2015 18:08:20 +0000"  >&lt;p&gt;Can you please attach your cassandra.yaml and full log file history since the problems began?&lt;/p&gt;</comment>
                            <comment id="14606057" author="mlowicki" created="Mon, 29 Jun 2015 18:19:24 +0000"  >&lt;p&gt;It started ~ 04.06 (at about the same time on all affected boxes - &lt;a href=&quot;https://www.dropbox.com/s/9c6p2xdmncktbnu/Screenshot%202015-06-29%2020.16.02.png?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/9c6p2xdmncktbnu/Screenshot%202015-06-29%2020.16.02.png?dl=0&lt;/a&gt;, &lt;a href=&quot;https://www.dropbox.com/s/gs8bztzr394icz0/Screenshot%202015-06-29%2020.16.24.png?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/gs8bztzr394icz0/Screenshot%202015-06-29%2020.16.24.png?dl=0&lt;/a&gt;). Will attach logs soon.&lt;/p&gt;</comment>
                            <comment id="14606066" author="benedict" created="Mon, 29 Jun 2015 18:24:08 +0000"  >&lt;p&gt;Thanks. It is likely the log files will be insufficient to diagnose, though, just to let you know. Assuming that&apos;s the case, the best next step is to obtain a heap dump during one of the spikes (doesn&apos;t need to be at the peak, just so long as it&apos;s well above where it was settled prior to upgrade). In the meantime I&apos;ll see if I can find a candidate by looking through recent changes.&lt;/p&gt;</comment>
                            <comment id="14606070" author="mlowicki" created="Mon, 29 Jun 2015 18:25:51 +0000"  >&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;-rw-r--r--  1 cassandra cassandra 1.3M Jun 12 14:32 system.log.6.zip
-rw-r--r--  1 cassandra cassandra 1.9M Jun 10 13:11 system.log.7.zip
-rw-r--r--  1 cassandra cassandra 1.9M Jun  6 21:55 system.log.8.zip
-rw-r--r--  1 cassandra cassandra 1.9M Jun  4 01:29 system.log.9.zip
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Logs from the time when it basically started. If more needed just let me know.&lt;/p&gt;</comment>
                            <comment id="14606170" author="benedict" created="Mon, 29 Jun 2015 19:07:04 +0000"  >&lt;p&gt;So, for posterity, I ran the following bash script for analysing the logs:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;grep -E &lt;span class=&quot;code-quote&quot;&gt;&quot;Completed flushing|Enqueuing flush of ([^:]+): [0-9]+ \(([0-9]+)%\)&quot;&lt;/span&gt; system.log.2* | grep -v compactions_in_progress | sed -r &lt;span class=&quot;code-quote&quot;&gt;&quot;s@.* - (.*)@\1@&quot;&lt;/span&gt; | sed -r &lt;span class=&quot;code-quote&quot;&gt;&quot;s@Completed flushing .*-([^-]+)-ka-[0-9]+-Data.db.*@completed \1@&quot;&lt;/span&gt; | sed -r &lt;span class=&quot;code-quote&quot;&gt;&apos;s@Enqueuing flush of ([^ :]+): [0-9]+ \(([0-9]+)%.*@started \1 \2@&apos;&lt;/span&gt; | awk &lt;span class=&quot;code-quote&quot;&gt;&apos;{ &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; ($1 == &lt;span class=&quot;code-quote&quot;&gt;&quot;started&quot;&lt;/span&gt;) { total[$2] += $3; list[$2][end[$2]] = $3; end[$2]++; } &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; { total[$2] -= list[$2][start[$2]]; delete list[$2][start[$2]]; start[$2]++; } print(&lt;span class=&quot;code-quote&quot;&gt;&quot;total:&quot;&lt;/span&gt; total[$2] &lt;span class=&quot;code-quote&quot;&gt;&quot; &quot;&lt;/span&gt; $0); }&apos;&lt;/span&gt; | sort | less
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This indicates flushing is happening as expected, and staying well within the bounds that are supposed to be enforced. These same numbers feed into those that are reported via JMX. In fact, they should be strictly greater than that returned by JMX, since JMX only reports the live memtables. So the numbers that suggest you&apos;re exceeding your memtable space limits are hard to explain.&lt;/p&gt;

&lt;p&gt;The heap dump will no doubt help a great deal.&lt;/p&gt;</comment>
                            <comment id="14606399" author="mlowicki" created="Mon, 29 Jun 2015 21:03:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://www.dropbox.com/s/nhgudkyxwjdrq0f/cassandra.bin?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/nhgudkyxwjdrq0f/cassandra.bin?dl=0&lt;/a&gt; This dump has been created when memtables heap size was ~800MB (on not affected boxes it&apos;s &amp;lt; 500MB).&lt;/p&gt;</comment>
                            <comment id="14606713" author="benedict" created="Tue, 30 Jun 2015 00:16:21 +0000"  >&lt;p&gt;Regrettably, that heap dump also looks 100% healthy. Approximately 500Mb of memtable space being used.&lt;/p&gt;

&lt;p&gt;It is possible we&apos;re getting some funky reporting, somehow, but I can&apos;t see an obvious candidate change. Could you possibly try obtaining another heap dump when under more significant pressure?&lt;/p&gt;

&lt;p&gt;Would be also great to get the log files for the 26th onwards, since that looks to be where the utilisation was most spiked.&lt;/p&gt;</comment>
                            <comment id="14607913" author="mlowicki" created="Tue, 30 Jun 2015 08:11:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://www.dropbox.com/s/cnv36bbdznbwc0g/Screenshot%202015-06-30%2010.07.27.png?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/cnv36bbdznbwc0g/Screenshot%202015-06-30%2010.07.27.png?dl=0&lt;/a&gt; - this if chart from the box I was creating heap dump. Please keep in mind that the metric changes rapidly. It can grow from ~300MB to over 1GB within 3 minutes. I&apos;ll prepare heap dump today once again.&lt;/p&gt;

&lt;p&gt;I&apos;m using jmap:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;root@db5:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;# jmap -F -dump:file=cassandra.bin 19189
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;and this C* node is dead for the rest of the cluster for ~40minutes (&lt;a href=&quot;https://gist.github.com/mlowicki/7645963e2a1ac4563578&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://gist.github.com/mlowicki/7645963e2a1ac4563578&lt;/a&gt;). Can this be avoided?&lt;/p&gt;
</comment>
                            <comment id="14607927" author="benedict" created="Tue, 30 Jun 2015 08:20:27 +0000"  >&lt;p&gt;40m? That&apos;s unreasonable, but I&apos;m afraid I&apos;m not sure what you can do to make it faster. It should be pretty quick, really. It&apos;s possible the -F option makes it slower, by taking a suboptimal avenue.&lt;/p&gt;

&lt;p&gt;It looks like the space used grows slowly but steadily in your previous graphs. Perhaps to avoid the issue of another healthy heap dump and other problems, we should wait until the line is consistently above the healthy mark...&lt;/p&gt;

&lt;p&gt;In the meantime, those other log files (during the most severe period) could be helpful.&lt;/p&gt;</comment>
                            <comment id="14607934" author="mlowicki" created="Tue, 30 Jun 2015 08:24:04 +0000"  >&lt;p&gt;Without -F it gives:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;root@db5:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;# jmap -dump:file=cassandra.bin 19189
19189: Unable to open socket file: target process not responding or HotSpot VM not loaded
The -F option can be used when the target process is not responding
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&apos;ve started dumping heap when metrics shows 1.7GB. Will attach soon. Logs will be available in a few.&lt;/p&gt;</comment>
                            <comment id="14607942" author="mlowicki" created="Tue, 30 Jun 2015 08:29:18 +0000"  >&lt;p&gt;Attaching logs from db5:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;-rw-r--r--  1 cassandra cassandra 3.3M Jun 30 07:58 system.log
-rw-r--r--  1 cassandra cassandra 854K Jun 29 14:19 system.log.1.zip
-rw-r--r--  1 cassandra cassandra 1.3M Jun 27 22:31 system.log.2.zip
-rw-r--r--  1 cassandra cassandra 1.8M Jun 24 11:43 system.log.3.zip
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Memtable heap size on this boxes behaves like on the chart - &lt;a href=&quot;https://www.dropbox.com/s/l9cgch2hlguco85/Screenshot%202015-06-30%2010.30.59.png?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/l9cgch2hlguco85/Screenshot%202015-06-30%2010.30.59.png?dl=0&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14607982" author="benedict" created="Tue, 30 Jun 2015 09:02:20 +0000"  >&lt;p&gt;Those logs are much more helpful. It suggests the heap dump will not be as useful as hoped - if it&apos;s nearly ready, please do upload it, but if it&apos;s degrading your cluster or causing you problems please feel free not to. Thread dumps spread over 10 minutely intervals might be helpful, however.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 08:47:56,322 ColumnFamilyStore.java:906 - Enqueuing flush of entity: 2469606408 (118%) on-heap, 0 (0%) off-heap
system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 08:54:09,873 ColumnFamilyStore.java:906 - Enqueuing flush of entity_by_id: 2633134676 (126%) on-heap, 0 (0%) off-heap
system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 09:00:21,353 ColumnFamilyStore.java:906 - Enqueuing flush of entity: 4191587102 (200%) on-heap, 0 (0%) off-heap
system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 09:10:39,736 ColumnFamilyStore.java:906 - Enqueuing flush of entity: 3455005866 (165%) on-heap, 0 (0%) off-heap
system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 09:19:18,950 ColumnFamilyStore.java:906 - Enqueuing flush of entity: 2879879873 (138%) on-heap, 0 (0%) off-heap
system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 09:26:28,874 ColumnFamilyStore.java:906 - Enqueuing flush of entity: 2400355359 (115%) on-heap, 0 (0%) off-heap
system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 09:32:16,612 ColumnFamilyStore.java:906 - Enqueuing flush of entity_by_id: 2630156881 (126%) on-heap, 0 (0%) off-heap
system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 09:38:35,445 ColumnFamilyStore.java:906 - Enqueuing flush of entity: 4232454175 (202%) on-heap, 0 (0%) off-heap
system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 09:47:56,679 ColumnFamilyStore.java:906 - Enqueuing flush of entity: 3475375055 (166%) on-heap, 0 (0%) off-heap
system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 09:56:44,672 ColumnFamilyStore.java:906 - Enqueuing flush of entity: 3046517722 (146%) on-heap, 0 (0%) off-heap
system.log.2015-06-29_1419:INFO  [SlabPoolCleaner] 2015-06-29 10:04:09,950 ColumnFamilyStore.java:906 - Enqueuing flush of entity: 2522479792 (121%) on-heap, 0 (0%) off-heap
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So, we can definitely see that things are messed up, and that Cassandra is aware of it. The real question is why it is permitting this to occur, and more importantly why it has changed. There is only one reason we ostensibly permit overshoot of the heap utilisation of memtables, and that is to permit writes that were already in progress when we decided to flush to complete. This doesn&apos;t seem to adequately explain the massive overshoot, and the behaviour here hasn&apos;t changed between these versions. &lt;/p&gt;

&lt;p&gt;Do you by any chance use secondary indexes? These are a potential (but unlikely) source of large writes that would have permission to exceed the heap space utilisation, if they were very stale.&lt;/p&gt;

&lt;p&gt;Could you upload your schema, and let us know what compaction strategy you&apos;re using?&lt;/p&gt;

&lt;p&gt;I think it will likely be necessary to release a patched version for you with some improved logging to help diagnose.&lt;/p&gt;</comment>
                            <comment id="14608001" author="mlowicki" created="Tue, 30 Jun 2015 09:13:15 +0000"  >&lt;p&gt;Attaching our schema.&lt;/p&gt;

&lt;p&gt;We&apos;re using LCS and we aren&apos;t using secondary indexes.&lt;/p&gt;

&lt;p&gt;Heap dump is uploading to Google Drive so should be available soon.&lt;/p&gt;</comment>
                            <comment id="14608082" author="mlowicki" created="Tue, 30 Jun 2015 10:25:50 +0000"  >&lt;p&gt;Heap dump - &lt;a href=&quot;https://drive.google.com/file/d/0B_8mc_afWmd2bGhpd0p2Ql9UMkU/view?usp=sharing&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://drive.google.com/file/d/0B_8mc_afWmd2bGhpd0p2Ql9UMkU/view?usp=sharing&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="14608182" author="benedict" created="Tue, 30 Jun 2015 12:05:15 +0000"  >&lt;p&gt;Thanks. I&apos;ve pinpointed the malfunctioning area, so it should only be a matter of time until I can pinpoint what broke it, and introduce a fix along with some redundancy to catch this kind of issue earlier.&lt;/p&gt;</comment>
                            <comment id="14608231" author="mlowicki" created="Tue, 30 Jun 2015 12:46:32 +0000"  >&lt;p&gt;Cool. If more logs / dumps / cheers needed just let me know.&lt;/p&gt;</comment>
                            <comment id="14608239" author="benedict" created="Tue, 30 Jun 2015 12:56:14 +0000"  >&lt;p&gt;I suspect I have found the issue.&lt;/p&gt;

&lt;p&gt;We can, under some circumstances, reduce the amount of memory used in a memtable when we modify its contents (typically it only grows, or stays stable). If this happens, we corrupt the bookkeeping, as a result of some suboptimal choices in the API. The amount of corruption is likely to be very small, however it accumulates over time.&lt;/p&gt;

&lt;p&gt;I have a patch uploaded &lt;a href=&quot;https://github.com/belliottsmith/cassandra/tree/9681&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;, that I&apos;m just waiting on CI results to confirm doesn&apos;t break anything. This improves the API to avoid this problem, fails earlier if the API is misused (although it should now be robust to it), and logs more useful information for spotting this kind of issue with greater ease. I will also follow up with specific regression tests.&lt;/p&gt;

&lt;p&gt;I&apos;ll let you know when the patch is ready to trial. If you could confirm it fixes your issue, that would be greatly appreciated.&lt;/p&gt;</comment>
                            <comment id="14608278" author="mlowicki" created="Tue, 30 Jun 2015 13:34:33 +0000"  >&lt;p&gt;Sure, just let me know and we&apos;ll try to apply the patch.&lt;/p&gt;</comment>
                            <comment id="14608379" author="benedict" created="Tue, 30 Jun 2015 14:28:26 +0000"  >&lt;p&gt;OK, looks ready to take for a spin.&lt;/p&gt;</comment>
                            <comment id="14609685" author="mlowicki" created="Wed, 1 Jul 2015 07:33:44 +0000"  >&lt;p&gt;So far so good - &lt;a href=&quot;https://www.dropbox.com/s/ad8te1g6iz2wofe/Screenshot%202015-07-01%2009.31.00.png?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/ad8te1g6iz2wofe/Screenshot%202015-07-01%2009.31.00.png?dl=0&lt;/a&gt;. I&apos;ll let you know if it&apos;ll degrade or not. GC pauses we&apos;ve talked about yesterday are probably caused by misbehaving Logstash or Kibana as I&apos;ve checked using jstat and gc.log that everything is fine on this boxes.&lt;/p&gt;

&lt;p&gt;All nodes in the cluster have been patched ~7am.&lt;/p&gt;</comment>
                            <comment id="14609763" author="benedict" created="Wed, 1 Jul 2015 08:51:29 +0000"  >&lt;p&gt;Thanks, that&apos;s great to hear. I&apos;ve pushed some small test additions. This is now ready for review.&lt;/p&gt;</comment>
                            <comment id="14609848" author="mlowicki" created="Wed, 1 Jul 2015 09:51:33 +0000"  >&lt;p&gt;After couple of hours it&apos;s still fine - &lt;a href=&quot;https://www.dropbox.com/s/ox5xzxqbojyv7wz/Screenshot%202015-07-01%2011.49.53.png?dl=0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://www.dropbox.com/s/ox5xzxqbojyv7wz/Screenshot%202015-07-01%2011.49.53.png?dl=0&lt;/a&gt;. It always started to grow right after restart so we can assume that this problem is fixed.&lt;/p&gt;</comment>
                            <comment id="14609861" author="benedict" created="Wed, 1 Jul 2015 10:03:08 +0000"  >&lt;p&gt;Marked as since 2.1.0, since this possibility has been present since then. I&apos;m unsure what changed to increase the incidence of it since.&lt;/p&gt;</comment>
                            <comment id="14610749" author="jbellis" created="Wed, 1 Jul 2015 18:06:54 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt; to review&lt;/p&gt;</comment>
                            <comment id="14610965" author="tjake" created="Wed, 1 Jul 2015 20:36:30 +0000"  >&lt;p&gt;So basically we were always calling allocate and not adjusting for a smaller cell size. Makes sense +1&lt;/p&gt;

&lt;p&gt;Some Minor nits:&lt;/p&gt;

&lt;p&gt;MemtableAllocator.adjust() needs formatting for if/then&lt;/p&gt;

&lt;p&gt;MemtablePool there are some dead methods now: adjustAcquired and adjustReclaiming&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12742543" name="cassandra.yaml" size="35802" author="mlowicki" created="Mon, 29 Jun 2015 18:12:06 +0000"/>
                            <attachment id="12742741" name="db5.system.log" size="3419808" author="mlowicki" created="Tue, 30 Jun 2015 08:29:18 +0000"/>
                            <attachment id="12742742" name="db5.system.log.1.zip" size="873502" author="mlowicki" created="Tue, 30 Jun 2015 08:29:18 +0000"/>
                            <attachment id="12742743" name="db5.system.log.2.zip" size="1304947" author="mlowicki" created="Tue, 30 Jun 2015 08:29:18 +0000"/>
                            <attachment id="12742744" name="db5.system.log.3.zip" size="1827015" author="mlowicki" created="Tue, 30 Jun 2015 08:29:18 +0000"/>
                            <attachment id="12742749" name="schema.cql" size="10075" author="mlowicki" created="Tue, 30 Jun 2015 09:13:15 +0000"/>
                            <attachment id="12742552" name="system.log.6.zip" size="1945928" author="mlowicki" created="Mon, 29 Jun 2015 18:25:50 +0000"/>
                            <attachment id="12742553" name="system.log.7.zip" size="1942354" author="mlowicki" created="Mon, 29 Jun 2015 18:25:50 +0000"/>
                            <attachment id="12742554" name="system.log.8.zip" size="1929938" author="mlowicki" created="Mon, 29 Jun 2015 18:25:50 +0000"/>
                            <attachment id="12742555" name="system.log.9.zip" size="1942163" author="mlowicki" created="Mon, 29 Jun 2015 18:25:51 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>10.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[benedict]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 20 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2gmw7:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311421" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Reproduced In</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12327256">2.1.0</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>tjake</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[tjake]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12963"><![CDATA[Critical]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>