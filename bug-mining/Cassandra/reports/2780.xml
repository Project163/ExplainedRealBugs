<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:44:35 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-2434] range movements can violate consistency</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-2434</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;My reading (a while ago) of the code indicates that there is no logic involved during bootstrapping that avoids consistency level violations. If I recall correctly it just grabs neighbors that are currently up.&lt;/p&gt;

&lt;p&gt;There are at least two issues I have with this behavior:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;If I have a cluster where I have applications relying on QUORUM with RF=3, and bootstrapping complete based on only one node, I have just violated the supposedly guaranteed consistency semantics of the cluster.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;Nodes can flap up and down at any time, so even if a human takes care to look at which nodes are up and things about it carefully before bootstrapping, there&apos;s no guarantee.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;A complication is that not only does it depend on use-case where this is an issue (if all you ever do you do at CL.ONE, it&apos;s fine); even in a cluster which is otherwise used for QUORUM operations you may wish to accept less-than-quorum nodes during bootstrap in various emergency situations.&lt;/p&gt;

&lt;p&gt;A potential easy fix is to have bootstrap take an argument which is the number of hosts to bootstrap from, or to assume QUORUM if none is given.&lt;/p&gt;

&lt;p&gt;(A related concern is bootstrapping across data centers. You may &lt;b&gt;want&lt;/b&gt; to bootstrap to a local node and then do a repair to avoid sending loads of data across DC:s while still achieving consistency. Or even if you don&apos;t care about the consistency issues, I don&apos;t think there is currently a way to bootstrap from local nodes only.)&lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12503655">CASSANDRA-2434</key>
            <summary>range movements can violate consistency</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="tjake">T Jake Luciani</assignee>
                                    <reporter username="scode">Peter Schuller</reporter>
                        <labels>
                    </labels>
                <created>Thu, 7 Apr 2011 17:23:55 +0000</created>
                <updated>Tue, 16 Apr 2019 09:33:02 +0000</updated>
                            <resolved>Thu, 1 May 2014 13:52:35 +0000</resolved>
                                        <fixVersion>2.1 beta2</fixVersion>
                                    <component>Legacy/Streaming and Messaging</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>20</watches>
                                                                                                                <comments>
                            <comment id="13017641" author="jbellis" created="Fri, 8 Apr 2011 21:00:17 +0000"  >&lt;p&gt;ISTM the easiest fix is to always stream from the node that will be removed from the replicas for each range, unless given permission from the operator to choose a replica that is closer / less dead.&lt;/p&gt;</comment>
                            <comment id="13028225" author="jbellis" created="Tue, 3 May 2011 13:41:37 +0000"  >&lt;p&gt;Related: &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-833&quot; title=&quot;fix consistencylevel during bootstrap&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-833&quot;&gt;&lt;del&gt;CASSANDRA-833&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13091418" author="thepaul" created="Thu, 25 Aug 2011 23:30:24 +0000"  >&lt;p&gt;So, it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it (other nodes being bootstrapped/moved/decom&apos;d during that time period); in some cases, that could still lead to a consistency violation.  Is that unlikely enough that we don&apos;t care, here?  At least the situation would be better with the proposed fix than it is now.&lt;/p&gt;

&lt;p&gt;Second question: what might the &quot;permission from the operator to choose a replica that is closer/less dead&quot; look like?  Maybe just a boolean flag saying &quot;it&apos;s ok to stream from any node for any range you need to stream&quot;?  Or would we want to allow specifying precise source nodes for any/all affected address ranges?&lt;/p&gt;</comment>
                            <comment id="13091851" author="jbellis" created="Fri, 26 Aug 2011 16:20:38 +0000"  >&lt;blockquote&gt;&lt;p&gt;it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It&apos;s always been unsupported to bootstrap a second node into the same &quot;token arc&quot; while a previous one is ongoing.  Does that cover what you&apos;re thinking of or are we still on the hook?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;what might the &quot;permission from the operator to choose a replica that is closer/less dead&quot; look like?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;It seems to me that the two valid choices are&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Stream from &quot;correct&quot; replica&lt;/li&gt;
	&lt;li&gt;Stream from closest replica&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I can&apos;t think of a reason to stream from an arbitrary replica other than those options.&lt;/p&gt;</comment>
                            <comment id="13091881" author="thepaul" created="Fri, 26 Aug 2011 17:14:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;It&apos;s always been unsupported to bootstrap a second node into the same &quot;token arc&quot; while a previous one is ongoing. Does that cover what you&apos;re thinking of or are we still on the hook?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Is it also unsupported to decom within X&apos;s token arc, or move into/out of that arc, while X is bootstrapping? I think we&apos;re safe if so.&lt;/p&gt;</comment>
                            <comment id="13091884" author="jbellis" created="Fri, 26 Aug 2011 17:19:57 +0000"  >&lt;p&gt;Yes.&lt;/p&gt;</comment>
                            <comment id="13094795" author="thepaul" created="Wed, 31 Aug 2011 19:03:51 +0000"  >&lt;p&gt;This still needs some testing, but I&apos;m putting it up now in case anyone has some time to take a look and make sure my approach is sane.&lt;/p&gt;

&lt;p&gt;Adds an optional &quot;-n&quot; argument to &quot;nodetool join&quot; to allow bootstrapping from closest live node (n == non-strict). Also recognizes an optional property &quot;cassandra.join_strict&quot; which can be set to false when a bootstrap is triggered by cassandra.join_ring.&lt;/p&gt;</comment>
                            <comment id="13094818" author="nickmbailey" created="Wed, 31 Aug 2011 19:30:32 +0000"  >&lt;p&gt;Seems to me like the option to stream from the closest replica might just add more confusion without really gaining anything. The node that is leaving the replica set will never be in another datacenter. It could be on a different rack, but if you are following best practices and alternating racks then it is likely either on the same rack or there is only one copy on that rack and the best case possible is streaming from another rack anyway.&lt;/p&gt;</comment>
                            <comment id="13094845" author="thepaul" created="Wed, 31 Aug 2011 19:56:17 +0000"  >&lt;p&gt;Judging by the irc channel and user list, assuming people will follow best practices seems a bit of a dead end. Plus, what about the case where the node leaving the replica set is dead? You still want the option to allow choosing another to stream from. And we probably shouldn&apos;t default to choosing another without explicit permission, because of the consistency violation stuff.&lt;/p&gt;</comment>
                            <comment id="13094846" author="jbellis" created="Wed, 31 Aug 2011 19:56:51 +0000"  >&lt;blockquote&gt;&lt;p&gt;The node that is leaving the replica set will never be in another datacenter&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I think that&apos;s only strictly true for NTS, but I&apos;m fine leaving it out.  Not worth adding complexity for ONTS at this point.&lt;/p&gt;</comment>
                            <comment id="13094848" author="jbellis" created="Wed, 31 Aug 2011 19:57:56 +0000"  >&lt;blockquote&gt;&lt;p&gt;what about the case where the node leaving the replica set is dead&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good point.  We do need something to make that possible.&lt;/p&gt;</comment>
                            <comment id="13094850" author="nickmbailey" created="Wed, 31 Aug 2011 20:02:01 +0000"  >&lt;p&gt;Yeah I was assuming ONTS is basically deprecated at this point. Didn&apos;t think about the dead case though. I suppose just a &apos;force&apos; type of option and a warning indicating the possible consistency issues works.&lt;/p&gt;</comment>
                            <comment id="13094853" author="jbellis" created="Wed, 31 Aug 2011 20:03:42 +0000"  >&lt;p&gt;As long as we need to handle the dead case I don&apos;t see any harm in having a slightly more generally-useful &quot;use closest&quot; option instead of &quot;force to pick random live replica&quot; option.&lt;/p&gt;</comment>
                            <comment id="13094864" author="nickmbailey" created="Wed, 31 Aug 2011 20:16:41 +0000"  >&lt;p&gt;Well, I imagine the &apos;force&apos; option would pick the nearest live node. By &apos;force&apos; I mean the option should be posed to the user as &quot;We can&apos;t guarantee consistency in your cluster after this bootstrap since a node is down, if you would like to do this anyway, specify option X&quot;. Just saying you can either bootstrap or bootstrap from the closest node doesn&apos;t convey the implications as well I don&apos;t think.&lt;/p&gt;

&lt;p&gt;Maybe we are on the same page and arguing over wording though.&lt;/p&gt;</comment>
                            <comment id="13096138" author="nickmbailey" created="Fri, 2 Sep 2011 17:10:42 +0000"  >&lt;p&gt;Just as initial feedback, I&apos;m not sure we need a new getRangesWithSources method, especially with so much duplication between them. Seems like strict could be passed to the current method. Also, what about leaving getRangesWithSources how it is and passing strict to getWorkMap? That method can do the endpoint set math if it needs to and throw a more informative exception in the case that strict is set and the endpoint we want to fetch from is dead.&lt;/p&gt;</comment>
                            <comment id="13096257" author="thepaul" created="Fri, 2 Sep 2011 20:03:45 +0000"  >&lt;p&gt;I did that (passing strict to getWorkMap) at first, but it wasn&apos;t too clean since it required adding a &apos;table&apos; argument as well as &apos;strict&apos;, and it ended up replacing too much of the getRangesWithSources functionality. So then I added &apos;strict&apos; as a parameter to getRangesWithSources (actually, it looks like I neglected to update its javadoc comment), but the differences between getRangesWithSources and getRangesWithStrictSource are such that a combined method feels a lot more special-casey and clunky. I like this way best in the end.&lt;/p&gt;</comment>
                            <comment id="13096274" author="nickmbailey" created="Fri, 2 Sep 2011 20:21:51 +0000"  >&lt;p&gt;Well I guess it kind of depends on which approach we take as well. Is the option A) bootstrap from the right token or bootstrap from the closest token, or B) bootstrap from the right token, but if that one isn&apos;t up, bootstrap from any other token preferring the closer ones.&lt;/p&gt;

&lt;p&gt;Like I said, I&apos;d say B, but if you and Jonathan both disagree.&lt;/p&gt;</comment>
                            <comment id="13096349" author="thepaul" created="Fri, 2 Sep 2011 21:50:09 +0000"  >&lt;p&gt;Yeah. B is probably easier on everyone, but I would say we simply can&apos;t do anything that might violate the consistency guarantee without explicit permission from the user.&lt;/p&gt;</comment>
                            <comment id="13096352" author="nickmbailey" created="Fri, 2 Sep 2011 21:55:19 +0000"  >&lt;p&gt;Ok, so if we always prefer to bootstrap from the correct token, then I still think we should combine getRangesWithStrictSource and getRangesWithSources. Basically the logic should be, find the &apos;best&apos; node to stream from. If the user requested it, also find a list of other candidates and order them by proximity. Right?&lt;/p&gt;</comment>
                            <comment id="13096807" author="jbellis" created="Sun, 4 Sep 2011 03:38:04 +0000"  >&lt;p&gt;I&apos;m okay with either A or B.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I would say we simply can&apos;t do anything that might violate the consistency guarantee without explicit permission from the user&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure I understand, are you saying that B would violate this, or just that the status quo does?&lt;/p&gt;</comment>
                            <comment id="13096981" author="hanzhu" created="Mon, 5 Sep 2011 02:05:20 +0000"  >&lt;p&gt;Is it possible to make the node does not reply to any request before bootstrap and anti-entrophy repair is finished?&lt;/p&gt;

&lt;p&gt;This could fix the consistency problem brought by bootstrap.&lt;/p&gt;</comment>
                            <comment id="13096990" author="jbellis" created="Mon, 5 Sep 2011 03:09:05 +0000"  >&lt;p&gt;Repair is a much, much more heavyweight solution to the problem than just &quot;stream from the node that is &apos;displaced.&apos;&quot;&lt;/p&gt;</comment>
                            <comment id="13097106" author="hanzhu" created="Mon, 5 Sep 2011 10:16:23 +0000"  >&lt;p&gt;Sometimes, the node is replaced because the hardware is crashed. If so, &quot;streaming from the node being replaced&quot; is not available.&lt;/p&gt;

&lt;p&gt;How about force the repair happens if the user specifies he needs the consistency of quorum while the original node has gone.&lt;/p&gt;</comment>
                            <comment id="13097189" author="thepaul" created="Mon, 5 Sep 2011 15:57:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;Ok, so if we always prefer to bootstrap from the correct token, then I still think we should combine getRangesWithStrictSource and getRangesWithSources. Basically the logic should be, find the &apos;best&apos; node to stream from. If the user requested it, also find a list of other candidates and order them by proximity. Right?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think so. I would still want to leave the option to stream from the closest even if the strict best node is available.&lt;/p&gt;</comment>
                            <comment id="13097190" author="thepaul" created="Mon, 5 Sep 2011 16:00:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m not sure I understand, are you saying that B would violate this, or just that the status quo does?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m saying B would violate this, yes. B was &quot;bootstrap from the right token, but if that one isn&apos;t up, bootstrap from any other token preferring the closer ones&quot;, right? I&apos;m saying we can&apos;t just automatically choose another token if the user didn&apos;t specifically say it&apos;s ok.&lt;/p&gt;</comment>
                            <comment id="13097204" author="nickmbailey" created="Mon, 5 Sep 2011 16:38:12 +0000"  >&lt;p&gt;Paul,&lt;/p&gt;

&lt;p&gt;The suggestion was that if the &apos;correct&apos; node is down, you can force the bootstrap to complete anyway (probably from the closest node, but that is transparent to the user), but only if the &apos;correct&apos; node is down. It sounds like you agree with Jonathan on the more general approach though.&lt;/p&gt;

&lt;p&gt;Zhu,&lt;/p&gt;

&lt;p&gt;Repair doesn&apos;t help in the case when you lost data due to a node going down. Also if only one node is down you should still be able to read/write at quorum and achieve consistency (assuming your replication factor is greater than 2).&lt;/p&gt;</comment>
                            <comment id="13097275" author="jbellis" created="Mon, 5 Sep 2011 19:25:06 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m saying we can&apos;t just automatically choose another token if the user didn&apos;t specifically say it&apos;s ok.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh, ok.  Right.  (I thought we were just bikeshedding over whether to call the &quot;manual override&quot; option &quot;use closest&quot; or &quot;force bootstrap.&quot;)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Repair doesn&apos;t help in the case when you lost data due to a node going down&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Additionally, I don&apos;t like the idea of automatically doing expensive things like repair; it feels cleaner to not do it automatically, and allow using the existing tool to perform one if desired, than to do it by default and have to add an option to skip it for when that&apos;s not desirable.&lt;/p&gt;</comment>
                            <comment id="13097625" author="hanzhu" created="Mon, 5 Sep 2011 23:19:58 +0000"  >&lt;blockquote&gt;&lt;p&gt;Also if only one node is down you should still be able to read/write at quorum and achieve consistency&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I suppose quorum read plus quorum write should provide monotonic read consistency. &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; Supposing  quorum write on key1 hits node A and node B, not on node C due to temporal network partition. After that node B is replaced by node D since it is down, and node D streams data from node C. If the following quorum read on key1 hits only node C and node D, the monotonic consistency is violated. This is rare but not unrealistic, especially when hint handoff is disabled. &lt;/p&gt;

&lt;p&gt;Maybe it is more resonable to give the admin an option, to specify that the bootstrapped node should not accept any read request until the admin turn it on manually. So the admin can start a manual repair if he wants to assure everything goes fine.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;http://www.allthingsdistributed.com/2007/12/eventually_consistent.html&lt;/p&gt;</comment>
                            <comment id="13097695" author="thepaul" created="Tue, 6 Sep 2011 01:41:57 +0000"  >&lt;blockquote&gt;&lt;p&gt;The suggestion was that if the &apos;correct&apos; node is down, you can force the bootstrap to complete anyway (probably from the closest node, but that is transparent to the user), but only if the &apos;correct&apos; node is down.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh, ok. I misunderstood. This seems reasonable. I&apos;d lean for the more general solution, yeah, but I don&apos;t feel very strongly about it.&lt;/p&gt;</comment>
                            <comment id="13097703" author="hanzhu" created="Tue, 6 Sep 2011 02:00:25 +0000"  >&lt;p&gt;As peter suggested before, another approach to fix the consistency problem is streaming sstables from all alive peers if the &quot;correct&quot; node is down. And then leave them to normal compaction.  &lt;/p&gt;

&lt;p&gt;This could be much lightweight than anti-entrophy repair, except the network IO pressure on the bootstrapping node.&lt;/p&gt;</comment>
                            <comment id="13099270" author="thepaul" created="Wed, 7 Sep 2011 20:08:51 +0000"  >&lt;p&gt;updated patch fixes the docstring for getRangesWithStrictSource().&lt;/p&gt;</comment>
                            <comment id="13099274" author="thepaul" created="Wed, 7 Sep 2011 20:11:14 +0000"  >&lt;p&gt;Patch 2434-testery.patch.txt adds a bit to unit tests to exercise o.a.c.dht.BootStrapper.getRangesWithStrictSource().&lt;/p&gt;</comment>
                            <comment id="13100732" author="thepaul" created="Thu, 8 Sep 2011 21:57:34 +0000"  >&lt;p&gt;2434-3.patch.txt removes the bits that add the &quot;-n&quot; option to nodetool join. Apparently no &quot;nodetool join&quot; should ever result in a bootstrap, so it doesn&apos;t matter whether the caller wants &quot;strict&quot; or not.&lt;/p&gt;</comment>
                            <comment id="13104573" author="jbellis" created="Wed, 14 Sep 2011 15:29:45 +0000"  >&lt;p&gt;Do we need to do anything special for move/decommission as well?&lt;/p&gt;</comment>
                            <comment id="13104618" author="nickmbailey" created="Wed, 14 Sep 2011 16:21:45 +0000"  >&lt;p&gt;I had a note to remember to create a ticket for that, but if we want to do it here that works as well.&lt;/p&gt;

&lt;p&gt;In any case, yes the same concerns exist when giving away ranges as when gaining ranges.&lt;/p&gt;</comment>
                            <comment id="13104685" author="thepaul" created="Wed, 14 Sep 2011 17:23:45 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do we need to do anything special for move/decommission as well?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, it looks like we do need to add similar logic for move. Expand the scope of this ticket accordingly?&lt;/p&gt;

&lt;p&gt;I don&apos;t see any way decommission could be affected by this sort of problem.&lt;/p&gt;</comment>
                            <comment id="13104690" author="thepaul" created="Wed, 14 Sep 2011 17:27:22 +0000"  >&lt;blockquote&gt;&lt;p&gt;In any case, yes the same concerns exist when giving away ranges as when gaining ranges.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Oh? I must be missing something. What would a consistency violation failure scenario look like for giving away ranges?&lt;/p&gt;</comment>
                            <comment id="13104694" author="jbellis" created="Wed, 14 Sep 2011 17:30:17 +0000"  >&lt;blockquote&gt;&lt;p&gt;Expand the scope of this ticket accordingly?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Yes, let&apos;s solve them both here.&lt;/p&gt;</comment>
                            <comment id="13104701" author="nickmbailey" created="Wed, 14 Sep 2011 17:32:01 +0000"  >&lt;p&gt;My comment wasn&apos;t very clear. Both decom and move currently, attempt to do the right thing. When a node is leaving, there should be one new replica for all the ranges it is responsible for. If it can&apos;t stream data to that replica there is a consistency problem.&lt;/p&gt;

&lt;p&gt;Both operations currently try to do stream to that replica, but we should use the &apos;strict&apos; logic in those cases as well and fail if we can&apos;t guarantee consistency and the user hasn&apos;t disabled strict.&lt;/p&gt;</comment>
                            <comment id="13104709" author="thepaul" created="Wed, 14 Sep 2011 17:41:28 +0000"  >&lt;p&gt;If decom can&apos;t stream data to the appropriate replica, then it should just fail, right? Do we support decom in cases where a consistency violation would result? Seems like it has to be the user&apos;s responsibility to bring up or decom the other node first.&lt;/p&gt;

&lt;p&gt;move could introduce a violation when it gains a new range, though, in the same cases as the bootstrap issue explained above.&lt;/p&gt;</comment>
                            <comment id="13104715" author="nickmbailey" created="Wed, 14 Sep 2011 17:47:03 +0000"  >&lt;p&gt;If we support it for bootstrapping I don&apos;t see why we shouldn&apos;t support it for decom. Right, move has the problem in both cases (giving away ranges, gaining ranges).&lt;/p&gt;</comment>
                            <comment id="13104718" author="thepaul" created="Wed, 14 Sep 2011 17:53:11 +0000"  >&lt;p&gt;I think we&apos;re talking about different things. Requiring the user to have the right nodes available for operation X is not the same as &quot;cassandra can &apos;lose&apos; writes when it happens to stream from the wrong node, even if the user did everything right&quot;.&lt;/p&gt;

&lt;p&gt;This ticket is about the latter, I think.&lt;/p&gt;</comment>
                            <comment id="13104756" author="thepaul" created="Wed, 14 Sep 2011 18:32:40 +0000"  >&lt;p&gt;Conversation on #cassandra-dev resulted in the conclusion that we&apos;ll fix this bug for range acquisition (bootstrap and move) now, and plan to allow the same looseness (non-strict mode, or whatever) for range egress (move and decom) in the future.&lt;/p&gt;

&lt;p&gt;I think.&lt;/p&gt;</comment>
                            <comment id="13108617" author="jbellis" created="Tue, 20 Sep 2011 11:58:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;It&apos;s always been unsupported to bootstrap a second node into the same &quot;token arc&quot; while a previous one is ongoing.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m pretty sure now that this is incorrect; we fixed it back in &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-603&quot; title=&quot;pending range collision between nodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-603&quot;&gt;&lt;del&gt;CASSANDRA-603&lt;/del&gt;&lt;/a&gt;.  I&apos;m updating the comments in TokenMetadata as follows:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    // Prior to CASSANDRA-603, we just had &amp;lt;tt&amp;gt;Map&amp;lt;Range, InetAddress&amp;gt; pendingRanges&amp;lt;tt&amp;gt;,
    // which was added to when a node began bootstrap and removed from when it finished.
    //
    // This is inadequate when multiple changes are allowed simultaneously.  For example,
    // suppose that there is a ring of nodes A, C and E, with replication factor 3.
    // Node D bootstraps between C and E, so its pending ranges will be E-A, A-C and C-D.
    // Now suppose node B bootstraps between A and C at the same time. Its pending ranges
    // would be C-E, E-A and A-B. Now both nodes need to be assigned pending range E-A,
    // which we would be unable to represent with the old Map.  The same thing happens
    // even more obviously for any nodes that boot simultaneously between same two nodes.
    //
    // So, we made two changes:
    //
    // First, we changed pendingRanges to a &amp;lt;tt&amp;gt;Multimap&amp;lt;Range, InetAddress&amp;gt;&amp;lt;/tt&amp;gt; (now
    // &amp;lt;tt&amp;gt;Map&amp;lt;String, Multimap&amp;lt;Range, InetAddress&amp;gt;&amp;gt;&amp;lt;/tt&amp;gt;, because replication strategy
    // and options are per-KeySpace).
    //
    // Second, we added the bootstrapTokens and leavingEndpoints collections, so we can
    // rebuild pendingRanges from the complete information of what is going on, when
    // additional changes are made mid-operation.
    //
    // Finally, note that recording the tokens of joining nodes in bootstrapTokens also
    // means we can detect and reject the addition of multiple nodes at the same token
    // before one becomes part of the ring.
    private BiMap&amp;lt;Token, InetAddress&amp;gt; bootstrapTokens = HashBiMap.create();
    // (don&apos;t need to record Token here since it&apos;s still part of tokenToEndpointMap until it&apos;s done leaving)
    private Set&amp;lt;InetAddress&amp;gt; leavingEndpoints = new HashSet&amp;lt;InetAddress&amp;gt;();
    // this is a cache of the calculation from {tokenToEndpointMap, bootstrapTokens, leavingEndpoints}
    private ConcurrentMap&amp;lt;String, Multimap&amp;lt;Range, InetAddress&amp;gt;&amp;gt; pendingRanges = new ConcurrentHashMap&amp;lt;String, Multimap&amp;lt;Range, InetAddress&amp;gt;&amp;gt;();
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13108793" author="thepaul" created="Tue, 20 Sep 2011 15:55:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;m pretty sure now that this is incorrect;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Well, doh. That puts us back at my first question:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So, it looks like it will be possible for the node-that-will-be-removed to change between starting a bootstrap and finishing it (other nodes being bootstrapped/moved/decom&apos;d during that time period); in some cases, that could still lead to a consistency violation. Is that unlikely enough that we don&apos;t care, here? At least the situation would be better with the proposed fix than it is now.&lt;/p&gt;&lt;/blockquote&gt;</comment>
                            <comment id="13108800" author="jbellis" created="Tue, 20 Sep 2011 16:01:31 +0000"  >&lt;p&gt;Can you give an example for illustration?&lt;/p&gt;</comment>
                            <comment id="13109230" author="nickmbailey" created="Wed, 21 Sep 2011 02:48:42 +0000"  >&lt;p&gt;Ok, so I think there are really two consistency issues here. Firstly, picking the &apos;right&apos; node to stream data to/from when making changes to the ring. Secondly, disallowing concurrent changes that have overlapping ranges.&lt;/p&gt;

&lt;p&gt;Currently we only disallow nodes from moving/decommissioning when they may potentially have data being streamed to them. There are a few examples of things we currently allow which I think are generally a bad idea.&lt;/p&gt;

&lt;p&gt;1) Say you have nodes A and D, if you bootstrap nodes B and C at the same time in between A and D, it may turn out that the correct node to stream from for both nodes is D. Now say node C finishes bootstrapping before node B. At that point, the correct node for B to bootstrap from is technically C, although D still has the data. However, since D is no longer technically responsible for the data, the user could run cleanup on D and delete the data that B is attempting to stream.&lt;/p&gt;

&lt;p&gt;2) The above case is also a problem when you bootstrap a node and the node it decides it needs to stream from is moving. Once that node finishes moving you could run cleanup on that node and delete data that the bootstrapping node needs. In this case, all documentation indicates you should do a cleanup after a move in order to remove old data, so it seems possibly more likely.&lt;/p&gt;

&lt;p&gt;3) A variation of the above case is when you bootstrap a node and the node it streams from is leaving. In that case the decom may finish and the user could terminate the cassandra process and/or node breaking any streams. Not to mention the idea of a node in a decommissioned state continuing to stream seems like a bad idea. I believe it would work currently, but I&apos;m not sure and it seems likely to break.&lt;/p&gt;

&lt;p&gt;I can&apos;t really think of any other examples but I think thats enough to illustrate that overlapping concurrent ring changes are a bad idea and we should just attempt to prevent them in all cases. An argument could be made that this would prevent you from doubling your cluster (the best way to grow) all at once, but I don&apos;t think that&apos;s really a huge deal. At most you would need RF steps to double your cluster.&lt;/p&gt;</comment>
                            <comment id="13109742" author="thepaul" created="Wed, 21 Sep 2011 18:21:04 +0000"  >&lt;p&gt;I think we can still allow overlapping concurrent ring changes, with the right set of invariants and/or operational rules. I&apos;ve been trying to work on defining those. It&apos;s pretty tricky but I think I have the right way of approaching the model now. Will update later today with more.&lt;/p&gt;

&lt;p&gt;Nick is right though, c* probably shouldn&apos;t support overlapping changes as is. Think bootstrapping &amp;gt;N nodes between the same two old nodes, decom&apos;ing one node and bootstrapping another in such a way that the bootstrap source stream node becomes the wrong source, etc.&lt;/p&gt;</comment>
                            <comment id="13109749" author="nickmbailey" created="Wed, 21 Sep 2011 18:30:29 +0000"  >&lt;p&gt;So the fact that the &apos;correct&apos; bootstrap source switches mid stream isn&apos;t really the problem I don&apos;t think. We set up pending ranges so that when a node gets ready to bootstrap, writes start getting duplicated to both the currently correct replica, and the bootstrapping node. Since all new writes are duplicated we can stream from that node and as long as we get the entire dataset, consistency should be fine. The problem is there is nothing in place preventing someone from running cleanup or killing a decommed node or something.&lt;/p&gt;

&lt;p&gt;I&apos;m doubtful that the complexity of a correct set of rules for allowing overlapping ring changes is really worth the time/effort/fragility. It doesn&apos;t seem like that much of a loss to me to disallow them. Perhaps your set of rules will be super simple though &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;.&lt;/p&gt;</comment>
                            <comment id="13109763" author="thepaul" created="Wed, 21 Sep 2011 18:40:54 +0000"  >&lt;p&gt;Right, I know how the pending-ranges writes work. But there are still possible openings for consistency violations for previously-written data, similar to the one outlined in the original ticket description. If the bootstrap stream source has an outdated value and it gets duplicated to a bootstrapping node, but then the bootstrap stream source doesn&apos;t leave the replication set (because something else changed in the interim), the outdated value is now on more nodes than it used to be- possibly now QUORUM, when previously it was safely below.&lt;/p&gt;</comment>
                            <comment id="13112905" author="thepaul" created="Thu, 22 Sep 2011 21:01:23 +0000"  >
&lt;p&gt;Ok, prospective approach to totally safe range movements:&lt;/p&gt;

&lt;p&gt;Operational rules:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Cassandra will not allow two range motion operations (move, bootstrap, decom) at the same time on the same node.&lt;/li&gt;
	&lt;li&gt;When a range motion operation is already pending, User should refrain from starting another range motion operation (if either motion operation overlaps the arc-of-effect of the other) until the gossip info about the first change has propagated to all affected nodes. (This is more simply approximated by the &quot;two minute rule&quot;.)&lt;/li&gt;
	&lt;li&gt;Every point in the tokenspace has the same number of natural endpoints, and they&apos;re ordered the same from the perspective of all nodes (is this an ok assumption?).&lt;/li&gt;
	&lt;li&gt;It is User&apos;s responsibility to make sure that the right streaming source nodes are available. If they&apos;re not, the range motion operation may fail.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Procedure:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;For any motion involving range &lt;em&gt;R&lt;/em&gt;, there will be a stream from endpoint &lt;em&gt;EP_source&lt;/em&gt; to endpoint &lt;em&gt;EP_dest&lt;/em&gt;. Given the same information about what range motion operations are pending (&lt;em&gt;TokenMetadata&lt;/em&gt;) and the range &lt;em&gt;R&lt;/em&gt;, there is a bijection from &lt;em&gt;EP_source&lt;/em&gt; to &lt;em&gt;EP_dest&lt;/em&gt;, shared by all nodes in the ring.&lt;/li&gt;
	&lt;li&gt;Procedure to determine &lt;em&gt;EP_source&lt;/em&gt; from &lt;em&gt;EP_dest&lt;/em&gt;:
	&lt;ul&gt;
		&lt;li&gt;Let &lt;em&gt;REP_current&lt;/em&gt; be the existing (ordered) list of natural endpoints for &lt;em&gt;R&lt;/em&gt;.&lt;/li&gt;
		&lt;li&gt;Let &lt;em&gt;TM_future&lt;/em&gt; be a clone of the current &lt;em&gt;TokenMetadata&lt;/em&gt;, but with all ongoing bootstraps, moves, and decoms resolved and completed.&lt;/li&gt;
		&lt;li&gt;Let &lt;em&gt;REP_future&lt;/em&gt; be the list of (ordered) natural endpoints for &lt;em&gt;R&lt;/em&gt; according to &lt;em&gt;TM_future&lt;/em&gt;.&lt;/li&gt;
		&lt;li&gt;Let &lt;em&gt;EPL_entering&lt;/em&gt; be the list of endpoints in &lt;em&gt;REP_future&lt;/em&gt; which are not in &lt;em&gt;REP_current&lt;/em&gt; (preserving their order in &lt;em&gt;REP_future&lt;/em&gt;).&lt;/li&gt;
		&lt;li&gt;Let &lt;em&gt;EPL_leaving&lt;/em&gt; be the list of endpoints in &lt;em&gt;REP_current&lt;/em&gt; which are not in &lt;em&gt;REP_future&lt;/em&gt; (preserving their order in &lt;em&gt;REP_current&lt;/em&gt;).&lt;/li&gt;
		&lt;li&gt;&lt;em&gt;EPL_entering&lt;/em&gt; and &lt;em&gt;EPL_leaving&lt;/em&gt; are of the same length.&lt;/li&gt;
		&lt;li&gt;Let &lt;em&gt;Pos&lt;/em&gt; be the position/index of &lt;em&gt;EP_dest&lt;/em&gt; in &lt;em&gt;EPL_entering&lt;/em&gt;.&lt;/li&gt;
		&lt;li&gt;Let &lt;em&gt;EP_source&lt;/em&gt; be the endpoint at position &lt;em&gt;Pos&lt;/em&gt; in &lt;em&gt;EPL_leaving&lt;/em&gt;.&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Intuitively, this is the same as the rule expressed earlier in this ticket (stream from the node you&apos;ll replace), but also handles other ongoing range movements in the same token arc.&lt;/li&gt;
	&lt;li&gt;These rules can be pretty trivially inverted to determine &lt;em&gt;EP_dest&lt;/em&gt; from &lt;em&gt;EP_source&lt;/em&gt;.&lt;/li&gt;
	&lt;li&gt;When any node gets gossip about a range motion occurring with its token arc-of-effect, it calculates (or recalculates) the streams in which it should be involved. Any ongoing streams which are no longer necessary are canceled, and any newly necessary streams are instigated.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I tried to construct a ruleset without that last rearrange-ongoing-streams rule, but it ended up with a pretty complicated set of extra restrictions, and a more complicated set of procedures than this.&lt;/p&gt;

&lt;p&gt;This set of rules might look complicated, but I think it should be fairly straightforward to implement, and may even end up simpler overall than our current code.&lt;/p&gt;

&lt;p&gt;Note that this procedure even maintains the consistency guarantee in cases like:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;In an RF=3 cluster with nodes A, E, and F, bootstrap B, C, and D in quick succession (E streams to B, F streams to C, A streams to D)&lt;/li&gt;
	&lt;li&gt;In an RF=3 cluster with nodes A, C, and E, bootstrap B, D, and F, and decommission A, C, and E, all in quick succession (A streams to B, C streams to D, E streams to F)&lt;/li&gt;
	&lt;li&gt;In an RF=3 cluster with nodes A, B, C, D, and E, decommission B and C in quick succession (B streams to D, C streams to E)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13112960" author="thepaul" created="Thu, 22 Sep 2011 22:13:38 +0000"  >&lt;p&gt;Clarification: the cleanup operation would need to consider pending ranges in addition to the current natural range-endpoint mapping.&lt;/p&gt;

&lt;p&gt;It would be possible with this proposal for a node &lt;em&gt;M&lt;/em&gt; to be streaming range &lt;em&gt;S&lt;/em&gt; to a bootstrapping node &lt;em&gt;Y&lt;/em&gt;, and midway, for &lt;em&gt;M&lt;/em&gt; to stop being part of the replication set for &lt;em&gt;S&lt;/em&gt; (perhaps some other bootstraps nearby completed first). This should be ok for consistency, but a cleanup operation on &lt;em&gt;M&lt;/em&gt; while the stream is ongoing could potentially remove all the data in &lt;em&gt;S&lt;/em&gt; unless this change is made.&lt;/p&gt;

&lt;p&gt;Further clarification: instead of the W+1 special case we now have for writing to a range &lt;em&gt;T&lt;/em&gt; with a pending motion, we would need to write to W+C replicas instead, where C is the number of pending motions within the replication set of &lt;em&gt;T&lt;/em&gt;.&lt;/p&gt;</comment>
                            <comment id="13151570" author="jbellis" created="Wed, 16 Nov 2011 22:12:25 +0000"  >&lt;p&gt;Coming back to this after the 1.0 scramble.&lt;/p&gt;

&lt;p&gt;It sounds like you have a reasonable solution here, is there any reason not to implement it for 1.1?&lt;/p&gt;</comment>
                            <comment id="13151620" author="thepaul" created="Wed, 16 Nov 2011 23:21:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;It sounds like you have a reasonable solution here, is there any reason not to implement it for 1.1?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Just that it&apos;s quite a bit more complex than simply disallowing overlapping ring movements, and the extra problems that come with higher complexity. I think this feature is worth it, on its own, but when i think of how much pain Brandon seems to be going through dealing with streaming code, maybe it&apos;s not.&lt;/p&gt;</comment>
                            <comment id="13151640" author="jbellis" created="Wed, 16 Nov 2011 23:41:44 +0000"  >&lt;p&gt;No longer very optimistic on the &quot;may even end up simpler overall than our current code&quot; front?&lt;/p&gt;

&lt;p&gt;TBH this area of the code is fragile and hairy and maybe starting from a clean slate with a real plan instead of trying to patch things in haphazardly would be a good thing.&lt;/p&gt;

&lt;p&gt;But, I&apos;d be okay with re-imposing the &quot;no overlapping moves&quot; rule and fixing the stream source problem if that&apos;s going to be substantially simpler.&lt;/p&gt;</comment>
                            <comment id="13152123" author="thepaul" created="Thu, 17 Nov 2011 16:26:45 +0000"  >&lt;p&gt;No, I do think that if we tore out the existing code and replaced it, it would be simpler overall, but (a) that would probably also be true if we rewrote the existing code without implementing this; (b) it will be rather a lot of work; and (c) it may engender a whole new generation of subtle corner-case bugs (or maybe it will eliminate a lot of such bugs that already exist).&lt;/p&gt;</comment>
                            <comment id="13152136" author="jbellis" created="Thu, 17 Nov 2011 16:52:52 +0000"  >&lt;p&gt;How much work would it be to add &quot;just one more bandaid&quot; for the stream source thing, in comparison?&lt;/p&gt;</comment>
                            <comment id="13152140" author="thepaul" created="Thu, 17 Nov 2011 17:06:31 +0000"  >&lt;p&gt;Do you mean to implement the new &quot;safe range movements&quot; procedure outlined above, without rewriting the rest of the range movement code?&lt;/p&gt;

&lt;p&gt;If so, I submit a SWAG of &quot;1/3-1/2 the cost of the rewrite option&quot;. The bandaid would still touch a lot of different moving parts.&lt;/p&gt;</comment>
                            <comment id="13158701" author="jbellis" created="Mon, 28 Nov 2011 20:01:14 +0000"  >&lt;p&gt;So either way, a substantial amount of work, but the bandaid still leaves us with rules that the operator must enforce or hit subtle problems.&lt;/p&gt;

&lt;p&gt;My hang-up with the bandaid is the part where C* can&apos;t effectively enforce the guidelines to be safe.  Even &quot;wait X seconds between bootstrap operations&quot; is not a prereq I am comfortable with.&lt;/p&gt;

&lt;p&gt;Unless the above is incorrect, I think we should bite the bullet and fix it &quot;right.&quot;&lt;/p&gt;</comment>
                            <comment id="13203260" author="thepaul" created="Wed, 8 Feb 2012 04:16:41 +0000"  >&lt;p&gt;So, I believe that the rules outlined above can still work without the &quot;wait X seconds between bootstrap operations&quot; prereq, if a pretty simple extra step is added:&lt;/p&gt;

&lt;p&gt;If any node learns about conflicting move operations, then some rules are applied to choose which will be honored and which will return an error to its caller (if still possible).&lt;/p&gt;

&lt;p&gt;Those rules are:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;A decom for node X beats a move or bootstrap for node X&lt;/li&gt;
	&lt;li&gt;Two decoms for node X from coordinator nodes Y and Z: the coordinator with the higher token wins&lt;/li&gt;
	&lt;li&gt;Any other conflicts between move/bootstrap operations for the same node (which can arise in certain partition situations) are easily resolved by latest VersionedValue.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This should guarantee convergence of TokenMetadata across any affected parts of a cluster.&lt;/p&gt;</comment>
                            <comment id="13204919" author="jbellis" created="Thu, 9 Feb 2012 21:44:58 +0000"  >&lt;p&gt;Sounds reasonable.&lt;/p&gt;</comment>
                            <comment id="13266743" author="jbellis" created="Wed, 2 May 2012 17:32:00 +0000"  >&lt;p&gt;As a half measure, we can stream from the &quot;right&quot; node very easily if we continue to make the simplifying assumption that no other node movement happens in overlapping ranges during the operation.&lt;/p&gt;</comment>
                            <comment id="13905719" author="tjake" created="Wed, 19 Feb 2014 17:31:29 +0000"  >&lt;p&gt;I&apos;ve taken a crack at this, initially for 1.2 since it solves my pain. Appreciate a review.&lt;/p&gt;

&lt;p&gt;As &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jbellis&quot; class=&quot;user-hover&quot; rel=&quot;jbellis&quot;&gt;jbellis&lt;/a&gt; mentions above it requires only one node to be added at a time. Also bootstrapping node must add -Dconsistent.bootstrap=true&lt;/p&gt;

&lt;p&gt;#code&lt;br/&gt;
&lt;a href=&quot;https://github.com/tjake/cassandra/tree/2434&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tjake/cassandra/tree/2434&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;#dtest showing it works (use ENABLE_VNODES=yes)&lt;br/&gt;
&lt;a href=&quot;https://github.com/tjake/cassandra-dtest/tree/2434&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tjake/cassandra-dtest/tree/2434&lt;/a&gt;&lt;/p&gt;
</comment>
                            <comment id="13905819" author="jbellis" created="Wed, 19 Feb 2014 18:33:57 +0000"  >&lt;p&gt;(&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=thobbs&quot; class=&quot;user-hover&quot; rel=&quot;thobbs&quot;&gt;thobbs&lt;/a&gt; to review)&lt;/p&gt;</comment>
                            <comment id="13906072" author="thobbs" created="Wed, 19 Feb 2014 21:05:31 +0000"  >&lt;p&gt;Thanks, Jake.&lt;/p&gt;

&lt;p&gt;I strongly prefer to default to the strict/safe behavior and make the user supply a &quot;force&quot; option for non-strict behavior, like Nick and Paul agreed on above.  If the bootstrapping node cannot stream from the correct replica and the &quot;force&quot; option isn&apos;t set, it should abort the bootstrap with an error that describes the implications and mentions how to use the &quot;force&quot; option.&lt;/p&gt;

&lt;p&gt;Additionally, I think your logic for picking the preferred replica could be greatly simplified.  Paul&apos;s 2434-3.patch.txt has a really simple version of this and also has the strict-by-default behavior.  It might be worthwhile to look at rebasing that patch as a start.&lt;/p&gt;

&lt;p&gt;Paul mentioned this:&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Conversation on #cassandra-dev resulted in the conclusion that we&apos;ll fix this bug for range acquisition (bootstrap and move) now, and plan to allow the same looseness (non-strict mode, or whatever) for range egress (move and decom) in the future.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Looking at the irc logs, there wasn&apos;t a strong reason for this.  There&apos;s a lot of code overlap there, so it would be ideal to fix both types of operations at once.  Do you think you could take a stab at that?&lt;/p&gt;</comment>
                            <comment id="13906211" author="tjake" created="Wed, 19 Feb 2014 22:29:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;Additionally, I think your logic for picking the preferred replica could be greatly simplified. Paul&apos;s 2434-3.patch.txt has a really simple version of this and also has the strict-by-default behavior. It might be worthwhile to look at rebasing that patch as a start.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I did look at the patch and I&apos;ll see how I can simplify my version.  Most of the complexity comes from multiple ranges living on the same address (vnodes). Which the old version didn&apos;t have to worry about.&lt;/p&gt;

&lt;p&gt;I do think we can fix the other operations but those are less of a priority IMO and should be part of a follow up.  (does anyone use move with vnodes?)  &lt;/p&gt;
</comment>
                            <comment id="13906223" author="brandon.williams" created="Wed, 19 Feb 2014 22:39:18 +0000"  >&lt;blockquote&gt;&lt;p&gt;does anyone use move with vnodes?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;No, but now they can use relocate (taketoken in nodetool)&lt;/p&gt;</comment>
                            <comment id="13907318" author="tjake" created="Thu, 20 Feb 2014 18:57:17 +0000"  >&lt;p&gt;Pushed an update to &lt;a href=&quot;https://github.com/tjake/cassandra/tree/2434&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tjake/cassandra/tree/2434&lt;/a&gt; that addresses the comments.  I&apos;m going to work on support for move/relocate.&lt;/p&gt;</comment>
                            <comment id="13907468" author="tjake" created="Thu, 20 Feb 2014 20:41:56 +0000"  >&lt;p&gt;Do we care about decommissions?  It seems when we &quot;push&quot; data to other nodes there isn&apos;t anything todo.  Only when we pick the replica to stream from does this ticket apply&lt;/p&gt;</comment>
                            <comment id="13907528" author="thobbs" created="Thu, 20 Feb 2014 21:27:47 +0000"  >&lt;blockquote&gt;&lt;p&gt;Do we care about decommissions? It seems when we &quot;push&quot; data to other nodes there isn&apos;t anything todo. Only when we pick the replica to stream from does this ticket apply&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I checked that &lt;tt&gt;StorageService.getChangedRangesForLeaving()&lt;/tt&gt; pushes to the correct nodes (those that are gaining a range), so you&apos;re right, we don&apos;t need to do anything new for decom.&lt;/p&gt;</comment>
                            <comment id="13907620" author="tjake" created="Thu, 20 Feb 2014 22:22:19 +0000"  >&lt;p&gt;Updated branches with move/relocate support and added a dtest for move (in dtest branch linked above)&lt;/p&gt;</comment>
                            <comment id="13907635" author="brandon.williams" created="Thu, 20 Feb 2014 22:33:32 +0000"  >&lt;p&gt;We should probably add a test for relocate too since it&apos;s fundamentally different from move.&lt;/p&gt;</comment>
                            <comment id="13908769" author="thobbs" created="Fri, 21 Feb 2014 20:12:14 +0000"  >&lt;p&gt;I tested bootstrapping a node while the preferred replica was down.  It turns out that &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6385&quot; title=&quot;FD phi estimator initial conditions&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6385&quot;&gt;&lt;del&gt;CASSANDRA-6385&lt;/del&gt;&lt;/a&gt; makes the bootstrapping node consider the replica up for long enough to pass the checks.  It looks like we need to special case the 6385 behavior for bootstraps if we want this patch to work.&lt;/p&gt;</comment>
                            <comment id="13908788" author="brandon.williams" created="Fri, 21 Feb 2014 20:24:39 +0000"  >&lt;p&gt;You can test this now by setting cassandra.fd_initial_value_ms.&lt;/p&gt;</comment>
                            <comment id="13909057" author="thobbs" created="Sat, 22 Feb 2014 00:42:09 +0000"  >&lt;p&gt;Okay, with the workaround on the FD, bootstrap seems to work.  Do we want to split that fix into a separate ticket?&lt;/p&gt;

&lt;p&gt;However, relocate seems to be seriously broken.  With a three node cluster and one of the nodes down, I can make relocate fail in a couple of ways:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;oldEndpoints&lt;/tt&gt; == &lt;tt&gt;newEndpoints&lt;/tt&gt;, so the assertion that the difference between them has length 1 fails&lt;/li&gt;
	&lt;li&gt;There are no ranges that contain the &quot;desiredRange&quot;, resulting in the IllegalStateException being thrown (&quot;No sources found for &quot; + toFetch);&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;With that said, nothing (including the tools) uses relocate.  (EDIT: shuffle uses it, but nobody uses shuffle in practice due to other problems.) The JMX version doesn&apos;t work with jconsole, so I had to add a method to test this.  I&apos;m not even sure that relocate worked before this patch for vnodes, because there&apos;s only minimal test coverage for relocate.  IMO, we shouldn&apos;t even try to modify this without good test coverage.  But if nothing even uses relocate... I&apos;m not sure what to do.  Thoughts?&lt;/p&gt;</comment>
                            <comment id="13913251" author="tjake" created="Wed, 26 Feb 2014 18:09:07 +0000"  >&lt;p&gt;I will try working on a relocate test and look at addressing this&lt;/p&gt;</comment>
                            <comment id="13934984" author="tjake" created="Fri, 14 Mar 2014 13:22:25 +0000"  >&lt;p&gt;What version should this go into?  I personally need this for 1.2 but I would put it in with the default of non-strict.&lt;/p&gt;</comment>
                            <comment id="13939694" author="thobbs" created="Tue, 18 Mar 2014 19:45:33 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt; I think we want 2.1 with a default of strict.&lt;/p&gt;</comment>
                            <comment id="13984693" author="tjake" created="Tue, 29 Apr 2014 19:37:53 +0000"  >&lt;p&gt;Rebased to 2.1 branch and pushed to &lt;a href=&quot;https://github.com/tjake/cassandra/tree/2434-2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tjake/cassandra/tree/2434-2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Also added a relocation dtest &lt;a href=&quot;https://github.com/tjake/cassandra-dtest/tree/2434&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tjake/cassandra-dtest/tree/2434&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="13986144" author="thobbs" created="Wed, 30 Apr 2014 21:57:12 +0000"  >&lt;p&gt;+1 overall, with a few minor nitpicks on the dtest:&lt;/p&gt;

&lt;p&gt;You can replace that string-building loop with:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;tl = &quot; &quot;.join(str(t) for t in tokens[0][:8])
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;There&apos;s also a leftover line: &lt;tt&gt;#assert 1 == 0&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Don&apos;t forget to mark the new tests with a &lt;tt&gt;@since(&apos;2.1&apos;)&lt;/tt&gt; decorator.&lt;/p&gt;</comment>
                            <comment id="13986589" author="tjake" created="Thu, 1 May 2014 13:52:35 +0000"  >&lt;p&gt;Committed c* code , I&apos;ll push to dtests now&lt;/p&gt;</comment>
                            <comment id="14069309" author="kohlisankalp" created="Mon, 21 Jul 2014 21:18:15 +0000"  >&lt;p&gt;This will be very nice to have in 2.0&lt;br/&gt;
cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=brandon.williams&quot; class=&quot;user-hover&quot; rel=&quot;brandon.williams&quot;&gt;brandon.williams&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14069351" author="brandon.williams" created="Mon, 21 Jul 2014 21:44:49 +0000"  >&lt;p&gt;Does seem like mostly new code we could just add with the flag defaulting to off to give 2.0 the option.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10032">
                    <name>Blocker</name>
                                            <outwardlinks description="blocks">
                                        <issuelink>
            <issuekey id="12532274">CASSANDRA-3516</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12493677" name="2434-3.patch.txt" size="14505" author="thepaul" created="Thu, 8 Sep 2011 21:57:34 +0000"/>
                            <attachment id="12493374" name="2434-testery.patch.txt" size="2170" author="thepaul" created="Wed, 7 Sep 2011 20:11:13 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>2.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[tjake]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>20623</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            11 years, 18 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i07z8f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>44475</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>thobbs</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[thobbs]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>