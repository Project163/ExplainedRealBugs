<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:16:56 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-1216] removetoken drops node from ring before re-replicating its data is finished</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-1216</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;this means that if something goes wrong during the re-replication (e.g. a source node is restarted) there is (a) no indication that anything has gone wrong and (b) no way to restart the process (other than the Big Hammer of running repair)&lt;/p&gt;</description>
                <environment></environment>
        <key id="12467577">CASSANDRA-1216</key>
            <summary>removetoken drops node from ring before re-replicating its data is finished</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="nickmbailey">Nick Bailey</assignee>
                                    <reporter username="jbellis">Jonathan Ellis</reporter>
                        <labels>
                    </labels>
                <created>Tue, 22 Jun 2010 14:28:20 +0000</created>
                <updated>Tue, 16 Apr 2019 09:33:21 +0000</updated>
                            <resolved>Tue, 28 Sep 2010 05:56:10 +0000</resolved>
                                        <fixVersion>0.7 beta 2</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>1</watches>
                                                                                                                <comments>
                            <comment id="12884714" author="nickmbailey" created="Fri, 2 Jul 2010 15:16:54 +0000"  >&lt;p&gt;A possible solution I see for this is to keep nodes in the justRemovedEndpoints map in Gossiper until we can verify that replication has completed.  I think we could accomplish verification through a callback on the replicate request.  I&apos;m unsure about what data gets persisted so I don&apos;t know if a restart would wipe out the justRemovedEndpoints map&lt;/p&gt;</comment>
                            <comment id="12884736" author="nickmbailey" created="Fri, 2 Jul 2010 16:12:01 +0000"  >&lt;p&gt;So clearly that solution would fail in the case of the node that is attempting to retrive the data failing.  Perhaps a better solution is simply not removing the node until replication and done.  Perhaps marking it with a new state?&lt;/p&gt;</comment>
                            <comment id="12885653" author="nickmbailey" created="Tue, 6 Jul 2010 19:41:19 +0000"  >&lt;p&gt;It seems like this should follow a pattern similar to decommissioning a node.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;If nodeA has removeToken called on it, it becomes responsible for nodeB, the node to remove&lt;/li&gt;
	&lt;li&gt;nodeA sets the MOVE_STATE of nodeB to STATE_REMOVING&lt;/li&gt;
	&lt;li&gt;This is gossipped throughout the ring.&lt;/li&gt;
	&lt;li&gt;Nodes see this change and fetch any ranges they are becoming responsible for
	&lt;ul&gt;
		&lt;li&gt;After this is complete they will need to notify nodeA somehow that this is complete&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;Once nodeA sees all replications have finished, change state of nodeB to STATE_REMOVED&lt;/li&gt;
	&lt;li&gt;All nodes then remove nodeB from their ring.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12885655" author="jbellis" created="Tue, 6 Jul 2010 19:42:56 +0000"  >&lt;p&gt;agreed&lt;/p&gt;</comment>
                            <comment id="12885702" author="nickmbailey" created="Tue, 6 Jul 2010 21:46:59 +0000"  >&lt;p&gt;A side effect of this approach may be that you would need to call removeToken on a node that had seen the token previously.&lt;/p&gt;</comment>
                            <comment id="12885730" author="jbellis" created="Tue, 6 Jul 2010 23:19:05 +0000"  >&lt;p&gt;since all tokens will be propagated to all nodes (even ones brought up after the dead node went down), that&apos;s not a problem&lt;/p&gt;</comment>
                            <comment id="12890489" author="nickmbailey" created="Tue, 20 Jul 2010 22:37:17 +0000"  >&lt;ul&gt;
	&lt;li&gt;0001 - changes to make removeToken behave similarly to decomission&lt;/li&gt;
	&lt;li&gt;0002 - fixes to existing tests since the state for STATE_LEFT changed&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I am still working on some good unit tests for these changes but these are the changes so far.&lt;/p&gt;

&lt;p&gt;The new process for removeToken is basically the one outlined above. One change is that instead of a STATE_REMOVED state it seemed like tokens that are removed should just go into STATE_LEFT similar to nodes that are decommissioned.&lt;/p&gt;

&lt;p&gt;One thing I&apos;m not sure of is the timeout values for waiting for replications to stream and for waiting for replication notifications. Currently they are just set arbitrarily in that patch. Need to determine good values for these.&lt;/p&gt;</comment>
                            <comment id="12892922" author="nickmbailey" created="Tue, 27 Jul 2010 20:53:38 +0000"  >&lt;p&gt;Some fixes and tests added.&lt;/p&gt;

&lt;p&gt;There is one thing that still needs to be fixed.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Currently the call to removeToken blocks either:
	&lt;ul&gt;
		&lt;li&gt;until all nodes confirm that they have replicated the data for the dead node.&lt;/li&gt;
		&lt;li&gt;or a timeout is reached&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;I&apos;m not sure what the timeout for this should be. Additionally when nodes throughout the ring attempt to replicate data there should be a similar timeout before they give up on a source and retry.&lt;/li&gt;
	&lt;li&gt;Also clients may timeout before the timeout is even reached or all the data is replicated. I&apos;m not sure how the user will be able to determine if the remove finished correctly or repair should be run.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12892944" author="nickmbailey" created="Tue, 27 Jul 2010 21:39:02 +0000"  >&lt;p&gt;Updated 0001 patch. It was missing a class before. Oops.&lt;/p&gt;</comment>
                            <comment id="12898253" author="gdusbabek" created="Fri, 13 Aug 2010 14:36:12 +0000"  >&lt;p&gt;Nick, can you rebase?&lt;/p&gt;</comment>
                            <comment id="12898400" author="nickmbailey" created="Fri, 13 Aug 2010 20:11:21 +0000"  >&lt;p&gt;Rebased.&lt;/p&gt;</comment>
                            <comment id="12898442" author="nickmbailey" created="Fri, 13 Aug 2010 22:18:48 +0000"  >&lt;p&gt;Re-rebased.&lt;/p&gt;</comment>
                            <comment id="12899499" author="gdusbabek" created="Tue, 17 Aug 2010 18:08:08 +0000"  >&lt;p&gt;RemoveTest needs some cleanup.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;ReplicationSink doesn&apos;t need callCount&lt;/li&gt;
	&lt;li&gt;NotificationSink doesn&apos;t need hitList&lt;/li&gt;
	&lt;li&gt;testRemoveToken and testStartRemoving abuse Gossiper.start().  Consider adding a method to Gossiper that initializes the epstate for a given node.  E.g.: initializeNodeUnsafe(InetAddr addr, int generation).&lt;/li&gt;
	&lt;li&gt;(minor nit) I wish there were a way to assert that tmd.getLeavingNodes() actually has nodes in it.&lt;/li&gt;
	&lt;li&gt;all the methods throw UnknownHostException, but don&apos;t need to (IOException covers it)&lt;/li&gt;
	&lt;li&gt;testStartRemoving should assert preconditions before calling ss.onChange (it also makes the same assertion twice).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;StorageService:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;(minor nit) a comment describing the distinction between the leaving and removing constants.&lt;/li&gt;
	&lt;li&gt;SS.removeToken() shouldn&apos;t throw a RuntimeException, as the client won&apos;t know what to make of it.  Declare an exception in the interface and throw it in the impl.  I imagine this will be a fairly common case (e.g.: when a node is down).&lt;/li&gt;
	&lt;li&gt;SS.setReplicatingNodes and clearReplicatingNodes can be inlined into removeToken. It saves a few lines and obviates a local var.&lt;/li&gt;
	&lt;li&gt;SS.replicateTables should probably be merged into SS.restoreReplicaCount.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Was the intent that SS.replicateTables block until the files are transferred?  Because it doesn&apos;t.  AFAICT it blocks until the first ack comes back from each source node, which is a good indication that streaming has started, but not that it is finished.&lt;/p&gt;

&lt;p&gt;I couldn&apos;t verify that the callbacks are ever called.  That happens on the READ_RESPONSE stage and afaict, none of the streaming code path ever puts a task there.  That&apos;s a painful interface to follow though, so I might be wrong.&lt;/p&gt;</comment>
                            <comment id="12899993" author="nickmbailey" created="Wed, 18 Aug 2010 19:51:13 +0000"  >&lt;p&gt;Yeah I wasn&apos;t really understanding that streaming/messaging code at all.&lt;/p&gt;

&lt;p&gt;The current StreamOut implementation has a callback concept however.  I think this should be moved into the StreamContext object and then both StreamOut and StreamIn can perform callbacks on actual stream completion.  &lt;/p&gt;</comment>
                            <comment id="12900015" author="gdusbabek" created="Wed, 18 Aug 2010 20:15:40 +0000"  >&lt;p&gt;The StreamOut callback works differently than the MessagingService callback.  Your approach sounds workable.  I don&apos;t think it matters where you push the callback to, so long as you make sure it gets executed after the stream is finished.&lt;/p&gt;</comment>
                            <comment id="12901589" author="nickmbailey" created="Mon, 23 Aug 2010 21:12:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;(minor nit) I wish there were a way to assert that tmd.getLeavingNodes() actually has nodes in it.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is what tmd.isLeaving() does&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;testStartRemoving should assert preconditions before calling ss.onChange (it also makes the same assertion twice).&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not sure what preconditions you mean. I added an assertion to make sure there are no endpoints already leaving.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;SS.removeToken() shouldn&apos;t throw a RuntimeException,&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Do you think the UnsupportedOperationExceptions should be removed as well? These existed previously.&lt;/p&gt;

&lt;p&gt;I modified the callback support for streaming so that the code should wait for all streams to finish before confirming. I also added a reply to the ReplicationFinishedHandler so the IAsyncResult will be updated.  &lt;/p&gt;

&lt;p&gt;Thoughts?&lt;/p&gt;

&lt;p&gt;The timeout values for waiting on the latches still need to be updated.&lt;/p&gt;</comment>
                            <comment id="12902468" author="gdusbabek" created="Wed, 25 Aug 2010 14:51:51 +0000"  >&lt;p&gt;&amp;gt; Do you think the UnsupportedOperationExceptions should be removed as well? These existed previously.&lt;br/&gt;
My bad; I didn&apos;t notice that.  RTE was probably ok.&lt;/p&gt;

&lt;p&gt;&amp;gt; I modified the callback support for streaming so that the code should wait for all streams to finish before confirming. I also added a reply to the ReplicationFinishedHandler so the IAsyncResult will be updated&lt;br/&gt;
First glance tells me this will work.  I&apos;ll run some tests after I&apos;m done reviewing.&lt;/p&gt;

&lt;p&gt;&amp;gt; The timeout values for waiting on the latches still need to be updated.&lt;br/&gt;
Is this coming in another patch?&lt;/p&gt;</comment>
                            <comment id="12902503" author="gdusbabek" created="Wed, 25 Aug 2010 16:30:39 +0000"  >&lt;p&gt;I see this in RemoveTest:&lt;/p&gt;


&lt;p&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; Testsuite: org.apache.cassandra.service.RemoveTest&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 1.97 sec&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; &lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ------------- Standard Error -----------------&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ERROR 11:27:58,277 Did not find matching ranges on /127.0.0.6&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ERROR 11:27:58,279 Did not find matching ranges on /127.0.0.6&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ERROR 11:27:58,280 Did not find matching ranges on /127.0.0.5&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ERROR 11:27:58,280 Did not find matching ranges on /127.0.0.4&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ERROR 11:27:58,280 Did not find matching ranges on /127.0.0.2&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ERROR 11:27:59,264 Did not find matching ranges on /127.0.0.6&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ERROR 11:27:59,272 Did not find matching ranges on /127.0.0.6&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ERROR 11:27:59,276 Did not find matching ranges on /127.0.0.5&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ERROR 11:27:59,279 Did not find matching ranges on /127.0.0.4&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ERROR 11:27:59,283 Did not find matching ranges on /127.0.0.2&lt;br/&gt;
    &lt;span class=&quot;error&quot;&gt;&amp;#91;junit&amp;#93;&lt;/span&gt; ------------- ---------------- ---------------&lt;/p&gt;

&lt;p&gt;Is that ok?&lt;/p&gt;</comment>
                            <comment id="12902509" author="nickmbailey" created="Wed, 25 Aug 2010 16:38:50 +0000"  >&lt;p&gt;Re: timeouts&lt;/p&gt;

&lt;p&gt;Yes I&apos;m just not sure how to approach determining the right values for these.  Depends mostly on the amount of data and network bandwidth.&lt;/p&gt;

&lt;p&gt;Re: RemoveTest&lt;/p&gt;

&lt;p&gt;Yeah. The message sink in the test immediately responds to the stream request saying there are no files to stream.  This makes the StreamInManager think the data didn&apos;t exist remotely.  Doing it that way seems much easier than trying to make the test actually stream something.&lt;/p&gt;</comment>
                            <comment id="12902511" author="gdusbabek" created="Wed, 25 Aug 2010 16:42:14 +0000"  >&lt;p&gt;Some questions about the coordinator...  I see that removeToken() is quasi-blocking now, like unbootstrap() (it was fire-and-forget before).  What are the consequences of the coordinator node going down?  Assuming a dead coordinator, would it be Bad for another node to remove-token on the same token while the transfers initiated by the original failed coordinator were in process?  Or assuming the transfers were finished, would a remove-token on a new coordinator generally do little other than get the state to LEFT?&lt;/p&gt;

&lt;p&gt;I think I&apos;m of the opinion that removeToken should either block until the transfer is complete (or failed), or should return instantly, and that we need to make sure that subsequent removeToken calls do not upset existing transfers.  Having it return error after a timeout (which is possible in the case of LOTS of data) makes me think we should be doing differently.&lt;/p&gt;

&lt;p&gt;Or is the only recourse to repair?&lt;/p&gt;</comment>
                            <comment id="12902523" author="nickmbailey" created="Wed, 25 Aug 2010 17:06:01 +0000"  >&lt;p&gt;I believe the only consequences of calling removeToken on another node when the coordinator goes down would be that the entire operation would be repeated. So any data that was transferred before would be transferred again.  I think this is the right behavior since there is no way of knowing what was transferred before the coordinator went down.  &lt;/p&gt;

&lt;p&gt;It might be useful to add a &apos;force&apos; option though.  If the coordinator goes down and the token gets stuck in a REMOVING state you may want to force removal rather than redoing the entire operation. &lt;/p&gt;

&lt;p&gt;It should be possible to remove the timeout so that removeToken blocks until the transfer is completely finished.  The code for streaming in the remote data blocks until all streams are complete and the code for sending a confirmation to the coordinator will keep retrying until it is received or the coordinator dies.  &lt;/p&gt;

&lt;p&gt;I think this would work if a check was added so that you can only call removeToken a second time if the coordinator is down.  It wouldn&apos;t handle two calls that occurred before the state made its way through gossip though.  &lt;/p&gt;
</comment>
                            <comment id="12903538" author="nickmbailey" created="Fri, 27 Aug 2010 19:06:33 +0000"  >&lt;p&gt;After some more thinking I think there are two problems here.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;The timeout for waiting on a stream to complete - An arbitrary timeout here is not the right way to do this. What we really need is the concept of stream progress. We should be able to verify that a stream is progressing or not and based on that retry it.  &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-1438&quot; title=&quot;Stream*Manager doesn&amp;#39;t clean up broken Streams&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-1438&quot;&gt;&lt;del&gt;CASSANDRA-1438&lt;/del&gt;&lt;/a&gt; kind of relates to this problem and could be modified to implement this.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The timeout waiting for nodes to confirm replication - Ideally there could be no timeout here. The problem though is if a node that should be grabbing data goes down permanently, removeToken will wait forever.  I think it&apos;s reasonable to have some sort of timeout in this case. A log message/error can indicate which machines were being waited on for replication. An administrator should know if that machine went down or is still streaming. That will determine if repair needs to be run.  The alternative to this I guess would be periodically waking up and checking that the nodes we are waiting on are still alive.  That wouldn&apos;t be particularly hard to implement&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I don&apos;t think returning immediately from the call is the right approach.  That is part of the reason why this ticket is created. In the case that replication fails somewhere, there is no feedback to the user.  At least timing out eventually provides information about which machines we think failed to replicate data.  &lt;/p&gt;

&lt;p&gt;As far as multiple remove calls and the coordinator going down.  I think there should be a &apos;force&apos; option in the case the coordinator goes down and you believe the rest of the nodes completed the operation.  To prevent multiple calls to removeToken there should just be a check to make sure the coordinator is dead before another call can be performed.&lt;/p&gt;

&lt;p&gt;So besides those few changes above, I think we should either implement this part way with a time out for stream replication or postpone completion here until we add the concept of stream progress.&lt;/p&gt;</comment>
                            <comment id="12913206" author="nickmbailey" created="Tue, 21 Sep 2010 19:43:39 +0000"  >&lt;p&gt;Patches:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;0001
	&lt;ul&gt;
		&lt;li&gt;Modifies the removeToken operation to follow a pattern of NORMAL-&amp;gt;REMOVING-&amp;gt;LEFT, rather than the current pattern of a coordinator node setting its own status to a special cased version of NORMAL.&lt;/li&gt;
		&lt;li&gt;Fixes a small bug in StreamHeader serialization&lt;/li&gt;
		&lt;li&gt;Adds the ability to either get the status of a remove operation taking place or force a remove operation to finish immediately&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;0002
	&lt;ul&gt;
		&lt;li&gt;Tests for removing tokens&lt;/li&gt;
		&lt;li&gt;Move shared code for creating a ring to Util class&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;Removal Process:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Normal Case
	&lt;ol&gt;
		&lt;li&gt;Coordinator sets status of failed node to REMOVING&lt;/li&gt;
		&lt;li&gt;Coordinator blocks on confirmation from other nodes&lt;/li&gt;
		&lt;li&gt;Any newly responsible nodes stream data&lt;/li&gt;
		&lt;li&gt;Newly responsible nodes send confirmation once all data has streamed&lt;/li&gt;
		&lt;li&gt;Coordinator updates status of failed node to LEFT&lt;/li&gt;
		&lt;li&gt;Done&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
	&lt;li&gt;Failure Cases
	&lt;ul&gt;
		&lt;li&gt;Coordinator failure
		&lt;ul&gt;
			&lt;li&gt;If the coordinator fails the remove operation will need to be retried&lt;/li&gt;
			&lt;li&gt;This can be done on any node in the cluster.&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
		&lt;li&gt;Newly responsible node failure
		&lt;ul&gt;
			&lt;li&gt;If a newly responsible node fails but comes back up, it should see the REMOVING status in gossip and restart the operation&lt;/li&gt;
			&lt;li&gt;If a newly responsible node fails permanently or a streaming operation fails and the node stays up, the coordinator will block forever while waiting for confirmation.  The best solution is to force the remove operation to complete and then run repair on the failed node.&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="12913634" author="nickmbailey" created="Wed, 22 Sep 2010 16:01:26 +0000"  >&lt;p&gt;Bah.  Gossip marks the node alive when it receives an updated application state. Reverting it to modifying the coordinator nodes state.&lt;/p&gt;</comment>
                            <comment id="12913764" author="nickmbailey" created="Wed, 22 Sep 2010 20:29:38 +0000"  >&lt;p&gt;Ok this should be ready for review now.  The process is:&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;Coordinator node modifies its own status to NORMAL - REMOVING to indicate which node is being removed&lt;/li&gt;
	&lt;li&gt;Coordinator blocks on removal confirmaton from other nodes&lt;/li&gt;
	&lt;li&gt;Newly responsible nodes see this status and begin fetching new data&lt;/li&gt;
	&lt;li&gt;Newly responsible nodes notify coordinator they have replicated all data&lt;/li&gt;
	&lt;li&gt;Coordinator node updates its own status to NORMAL - REMOVED to indicate the removal is complete&lt;/li&gt;
	&lt;li&gt;This causes all nodes to remove the node from gossip/tokenmetadata.&lt;/li&gt;
	&lt;li&gt;Done&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Tested this with a 3 node cluster in the cloud, as well as testing the new getStatus and forceRemoval operations.&lt;/p&gt;</comment>
                            <comment id="12915227" author="gdusbabek" created="Mon, 27 Sep 2010 07:58:07 +0000"  >&lt;p&gt;This looks good.&lt;/p&gt;

&lt;p&gt;1.  There were a few unused local variables in SS.retoreReplicationCount().  Was this just leftovers from a rebase?&lt;br/&gt;
2.  SS.handleStateRemoving removes a null check that previously existed for epThatLeft (renamed removeEndpoint).  Was the original null-check pointless or was something missed in the change?&lt;br/&gt;
3.  You made a change to StreamHeader that made me think you were running into cases where SH.pendingFiles == null.  Is that true?  Tracing the codepaths makes me think this is not possible.&lt;/p&gt;

&lt;p&gt;Don&apos;t bother with the cleanup in 1.  I&apos;m more curious about 2 and 3.&lt;/p&gt;</comment>
                            <comment id="12915340" author="nickmbailey" created="Mon, 27 Sep 2010 15:54:58 +0000"  >&lt;p&gt;1 and 2 are just errors on my part. I changed 3 because I was under the impression that a stream request to an endpoint that doesn&apos;t contain any of the ranges requested would create a header with null for pendingFiles.  I at first wrote one of the tests to behave like that, and got the NPE.  Looks like it changed or was never like that.&lt;/p&gt;

&lt;p&gt;Fixed all that in a quick patch and attached it.&lt;/p&gt;</comment>
                            <comment id="12915629" author="gdusbabek" created="Tue, 28 Sep 2010 05:56:09 +0000"  >&lt;p&gt;committed.&lt;/p&gt;</comment>
                            <comment id="12915742" author="hudson" created="Tue, 28 Sep 2010 13:31:34 +0000"  >&lt;p&gt;Integrated in Cassandra #549 (See &lt;a href=&quot;https://hudson.apache.org/hudson/job/Cassandra/549/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://hudson.apache.org/hudson/job/Cassandra/549/&lt;/a&gt;)&lt;br/&gt;
    changes update for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-1216&quot; title=&quot;removetoken drops node from ring before re-replicating its data is finished&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-1216&quot;&gt;&lt;del&gt;CASSANDRA-1216&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
modify removetoken so that the coordinator relies on replicating nodes for updates. patch by Nick Bailey, reviewed by Gary Dusbabek. &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-1216&quot; title=&quot;removetoken drops node from ring before re-replicating its data is finished&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-1216&quot;&gt;&lt;del&gt;CASSANDRA-1216&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12455298" name="0001-Modify-removeToken-to-be-similar-to-decommission.patch" size="31525" author="nickmbailey" created="Wed, 22 Sep 2010 20:24:11 +0000"/>
                            <attachment id="12455299" name="0002-Additional-tests-for-removeToken.patch" size="23159" author="nickmbailey" created="Wed, 22 Sep 2010 20:24:11 +0000"/>
                            <attachment id="12455666" name="0003-Fixes-from-review.patch" size="2584" author="nickmbailey" created="Mon, 27 Sep 2010 15:54:58 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[nickmbailey]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>20035</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            15 years, 9 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0g3pj:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>92031</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>gdusbabek</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[gdusbabek]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>