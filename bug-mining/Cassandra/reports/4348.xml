<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:03:33 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-11349] MerkleTree mismatch when multiple range tombstones exists for the same partition and interval</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-11349</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;We observed that repair, for some of our clusters, streamed a lot of data and many partitions were &quot;out of sync&quot;.&lt;br/&gt;
Moreover, the read repair mismatch ratio is around 3% on those clusters, which is really high.&lt;/p&gt;

&lt;p&gt;After investigation, it appears that, if two range tombstones exists for a partition for the same range/interval, they&apos;re both included in the merkle tree computation.&lt;br/&gt;
But, if for some reason, on another node, the two range tombstones were already compacted into a single range tombstone, this will result in a merkle tree difference.&lt;br/&gt;
Currently, this is clearly bad because MerkleTree differences are dependent on compactions (and if a partition is deleted and created multiple times, the only way to ensure that repair &quot;works correctly&quot;/&quot;don&apos;t overstream data&quot; is to major compact before each repair... which is not really feasible).&lt;/p&gt;

&lt;p&gt;Below is a list of steps allowing to easily reproduce this case:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;ccm create test -v 2.1.13 -n 2 -s
ccm node1 cqlsh
CREATE KEYSPACE test_rt WITH replication = {&apos;class&apos;: &apos;SimpleStrategy&apos;, &apos;replication_factor&apos;: 2};
USE test_rt;
CREATE TABLE IF NOT EXISTS table1 (
    c1 text,
    c2 text,
    c3 float,
    c4 float,
    PRIMARY KEY ((c1), c2)
);
INSERT INTO table1 (c1, c2, c3, c4) VALUES ( &apos;a&apos;, &apos;b&apos;, 1, 2);
DELETE FROM table1 WHERE c1 = &apos;a&apos; AND c2 = &apos;b&apos;;
ctrl ^d
# now flush only one of the two nodes
ccm node1 flush 
ccm node1 cqlsh
USE test_rt;
INSERT INTO table1 (c1, c2, c3, c4) VALUES ( &apos;a&apos;, &apos;b&apos;, 1, 3);
DELETE FROM table1 WHERE c1 = &apos;a&apos; AND c2 = &apos;b&apos;;
ctrl ^d
ccm node1 repair
# now grep the log and observe that there was some inconstencies detected between nodes (while it shouldn&apos;t have detected any)
ccm node1 showlog | grep &quot;out of sync&quot;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Consequences of this are a costly repair, accumulating many small SSTables (up to thousands for a rather short period of time when using VNodes, the time for compaction to absorb those small files), but also an increased size on disk.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12949698">CASSANDRA-11349</key>
            <summary>MerkleTree mismatch when multiple range tombstones exists for the same partition and interval</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="blambov">Branimir Lambov</assignee>
                                    <reporter username="frousseau">Fabien Rousseau</reporter>
                        <labels>
                            <label>repair</label>
                    </labels>
                <created>Sun, 13 Mar 2016 11:48:45 +0000</created>
                <updated>Tue, 16 Apr 2019 09:30:39 +0000</updated>
                            <resolved>Tue, 5 Jul 2016 09:27:30 +0000</resolved>
                                        <fixVersion>2.1.16</fixVersion>
                    <fixVersion>2.2.8</fixVersion>
                                        <due></due>
                            <votes>3</votes>
                                    <watches>17</watches>
                                                                                                                <comments>
                            <comment id="15204971" author="spodxx@gmail.com" created="Mon, 21 Mar 2016 19:51:34 +0000"  >&lt;p&gt;Looks like the &lt;tt&gt;MergeIterator.ManyToOne&lt;/tt&gt; logic gets in the way of &lt;tt&gt;LazilyCompactedRow.Reducer&lt;/tt&gt; doing it&apos;s job. The iterator will stop adding atoms to the reducer and continue to advance, once two range tombstones with different deletion times are about to be merged.&lt;/p&gt;</comment>
                            <comment id="15208345" author="spodxx@gmail.com" created="Wed, 23 Mar 2016 12:37:21 +0000"  >

&lt;p&gt;I gave the patch some more thoughts and I&apos;m now confident that the proposed change is the best way to address the issue. &lt;/p&gt;

&lt;p&gt;Basically what happens during validation compaction is that a scanner is created for each sstable. The &lt;tt&gt;CompactionIterable.Reducer&lt;/tt&gt; will then create a &lt;tt&gt;LazilyCompactedRow&lt;/tt&gt; with an iterable of &lt;tt&gt;OnDiskAtom&lt;/tt&gt; for the same key in each sstable. The purpose of &lt;tt&gt;LazilyCompactedRow&lt;/tt&gt; during validation compaction is to create a digest of the compacted version of all atoms that would represent a single row. This is done cell by cell, where each collection of atoms for a single cell name is consumed by &lt;tt&gt;LazilyCompactedRow.Reducer&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt; The decision on whether &lt;tt&gt;LazilyCompactedRow.Reducer&lt;/tt&gt; should finish to merge a cell and move to the next one is currently being done by &lt;tt&gt;AbstractCellNameType.onDiskAtomComparator&lt;/tt&gt;, as evaluated by &lt;tt&gt;MergeIterator.ManyToOne&lt;/tt&gt;. However, the comparator does not only compare by name, but also by &lt;tt&gt;DeletionTime&lt;/tt&gt; in case of &lt;tt&gt;RangeTombstone&lt;/tt&gt;. As a consequence, &lt;tt&gt;MergeIterator.ManyToOne&lt;/tt&gt; will advance in case two &lt;tt&gt;RangeTombstone&lt;/tt&gt; with different deletion times are read, which breaks the &quot;&lt;em&gt;will be called one or more times with cells that share the same column name&lt;/em&gt;&quot; contract in &lt;tt&gt;LazilyCompactedRow.Reducer&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;The submitted patch will introduce a new &lt;tt&gt;Comparator&amp;lt;OnDiskAtom&amp;gt;&lt;/tt&gt; that will basically work like &lt;tt&gt;onDiskAtomComparator&lt;/tt&gt;, but does not compare deletion time. As simple as that.&lt;/p&gt;


&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;2.1&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;2.2&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/spodkowinski/cassandra/tree/CASSANDRA-11349-2.1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/spodkowinski/cassandra/tree/CASSANDRA-11349-2.2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.1-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.2-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.1-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.2-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;



&lt;p&gt;The only other places where &lt;tt&gt;LazilyCompactedRow&lt;/tt&gt; is being used except validation compaction are the cleanup and scrub functions, which shouldn&apos;t be affected, as those are working on individual sstables and I assume that there&apos;s no case where an sstable can have multiple identical range tombstones with different timestamps.&lt;/p&gt;</comment>
                            <comment id="15208851" author="frousseau" created="Wed, 23 Mar 2016 17:45:11 +0000"  >&lt;p&gt;Nice patch.&lt;/p&gt;

&lt;p&gt;I will be able to test it on a dev environment either this week or at the beginning of next week.&lt;/p&gt;

&lt;p&gt;There is still one case not covered (though not sure it can happen).&lt;br/&gt;
Suppose that in SSTable 1, there is a range tombstone covering the columns &quot;a&quot; through &quot;g&quot; at time t1, and in SSTable 2, there is a range tombstone covering the columns &quot;c&quot; through &quot;d&quot; at time t2.&lt;br/&gt;
If those two SSTable are merged (for example on another replica), it will be split in 3 range tombstones in one SSTable (one range tombstone &quot;a&quot; -&amp;gt; &quot;b&quot; at t1, &quot;c&quot; -&amp;gt; &quot;d&quot; at t2, &quot;e&quot; -&amp;gt; &quot;f&quot; at t1).&lt;br/&gt;
Computing the merkle tree for those two hosts will still be different (not the same range tombstones).&lt;/p&gt;

&lt;p&gt;As said above, not sure if it can happen, and anyway, this patch is a good improvement and probably fits 99% cases.&lt;/p&gt;
</comment>
                            <comment id="15218940" author="frousseau" created="Wed, 30 Mar 2016 21:50:04 +0000"  >&lt;p&gt;I tested the patch on a dev environment containing production data and there were still some differences.&lt;/p&gt;

&lt;p&gt;The test procedure was:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;use ccm &amp;amp; the branch linked to this ticket (I verified that the classpath is ok)&lt;/li&gt;
	&lt;li&gt;copy a fresh backup of production data&lt;/li&gt;
	&lt;li&gt;did a first full repair -&amp;gt; it had some differences on all CFs but this can be explained if there is a small delay when snapshotting all hosts for the backup (this cluster receive a few thousands writes per second)&lt;/li&gt;
	&lt;li&gt;did a second full repair -&amp;gt; only one CF had no differences (the one without range tombstone) and all others had differences (while they should not because there are no reads &amp;amp; writes on this dev environment)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I will continue to investigate and try to isolate those differences...&lt;/p&gt;</comment>
                            <comment id="15221854" author="frousseau" created="Fri, 1 Apr 2016 15:39:34 +0000"  >&lt;p&gt;Thanks, the patch is OK.&lt;/p&gt;

&lt;p&gt;In fact, the differences were produced by another bug that I will create separately.&lt;/p&gt;</comment>
                            <comment id="15221878" author="slebresne" created="Fri, 1 Apr 2016 15:56:40 +0000"  >&lt;p&gt;I suspect this doesn&apos;t affect 3.x: has someone checked, and if not, can someone do so we know if some 3.x version is needed or not for this?&lt;/p&gt;</comment>
                            <comment id="15221919" author="frousseau" created="Fri, 1 Apr 2016 16:22:32 +0000"  >&lt;p&gt;I tested against the 3.0.4 and it is not affected (not tested the 3.X, but assumed that it&apos;s not affected).&lt;br/&gt;
There is another similar ticket (the 3.0.4 is not affected): &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11477&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/CASSANDRA-11477&lt;/a&gt; &lt;/p&gt;</comment>
                            <comment id="15222297" author="mkjellman" created="Fri, 1 Apr 2016 20:26:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spodxx%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;spodxx@gmail.com&quot;&gt;spodxx@gmail.com&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=slebresne&quot; class=&quot;user-hover&quot; rel=&quot;slebresne&quot;&gt;slebresne&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=frousseau&quot; class=&quot;user-hover&quot; rel=&quot;frousseau&quot;&gt;frousseau&lt;/a&gt; I&apos;m confused here. Why should repair be special cased over normal compaction in this case? If the times are different then you &lt;b&gt;do&lt;/b&gt; still need to resolve it as you need to take the greater time.&lt;/p&gt;

&lt;p&gt;It seems to me the crux of the current patch is to &quot;fix&quot; this by special casing the comparator to just compare just the max value of the interval during repair validation:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;    &lt;span class=&quot;code-comment&quot;&gt;// only compare interval, but not deletion time
&lt;/span&gt;+  &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; AbstractCellNameType.&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.compare(((RangeTombstone)c1).max, ((RangeTombstone)c2).max);
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I just did my best to merge and compare the code between 2.0 and 2.1 and I&apos;m still trying to parse how this code is different in 2.0 vs. 2.1... We&apos;ve been unable to reproduce this in 2.0 so far, but the bits of the code being touched here don&apos;t seem to be different so I&apos;m trying to understand why 2.1 would hit this and not 2.0.&lt;/p&gt;

&lt;p&gt;Could you please explain a bit more why we we can ignore the timestamp?&lt;/p&gt;</comment>
                            <comment id="15222304" author="mkjellman" created="Fri, 1 Apr 2016 20:30:51 +0000"  >&lt;p&gt;And just for my sanity and for discussion in the Jira, here is the current handling in the comparator&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (c1 &lt;span class=&quot;code-keyword&quot;&gt;instanceof&lt;/span&gt; RangeTombstone)
{
    &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (c2 &lt;span class=&quot;code-keyword&quot;&gt;instanceof&lt;/span&gt; RangeTombstone)
    {
            RangeTombstone t1 = (RangeTombstone)c1;
            RangeTombstone t2 = (RangeTombstone)c2;
            &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; comp2 = AbstractCellNameType.&lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt;.compare(t1.max, t2.max);
            &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; comp2 == 0 ? t1.data.compareTo(t2.data) : comp2;
        }
        &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;
        {
            &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; -1;
        }
    }
    &lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt;
    {
        &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; c2 &lt;span class=&quot;code-keyword&quot;&gt;instanceof&lt;/span&gt; RangeTombstone ? 1 : 0;
    }
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15222330" author="rlow" created="Fri, 1 Apr 2016 20:56:07 +0000"  >&lt;p&gt;I&apos;m also not sure how this is meant to fix it. Special casing validation compaction may fix repairs but you&apos;d still get the digest mismatches on reads.&lt;/p&gt;</comment>
                            <comment id="15222365" author="thobbs" created="Fri, 1 Apr 2016 21:28:05 +0000"  >&lt;p&gt;I think there are a couple of things wrong with the current patch.&lt;/p&gt;

&lt;p&gt;First, the comparator needs to continue to compare the full cell name first, and only break ties on range tombstones with &lt;tt&gt;compare(t1.max, t2.max)&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Second, I believe we should remove the timestamp tie-breaking behavior from the comparator in general, and not just for validation compactions.  In other words, I think we&apos;re doing the comparison incorrectly for all compactions right now.&lt;/p&gt;

&lt;p&gt;We want the comparison to return 0 whenever range tombstones have equal names and ranges, even if they have different timestamps.  This will result in &lt;tt&gt;LazilyCompactedRow.Reducer.reduce()&lt;/tt&gt; being called in one round with each of the tombstones that only differ in timestamp.  The logic in &lt;tt&gt;LCR.Reducer.reduce()&lt;/tt&gt; already handles the case of multiple range tombstones with different timestamps by picking the one with the highest timestamp, so these will correctly be reduced to a single RT.  It looks like the current codebase will keep both range tombstones during a compaction, which isn&apos;t necessarily harmful, but is suboptimal.  For repair purposes, though, this is incorrect as it produces a different digest.&lt;/p&gt;

&lt;p&gt;To summarize: I think all we need to do is remove the timestamp tie-breaking logic from the existing comparator.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=slebresne&quot; class=&quot;user-hover&quot; rel=&quot;slebresne&quot;&gt;slebresne&lt;/a&gt; should double-check my logic, though.&lt;/p&gt;</comment>
                            <comment id="15222902" author="spodxx@gmail.com" created="Sat, 2 Apr 2016 14:14:43 +0000"  >&lt;p&gt;It makes sense just to modify &lt;tt&gt;onDiskAtomComparator&lt;/tt&gt;. Given the generic name I assumed the comparator is used in other places as well, but since its only used in &lt;tt&gt;LazyCompactedRow&lt;/tt&gt; we can just change the patch as suggested and simply remove the timestamp tie-break behaviour in &lt;tt&gt;onDiskAtomComparator&lt;/tt&gt;. &lt;/p&gt;

&lt;p&gt;As for regular compactions, I agree with Tyler that this should not effect compactions in a way that it does with validation compaction. Before the patch, &lt;tt&gt;LazyCompactedRow&lt;/tt&gt; would not reduce both RTs but instead have &lt;tt&gt;ColumnIndex.buildForCompaction()&lt;/tt&gt; iterate over both RTs and have them added to the &lt;tt&gt;RangeTombstone.Tracker&lt;/tt&gt;. The tracker would merge them in a way &lt;tt&gt;LCR.Reducer.getReduced&lt;/tt&gt; would after the patch. However, I&#8217;m not fully sure if there could be some other for more complex cases where this still would cause problems.&lt;/p&gt;

&lt;p&gt;Although the patch should fix the described issue, the way we deal with RTs during validation compaction is still not ideal. The problem is that LCR lacks some handling of relationships between RTs compared to &lt;tt&gt;RangeTombstone.Tracker&lt;/tt&gt;. If we create digests column by column, we get wrong results for shadowing tombstones not sharing the same intervals.&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;CREATE KEYSPACE test_rt WITH replication = {&apos;class&apos;: &apos;SimpleStrategy&apos;, &apos;replication_factor&apos;: 2};
USE test_rt;
CREATE TABLE IF NOT EXISTS table1 (
    c1 text,
    c2 text,
    c3 text,
    c4 float,
    PRIMARY KEY (c1, c2, c3)
) WITH compaction = {&apos;class&apos;: &apos;SizeTieredCompactionStrategy&apos;, &apos;enabled&apos;: &apos;false&apos;};
DELETE FROM table1 WHERE c1 = &apos;a&apos; AND c2 = &apos;b&apos; AND c3 = &apos;c&apos;;

ccm node1 flush

DELETE FROM table1 WHERE c1 = &apos;a&apos; AND c2 = &apos;b&apos;;

ccm node1 repair test_rt table1
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;In this case the (c1, c2, c3) RT will always be repaired after it has been compacted with (c1, c2) on any node. &lt;br/&gt;
So I&#8217;m wondering if we shouldn&#8217;t take a more bold approach here than the patch does. &lt;/p&gt;</comment>
                            <comment id="15227163" author="frousseau" created="Tue, 5 Apr 2016 21:28:42 +0000"  >&lt;p&gt;Using the RangeTombstone.Tracker can help in the situation described just above.&lt;/p&gt;

&lt;p&gt;In fact, the RT should always update the tracker (see &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11477&quot; title=&quot;MerkleTree mismatch when a cell is shadowed by a range tombstone in different SSTables&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11477&quot;&gt;&lt;del&gt;CASSANDRA-11477&lt;/del&gt;&lt;/a&gt;).&lt;br/&gt;
The trick here is to always considered it as &quot;expired&quot; in the tracker (even if not) so the tombstones are not accumulated during compaction (if expired the tracker keeps only the list of opened RTs and if not, it keeps all unwritten RTs, ie all RTs because it&apos;s a validation compaction...).&lt;/p&gt;

&lt;p&gt;Having a look at the update method of the Tracker, it already check if the tombstone is superseded by another one (and don&apos;t add it as &quot;opened&quot; if superseded).&lt;/p&gt;

&lt;p&gt;Thus, the v2 patch:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;includes the previous patch&lt;/li&gt;
	&lt;li&gt;always update the tracker with the RT (considering it as expired even if not, just to not retain too many of them in memory, and because it&apos;s for validation, it&apos;s a read only and won&apos;t affect anything)&lt;/li&gt;
	&lt;li&gt;test if the RT was added in the openedTombstones list, and if that&apos;s not the case, skip it for digest.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I know that the patch may be a bit rough (at least on the &quot;isLastOpened&quot; method) but it is more to validate the approach first and did not want the patch to be too invasive (by modifying the returned value of the update method).&lt;/p&gt;

&lt;p&gt;WDYT ?&lt;/p&gt;

&lt;p&gt;Note: I have not yet tested it against our production data&lt;br/&gt;
Note2: Regarding the read-repair, this seems to be a different story and can&apos;t see anything for now that could explain those differences (will dig later on this as this is less urgent)&lt;/p&gt;</comment>
                            <comment id="15230127" author="spodxx@gmail.com" created="Thu, 7 Apr 2016 11:52:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=frousseau&quot; class=&quot;user-hover&quot; rel=&quot;frousseau&quot;&gt;frousseau&lt;/a&gt;, what makes things more complicated here is that changes to LCR will effect regular compactions as well. Adding all tombstones as expired in your &lt;tt&gt;11349-2.1-v2.patch&lt;/tt&gt; will have unwanted side effects for regular compactions, e.g. try &lt;tt&gt;RangeTombstoneMergeTest&lt;/tt&gt; with it.&lt;/p&gt;

&lt;p&gt;I&apos;ve now spend some time trying to make use of the RT.Tracker there but without much success. Adding non-expired range tombstones to the tracker from within LCR would cause corrupted sstables. Even creating an edge case just for validation compaction would not handle all potential TS shadowing scenarios and will probably cause more harm than good (and potential digest mismatch storms). I&apos;m not even sure it&apos;s possible given the current iterative MergeIterator &amp;gt; LazilyCompactedRow &amp;gt; RT.Tracker interaction. &lt;/p&gt;

&lt;p&gt;I&apos;m now at a point where I&apos;d suggest to just stick with &lt;tt&gt;11349-2.1.patch&lt;/tt&gt; unless someone else has a better idea how to solve this. I&apos;ve updated the &lt;a href=&quot;https://github.com/riptano/cassandra-dtest/pull/881&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest PR&lt;/a&gt; with two of the described shadowing scenarios that will only work with 3.0+ even after the patch, if someone wants to give it a try.&lt;/p&gt;

&lt;p&gt;Cassci results for &lt;tt&gt;11349-2.1.patch&lt;/tt&gt;:&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;2.1&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;2.2&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/spodkowinski/cassandra/tree/CASSANDRA-11349-2.1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/spodkowinski/cassandra/tree/CASSANDRA-11349-2.2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.1-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.2-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.1-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.2-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;

</comment>
                            <comment id="15253514" author="spodxx@gmail.com" created="Fri, 22 Apr 2016 08:10:32 +0000"  >&lt;p&gt;Can we keep the conversation going to get this patch into the next 2.x release?&lt;/p&gt;</comment>
                            <comment id="15254670" author="frousseau" created="Fri, 22 Apr 2016 20:58:12 +0000"  >&lt;p&gt;Sorry for not being reactive lately, I&apos;m rather busy atm...&lt;/p&gt;

&lt;p&gt;I&apos;d be more than happy&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; to see this patch in the next release.&lt;br/&gt;
I haven&apos;t tested it yet and probably can find some time next week to test it on a dev cluster if it can help.&lt;br/&gt;
Nevertheless, I won&apos;t be able to tell if it really worked because there will still have some mismatches (due to &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11477&quot; title=&quot;MerkleTree mismatch when a cell is shadowed by a range tombstone in different SSTables&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11477&quot;&gt;&lt;del&gt;CASSANDRA-11477&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;I have started working on a patch which should be able to handle both &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11477&quot; title=&quot;MerkleTree mismatch when a cell is shadowed by a range tombstone in different SSTables&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11477&quot;&gt;&lt;del&gt;CASSANDRA-11477&lt;/del&gt;&lt;/a&gt; and the last edge case.&lt;/p&gt;

&lt;p&gt;What it basically does:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Tracker is now an interface&lt;/li&gt;
	&lt;li&gt;there are two implementations: one called RegularCompactionTracker and another one ValidationCompactionTracker&lt;/li&gt;
	&lt;li&gt;the ColumnIndexer.Builder has one more optional parameter : a boolean to know if it is built for validation&lt;/li&gt;
	&lt;li&gt;the RegularCompactionTracker is identical to the existing Tracker + one empty method&lt;/li&gt;
	&lt;li&gt;the ValidationCompactionTracker is similar to the existing Tracker but retain only opened tombstones (most methods are thus empty)&lt;/li&gt;
	&lt;li&gt;the Reducer slightly changed but its behaviour is the same regarding the regular compactions&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I can share it if you&apos;re interested (code compiles but I still haven&apos;t tested it at all and plan to do it soon and share it after).&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; Just to share more information: those issues are important to us, because a few of our clusters are impacted and a few days after filing the bug, we decided to temporarily stop repairing some tables (knowing that we could live with inconsistencies on those tables)  which were heavily impacted by those bugs (each repair increased disk occupancy by a few percent), and did a major compaction. This resulted in two to three times less disk occupancy (One table shrinked from 243GB to 79GB. Note that this was not due to tombstones reclaiming old data because, it&apos;s been nearly a month now, and the big SSTable resulting from the major compaction is still there but disk usage has not grown that much). &lt;/p&gt;</comment>
                            <comment id="15262171" author="frousseau" created="Thu, 28 Apr 2016 13:57:23 +0000"  >&lt;p&gt;Ok, I uploaded a new version of the patch (11349-2.1-v2.patch)&lt;/p&gt;

&lt;p&gt;As said above, there are two Tracker implementations now: one for regular compaction and another one for validation compaction.&lt;br/&gt;
It solves both cases described here (the one in the ticket + the one in the comment of Stefan) and &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-11477&quot; title=&quot;MerkleTree mismatch when a cell is shadowed by a range tombstone in different SSTables&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-11477&quot;&gt;&lt;del&gt;CASSANDRA-11477&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="15266474" author="spodxx@gmail.com" created="Mon, 2 May 2016 12:02:52 +0000"  >&lt;p&gt;I&apos;m not sure introducing a new tracker interface is the best way to handle this. It took me a while to actually figure out the differences between the &lt;tt&gt;update&lt;/tt&gt; implementations in both trackers, since for most parts it&apos;s sharing the same copied code. It would probably be better to have ValidationTracker subclass RegularCompactionTracker, add &lt;tt&gt;remove/addUnwrittenTombstone&lt;/tt&gt; implemented empty for validation.&lt;/p&gt;

&lt;p&gt;The &lt;tt&gt;addRangeTombstone&lt;/tt&gt; semantics also look like a case of leaky abstractions to me. It&apos;s adding nothing at all for regular compaction, but serves as early exit path for validation. &lt;/p&gt;

&lt;p&gt;Good news is that the dtests and unit tests seem to pass with the patch. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15266682" author="frousseau" created="Mon, 2 May 2016 14:28:43 +0000"  >&lt;p&gt;Great.&lt;/p&gt;

&lt;p&gt;I just created a new patch (11349-2.1-v3.patch) where the &apos;update&apos; method is empty in the validation tracker (in fact, it was a left-over from previous attempts and should have been empty ).&lt;/p&gt;

&lt;p&gt;The main difference for example between the &quot;update&quot; method from the regular compaction, and the addRangeTombstone from the validation compaction is the returned value. In the latter case, it&apos;s wether the RT is superseded/shadowed by another previously met tombstone.&lt;br/&gt;
To be honest, I did not managed to factorize both of them without compromising readability even if they share some similarities.&lt;/p&gt;

&lt;p&gt;I&apos;m a bit skeptical with ValidationCompactionTracker extending RegularCompactionTracker because RegularCompaction has more fields (unwrittenTombstones, atomCount) which would not be used by the ValidationCompactionTracker (and it feels odd to have unused fields).&lt;br/&gt;
Doing it the other side, ie RegularCompactionTracker extending ValidationCompactionTracker, seemed a better fit (RegularCompaction reuses the comparator and openedTombstones), adds more fields, but there is not much to win: only the isDeleted method is in common...&lt;br/&gt;
Thus the interface did not seem a bad choice: implementations are less coupled (and could diverge more in the future if needed).&lt;br/&gt;
But this can be changed if needed (I just wanted to explain design choices and am not opposed to inheritance)&lt;/p&gt;

&lt;p&gt;I agree that this way of doing is a &quot;leaky abstraction&quot;. Nevertheless, the main idea is to have a patch doing minimal architectural changes to the current code base (did not want to refactor anything) to avoid introducing bugs. Moreover, because the 3.X and 3.0.X are not affected, this will stay in the 2.1.X and 2.2.X branches (and won&apos;t be technical debt).&lt;br/&gt;
Anyway, it&apos;s more a pragmatic solution than an elegant one (and evidently, I am open to a more elegant solution).&lt;/p&gt;</comment>
                            <comment id="15268627" author="spodxx@gmail.com" created="Tue, 3 May 2016 12:35:41 +0000"  >&lt;p&gt;Sounds reasonable and I agree that the code changes should be less invasive as possible. We&apos;re talking about 2.x so we should avoid heavy refactoring. Mentioned class design could possibly still be improved, but that depends where to go from here..&lt;/p&gt;

&lt;p&gt;To wrap up available patch options:&lt;/p&gt;

&lt;p&gt;1) &lt;tt&gt;1349-2.1.patch&lt;/tt&gt; with 2 changed lines would address the issue initially described&lt;br/&gt;
2) &lt;tt&gt;11349-2.1-v3.patch&lt;/tt&gt; introduces a bit more changes but will also create correct digests for shadowed range tombstones and cells (see &lt;a href=&quot;https://github.com/spodkowinski/cassandra-dtest/blob/CASSANDRA-11349/repair_tests/repair_test.py#L425&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Any opinions on this except from Fabian&apos;s and mine? It would be good to get some feedback from someone how would be actually willing to commit something like this.&lt;/p&gt;</comment>
                            <comment id="15270572" author="blambov" created="Wed, 4 May 2016 12:54:15 +0000"  >&lt;p&gt;As I see it neither solution will be sufficient. A lot of the visible effects of the problem come as a side effect of &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-7953&quot; title=&quot;RangeTombstones not merging during compaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-7953&quot;&gt;&lt;del&gt;CASSANDRA-7953&lt;/del&gt;&lt;/a&gt;, but there are some underlying issues that are only really solved in 3.0 by the new tombstone handling from &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-8099&quot; title=&quot;Refactor and modernize the storage engine&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-8099&quot;&gt;&lt;del&gt;CASSANDRA-8099&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Whether we change &lt;tt&gt;onDiskAtomComparator&lt;/tt&gt; or not, we will still get disordered or multiple equal range tombstones from a single source as that&apos;s how they are written in the sstables. &lt;tt&gt;MergeIterator&lt;/tt&gt; will not combine equal entries from the same source, even if it did and everything was written using the &lt;tt&gt;onDiskAtomComparator&lt;/tt&gt; (which I don&apos;t believe to be the case), it is still in the wrong order for resolving which tombstones can be deleted without delaying their processing.&lt;/p&gt;

&lt;p&gt;In other words the problem cannot be solved by changing the reducer; we can, however, do it if we change &lt;tt&gt;update&lt;/tt&gt; to follow closely or, better still, &lt;em&gt;call&lt;/em&gt; &lt;tt&gt;IndexBuilder.buildForCompaction&lt;/tt&gt; and make the builder accept a prepared atom serializer (or some subinterface) instead of an output file, and update the digest in the calls to that serializer.&lt;/p&gt;</comment>
                            <comment id="15272427" author="blambov" created="Thu, 5 May 2016 14:07:02 +0000"  >&lt;p&gt;I have something like &lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...blambov:11349&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this&lt;/a&gt; in mind.&lt;/p&gt;</comment>
                            <comment id="15277977" author="spodxx@gmail.com" created="Tue, 10 May 2016 11:43:11 +0000"  >&lt;p&gt;To quickly sum up the current behavior.. &lt;tt&gt;ColumnIndex.Builder&lt;/tt&gt; is created for each &lt;tt&gt;LazilyCompactedRow.update()&lt;/tt&gt; call. The builder will iterate through all atoms produced by the &lt;tt&gt;MergeIterator&lt;/tt&gt; and uses a &lt;tt&gt;RangeTombstone.Tracker&lt;/tt&gt; instance for tombstone normalization. Tombstones will be added to the tracker from &lt;tt&gt;Builder.add()&lt;/tt&gt; and by &lt;tt&gt;LCR.Reducer.getReduced()&lt;/tt&gt;, which in turn will be called once for all atoms for the same column as considered by &lt;tt&gt;onDiskAtomComparator&lt;/tt&gt;. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=blambov&quot; class=&quot;user-hover&quot; rel=&quot;blambov&quot;&gt;blambov&lt;/a&gt;, so what you&apos;re saying is that we can&apos;t be sure that the &lt;tt&gt;MergeIterator&lt;/tt&gt; will always be able to provide deterministically ordered values, as write order may be different and we therefor cannot simply iterate through the reducer to create a correct digest. &lt;/p&gt;

&lt;p&gt;What I&apos;m a bit concerned about while trying to understand Branimir&apos;s approach is that at some point &lt;tt&gt;getReduced()&lt;/tt&gt; will add the RT to the tracker while in another scenario the RT will be added later and will cause the serializer be called differently as well. Or to put this in other words, if we can&apos;t be sure about the reducer returning deterministically ordered values, won&apos;t this effect the tracker and digest calculation in the builder as well?&lt;/p&gt;</comment>
                            <comment id="15278032" author="blambov" created="Tue, 10 May 2016 12:42:08 +0000"  >&lt;p&gt;Not precisely: One part of the problem is that we cannot ensure that e.g. the same range tombstone will not come twice from the same sstable (in which case &lt;tt&gt;MergeIterator&lt;/tt&gt; would issue two separate &lt;tt&gt;getReduced&lt;/tt&gt; calls) or from two different sstables (in which case &lt;tt&gt;MergeIterator&lt;/tt&gt; would call &lt;tt&gt;getReduced&lt;/tt&gt; once). Another is that while compaction uses the tracker to identify when a tombstone is redundant and can be omitted, &lt;tt&gt;getReduced&lt;/tt&gt; does not have that information at the time it processes that tombstone because the covering tombstone has not arrived yet.&lt;/p&gt;

&lt;p&gt;The tracker can properly resolve these situations, but it can&apos;t do it without delaying which causes the necessity for abusing the serializer.&lt;/p&gt;

&lt;p&gt;The reducer only adds RTs to the tracker if it would not return them in the output for some reason (e.g. expiration), the point being to always pass on the full stream of RTs; the order should not be affected by it choosing to do that.&lt;/p&gt;</comment>
                            <comment id="15280088" author="spodxx@gmail.com" created="Wed, 11 May 2016 13:13:33 +0000"  >&lt;p&gt;Thanks for the clarification. It&apos;s really helpful to understand the intention of how those parts are suppose to work together. &lt;/p&gt;

&lt;p&gt;The serializer approach seems to be a good idea how to handle this, but  there are still &lt;a href=&quot;https://github.com/spodkowinski/cassandra-dtest/blob/b110685bceddbcb63ebc744ba54a25cb268f2478/repair_tests/repair_test.py#L438:L451&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;cases&lt;/a&gt; [1] not handled correctly. I&apos;m going to take a closer look to understand why. I&apos;d also like to do some more testing for potential digest mismatch storms during rolling upgrades, but wouldn&apos;t expect any blockers so far. &lt;/p&gt;

&lt;p&gt;[1] nosetests repair_tests/repair_test.py:TestRepair.shadowed_range_tombstone_digest_parallel_repair_test&lt;/p&gt;</comment>
                            <comment id="15287685" author="frousseau" created="Tue, 17 May 2016 21:53:44 +0000"  >&lt;p&gt;Ok, this seems a better approach (with this approach RT are added to the tracker through the ColumnIndex.add method).&lt;br/&gt;
I had some time to test it on a dev environment and repair did not find any difference (which is a good thing).&lt;/p&gt;

&lt;p&gt;Regarding the case that is not working correctly, I think the solution is to use a RangeTombstoneList before writing RangeTombstones.&lt;/p&gt;

&lt;p&gt;The current implementation Tracker.writeUnwrittenTombstones(...) is:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;            for (RangeTombstone rt : unwrittenTombstones)
            {
                size += writeTombstone(rt, out, atomSerializer);
            }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And should be replaced by:&lt;/p&gt;
 &lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;            RangeTombstoneList rtl = new RangeTombstoneList(comparator, unwrittenTombstones.size());
            for (RangeTombstone rt : unwrittenTombstones)
            {
                rtl.add(rt);
            }
            for (RangeTombstone rt : rtl)
            {
                size += writeTombstone(rt, out, atomSerializer);
            }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I haven&apos;t tested this but it should work.&lt;br/&gt;
The explanation for this is the following:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;on node1, due to the flushes, each RT is written in its own SSTable&lt;/li&gt;
	&lt;li&gt;on node2, because all RTs are kept in memory, they&apos;re kept in a RangeTombstoneList. This RangeTombstoneList will keep non overlapping RTs.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;During repair, on node1, RTs are merged but are kept as is (ie some RTs can be overlapped) while on node2, they can&apos;t.&lt;br/&gt;
By using the RangeTombstoneList before serializing the unwritten RT, no RT can overlap another RT.&lt;/p&gt;

&lt;p&gt;Note: doing the change above will also change the way RT are serialized during normal compactions...&lt;/p&gt;</comment>
                            <comment id="15290648" author="blambov" created="Thu, 19 May 2016 07:55:51 +0000"  >&lt;p&gt;There will be cases where this &lt;tt&gt;RangeTombstoneList&lt;/tt&gt; solution is not sufficient (e.g. inserting &lt;tt&gt;c1 = &apos;a&apos; AND c2 = &apos;b&apos; AND c3 = &apos;a&apos;&lt;/tt&gt; at the end of the test above).&lt;/p&gt;

&lt;p&gt;Is it imperative that we fix all scenarios here if 3.0 has the proper solution?&lt;/p&gt;</comment>
                            <comment id="15291211" author="spodxx@gmail.com" created="Thu, 19 May 2016 14:49:20 +0000"  >&lt;p&gt;I&apos;ve been debuging the latest mentioned error case using the following cql/ccm statements and a local 2 node cluster.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;create keyspace ks WITH replication = {&lt;span class=&quot;code-quote&quot;&gt;&apos;class&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;SimpleStrategy&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;replication_factor&apos;&lt;/span&gt;: 2};
use ks;
CREATE TABLE IF NOT EXISTS table1 ( c1 text, c2 text, c3 text, c4 &lt;span class=&quot;code-object&quot;&gt;float&lt;/span&gt;,
 PRIMARY KEY (c1, c2, c3)
) WITH compaction = {&lt;span class=&quot;code-quote&quot;&gt;&apos;class&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;SizeTieredCompactionStrategy&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;enabled&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;&lt;/span&gt;};
DELETE FROM table1 USING TIMESTAMP 1463656272791 WHERE c1 = &lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt; AND c2 = &lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt; AND c3 = &lt;span class=&quot;code-quote&quot;&gt;&apos;c&apos;&lt;/span&gt;;
ccm node1 flush
DELETE FROM table1 USING TIMESTAMP 1463656272792 WHERE c1 = &lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt; AND c2 = &lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt;;
ccm node1 flush
DELETE FROM table1 USING TIMESTAMP 1463656272793 WHERE c1 = &lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt; AND c2 = &lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt; AND c3 = &lt;span class=&quot;code-quote&quot;&gt;&apos;d&apos;&lt;/span&gt;;
ccm node1 flush
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Timestamps have been added for easier tracking of the specific tombstone in the debugger.&lt;/p&gt;

&lt;p&gt;ColmnIndex.Builder.buildForCompaction() will add tombstones in the following order to the tracker:&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Node1&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;1463656272792: c1 = &apos;a&apos; AND c2 = &apos;b&apos;&lt;/tt&gt;&lt;br/&gt;
First RT, added to unwritten + opened tombstones&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;1463656272791: c1 = &apos;a&apos; AND c2 = &apos;b&apos; AND c3 = &apos;c&apos;&lt;/tt&gt;&lt;br/&gt;
Overshadowed by RT added before while being older at the same time. Will not be added and simply ignored.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;1463656272793: c1 = &apos;a&apos; AND c2 = &apos;b&apos; AND c3 = &apos;d&apos;&lt;/tt&gt;&lt;br/&gt;
Overshaded by first and only RT added to opened so far, but newer and will thus be added to unwritten+opened&lt;/p&gt;

&lt;p&gt;We end up with 2 unwritten tombstones (..92+..93) passed to the serializer for message digest.&lt;/p&gt;


&lt;p&gt;&lt;b&gt;Node2&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;1463656272792: c1 = &apos;a&apos; AND c2 = &apos;b&apos;&lt;/tt&gt; (EOC.START)&lt;br/&gt;
First RT, added to unwritten + opened tombstones&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;1463656272793: c1 = &apos;a&apos; AND c2 = &apos;b&apos; AND c3 = &apos;d&apos;&lt;/tt&gt; (EOC.END)&lt;br/&gt;
comparision of EOC flag (Tracker:251) of previously added RT will cause having it removed from the opened list (Tracker:258). Afterwards the current RT will be added to unwritten + opened.&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;1463656272792: c1 = &apos;a&apos; AND c2 = &apos;b&apos;&lt;/tt&gt; (&lt;font color=&quot;red&quot;&gt;again!&lt;/font&gt;)&lt;br/&gt;
Gets compared with prev. added RT, which supersedes the current one and thus stays in the list. Will again be added to unwritten + opened list.&lt;/p&gt;

&lt;p&gt;We end up with 3 unwritten RTs, including 1463656272792 twice.&lt;/p&gt;

&lt;p&gt;&lt;del&gt;I still haven&apos;t been able to exactly pinpoint why the reducer will be called twice with the same TS, but since &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=blambov&quot; class=&quot;user-hover&quot; rel=&quot;blambov&quot;&gt;blambov&lt;/a&gt; explicitly mentioned that possibility, I guess it&apos;s intended behavior (but why? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;).&lt;/del&gt;&lt;/p&gt;

&lt;p&gt;Running sstable2json makes it more obvious how node2 flushes the RTs:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;[
{&quot;key&quot;: &quot;a&quot;,
 &quot;cells&quot;: [[&quot;b:_&quot;,&quot;b:d:_&quot;,1463656272792,&quot;t&quot;,1463731877],
           [&quot;b:d:_&quot;,&quot;b:d:!&quot;,1463656272793,&quot;t&quot;,1463731886],
           [&quot;b:d:!&quot;,&quot;b:!&quot;,1463656272792,&quot;t&quot;,1463731877]]}
]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="15293041" author="frousseau" created="Fri, 20 May 2016 09:10:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=blambov&quot; class=&quot;user-hover&quot; rel=&quot;blambov&quot;&gt;blambov&lt;/a&gt; We have 4 clusters impacted by this bug, and for 3 out of 4, what you have in mind works.&lt;br/&gt;
I still need to verify for the 4th one. I&apos;ll try to verify this today.&lt;br/&gt;
Regarding the 3.0, migrating 60 nodes is not something done easily.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=spodxx%40gmail.com&quot; class=&quot;user-hover&quot; rel=&quot;spodxx@gmail.com&quot;&gt;spodxx@gmail.com&lt;/a&gt; Yes, there are 3 RT on node2, because, in memory, RT are stored in a RangeTombstoneList (then serialized). The RangeTombstoneList automatically split the tombstones which are overlapping.&lt;/p&gt;</comment>
                            <comment id="15293709" author="frousseau" created="Fri, 20 May 2016 16:57:27 +0000"  >&lt;p&gt;Ok, it appears that the initial idea by &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=blambov&quot; class=&quot;user-hover&quot; rel=&quot;blambov&quot;&gt;blambov&lt;/a&gt; is sufficient (after having done some basic testing for our 4th cluster).&lt;/p&gt;

&lt;p&gt;Nevertheless, I&apos;m surprised that we seems to be the only one affected by this issue. Maybe it&apos;s because it took us some time to realize it and investigate it, and there was no clear sign apart from big streams during repairs + data set size increasing too fast. So this may explain why not many people reported it, but there may be others affected out in the wild. That&apos;s why it&apos;s probably best to try to fix most of it (if it&apos;s not possible to fix it entirely), but I also understand that the less changes there are, the less risky it is...&lt;/p&gt;

&lt;p&gt;So I&apos;m good with it either partially fixed or mostly fixed. &lt;/p&gt;</comment>
                            <comment id="15293719" author="spodxx@gmail.com" created="Fri, 20 May 2016 16:58:58 +0000"  >&lt;p&gt;I&apos;ve now created a new patch version &lt;a href=&quot;https://github.com/spodkowinski/cassandra/commit/c8601f8cd3921e754bcbe8c9362cf3d2e7072e1e&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;  that basically combines both of your ideas of doing the digest updates in the serializer and using &lt;tt&gt;RangeTombstonesList&lt;/tt&gt; to normalize RT intervals. Tests look good, feel free to add your own. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=blambov&quot; class=&quot;user-hover&quot; rel=&quot;blambov&quot;&gt;blambov&lt;/a&gt;, can you think of any further cases that would not be covered by this approach? &lt;/p&gt;</comment>
                            <comment id="15308051" author="frousseau" created="Tue, 31 May 2016 16:31:36 +0000"  >&lt;p&gt;Thanks Stefan.&lt;/p&gt;

&lt;p&gt;So if I understand well, your latest branch does not changes how SSTables are serialized on disk (by using 2 specialized serializers: one for compaction and one for validation) but still solves all cases (or at least all known cases).&lt;/p&gt;

&lt;p&gt;Any chance that this patch can be included in 2.1.15 ?&lt;/p&gt;</comment>
                            <comment id="15312087" author="spodxx@gmail.com" created="Thu, 2 Jun 2016 10:15:07 +0000"  >&lt;p&gt;I&apos;ve now attached a patch for the last mentioned implementation as &lt;tt&gt;11349-2.1-v4.patch&lt;/tt&gt; and &lt;tt&gt;11349-2.2-v4.patch&lt;/tt&gt; to the ticket.&lt;/p&gt;

&lt;p&gt;Test results are as follows (reported failures cannot be reproduced locally and seem to be unrelated to me):&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;2.1&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;2.2&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/spodkowinski/cassandra/tree/CASSANDRA-11349-2.1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/spodkowinski/cassandra/tree/CASSANDRA-11349-2.2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.1-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.2-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.1-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/spodkowinski/job/spodkowinski-CASSANDRA-11349-2.2-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;Anyone willing to take another look and actually commit a patch for this issue? I&apos;ve pushed my WIP branch &lt;a href=&quot;https://github.com/spodkowinski/cassandra/commits/WIP2-11349&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt; with individual commits that might help during the review.&lt;/p&gt;</comment>
                            <comment id="15321446" author="blambov" created="Wed, 8 Jun 2016 21:08:07 +0000"  >&lt;p&gt;Does this really solve the problem with the test you mentioned? Putting the tombstones through &lt;tt&gt;RangeTombstoneList&lt;/tt&gt; will normalize them, but they may not be issued in the right position, i.e. the RTL solution only works if the data contains only tombstones. For example, the &lt;tt&gt;[&quot;b:d:&amp;#33;&quot;,&quot;b:&amp;#33;&quot;,1463656272792,&quot;t&quot;,1463731877]&lt;/tt&gt; part from the test above gets issued before a potential token that may come before &lt;tt&gt;b:d:!&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;The test needs to be extended to include live tokens, for example by adding each of&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;INSERT INTO table1 (c1, c2, c3, c4) VALUES (&lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt;, 1)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;INSERT INTO table1 (c1, c2, c3, c4) VALUES (&lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;d&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt;, 1)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;INSERT INTO table1 (c1, c2, c3, c4) VALUES (&lt;span class=&quot;code-quote&quot;&gt;&apos;b&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;e&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;a&apos;&lt;/span&gt;, 1)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;after the deletions.&lt;/p&gt;

&lt;p&gt;The RTL solution will break (in different ways) for at least two of the above. It also has performance implications that I am not really happy to take. A proper solution is to either fully replicate what RTL does in the tombstone tracker (which may be not be worth it so late in the lifespan of 2.1 and 2.2), or make the tombstone tracker wrap around an RTL (which may be inefficient and is still somewhat tricky).&lt;/p&gt;

&lt;p&gt;If (as Fabien&apos;s testing seems to imply) doing the digest update as serialization solves the majority of the differences and repair pain, I would prefer to stop there.&lt;/p&gt;</comment>
                            <comment id="15324477" author="spodxx@gmail.com" created="Fri, 10 Jun 2016 13:47:41 +0000"  >&lt;p&gt;You&apos;re correct by pointing out that live columns can prevent fully normalizing all RTs using the RTL approach in patch v4. It will still be more accurate than without RTL consolidation, but the question is if the additional complexity is worth it. If you&apos;d be more comfortable going with the patch initially suggested by yourself, I&apos;m confident that this will still be a big improvement. &lt;/p&gt;</comment>
                            <comment id="15333457" author="frousseau" created="Thu, 16 Jun 2016 09:38:28 +0000"  >&lt;p&gt;Just to let you know that we packaged the patch done by Branimir (as it is the one that have more chances to be included mainstream).&lt;/p&gt;

&lt;p&gt;We restored one cluster (3 nodes, 100GB of data per node, affected table is 25GB) from a snapshot on new hardware, and did a full repair. So far, so good, not much differences are found for the affected table but this was expected because repairs are not run for a few months (around a hundred VS a few hundred of thousands before).&lt;/p&gt;

&lt;p&gt;We will continue testing by recreating all of our clusters, and then, deploy it on our production (and I&apos;ll let you know once this is done).&lt;/p&gt;</comment>
                            <comment id="15352747" author="slebresne" created="Tue, 28 Jun 2016 10:18:28 +0000"  >&lt;p&gt;Had a look here, and I&apos;m more comfortable with sticking to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=blambov&quot; class=&quot;user-hover&quot; rel=&quot;blambov&quot;&gt;blambov&lt;/a&gt; approach. For 2.1 and 2.2, we&apos;re now in &quot;only critical bug fixes&quot; and running things through RTL definitively changes things too much for my comfort. That imply I&apos;m fine not fixing every possible problems if that gets us too far (especially since it&apos;s properly fixed in 3.0 and not that many people seems to have reported this). And Branimir&apos;s approach seems to be making a good enough impact in practice.&lt;/p&gt;

&lt;p&gt;So &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=blambov&quot; class=&quot;user-hover&quot; rel=&quot;blambov&quot;&gt;blambov&lt;/a&gt;, could you rebase your patch for 2.1 and 2.2 and run CI. After which, if tests are good, I&apos;m +1 committing. &lt;/p&gt;</comment>
                            <comment id="15361016" author="blambov" created="Mon, 4 Jul 2016 08:33:54 +0000"  >&lt;p&gt;Rebased patch here:&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/blambov/cassandra/tree/11349&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;2.1&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/blambov/job/blambov-11349-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;utests&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/blambov/job/blambov-11349-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtests&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/blambov/cassandra/tree/11349-2.2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;2.2&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/blambov/job/blambov-11349-2.2-testall/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;utests&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/blambov/job/blambov-11349-2.2-dtest/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtests&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
</comment>
                            <comment id="15361168" author="blambov" created="Mon, 4 Jul 2016 11:08:47 +0000"  >&lt;p&gt;Tests look ok, all failures are either failing on base or failed very recently.&lt;/p&gt;</comment>
                            <comment id="15362272" author="slebresne" created="Tue, 5 Jul 2016 09:27:30 +0000"  >&lt;p&gt;Perfect, thanks, committed.&lt;/p&gt;</comment>
                            <comment id="15383900" author="frousseau" created="Tue, 19 Jul 2016 10:14:16 +0000"  >&lt;p&gt;Here is some quick feedback: patch is deployed on production for more than a week and it diminished a lot the streaming during repairs.&lt;/p&gt;

&lt;p&gt;A patched version of C* 2.1.14 (containing the patch) has been deployed on all of our production clusters more than a week ago and it works well.&lt;br/&gt;
There are still a few differences during repairs, but a lot less than before, and this is &quot;manageable&quot;.&lt;/p&gt;

&lt;p&gt;Thanks to all of you for your help.&lt;/p&gt;</comment>
                            <comment id="15792783" author="spodxx@gmail.com" created="Mon, 2 Jan 2017 12:19:26 +0000"  >&lt;p&gt;I&apos;ve looked at some metrics today for one of our clusters that has been updated to 2.1.16 a couple of weeks ago. We used to see tens of thousands of sstables getting streamed each night during repairs with many GBs.&lt;/p&gt;

&lt;p&gt;With 2.1.16 the number of streamed sstables went down to almost none. Thanks for fixing this to everyone involved! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310060">
                    <name>Container</name>
                                            <outwardlinks description="contains">
                                        <issuelink>
            <issuekey id="12955334">CASSANDRA-11477</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="12955334">CASSANDRA-11477</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12801237" name="11349-2.1-v2.patch" size="14510" author="frousseau" created="Thu, 28 Apr 2016 13:49:40 +0000"/>
                            <attachment id="12801745" name="11349-2.1-v3.patch" size="14088" author="frousseau" created="Mon, 2 May 2016 14:30:36 +0000"/>
                            <attachment id="12807673" name="11349-2.1-v4.patch" size="11627" author="spod" created="Thu, 2 Jun 2016 10:15:58 +0000"/>
                            <attachment id="12797506" name="11349-2.1.patch" size="1839" author="spod" created="Thu, 7 Apr 2016 11:54:06 +0000"/>
                            <attachment id="12807674" name="11349-2.2-v4.patch" size="11705" author="spod" created="Thu, 2 Jun 2016 10:15:58 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[blambov]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 46 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2ukj3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12311421" key="com.atlassian.jira.plugin.system.customfieldtypes:multiversion">
                        <customfieldname>Reproduced In</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue id="12334273">2.1.13</customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                    <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>frousseau</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[frousseau]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>