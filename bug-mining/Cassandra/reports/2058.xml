<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:37:25 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-5182] Deletable rows are sometimes not removed during compaction</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-5182</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;Our use case is write heavy and read seldom.  To optimize the space used, we&apos;ve set the bloom_filter_fp_ratio=1.0  That along with the fact that each row is only written to one time and that there are more than 20 SSTables keeps the rows from ever being compacted. Here is the code:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/compaction/CompactionController.java#L162&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/compaction/CompactionController.java#L162&lt;/a&gt;&lt;br/&gt;
We hit this conner case and because of this C* keeps consuming more and more space on disk while it should not.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12628835">CASSANDRA-5182</key>
            <summary>Deletable rows are sometimes not removed during compaction</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="yukim">Yuki Morishita</assignee>
                                    <reporter username="binhnv">Binh Van Nguyen</reporter>
                        <labels>
                    </labels>
                <created>Wed, 23 Jan 2013 01:22:16 +0000</created>
                <updated>Tue, 16 Apr 2019 09:32:19 +0000</updated>
                            <resolved>Mon, 4 Mar 2013 19:37:12 +0000</resolved>
                                        <fixVersion>1.2.3</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="13560302" author="binhnv" created="Wed, 23 Jan 2013 01:43:02 +0000"  >&lt;p&gt;Here is simple code to reproduce the issue. The simple code uses 10 threads to continuously write to C*. All the column has 300 in its TTL. While running the code, check the number of sstables of column family, you will see that it keeps growing and never stop&lt;br/&gt;
Here are steps to run:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Start C* on your localhost&lt;/li&gt;
	&lt;li&gt;Create a keyspace named test&lt;/li&gt;
	&lt;li&gt;Use the following command to create column family cf&lt;br/&gt;
CREATE COLUMN FAMILY cf WITH comparator = UTF8Type AND key_validation_class = UTF8Type AND default_validation_class = UTF8Type AND gc_grace = 0 AND caching = none AND bloom_filter_fp_chance = 1.0 AND compaction_strategy=&apos;LeveledCompactionStrategy&apos; AND compaction_strategy_options = 
{ sstable_size_in_mb : 1 }
&lt;p&gt; AND compression_options = &lt;/p&gt;
{ chunk_length_kb : 64, sstable_compression : &apos;SnappyCompressor&apos; }
&lt;p&gt;;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Extract the code switch to extracted directory&lt;/li&gt;
	&lt;li&gt;Run mvn package&lt;/li&gt;
	&lt;li&gt;Run java -jar target/test_ttl-1.0-SNAPSHOT-jar-with-dependencies.jar 10&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13561072" author="batalbot" created="Wed, 23 Jan 2013 20:42:45 +0000"  >&lt;p&gt;A mailing list thread with more details about the use case and symptoms can be found at &lt;a href=&quot;http://www.mail-archive.com/user@cassandra.apache.org/msg27049.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.mail-archive.com/user@cassandra.apache.org/msg27049.html&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="13561225" author="yukim" created="Wed, 23 Jan 2013 23:26:57 +0000"  >&lt;p&gt;Binh,&lt;/p&gt;

&lt;p&gt;Thanks for investigation. You are right that purging row depends on bloom filter check, so if you have bloomfilter_fp_chance of 1.0, it is very likely that the row is not going to be purged.&lt;/p&gt;

&lt;p&gt;In 1.2 and above, we use AlwaysPresentFilter which always returns true for row presence when fp_change is 1.0, so the row is never going to be purged when you set fp_change=1.0.&lt;/p&gt;

&lt;p&gt;Simple fix is attached. Instead of only hitting bloom filter, we check through key cache and index file for actual row presence.&lt;/p&gt;</comment>
                            <comment id="13561229" author="yukim" created="Wed, 23 Jan 2013 23:32:37 +0000"  >&lt;p&gt;Maybe it is better to check if fp_chance is high before going through index file, since it has performance penalty.&lt;/p&gt;</comment>
                            <comment id="13561242" author="binhnv" created="Wed, 23 Jan 2013 23:52:04 +0000"  >&lt;p&gt;I agreed that we should find a better way since getPosition will check bloom filter, key cache and in the worst case (which is our case) it will scan whole index table. This will cause the performance issue.&lt;/p&gt;</comment>
                            <comment id="13561257" author="jbellis" created="Thu, 24 Jan 2013 00:01:27 +0000"  >&lt;p&gt;Do you want a performance issue, or do you only want to remove tombstones during major compaction? &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13561259" author="jbellis" created="Thu, 24 Jan 2013 00:02:35 +0000"  >&lt;p&gt;Personally I am +1 on the fix; if you run a lot of deletes and can&apos;t cache your index files in ram, then don&apos;t disable bloom filters.&lt;/p&gt;</comment>
                            <comment id="13561271" author="batalbot" created="Thu, 24 Jan 2013 00:25:16 +0000"  >&lt;p&gt;Using the test program attached, I&apos;ve reproduce the problem using 1.1.9 and then upgraded that cluster (1 node on laptop) to 1.2.0.  The problem remains with the load and sstable count increasing.&lt;/p&gt;

&lt;p&gt;However, when I run the test program on a fresh 1.2.0 cluster the problem does not come up.  My process to reproduce on upgrade is:&lt;/p&gt;

&lt;p&gt;install fresh 1.1.9&lt;br/&gt;
run test to get 500 MB of data (20-30 mins)&lt;br/&gt;
drain and shutdown 1.1.9&lt;br/&gt;
start 1.2.0&lt;br/&gt;
run nodetool upgradesstables&lt;br/&gt;
run test and watch load grow to 2.5 GB while away at lunch&lt;/p&gt;


&lt;p&gt;When running the test program on a fresh 1.2.0 installation, the load tops out at about 200 MB and 90 or so SSTables which is what is desired.&lt;/p&gt;</comment>
                            <comment id="13561274" author="batalbot" created="Thu, 24 Jan 2013 00:28:09 +0000"  >&lt;p&gt;About the check for a high fp_chance before checking indexes.  Did you mean to only check index files if fp_chance is high (say over 0.5 or something)?  That way the additional check is only incurred with bloom filters are effectively disabled and the common case using an effective (low fp) bloom filter is not impacted.&lt;/p&gt;</comment>
                            <comment id="13561308" author="jbellis" created="Thu, 24 Jan 2013 01:24:38 +0000"  >&lt;p&gt;getPosition does the right thing here: it checks the index file only on bloom filter positives, so a high bloom filter setting will benefit automatically.&lt;/p&gt;

&lt;p&gt;The only improvement I think makes sense would be adding support for compaction strategy tombstone threshold.&lt;/p&gt;</comment>
                            <comment id="13561502" author="slebresne" created="Thu, 24 Jan 2013 08:42:46 +0000"  >&lt;blockquote&gt;&lt;p&gt;Maybe it is better to check if fp_chance is high before going through index file&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually, I agree with Yuki on that and I&apos;m kind of -1 on the patch in his current form. The current patch means that whatever your fp_chance is, each time the row is indeed present in a non compacted sstable (which does prevent gcing the row for this compaction but is not something that will necessarily be rare) might hit the disk (unless the key cache save you). So I&apos;d be in favor of using getPosition only if fp_chance == 1, at least on 1.1 as we have no idea of the impact this can have on people that haven&apos;t disabled bloom filter and have no problem whatsoever with gcing tombstone.&lt;/p&gt;

&lt;p&gt;As a side note, I&apos;ve opened &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-5183&quot; title=&quot;Improve cases where we purge tombstone on (minor) compaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-5183&quot;&gt;&lt;del&gt;CASSANDRA-5183&lt;/del&gt;&lt;/a&gt; that is related to this purge tombstone problem.&lt;/p&gt;</comment>
                            <comment id="13561640" author="jbellis" created="Thu, 24 Jan 2013 14:28:42 +0000"  >&lt;p&gt;I don&apos;t think we should touch 1.1 at all.  Updated fixver, and updated affectsver to when we added configurable bf_fp_chance.&lt;/p&gt;</comment>
                            <comment id="13572475" author="yukim" created="Wed, 6 Feb 2013 15:15:26 +0000"  >&lt;p&gt;Patch attached for 1.2 and above. It checks index file using getPosition if sstable has AlwaysPresentFilter as a bloom filter.&lt;/p&gt;</comment>
                            <comment id="13577178" author="jbellis" created="Tue, 12 Feb 2013 23:37:03 +0000"  >&lt;p&gt;I&apos;m still not comfortable with this.&lt;/p&gt;

&lt;p&gt;If our goal is to throw out the maximum possible amount of obsolete data, we should perform getPosition across the board.&lt;/p&gt;

&lt;p&gt;But if our goal is to be minimally impactful with compaction then we shouldn&apos;t do it at all, and rely instead on the timestamp check.  If that&apos;s not enough, then you shouldn&apos;t disable bloom filters on workloads that perform deletes.  I&apos;m okay with that message.&lt;/p&gt;</comment>
                            <comment id="13577189" author="batalbot" created="Tue, 12 Feb 2013 23:53:55 +0000"  >&lt;p&gt;Our use case doesn&apos;t require maximum effort to delete rows.  What we ran into was an unexpected interaction between two features: bloom filter tuned for low read rate, and deleting tombstoned rows.  With that configuration NO rows were being removed.  &lt;/p&gt;

&lt;p&gt;As long as there is some reasonable effort to remove rows with bloom filter disabled OR it&apos;s clearly known that a reasonable FP setting is required to remove tombstones, I think we could have avoided a lot of headaches.&lt;/p&gt;

&lt;p&gt;How does the new tombstone histogram feature in 1.2 affect this issue?  If that feature solves the problem already, maybe this fix is irrelevant.&lt;/p&gt;</comment>
                            <comment id="13577446" author="slebresne" created="Wed, 13 Feb 2013 09:48:41 +0000"  >&lt;blockquote&gt;&lt;p&gt;If our goal is to throw out the maximum possible amount of obsolete data&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I kind agree with Bryan, this doesn&apos;t have to be black and white. What we want is doing the best we can to remove obsolete rows without impacting compaction too much. Now if you do have active bloom filters, then I think just checking the bloom filters as we do now is the right trade-off: it maximize  with a very high probability the amount of removed data at the relatively cheap cost. Using getPosition in that case would be a bad idea, because the reward (a tiny fraction of additional data potentially removed) is not worth the cost (hitting disk each time a row we compact is also in a non-compacted sstable) imo, hence my opposition to the idea.&lt;/p&gt;

&lt;p&gt;But if you deactivate bloom filters, you also fully destroy our bloom filter trade-off. So using getPosition does now provide a substantial benefit as it allows to go from &apos;no deletion&apos; to &apos;maximize deletion&apos;. The reward is, in that case, likely worth the cost, especially since people shouldn&apos;t desactivate bloom filters unless their index files fits in memory, in which case getPosition costs won&apos;t be that big.&lt;/p&gt;

&lt;p&gt;So overall I do like the last patch attached by Yuki. Of course, the solution of just saying &quot;you shouldn&apos;t disable bloom filters on workloads that perform deletes&quot; works too, and I wouldn&apos;t oppose it, but it doesn&apos;t have my preference because I&apos;m always a bit afraid of solving an issue by saying &quot;don&apos;t do this&quot;, as it usually end up in people getting bitten first and hearing they shouldn&apos;t have done it second. &lt;/p&gt;</comment>
                            <comment id="13590601" author="jbellis" created="Fri, 1 Mar 2013 15:02:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;So overall I do like the last patch attached by Yuki. Of course, the solution of just saying &quot;you shouldn&apos;t disable bloom filters on workloads that perform deletes&quot; works too, and I wouldn&apos;t oppose it, but it doesn&apos;t have my preference because I&apos;m always a bit afraid of solving an issue by saying &quot;don&apos;t do this&quot;, as it usually end up in people getting bitten first and hearing they shouldn&apos;t have done it second. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The problem is it&apos;s not as simple as &quot;people get bitten if we don&apos;t getPosition, and don&apos;t if we do&quot; &amp;#8211; they get bitten either way, and IMO the bite from getPosition is worse, since it will destroy compaction performance for any workload where index doesn&apos;t fit entirely in ram, which makes BF disabling almost useless.  But if we say &quot;only disable BF where you&apos;re not doing deletes,&quot; it has a legitimate if narrow use case.&lt;/p&gt;</comment>
                            <comment id="13590683" author="slebresne" created="Fri, 1 Mar 2013 16:35:48 +0000"  >&lt;blockquote&gt;&lt;p&gt;if we say &quot;only disable BF where you&apos;re not doing deletes,&quot; it has a legitimate if narrow use case&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I guess I agree on the principle that we should say &quot;only disable BF where you&apos;re not doing deletes&quot;. That being said, if we do use getPosition, we extend the possible use cases, since it become &quot;only disable BF where you&apos;re not doing deletes or your index fit entirely in RAM&quot; (because getPosition will not destroy performance for the &quot;not doing delete case&quot;, since we don&apos;t even call shouldPurge() unless we know there is tombstones).&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;and IMO the bite from getPosition is worse, since it will destroy compaction performance&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m not totally sure I agree on the worse. As said above, if people have not tombstone, it won&apos;t destroy compaction performance. So I guess the question is: for people that 1) do not follow recommendation (cause we should definitively say when disabling BF is ok or not) and that 2) do have deletes, is it better for them to be bitten by a) bad compaction performance or b) their tombstones not being purged ever.&lt;/p&gt;

&lt;p&gt;I don&apos;t doubt that which of a) or b) is worse is a matter of perspective. That being said, my own personal preference goes to avoiding because:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;to me b) is a break of correctness which somewhat trumps performance consideration. It purely subjective though.&lt;/li&gt;
	&lt;li&gt;accumulating tombstones forever is a pretty nasty time-bomb. Having compaction being slow because it hit disk more than it should on the other seems easier to me to detect (and thus fix by following the recommendation of not disabling BF when you shouldn&apos;t).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;So, I still have a preference for using Yuki&apos;s last patch (and making it clear that you shall &quot;only disable BF where you&apos;re not doing deletes or your index fit entirely in RAM&quot;). If only because that&apos;s a bit better than &quot;only disable BF where you&apos;re not doing deletes&quot;. But if you still prefer keeping the status quo, I won&apos;t oppose, do feel free to close that issue (we still should write the recommendation on when to disable BF somewhere in any case).&lt;/p&gt;</comment>
                            <comment id="13590940" author="jbellis" created="Fri, 1 Mar 2013 21:07:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;if we do use getPosition, we extend the possible use cases, since it become &quot;only disable BF where you&apos;re not doing deletes or your index fit entirely in RAM&quot;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;That makes sense.  Let&apos;s ship Yuki&apos;s patch.&lt;/p&gt;</comment>
                            <comment id="13592547" author="yukim" created="Mon, 4 Mar 2013 19:37:12 +0000"  >&lt;p&gt;Committed to 1.2 and above. Thanks!&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12566217" name="5182-1.1.txt" size="762" author="yukim" created="Wed, 23 Jan 2013 23:26:57 +0000"/>
                            <attachment id="12568232" name="5182-1.2.txt" size="1595" author="yukim" created="Wed, 6 Feb 2013 15:15:26 +0000"/>
                            <attachment id="12566071" name="test_ttl.tar.gz" size="1919" author="binhnv" created="Wed, 23 Jan 2013 01:43:02 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[yukim]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>308314</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            12 years, 38 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i1b46f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>272975</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>slebresne</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[slebresne]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>