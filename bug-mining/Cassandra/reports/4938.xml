<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 23:10:11 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-13948] Reload compaction strategies when JBOD disk boundary changes</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-13948</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;The thread dump below shows a race between an sstable replacement by the &lt;tt&gt;IndexSummaryRedistribution&lt;/tt&gt; and &lt;tt&gt;AbstractCompactionTask.getNextBackgroundTask&lt;/tt&gt;:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Thread 94580: (state = BLOCKED)
 - sun.misc.Unsafe.park(boolean, long) @bci=0 (Compiled frame; information may be imprecise)
 - java.util.concurrent.locks.LockSupport.park(java.lang.Object) @bci=14, line=175 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt() @bci=1, line=836 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireQueued(java.util.concurrent.locks.AbstractQueuedSynchronizer$Node, int) @bci=67, line=870 (Compiled frame)
 - java.util.concurrent.locks.AbstractQueuedSynchronizer.acquire(int) @bci=17, line=1199 (Compiled frame)
 - java.util.concurrent.locks.ReentrantReadWriteLock$WriteLock.lock() @bci=5, line=943 (Compiled frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.handleListChangedNotification(java.lang.Iterable, java.lang.Iterable) @bci=359, line=483 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.handleNotification(org.apache.cassandra.notifications.INotification, java.lang.Object) @bci=53, line=555 (Interpreted frame)
 - org.apache.cassandra.db.lifecycle.Tracker.notifySSTablesChanged(java.util.Collection, java.util.Collection, org.apache.cassandra.db.compaction.OperationType, java.lang.Throwable) @bci=50, line=409 (Interpreted frame)
 - org.apache.cassandra.db.lifecycle.LifecycleTransaction.doCommit(java.lang.Throwable) @bci=157, line=227 (Interpreted frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.commit(java.lang.Throwable) @bci=61, line=116 (Compiled frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.commit() @bci=2, line=200 (Interpreted frame)
 - org.apache.cassandra.utils.concurrent.Transactional$AbstractTransactional.finish() @bci=5, line=185 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryRedistribution.redistributeSummaries() @bci=559, line=130 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionManager.runIndexSummaryRedistribution(org.apache.cassandra.io.sstable.IndexSummaryRedistribution) @bci=9, line=1420 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries(org.apache.cassandra.io.sstable.IndexSummaryRedistribution) @bci=4, line=250 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager.redistributeSummaries() @bci=30, line=228 (Interpreted frame)
 - org.apache.cassandra.io.sstable.IndexSummaryManager$1.runMayThrow() @bci=4, line=125 (Interpreted frame)
 - org.apache.cassandra.utils.WrappedRunnable.run() @bci=1, line=28 (Interpreted frame)
 - org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run() @bci=4, line=118 (Compiled frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.runAndReset() @bci=47, line=308 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask) @bci=1, line=180 (Compiled frame)
 - java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run() @bci=37, line=294 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(java.lang.Runnable) @bci=1, line=81 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$8.run() @bci=4 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Thread 94573: (state = IN_JAVA)
 - java.util.HashMap$HashIterator.nextNode() @bci=95, line=1441 (Compiled frame; information may be imprecise)
 - java.util.HashMap$KeyIterator.next() @bci=1, line=1461 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.View$3.apply(org.apache.cassandra.db.lifecycle.View) @bci=20, line=268 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.View$3.apply(java.lang.Object) @bci=5, line=265 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.Tracker.apply(com.google.common.base.Predicate, com.google.common.base.Function) @bci=13, line=133 (Compiled frame)
 - org.apache.cassandra.db.lifecycle.Tracker.tryModify(java.lang.Iterable, org.apache.cassandra.db.compaction.OperationType) @bci=31, line=99 (Compiled frame)
 - org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(int) @bci=84, line=139 (Compiled frame)
 - org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(int) @bci=105, line=119 (Interpreted frame)
 - org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run() @bci=84, line=265 (Interpreted frame)
 - java.util.concurrent.Executors$RunnableAdapter.call() @bci=4, line=511 (Compiled frame)
 - java.util.concurrent.FutureTask.run() @bci=42, line=266 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor.runWorker(java.util.concurrent.ThreadPoolExecutor$Worker) @bci=95, line=1149 (Compiled frame)
 - java.util.concurrent.ThreadPoolExecutor$Worker.run() @bci=5, line=624 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(java.lang.Runnable) @bci=1, line=81 (Interpreted frame)
 - org.apache.cassandra.concurrent.NamedThreadFactory$$Lambda$8.run() @bci=4 (Interpreted frame)
 - java.lang.Thread.run() @bci=11, line=748 (Compiled frame)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This particular node remain in this state forever, indicating &lt;tt&gt;LeveledCompactionStrategyTask.getNextBackgroundTask&lt;/tt&gt; was looping indefinitely.&lt;/p&gt;

&lt;p&gt;What happened is that sstable references were replaced on the tracker by the &lt;tt&gt;IndexSummaryRedistribution&lt;/tt&gt; thread, so the &lt;tt&gt;AbstractCompactionStrategy.getNextBackgroundTask&lt;/tt&gt; could not create the transaction with the old references, and the &lt;tt&gt;IndexSummaryRedistribution&lt;/tt&gt; could not update the sstable reference in the compaction strategy because &lt;tt&gt;AbstractCompactionStrategy.getNextBackgroundTask&lt;/tt&gt; was holding the &lt;tt&gt;CompactionStrategyManager&lt;/tt&gt; lock.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13108507">CASSANDRA-13948</key>
            <summary>Reload compaction strategies when JBOD disk boundary changes</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="pauloricardomg">Paulo Motta</assignee>
                                    <reporter username="pauloricardomg">Paulo Motta</reporter>
                        <labels>
                            <label>jbod-aware-compaction</label>
                    </labels>
                <created>Wed, 11 Oct 2017 06:51:06 +0000</created>
                <updated>Fri, 15 May 2020 08:03:21 +0000</updated>
                            <resolved>Fri, 8 Dec 2017 18:51:40 +0000</resolved>
                                        <fixVersion>3.11.2</fixVersion>
                    <fixVersion>4.0-alpha1</fixVersion>
                    <fixVersion>4.0</fixVersion>
                                    <component>Local/Compaction</component>
                        <due></due>
                            <votes>1</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="16199899" author="pauloricardomg" created="Wed, 11 Oct 2017 07:13:53 +0000"  >&lt;p&gt;I think we can get rid of the &lt;tt&gt;while (true)&lt;/tt&gt; loops on &lt;tt&gt;*CompactionStrategy.getNextBackgroundTask&lt;/tt&gt; when not able to lock sstables for compaction, and just submit a new background task when receiving any notifications from the tracker to ensure a new compaction will operate on the most updated references.&lt;/p&gt;

&lt;p&gt;I also removed the &lt;tt&gt;CompactionStrategyManager.replaceFlushed&lt;/tt&gt; method because it should no longer be necessary because a new background compaction candidate will be submitted when receiving an &lt;tt&gt;SSTableAddedNotification&lt;/tt&gt; from the tracker. &lt;/p&gt;

&lt;p&gt;Similarly we no longer need to call &lt;tt&gt;CompactionManager.instance.submitBackground&lt;/tt&gt; after a &lt;tt&gt;BackgroundCompactionCandidate&lt;/tt&gt; because it will already be called after receiving an &lt;tt&gt;SSTableListChangedNotification&lt;/tt&gt;, so we centralize calls to &lt;tt&gt;CompactionManager.instance.submitBackground&lt;/tt&gt; on &lt;tt&gt;CompactionStrategyManager.handleNotification&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;I added a unit test to check that &lt;tt&gt;*CompactionStrategy.getNextBackgroundTask&lt;/tt&gt; never blocks indefinitely, even when not able to lock sstables in the tracker.&lt;/p&gt;

&lt;p&gt;A race there should be pretty unlikely, but in case it happens I logged a warning to detect potential problems if it happens frequently due to some wrong condition: &lt;tt&gt;&quot;Could not acquire references for compacting SSTables {} which is not a problem per se, unless it happens frequently, in which case it must be reported. Will retry later.&quot;&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;Patch available &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/tree/3.11-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Mind having a look &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt;?&lt;/p&gt;

&lt;p&gt;I submitted internal CI, will post the results here once available.&lt;/p&gt;</comment>
                            <comment id="16199902" author="krummas" created="Wed, 11 Oct 2017 07:17:17 +0000"  >&lt;p&gt;sure, I&apos;ll review&lt;/p&gt;</comment>
                            <comment id="16202706" author="dkinder" created="Thu, 12 Oct 2017 22:13:30 +0000"  >&lt;p&gt;Just a heads up, I have been seeing these deadlocks happen easily, so I am running your patch &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt; in addition to Marcus&apos;s&lt;a href=&quot;https://github.com/krummas/cassandra/commits/marcuse/13215&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;patch&lt;/a&gt; from &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13215&quot; title=&quot;Cassandra nodes startup time 20x more after upgarding to 3.x&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13215&quot;&gt;&lt;del&gt;CASSANDRA-13215&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I do see a large number of &quot;Could not acquire references for compacting SSTable&quot; happening, in bursts. Will upload a log file.&lt;/p&gt;

&lt;p&gt;I also see some of this:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.Memory.checkBounds(Memory.java:344) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:291) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.IndexSummary.getPositionInSummary(IndexSummary.java:148) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.IndexSummary.fillTemporaryKey(IndexSummary.java:162) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:121) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.format.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:1370) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.io.sstable.format.SSTableReader.estimatedKeysForRanges(SSTableReader.java:1326) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:441) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.findDroppableSSTable(LeveledCompactionStrategy.java:503) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:121) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.CompactionStrategyManager.getNextBackgroundTask(CompactionStrategyManager.java:124) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:262) ~[apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_144]
        at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_144]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) ~[na:1.8.0_144]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) [na:1.8.0_144]
        at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$threadLocalDeallocator$0(NamedThreadFactory.java:81) [apache-cassandra-3.11.0.jar:3.11.2-SNAPSHOT]
        at java.lang.Thread.run(Thread.java:748) ~[na:1.8.0_144]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;UPDATE: the node does successfully complete compactions for a while but gradually does fewer and fewer and then stops compacting altogether, even though compactionstats says there are pending compactions:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;dkinder@seu-walker-fs01:~$ nodetool compactionstats
pending tasks: 102
- walker.links: 102

dkinder@seu-walker-fs01:~$
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;It stays this way and periodically prints out some &quot;Could not acquire references for compacting SSTable&quot; messages. It is able to compact some more if I restart the node.&lt;/p&gt;

&lt;p&gt;tablestats for walker.links:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Keyspace : walker
        Read Count: 16952
        Read Latency: 10.388668062765454 ms.
        Write Count: 277291
        Write Latency: 0.0186555207345352 ms.
        Pending Flushes: 0
                Table: links
                SSTable count: 8507
                SSTables in each level: [73/4, 81/10, 297/100, 2078/1000, 2402, 3635, 0, 0, 0]
                Space used (live): 4902698702556
                Space used (total): 4902698702556
                Space used by snapshots (total): 13788057680993
                Off heap memory used (total): 9835235
                SSTable Compression Ratio: -1.0
                Number of partitions (estimate): 19996043
                Memtable cell count: 360248
                Memtable data size: 36729792
                Memtable off heap memory used: 0
                Memtable switch count: 0
                Local read count: 0
                Local read latency: NaN ms
                Local write count: 360248
                Local write latency: 0.017 ms
                Pending flushes: 0
                Percent repaired: 0.0
                Bloom filter false positives: 0
                Bloom filter false ratio: 0.00000
                Bloom filter space used: 8538560
                Bloom filter off heap memory used: 8470504
                Index summary off heap memory used: 1364731
                Compression metadata off heap memory used: 0
                Compacted partition minimum bytes: 43
                Compacted partition maximum bytes: 190420296972
                Compacted partition mean bytes: 247808
                Average live cells per slice (last five minutes): NaN
                Maximum live cells per slice (last five minutes): 0
                Average tombstones per slice (last five minutes): NaN
                Maximum tombstones per slice (last five minutes): 0
                Dropped Mutations: 2
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16207222" author="krummas" created="Tue, 17 Oct 2017 09:27:37 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt; did you get those CI results?&lt;/p&gt;</comment>
                            <comment id="16210935" author="pauloricardomg" created="Thu, 19 Oct 2017 12:29:16 +0000"  >&lt;p&gt;I think I was able to get to the bottom of this issue with the help of &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=dkinder&quot; class=&quot;user-hover&quot; rel=&quot;dkinder&quot;&gt;dkinder&lt;/a&gt;&apos;s logs&lt;/p&gt;

&lt;p&gt;It turns out that the &lt;tt&gt;ColumnFamilyStore&lt;/tt&gt; (and subsequently the &lt;tt&gt;CompactionStrategyManager&lt;/tt&gt;) is initialized before gossip is settled, so the node&apos;s local ranges are not properly computed during startup, causing the computed disk boundaries to not match the actual boundaries. &lt;/p&gt;

&lt;p&gt;This happens because &lt;tt&gt;GossipingPropertyFileSnitch&lt;/tt&gt; fallbacks to the &lt;tt&gt;FilePropertySnitch&lt;/tt&gt;, so when a node is not found is gossip it will pick the DCs/racks from the &lt;tt&gt;cassandra-topology.properties&lt;/tt&gt; file, and if it&apos;s not defined there it will use the &lt;tt&gt;DEFAULT&lt;/tt&gt; dc/rack and mess up the local ranges and disk boundary computation.&lt;/p&gt;

&lt;p&gt;The log lines below show the following steps:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;tt&gt;GossipingPropertyFileSnitch&lt;/tt&gt; falling back to &lt;tt&gt;PropertyFileSnitch&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;Compactions on system keyspaces being scheduled on the same disk, while compaction on user keyspaces being run on SSTables from different disks, indicating the disk boundaries were not calculated correctly for NTS keyspaces&lt;/li&gt;
	&lt;li&gt;After gossip settles, lot&apos;s of &lt;tt&gt;SSTable from level 0 is not on corresponding level in the leveled manifest&lt;/tt&gt; warnings, indicating the disk boundary layout changed after gossip settled but the compaction strategies were not reloaded with the new layout&lt;/li&gt;
&lt;/ul&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-none&quot;&gt;
INFO  [main] 2017-10-12 14:39:32,928 GossipingPropertyFileSnitch.java:64 - Loaded cassandra-topology.properties for compatibility
DEBUG [CompactionExecutor:26] 2017-10-12 15:02:09,442 CompactionTask.java:155 - Compacting (fe490ea1-af98-11e7-b5a1-57bcefdac924) [/srv/disk6/cassandra-data/walker/domain_info/.domain_info_claim_tok_idx/mc-18386-big-Data.db:level=0, /srv/disk9/cassandra-data/walker/domain_info/.domain_info_claim_tok_idx/mc-18387-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:31] 2017-10-12 15:02:09,442 CompactionTask.java:155 - Compacting (fe490ea0-af98-11e7-b5a1-57bcefdac924) [/srv/disk10/cassandra-data/system/peers/mc-1671-big-Data.db:level=0, /srv/disk10/cassandra-data/system/peers/mc-1659-big-Data.db:level=0, /srv/disk10/cassandra-data/system/peers/mc-1656-big-Data.db:level=0, /srv/disk10/cassandra-data/system/peers/mc-1690-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:17] 2017-10-12 15:02:09,442 CompactionTask.java:155 - Compacting (fe490ea8-af98-11e7-b5a1-57bcefdac924) [/srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2625-big-Data.db:level=0, /srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2605-big-Data.db:level=0, /srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2597-big-Data.db:level=0, /srv/disk5/cassandra-data/system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/mc-2617-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:29] 2017-10-12 15:02:09,952 CompactionTask.java:155 - Compacting (fe9a8a00-af98-11e7-b5a1-57bcefdac924) [/srv/disk11/cassandra-data/walker/domain_info/.domain_info_dispatched_idx/mc-18474-big-Data.db:level=0, /srv/disk3/cassandra-data/walker/domain_info/.domain_info_dispatched_idx/mc-18473-big-Data.db:level=0, ]
DEBUG [CompactionExecutor:18] 2017-10-12 15:04:08,939 CompactionTask.java:155 - Compacting (45865ca0-af99-11e7-b5a1-57bcefdac924) [/srv/disk8/cassandra-data/walker/links/mc-6571323-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566335-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566315-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566361-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571043-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566330-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566322-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571261-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571335-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6570145-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566346-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571249-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571311-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566307-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571285-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566341-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6570765-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571299-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566352-big-Data.db:level=1, ]
DEBUG [CompactionExecutor:30] 2017-10-12 15:04:08,866 DiskBoundaryManager.java:69 - Cached ring version 84 != current ring version 86
DEBUG [CompactionExecutor:30] 2017-10-12 15:04:08,867 DiskBoundaryManager.java:83 - Refreshing disk boundary cache for walker.links
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,888 LeveledCompactionStrategy.java:140 - Could not acquire references for compacting SSTables [BigTableReader(path=&apos;/srv/disk2/cassandra-data/walke
r/links/mc-6566879-big-Data.db&apos;)] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,892 LeveledCompactionStrategy.java:140 - Could not acquire references for compacting SSTables [BigTableReader(path=&apos;/srv/disk11/cassandra-data/walk
er/links/mc-6566381-big-Data.db&apos;)] which is not a problem per se,unless it happens frequently, in which case it must be reported. Will retry later.
DEBUG [CompactionExecutor:30] 2017-10-12 15:04:08,939 CompactionTask.java:255 - Compacted (2f095f90-af99-11e7-b5a1-57bcefdac924) 1 sstables to [/srv/disk1/cassandra-data/walker/links/mc-6571348-big
,] to level=4.  271.077MiB to 271.077MiB (~100% of original) in 37,729ms.  Read Throughput = 7.185MiB/s, Write Throughput = 7.185MiB/s, Row Throughput = ~116,380/s.  741 total partitions merged to 
741.  Partition merge counts were {1:741, }
DEBUG [CompactionExecutor:18] 2017-10-12 15:04:08,939 CompactionTask.java:155 - Compacting (45865ca0-af99-11e7-b5a1-57bcefdac924) [/srv/disk8/cassandra-data/walker/links/mc-6571323-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566335-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566315-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566361-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571043-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566330-big-Data.db:level=1, /srv/disk7/cassandra-data/walker/links/mc-6566322-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571261-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571335-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6570145-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566346-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571249-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571311-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566307-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6571285-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566341-big-Data.db:level=1, /srv/disk8/cassandra-data/walker/links/mc-6570765-big-Data.db:level=0, /srv/disk8/cassandra-data/walker/links/mc-6571299-big-Data.db:level=0, /srv/disk7/cassandra-data/walker/links/mc-6566352-big-Data.db:level=1, ]
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,941 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571323-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566335-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566315-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566361-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566330-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566322-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571261-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571335-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566346-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571311-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,942 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566307-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,943 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk8/cassandra-data/walker/links/mc-6571285-big-Data.db from level 0 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,943 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566341-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
WARN  [CompactionExecutor:18] 2017-10-12 15:04:08,943 LeveledCompactionStrategy.java:274 - Live sstable /srv/disk7/cassandra-data/walker/links/mc-6566352-big-Data.db from level 1 is not on corresponding level in the leveled manifest. This is not a problem per se, but may indicate an orphaned sstable due to a failed compaction not cleaned up properly.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From the above I draw the following conclusions:&lt;br/&gt;
1. We should wait for gossip to settle before starting compactions, since the disk boundaries may be dependent on gossip when any gossiping snitch is used. (done on &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/d5bd9eab53ab423dde024400c299f01e335cdf4c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this commit&lt;/a&gt;)&lt;br/&gt;
2. We should reload the compaction strategies when the disk layout changes, so SSTables are correctly mapped to their corresponding compaction strategies. (done on &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/073bfa4e548ac807b523a919288a5a71379bfd21&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this commit&lt;/a&gt;, in addition to testing and some other simplifications on &lt;tt&gt;CompactionStrategyManager&lt;/tt&gt;)&lt;br/&gt;
3. A race when acquiring references on &lt;tt&gt;CompactionStrategyManager.getNextBackgroundTask&lt;/tt&gt; is quite common when there are multiple concurrent compactions, since there will be parallel flushes on multiple disks, so rather than removing the &lt;tt&gt;while true&lt;/tt&gt; loop to acquire references, I updated the compaction strategies to stop when the current candidate is the same as before, indicating there is a race with the tracker (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/3f971daa48036f14ea7aba7fd1d56150e78415b3&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;commit&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;I will work on dtests to simulate the above case as well as test disk boundary refreshing, but this &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/tree/3.11-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt; should be ready for a first round of review (cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt;). I will submit a round of unit and dtests on internal CI and will post the results here when ready.&lt;/p&gt;</comment>
                            <comment id="16210961" author="krummas" created="Thu, 19 Oct 2017 12:46:14 +0000"  >&lt;p&gt;have not had a close look yet, but did you see &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13215&quot; title=&quot;Cassandra nodes startup time 20x more after upgarding to 3.x&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13215&quot;&gt;&lt;del&gt;CASSANDRA-13215&lt;/del&gt;&lt;/a&gt; ?&lt;/p&gt;</comment>
                            <comment id="16210965" author="krummas" created="Thu, 19 Oct 2017 12:49:08 +0000"  >&lt;blockquote&gt;&lt;p&gt;It turns out that the ColumnFamilyStore (and subsequently the CompactionStrategyManager) is initialized before gossip is settled, so the node&apos;s local ranges are not properly computed during startup, causing the computed disk boundaries to not match the actual boundaries.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;we should probably fix that instead (or, as well) - flushing sstables also depends on the boundaries, and we can&apos;t delay that until gossip has settled (commitlog replay might have to flush)&lt;/p&gt;</comment>
                            <comment id="16211253" author="pauloricardomg" created="Thu, 19 Oct 2017 15:54:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;we should probably fix that instead (or, as well) - flushing sstables also depends on the boundaries, and we can&apos;t delay that until gossip has settled (commitlog replay might have to flush)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The reason why the ring boundaries were not computed correctly on startup was because the &lt;tt&gt;GossipingPropertyFileSnitch&lt;/tt&gt; did not have rack/dc info about all nodes on gossip so it fallback to the sample &lt;tt&gt;conf/cassandra-rackdc.properties&lt;/tt&gt; file from &lt;tt&gt;PropertyFileSnitch&lt;/tt&gt; so I created &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13970&quot; title=&quot;Remove PFS compatibility mode from GPFS&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13970&quot;&gt;CASSANDRA-13970&lt;/a&gt; to fix that.&lt;/p&gt;

&lt;p&gt;In any case, even with that fixed, the &lt;tt&gt;CompactionStrategyManager&lt;/tt&gt; (CSM) does not reload its compaction strategies when the disk boundaries are updated (either because of range movements, when the node first joins the ring, or a disk breaks) what can cause &lt;tt&gt;CompactionStrategyManager.getCompactionStrategyIndex&lt;/tt&gt; to return a different sstable-&amp;gt;disk assignment to the one currently on the CSM, so some SStables may not be correctly updated in the compaction strategies after being updated/replaced/removed.  Perhaps this is better illustrated by &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/073bfa4e548ac807b523a919288a5a71379bfd21#diff-f9c882c974db60a710cf1f195cfdb801R95&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this test&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;So it shouldn&apos;t be a problem if on a race with gossip flush writes an SSTable to a wrong disk, as long as after the boundary is updated, the SSTable is placed on the correct compaction strategy and its compaction output will be placed in the correct disk (should probably add a test with this scenario).&lt;/p&gt;

&lt;p&gt;Also this is a bit orthogonal to &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13215&quot; title=&quot;Cassandra nodes startup time 20x more after upgarding to 3.x&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13215&quot;&gt;&lt;del&gt;CASSANDRA-13215&lt;/del&gt;&lt;/a&gt; - the objective there is to cache the disk boundary computation, while here is to make sure CSM can gracefully handle boundary changes.&lt;/p&gt;</comment>
                            <comment id="16211402" author="krummas" created="Thu, 19 Oct 2017 17:30:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;Also this is a bit orthogonal to &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13215&quot; title=&quot;Cassandra nodes startup time 20x more after upgarding to 3.x&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13215&quot;&gt;&lt;del&gt;CASSANDRA-13215&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I was thinking that with 13215 we will have a central point which will keep track of when we need to refresh compaction strategies (we could notify CSM once cache in the DiskBoundaryManager has been invalidated for example)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;So it shouldn&apos;t be a problem if on a race with gossip flush writes an SSTable to a wrong disk,&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The flushed sstable will have wrong boundaries, it will have tokens that shouldn&apos;t be on that disk, does not really matter if the first token is on the correct disk - more tokens might be on the correct disk than not - this is why 6696 didn&apos;t reload compaction strategies after boundary changes. I didn&apos;t really consider the case from these logs though, where we are completely wrong at startup, but then go back to the correct pre-restart state, then it totally makes sense to reload them, and I agree, we should always do it for consistency.&lt;/p&gt;

&lt;p&gt;Could we create a new ticket for that though as it is not really related to the problem we are trying to solve here&lt;/p&gt;</comment>
                            <comment id="16233631" author="pauloricardomg" created="Wed, 1 Nov 2017 04:21:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;I was thinking that with 13215 we will have a central point which will keep track of when we need to refresh compaction strategies (we could notify CSM once cache in the DiskBoundaryManager has been invalidated for example)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Even with that we will probably need to cache the current boundaries on the CSM, to prevent a race where the disk boundaries change in the boundary manager and a flush puts an SSTable in the wrong strategy (due to the index having changed) before the CSM is notified about the boundary change - quite unlikely but still possible.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Could we create a new ticket for that though as it is not really related to the problem we are trying to solve here&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Actually the issue in the original ticket description only surfaced because the compaction strategies were not properly reloaded after the disk boundary changes, which is the core issue to solve here, so I will update the ticket description to better reflect that.&lt;/p&gt;

&lt;p&gt;I added two new dtests to check that the compaction strategies are being properly reloaded when the disk boundary changes due to bootstrap, decommission and delayed join (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra-dtest/commit/d53f73419e68eb6925b5baf06824b80d0ccf30b7&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;) and was able to reproduce the deadlock in current 3.11/trunk.&lt;/p&gt;

&lt;p&gt;While debugging these dtests I found that reloading the compaction strategy manager when receiving a notification from the tracker can cause an SSTable to be added twice to the &lt;tt&gt;LeveledManifest&lt;/tt&gt; (first during re-initialization of the CS and second when processing the SSTableAddedNotification), so I stopped reloading the CSM when receiving a notification from the tracker and added a warning on &lt;tt&gt;LeveledManifest&lt;/tt&gt; when trying to add an SSTable which is already present (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/a3eaa8a408cf0e8c5524062fa0fdee9a1eb0d6c0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;) - this shouldn&apos;t be a problem since we maybeReload the CSM before submitting a new background tasks.&lt;/p&gt;

&lt;p&gt;In summary this patch makes the following changes:&lt;br/&gt;
1) Reload compaction strategies when JBOD disk boundary changes (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/efb2afb22792a06d83020ac7097154593b9e684d&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;commit&lt;/a&gt;)&lt;br/&gt;
2) Ensure compaction strategies do not loop indefinitely when not able to acquire Tracker lock (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/9fdb8f0fb40954a8ed9570cb568a7084de4c80c5&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;commit&lt;/a&gt;)&lt;br/&gt;
3) Only enable compaction strategies after gossip settles to prevent unnecessary relocation work (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/c524ff724f2ca9e7eed59cb07f81b9211098fb5c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;commit&lt;/a&gt;)&lt;br/&gt;
4) Do not reload compaction strategies when receiving notifications and log warning when an SSTable is added multiple times to LCS (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/a3eaa8a408cf0e8c5524062fa0fdee9a1eb0d6c0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;commit&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;The CI of the previous version of this patch was successful, so this is ready for review. I submitted another round on 3.11 and trunk with the latest version and will update the results here when ready.&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/tree/3.11-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.11 patch&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/tree/trunk-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;trunk patch&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Please let me know what do you think.&lt;/p&gt;</comment>
                            <comment id="16234385" author="llambiel" created="Wed, 1 Nov 2017 17:02:52 +0000"  >&lt;p&gt;I tried your patch on 3.11.2 and got the following errors:&lt;/p&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,397 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@51f52b91) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.MmappedRegions$Tidier@1358582595:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103504-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,413 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@70a08046) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.MmappedRegions$Tidier@632323950:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103503-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,413 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5d6161ea) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.FileHandle$Cleanup@1594052942:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103502-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,429 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2bd55858) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.MmappedRegions$Tidier@230164803:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103502-big-Data.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,429 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@5b00472f) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.SafeMemory$MemoryTidy@508355616:Memory@[7f6b54130b10..7f6b54136f10) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,429 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1f5a7829) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@1390774416:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@b8594dc) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.FileHandle$Cleanup@1913719912:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103503-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3ec6a933) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1083770739:Memory@[7f6b5453ff30..7f6b54546330) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@671d48c8) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@496335375:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@1611d7bf) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.SafeMemory$MemoryTidy@515635345:Memory@[7f6b540f7dc0..7f6b540fe1c0) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@136db886) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@622788070:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3daa7ad5) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.FileHandle$Cleanup@2090103425:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103504-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@2435c1d) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.SafeMemory$MemoryTidy@1348493438:Memory@[7f6b546ebd80..7f6b546f2180) was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,430 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@3e72d475) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.utils.concurrent.WrappedSharedCloseable$Tidy@1638775104:[Memory@[0..20), Memory@[0..240)] was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,431 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@7916a2dc) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.FileHandle$Cleanup@715546128:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103505-big-Index.db was not released before the reference was garbage collected
ERROR [Reference-Reaper:1] 2017-11-01 17:51:35,446 Ref.java:224 - LEAK DETECTED: a reference (org.apache.cassandra.utils.concurrent.Ref$State@25fe64d) to &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;org.apache.cassandra.io.util.MmappedRegions$Tidier@238299391:/&lt;span class=&quot;code-keyword&quot;&gt;var&lt;/span&gt;/lib/cassandra/data/datadisk7/blobstore/block-ad8329f0740d11e68fe6cba3b122d983/mc-103505-big-Data.db was not released before the reference was garbage collected
ERROR [CompactionExecutor:71] 2017-11-01 17:51:36,872 CassandraDaemon.java:228 - Exception in thread &lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;[CompactionExecutor:71,1,main]
java.lang.AssertionError: &lt;span class=&quot;code-keyword&quot;&gt;null&lt;/span&gt;
	at org.apache.cassandra.io.compress.CompressionMetadata$Chunk.&amp;lt;init&amp;gt;(CompressionMetadata.java:474) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:239) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.updateState(MmappedRegions.java:163) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.&amp;lt;init&amp;gt;(MmappedRegions.java:73) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.&amp;lt;init&amp;gt;(MmappedRegions.java:61) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.MmappedRegions.map(MmappedRegions.java:104) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.util.FileHandle$Builder.complete(FileHandle.java:362) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.sstable.format.big.BigTableWriter.openEarly(BigTableWriter.java:290) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.sstable.SSTableRewriter.maybeReopenEarly(SSTableRewriter.java:179) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:134) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.writers.MaxSSTableSizeWriter.realAppend(MaxSSTableSizeWriter.java:98) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.writers.CompactionAwareWriter.append(CompactionAwareWriter.java:141) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.CompactionTask.runMayThrow(CompactionTask.java:201) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:85) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:61) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionCandidate.run(CompactionManager.java:268) ~[apache-cassandra-3.11.2.jar:3.11.2]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511) ~[na:1.8.0_131]
	at java.util.concurrent.FutureTask.run(FutureTask.java:266) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142) ~[na:1.8.0_131]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617) [na:1.8.0_131]
	at org.apache.cassandra.concurrent.NamedThreadFactory.lambda$t
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Since we are heavily impacted by this bug (see also &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13980&quot; title=&quot;Compaction deadlock&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13980&quot;&gt;&lt;del&gt;CASSANDRA-13980&lt;/del&gt;&lt;/a&gt;), I&apos;m ok to test as soon as you&apos;ve an updated version of the patch.&lt;/p&gt;</comment>
                            <comment id="16234443" author="krummas" created="Wed, 1 Nov 2017 17:34:56 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=llambiel&quot; class=&quot;user-hover&quot; rel=&quot;llambiel&quot;&gt;llambiel&lt;/a&gt; is that with the patch posted today (or yesterday depending on your tz)?&lt;/p&gt;</comment>
                            <comment id="16234460" author="llambiel" created="Wed, 1 Nov 2017 17:42:36 +0000"  >&lt;p&gt;Yes &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt;, build including the latest patch.&lt;/p&gt;</comment>
                            <comment id="16234865" author="pauloricardomg" created="Wed, 1 Nov 2017 22:28:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;I tried your patch on 3.11.2 and got the following errors:&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks for reporting, can you attach the full debug.log leading to that? What is the compaction strategy of the block table? Was this a one-off error or is it repeating periodically?&lt;/p&gt;</comment>
                            <comment id="16234902" author="llambiel" created="Wed, 1 Nov 2017 22:53:17 +0000"  >&lt;p&gt;The strategy is LCS, node with 9 data locations. The error is repeating frequently. I&apos;ll attach a debug log tomorrow and make some additional tests.&lt;/p&gt;</comment>
                            <comment id="16234924" author="pauloricardomg" created="Wed, 1 Nov 2017 23:08:14 +0000"  >&lt;blockquote&gt;&lt;p&gt;I&apos;ll attach a debug log tomorrow and make some additional tests.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks, since the exception seems related to early re-opening, it would be nice if during your tests you could try setting &lt;tt&gt;sstable_preemptive_open_interval_in_mb=-1&lt;/tt&gt; on &lt;tt&gt;cassandra.yaml&lt;/tt&gt; to check if the errors will go away (and return once you re-enable it).&lt;/p&gt;</comment>
                            <comment id="16236514" author="llambiel" created="Thu, 2 Nov 2017 20:17:00 +0000"  >&lt;p&gt;Did additional testing and wasn&apos;t able to reproduce :-/ &lt;/p&gt;

&lt;p&gt;I&apos;ll try the patch on more representative nodes in the coming days and report back any issue.&lt;/p&gt;</comment>
                            <comment id="16237059" author="pauloricardomg" created="Fri, 3 Nov 2017 04:12:12 +0000"  >&lt;blockquote&gt;&lt;p&gt;Did additional testing and wasn&apos;t able to reproduce :-/ &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;this looks similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-12743&quot; title=&quot;Assertion error while running compaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-12743&quot;&gt;&lt;del&gt;CASSANDRA-12743&lt;/del&gt;&lt;/a&gt;, so I wonder if it&apos;s an existing race that showed up due to the large compaction backlog after the deadlock was fixed.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;ll try the patch on more representative nodes in the coming days and report back any issue.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;sounds good, if you manage to reproduce it would be nice if you could change the log level of the &lt;tt&gt;org.apache.cassandra.db.compaction&lt;/tt&gt; package to &lt;tt&gt;TRACE&lt;/tt&gt;.&lt;/p&gt;</comment>
                            <comment id="16237255" author="llambiel" created="Fri, 3 Nov 2017 08:15:58 +0000"  >&lt;p&gt;Ok I was able to reproduce it on one node. I&apos;ve attached the trace log. It&apos;s unfiltered since I didn&apos;t managed to filter only to org.apache.cassandra.db.compaction&lt;/p&gt;


</comment>
                            <comment id="16238384" author="llambiel" created="Fri, 3 Nov 2017 21:08:55 +0000"  >&lt;p&gt;I&apos;ve deployed the patch on a few big nodes. I&apos;ve not seen the error popping up so far.&lt;/p&gt;

&lt;p&gt;However I&apos;m still facing issues with compactions. These are big nodes with with a big CF, holding many SSTables and pending compactions. According the thread dump it seems to be stuck around getNextBackgroundTask. Compactions are still being processed for the other keyspace. Beside that the node is running normally. Some nodetool commands takes time to proceed like compactionstats. Debug log doesn&apos;t show any error.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
CREATE TABLE blobstore.block (
    inode uuid,
    version timeuuid,
    block bigint,
    offset bigint,
    chunksize &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;,
    payload blob,
    PRIMARY KEY ((inode, version, block), offset)
) WITH CLUSTERING ORDER BY (offset ASC)
    AND bloom_filter_fp_chance = 0.01
    AND caching = {&lt;span class=&quot;code-quote&quot;&gt;&apos;keys&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;ALL&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;rows_per_partition&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;NONE&apos;&lt;/span&gt;}
    AND comment = &apos;&apos;
    AND compaction = {&lt;span class=&quot;code-quote&quot;&gt;&apos;class&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.cassandra.db.compaction.LeveledCompactionStrategy&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;enabled&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;tombstone_compaction_interval&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;60&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;tombstone_threshold&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;0.2&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;unchecked_tombstone_compaction&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;&apos;&lt;/span&gt;}
    AND compression = {&lt;span class=&quot;code-quote&quot;&gt;&apos;chunk_length_in_kb&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;64&apos;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&apos;class&apos;&lt;/span&gt;: &lt;span class=&quot;code-quote&quot;&gt;&apos;org.apache.cassandra.io.compress.LZ4Compressor&apos;&lt;/span&gt;}
    AND crc_check_chance = 1.0
    AND dclocal_read_repair_chance = 0.1
    AND default_time_to_live = 0
    AND gc_grace_seconds = 172000
    AND max_index_interval = 2048
    AND memtable_flush_period_in_ms = 0
    AND min_index_interval = 128
    AND read_repair_chance = 0.0
    AND speculative_retry = &lt;span class=&quot;code-quote&quot;&gt;&apos;99PERCENTILE&apos;&lt;/span&gt;;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Keyspace : blobstore
	Read Count: 97019
	Read Latency: 2.4842547026871027 ms.
	Write Count: 472590
	Write Latency: 0.060107954040500226 ms.
	Pending Flushes: 0
		Table: block
		SSTable count: 43373
		SSTables in each level: [18890/4, 115/10, 198/100, 1905/1000, 9451, 12814, 0, 0, 0]
		Space used (live): 4839933810943
		Space used (total): 4839933815913
		Space used by snapshots (total): 0
		Off heap memory used (total): 3273703284
		SSTable Compression Ratio: 0.9416884172984209
		&lt;span class=&quot;code-object&quot;&gt;Number&lt;/span&gt; of partitions (estimate): 2925826
		Memtable cell count: 41542
		Memtable data size: 2631688187
		Memtable off heap memory used: 2638649871
		Memtable &lt;span class=&quot;code-keyword&quot;&gt;switch&lt;/span&gt; count: 7
		Local read count: 87281
		Local read latency: 2.186 ms
		Local write count: 465591
		Local write latency: 0.124 ms
		Pending flushes: 0
		Percent repaired: 4.01
		Bloom filter &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; positives: 297882
		Bloom filter &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt; ratio: 0.69198
		Bloom filter space used: 5111208
		Bloom filter off heap memory used: 4764232
		Index summary off heap memory used: 3360917
		Compression metadata off heap memory used: 626928264
		Compacted partition minimum bytes: 61
		Compacted partition maximum bytes: 186563160
		Compacted partition mean bytes: 1797922
		Average live cells per slice (last five minutes): 8.641592920353983
		Maximum live cells per slice (last five minutes): 258
		Average tombstones per slice (last five minutes): 1.0
		Maximum tombstones per slice (last five minutes): 1
		Dropped Mutations: 0

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
nodetool compactionstats
pending tasks: 3362
- blobstore.block: 3362
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16240296" author="krummas" created="Mon, 6 Nov 2017 13:33:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=llambiel&quot; class=&quot;user-hover&quot; rel=&quot;llambiel&quot;&gt;llambiel&lt;/a&gt; I would guess you also need to apply &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13215&quot; title=&quot;Cassandra nodes startup time 20x more after upgarding to 3.x&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13215&quot;&gt;&lt;del&gt;CASSANDRA-13215&lt;/del&gt;&lt;/a&gt; for that many sstables, but I doubt both these apply cleanly now, hopefully we&apos;ll get them in real soon&lt;/p&gt;</comment>
                            <comment id="16240341" author="llambiel" created="Mon, 6 Nov 2017 14:07:13 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt; The patches from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt; already reduced the startup time by at least 10x&lt;/p&gt;</comment>
                            <comment id="16241711" author="pauloricardomg" created="Tue, 7 Nov 2017 09:06:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;Ok I was able to reproduce it on one node. I&apos;ve attached the trace log. It&apos;s unfiltered since I didn&apos;t managed to filter only to org.apache.cassandra.db.compaction&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I wasn&apos;t able to track down the root cause of this condition from the logs, but a similar issue was reported on &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-12743&quot; title=&quot;Assertion error while running compaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-12743&quot;&gt;&lt;del&gt;CASSANDRA-12743&lt;/del&gt;&lt;/a&gt;, so I think this is some kind of race condition showing up due to the amount of concurrent compactions happening and is not a consequence of this fix, so I prefer to investigate this separately. If you still see this issue please feel free to reopen &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-12743&quot; title=&quot;Assertion error while running compaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-12743&quot;&gt;&lt;del&gt;CASSANDRA-12743&lt;/del&gt;&lt;/a&gt; with details.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;However I&apos;m still facing issues with compactions. These are big nodes with with a big CF, holding many SSTables and pending compactions. According the thread dump it seems to be stuck around getNextBackgroundTask. Compactions are still being processed for the other keyspace. Beside that the node is running normally. Some nodetool commands takes time to proceed like compactionstats. Debug log doesn&apos;t show any error.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;After having a look at the thread dump, it turns out that my previous patch generated a lock contention between compaction and cleanup, because each removed SSTable from cleanup generated a &lt;tt&gt;SSTableDeletingNotification&lt;/tt&gt; and my previous patch submitted a new compaction task after each received notification which competed with the next &lt;tt&gt;SSTableDeletingNotification&lt;/tt&gt; for the &lt;tt&gt;writeLock&lt;/tt&gt; - making things slow overall, so I updated the patch to only submit a new compaction after receiving a flush notification as it was before, so this should be fixed now. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=llambiel&quot; class=&quot;user-hover&quot; rel=&quot;llambiel&quot;&gt;llambiel&lt;/a&gt;  would you mind trying the latest version now?&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=krummas&quot; class=&quot;user-hover&quot; rel=&quot;krummas&quot;&gt;krummas&lt;/a&gt; this should be ready for review now, the latest version already got a clean CI run, but I resubmitted a new internal CI run after doing the minor fix above and will update here when ready.&lt;/p&gt;

&lt;p&gt;Summary of changes:&lt;br/&gt;
1) Reload compaction strategies when JBOD disk boundary changes (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/6cab7e0a31a638cc4a957c4ecfa592035d874058&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;commit&lt;/a&gt;)&lt;br/&gt;
2) Ensure compaction strategies do not loop indefinitely when not able to acquire Tracker lock (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/3ef833d1e56c25f67bc8a3b49acf97b2efdf401d&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;commit&lt;/a&gt;)&lt;br/&gt;
3) Only enable compaction strategies after gossip settles to prevent unnecessary relocation work (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/eaf63dc3d52566ce0c4f91bbfec478305597f014&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;commit&lt;/a&gt;)&lt;br/&gt;
4) Do not reload compaction strategies when receiving notifications and log warning when an SSTable is added multiple times to LCS (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/3e61df70025e704ee0c9d6ee8754ccdd38f5ab6d&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;commit&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;Patches&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/tree/3.11-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.11&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/tree/trunk-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;trunk&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I wonder if now that CSM caches the disk boundaries we can make the handling of notifications use the readLock instead of the writeLock, to reduce contention when there is a high number of concurrent compactors, do you see any potential problems with this? Even if the notification handling races with getNextBackground task, as long as the individual compaction strategies are synchronized getNextBackground task should get a consistent view of the strategy sstables when there is a concurrent notification from the tracker.&lt;/p&gt;</comment>
                            <comment id="16244335" author="llambiel" created="Wed, 8 Nov 2017 17:13:37 +0000"  >&lt;p&gt;Compactions are now processing as expected with your latest patch &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;However I&apos;m facing an issue with nodetool cleanup, dunno if it is related or not.&lt;/p&gt;

&lt;p&gt;Starting a cleanup cancel the ongoing compactions (which is expected from my understanding) and then get lost. Not performing any cleanup nor processing the pending compactions. 1 thread is using 100% of a core all the time. The log doesn&apos;t show any error. I&apos;ve attached a thread dump.&lt;/p&gt;

&lt;p&gt;I&apos;m happy to open another Jira if it&apos;s not related.&lt;/p&gt;</comment>
                            <comment id="16244697" author="pauloricardomg" created="Wed, 8 Nov 2017 20:44:49 +0000"  >&lt;blockquote&gt;&lt;p&gt;Starting a cleanup cancel the ongoing compactions (which is expected from my understanding) and then get lost. Not performing any cleanup nor processing the pending compactions. 1 thread is using 100% of a core all the time. The log doesn&apos;t show any error. I&apos;ve attached a thread dump.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;this looks similar to &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13362&quot; title=&quot;Cassandra 2.1.15 main thread stuck in logback stack trace upon joining existing cluster&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13362&quot;&gt;&lt;del&gt;CASSANDRA-13362&lt;/del&gt;&lt;/a&gt; and unrelated to this issue. looking at the jstack it seems like there is some kind of deadlock on logback, do you have enough disk space in your log dir? did you set your log level back to DEBUG from TRACE level (which can be quite verbose and generate too many log rotations - what may be contributing to the logback problem)?&lt;/p&gt;</comment>
                            <comment id="16245169" author="pauloricardomg" created="Thu, 9 Nov 2017 03:55:49 +0000"  >&lt;p&gt;Just a heads up that internal CI looks good for the latest version of the patch (there are a bunch of unrelated failures on trunk). I will rebase this on top of &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13215&quot; title=&quot;Cassandra nodes startup time 20x more after upgarding to 3.x&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13215&quot;&gt;&lt;del&gt;CASSANDRA-13215&lt;/del&gt;&lt;/a&gt; after the first round of review.&lt;/p&gt;</comment>
                            <comment id="16247193" author="krummas" created="Fri, 10 Nov 2017 08:29:51 +0000"  >&lt;p&gt;Just had a first pass of the commits, and in general it looks good, I added a few comments on github&lt;/p&gt;

&lt;p&gt;In general it feels a bit inconsistent about when it calls the &lt;tt&gt;CSM#maybeReload()&lt;/tt&gt; - with 13215 in, perhaps we could just always call it as it will be cheap? I&apos;ll do a more thorough review of that last commit once it has been rebased on top of 13215&lt;/p&gt;</comment>
                            <comment id="16249067" author="pauloricardomg" created="Mon, 13 Nov 2017 03:14:24 +0000"  >&lt;blockquote&gt;&lt;p&gt;Just had a first pass of the commits, and in general it looks good, I added a few comments on github&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks for the review! Addressed github comments and rebased this on top of &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13215&quot; title=&quot;Cassandra nodes startup time 20x more after upgarding to 3.x&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13215&quot;&gt;&lt;del&gt;CASSANDRA-13215&lt;/del&gt;&lt;/a&gt; on this v2 branch: &lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...pauloricardomg:3.11-13948-v2&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.11-13948-v2&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I forgot to link the dtest branch here last time: &lt;a href=&quot;https://github.com/pauloricardomg/cassandra-dtest/tree/13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;13948-dtest&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Will submit a new CI and post results here when ready.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In general it feels a bit inconsistent about when it calls the CSM#maybeReload() - with 13215 in, perhaps we could just always call it as it will be cheap? I&apos;ll do a more thorough review of that last commit once it has been rebased on top of 13215&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;You mean not calling it on &lt;tt&gt;handleNotification&lt;/tt&gt; or inconsistent use in general across &lt;tt&gt;CSM&lt;/tt&gt;? Calling &lt;tt&gt;maybeReload&lt;/tt&gt; on &lt;tt&gt;handleNotification&lt;/tt&gt; is unnecessary, because if the CSM strategy is reloaded then the notification is no longer necessary - since all added or removed SSTables will already be in the correct place after reload - so we just call it whenever building a new compaction task - perhaps we could try reloading on &lt;tt&gt;handleNotification&lt;/tt&gt; and avoid delivering the notification if the reload is successful?&lt;/p&gt;</comment>
                            <comment id="16249228" author="pauloricardomg" created="Mon, 13 Nov 2017 08:21:00 +0000"  >&lt;p&gt;Testall passed with no failures, and &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12897298/dtest13948.png&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;dtest failures&lt;/a&gt; look unrelated.&lt;/p&gt;</comment>
                            <comment id="16263933" author="krummas" created="Thu, 23 Nov 2017 07:59:31 +0000"  >&lt;p&gt;I&apos;m +1 on the code, just a few small comments;&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;A few unused imports in CSM&lt;/li&gt;
	&lt;li&gt;Could we update the class comment in CSM to cover the locking strategy (when do we need the read / write lock) and when maybeReload is called?&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16267272" author="pauloricardomg" created="Mon, 27 Nov 2017 19:06:14 +0000"  >&lt;p&gt;Thanks for the review!&lt;/p&gt;

&lt;p&gt;After rebasing this on top of &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-13215&quot; title=&quot;Cassandra nodes startup time 20x more after upgarding to 3.x&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-13215&quot;&gt;&lt;del&gt;CASSANDRA-13215&lt;/del&gt;&lt;/a&gt; and addressing your latest comments, I noticed a few things which could be improved and did the following updates:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Since blacklisting a directory will refresh the disk boundaries, we only need to reload strategies when the disk boundary changes or the table parameters change. To avoid equals comparison every time we call &lt;tt&gt;maybeReload&lt;/tt&gt;, I moved the &lt;tt&gt;isOutOfDate&lt;/tt&gt; check from the &lt;tt&gt;DiskBoundaryManager&lt;/tt&gt; to the &lt;tt&gt;DiskBoundaries&lt;/tt&gt; object - which is invalidated when there are any boundary changes. (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/662cd063ca2e1c382ba3cd5dc8032b0d3f12683c&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;commit&lt;/a&gt;)&lt;/li&gt;
	&lt;li&gt;I thought that it no longer makes sense to expose the compaction strategy index to outside the compaction strategy manager since it&apos;s possible to get the correct disk placement directly from &lt;tt&gt;CFS.getDiskBoundaries&lt;/tt&gt;. This should prevent races when the &lt;tt&gt;CompactionStrategyManager&lt;/tt&gt; reloads boundaries between successive calls to &lt;tt&gt;CSM.getCompactionStrategyIndex&lt;/tt&gt;. &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/abd1340b000d4596d71f00e5de8507de967ee7a5&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;This commit&lt;/a&gt; updates &lt;tt&gt;relocatesstables&lt;/tt&gt; and &lt;tt&gt;scrub&lt;/tt&gt; to use &lt;tt&gt;CFS.getDiskBoundaries&lt;/tt&gt; instead, and make &lt;tt&gt;CSM.getCompactionStrategyIndex&lt;/tt&gt; private.&lt;/li&gt;
	&lt;li&gt;I found it a bit hard to reason about when to use &lt;tt&gt;maybeReload&lt;/tt&gt; to write the documentation and made its use consistent across &lt;tt&gt;CompactionStrategyManager&lt;/tt&gt; on &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/c0926e99edb1ffdcda16640eda6faf8e78da9e46&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this commit&lt;/a&gt;) (as you suggested before) along with the documentation. I kept the previous call to &lt;tt&gt;maybeReload&lt;/tt&gt; from &lt;tt&gt;ColumnFamilyStore.reload&lt;/tt&gt;, but we could probably avoid this and make &lt;tt&gt;maybeReload&lt;/tt&gt; private-only as this is being called on pretty much every operation.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;It feels like we can simplify this and get rid of these locks altogether (or at least greatly reduce their scope) by encapsulating the disk boundaries and compaction strategies in an immutable object accessed with an atomic reference and pessimistically cancel any tasks with an old placement when the strategies are reloaded. This is a significant refactor of &lt;tt&gt;CompactionStrategyManager&lt;/tt&gt; so we should probably do it another ticket.&lt;/p&gt;

&lt;p&gt;I submitted internal CI with the &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/tree/3.11-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;latest branch&lt;/a&gt; and will post the results here when ready. I will create a trunk version after this follow-up is reviewed.&lt;/p&gt;</comment>
                            <comment id="16267903" author="pauloricardomg" created="Tue, 28 Nov 2017 01:04:48 +0000"  >&lt;p&gt;Canceling patch while I address some test failures, will resubmit when CI is green.&lt;/p&gt;</comment>
                            <comment id="16271933" author="pauloricardomg" created="Thu, 30 Nov 2017 00:57:29 +0000"  >&lt;p&gt;There were test failures on:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;testall: &lt;tt&gt;CompactionsCQLTest.testSetLocalCompactionStrategy&lt;/tt&gt; and &lt;tt&gt;testTriggerMinorCompactionSTCSNodetoolEnabled&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;dtest: &lt;tt&gt;disk_balance_test.TestDiskBalance.disk_balance_bootstrap_test&lt;/tt&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;&lt;tt&gt;testSetLocalCompactionStrategy&lt;/tt&gt; and &lt;tt&gt;testTriggerMinorCompactionSTCSNodetoolEnabled&lt;/tt&gt; were failing because when the strategy was updated via JMX, these manually set configurations were not surviving the compaction strategy reload - this was not introduced by this, but would also happen in case a directory was blacklisted before. This was fixed &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/11c9a130d9cb7a6cfc5a039fdf79963f7e779d08&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;on this commit&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;While investigating why the strategies were reloaded even without a ring change on the tests above, I noticed that &lt;tt&gt;Keyspace.createReplicationStrategy&lt;/tt&gt; was being called multiple times (on &lt;tt&gt;Keyspace&lt;/tt&gt; construction and &lt;tt&gt;setMetadata&lt;/tt&gt;), so I updated to only invalidate the disk boundaries when the replication settings actually change (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/8a398a5d0d261178547946ac4e457f9abeb90f18&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;After the fix above, &lt;tt&gt;disk_balance_bootstrap_test&lt;/tt&gt; started failing with imbalanced disks because the disk boundaries were not being invalidated after the joining node broadcasted its tokens via gossip, so &lt;tt&gt;TokenMetadata.getPendingRanges(keyspace, FBUtilities.getBroadcastAddress())&lt;/tt&gt; was returning empty during disk boundary creation and causing imbalance. This is not failing on trunk because the double invalidation above during keyspace creation was causing the compaction strategy manager to reload the strategies with the correct ring placement during streaming. The fix to this is to invalidate the cached ring after gossiping the local tokens (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/007d596ffe0c5f965cf398646c52daa8f73c5c46&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;This made me realize that when replacing a node with the same address, even though the node is on bootstrap mode, it doesn&apos;t have any pending range, because it sets its token to normal state during bootstrap, what will cause its boundaries to not be computed correctly. I added a &lt;a href=&quot;https://github.com/pauloricardomg/cassandra-dtest/commit/8d48b166c9bfce51f9ab6c3abd73dfd4779a7c04&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt; to show this and a &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/6efd9cd454ce2fbfd40e592b6aaeda9debdb1c2b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;fix&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Finally, I didn&apos;t find a good reason to pass &lt;tt&gt;ColumnFamilyStore&lt;/tt&gt; as argument to &lt;tt&gt;getDiskBoundaries&lt;/tt&gt;, so I updated it to make it a field instead (&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/5df0d5ebed67aaae6ef9350d25b602af2a1702cf&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;I submitted internal CI, and testall is green and dtest failures &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12899922/dtest2.png&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;seem unrelated&lt;/a&gt;. Setting to patch available as this should be ready for a new round of review now. Thanks!&lt;/p&gt;</comment>
                            <comment id="16272297" author="krummas" created="Thu, 30 Nov 2017 08:00:57 +0000"  >&lt;p&gt;This ticket is getting quite big and very hard to review&lt;/p&gt;

&lt;p&gt;Could we split out all the pre-existing bugs in other tickets and get them committed separately? Especially &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/commit/007d596ffe0c5f965cf398646c52daa8f73c5c46&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this&lt;/a&gt; as it involves tokenmetadata.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Finally, I didn&apos;t find a good reason to pass ColumnFamilyStore as argument to getDiskBoundaries&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;we shouldn&apos;t leak &lt;tt&gt;this&lt;/tt&gt; from the CFS constructor, I know we do it with CSM, but that is an ancient leftover that we should probably refactor away&lt;/p&gt;

&lt;p&gt;edit: seems we leak this in many places in the cfs constructor, but lets avoid it in this case as it really doesn&apos;t hurt passing cfs to the methods&lt;/p&gt;</comment>
                            <comment id="16272641" author="pauloricardomg" created="Thu, 30 Nov 2017 13:13:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;This ticket is getting quite big and very hard to review&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I tried to make things easier by splitting in different commits, but I agree it became a bit complicated for review.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Could we split out all the pre-existing bugs in other tickets and get them committed separately? Especially this as it involves tokenmetadata.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The problem is that some bugs (even though were pre-existing) only started showing up after this, so they have a dependency on this. &lt;/p&gt;

&lt;p&gt;I reorganized &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/tree/3.11-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this branch&lt;/a&gt; to keep only things essential to this ticket, created &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-14079&quot; title=&quot;Prevent compaction strategies from looping indefinitely&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-14079&quot;&gt;&lt;del&gt;CASSANDRA-14079&lt;/del&gt;&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-14081&quot; title=&quot;Remove unused and deprecated methods from AbstractCompactionStrategy&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-14081&quot;&gt;&lt;del&gt;CASSANDRA-14081&lt;/del&gt;&lt;/a&gt; with unrelated minor fixes, and will create two follow-up tickets which depend on this.&lt;/p&gt;

&lt;p&gt;This should be ready for review now, please let me know if some of the changes are not clear for you and needs better explanation. CI looked clean before the reorganization, but I will resubmit with the essential ticket just to make sure we didn&apos;t miss anything:&lt;/p&gt;

&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/pauloricardomg/cassandra/tree/3.11-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;3.11 patch&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/pauloricardomg/cassandra-dtest/tree/13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16274422" author="pauloricardomg" created="Fri, 1 Dec 2017 13:50:57 +0000"  >&lt;p&gt;CI looks good (unrelated &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12900221/13948testall.png&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;testall&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12900222/13948dtest.png&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;dtest&lt;/a&gt; failures).&lt;/p&gt;</comment>
                            <comment id="16276816" author="krummas" created="Mon, 4 Dec 2017 14:08:00 +0000"  >&lt;p&gt;+1 on the 3.11 patch&lt;/p&gt;</comment>
                            <comment id="16276818" author="krummas" created="Mon, 4 Dec 2017 14:09:28 +0000"  >&lt;p&gt;need to check the trunk patch as well, cancelling &quot;ready to commit&quot; (i hope)&lt;/p&gt;</comment>
                            <comment id="16280241" author="pauloricardomg" created="Wed, 6 Dec 2017 14:35:23 +0000"  >&lt;blockquote&gt;&lt;p&gt;+1 on the 3.11 patch&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Thanks for the review!&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;need to check the trunk patch as well, cancelling &quot;ready to commit&quot; (i hope)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The merge went smoothly, most of the conflicts were related to &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9143&quot; title=&quot;Fix consistency of incrementally repaired data across replicas&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9143&quot;&gt;&lt;del&gt;CASSANDRA-9143&lt;/del&gt;&lt;/a&gt;, so I &lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-13948#diff-f9c882c974db60a710cf1f195cfdb801R113&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;updated&lt;/a&gt; &lt;tt&gt;CompactionStrategyManagerTest&lt;/tt&gt; to mark a subset of sstables as repaired and pending repair to make sure sstables are being assigned the correct strategies for repaired and pending repair sstables.&lt;/p&gt;

&lt;p&gt;However, there were 2 test failures in the trunk branch after the merge:&lt;br/&gt;
1. &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/blob/41416af426c41cdd38e157f38fb440342bca4dd0/test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java#L163&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testSetLocalCompactionStrategy&lt;/a&gt;&lt;br/&gt;
2. &lt;a href=&quot;https://github.com/pauloricardomg/cassandra-dtest/blob/73d7a8e1deb5eab05867d804933621062c2f6762/disk_balance_test.py#L34&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;disk_balance_bootstrap_test&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1. was failing &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/blob/41416af426c41cdd38e157f38fb440342bca4dd0/test/unit/org/apache/cassandra/db/compaction/CompactionsCQLTest.java#L175&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt; because &lt;tt&gt;ALTER TABLE t WITH gc_grace_seconds = 1000&lt;/tt&gt; was causing the manually set compaction strategy to be replaced by the strategy defined on the schema. After investigation, it turned out that the disk boundaries were being invalidated due to the schema reload (introduced by &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9425&quot; title=&quot;Make node-local schema fully immutable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9425&quot;&gt;&lt;del&gt;CASSANDRA-9425&lt;/del&gt;&lt;/a&gt;), and &lt;tt&gt;maybeReload(TableMetadata)&lt;/tt&gt; was causing the compaction strategies to be reloaded with the schema settings instead of the manually set settings. In order to fix this, I split &lt;tt&gt;maybeReload&lt;/tt&gt; in the original &lt;tt&gt;maybeReload(TableMetadata)&lt;/tt&gt;, which should be called externally by &lt;tt&gt;ColumnFamilyStore&lt;/tt&gt; and only reloads the strategies when the schema table parameters change, and &lt;tt&gt;maybeReloadDiskBoundaries&lt;/tt&gt; which is used internally and reloads the compaction strategies with the same table settings when the disk boundaries are invalidated &lt;a href=&quot;https://github.com/apache/cassandra/commit/de5916e7c4f37736d5e1d06f0fc2b9c082b6bb99&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;2.since the local ranges are not defined when the bootstrapping node starts, the disk boundaries are empty, but before &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9425&quot; title=&quot;Make node-local schema fully immutable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9425&quot;&gt;&lt;del&gt;CASSANDRA-9425&lt;/del&gt;&lt;/a&gt; the boundaries were invalidated during keyspace construction (&lt;a href=&quot;https://github.com/apache/cassandra/blob/cassandra-3.11/src/java/org/apache/cassandra/config/Schema.java#L388&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;), so the correct boundaries were used during streaming. After &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-9425&quot; title=&quot;Make node-local schema fully immutable&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-9425&quot;&gt;&lt;del&gt;CASSANDRA-9425&lt;/del&gt;&lt;/a&gt;  the boundaries were no longer reloaded during keyspace creation (&lt;a href=&quot;https://github.com/apache/cassandra/blob/4c80eeece37d79f434078224a0504400ae10a20d/src/java/org/apache/cassandra/schema/Schema.java#L138&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;), so the empty boundaries were used during streaming and the disks were imbalanced. I had exactly the same problem on &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-14083&quot; title=&quot;Avoid invalidating disk boundaries unnecessarily&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-14083&quot;&gt;&lt;del&gt;CASSANDRA-14083&lt;/del&gt;&lt;/a&gt; (&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-14083?focusedCommentId=16272918&amp;amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-16272918&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;here&lt;/a&gt;). The solution is to invalidate the disk boundaries after the tokens are set during bootstrap (&lt;a href=&quot;https://github.com/apache/cassandra/commit/a37bbda45142e1b351908a4ff5196eb08e92082b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;After these two fixes, the tests were passing (failures seem unrelated - test screenshots from internal CI below):&lt;/p&gt;

&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;3.11&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;trunk&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;dtest&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/cassandra-3.11...pauloricardomg:3.11-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/apache/cassandra/compare/trunk...pauloricardomg:trunk-13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://github.com/apache/cassandra-dtest/compare/master...pauloricardomg:13948&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;branch&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12900864/3.11-13948-testall.png&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12900862/trunk-13948-testall.png&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;testall&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12900865/3.11-13948-dtest.png&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12900863/trunk-13948-dtest.png&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;dtest&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;
</comment>
                            <comment id="16280448" author="krummas" created="Wed, 6 Dec 2017 16:35:20 +0000"  >&lt;p&gt;this LGTM, +1, just one comment and want to mention one possible issue I just realized:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
        &lt;span class=&quot;code-comment&quot;&gt;// If reloaded, SSTables will be placed in their correct locations
&lt;/span&gt;        &lt;span class=&quot;code-comment&quot;&gt;// so there is no need to process notification
&lt;/span&gt;        &lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (maybeReloadDiskBoundaries())
            &lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt;;
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;so the code above, being run in &lt;tt&gt;handleListChangedNotification&lt;/tt&gt;, &lt;tt&gt;handleRepairStatusChangedNotification&lt;/tt&gt;, &lt;tt&gt;handleDeletingNotification&lt;/tt&gt; and &lt;tt&gt;handleFlushNotification&lt;/tt&gt; in CSM, should this call be run inside the read lock? My concern is that if something refreshes the boundaries right before the call, we might double-add/remove sstables in the compaction strategies. This is handled by the fix you made in &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-14079&quot; title=&quot;Prevent compaction strategies from looping indefinitely&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-14079&quot;&gt;&lt;del&gt;CASSANDRA-14079&lt;/del&gt;&lt;/a&gt;, but checking (or re-checking) with the lock should make sure we avoid the double-adding at all I think. I guess it would need some refactoring since we can&apos;t upgrade the read lock to a write lock. This ticket has dragged on long enough, so we could open a new ticket for this I guess since I don&apos;t think it will be a problem currently.&lt;/p&gt;

&lt;p&gt;trunk comment, feel free to address on commit;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;lets remove the deprecated &lt;tt&gt;public AbstractCompactionTask getUserDefinedTask(Collection&amp;lt;SSTableReader&amp;gt; sstables, int gcBefore)&lt;/tt&gt; in CSM (and the &lt;tt&gt;validateForCompaction&lt;/tt&gt; boolean to &lt;tt&gt;List&amp;lt;AbstractCompactionTask&amp;gt; getUserDefinedTasks(Collection&amp;lt;SSTableReader&amp;gt; sstables, int gcBefore, boolean validateForCompaction)&lt;/tt&gt;)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16284022" author="pauloricardomg" created="Fri, 8 Dec 2017 18:51:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;My concern is that if something refreshes the boundaries right before the call, we might double-add/remove sstables in the compaction strategies.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Good catch, even though unlikely this is indeed a possible scenario.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;This ticket has dragged on long enough, so we could open a new ticket for this I guess since I don&apos;t think it will be a problem currently.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Agreed, created &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-14103&quot; title=&quot;Fix potential race during compaction strategy reload&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-14103&quot;&gt;&lt;del&gt;CASSANDRA-14103&lt;/del&gt;&lt;/a&gt; to fix that and I commented a possible fix proposal there.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;trunk comment, feel free to address on commit;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Done, and I got too excited and removed it from 3.11 as well by mistake, so reinstated it on this commit: &lt;tt&gt;16bcbb9256392dc5364f2bd592f45649080935dc&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;Committed as &lt;tt&gt;25e46f05294fd42c111f2f1d5881082d97c572ea&lt;/tt&gt; to cassandra-3.11 and merge up to master. Thanks for the review!&lt;/p&gt;</comment>
                            <comment id="16286496" author="pauloricardomg" created="Mon, 11 Dec 2017 20:23:18 +0000"  >&lt;p&gt;Committed dtest as &lt;tt&gt;debe3780a4694c978f2516e565e071782dc7b2c8&lt;/tt&gt;. Thanks!&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310360">
                    <name>Dependent</name>
                                                                <inwardlinks description="Dependent">
                                        <issuelink>
            <issuekey id="13121912">CASSANDRA-14082</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="12310000">
                    <name>Duplicate</name>
                                            <outwardlinks description="duplicates">
                                        <issuelink>
            <issuekey id="13112867">CASSANDRA-13980</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13108030">CASSANDRA-13943</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="12856683">CASSANDRA-10099</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13121921">CASSANDRA-14083</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13123761">CASSANDRA-14103</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12900222" name="13948dtest.png" size="182762" author="pauloricardomg" created="Fri, 1 Dec 2017 13:49:37 +0000"/>
                            <attachment id="12900221" name="13948testall.png" size="44520" author="pauloricardomg" created="Fri, 1 Dec 2017 13:49:36 +0000"/>
                            <attachment id="12900865" name="3.11-13948-dtest.png" size="68811" author="pauloricardomg" created="Wed, 6 Dec 2017 13:38:56 +0000"/>
                            <attachment id="12900864" name="3.11-13948-testall.png" size="26708" author="pauloricardomg" created="Wed, 6 Dec 2017 13:38:55 +0000"/>
                            <attachment id="12891830" name="debug.log" size="4286052" author="dkinder" created="Thu, 12 Oct 2017 22:17:55 +0000"/>
                            <attachment id="12897298" name="dtest13948.png" size="204287" author="pauloricardomg" created="Mon, 13 Nov 2017 08:20:02 +0000"/>
                            <attachment id="12899922" name="dtest2.png" size="197776" author="pauloricardomg" created="Thu, 30 Nov 2017 00:47:13 +0000"/>
                            <attachment id="12896686" name="threaddump-cleanup.txt" size="394818" author="llambiel" created="Wed, 8 Nov 2017 17:04:21 +0000"/>
                            <attachment id="12895966" name="threaddump.txt" size="413140" author="llambiel" created="Fri, 3 Nov 2017 21:09:20 +0000"/>
                            <attachment id="12895588" name="trace.log" size="7352186" author="llambiel" created="Fri, 3 Nov 2017 08:12:58 +0000"/>
                            <attachment id="12900863" name="trunk-13948-dtest.png" size="42136" author="pauloricardomg" created="Wed, 6 Dec 2017 13:38:55 +0000"/>
                            <attachment id="12900862" name="trunk-13948-testall.png" size="43604" author="pauloricardomg" created="Wed, 6 Dec 2017 13:38:56 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>12.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[pauloricardomg]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 49 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3l4b3:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>marcuse</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[marcuse]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>