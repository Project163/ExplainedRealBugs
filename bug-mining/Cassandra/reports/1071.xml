<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:22:27 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-2388] ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-2388</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;ColumnFamilyRecordReader only tries the first location for a given split. We should try multiple locations for a given split.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12502417">CASSANDRA-2388</key>
            <summary>ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10003" iconUrl="https://issues.apache.org/jira/images/icons/priorities/trivial.svg">Low</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="pauloricardomg">Paulo Motta</assignee>
                                    <reporter username="eldondev">Eldon Stegall</reporter>
                        <labels>
                            <label>hadoop</label>
                            <label>inputformat</label>
                    </labels>
                <created>Fri, 25 Mar 2011 20:40:24 +0000</created>
                <updated>Tue, 16 Apr 2019 09:33:03 +0000</updated>
                            <resolved>Fri, 20 Nov 2015 13:54:28 +0000</resolved>
                                        <fixVersion>2.1.12</fixVersion>
                    <fixVersion>2.2.4</fixVersion>
                                    <component>Legacy/Tools</component>
                        <due></due>
                            <votes>6</votes>
                                    <watches>13</watches>
                                                                                                                <comments>
                            <comment id="13011464" author="brandon.williams" created="Fri, 25 Mar 2011 22:45:55 +0000"  >&lt;p&gt;I&apos;m not sure special casing NoRouteToHostException to be blacklisted is the best thing to do.  I don&apos;t think connections are being setup so often that maintaining a blacklist for any reason is needed.&lt;/p&gt;</comment>
                            <comment id="13011667" author="brandon.williams" created="Sat, 26 Mar 2011 18:24:39 +0000"  >&lt;p&gt;Unfortunately, I thought of another problem here.  If we go over the entire replica set, we&apos;re potentially going outside of the DC, which is bad since a lot of installations have a DC dedicated to analytics so it doesn&apos;t affect their app. It seems that the local address is preferred though, are your task trackers not on the same machines as Cassandra?&lt;/p&gt;</comment>
                            <comment id="13035052" author="jbellis" created="Tue, 17 May 2011 21:07:16 +0000"  >&lt;p&gt;Eldon, are you planning to take another stab at this?&lt;/p&gt;</comment>
                            <comment id="13035053" author="tjake" created="Tue, 17 May 2011 21:09:01 +0000"  >&lt;p&gt;We need to return the list of replicas in the same DC&lt;/p&gt;</comment>
                            <comment id="13037975" author="jbellis" created="Mon, 23 May 2011 14:45:44 +0000"  >&lt;p&gt;Mck, do you want to take a stab at this?&lt;/p&gt;</comment>
                            <comment id="13038026" author="michaelsembwever" created="Mon, 23 May 2011 16:27:46 +0000"  >&lt;p&gt;I&apos;m having a go currently at &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-1125&quot; title=&quot;Filter out ColumnFamily rows that aren&amp;#39;t part of the query (using a KeyRange)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-1125&quot;&gt;&lt;del&gt;CASSANDRA-1125&lt;/del&gt;&lt;/a&gt; so i might as well look at this too. (but you&apos;ve caught me on a holiday-week...)&lt;/p&gt;</comment>
                            <comment id="13045443" author="michaelsembwever" created="Tue, 7 Jun 2011 14:11:07 +0000"  >&lt;p&gt;How do i obtain the DataCenter name for a given address?&lt;/p&gt;

&lt;p&gt;IEndpointSnitch.getDataCenter(inetAddress) would work nicely for me but how do i get the snitch client-side?&lt;/p&gt;</comment>
                            <comment id="13045620" author="michaelsembwever" created="Tue, 7 Jun 2011 20:41:15 +0000"  >&lt;p&gt;Initial attempt at solution. Although I&apos;m a little apprehensive to the additions to cassandra.thrift&lt;br/&gt;
(describe_rack(..) isn&apos;t used anywhere, it just made sense to add describe_datacenter(..) and describe_rack(..) at the same time).&lt;/p&gt;

&lt;p&gt;I&apos;ve tested that existing hadoop jobs work but the new functionality hasn&apos;t been tested (as i currently don&apos;t have any RF=2 data setup).&lt;/p&gt;

&lt;p&gt;This patch does not include the required re-generated Cassandra.java&lt;/p&gt;</comment>
                            <comment id="13046357" author="michaelsembwever" created="Thu, 9 Jun 2011 06:29:04 +0000"  >&lt;p&gt;Second attempt. (god only knows what i was trying to test last patch &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
this patch:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;adds describe_datacenter and describe_rack to cassandra.thrift&lt;/li&gt;
	&lt;li&gt;adds locations in ColumnFamilyRecordReader from the split&apos;s alternative endpoints if dc is the same&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This patch does not include the required re-generated Cassandra.java&lt;/p&gt;</comment>
                            <comment id="13046450" author="michaelsembwever" created="Thu, 9 Jun 2011 10:22:57 +0000"  >&lt;p&gt;I have tested this now on data w/ RF=2.&lt;br/&gt;
Seems to work ~ok as far as i can see.&lt;/p&gt;

&lt;p&gt;One side-effect of this patch is where once one could configure ConfigHelper.setInitialAddress(conf, &quot;localhost&quot;) this will no longer work for tasks trying to run on the down node.&lt;br/&gt;
ColumnFamilyRecordReader.getLocations() will ConnectException trying to call describe_datacenter(..). This will lead to the task failing. Hadoop re-runs the task then on another node and eventually the job will complete. But the fall back to replica never is used.&lt;/p&gt;

&lt;p&gt;If the initialAddress is hardcoded to one node then we no longer have a decentralised job.&lt;/p&gt;

&lt;p&gt;I would like to allow a comma-separated in initialAddress, for example it could be &quot;localhost, node01, node02, node03&quot;. This would give preference to localhost and avoid any centralisation.&lt;/p&gt;

&lt;p&gt;I would also like to make ColumnFamilyRecordReader.getLocations() return an iterator instead of an array.&lt;br/&gt;
The createConnection(..) and client.describe_datacenter(..) calls are an unnecessary overhead when all nodes (or first endpoint location) are up, and could be avoided by lazy-loading the list.&lt;/p&gt;</comment>
                            <comment id="13047871" author="michaelsembwever" created="Sat, 11 Jun 2011 09:15:23 +0000"  >&lt;p&gt;New patch. I think i&apos;m at last happy with it.&lt;/p&gt;

&lt;p&gt;getLocations() returns an iterator so client.describe_datacenter() is only called when necessary.&lt;/p&gt;

&lt;p&gt;Rather than provide a list in initialAddress it was possible to use either the initialAddress OR the endpoint. This gave the benefit in not listing a location that can&apos;t actually be connected to.&lt;/p&gt;

&lt;p&gt;The &quot;only use replica from same DC&quot; is an option now in ConfigHelper. By default it is true.&lt;/p&gt;

&lt;p&gt;Again the re-generated Cassandra.java is not included in the patch.&lt;/p&gt;

&lt;p&gt;I have tested this on normal jobs, and RF=2 jobs with a node down.&lt;/p&gt;</comment>
                            <comment id="13048597" author="tjake" created="Mon, 13 Jun 2011 15:23:24 +0000"  >&lt;p&gt;The get_rack seems unused so it should be removed.&lt;/p&gt;

&lt;p&gt;Also, it might be better to pass all locations in the get_datacenter thrift call since you can get the results in one shot and sort them by the dynamic snitch, filtering out the dead nodes:&lt;/p&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  DatabaseDescriptor.getEndpointSnitch().sortByProximity(FBUtilities.getLocalAddress(), endpoints); 
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;  FailureDetector.instance.isAlive(endpoint)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
</comment>
                            <comment id="13048622" author="michaelsembwever" created="Mon, 13 Jun 2011 16:15:18 +0000"  >&lt;p&gt;Then (if i understand you correctly) i would need in cassandra.thrift&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;      /** returns alive endpoints, sorted by proximity, that belong in the same datacenter as the given endpoint */
  list&amp;lt;string&amp;gt; get_endpoints_in_same_datacenter(1: string endpoint, 2: required list&amp;lt;string&amp;gt; endpoints)
    throws (1:InvalidRequestException ire)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then the API becomes quite specific to this usecase. Is the performance gain worth it? What&apos;s the cost of each client.describe_datacenter(..) call, and probably more important what is the lost performance of writing to the furthest node that&apos;s within the same datacenter?&lt;/p&gt;</comment>
                            <comment id="13048641" author="michaelsembwever" created="Mon, 13 Jun 2011 16:40:04 +0000"  >&lt;p&gt;Just make sure i understand you T Jake, you would rather something like this in CassandraServer.java?&lt;br/&gt;
(I&apos;ve renamed from the previous comment get_endpoints_in_same_datacenter(..) to sort_endpoints_by_proximity(..))&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC) 
        throws TException, InvalidRequestException
    {
        try
        {
            List&amp;lt;String&amp;gt; results = new ArrayList&amp;lt;String&amp;gt;();
            InetAddress address = InetAddress.getByName(endpoint);
            String datacenter = DatabaseDescriptor.getEndpointSnitch().getDatacenter(address);
            List&amp;lt;InetAddress&amp;gt; addresses = new ArrayList&amp;lt;InetAddress&amp;gt;();
            for(String ep : endpoints)
            {
                addresses.add(InetAddress.getByName(ep));
            }
            DatabaseDescriptor.getEndpointSnitch().sortByProximity(address, addresses);
            for(InetAddress ep : addresses)
            {
                String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(ep);
                if(FailureDetector.instance.isAlive(ep) &amp;amp;&amp;amp; (!restrictToSameDC || datacenter.equals(dc)))
                {
                    results.add(ep.getHostName());
                }
            }
            return results.toArray(new String[results.size()]);
        }
        catch (UnknownHostException e)
        {
            throw new InvalidRequestException(e.getMessage());
        }
    }
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13048654" author="tjake" created="Mon, 13 Jun 2011 16:59:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;what is the lost performance of writing to the furthest node that&apos;s within the same datacenter?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The benefit is really the DynamicSnitch. if a node it slow due to compaction then this would avoid sending requests there... &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC)&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node.  It&apos;s a reasonable assumption that the endpoint it&apos;s talking to is local enough to the client to use that.&lt;/p&gt;
</comment>
                            <comment id="13048671" author="michaelsembwever" created="Mon, 13 Jun 2011 17:52:38 +0000"  >&lt;blockquote&gt;
&lt;blockquote&gt;&lt;p&gt;public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I don&apos;t think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node. It&apos;s a reasonable assumption that the endpoint it&apos;s talking to is local enough to the client to use that.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;For the test set i was running against, RF=2, each split&apos;s has two endpoints always in different datacenters.&lt;/p&gt;

&lt;p&gt;If the &quot;local&quot; endpoint is down then getLocations() will then call client.sort_endpoints_by_proximity(..) and this will fail (being the same endpoint).&lt;br/&gt;
It then makes a client connection through the &quot;other&quot; endpoint. [see CFRR.describeDatacenter(..)].&lt;br/&gt;
This will presume the wrong datacenter and return itself as a valid endpoint. &lt;br/&gt;
I need some way to know what the original datacenter is, even when it is down.&lt;/p&gt;</comment>
                            <comment id="13048691" author="tjake" created="Mon, 13 Jun 2011 18:19:43 +0000"  >&lt;p&gt;ok but why not change the response to map&amp;lt;string,list&amp;lt;string&amp;gt;&amp;gt;  where key is DC and value are proximity sorted endpoints?&lt;/p&gt;</comment>
                            <comment id="13048727" author="michaelsembwever" created="Mon, 13 Jun 2011 19:19:25 +0000"  >&lt;p&gt;Won&apos;t the sorting still be wrong?&lt;br/&gt;
For the use-case above it will solve restricting to the correct datacenter, but the sorting will still be based on proximity to the wrong node?&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I don&apos;t think it makes sense to send the client endpoint to this call since the endpoint might not be a cassandra node. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;It might not be an alive cassandra node, but it should be a cassandra node. It comes from the split&apos;s list of endpoints. At least in this use-case, or are you referring to general usage for this new api?&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;It&apos;s a reasonable assumption that the endpoint it&apos;s talking to is local enough to the client to use that.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I don&apos;t think so... The endpoint that it talks to is a completely random (just the next endpoint listed in the split&apos;s list). This is why i think that such sorting won&apos;t just be wrong but not even close. Does this make sense?&lt;/p&gt;</comment>
                            <comment id="13049169" author="tjake" created="Tue, 14 Jun 2011 13:07:49 +0000"  >&lt;p&gt;I think the core issue is you can&apos;t assume the hadoop node is running on a cassandra node...&lt;/p&gt;

&lt;p&gt;If it is then the logic is straight forward, if not then it&apos;s possible the connection could cross DC boundaries. One possibility is to use the ip octets like the RackInferringSnitch.  &lt;/p&gt;

&lt;p&gt;How&apos;s this proposal then?  keep the sort_endpoints_by_proximity signature as is and pass the client endpoint along with the list of data endpoints and add the following logic:&lt;/p&gt;

&lt;p&gt;1) sort the endpoints using the endpoint_snitch.&lt;br/&gt;
2) if client endpoint &lt;b&gt;is&lt;/b&gt; a valid cassandra node get the nodes DC and prune nodes outside of this DC&lt;br/&gt;
3) if client endpoint &lt;b&gt;is not&lt;/b&gt; a valid cassandra node try to infer the DC from its ip and prune dataendpoint nodes in a different DC. If no cassandra nodes are in the DC list goto 3).&lt;br/&gt;
4) all else fails return the sorted endpoint list&lt;/p&gt;</comment>
                            <comment id="13049724" author="michaelsembwever" created="Wed, 15 Jun 2011 11:23:15 +0000"  >&lt;blockquote&gt;&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;snip&amp;#93;&lt;/span&gt; One possibility is to use the ip octets like the RackInferringSnitch. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;In our usecase we have three nodes defined via PropertyFileSnitch:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;152.90.241.22=DC1:RAC1 #node1
152.90.241.23=DC2:RAC1 #node2
152.90.241.24=DC1:RAC1 #node3&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The only way to infer here is even addresses belong to one dc, odd to the other. This is not how RackInferringSnithc works.&lt;/p&gt;

&lt;p&gt;When we make the connection through the &quot;other&quot; (node2) endpoint taking the rack inferring approach &quot;152.90.&quot; will say it&apos;s in DC2. (again) this is the wrong DC and will return itself as a valid endpoint....&lt;/p&gt;

&lt;p&gt;Step (3) seems to me to be too specific to be included here.&lt;br/&gt;
If i go only with steps (1),(2),and (4) we get this code:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    public String[] sort_endpoints_by_proximity(String endpoint, String[] endpoints, boolean restrictToSameDC) 
            throws TException, InvalidRequestException
    {
        try
        {
            List&amp;lt;String&amp;gt; results = new ArrayList&amp;lt;String&amp;gt;();
            InetAddress address = InetAddress.getByName(endpoint);
            boolean endpointValid = null != Gossiper.instance.getEndpointStateForEndpoint(address);
            String datacenter = DatabaseDescriptor
                    .getEndpointSnitch().getDatacenter(endpointValid ? address : FBUtilities.getLocalAddress());
            List&amp;lt;InetAddress&amp;gt; addresses = new ArrayList&amp;lt;InetAddress&amp;gt;();
            for(String ep : endpoints)
            {
                addresses.add(InetAddress.getByName(endpoint));
            }
            DatabaseDescriptor.getEndpointSnitch().sortByProximity(address, addresses);
            for(InetAddress ep : addresses)
            {
                String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(ep);
                if(FailureDetector.instance.isAlive(ep) &amp;amp;&amp;amp; (!restrictToSameDC || datacenter.equals(dc)))
                {
                    results.add(ep.getHostName());
                }
            }
            return results.toArray(new String[results.size()]);
        }
        catch (UnknownHostException e)
        {
            throw new InvalidRequestException(e.getMessage());
        }
    }&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I&apos;m happy with this (except that &lt;tt&gt;Gossiper.instance.getEndpointStateForEndpoint(address)&lt;/tt&gt; is only my guess on how to tell if an endpoint is valid as such).&lt;/p&gt;</comment>
                            <comment id="13053233" author="michaelsembwever" created="Wed, 22 Jun 2011 13:04:06 +0000"  >&lt;p&gt;Problem with the suggested approach is that sortByProximity(..) &lt;b&gt;only&lt;/b&gt; works when address is the local address. See assert statement DynamicEndpointSnitch:134&lt;/p&gt;

&lt;p&gt;I could hack this and rewrite the line to&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;IEndpointSnitch snitch = DatabaseDescriptor.getEndpointSnitch();
snitch = snitch instanceof DynamicEndpointSnitch ? ((DynamicEndpointSnitch)snitch).subsnitch : snitch;
snitch.sortByProximity(address, addresses);&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;But this of course means that we always bypass DynamicEndpointSnitch&apos;s &quot;scores&quot;.&lt;/p&gt;</comment>
                            <comment id="13053258" author="michaelsembwever" created="Wed, 22 Jun 2011 13:46:37 +0000"  >&lt;p&gt;Up to date patch.&lt;br/&gt;
Follows T Jake&apos;s points (1),(2), and (4).&lt;br/&gt;
And bypasses DynamicEndpointSnitch when sorting by proximity.&lt;/p&gt;</comment>
                            <comment id="13054510" author="tjake" created="Fri, 24 Jun 2011 15:43:18 +0000"  >&lt;p&gt;committed with a change to use the dynamic snitch id the passed endpoint is valid.&lt;/p&gt;</comment>
                            <comment id="13054536" author="hudson" created="Fri, 24 Jun 2011 16:43:50 +0000"  >&lt;p&gt;Integrated in Cassandra-0.8 #191 (See &lt;a href=&quot;https://builds.apache.org/job/Cassandra-0.8/191/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Cassandra-0.8/191/&lt;/a&gt;)&lt;br/&gt;
    Change ColumnFamilyRecordReader to read split from replicas if primary is down&lt;/p&gt;

&lt;p&gt;Patch by Mck SembWever; reviewed by tjake for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-2388&quot; title=&quot;ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-2388&quot;&gt;&lt;del&gt;CASSANDRA-2388&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;jake : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1139358&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1139358&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java&lt;/li&gt;
	&lt;li&gt;/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ConfigHelper.java&lt;/li&gt;
	&lt;li&gt;/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyInputFormat.java&lt;/li&gt;
	&lt;li&gt;/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java&lt;/li&gt;
	&lt;li&gt;/cassandra/branches/cassandra-0.8/interface/cassandra.thrift&lt;/li&gt;
	&lt;li&gt;/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13054780" author="jeromatron" created="Sat, 25 Jun 2011 00:50:01 +0000"  >&lt;p&gt;This patch applies to the current 0.7-branch with minimal problems - just some imports on CassandraServer that it couldn&apos;t resolve properly.  Can this be committed against 0.7-branch for inclusion in 0.7.7?&lt;/p&gt;</comment>
                            <comment id="13054789" author="jeromatron" created="Sat, 25 Jun 2011 01:05:34 +0000"  >&lt;p&gt;I&apos;ve done basic testing with the word count and pig examples to make sure that the basic hadoop integration isn&apos;t negatively affected by this.  I&apos;ll also try it against our dev cluster before and after the patch - killing one node to see if it fails over to another replica - to make sure it does what it should that way.&lt;/p&gt;</comment>
                            <comment id="13054791" author="jeromatron" created="Sat, 25 Jun 2011 01:15:43 +0000"  >&lt;p&gt;Reopening for testing against 0.7.6.&lt;/p&gt;</comment>
                            <comment id="13054803" author="jbellis" created="Sat, 25 Jun 2011 02:44:39 +0000"  >&lt;p&gt;Took a look at this belatedly.  I don&apos;t understand the contortions at all.  It looks like there&apos;s a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes.  Why not just make that work instead?&lt;/p&gt;</comment>
                            <comment id="13054805" author="jbellis" created="Sat, 25 Jun 2011 02:48:46 +0000"  >&lt;p&gt;also: running hadoop on a non-cassandra node is dumb.  i don&apos;t see a point in supporting that really.  (yes, my fault it was written that way to begin with, mea culpa.)&lt;/p&gt;</comment>
                            <comment id="13054806" author="jeromatron" created="Sat, 25 Jun 2011 02:58:15 +0000"  >&lt;p&gt;Jonathan - is it possible to attach an updated patch based on your changes to 0.8 branch?  Not sure if that would be simple to extract.&lt;/p&gt;</comment>
                            <comment id="13054808" author="jbellis" created="Sat, 25 Jun 2011 03:07:16 +0000"  >&lt;p&gt;with svn: svn diff -r 1139323:1139483 and hack out the OutboundTcpConnection change in the middle manually from the output.&lt;/p&gt;

&lt;p&gt;with git: create a branch, rip out the offending OTC change and squash the other two&lt;/p&gt;</comment>
                            <comment id="13054809" author="jbellis" created="Sat, 25 Jun 2011 03:07:59 +0000"  >&lt;p&gt;I think there&apos;s deep surgery to be done here still though.  Backporting is probably premature.&lt;/p&gt;</comment>
                            <comment id="13054810" author="jbellis" created="Sat, 25 Jun 2011 03:12:32 +0000"  >&lt;blockquote&gt;&lt;p&gt;It looks like there&apos;s a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Wait, why do we even care?  &quot;local node&quot; IS the right host to sort against &amp;#8211; we want the split that is closest to the node running the job, this is not the same as some other C* node we contact.&lt;/p&gt;</comment>
                            <comment id="13056524" author="michaelsembwever" created="Tue, 28 Jun 2011 14:13:20 +0000"  >&lt;blockquote&gt;&lt;p&gt;It looks like there&apos;s a ton of effort put in to avoiding making sortByProximity work w/ non-local nodes&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Because it&apos;s only when that local node is down that we actually need to sort...&lt;br/&gt;
When/if DynamicEndpointSnitch&apos;s limitation is fixed (and it can sort by non-local nodes) then CassandraServer.java need not bypass it. But this won&apos;t simplify the code in CFRR. Now that CFIF supports multiple initialAddresses the method CFRR.sortEndpointsByProximity(..) can be rewritten (ie any connection to any initialAddress is all we need, no need to mess around with trying to connect through replica&apos;s to find information about replicas...)&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Wait, why do we even care? &quot;local node&quot; IS the right host to sort against&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Depends on this is CFRR&apos;s &quot;local node&quot; or CassandraServer&apos;s &quot;local node&quot;... &lt;br/&gt;
CFRR&apos;s local node is the right and only node worth sorting against, it being the &quot;task tracker node&quot;. &lt;br/&gt;
But when c* on the &quot;task tracker node&quot; is down, then we randomly connect to another c* node so to find out of the replica we know about which are 1) up, 2) closest, and 3) in the same dc. Then it is a random c* node that becomes the &quot;local node&quot; and the call needs to be &lt;tt&gt;snitch.sortByProximity(initialAddress, addresses)&lt;/tt&gt;.&lt;br/&gt;
But yes... the CFRR code is contorted. In many ways i prefer the simplicity of the first patch (both in api and in implementation) despite it not being &quot;as correct&quot;. i thought of this &quot;fallback to replica&quot; as a last resort to keep the m/r job running, rather than an actively used feature where DynamicEndpointSnitch&apos;s scores will maximise performance. But then i&apos;m only thinking in terms of a small c* cluster and i certainly am naive about what performance gains these scores can give...&lt;/p&gt;</comment>
                            <comment id="13056546" author="jbellis" created="Tue, 28 Jun 2011 14:51:07 +0000"  >&lt;blockquote&gt;&lt;p&gt;CFRR&apos;s local node is the right and only node worth sorting against, it being the &quot;task tracker node&quot;. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Then it is a random c* node that becomes the &quot;local node&quot;&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;We still want to sort by proxmity-to-TT, because CFRR connects directly to the split owner to do the reads.  initialAddress isn&apos;t involved post-split-discovery.&lt;/p&gt;

&lt;p&gt;Again, all the complexity goes away if we just embed the snitch into CFIF/TT.&lt;/p&gt;

&lt;p&gt;One wrinkle: ec2snitch requires gossip, so TT would need a separate local ip to participate in the gossip ring.  We could make that optional (and fall back to old &quot;recognize local data, otherwise you get a random replica&quot; behavior otherwise).&lt;/p&gt;</comment>
                            <comment id="13056556" author="jbellis" created="Tue, 28 Jun 2011 15:01:23 +0000"  >&lt;p&gt;Taking a step back: aren&apos;t we optimizing for (1) a corner case with (2) the wrong solution?&lt;/p&gt;

&lt;p&gt;Here&apos;s what I mean:&lt;/p&gt;

&lt;p&gt;1) CFRR already prioritizes the local replica.  So if you have &amp;gt;= one TT for each replica, this only helps if the local C* node dies, BUT the TT does not.  This doesn&apos;t happen often.&lt;/p&gt;

&lt;p&gt;2) If we ARE in that situation, the &quot;right&quot; solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica.  How can we signal that?  &lt;/p&gt;</comment>
                            <comment id="13056557" author="michaelsembwever" created="Tue, 28 Jun 2011 15:02:16 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-2388&quot; title=&quot;ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-2388&quot;&gt;&lt;del&gt;CASSANDRA-2388&lt;/del&gt;&lt;/a&gt;-addition1.patch: Simplify CFRR now that multiple initialAddresses are supported.&lt;/p&gt;</comment>
                            <comment id="13056787" author="brandon.williams" created="Tue, 28 Jun 2011 21:11:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;If we ARE in that situation, the &quot;right&quot; solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica. How can we signal that?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;ISTM the right thing to do in that situation is just fail and let the JT reschedule somewhere else.&lt;/p&gt;</comment>
                            <comment id="13057002" author="michaelsembwever" created="Wed, 29 Jun 2011 04:58:43 +0000"  >&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;This does happen already (i&apos;ve seen it while testing initial patches that were no good).&lt;br/&gt;
Problem is that the TT is blacklisted, reducing hadoop&apos;s throughput for all jobs running.&lt;br/&gt;
I bet too that a fallback to a replica is faster than a fallback to another TT.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;There is no guarantee that any given TT will have its split accessible via a local c* node - this is only a preference in CFRR. A failed task may just as likely go to a random c* node. At least now we can actually properly limit to the one DC and sort by proximity.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;One thing we&apos;re not doing here is applying this same DC limit and sort by proximity in the case when there isn&apos;t a localhost preference. See CFRR.initialize(..)&lt;br/&gt;
It would make sense to rewrite CFRR.getLocations(..) to
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    private Iterator&amp;lt;String&amp;gt; getLocations(final Configuration conf) throws IOException
    {
        return new SplitEndpointIterator(conf);
    }&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt; and then to move the finding-a-preference-to-localhost code into SplitEndpointIterator...&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A bug i can see in the patch that did get accepted already is in CassandraServer.java:763 when endpointValid is false and restrictToSameDC is true we end up restricting to a random DC. I could fix this so restrictToSameDC is disabled in such situations but this actually invalidates the previous point: we can&apos;t restrict to DC anymore and we can only sortByProximity to a random node... I think this supports Jonathan&apos;s point that it&apos;s overall a poor approach. I&apos;m more and more in preference of my original approach using just client.getDatacenter(..) and not worrying about proximity within the datacenter.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Another bug is that, contray to my patch, the code committed
&lt;blockquote&gt;&lt;p&gt;committed with a change to use the dynamic snitch id the passed endpoint is valid.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt; can call &lt;tt&gt;DynamicEndpointSnitch.sortByProximity(..)&lt;/tt&gt; with an address that is not localhost and this breaks the assertion in the method. &lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13057392" author="brandon.williams" created="Wed, 29 Jun 2011 18:40:40 +0000"  >&lt;blockquote&gt;
&lt;p&gt;This does happen already (i&apos;ve seen it while testing initial patches that were no good).&lt;br/&gt;
Problem is that the TT is blacklisted, reducing hadoop&apos;s throughput for all jobs running.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;If the cassandra node where the TT resides isn&apos;t working, then throughput is reduced regardless.&lt;/p&gt;


&lt;blockquote&gt;&lt;p&gt;I bet too that a fallback to a replica is faster than a fallback to another TT.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I doubt that for any significant job.  Locality is important.  Move the job to the data, not the data to the job.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;There is no guarantee that any given TT will have its split accessible via a local c* node - this is only a preference in CFRR. A failed task may just as likely go to a random c* node. At least now we can actually properly limit to the one DC and sort by proximity.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This sounds like the thing we need to fix, then.  Ensuring that the TT assigned to the map has a local replica.&lt;/p&gt;</comment>
                            <comment id="13057413" author="jbellis" created="Wed, 29 Jun 2011 19:25:02 +0000"  >&lt;blockquote&gt;&lt;p&gt;If the cassandra node where the TT resides isn&apos;t working, then throughput is reduced regardless.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right: we &lt;em&gt;want&lt;/em&gt; it to be blacklisted in that scenario.&lt;/p&gt;</comment>
                            <comment id="13057427" author="jbellis" created="Wed, 29 Jun 2011 19:45:21 +0000"  >&lt;blockquote&gt;&lt;p&gt;This sounds like the thing we need to fix, then. Ensuring that the TT assigned to the map has a local replica.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;reverted 1139358, 1139483 to make a fresh start for this.&lt;/p&gt;

&lt;p&gt;how do we &quot;ensure&quot; this?  isn&apos;t that the JT&apos;s job, to send jobs to the splits we gave it from CFIF?  (which does make sure that only nodes with the data, are included in the split source list.)&lt;/p&gt;</comment>
                            <comment id="13057429" author="michaelsembwever" created="Wed, 29 Jun 2011 19:49:28 +0000"  >&lt;blockquote&gt;&lt;p&gt;If the cassandra node where the TT resides isn&apos;t working, then throughput is reduced regardless.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Right: we want it to be blacklisted in that scenario.&lt;/p&gt;&lt;/blockquote&gt;&lt;/blockquote&gt;
&lt;p&gt;This is making the presumption that the hadoop cluster is only used with CFIF.&lt;br/&gt;
The TT could still be useful for other jobs submitted.&lt;br/&gt;
Furthermore a blacklisted TT does&apos;t automatically come back - it needs to be manually restarted. Isn&apos;t this creating more headache for operations?&lt;/p&gt;</comment>
                            <comment id="13057437" author="tjake" created="Wed, 29 Jun 2011 19:59:09 +0000"  >&lt;p&gt;I dont think we should require the TT to be running locally. The whole idea is to support access to Cassandra data from hadoop even if it&apos;s just an import. &lt;/p&gt;

&lt;p&gt;This patch does spend a lot of time dealing with non local data for that reason. &lt;/p&gt;</comment>
                            <comment id="13057461" author="brandon.williams" created="Wed, 29 Jun 2011 20:47:10 +0000"  >&lt;blockquote&gt;
&lt;p&gt;This is making the presumption that the hadoop cluster is only used with CFIF.&lt;br/&gt;
The TT could still be useful for other jobs submitted.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m fine with that assumption.  If you want to run other jobs, use a different cluster.  Cassandra&apos;s JVM is eating wasteful memory at that point.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Furthermore a blacklisted TT does&apos;t automatically come back - it needs to be manually restarted. Isn&apos;t this creating more headache for operations?&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I don&apos;t think this is actually the case, see &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-4305&quot; title=&quot;repeatedly blacklisted tasktrackers should get declared dead&quot; class=&quot;issue-link&quot; data-issue-key=&quot;HADOOP-4305&quot;&gt;&lt;del&gt;HADOOP-4305&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;I dont think we should require the TT to be running locally. The whole idea is to support access to Cassandra data from hadoop even if it&apos;s just an import.&lt;/p&gt;

&lt;p&gt;This patch does spend a lot of time dealing with non local data for that reason.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I&apos;m fine with dropping support for non-colocated TTs, or at least saying there&apos;s no DC-specific support.  Because frankly, that is a very suboptimal thing to do, transfer the data across the network all the time, and flies in the face of Hadoop&apos;s core principles.&lt;/p&gt;</comment>
                            <comment id="13057467" author="jbellis" created="Wed, 29 Jun 2011 21:00:55 +0000"  >&lt;blockquote&gt;&lt;p&gt;a blacklisted TT does&apos;t automatically come back&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;tlipcon says it comes back after 24h, fwiw.  In any case it&apos;s still the case that we DO want to blacklist it while it&apos;s down.  (Brisk could perhaps add a &quot;clear my tasktracker on restart&quot; operation as a further enhancement.)&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I&apos;m fine with dropping support for non-colocated TTs&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;+1, it was a bad idea and I&apos;m sorry I wrote it. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="13057470" author="michaelsembwever" created="Wed, 29 Jun 2011 21:16:35 +0000"  >&lt;blockquote&gt;&lt;p&gt;tlipcon says it comes back after 24h&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;just to be clear about my concerns. &lt;br/&gt;
this means a dead c* node will bring down a TT. In a hadoop cluster with 3 nodes this means for 24hrs you&apos;re lost 33% throughput. (If less than 10% of hadoop jobs used CFIF i could well imagine some pissed users). (What if you have a temporarily problem with flapping c* nodes and you end up with a handful of blacklisted TTs? etc etc etc).&lt;/p&gt;

&lt;p&gt;All this when using a replica, any replica, could have kept things going smoothly, the only slowdown being some of the data into CFIF had to go over the network instead...&lt;/p&gt;</comment>
                            <comment id="13057568" author="jbellis" created="Thu, 30 Jun 2011 00:32:40 +0000"  >&lt;blockquote&gt;&lt;p&gt;this means a dead c* node will bring down a TT&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Again: &lt;em&gt;this is what you want to happen&lt;/em&gt;.  As long as the C* process on the same node is down, you want the TT to be blacklisted and the jobs to go elsewhere.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;In a hadoop cluster with 3 nodes this means for 24hrs you&apos;re lost 33% throughput&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Right, but the real cause is because the C* process is dead, not b/c the TT is blacklisted.  Making the TT read from other nodes will only hurt your network, not fix the throughput problem, b/c i/o is the bottleneck.&lt;/p&gt;</comment>
                            <comment id="13057659" author="michaelsembwever" created="Thu, 30 Jun 2011 07:50:51 +0000"  >&lt;p&gt;Then i would hope for two separate InputFormats. One optimised for local node connection, where cassandra is deemed the more important system over hadoop, and another where data can be read in from anywhere. I think the latter should be supported in some manner  since users may not always have the possibility to install hadoop and cassandra on the same servers, or they might not think it to be so critical part (eg if CFIF is reading using a IndexClause the input data set might be quite small and the remaining code in the m/r be the bulk of the processing...)&lt;/p&gt;</comment>
                            <comment id="13057810" author="jbellis" created="Thu, 30 Jun 2011 13:17:37 +0000"  >&lt;blockquote&gt;&lt;p&gt;another where data can be read in from anywhere&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This is totally antithetical to how hadoop is designed to work.  I don&apos;t think it&apos;s worth supporting in-tree.&lt;/p&gt;</comment>
                            <comment id="13058095" author="michaelsembwever" created="Thu, 30 Jun 2011 21:43:26 +0000"  >&lt;p&gt;Is &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-2388&quot; title=&quot;ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-2388&quot;&gt;&lt;del&gt;CASSANDRA-2388&lt;/del&gt;&lt;/a&gt;-local-nodes-only-rough-sketch the direction we want then?&lt;/p&gt;

&lt;p&gt;This is very initial code, i can&apos;t get &lt;tt&gt;new JobClient(JobTracker.getAddress(conf), conf).getClusterStatus().getActiveTrackerNames()&lt;/tt&gt; to work, need a little help here.&lt;br/&gt;
(Also CFRR.getLocations() can be drastically reduced).&lt;/p&gt;</comment>
                            <comment id="13058757" author="jbellis" created="Fri, 1 Jul 2011 20:05:13 +0000"  >&lt;p&gt;+1 to CFRR changes&lt;/p&gt;

&lt;p&gt;wasn&apos;t immediately clear to me what CFIF changes are doing, can you elaborate?&lt;/p&gt;</comment>
                            <comment id="13059134" author="michaelsembwever" created="Sat, 2 Jul 2011 22:07:26 +0000"  >&lt;p&gt;The idea is to setup splits to have only endpoints that are valid trackers. But now i see this is just a brainfart &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; Ofc the jobTracker will apply this match for us. And that CFIF was always &apos;restricted&apos; to running on endpoints. Although the documentation on inputSplit.getLocations() is a little thin as to whether this restricts which trackers it should run on or whether is just a preference... I guess it doesn&apos;t matter, as you point out Jonathan all that&apos;s required here is the one line changed in CFRR.&lt;/p&gt;
</comment>
                            <comment id="13059136" author="michaelsembwever" created="Sat, 2 Jul 2011 22:16:38 +0000"  >&lt;p&gt;the new &quot;one-liner&quot; &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-2388&quot; title=&quot;ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-2388&quot;&gt;&lt;del&gt;CASSANDRA-2388&lt;/del&gt;&lt;/a&gt; attached. i&apos;ll &quot;submit patch&quot; once i&apos;ve tested it some...&lt;/p&gt;</comment>
                            <comment id="13059141" author="jbellis" created="Sat, 2 Jul 2011 22:53:36 +0000"  >&lt;p&gt;Sounds good, thanks!&lt;/p&gt;</comment>
                            <comment id="13059334" author="michaelsembwever" created="Mon, 4 Jul 2011 06:39:00 +0000"  >&lt;p&gt;&lt;blockquote&gt;&lt;p&gt;2) If we ARE in that situation, the &quot;right&quot; solution would be to send the job to a TT whose local replica IS live, not to read the data from a nonlocal replica. How can we signal that?&lt;/p&gt;&lt;/blockquote&gt;To /really/ solve this issue can we do the following? &lt;br/&gt;
In CFIF.getRangeMap() take out of each range any endpoints that are not alive. A client connection already exists in this method. This filtering out of dead endpoints wouldn&apos;t be difficult, and would move tasks &lt;b&gt;to&lt;/b&gt; the data making use of replica. This approach does need a new method in cassandra.thrift, eg &lt;tt&gt;list&amp;lt;string&amp;gt; describe_alive_nodes()&lt;/tt&gt;&lt;/p&gt;</comment>
                            <comment id="13085255" author="jbellis" created="Mon, 15 Aug 2011 18:47:28 +0000"  >&lt;p&gt;Does that really fix things though?  Because you could have a data node be reachable from the coordinator answering describe_alive_nodes, but unreachable from the client.  So the client still needs to be able to skip unreachable endpoints itself, so describe_alive seems like gratuitous complexity.&lt;/p&gt;</comment>
                            <comment id="13087915" author="patrik.modesto" created="Fri, 19 Aug 2011 19:39:51 +0000"  >&lt;p&gt;I&apos;d like to point out the situation in which no node for a given range of keys is available. It can happen for example with keyspace set to RF=1 and a node goes down. I created a patch that gives a user a chance to ignore missing range/node and continue runnig the MapReduce job. The patch is here: &lt;a href=&quot;http://pastebin.com/hhrr8m9P&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://pastebin.com/hhrr8m9P&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Jonathan already replied to the ML with &quot;ignoring unavailable ranges is a misfeature, imo&quot;.&lt;/p&gt;

&lt;p&gt;In our case it&apos;s very usefull, although there may be another/smarter solution. We have a keyspace with RF=1 and the nature of our data allows us to ignore temporarily missing node. The current ColumnFamilyInputFormat fails with RuntimeException and AFAIK there is no way around.&lt;/p&gt;</comment>
                            <comment id="13087965" author="brandon.williams" created="Fri, 19 Aug 2011 20:59:36 +0000"  >&lt;blockquote&gt;&lt;p&gt;Does that really fix things though? Because you could have a data node be reachable from the coordinator answering describe_alive_nodes, but unreachable from the client. So the client still needs to be able to skip unreachable endpoints itself, so describe_alive seems like gratuitous complexity.&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I agree, since the view is from the coordinator, describe_alive_nodes isn&apos;t very helpful, and also has to wait on the failure detector to mark the node down anyway.&lt;/p&gt;</comment>
                            <comment id="13088039" author="jbellis" created="Fri, 19 Aug 2011 22:44:52 +0000"  >&lt;blockquote&gt;&lt;p&gt;I agree, since the view is from the coordinator, describe_alive_nodes isn&apos;t very helpful&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Committed Mck&apos;s most recent patch.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;We have a keyspace with RF=1 and the nature of our data allows us to ignore temporarily missing node&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;The &quot;right&quot; fix is to increase RF.  Ignoring missing data is not a scenario we want to support.&lt;/p&gt;</comment>
                            <comment id="13088566" author="hudson" created="Mon, 22 Aug 2011 08:27:05 +0000"  >&lt;p&gt;Integrated in Cassandra-0.7 #539 (See &lt;a href=&quot;https://builds.apache.org/job/Cassandra-0.7/539/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Cassandra-0.7/539/&lt;/a&gt;)&lt;br/&gt;
    fail jobs when Cassandra node has failed but TaskTracker has not&lt;br/&gt;
patch by Mck SembWever; reviewed by jbellis and brandonwilliams for &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-2388&quot; title=&quot;ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-2388&quot;&gt;&lt;del&gt;CASSANDRA-2388&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;jbellis : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1159807&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1159807&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/cassandra/branches/cassandra-0.7/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13094097" author="michaelsembwever" created="Tue, 30 Aug 2011 20:55:15 +0000"  >&lt;p&gt;This approach isn&apos;t really working for me and was committed too quickly i believe.&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;Although the documentation on inputSplit.getLocations() is a little thin as to whether this restricts which trackers it should run on or whether is just a preference&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Tasks are still being evenly distributed around the ring regardless of what the ColumnFamilySplit.locations is.&lt;/p&gt;

&lt;p&gt;The chance of a task actually working is RF/N. Therefore the chances of a blacklisted node are high. Worse is that the whole ring can quickly become blacklisted.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://abel-perez.com/hadoop-task-assignment&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://abel-perez.com/hadoop-task-assignment&lt;/a&gt; has an interesting section in it explaining how the task assignment is supposed to work (and that data locality is preferred but not a requirement). Could ColumnFamilySplit.locations be in the wrong format? (eg they should ip not hostname?).&lt;/p&gt;</comment>
                            <comment id="13094658" author="michaelsembwever" created="Wed, 31 Aug 2011 16:23:23 +0000"  >&lt;p&gt;see last comment. (say if this should be a separate bug...)&lt;/p&gt;

&lt;p&gt;Maybe hadoop&apos;s task allocation isn&apos;t working properly because i&apos;ve an unbalanced ring (i&apos;m working in parallel to fix that).&lt;br/&gt;
If this is the case i think it&apos;s an unfortunate limitation (the ring must be balanced to get any decent hadoop performance).&lt;br/&gt;
It&apos;s also probably likely when using &lt;tt&gt;ConfigHelper.setInputRange(..)&lt;/tt&gt; that the number of nodes involved is small (approaching RF).&lt;br/&gt;
With the default hadoop scheduler your hadoop cluster is occupied while just a few taskTrackers are busy. Of course switching to FairScheduler will help some here.&lt;/p&gt;

&lt;p&gt;I&apos;ll take a look into hadoop&apos;s task allocation code as well...&lt;/p&gt;</comment>
                            <comment id="13100093" author="michaelsembwever" created="Thu, 8 Sep 2011 06:01:39 +0000"  >&lt;p&gt;In the meantime could we make this behavior configurable.&lt;br/&gt;
eg replace CFRR:176 with something like&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;    if(ConfigHelper.isDataLocalityDisabled())
    {
        return split.getLocations()[0];
    }
    else
    {
        throw new UnsupportedOperationException(&quot;no local connection available&quot;);
    }&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="13100281" author="jbellis" created="Thu, 8 Sep 2011 12:55:51 +0000"  >&lt;p&gt;Should we just revert the change for now?&lt;/p&gt;</comment>
                            <comment id="13100717" author="michaelsembwever" created="Thu, 8 Sep 2011 21:36:39 +0000"  >&lt;p&gt;Well that would work for me, was only thinking you want to push a &quot;default behavior&quot; (especially for those using a RP). &lt;br/&gt;
But I think a better understanding (at least from me) of hadoop&apos;s task scheduling is required before enforcing data locality, as as-is it certainly doesn&apos;t work for all.&lt;/p&gt;</comment>
                            <comment id="13104179" author="tjake" created="Wed, 14 Sep 2011 02:25:54 +0000"  >&lt;p&gt;I just want to confirm what this ticket is about.&lt;/p&gt;

&lt;p&gt;The JT has a list of endpoints for a given split.&lt;br/&gt;
When a task runs it may or may not be on one of those nodes &lt;br/&gt;
If other tasks are running on all those replicas the JT may put them on a remote node.&lt;/p&gt;

&lt;p&gt;So we need to decide which endpoint to connect to given the chance that nodes are down.&lt;/p&gt;

&lt;p&gt;1. Check if the node running CFRR is one of the replicas (we have this) this means JT has assigned a data-local task (good)&lt;br/&gt;
2. If none of these nodes are local then pick another.&lt;br/&gt;
3. If connection fails try the one other nodes.&lt;br/&gt;
4. Try to avoid endpoints in a different DC.&lt;/p&gt;

&lt;p&gt;The biggest problem is 4.  Maybe the way todo this is change getSplits logic to never return replicas in another DC.  I think this would require adding DC info to the describe_ring call.  Then we only need to worry about 1-3.&lt;/p&gt;





</comment>
                            <comment id="13104537" author="hudson" created="Wed, 14 Sep 2011 14:49:04 +0000"  >&lt;p&gt;Integrated in Cassandra-0.7 #552 (See &lt;a href=&quot;https://builds.apache.org/job/Cassandra-0.7/552/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://builds.apache.org/job/Cassandra-0.7/552/&lt;/a&gt;)&lt;br/&gt;
    revert &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-2388&quot; title=&quot;ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-2388&quot;&gt;&lt;del&gt;CASSANDRA-2388&lt;/del&gt;&lt;/a&gt; (again)&lt;/p&gt;

&lt;p&gt;jbellis : &lt;a href=&quot;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1170333&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&amp;amp;view=rev&amp;amp;rev=1170333&lt;/a&gt;&lt;br/&gt;
Files : &lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;/cassandra/branches/cassandra-0.7/CHANGES.txt&lt;/li&gt;
	&lt;li&gt;/cassandra/branches/cassandra-0.7/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13225620" author="jbellis" created="Thu, 8 Mar 2012 22:38:35 +0000"  >&lt;p&gt;Marking as minor since the job should get re-submitted, and it&apos;s very difficult to reproduce when the tasktrackers are colocated with cassandra nodes (the recommended configuration).&lt;/p&gt;</comment>
                            <comment id="13487922" author="lannyripple" created="Wed, 31 Oct 2012 16:33:01 +0000"  >&lt;p&gt;Would very much like a fix to this.  We have a 40 node ring running 2x hadoop clusters on 20 nodes each.  One cluster is on systems that are more flaky than the other (bad batch of memory).  When building a split on the first cluster if a ring node is down in the area of the second cluster we get timeouts with no way to blacklist the offending node even though we have replicas local to the first cluster.&lt;/p&gt;

&lt;p&gt;The ring is partitioned into DC1:2, DC2:2 with a hadoop cluster over each DC.&lt;/p&gt;</comment>
                            <comment id="13501857" author="jbellis" created="Wed, 21 Nov 2012 11:06:57 +0000"  >&lt;p&gt;Jake&apos;s plan above seems like a reasonable approach, but let me back up a step.  I&apos;m just not convinced that the problem we&apos;re trying to solve is a real one.  Why do we want to suck a split&apos;s worth of data off-node?  If it&apos;s because you don&apos;t have TackTrackers running on your Cassandra nodes, well, go fix that.&lt;/p&gt;

&lt;p&gt;If it&apos;s because Hadoop has created too many tasks and all the local replicas have their task queue full, won&apos;t assigning it to a non-local TT just cause more contention, than waiting for a local slot to free up?&lt;/p&gt;</comment>
                            <comment id="13502016" author="scottfines" created="Wed, 21 Nov 2012 14:42:53 +0000"  >&lt;p&gt;I have two distinct use-cases where running TaskTrackers alongside Cassandra nodes does not accomplish our goals:&lt;/p&gt;

&lt;p&gt;1. Joining data. We have a large data set in cassandra, true, but we have a &lt;b&gt;much&lt;/b&gt; larger data set held in Hadoop itself (around 4 orders of magnitude larger in hadoop than in cassandra). We need to join the two datasets together, and use the output from that join to feed multiple systems, none of which are cassandra. Since the data in Hadoop is so much larger than that in Cassandra, we have to bring the Cassandra data to hadoop, not the other way around. Because of security concerns, we can&apos;t spread our hadoop data onto our cassandra nodes (even if that didn&apos;t screw with our capacity planning), so we have no other choice but to move the Cassandra data (in small chunks) onto Hadoop. Why not use HBase, you say? We needed Cassandra for its write performance for other problems than this one. &lt;/p&gt;

&lt;p&gt;1. Offline, incremental backups. We have a large volume of time-series data held in Cassandra, and taking nightly snapshots and moving them to our archival center is prohibitively slow-&lt;del&gt;it turns out that moving RF copies of our entire dataset over a leased line every night is a pretty bad idea. Instead, I use MapReduce to take an incremental backup of a much smaller subset of the data, then move that. That way, we not only are not moving the entire data set, but we are also using Cassandra&apos;s consistency mechanisms to resolve all the replicas. The only efficient way I&apos;ve found to do this is via MapReduce (we use the Random Partitioner), and since it&apos;s an offline backup, we need to move it over the network anyway&lt;/del&gt;-may as well use the optimized network connecting Hadoop and Cassandra instead of the tiny pipe connecting cassandra to our archival center. &lt;/p&gt;

&lt;p&gt;Both of these reasons dictate that we &lt;b&gt;not&lt;/b&gt; run a TT alongside our Cassandra nodes, no matter what the &lt;b&gt;recommended&lt;/b&gt; approach is. In this case, we need a strong, fault-tolerant CFIF to serve our purposes.&lt;/p&gt;
</comment>
                            <comment id="13661436" author="michaelsembwever" created="Sat, 18 May 2013 21:50:22 +0000"  >&lt;p&gt;Jonathan,&lt;br/&gt;
 I can&apos;t say i&apos;m in favour of enforcing data locality.&lt;br/&gt;
Because  data locality in hadoop doesn&apos;t work this way&#8230; when a tasktracker through the next heartbeat announces that it has a task slot free the jobtracker will do its best to assign a task with data locality to it but failing this will assign it a random task. the number of these random tasks can be quite high, just like i mentioned above&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt; Tasks are still being evenly distributed around the ring regardless of what the ColumnFamilySplit.locations is. &lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;This can be almost solved by upgrading to hadoop-0.21+, using the fair scheduler and setting the property &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&amp;lt;property&amp;gt;
        &amp;lt;name&amp;gt;mapred.fairscheduler.locality.delay&amp;lt;/name&amp;gt;
        &amp;lt;value&amp;gt;360000000&amp;lt;/value&amp;gt;
&amp;lt;property&amp;gt;&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;.&lt;/p&gt;

&lt;p&gt;At the end of the day while hadoop encourages data locality it does not enforce it.&lt;br/&gt;
The ideal approach would be to sort all locations by proximity.&lt;br/&gt;
The feasible approach hopefully is still &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tjake&quot; class=&quot;user-hover&quot; rel=&quot;tjake&quot;&gt;tjake&lt;/a&gt;&apos;s above. In addition i&apos;d be in favour of a setting in the job&apos;s configuration as to whether a location from another datacenter can be used.&lt;/p&gt;

&lt;p&gt;references:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.infoq.com/articles/HadoopInputFormat&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.infoq.com/articles/HadoopInputFormat&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.mentby.com/matei-zaharia/running-only-node-local-jobs.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.mentby.com/matei-zaharia/running-only-node-local-jobs.html&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://groups.google.com/a/cloudera.org/forum/?fromgroups#!topic/cdh-user/3ggnE5hV0PY&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://groups.google.com/a/cloudera.org/forum/?fromgroups#!topic/cdh-user/3ggnE5hV0PY&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://www.cs.berkeley.edu/~matei/papers/2010/eurosys_delay_scheduling.pdf&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.cs.berkeley.edu/~matei/papers/2010/eurosys_delay_scheduling.pdf&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="13667820" author="jbellis" created="Mon, 27 May 2013 16:02:01 +0000"  >&lt;blockquote&gt;&lt;p&gt;The feasible approach hopefully is still T Jake Luciani&apos;s above&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Okay.  Referring back to Jake&apos;s comments,&lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;The biggest problem is &lt;span class=&quot;error&quot;&gt;&amp;#91;avoiding endpoints in a different DC&amp;#93;&lt;/span&gt;. Maybe the way todo this is change getSplits logic to never return replicas in another DC. I think this would require adding DC info to the describe_ring call&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;I note that we expose node snitch location in system.peers.  So at worst we could &quot;join&quot; against that manually.&lt;/p&gt;</comment>
                            <comment id="13667902" author="michaelsembwever" created="Mon, 27 May 2013 18:41:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;The biggest problem is &lt;span class=&quot;error&quot;&gt;&amp;#91;avoiding endpoints in a different DC&amp;#93;&lt;/span&gt;. Maybe the way todo this is change getSplits logic to never return replicas in another DC. I think this would require adding DC info to the describe_ring call&lt;/p&gt;&lt;/blockquote&gt;

&lt;p&gt;Tasktrackers may have access to a set of datacenters, so this DC info needs contain a list of DCs.&lt;/p&gt;

&lt;p&gt;For example, our setup separates datacenters by physical datacenter and hadoop-usage, like:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;DC1 &quot;Production + Hadoop&quot;
  c*01 c*03
DC2 &quot;Production + Hadoop&quot;
  c*02 c*04
DC3 &quot;Production&quot;
  c*05
DC4 &quot;Production&quot;
  c*06&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;So here we&apos;d pass to getSplits() a DC info like &quot;DC1,DC2&quot;.&lt;br/&gt;
But the problem remain, given a task executing on c*01 that fails to connect to localhost, although we can now prevent a connection to DC3 or DC4, we can&apos;t favour a connection to any other split in DC1 over anything in DC2. Is this solvable? &lt;/p&gt;</comment>
                            <comment id="14025213" author="pauloricardomg" created="Mon, 9 Jun 2014 14:19:48 +0000"  >&lt;p&gt;Attached &lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/12649379/2.0-CASSANDRA-2388.patch&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;2.0-CASSANDRA-2388.patch&lt;/a&gt; (against 2.0), where &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6302&quot; title=&quot;make CqlPagingRecordReader more robust to failures&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6302&quot;&gt;&lt;del&gt;CASSANDRA-6302&lt;/del&gt;&lt;/a&gt; is ported to ColumnFamilyRecordReader, so the next replicas are tried when the first one fails.&lt;/p&gt;

&lt;p&gt;Reading from a non-local-DC replica is not a problem anymore due to the introduction of describe_local_ring (&lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-6268&quot; title=&quot;Poor performance of Hadoop if any DC is using VNodes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-6268&quot;&gt;&lt;del&gt;CASSANDRA-6268&lt;/del&gt;&lt;/a&gt;), that limits the input splits to the local DC.&lt;/p&gt;

&lt;p&gt;I would like to acknowledge my colleague Danilo Penna Queiroz who paired with me on this patch. &lt;/p&gt;</comment>
                            <comment id="14025228" author="jbellis" created="Mon, 9 Jun 2014 14:41:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pkolaczk&quot; class=&quot;user-hover&quot; rel=&quot;pkolaczk&quot;&gt;pkolaczk&lt;/a&gt; to review&lt;/p&gt;</comment>
                            <comment id="14040248" author="pauloricardomg" created="Sun, 22 Jun 2014 21:05:47 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pkolaczk&quot; class=&quot;user-hover&quot; rel=&quot;pkolaczk&quot;&gt;pkolaczk&lt;/a&gt; any update on this? cheers! &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14040489" author="pkolaczk" created="Mon, 23 Jun 2014 07:44:09 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt; I can&apos;t promise, but I try to do that at the end of this week. &lt;/p&gt;</comment>
                            <comment id="14043946" author="pauloricardomg" created="Wed, 25 Jun 2014 19:22:36 +0000"  >&lt;p&gt;Attaching patch based on 1.2.16 and fixed patch (v2) for 2.0.&lt;/p&gt;

&lt;p&gt;Maybe the 1.2 patch can still make it to 1.2.17...&lt;/p&gt;</comment>
                            <comment id="14089646" author="pauloricardomg" created="Thu, 7 Aug 2014 19:19:57 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pkolaczk&quot; class=&quot;user-hover&quot; rel=&quot;pkolaczk&quot;&gt;pkolaczk&lt;/a&gt; any update on this? this review has been roaming for quite some time now...&lt;br/&gt;
sorry for bothering but would be nice to see this integrated. cheers!&lt;/p&gt;</comment>
                            <comment id="14160882" author="pauloricardomg" created="Mon, 6 Oct 2014 20:27:28 +0000"  >&lt;p&gt;&lt;a href=&quot;http://mail-archives.apache.org/mod_mbox/cassandra-dev/201410.mbox/%3CCALdd-zjmvp7JOtguZ_k951RQHDtFt1cthX=RnHQ332C=gAZbjw@mail.gmail.com%3E&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://mail-archives.apache.org/mod_mbox/cassandra-dev/201410.mbox/%3CCALdd-zjmvp7JOtguZ_k951RQHDtFt1cthX=RnHQ332C=gAZbjw@mail.gmail.com%3E&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14166644" author="michaelsembwever" created="Fri, 10 Oct 2014 11:16:23 +0000"  >&lt;p&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pkolaczk&quot; class=&quot;user-hover&quot; rel=&quot;pkolaczk&quot;&gt;pkolaczk&lt;/a&gt; + &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pauloricardomg&quot; class=&quot;user-hover&quot; rel=&quot;pauloricardomg&quot;&gt;pauloricardomg&lt;/a&gt;,  AFAIK everything thrift related is frozen, so i presume the patch isn&apos;t going to be applied to master.&lt;br/&gt;
Otherwise it&apos;s +1 on the patch from me.&lt;/p&gt;</comment>
                            <comment id="14229812" author="pkolaczk" created="Mon, 1 Dec 2014 14:02:24 +0000"  >&lt;p&gt;+1&lt;/p&gt;</comment>
                            <comment id="14709548" author="jbellis" created="Mon, 24 Aug 2015 16:18:46 +0000"  >&lt;p&gt;Paulo, can you rebase to 2.1?&lt;/p&gt;</comment>
                            <comment id="14711259" author="pauloricardomg" created="Tue, 25 Aug 2015 13:30:59 +0000"  >&lt;p&gt;Rebased 2.1 patch available &lt;a href=&quot;https://github.com/pauloricardomg/cassandra/tree/2388-2.1&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;2.1 tests:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.1-testall/lastCompletedBuild/testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.1-dtest/lastCompletedBuild/testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;2.2 tests (don&apos;t know if &lt;tt&gt;ColumnFamilyInputFormat&lt;/tt&gt; and &lt;tt&gt;ColumnFamilyRecordReader&lt;/tt&gt; should be deprecated by then, but the classes are still present there):&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.2-testall/lastCompletedBuild/testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;testall&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;http://cassci.datastax.com/view/Dev/view/paulomotta/job/pauloricardomg-2388-2.2-dtest/lastCompletedBuild/testReport/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;dtest&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15018046" author="slebresne" created="Fri, 20 Nov 2015 13:54:28 +0000"  >&lt;p&gt;Committed, thanks.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12614208">CASSANDRA-4886</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="12474683" name="0002_On_TException_try_next_split.patch" size="2164" author="eldondev" created="Sat, 26 Mar 2011 01:32:04 +0000"/>
                            <attachment id="12652488" name="1.2-CASSANDRA-2388.patch" size="3458" author="pauloricardomg" created="Wed, 25 Jun 2014 19:22:36 +0000"/>
                            <attachment id="12652489" name="2.0-CASSANDRA-2388-v2.patch" size="2831" author="pauloricardomg" created="Wed, 25 Jun 2014 19:22:36 +0000"/>
                            <attachment id="12649379" name="2.0-CASSANDRA-2388.patch" size="2826" author="pauloricardomg" created="Mon, 9 Jun 2014 14:11:17 +0000"/>
                            <attachment id="12484458" name="CASSANDRA-2388-addition1.patch" size="5549" author="mck" created="Tue, 28 Jun 2011 15:02:16 +0000"/>
                            <attachment id="12485146" name="CASSANDRA-2388-extended.patch" size="3954" author="mck" created="Mon, 4 Jul 2011 14:01:48 +0000"/>
                            <attachment id="12485071" name="CASSANDRA-2388.patch" size="938" author="mck" created="Sat, 2 Jul 2011 22:14:48 +0000"/>
                            <attachment id="12483441" name="CASSANDRA-2388.patch" size="15479" author="mck" created="Wed, 22 Jun 2011 13:46:37 +0000"/>
                            <attachment id="12482140" name="CASSANDRA-2388.patch" size="14564" author="mck" created="Sat, 11 Jun 2011 09:15:23 +0000"/>
                            <attachment id="12481890" name="CASSANDRA-2388.patch" size="7690" author="mck" created="Thu, 9 Jun 2011 06:29:04 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>10.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[pauloricardomg]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>20597</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i07hqv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>41642</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_10022" key="com.atlassian.jira.plugin.system.customfieldtypes:userpicker">
                        <customfieldname>Reviewer</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>pkolaczk</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313420" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Reviewers</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[pkolaczk]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12961"><![CDATA[Low]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>