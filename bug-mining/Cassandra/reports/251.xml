<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 22:12:12 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[CASSANDRA-651] cassandra 0.5 version throttles and sometimes kills traffic to a node if you restart it.</title>
                <link>https://issues.apache.org/jira/browse/CASSANDRA-651</link>
                <project id="12310865" key="CASSANDRA">Apache Cassandra</project>
                    <description>&lt;p&gt;From the cassandra user message board: &lt;br/&gt;
&quot;I just recently upgraded to latest in 0.5 branch, and I am running&lt;br/&gt;
into a serious issue. I have a cluster with 4 nodes, rackunaware&lt;br/&gt;
strategy, and using my own tokens distributed evenly over the hash&lt;br/&gt;
space. I am writing/reading equally to them at an equal rate of about&lt;br/&gt;
230 reads/writes per second(and cfstats shows that). The first 3 nodes&lt;br/&gt;
are seeds, the last one isn&apos;t. When I start all the nodes together at&lt;br/&gt;
the same time, they all receive equal amounts of reads/writes (about&lt;br/&gt;
230).&lt;br/&gt;
When I bring node 4 down and bring it back up again, node 4&apos;s load&lt;br/&gt;
fluctuates between the 230 it used to get to sometimes no traffic at&lt;br/&gt;
all. The other 3 still have the same amount of traffic. And no errors&lt;br/&gt;
what so ever seen in logs. &quot; &lt;/p&gt;</description>
                <environment>&lt;p&gt;latest in 0.5 branch&lt;/p&gt;</environment>
        <key id="12444117">CASSANDRA-651</key>
            <summary>cassandra 0.5 version throttles and sometimes kills traffic to a node if you restart it.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="10002" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Normal</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gdusbabek">Gary Dusbabek</assignee>
                                    <reporter username="rrabah">Ramzi Rabah</reporter>
                        <labels>
                    </labels>
                <created>Wed, 23 Dec 2009 17:35:22 +0000</created>
                <updated>Tue, 16 Apr 2019 09:33:30 +0000</updated>
                            <resolved>Wed, 30 Dec 2009 23:20:42 +0000</resolved>
                                        <fixVersion>0.5</fixVersion>
                                        <due></due>
                            <votes>1</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="12794121" author="rrabah" created="Wed, 23 Dec 2009 17:39:27 +0000"  >&lt;p&gt;More info:&lt;br/&gt;
 I do see that Node X.X.X.X is dead, and&lt;br/&gt;
Node X.X.X.X has restarted.&lt;/p&gt;

&lt;p&gt;This show up on all the 3 other servers:&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;Timer-1&amp;#93;&lt;/span&gt; 2009-12-22 20:38:43,738 Gossiper.java (line 194)&lt;br/&gt;
InetAddress /10.6.168.20 is now dead.&lt;/p&gt;

&lt;p&gt;Node /10.6.168.20 has restarted, now UP again&lt;br/&gt;
 INFO &lt;span class=&quot;error&quot;&gt;&amp;#91;GMFD:1&amp;#93;&lt;/span&gt; 2009-12-22 20:43:12,812 StorageService.java (line 475)&lt;br/&gt;
Node /10.6.168.20 state jump to normal&lt;/p&gt;
</comment>
                            <comment id="12794124" author="rrabah" created="Wed, 23 Dec 2009 17:43:09 +0000"  >&lt;p&gt;This is definitely a regression in 0.5. I tested out 0.4.2 and it works perfectly fine, and load goes back up to 100% on the restarted node. &lt;/p&gt;</comment>
                            <comment id="12794945" author="brandon.williams" created="Mon, 28 Dec 2009 22:00:42 +0000"  >&lt;p&gt;I was able to reproduce this in a 4 node setup as well.  The recovered node does not appear to receive any writes after rejoining the cluster, and I receive TimedOutExceptions in the client.   I was able to write directly to the recovered node and things appeared to work, however after a while a different node OOM&apos;d.  I examined the dump in MAT and it shows org.apache.cassandra.net.MessagingService occupies 72.53% of the available heap, followed by java.util.concurrent.LinkedBlockingQueue using 11.87%.&lt;/p&gt;</comment>
                            <comment id="12795010" author="steel_mental" created="Tue, 29 Dec 2009 03:34:49 +0000"  >&lt;p&gt;Confirm this issue by 8 nodes tests,&lt;/p&gt;

&lt;p&gt;After read system.log, I found after one node down and up again, some other nodes will not establish tcp connection to it(on tcp port 7000 ) forever! &lt;br/&gt;
And read request sent to it (into Pending-Writes because socket channel is closed) will not sent to ethernet forever(from observing tcpdump).&lt;/p&gt;

&lt;p&gt;maybe PendingWrites Queue consume lots memory and OOM (yes, it not the recovered node, but  the node who try to send request to recovered node!)&lt;/p&gt;

&lt;p&gt;It&apos;s seems when recovered node going down, some other node&apos;s socket channel was reset , after it come back, these socket channel remain closed, forever&lt;/p&gt;</comment>
                            <comment id="12795062" author="jbellis" created="Tue, 29 Dec 2009 14:26:00 +0000"  >&lt;p&gt;Brandon Williams said: &quot;I see what&apos;s happening with 651 &amp;#8211; TcpConnectionManager keeps trying to reuse a closed connection and never opens new ones to the recovered node&quot;&lt;/p&gt;

&lt;p&gt;sounds like a regression from &lt;a href=&quot;https://issues.apache.org/jira/browse/CASSANDRA-488&quot; title=&quot;TcpConnectionManager only ever has one connection&quot; class=&quot;issue-link&quot; data-issue-key=&quot;CASSANDRA-488&quot;&gt;&lt;del&gt;CASSANDRA-488&lt;/del&gt;&lt;/a&gt; to me.&lt;/p&gt;</comment>
                            <comment id="12795101" author="gdusbabek" created="Tue, 29 Dec 2009 18:45:44 +0000"  >&lt;p&gt;Patched into trunk.  Link the gossip and messaging service so that invalid connection pools can be shutdown when a node goes offline.&lt;/p&gt;</comment>
                            <comment id="12795102" author="gdusbabek" created="Tue, 29 Dec 2009 18:46:20 +0000"  >&lt;p&gt;Patch is for trunk.&lt;/p&gt;</comment>
                            <comment id="12795103" author="jbellis" created="Tue, 29 Dec 2009 18:51:48 +0000"  >&lt;p&gt;relying on FD to notice is not going to work, though, since FD is not instantaneous (and cannot be made so).  &lt;span class=&quot;error&quot;&gt;&amp;#91;edit: that is, a node could die and come back, or be partitioned and be available again, quickly enough that FD does not notice but old connections are still invalid.&amp;#93;&lt;/span&gt; &lt;/p&gt;

&lt;p&gt;can we have it attempt to reconnect when it encounters an error sending instead?&lt;/p&gt;</comment>
                            <comment id="12795104" author="gdusbabek" created="Tue, 29 Dec 2009 18:58:38 +0000"  >&lt;p&gt;The last patch diffed in the wrong direction.  This one is correct.&lt;/p&gt;</comment>
                            <comment id="12795110" author="jbellis" created="Tue, 29 Dec 2009 19:09:02 +0000"  >&lt;p&gt;that said, it could be good to have the FD &lt;b&gt;in addition&lt;/b&gt; to the other, so that if a node goes down for a while that doesn&apos;t have much traffic, we don&apos;t lose the first attempted message once it&apos;s back up unnecessarily.&lt;/p&gt;</comment>
                            <comment id="12795125" author="brandon.williams" created="Tue, 29 Dec 2009 19:54:42 +0000"  >&lt;p&gt;I can still reproduce the issue with this patch applied.  I&apos;m receiving the following traceback:&lt;/p&gt;

&lt;p&gt;ERROR - Fatal exception in thread Thread&lt;span class=&quot;error&quot;&gt;&amp;#91;TCP Selector Manager,5,main&amp;#93;&lt;/span&gt;&lt;br/&gt;
java.lang.AssertionError&lt;br/&gt;
        at org.apache.cassandra.net.TcpConnectionManager.destroy(TcpConnectionManager.java:85)&lt;br/&gt;
        at org.apache.cassandra.net.TcpConnection.errorClose(TcpConnection.java:319)&lt;br/&gt;
        at org.apache.cassandra.net.TcpConnection.connect(TcpConnection.java:364)&lt;br/&gt;
        at org.apache.cassandra.net.SelectorManager.doProcess(SelectorManager.java:143)&lt;br/&gt;
        at org.apache.cassandra.net.SelectorManager.run(SelectorManager.java:107)&lt;/p&gt;

&lt;p&gt;Because ackCon is already null.  With the assert removed, I&apos;m unable to reproduce.&lt;/p&gt;</comment>
                            <comment id="12795184" author="gdusbabek" created="Tue, 29 Dec 2009 23:09:00 +0000"  >&lt;p&gt;Updated to include replacing calls to destroy() with shutdown().  Chances are if one TC is crappy, the other is not going to be useful either.&lt;/p&gt;</comment>
                            <comment id="12795212" author="brandon.williams" created="Wed, 30 Dec 2009 01:02:16 +0000"  >&lt;p&gt;+1, no longer reproducible with this patch.  The recovered node begins receiving writes normally.&lt;/p&gt;</comment>
                            <comment id="12795310" author="gdusbabek" created="Wed, 30 Dec 2009 13:40:49 +0000"  >&lt;p&gt;Jaako and I had a discussion in which we agreed that it would be better to have MessagingService implement IEndPointStateChangeSubscriber and subscribe to the gossiper rather than just implement IFailureDetector.  This would have provided a way to basically turn connection pools on and off and would allow writes to fail a bit faster.&lt;/p&gt;

&lt;p&gt;I implemented that this morning.  It&apos;s a few more lines of code and all it really buys us is more descriptive error messages.  We&apos;ll just have to put up with errored writes until gossip takes the failed node out of the ring.&lt;/p&gt;</comment>
                            <comment id="12795388" author="rrabah" created="Wed, 30 Dec 2009 18:51:52 +0000"  >&lt;p&gt;+1 from me too, this seems to have fixed the problem. &lt;/p&gt;</comment>
                            <comment id="12795390" author="jbellis" created="Wed, 30 Dec 2009 19:01:25 +0000"  >&lt;p&gt;I still think we should not rely on FD, or we will still hit this bug with short-lived partitions (which do occur in the wild).&lt;/p&gt;

&lt;p&gt;something like brandon&apos;s throwing an exception if not connected or awaiting connection.&lt;/p&gt;

&lt;p&gt;I&apos;m still baffled that write() apparently doesn&apos;t throw when the connection dies...  Are we missing something there?&lt;/p&gt;</comment>
                            <comment id="12795403" author="jbellis" created="Wed, 30 Dec 2009 19:27:36 +0000"  >&lt;p&gt;My mistake: write &lt;em&gt;was&lt;/em&gt; throwing, but clearing the old conn out was not working.  Still trying to understand why.&lt;/p&gt;</comment>
                            <comment id="12795405" author="gdusbabek" created="Wed, 30 Dec 2009 19:30:28 +0000"  >&lt;p&gt;MessagingService.sendOneWay inconveniently swallows errors, so StorageProxy is never wise about them.&lt;/p&gt;</comment>
                            <comment id="12795433" author="jbellis" created="Wed, 30 Dec 2009 20:28:47 +0000"  >&lt;p&gt;Got it: the problem is that sendOneWay calls shutdown on SocketException, not errorClose.  so the part that really matters in gary&apos;s fix is adding the nulling out to shutdown.&lt;/p&gt;</comment>
                            <comment id="12795437" author="jbellis" created="Wed, 30 Dec 2009 20:35:53 +0000"  >&lt;p&gt;this version gets rid of the formely-problematic-and-now-redundant SocketException block, and renames TCM.shutdown to reset.&lt;/p&gt;</comment>
                            <comment id="12795466" author="gdusbabek" created="Wed, 30 Dec 2009 22:06:42 +0000"  >&lt;p&gt;Reviewed.  +1 on the v4 patch.&lt;/p&gt;</comment>
                            <comment id="12795487" author="gdusbabek" created="Wed, 30 Dec 2009 23:20:40 +0000"  >&lt;p&gt;Fixed in trunk and 0.5 branch.  Patch by Gary Dusbabek and Jonathan Ellis.  Reviewed by same.&lt;/p&gt;</comment>
                            <comment id="12795593" author="hudson" created="Thu, 31 Dec 2009 12:38:20 +0000"  >&lt;p&gt;Integrated in Cassandra #309 (See &lt;a href=&quot;http://hudson.zones.apache.org/hudson/job/Cassandra/309/&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://hudson.zones.apache.org/hudson/job/Cassandra/309/&lt;/a&gt;)&lt;br/&gt;
      TcpConnectionManager was holding on to disconnected connections, giving the false indication they were being used.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12429078" name="651-v2.patch" size="3246" author="gdusbabek" created="Tue, 29 Dec 2009 18:58:38 +0000"/>
                            <attachment id="12429102" name="651-v3.patch" size="3791" author="gdusbabek" created="Tue, 29 Dec 2009 23:09:00 +0000"/>
                            <attachment id="12429168" name="651-v4.patch" size="4689" author="jbellis" created="Wed, 30 Dec 2009 20:36:20 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12313920" key="com.atlassian.jira.plugin.system.customfieldtypes:multiuserpicker">
                        <customfieldname>Authors</customfieldname>
                        <customfieldvalues>
                                    <customfieldvalue><![CDATA[gdusbabek]]></customfieldvalue>
    
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>19803</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                    <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            15 years, 47 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                            <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i0g08v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>91470</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12313820" key="com.atlassian.jira.plugin.system.customfieldtypes:select">
                        <customfieldname>Severity</customfieldname>
                        <customfieldvalues>
                                <customfieldvalue key="12962"><![CDATA[Normal]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    </customfields>
    </item>
</channel>
</rss>