diff --git a/java/engine/org/apache/derby/iapi/store/access/DiskHashtable.java b/java/engine/org/apache/derby/iapi/store/access/DiskHashtable.java
index d7e40ac59..047a73eeb 100644
--- a/java/engine/org/apache/derby/iapi/store/access/DiskHashtable.java
+++ b/java/engine/org/apache/derby/iapi/store/access/DiskHashtable.java
@@ -18,7 +18,6 @@
    limitations under the License.
 
  */
-
 package org.apache.derby.iapi.store.access;
 
 import java.util.Enumeration;
@@ -29,17 +28,18 @@ import org.apache.derby.iapi.error.StandardException;
 import org.apache.derby.iapi.services.io.FormatableBitSet;
 import org.apache.derby.iapi.types.DataValueDescriptor;
 import org.apache.derby.iapi.types.SQLInteger;
-import org.apache.derby.impl.store.access.heap.HeapRowLocation;
 import org.apache.derby.iapi.types.RowLocation;
 import org.apache.derby.iapi.services.context.ContextService;
 import org.apache.derby.iapi.sql.conn.LanguageConnectionContext;
+import org.apache.derby.iapi.services.sanity.SanityManager;
 
 /**
- * This class is used by BackingStoreHashtable when the BackingStoreHashtable must spill to disk.
- * It implements the methods of a hash table: put, get, remove, elements, however it is not implemented
- * as a hash table. In order to minimize the amount of unique code it is implemented using a Btree and a heap
- * conglomerate. The Btree indexes the hash code of the row key. The actual key may be too long for
- * our Btree implementation.
+ * This class is used by BackingStoreHashtable when the BackingStoreHashtable 
+ * must spill to disk.  It implements the methods of a hash table: put, get, 
+ * remove, elements, however it is not implemented as a hash table. In order to
+ * minimize the amount of unique code it is implemented using a Btree and a 
+ * heap conglomerate. The Btree indexes the hash code of the row key. The 
+ * actual key may be too long for our Btree implementation.
  *
  * Created: Fri Jan 28 13:58:03 2005
  *
@@ -49,77 +49,126 @@ import org.apache.derby.iapi.sql.conn.LanguageConnectionContext;
 
 public class DiskHashtable 
 {
-    private final long rowConglomerateId;
-    private ConglomerateController rowConglomerate;
-    private final long btreeConglomerateId;
-    private ConglomerateController btreeConglomerate;
-    private final DataValueDescriptor[] btreeRow;
-    private final int[] key_column_numbers;
-    private final boolean remove_duplicates;
-    private final TransactionController tc;
-    private final DataValueDescriptor[] row;
-    private final DataValueDescriptor[] scanKey = { new SQLInteger()};
-    private int size;
-    private boolean keepStatistics;
+    private final long                    rowConglomerateId;
+    private       ConglomerateController  rowConglomerate;
+    private final long                    btreeConglomerateId;
+    private       ConglomerateController  btreeConglomerate;
+    private final DataValueDescriptor[]   btreeRow;
+    private final int[]                   key_column_numbers;
+    private final boolean                 remove_duplicates;
+    private final TransactionController   tc;
+    private final DataValueDescriptor[]   row;
+    private final DataValueDescriptor[]   scanKey = { new SQLInteger()};
+    private int                           size;
+    private boolean                       keepStatistics;
 
     /**
      * Creates a new <code>DiskHashtable</code> instance.
      *
      * @param tc
-     * @param template An array of DataValueDescriptors that serves as a template for the rows.
-     * @param key_column_numbers The indexes of the key columns (0 based)
-     * @param remove_duplicates If true then rows with duplicate keys are removed
-     * @param keepAfterCommit If true then the hash table is kept after a commit
+     * @param template              An array of DataValueDescriptors that 
+     *                              serves as a template for the rows.
+     * @param key_column_numbers    The indexes of the key columns (0 based)
+     * @param remove_duplicates     If true then rows with duplicate keys are 
+     *                              removed.
+     * @param keepAfterCommit       If true then the hash table is kept after 
+     *                              a commit
      */
-    public DiskHashtable( TransactionController tc,
-                          DataValueDescriptor[] template,
-                          int[] key_column_numbers,
-                          boolean remove_duplicates,
-                          boolean keepAfterCommit)
+    public DiskHashtable( 
+    TransactionController   tc,
+    DataValueDescriptor[]   template,
+    int[]                   key_column_numbers,
+    boolean                 remove_duplicates,
+    boolean                 keepAfterCommit)
         throws StandardException
     {
-        this.tc = tc;
-        this.key_column_numbers = key_column_numbers;
-        this.remove_duplicates = remove_duplicates;
-        LanguageConnectionContext lcc = (LanguageConnectionContext)
-				ContextService.getContextOrNull(LanguageConnectionContext.CONTEXT_ID);
+        this.tc                         = tc;
+        this.key_column_numbers         = key_column_numbers;
+        this.remove_duplicates          = remove_duplicates;
+        LanguageConnectionContext lcc   = (LanguageConnectionContext)
+            ContextService.getContextOrNull(
+                LanguageConnectionContext.CONTEXT_ID);
+
         keepStatistics = (lcc != null) && lcc.getRunTimeStatisticsMode();
-        row = new DataValueDescriptor[ template.length];
+
+        // Create template row used for creating the conglomerate and 
+        // fetching rows.
+        row = new DataValueDescriptor[template.length];
         for( int i = 0; i < row.length; i++)
+        {
             row[i] = template[i].getNewNull();
-        int tempFlags = keepAfterCommit ? (TransactionController.IS_TEMPORARY | TransactionController.IS_KEPT)
-          : TransactionController.IS_TEMPORARY;
+
+            if (SanityManager.DEBUG)
+            {
+                // must have an object template for all cols in hash overflow.
+                SanityManager.ASSERT(
+                    row[i] != null, 
+                    "Template for the hash table must have non-null object");
+            }
+        }
+
+        int tempFlags = 
+            keepAfterCommit ? 
+            (TransactionController.IS_TEMPORARY | 
+             TransactionController.IS_KEPT) : 
+            TransactionController.IS_TEMPORARY;
         
-        rowConglomerateId = tc.createConglomerate( "heap",
-                                                   template,
-                                                   (ColumnOrdering[]) null,
-                                                   (Properties) null,
-                                                   tempFlags);
-        rowConglomerate = tc.openConglomerate( rowConglomerateId,
-                                               keepAfterCommit,
-                                               TransactionController.OPENMODE_FORUPDATE,
-                                               TransactionController.MODE_TABLE,
-                                               TransactionController.ISOLATION_NOLOCK /* Single thread only */ );
-
-        btreeRow = new DataValueDescriptor[] { new SQLInteger(), rowConglomerate.newRowLocationTemplate()};
+        // create the "base" table of the hash overflow.
+        rowConglomerateId = 
+            tc.createConglomerate( 
+                "heap",
+                template,
+                (ColumnOrdering[]) null,
+                (Properties) null,
+                tempFlags);
+
+        // open the "base" table of the hash overflow.
+        rowConglomerate = 
+            tc.openConglomerate( 
+                rowConglomerateId,
+                keepAfterCommit,
+                TransactionController.OPENMODE_FORUPDATE,
+                TransactionController.MODE_TABLE,
+                TransactionController.ISOLATION_NOLOCK/* Single thread only */);
+
+        // create the index on the "hash" base table.  The key of the index
+        // is the hash code of the row key.  The second column is the 
+        // RowLocation of the row in the "base" table of the hash overflow.
+        btreeRow = 
+            new DataValueDescriptor[] 
+                { new SQLInteger(), rowConglomerate.newRowLocationTemplate()};
+
         Properties btreeProps = new Properties();
-        btreeProps.put( "baseConglomerateId", String.valueOf( rowConglomerateId));
-        btreeProps.put( "rowLocationColumn", "1");
-        btreeProps.put( "allowDuplicates", "false"); // Because the row location is part of the key
-        btreeProps.put( "nKeyFields", "2"); // Include the row location column
-        btreeProps.put( "nUniqueColumns", "2"); // Include the row location column
-        btreeProps.put( "maintainParentLinks", "false");
-        btreeConglomerateId = tc.createConglomerate( "BTREE",
-                                                     btreeRow,
-                                                     (ColumnOrdering[]) null,
-                                                     btreeProps,
-                                                     tempFlags);
-
-        btreeConglomerate = tc.openConglomerate( btreeConglomerateId,
-                                                 keepAfterCommit,
-                                                 TransactionController.OPENMODE_FORUPDATE,
-                                                 TransactionController.MODE_TABLE,
-                                                 TransactionController.ISOLATION_NOLOCK /* Single thread only */ );
+
+        btreeProps.put("baseConglomerateId", 
+                String.valueOf(rowConglomerateId));
+        btreeProps.put("rowLocationColumn",  
+                "1");
+        btreeProps.put("allowDuplicates",    
+                "false"); // Because the row location is part of the key
+        btreeProps.put("nKeyFields",         
+                "2"); // Include the row location column
+        btreeProps.put("nUniqueColumns",     
+                "2"); // Include the row location column
+        btreeProps.put("maintainParentLinks", 
+                "false");
+        btreeConglomerateId = 
+            tc.createConglomerate( 
+                "BTREE",
+                btreeRow,
+                (ColumnOrdering[]) null,
+                btreeProps,
+                tempFlags);
+
+        // open the "index" of the hash overflow.
+        btreeConglomerate = 
+            tc.openConglomerate( 
+                btreeConglomerateId,
+                keepAfterCommit,
+                TransactionController.OPENMODE_FORUPDATE,
+                TransactionController.MODE_TABLE,
+                TransactionController.ISOLATION_NOLOCK /*Single thread only*/ );
+
     } // end of constructor
 
     public void close() throws StandardException
@@ -135,49 +184,60 @@ public class DiskHashtable
      *
      * @param row The row to be inserted.
      *
-     * @return true if the row was added,
-     *         false if it was not added (because it was a duplicate and we are eliminating duplicates).
+     * @return true  if the row was added,
+     *         false if it was not added (because it was a duplicate and we 
+     *               are eliminating duplicates).
      *
      * @exception StandardException standard error policy
      */
-    public boolean put( Object key, Object[] row)
+    public boolean put(Object key, Object[] row)
         throws StandardException
     {
         boolean isDuplicate = false;
-        if( remove_duplicates || keepStatistics)
+        if (remove_duplicates || keepStatistics)
         {
             // Go to the work of finding out whether it is a duplicate
-            isDuplicate = (getRemove( key, false, true) != null);
-            if( remove_duplicates && isDuplicate)
+            isDuplicate = (getRemove(key, false, true) != null);
+            if (remove_duplicates && isDuplicate)
                 return false;
         }
-        rowConglomerate.insertAndFetchLocation( (DataValueDescriptor[]) row, (RowLocation) btreeRow[1]);
+
+        // insert the row into the "base" conglomerate.
+        rowConglomerate.insertAndFetchLocation( 
+            (DataValueDescriptor[]) row, (RowLocation) btreeRow[1]);
+
+        // create index row from hashcode and rowlocation just inserted, and
+        // insert index row into index.
         btreeRow[0].setValue( key.hashCode());
         btreeConglomerate.insert( btreeRow);
-        if( keepStatistics && !isDuplicate)
+
+        if (keepStatistics && !isDuplicate)
             size++;
+
         return true;
+
     } // end of put
 
     /**
      * Get a row from the overflow structure.
      *
-     * @param key If the rows only have one key column then the key value. If there is more than one
-     *            key column then a KeyHasher
+     * @param key If the rows only have one key column then the key value. 
+     *            If there is more than one key column then a KeyHasher
      *
      * @return null if there is no corresponding row,
-     *         the row (DataValueDescriptor[]) if there is exactly one row with the key
+     *         the row (DataValueDescriptor[]) if there is exactly one row 
+     *         with the key, or
      *         a Vector of all the rows with the key if there is more than one.
      *
      * @exception StandardException
      */
-    public Object get( Object key)
+    public Object get(Object key)
         throws StandardException
     {
-        return getRemove( key, false, false);
+        return getRemove(key, false, false);
     }
 
-    private Object getRemove( Object key, boolean remove, boolean existenceOnly)
+    private Object getRemove(Object key, boolean remove, boolean existenceOnly)
         throws StandardException
     {
         int hashCode = key.hashCode();
@@ -185,37 +245,49 @@ public class DiskHashtable
         Object retValue = null;
 
         scanKey[0].setValue( hashCode);
-        ScanController scan = tc.openScan( btreeConglomerateId,
-                                           false, // do not hold
-                                           remove ? TransactionController.OPENMODE_FORUPDATE : 0,
-                                           TransactionController.MODE_TABLE,
-                                           TransactionController.ISOLATION_READ_UNCOMMITTED,
-                                           null, // Scan all the columns
-                                           scanKey,
-                                           ScanController.GE,
-                                           (Qualifier[][]) null,
-                                           scanKey,
-                                           ScanController.GT);
+        ScanController scan = 
+            tc.openScan( 
+                btreeConglomerateId,
+                false, // do not hold
+                remove ? TransactionController.OPENMODE_FORUPDATE : 0,
+                TransactionController.MODE_TABLE,
+                TransactionController.ISOLATION_READ_UNCOMMITTED,
+                null, // Scan all the columns
+                scanKey,
+                ScanController.GE,
+                (Qualifier[][]) null,
+                scanKey,
+                ScanController.GT);
         try
         {
-            while( scan.fetchNext( btreeRow))
+            while (scan.fetchNext(btreeRow))
             {
-                if( rowConglomerate.fetch( (RowLocation) btreeRow[1], row, (FormatableBitSet) null /* all columns */)
+                if (rowConglomerate.fetch(
+                        (RowLocation) btreeRow[1], 
+                        row, 
+                        (FormatableBitSet) null /* all columns */)
                     && rowMatches( row, key))
                 {
                     if( existenceOnly)
                         return this;
 
                     rowCount++;
-                    if( rowCount == 1) 
+                    if( rowCount == 1)
                     {
-                        retValue = BackingStoreHashtable.shallowCloneRow( row);                        
-                    } 
+                        // if there is only one matching row just return row. 
+                        retValue = BackingStoreHashtable.shallowCloneRow( row);
+                    }
                     else 
                     {
+                        // if there is more than one row, return a vector of
+                        // the rows.
+                        //
                         Vector v;
                         if( rowCount == 2)
                         {
+                            // convert the "single" row retrieved from the
+                            // first trip in the loop, to a vector with the
+                            // first two rows.
                             v = new Vector( 2);
                             v.add( retValue);
                             retValue = v;
@@ -246,8 +318,9 @@ public class DiskHashtable
     } // end of getRemove
 
 
-    private boolean rowMatches( DataValueDescriptor[] row,
-                                Object key)
+    private boolean rowMatches( 
+    DataValueDescriptor[] row,
+    Object                key)
     {
         if( key_column_numbers.length == 1)
             return row[ key_column_numbers[0]].equals( key);
diff --git a/java/testing/org/apache/derbyTesting/functionTests/master/st_derby1939.out b/java/testing/org/apache/derbyTesting/functionTests/master/st_derby1939.out
new file mode 100644
index 000000000..84b127d37
--- /dev/null
+++ b/java/testing/org/apache/derbyTesting/functionTests/master/st_derby1939.out
@@ -0,0 +1,4 @@
+Creating tables and index...
+Doing inserts...
+Done preparing, about to execute...
+-=-> Ran without error, retrieved first 10 rows.
diff --git a/java/testing/org/apache/derbyTesting/functionTests/suites/storetests.runall b/java/testing/org/apache/derbyTesting/functionTests/suites/storetests.runall
index 024cf70f5..1b4935755 100644
--- a/java/testing/org/apache/derbyTesting/functionTests/suites/storetests.runall
+++ b/java/testing/org/apache/derbyTesting/functionTests/suites/storetests.runall
@@ -1,4 +1,5 @@
 storetests/st_schema.sql
+storetests/st_derby1939.java
 storetests/st_derby1189.sql
 storetests/st_reclaim_longcol.java
 storetests/st_derby715.java
diff --git a/java/testing/org/apache/derbyTesting/functionTests/tests/storetests/st_derby1939.java b/java/testing/org/apache/derbyTesting/functionTests/tests/storetests/st_derby1939.java
new file mode 100644
index 000000000..adb64a328
--- /dev/null
+++ b/java/testing/org/apache/derbyTesting/functionTests/tests/storetests/st_derby1939.java
@@ -0,0 +1,201 @@
+/*
+
+   Derby - Class org.apache.derbyTesting.functionTests.harness.procedure
+
+   Copyright 2006 The Apache Software Foundation or its licensors, as applicable.
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+      http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+ */
+
+package org.apache.derbyTesting.functionTests.tests.storetests;
+
+import org.apache.derby.tools.ij;
+
+import java.sql.*;
+
+/**
+ * Repro for DERBY-1939.  In effect what we have to do is execute
+ * a query (using a PreparedStatement) for which the optimizer
+ * will choose to do a Hash Join using an IndexToBaseRow result
+ * result.  But that's not enough--at execution time, we then
+ * have to force a situation where the Hash Table "spills" to
+ * disk, and only then will the error occur.
+ *
+ * In order to get the optimizer to choose the necessary plan
+ * we have a moderately complex query that has a predicate
+ * which can be pushed to table T1.  T1 in turn has an index
+ * declared on the appropriate column.  The optimizer will
+ * then choose to do a Hash Join between T2 and T1 and
+ * will use the index on T1, as desired.
+ *
+ * Then, in order to force the "spill" to disk, we use the
+ * Derby property "maxMemoryPerTable" and set it to a
+ * "magic" value that a) is large enough to allow the optimizer
+ * to choose a Hash Join, but b) is small enough to cause
+ * hash-table-spill-over at execution time.  It took a while
+ * find out what value this property should have given the
+ * data in the tables, but having found it we can now reliably
+ * reproduce the failure.
+ */
+public class st_derby1939 {
+
+	// We have a VARCHAR column in the table to help with the
+	// hash table "spill-over".
+	private final int VC_SIZE = 1024;
+	private char[] cArr = new char[VC_SIZE];
+
+	public static void main(String [] args)
+	{
+
+		try {
+            System.setProperty("derby.language.maxMemoryPerTable", "140");
+            System.setProperty("derby.optimizer.noTimeout", "true");
+
+            ij.getPropertyArg(args);
+            Connection conn = ij.startJBMS();
+
+            st_derby1939 test = new st_derby1939();
+            test.doLoad(conn);
+            test.doQuery(conn);
+            conn.close();
+		} catch (Throwable t) {
+			System.out.println("OOPS, unexpected error:");
+			t.printStackTrace();
+		}
+	}
+
+	private void doLoad(Connection conn) throws Exception
+	{
+		conn.setAutoCommit(false);
+		Statement st = conn.createStatement();
+		try {
+			st.execute("drop table d1939_t1");
+		} catch (SQLException se) {}
+		try {
+			st.execute("drop table d1939_t2");
+		} catch (SQLException se) {}
+
+		System.out.println("Creating tables and index...");
+		st.execute("create table d1939_t1 (i smallint, vc varchar(" + VC_SIZE + "))");
+		st.execute("create table d1939_t2 (j smallint, val double, vc varchar(" + VC_SIZE + "))");
+		st.execute("create index ix_d1939_t1 on d1939_t1 (i)");
+
+		PreparedStatement pSt = conn.prepareStatement(
+			"insert into d1939_t1(i, vc) values (?, ?)");
+
+		PreparedStatement pSt2 = conn.prepareStatement(
+			"insert into d1939_t2 values (?, ?, ?)");
+
+		String str = null;
+		System.out.println("Doing inserts...");
+	
+		// Number of rows and columns here is pretty much just "magic";
+		// changing any of them can make it so that the problem doesn't
+		// reproduce...
+		for (int i = 0; i < 69; i++)
+		{
+			/* In order for the repro to work, the data in the tables
+			 * has to be sequential w.r.t the smallint column.  I.e.
+			 * instead of inserting "1, 2, 3, 1, 2, 3, ..." we have to
+			 * insert "1, 1, 1, 2, 2, 2, ...".  So that's what the
+			 * "i % 10" achieves in this code.
+			 */
+			for (int j = 0; j < 10; j++)
+			{
+				str = buildString(i + ":" + j);
+				pSt.setInt(1, i % 10);
+				pSt.setString(2, str);
+				pSt.execute();
+				pSt2.setInt(1, i % 10);
+				pSt2.setDouble(2, j*2.0d);
+				if (j % 2 == 1)
+					pSt2.setString(3, "shorty-string");
+				else
+					pSt2.setString(3, str);
+				pSt2.execute();
+			}
+
+			// Add some extra rows T2, just because.
+			pSt2.setInt(1, i);
+			pSt2.setDouble(2, i*2.0d);
+			pSt2.setNull(3, Types.VARCHAR);
+			pSt2.execute();
+		}
+
+		pSt2.setNull(1, Types.INTEGER);
+		pSt2.setDouble(2, 48.0d);
+		pSt.close();
+		conn.commit();
+	}
+
+	private void doQuery(Connection conn) throws Exception
+	{
+		/* Set Derby properties to allow the optimizer to find the
+		 * best plan (Hash Join with Index) and also to set a max
+		 * memory size on the hash table, which makes it possible
+		 * to "spill" to disk.
+		 */
+
+
+		conn.setAutoCommit(false);
+		PreparedStatement pSt = conn.prepareStatement(
+			"select * from d1939_t2 " +
+			"  left outer join " +
+			"    (select distinct d1939_t1.i, d1939_t2.j, d1939_t1.vc from d1939_t2 " + 
+			"      left outer join d1939_t1 " +
+			"        on d1939_t2.j = d1939_t1.i " +
+			"        and d1939_t1.i = ? " + 
+			"    ) x1 " + 
+			"  on d1939_t2.j = x1.i");
+
+		System.out.println("Done preparing, about to execute...");
+		pSt.setShort(1, (short)8);
+		int count = 0;
+		try {
+
+			// Will fail on next line without fix for DERBY-1939.
+			ResultSet rs = pSt.executeQuery();
+
+			// To iterate through the rows actually takes quite a long time,
+			// so just get the first 10 rows as a sanity check.
+			for (count = 0; rs.next() && count < 10; count++);
+			rs.close();
+			System.out.println("-=-> Ran without error, retrieved first "
+				 + count + " rows.");
+
+		} catch (SQLException se) {
+
+			if (se.getSQLState().equals("XSDA7"))
+			{
+				System.out.println("-=-> Reproduced DERBY-1939:\n" +
+					" -- " + se.getMessage());
+			}
+			else
+				throw se;
+
+		}
+
+		pSt.close();
+		conn.rollback();
+	}
+
+	private String buildString(String s) {
+
+		char [] sArr = new char [] { s.charAt(0), s.charAt(1), s.charAt(2) };
+		for (int i = 0; i < cArr.length; i++)
+			cArr[i] = sArr[i % 3];
+
+		return new String(cArr);
+	}
+}
