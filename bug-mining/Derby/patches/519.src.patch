diff --git a/java/engine/org/apache/derby/iapi/store/access/BackingStoreHashtable.java b/java/engine/org/apache/derby/iapi/store/access/BackingStoreHashtable.java
index d806d69ab..de7c5dcdf 100644
--- a/java/engine/org/apache/derby/iapi/store/access/BackingStoreHashtable.java
+++ b/java/engine/org/apache/derby/iapi/store/access/BackingStoreHashtable.java
@@ -279,16 +279,8 @@ public class BackingStoreHashtable
                     double rowUsage = getEstimatedMemUsage(row);
                     hash_table = new Hashtable((int)(max_inmemory_size / rowUsage));
                 }
-
-                if (needsToClone)
-                {
-                    row = cloneRow(row);
-                }
-
-                Object key = 
-                    KeyHasher.buildHashKey(row, key_column_numbers);
-
-                add_row_to_hash_table(hash_table, key, row);
+               
+                add_row_to_hash_table(hash_table, row, needsToClone);
             }
         }
 
@@ -378,24 +370,53 @@ public class BackingStoreHashtable
         return(new_row);
     }
 
+    /**
+     * Return a shallow cloned row
+     *
+     * @return The cloned row row to use.
+     *
+     * @exception  StandardException  Standard exception policy.
+     **/
+    static DataValueDescriptor[] shallowCloneRow(DataValueDescriptor[] old_row)
+        throws StandardException
+    {
+        DataValueDescriptor[] new_row = new DataValueDescriptor[old_row.length];
+        // the only difference between getClone and cloneObject is cloneObject does
+        // not objectify a stream.  We use cloneObject here.  DERBY-802
+        for (int i = 0; i < old_row.length; i++)
+        {
+            if( old_row[i] != null)
+                new_row[i] = (DataValueDescriptor) 
+                    ((CloneableObject) old_row[i]).cloneObject();
+        }
+
+        return(new_row);
+    }
+
     /**
      * Do the work to add one row to the hash table.
      * <p>
      *
      * @param row               Row to add to the hash table.
      * @param hash_table        The java HashTable to load into.
+     * @param needsToClone      If the row needs to be cloned
      *
 	 * @exception  StandardException  Standard exception policy.
      **/
     private void add_row_to_hash_table(
     Hashtable   hash_table,
-    Object      key,
-    Object[]    row)
+    Object[]    row,
+    boolean needsToClone )
 		throws StandardException
     {
-        if( spillToDisk( hash_table, key, row))
+        if( spillToDisk( hash_table, row))
             return;
         
+        if (needsToClone)
+        {
+            row = cloneRow(row);
+        }
+        Object key = KeyHasher.buildHashKey(row, key_column_numbers);
         Object  duplicate_value = null;
 
         if ((duplicate_value = hash_table.put(key, row)) == null)
@@ -451,7 +472,6 @@ public class BackingStoreHashtable
      * Determine whether a new row should be spilled to disk and, if so, do it.
      *
      * @param hash_table The in-memory hash table
-     * @param key The row's key
      * @param row
      *
      * @return true if the row was spilled to disk, false if not
@@ -459,7 +479,6 @@ public class BackingStoreHashtable
      * @exception  StandardException  Standard exception policy.
      */
     private boolean spillToDisk( Hashtable   hash_table,
-                                 Object      key,
                                  Object[]    row)
 		throws StandardException
     {
@@ -472,7 +491,8 @@ public class BackingStoreHashtable
                 if( inmemory_rowcnt < max_inmemory_rowcnt)
                     return false; // Do not spill
             }
-            else if( max_inmemory_size > 0)
+            else if( max_inmemory_size > getEstimatedMemUsage(row))
+                
                 return false;
             // Want to start spilling
             if( ! (row instanceof DataValueDescriptor[]))
@@ -488,7 +508,7 @@ public class BackingStoreHashtable
                                                remove_duplicates,
                                                keepAfterCommit);
         }
-        
+        Object key = KeyHasher.buildHashKey(row, key_column_numbers);
         Object duplicateValue = hash_table.get( key);
         if( duplicateValue != null)
         {
@@ -727,11 +747,6 @@ public class BackingStoreHashtable
 			}
 		}
 
-        if (needsToClone)
-        {
-            row = cloneRow(row);
-        }
-
         Object key = KeyHasher.buildHashKey(row, key_column_numbers);
 
         if ((remove_duplicates) && (get(key) != null))
@@ -740,7 +755,7 @@ public class BackingStoreHashtable
         }
         else
         {
-            add_row_to_hash_table(hash_table, key, row);
+            add_row_to_hash_table(hash_table, row, needsToClone);
             return(true);
         }
     }
diff --git a/java/engine/org/apache/derby/iapi/store/access/DiskHashtable.java b/java/engine/org/apache/derby/iapi/store/access/DiskHashtable.java
index 5ea3950db..c914705ca 100644
--- a/java/engine/org/apache/derby/iapi/store/access/DiskHashtable.java
+++ b/java/engine/org/apache/derby/iapi/store/access/DiskHashtable.java
@@ -206,8 +206,10 @@ public class DiskHashtable
                         return this;
 
                     rowCount++;
-                    if( rowCount == 1)
-                        retValue = BackingStoreHashtable.cloneRow( row);
+                    if( rowCount == 1) 
+                    {
+                        retValue = BackingStoreHashtable.shallowCloneRow( row);                        
+                    } 
                     else 
                     {
                         Vector v;
@@ -218,8 +220,10 @@ public class DiskHashtable
                             retValue = v;
                         }
                         else
+                        {
                             v = (Vector) retValue;
-                        v.add( BackingStoreHashtable.cloneRow( row));
+                        }
+                        v.add( BackingStoreHashtable.shallowCloneRow( row));
                     }
                     if( remove)
                     {
@@ -348,7 +352,7 @@ public class DiskHashtable
             try
             {
                 scan.fetch( row);
-                Object retValue =  BackingStoreHashtable.cloneRow( row);
+                Object retValue =  BackingStoreHashtable.shallowCloneRow( row);
                 hasMore = scan.next();
                 if( ! hasMore)
                 {
diff --git a/java/engine/org/apache/derby/iapi/types/SQLBinary.java b/java/engine/org/apache/derby/iapi/types/SQLBinary.java
index 8ff60c28b..6e963ef3a 100644
--- a/java/engine/org/apache/derby/iapi/types/SQLBinary.java
+++ b/java/engine/org/apache/derby/iapi/types/SQLBinary.java
@@ -109,14 +109,23 @@ abstract class SQLBinary
 
     public int estimateMemoryUsage()
     {
-        int sz = BASE_MEMORY_USAGE;
-        if( null != dataValue)
-            sz += dataValue.length;
-        return sz;
+        if (dataValue == null) {
+            if (streamValueLength>=0) {
+                return BASE_MEMORY_USAGE + streamValueLength;
+            } else {
+                return getMaxMemoryUsage();
+            }
+        } else {
+            return BASE_MEMORY_USAGE + dataValue.length;
+        }
     } // end of estimateMemoryUsage
-
 	  
 	  
+	/**
+	 * Return max memory usage for a SQL Binary
+	 */
+	abstract int getMaxMemoryUsage();
+
 	 /*
 	 * object state
 	 */
@@ -502,7 +511,7 @@ abstract class SQLBinary
 		if (stream == null)
 			return getClone();
 		SQLBinary self = (SQLBinary) getNewNull();
-		self.setStream(stream);
+		self.setValue(stream, streamValueLength);
 		return self;
 	}
 
diff --git a/java/engine/org/apache/derby/iapi/types/SQLBit.java b/java/engine/org/apache/derby/iapi/types/SQLBit.java
index befb598e4..fa7abfe98 100644
--- a/java/engine/org/apache/derby/iapi/types/SQLBit.java
+++ b/java/engine/org/apache/derby/iapi/types/SQLBit.java
@@ -21,6 +21,7 @@
 package org.apache.derby.iapi.types;
 
 import org.apache.derby.iapi.reference.SQLState;
+import org.apache.derby.iapi.reference.Limits;
 
 import org.apache.derby.iapi.services.io.ArrayInputStream;
 
@@ -79,6 +80,14 @@ public class SQLBit
 		return TypeId.BIT_NAME;
 	}
 
+	/**
+	 * Return max memory usage for a SQL Bit
+	 */
+	int getMaxMemoryUsage()
+	{
+		return Limits.DB2_CHAR_MAXWIDTH;
+	}
+
 	/*
 	 * Storable interface, implies Externalizable, TypedFormat
 	 */
diff --git a/java/engine/org/apache/derby/iapi/types/SQLBlob.java b/java/engine/org/apache/derby/iapi/types/SQLBlob.java
index 07bb2e77c..7b253d731 100644
--- a/java/engine/org/apache/derby/iapi/types/SQLBlob.java
+++ b/java/engine/org/apache/derby/iapi/types/SQLBlob.java
@@ -26,6 +26,7 @@ import org.apache.derby.iapi.types.TypeId;
 import org.apache.derby.iapi.types.BitDataValue;
 import org.apache.derby.iapi.types.DataValueDescriptor;
 import org.apache.derby.iapi.reference.SQLState;
+import org.apache.derby.iapi.reference.Limits;
 import org.apache.derby.iapi.error.StandardException;
 
 import org.apache.derby.iapi.types.Orderable;
@@ -81,6 +82,14 @@ public class SQLBlob extends SQLBinary
 			return TypeId.BLOB_NAME;
         }
 
+	/**
+	 * Return max memory usage for a SQL Blob
+	 */
+	int getMaxMemoryUsage()
+	{
+		return Limits.DB2_LOB_MAXWIDTH;
+	}
+
     /**
      * @see DataValueDescriptor#getNewNull
      */
diff --git a/java/engine/org/apache/derby/iapi/types/SQLLongVarbit.java b/java/engine/org/apache/derby/iapi/types/SQLLongVarbit.java
index 48054354e..2d7eea85b 100644
--- a/java/engine/org/apache/derby/iapi/types/SQLLongVarbit.java
+++ b/java/engine/org/apache/derby/iapi/types/SQLLongVarbit.java
@@ -26,6 +26,7 @@ import org.apache.derby.iapi.types.TypeId;
 import org.apache.derby.iapi.types.BitDataValue;
 import org.apache.derby.iapi.types.DataValueDescriptor;
 import org.apache.derby.iapi.reference.SQLState;
+import org.apache.derby.iapi.reference.Limits;
 import org.apache.derby.iapi.error.StandardException;
 
 import org.apache.derby.iapi.types.Orderable;
@@ -58,6 +59,14 @@ public class SQLLongVarbit extends SQLVarbit
 		return TypeId.LONGVARBIT_NAME;
 	}
 
+	/**
+	 * Return max memory usage for a SQL LongVarbit
+	 */
+	int getMaxMemoryUsage()
+	{
+		return Limits.DB2_LONGVARCHAR_MAXWIDTH;
+	}
+
 	/**
 	 * @see DataValueDescriptor#getNewNull
 	 */
diff --git a/java/engine/org/apache/derby/iapi/types/SQLVarbit.java b/java/engine/org/apache/derby/iapi/types/SQLVarbit.java
index 5f844a718..3866ea196 100644
--- a/java/engine/org/apache/derby/iapi/types/SQLVarbit.java
+++ b/java/engine/org/apache/derby/iapi/types/SQLVarbit.java
@@ -26,6 +26,7 @@ import org.apache.derby.iapi.types.TypeId;
 import org.apache.derby.iapi.types.BitDataValue;
 import org.apache.derby.iapi.types.DataValueDescriptor;
 import org.apache.derby.iapi.reference.SQLState;
+import org.apache.derby.iapi.reference.Limits;
 import org.apache.derby.iapi.error.StandardException;
 
 import org.apache.derby.iapi.types.Orderable;
@@ -60,6 +61,14 @@ public class SQLVarbit extends SQLBit
 		return TypeId.VARBIT_NAME;
 	}
 
+	/**
+	 * Return max memory usage for a SQL Varbit
+	 */
+	int getMaxMemoryUsage()
+	{
+		return Limits.DB2_VARCHAR_MAXWIDTH;
+	}
+
 	/**
 	 * @see DataValueDescriptor#getNewNull
 	 */
diff --git a/java/engine/org/apache/derby/impl/sql/execute/ProjectRestrictResultSet.java b/java/engine/org/apache/derby/impl/sql/execute/ProjectRestrictResultSet.java
index 80f4507e9..daf8915ae 100644
--- a/java/engine/org/apache/derby/impl/sql/execute/ProjectRestrictResultSet.java
+++ b/java/engine/org/apache/derby/impl/sql/execute/ProjectRestrictResultSet.java
@@ -532,15 +532,40 @@ public class ProjectRestrictResultSet extends NoPutResultSetImpl
 	public ExecRow doBaseRowProjection(ExecRow sourceRow)
 		throws StandardException
 	{
-		ExecRow result = null;
+		final ExecRow result;
 		if (source instanceof ProjectRestrictResultSet) {
 			ProjectRestrictResultSet prs = (ProjectRestrictResultSet) source;
 			result = prs.doBaseRowProjection(sourceRow);
 		} else {
-			result = sourceRow.getClone();
+			result = sourceRow.getNewNullRow();
+			result.setRowArray(sourceRow.getRowArray());
 		}
 		return doProjection(result);
 	}
+
+	/**
+	 * Get projection mapping array. The array consist of indexes which
+	 * maps the column in a row array to another position in the row array.
+	 * If the value is projected out of the row, the value is negative.
+	 * @return projection mapping array.
+	 */
+	public int[] getBaseProjectMapping() 
+	{
+		final int[] result;
+		if (source instanceof ProjectRestrictResultSet) {
+			result = new int[projectMapping.length];
+			final ProjectRestrictResultSet prs = (ProjectRestrictResultSet) source;
+			final int[] sourceMap = prs.getBaseProjectMapping();
+			for (int i=0; i<projectMapping.length; i++) {
+				if (projectMapping[i] > 0) {
+					result[i] = sourceMap[projectMapping[i] - 1];
+				}
+			}
+		} else {
+			result = projectMapping;
+		}
+		return result;
+	} 
 	
 	/**
 	 * Is this ResultSet or it's source result set for update
diff --git a/java/engine/org/apache/derby/impl/sql/execute/ScrollInsensitiveResultSet.java b/java/engine/org/apache/derby/impl/sql/execute/ScrollInsensitiveResultSet.java
index d462fe9b0..574fe3305 100644
--- a/java/engine/org/apache/derby/impl/sql/execute/ScrollInsensitiveResultSet.java
+++ b/java/engine/org/apache/derby/impl/sql/execute/ScrollInsensitiveResultSet.java
@@ -229,9 +229,8 @@ public class ScrollInsensitiveResultSet extends NoPutResultSetImpl
 		 * The 1st column, the position in the
 		 * scan, will be the key column.
 		 */
-		int[] keyCols = new int[1];
-		// keyCols[0] = 0; // not req. arrays initialized to zero
-
+		final int[] keyCols = new int[] { 0 };
+		
 		/* We don't use the optimizer row count for this because it could be
 		 * wildly pessimistic.  We only use Hash tables when the optimizer row count
 		 * is within certain bounds.  We have no alternative for scrolling insensitive 
@@ -992,12 +991,12 @@ public class ScrollInsensitiveResultSet extends NoPutResultSetImpl
 		 * and we do our own cloning since the 1st column
 		 * is not a wrapper.
 		 */
-		DataValueDescriptor[] sourceRowArray = sourceRow.getRowArrayClone();
+		DataValueDescriptor[] sourceRowArray = sourceRow.getRowArray();
 
 		System.arraycopy(sourceRowArray, 0, hashRowArray, extraColumns, 
 				sourceRowArray.length);
 
-		ht.put(false, hashRowArray);
+		ht.put(true, hashRowArray);
 
 		numToHashTable++;
 	}
@@ -1058,6 +1057,31 @@ public class ScrollInsensitiveResultSet extends NoPutResultSetImpl
 
 		return resultRow;
 	}
+	
+	/**
+	 * Get the row data at the specified position 
+	 * from the hash table.
+	 *
+	 * @param position	The specified position.
+	 *
+	 * @return	The row data at that position.
+	 *
+ 	 * @exception StandardException thrown on failure 
+	 */
+	private DataValueDescriptor[] getRowArrayFromHashTable(int position)
+		throws StandardException
+	{
+		positionInHashTable.setValue(position);
+		final DataValueDescriptor[] hashRowArray = (DataValueDescriptor[]) 
+			ht.get(positionInHashTable);
+		
+		// Copy out the Object[] without the position.
+		final DataValueDescriptor[] resultRowArray = new 
+			DataValueDescriptor[hashRowArray.length - extraColumns];
+		System.arraycopy(hashRowArray, extraColumns, resultRowArray, 0, 
+						 resultRowArray.length);
+		return resultRowArray;
+	}
 
 	/**
 	 * Positions the cursor in the last fetched row. This is done before
@@ -1082,10 +1106,13 @@ public class ScrollInsensitiveResultSet extends NoPutResultSetImpl
 	 * in the hash table with the new values for the row.
 	 */
 	public void updateRow(ExecRow row) throws StandardException {
-		ExecRow newRow = row.getClone();
+		ExecRow newRow = row;
+		boolean undoProjection = false;
+		
 		if (source instanceof ProjectRestrictResultSet) {
 			newRow = ((ProjectRestrictResultSet)source).
-					doBaseRowProjection(newRow);
+				doBaseRowProjection(row);
+			undoProjection = true;
 		}
 		positionInHashTable.setValue(currentPosition);
 		DataValueDescriptor[] hashRowArray = (DataValueDescriptor[]) 
@@ -1093,6 +1120,32 @@ public class ScrollInsensitiveResultSet extends NoPutResultSetImpl
 		RowLocation rowLoc = (RowLocation) hashRowArray[POS_ROWLOCATION];
 		ht.remove(new SQLInteger(currentPosition));
 		addRowToHashTable(newRow, currentPosition, rowLoc, true);
+		
+		// Modify row to refer to data in the BackingStoreHashtable.
+		// This allows reading of data which goes over multiple pages
+		// when doing the actual update (LOBs). Putting columns of
+		// type SQLBinary to disk, has destructive effect on the columns,
+		// and they need to be re-read. That is the reason this is needed.
+		if (undoProjection) {
+			
+			final DataValueDescriptor[] newRowData = newRow.getRowArray();
+			
+			// Array of original position in row
+			final int[] origPos =((ProjectRestrictResultSet)source).
+				getBaseProjectMapping(); 
+			
+			// We want the row to contain data backed in BackingStoreHashtable
+			final DataValueDescriptor[] backedData = 
+				getRowArrayFromHashTable(currentPosition);
+			
+			for (int i=0; i<origPos.length; i++) {
+				if (origPos[i]>=0) {
+					row.setColumn(origPos[i], backedData[i]);
+				}
+			}
+		} else {
+			row.setRowArray(getRowArrayFromHashTable(currentPosition));
+		}
 	}
 
 	/**
@@ -1112,7 +1165,7 @@ public class ScrollInsensitiveResultSet extends NoPutResultSetImpl
 			hashRowArray[i].setToNull();
 		}
 
-		ht.put(false, hashRowArray);
+		ht.put(true, hashRowArray);
 	}
 
 	/**
