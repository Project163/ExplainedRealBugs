diff --git a/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_300px[chromium].png b/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_300px[chromium].png
index 68efa8c03..8b0b767b7 100644
Binary files a/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_300px[chromium].png and b/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_300px[chromium].png differ
diff --git a/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_300px[firefox].png b/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_300px[firefox].png
index ecb97d8ab..a26c8de34 100644
Binary files a/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_300px[firefox].png and b/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_300px[firefox].png differ
diff --git a/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_stretch[chromium].png b/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_stretch[chromium].png
index c4f515f28..543f1b2d1 100644
Binary files a/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_stretch[chromium].png and b/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_stretch[chromium].png differ
diff --git a/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_stretch[firefox].png b/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_stretch[firefox].png
index 2e7494e12..cc088fef3 100644
Binary files a/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_stretch[firefox].png and b/e2e_playwright/__snapshots__/linux/st_audio_input_test/st_audio_input-width_stretch[firefox].png differ
diff --git a/e2e_playwright/st_audio_input.py b/e2e_playwright/st_audio_input.py
index d81d21a72..207f82020 100644
--- a/e2e_playwright/st_audio_input.py
+++ b/e2e_playwright/st_audio_input.py
@@ -136,6 +136,33 @@ def track_runs() -> None:
     st.write("Runs:", st.session_state.runs)
 
 
+# Sample Rate Tests
+def render_sample_rate_audio_inputs() -> None:
+    """Tests audio input with different sample rate configurations."""
+    st.header("Sample Rate Tests")
+
+    # Test default sample rate (16000 Hz)
+    audio_default = st.audio_input(
+        "Default Sample Rate (16 kHz)", key="sample_rate_default"
+    )
+    if audio_default:
+        st.write("Default rate recorded")
+
+    # Test explicit 48 kHz
+    audio_48k = st.audio_input(
+        "High Quality (48 kHz)", sample_rate=48000, key="sample_rate_48k"
+    )
+    if audio_48k:
+        st.write("48 kHz recorded")
+
+    # Test browser default (None)
+    audio_browser = st.audio_input(
+        "Browser Default", sample_rate=None, key="sample_rate_browser"
+    )
+    if audio_browser:
+        st.write("Browser default recorded")
+
+
 # Direct function calls to render the app
 st.title("Audio Input Test App")
 
@@ -145,6 +172,7 @@ test_fragment()  # Fragment function call
 render_special_audio_inputs()
 render_callback_audio_input()
 render_remount_test()
+render_sample_rate_audio_inputs()  # Sample rate tests
 track_runs()
 
 # Audio input with stretch width
diff --git a/e2e_playwright/st_audio_input_test.py b/e2e_playwright/st_audio_input_test.py
index 515b8b8c5..f6f93ea32 100644
--- a/e2e_playwright/st_audio_input_test.py
+++ b/e2e_playwright/st_audio_input_test.py
@@ -14,6 +14,10 @@
 
 from __future__ import annotations
 
+import os
+import tempfile
+import wave
+
 import pytest
 from playwright.sync_api import FrameLocator, Locator, Page, Route, expect
 
@@ -47,7 +51,17 @@ def ensure_waveform_is_not_rendered(audio_input: Locator):
 
 def ensure_waveform_rendered(audio_input: Locator):
     # Check for the waveform and time code
-    expect(audio_input.get_by_test_id("stAudioInputWaveSurfer")).to_be_visible()
+    wavesurfer = audio_input.get_by_test_id("stAudioInputWaveSurfer")
+    expect(wavesurfer).to_be_visible()
+
+    # Check that WaveSurfer has actually rendered content with a canvas
+    # A properly rendered waveform must have at least one canvas element
+    # WaveSurfer may create multiple canvases for waveform and progress
+    canvas = wavesurfer.locator("canvas")
+    expect(canvas.first).to_be_visible()
+    # Ensure there's at least one canvas (not zero which would indicate no rendering)
+    canvas_count = canvas.count()
+    assert canvas_count > 0, "No canvas elements found - waveform not rendered"
 
     time_code = audio_input.get_by_test_id("stAudioInputWaveformTimeCode")
     expect(time_code).to_be_visible()
@@ -61,7 +75,7 @@ def ensure_waveform_rendered(audio_input: Locator):
 def test_audio_input_renders(app: Page):
     """Test that the audio input component is rendered the correct number of times."""
     audio_input_elements = app.get_by_test_id("stAudioInput")
-    count = 9  # Expected number of audio input elements
+    count = 12  # Expected number of audio input elements
 
     # Verify that the expected number of elements is rendered
     expect(audio_input_elements).to_have_count(count)
@@ -404,11 +418,170 @@ def test_audio_input_error_state(
 
 def test_audio_input_widths(app: Page, assert_snapshot: ImageCompareFunction):
     """Test audio_input with different width configurations."""
-    stretch_width_input = app.get_by_test_id("stAudioInput").nth(7)
-    pixel_width_input = app.get_by_test_id("stAudioInput").nth(8)
+    stretch_width_input = app.get_by_test_id("stAudioInput").nth(10)
+    pixel_width_input = app.get_by_test_id("stAudioInput").nth(11)
 
     expect(stretch_width_input).to_be_visible()
     expect(pixel_width_input).to_be_visible()
 
     assert_snapshot(stretch_width_input, name="st_audio_input-width_stretch")
     assert_snapshot(pixel_width_input, name="st_audio_input-width_300px")
+
+
+@pytest.mark.only_browser("chromium")
+def test_audio_input_sample_rates_recording(app: Page):
+    """Test that audio_input records at different sample rates correctly."""
+    app.context.grant_permissions(["microphone"])
+
+    # Test 48 kHz recording
+    high_quality_input = (
+        app.get_by_test_id("stAudioInput")
+        .filter(has=app.get_by_text("High Quality (48 kHz)"))
+        .first
+    )
+    expect(high_quality_input).to_be_visible()
+
+    # Record audio at 48 kHz
+    high_quality_input.get_by_role("button", name="Record").click()
+    app.wait_for_timeout(2000)  # Record for 2 seconds
+    stop_recording(high_quality_input, app)
+    wait_for_app_run(app)
+
+    # Verify recording was created
+    expect(app.get_by_text("48 kHz recorded")).to_be_visible()
+
+    # Download and verify the sample rate
+    with app.expect_download() as download_info:
+        high_quality_input.get_by_role("button", name="Download as WAV").click()
+
+    download = download_info.value
+    temp_path = tempfile.mktemp(suffix=".wav")
+    download.save_as(temp_path)
+
+    try:
+        with wave.open(temp_path, "rb") as wav_file:
+            sample_rate = wav_file.getframerate()
+            # Verify it's 48 kHz
+            assert sample_rate == 48000, f"Expected 48000Hz, got {sample_rate}Hz"
+    finally:
+        os.unlink(temp_path)
+
+    # Test browser default (should be 44.1 or 48 kHz)
+    browser_default_input = (
+        app.get_by_test_id("stAudioInput")
+        .filter(has=app.get_by_text("Browser Default"))
+        .first
+    )
+
+    browser_default_input.get_by_role("button", name="Record").click()
+    app.wait_for_timeout(2000)
+    stop_recording(browser_default_input, app)
+    wait_for_app_run(app)
+
+    expect(app.get_by_text("Browser default recorded")).to_be_visible()
+
+    with app.expect_download() as download_info:
+        browser_default_input.get_by_role("button", name="Download as WAV").click()
+
+    download = download_info.value
+    temp_path = tempfile.mktemp(suffix=".wav")
+    download.save_as(temp_path)
+
+    try:
+        with wave.open(temp_path, "rb") as wav_file:
+            sample_rate = wav_file.getframerate()
+            # Browser default is typically 44100 or 48000
+            assert sample_rate in [44100, 48000], (
+                f"Expected browser default (44100 or 48000Hz), got {sample_rate}Hz"
+            )
+    finally:
+        os.unlink(temp_path)
+
+
+def test_audio_input_sample_rates_display(app: Page):
+    """Test that audio_input widgets with different sample rates display correctly."""
+    # Navigate to sample rate section
+    sample_rate_header = app.get_by_role("heading", name="Sample Rate Tests")
+    expect(sample_rate_header).to_be_visible()
+
+    # Test default sample rate widget
+    default_input = (
+        app.get_by_test_id("stAudioInput")
+        .filter(has=app.get_by_text("Default Sample Rate (16 kHz)"))
+        .first
+    )
+    expect(default_input).to_be_visible()
+
+    # Test 48 kHz widget
+    high_quality_input = (
+        app.get_by_test_id("stAudioInput")
+        .filter(has=app.get_by_text("High Quality (48 kHz)"))
+        .first
+    )
+    expect(high_quality_input).to_be_visible()
+
+    # Test browser default widget
+    browser_default_input = (
+        app.get_by_test_id("stAudioInput")
+        .filter(has=app.get_by_text("Browser Default"))
+        .first
+    )
+    expect(browser_default_input).to_be_visible()
+
+    # Verify all three widgets are independent
+    expect(app.get_by_test_id("stAudioInput")).to_have_count(
+        12
+    )  # Updated count including new widgets
+
+
+@pytest.mark.skip_browser("webkit")  # Webkit has CI audio permission issues
+def test_audio_input_re_recording(app: Page):
+    """Test that clicking record with an existing recording clears it and starts new recording."""
+    audio_input = app.get_by_test_id("stAudioInput").first
+
+    # Get the record button by aria-label since it's an icon button
+    record_button = audio_input.locator("[aria-label='Record']")
+
+    # Start first recording
+    record_button.click()
+
+    # Wait for recording to start - button changes to stop
+    stop_button = audio_input.get_by_role("button", name="Stop recording")
+    expect(stop_button).to_be_visible(timeout=2000)
+
+    # Record for 1 second
+    app.wait_for_timeout(1000)
+
+    # Stop recording
+    stop_button.click()
+
+    # Wait for recording to process
+    app.wait_for_timeout(3000)
+
+    # After stopping, should have both play and record buttons
+    play_button = audio_input.get_by_role("button", name="Play")
+    expect(play_button).to_be_visible()
+
+    record_button = audio_input.locator("[aria-label='Record']")
+    expect(record_button).to_be_visible()
+
+    # Now test re-recording: click record again with existing recording
+    # This should clear the old recording and start a new one immediately
+    record_button.click()
+
+    # The button should change to "Stop recording" indicating recording started
+    # This is the critical test - it should work with just one click
+    stop_button = audio_input.get_by_role("button", name="Stop recording")
+    expect(stop_button).to_be_visible(timeout=3000)
+
+    # Record for another second
+    app.wait_for_timeout(1000)
+
+    # Stop the second recording
+    stop_button.click()
+
+    # Wait for processing
+    app.wait_for_timeout(3000)
+
+    # Should have play button again
+    expect(play_button).to_be_visible()
diff --git a/frontend/lib/src/components/widgets/AudioInput/AudioInput.tsx b/frontend/lib/src/components/widgets/AudioInput/AudioInput.tsx
index 8d828a305..81c4e6f2e 100644
--- a/frontend/lib/src/components/widgets/AudioInput/AudioInput.tsx
+++ b/frontend/lib/src/components/widgets/AudioInput/AudioInput.tsx
@@ -98,15 +98,6 @@ const AudioInput: React.FC<Props> = ({
     key: "deleteFileUrl",
     defaultValue: null,
   })
-  const [recordPlugin, setRecordPlugin] = useState<RecordPlugin | null>(null)
-  // to eventually show the user the available audio devices
-  // eslint-disable-next-line @typescript-eslint/no-unused-vars
-  const [availableAudioDevices, setAvailableAudioDevices] = useState<
-    MediaDeviceInfo[]
-  >([])
-  const [activeAudioDeviceId, setActiveAudioDeviceId] = useState<
-    string | null
-  >(null)
 
   const [recordingUrl, setRecordingUrl] = useWidgetManagerElementState<
     string | null
@@ -116,10 +107,11 @@ const AudioInput: React.FC<Props> = ({
     key: "recordingUrl",
     defaultValue: null,
   })
+
   const [, setRerender] = useState(0)
-  const forceRerender = (): void => {
+  const forceRerender = useCallback((): void => {
     setRerender(prev => prev + 1)
-  }
+  }, [])
   const [progressTime, setProgressTime] = useState(STARTING_TIME_STRING)
 
   const [recordingTime, setRecordingTime] =
@@ -142,88 +134,143 @@ const AudioInput: React.FC<Props> = ({
   const widgetId = element.id
   const widgetFormId = element.formId
 
-  const transcodeAndUploadFile = useCallback(
-    async (blob: Blob) => {
-      setIsUploading(true)
-      if (notNullOrUndefined(widgetFormId))
-        widgetMgr.setFormsWithUploadsInProgress(new Set([widgetFormId]))
-
-      let wavBlob: Blob | undefined = undefined
+  const targetSampleRate = element.sampleRate || null
 
-      if (blob.type === "audio/wav") {
-        wavBlob = blob
-      } else {
-        wavBlob = await convertAudioToWav(blob)
-      }
+  const recordPluginRef = useRef<RecordPlugin | null>(null)
+  const recordPluginHandlersRef = useRef<{
+    handleRecordProgress?: (time: number) => void
+  }>({})
 
-      if (!wavBlob) {
-        setIsError(true)
-        return
-      }
-
-      const url = URL.createObjectURL(wavBlob)
-      const timestamp = new Date().toISOString().slice(0, 16).replace(":", "-")
-      const file = new File([wavBlob], `${timestamp}_audio.wav`, {
-        type: wavBlob.type,
-      })
+  const transcodeAndUploadFile = useCallback(
+    async (blob: Blob) => {
+      try {
+        setIsUploading(true)
+        if (notNullOrUndefined(widgetFormId))
+          widgetMgr.setFormsWithUploadsInProgress(new Set([widgetFormId]))
+
+        let wavBlob: Blob | undefined = undefined
+
+        if (blob.type === "audio/wav") {
+          wavBlob = blob
+        } else {
+          wavBlob = await convertAudioToWav(
+            blob,
+            targetSampleRate || undefined
+          )
+        }
+
+        if (!wavBlob) {
+          setIsError(true)
+          setIsUploading(false)
+          if (notNullOrUndefined(widgetFormId))
+            widgetMgr.setFormsWithUploadsInProgress(new Set())
+          return
+        }
+
+        let blobUrl: string
+        try {
+          blobUrl = URL.createObjectURL(wavBlob)
+        } catch {
+          setIsError(true)
+          setIsUploading(false)
+          if (notNullOrUndefined(widgetFormId))
+            widgetMgr.setFormsWithUploadsInProgress(new Set())
+          return
+        }
+
+        setRecordingUrl(blobUrl)
+
+        if (wavesurfer) {
+          void wavesurfer.load(blobUrl)
+          wavesurfer.setOptions({
+            interact: true,
+            waveColor: blend(
+              theme.colors.fadedText40,
+              theme.colors.secondaryBg
+            ),
+            progressColor: theme.colors.bodyText,
+          })
+        }
+
+        const timestamp = new Date()
+          .toISOString()
+          .slice(0, 16)
+          .replace(/:/g, "-")
+        const file = new File([wavBlob], `${timestamp}_audio.wav`, {
+          type: wavBlob.type,
+        })
 
-      setRecordingUrl(url)
+        try {
+          const { successfulUploads, failedUploads } = await uploadFiles({
+            files: [file],
+            uploadClient,
+            widgetMgr,
+            widgetInfo: { id: widgetId, formId: widgetFormId },
+            fragmentId,
+          })
 
-      void uploadFiles({
-        files: [file],
-        uploadClient,
-        widgetMgr,
-        widgetInfo: { id: widgetId, formId: widgetFormId },
-        fragmentId,
-      })
-        .then(({ successfulUploads, failedUploads }) => {
           if (failedUploads.length > 0) {
             setIsError(true)
             return
           }
+
+          setIsError(false)
           const upload = successfulUploads[0]
-          if (upload?.fileUrl.deleteUrl) {
+          if (upload?.fileUrl?.deleteUrl) {
             setDeleteFileUrl(upload.fileUrl.deleteUrl)
           }
-        })
-        .finally(() => {
+        } catch {
+          setIsError(true)
+        } finally {
           if (notNullOrUndefined(widgetFormId))
             widgetMgr.setFormsWithUploadsInProgress(new Set())
-
           setIsUploading(false)
-        })
+        }
+      } catch {
+        setIsError(true)
+        setIsUploading(false)
+        if (notNullOrUndefined(widgetFormId))
+          widgetMgr.setFormsWithUploadsInProgress(new Set())
+      }
     },
     [
-      setRecordingUrl,
       uploadClient,
       widgetMgr,
+      wavesurfer,
       widgetId,
       widgetFormId,
       fragmentId,
       setDeleteFileUrl,
+      targetSampleRate,
+      setRecordingUrl,
+      theme.colors.fadedText40,
+      theme.colors.secondaryBg,
+      theme.colors.bodyText,
     ]
   )
 
   const handleClear = useCallback(
-    ({
+    async ({
       updateWidgetManager,
       deleteFile,
     }: {
       updateWidgetManager: boolean
       deleteFile: boolean
-    }) => {
-      if (isNullOrUndefined(wavesurfer) || isNullOrUndefined(deleteFileUrl)) {
+    }): Promise<void> => {
+      if (isNullOrUndefined(wavesurfer)) {
         return
       }
+
+      const urlToRevoke = recordingUrl
+
       setRecordingUrl(null)
-      wavesurfer.empty()
-      if (deleteFile) {
-        // eslint-disable-next-line @typescript-eslint/no-floating-promises
-        uploadClient.deleteFile(deleteFileUrl)
-      }
       setDeleteFileUrl(null)
       setProgressTime(STARTING_TIME_STRING)
       setRecordingTime(STARTING_TIME_STRING)
+      setShouldUpdatePlaybackTime(false)
+
+      wavesurfer.empty()
+
       if (updateWidgetManager) {
         widgetMgr.setFileUploaderStateValue(
           element,
@@ -232,9 +279,17 @@ const AudioInput: React.FC<Props> = ({
           fragmentId
         )
       }
-      setShouldUpdatePlaybackTime(false)
-      if (notNullOrUndefined(recordingUrl)) {
-        URL.revokeObjectURL(recordingUrl)
+
+      if (deleteFile && deleteFileUrl) {
+        try {
+          await uploadClient.deleteFile(deleteFileUrl)
+        } catch {
+          // Silently handle deletion errors
+        }
+      }
+
+      if (notNullOrUndefined(urlToRevoke)) {
+        URL.revokeObjectURL(urlToRevoke)
       }
     },
     [
@@ -246,8 +301,8 @@ const AudioInput: React.FC<Props> = ({
       widgetMgr,
       fragmentId,
       setRecordingTime,
-      setRecordingUrl,
       setDeleteFileUrl,
+      setRecordingUrl,
     ]
   )
 
@@ -255,9 +310,9 @@ const AudioInput: React.FC<Props> = ({
     if (isNullOrUndefined(widgetFormId)) return
 
     const formClearHelper = new FormClearHelper()
-    formClearHelper.manageFormClearListener(widgetMgr, widgetFormId, () =>
-      handleClear({ updateWidgetManager: true, deleteFile: false })
-    )
+    formClearHelper.manageFormClearListener(widgetMgr, widgetFormId, () => {
+      void handleClear({ updateWidgetManager: true, deleteFile: false })
+    })
 
     return () => formClearHelper.disconnect()
   }, [widgetFormId, handleClear, widgetMgr])
@@ -278,7 +333,7 @@ const AudioInput: React.FC<Props> = ({
       barGap: BAR_GAP,
       barRadius: BAR_RADIUS,
       cursorWidth: CURSOR_WIDTH,
-      url: recordingUrl ?? undefined,
+      interact: true,
     })
 
     ws.on("timeupdate", time => {
@@ -289,37 +344,79 @@ const AudioInput: React.FC<Props> = ({
       forceRerender()
     })
 
-    const rp = ws.registerPlugin(
-      RecordPlugin.create({
-        scrollingWaveform: false,
-        renderRecordedAudio: true,
+    setWavesurfer(ws)
+
+    if (recordingUrl) {
+      void ws.load(recordingUrl)
+      ws.setOptions({
+        interact: true,
       })
-    )
+    }
 
-    rp.on("record-end", blob => {
-      // eslint-disable-next-line @typescript-eslint/no-floating-promises
-      transcodeAndUploadFile(blob)
-    })
+    const recordOptions: Record<string, unknown> = {
+      renderRecordedAudio: false,
+      scrollingWaveform: false,
+      mimeType: "audio/webm",
+    }
 
-    rp.on("record-progress", time => {
-      setRecordingTime(formatTime(time))
-    })
+    try {
+      const record = ws.registerPlugin(RecordPlugin.create(recordOptions))
+      recordPluginRef.current = record
 
-    setWavesurfer(ws)
-    setRecordPlugin(rp)
+      const handleRecordProgress = (time: number): void => {
+        setRecordingTime(formatTime(time))
+      }
+
+      record.on("record-progress", handleRecordProgress)
+
+      recordPluginHandlersRef.current = {
+        handleRecordProgress,
+      }
+    } catch (err) {
+      if (
+        err instanceof Error &&
+        (err.name === "NotAllowedError" ||
+          err.name === "PermissionDeniedError")
+      ) {
+        setHasNoMicPermissions(true)
+      }
+    }
 
     return () => {
+      if (recordPluginRef.current) {
+        if (recordPluginRef.current.isRecording()) {
+          recordPluginRef.current.stopRecording()
+        }
+        const handlers = recordPluginHandlersRef.current
+        if (handlers.handleRecordProgress) {
+          recordPluginRef.current.un(
+            "record-progress",
+            handlers.handleRecordProgress
+          )
+        }
+        recordPluginRef.current.destroy()
+        recordPluginRef.current = null
+        recordPluginHandlersRef.current = {}
+      }
       if (ws) ws.destroy()
-      if (rp) rp.destroy()
     }
-    // note: intentionally excluding theme so that we don't have to recreate the wavesurfer instance
-    // and colors will be updated separately
-    // TODO: Update to match React best practices
-    // eslint-disable-next-line react-hooks/react-compiler
-    // eslint-disable-next-line react-hooks/exhaustive-deps
-  }, [transcodeAndUploadFile])
+  }, [
+    theme,
+    setProgressTime,
+    forceRerender,
+    recordingUrl,
+    setRecordingTime,
+    setHasNoMicPermissions,
+  ])
 
-  useEffect(() => initializeWaveSurfer(), [initializeWaveSurfer])
+  useEffect(() => {
+    const cleanup = initializeWaveSurfer()
+    return cleanup
+  }, [initializeWaveSurfer])
+
+  // Note: We don't revoke blob URLs here because they need to persist
+  // across component remounts. They'll be cleaned up when the page unloads
+  // or when the user explicitly clears the recording.
 
   useEffect(() => {
     if (!isEqual(previousTheme, theme)) {
@@ -328,14 +425,20 @@ const AudioInput: React.FC<Props> = ({
           ? blend(theme.colors.fadedText40, theme.colors.secondaryBg)
           : theme.colors.primary,
         progressColor: theme.colors.bodyText,
+        interact: true,
       })
     }
   }, [theme, previousTheme, recordingUrl, wavesurfer])
 
   const onClickPlayPause = useCallback(() => {
-    if (wavesurfer) {
-      // eslint-disable-next-line @typescript-eslint/no-floating-promises
-      wavesurfer.playPause()
+    if (!wavesurfer) return
+
+    const handlePlayPause = async (): Promise<void> => {
+      try {
+        await wavesurfer.playPause()
+      } catch {
+        setIsError(true)
+      }
       // This is because we want the time to be the duration of the audio when they stop recording,
       // but once they start playing it, we want it to be the current time. So, once they start playing it
       // we'll start keeping track of the playback time from that point onwards (until re-recording).
@@ -343,87 +446,157 @@ const AudioInput: React.FC<Props> = ({
       // despite the state change above, this is still needed to force a rerender and make the time styling work
       forceRerender()
     }
-  }, [wavesurfer])
 
-  const startRecording = useCallback(async () => {
-    let audioDeviceId = activeAudioDeviceId
+    void handlePlayPause()
+  }, [wavesurfer, forceRerender])
 
+  const startRecording = useCallback(async () => {
     if (!hasRequestedMicPermissions) {
-      // this first part is to ensure we prompt for getting the user's media devices
-      await navigator.mediaDevices
-        .getUserMedia({ audio: true })
-        .then(() =>
-          RecordPlugin.getAvailableAudioDevices().then(devices => {
-            setAvailableAudioDevices(devices)
-            if (devices.length > 0) {
-              const { deviceId } = devices[0]
-              setActiveAudioDeviceId(deviceId)
-              audioDeviceId = deviceId
-            }
-          })
-        )
-        .catch(_err => {
-          setHasNoMicPermissions(true)
-        })
       setHasRequestedMicPermissions(true)
-    }
 
-    if (!recordPlugin || !audioDeviceId || !wavesurfer) {
-      return
-    }
+      try {
+        const stream = await navigator.mediaDevices.getUserMedia({
+          audio: true,
+        })
 
-    wavesurfer.setOptions({
-      waveColor: theme.colors.primary,
-    })
+        stream.getTracks().forEach(track => track.stop())
+      } catch {
+        setHasNoMicPermissions(true)
+        return
+      }
+    }
 
     if (recordingUrl) {
-      handleClear({ updateWidgetManager: false, deleteFile: true })
+      await handleClear({ updateWidgetManager: false, deleteFile: true })
     }
 
-    // eslint-disable-next-line @typescript-eslint/no-floating-promises
-    recordPlugin.startRecording({ deviceId: audioDeviceId }).then(() => {
-      // Update the record button to show the user that they can stop recording
-      forceRerender()
-    })
+    if (recordPluginRef.current && wavesurfer) {
+      wavesurfer.setOptions({
+        waveColor: theme.colors.primary,
+      })
+
+      const audioConstraints: MediaTrackConstraints = targetSampleRate
+        ? {
+            sampleRate: { ideal: targetSampleRate },
+          }
+        : {} // Default constraints
+
+      try {
+        await recordPluginRef.current.startRecording(audioConstraints)
+        setRecordingTime(formatTime(0))
+        forceRerender()
+      } catch (err) {
+        if (
+          err instanceof Error &&
+          (err.name === "NotAllowedError" ||
+            err.name === "PermissionDeniedError")
+        ) {
+          setHasNoMicPermissions(true)
+        } else {
+          setIsError(true)
+        }
+      }
+    } else if (!hasNoMicPermissions) {
+      setIsError(true)
+    }
   }, [
-    activeAudioDeviceId,
-    recordPlugin,
-    theme,
+    hasRequestedMicPermissions,
+    setRecordingTime,
+    hasNoMicPermissions,
+    targetSampleRate,
     wavesurfer,
+    theme.colors.primary,
+    forceRerender,
     recordingUrl,
     handleClear,
-    hasRequestedMicPermissions,
   ])
 
-  const stopRecording = useCallback(() => {
-    if (!recordPlugin) return
-
-    recordPlugin.stopRecording()
-
-    wavesurfer?.setOptions({
-      // We are blending this color instead of directly using the theme color (fadedText40)
-      // because the "faded" part of fadedText40 means introducing some transparency, which
-      // causes problems with the progress waveform color because wavesurfer is choosing to
-      // tint the waveColor with the progressColor instead of directly setting the progressColor.
-      // This means that the low opacity of fadedText40 causes the progress waveform to
-      // have the same opacity which makes it impossible to darken it enough to match designs.
-      // We fix this by blending the colors to figure out what the resulting color should be at
-      // full opacity, and we usee that color to set the waveColor.
-      waveColor: blend(theme.colors.fadedText40, theme.colors.secondaryBg),
-    })
-  }, [recordPlugin, wavesurfer, theme])
+  const waitForRecordEnd = useCallback(
+    (plugin: RecordPlugin): Promise<Blob> => {
+      return new Promise<Blob>((resolve, reject) => {
+        const handleRecordEnd = (blob: Blob): void => {
+          plugin.un("record-end", handleRecordEnd)
+
+          if (blob && blob instanceof Blob && blob.size > 0) {
+            resolve(blob)
+          } else {
+            reject(new Error("Invalid or empty recording blob"))
+          }
+        }
+
+        plugin.on("record-end", handleRecordEnd)
+
+        plugin.stopRecording()
+      })
+    },
+    []
+  )
+
+  const stopRecording = useCallback(async () => {
+    const recordPlugin = recordPluginRef.current
+    if (!recordPlugin?.isRecording()) {
+      return
+    }
+
+    try {
+      const blob = await waitForRecordEnd(recordPlugin)
+      await transcodeAndUploadFile(blob)
+
+      if (wavesurfer) {
+        // We are blending this color instead of directly using the theme color (fadedText40)
+        // because the "faded" part of fadedText40 means introducing some transparency, which
+        // causes problems with the progress waveform color because wavesurfer is choosing to
+        // tint the waveColor with the progressColor instead of directly setting the progressColor.
+        // This means that the low opacity of fadedText40 causes the progress waveform to
+        // have the same opacity which makes it impossible to darken it enough to match designs.
+        // We fix this by blending the colors to figure out what the resulting color should be at
+        // full opacity, and we use that color to set the waveColor.
+        wavesurfer.setOptions({
+          waveColor: blend(theme.colors.fadedText40, theme.colors.secondaryBg),
+          progressColor: theme.colors.bodyText,
+        })
+      }
+    } catch {
+      setIsError(true)
+    }
+  }, [transcodeAndUploadFile, wavesurfer, theme, waitForRecordEnd])
 
   const downloadRecording = useDownloadUrl(recordingUrl, "recording.wav")
 
-  const isRecording = Boolean(recordPlugin?.isRecording())
+  const isRecording = recordPluginRef.current?.isRecording() || false
   const isPlaying = Boolean(wavesurfer?.isPlaying())
 
   const isPlayingOrRecording = isRecording || isPlaying
+
   const showPlaceholder = !isRecording && !recordingUrl && !hasNoMicPermissions
 
   const showNoMicPermissionsOrPlaceholderOrError =
     hasNoMicPermissions || showPlaceholder || isError
 
+  const handleStartRecording = useCallback(() => {
+    void startRecording()
+  }, [startRecording])
+
+  const handleStopRecording = useCallback(() => {
+    void stopRecording()
+  }, [stopRecording])
+
+  const handleClearWithError = useCallback(() => {
+    void handleClear({ updateWidgetManager: false, deleteFile: true })
+    setIsError(false)
+  }, [handleClear])
+
+  const handleDownloadClick = useCallback(() => {
+    downloadRecording()
+  }, [downloadRecording])
+
+  const handleDeleteClick = useCallback(() => {
+    void handleClear({
+      updateWidgetManager: true,
+      deleteFile: true,
+    })
+  }, [handleClear])
+
   return (
     <StyledAudioInputContainerDiv
       className="stAudioInput"
@@ -452,16 +625,14 @@ const AudioInput: React.FC<Props> = ({
             <ToolbarAction
               label="Download as WAV"
               icon={FileDownload}
-              onClick={() => downloadRecording()}
+              onClick={handleDownloadClick}
             />
           )}
           {deleteFileUrl && (
             <ToolbarAction
               label="Clear recording"
               icon={Delete}
-              onClick={() =>
-                handleClear({ updateWidgetManager: true, deleteFile: true })
-              }
+              onClick={handleDeleteClick}
             />
           )}
         </Toolbar>
@@ -471,14 +642,10 @@ const AudioInput: React.FC<Props> = ({
           isUploading={isUploading}
           isError={isError}
           recordingUrlExists={Boolean(recordingUrl)}
-          // eslint-disable-next-line @typescript-eslint/no-misused-promises
-          startRecording={startRecording}
-          stopRecording={stopRecording}
+          startRecording={handleStartRecording}
+          stopRecording={handleStopRecording}
           onClickPlayPause={onClickPlayPause}
-          onClear={() => {
-            handleClear({ updateWidgetManager: false, deleteFile: true })
-            setIsError(false)
-          }}
+          onClear={handleClearWithError}
           disabled={disabled || hasNoMicPermissions}
         />
         <StyledWaveformInnerDiv>
diff --git a/frontend/lib/src/components/widgets/AudioInput/convertAudioToWav.test.ts b/frontend/lib/src/components/widgets/AudioInput/convertAudioToWav.test.ts
new file mode 100644
index 000000000..66520b6d4
--- /dev/null
+++ b/frontend/lib/src/components/widgets/AudioInput/convertAudioToWav.test.ts
@@ -0,0 +1,480 @@
+/**
+ * Copyright (c) Streamlit Inc. (2018-2022) Snowflake Inc. (2022-2025)
+ *
+ * Licensed under the Apache License, Version 2.0 (the "License");
+ * you may not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+import { afterEach, beforeEach, describe, expect, it, vi } from "vitest"
+
+import convertFileToWav from "./convertAudioToWav"
+
+describe("convertAudioToWav", () => {
+  let originalAudioContext: typeof AudioContext | undefined
+  let originalOfflineAudioContext: typeof OfflineAudioContext | undefined
+
+  // Helper function to create a blob with mocked arrayBuffer method
+  const createMockBlob = (size = 100): Blob => {
+    const testArrayBuffer = new ArrayBuffer(size)
+    const blob = new Blob([testArrayBuffer], { type: "audio/wav" })
+    // Mock the arrayBuffer method
+    blob.arrayBuffer = vi.fn().mockResolvedValue(testArrayBuffer)
+    return blob
+  }
+
+  beforeEach(() => {
+    // Save original values
+    originalAudioContext = window.AudioContext
+    originalOfflineAudioContext = window.OfflineAudioContext
+
+    // Clear console mocks
+    vi.clearAllMocks()
+  })
+
+  afterEach(() => {
+    // Restore original values
+    if (originalAudioContext !== undefined) {
+      window.AudioContext = originalAudioContext
+    }
+    if (originalOfflineAudioContext !== undefined) {
+      window.OfflineAudioContext = originalOfflineAudioContext
+    }
+  })
+
+  describe("error handling", () => {
+    it("should return undefined for null blob", async () => {
+      const result = await convertFileToWav(null as unknown as Blob)
+      expect(result).toBeUndefined()
+    })
+
+    it("should return undefined for empty blob", async () => {
+      const emptyBlob = new Blob([])
+      const result = await convertFileToWav(emptyBlob)
+      expect(result).toBeUndefined()
+    })
+
+    it("should handle AudioContext not being supported", async () => {
+      // Remove AudioContext completely
+      delete (window as Window & { AudioContext?: typeof AudioContext })
+        .AudioContext
+
+      const blob = createMockBlob()
+      const result = await convertFileToWav(blob)
+      expect(result).toBeUndefined()
+    })
+
+    it("should handle arrayBuffer() failure", async () => {
+      const mockBlob = {
+        size: 100,
+        arrayBuffer: vi.fn().mockRejectedValue(new Error("Failed to read")),
+        type: "audio/wav",
+      } as unknown as Blob
+
+      const mockClose = vi.fn()
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      const result = await convertFileToWav(mockBlob)
+      expect(result).toBeUndefined()
+      expect(mockClose).toHaveBeenCalled()
+    })
+
+    it("should handle decodeAudioData failure", async () => {
+      const mockClose = vi.fn()
+      const mockDecodeAudioData = vi
+        .fn()
+        .mockRejectedValue(new Error("Decode failed"))
+
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+        decodeAudioData: mockDecodeAudioData,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      const blob = createMockBlob()
+
+      const result = await convertFileToWav(blob)
+
+      expect(result).toBeUndefined()
+      expect(mockClose).toHaveBeenCalled()
+    })
+  })
+
+  describe("resampling functionality", () => {
+    const createMockAudioBuffer = (
+      sampleRate: number,
+      channels = 2,
+      length = 100
+    ): Partial<AudioBuffer> => ({
+      sampleRate,
+      numberOfChannels: channels,
+      length,
+      duration: length / sampleRate,
+      getChannelData: vi.fn().mockImplementation((_channel: number) => {
+        const data = new Float32Array(length)
+        for (let i = 0; i < length; i++) {
+          // Create a simple sine wave for testing
+          data[i] = Math.sin((2 * Math.PI * i) / 10) * 0.5
+        }
+        return data
+      }),
+    })
+
+    it("should not resample when target rate matches source rate", async () => {
+      const mockAudioBuffer = createMockAudioBuffer(44100)
+      const mockClose = vi.fn()
+      const mockDecodeAudioData = vi.fn().mockResolvedValue(mockAudioBuffer)
+
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+        decodeAudioData: mockDecodeAudioData,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      const blob = createMockBlob()
+      const result = await convertFileToWav(blob, 44100)
+
+      expect(result).toBeInstanceOf(Blob)
+      expect(result?.type).toBe("audio/wav")
+      // OfflineAudioContext should not be used when no resampling is needed
+      expect(window.OfflineAudioContext).toBeUndefined()
+    })
+
+    it.each([
+      {
+        sourceSampleRate: 22050,
+        targetSampleRate: 44100,
+        description: "upsample from 22kHz to 44.1kHz",
+      },
+      {
+        sourceSampleRate: 48000,
+        targetSampleRate: 16000,
+        description: "downsample from 48kHz to 16kHz",
+      },
+      {
+        sourceSampleRate: 16000,
+        targetSampleRate: 48000,
+        description: "upsample from 16kHz to 48kHz",
+      },
+      {
+        sourceSampleRate: 44100,
+        targetSampleRate: 22050,
+        description: "downsample from 44.1kHz to 22kHz",
+      },
+      {
+        sourceSampleRate: 8000,
+        targetSampleRate: 44100,
+        description: "upsample from 8kHz (telephony) to 44.1kHz",
+      },
+      {
+        sourceSampleRate: 96000,
+        targetSampleRate: 48000,
+        description: "downsample from 96kHz (high-res) to 48kHz",
+      },
+    ])(
+      "should resample audio: $description",
+      async ({ sourceSampleRate, targetSampleRate }) => {
+        const mockAudioBuffer = createMockAudioBuffer(sourceSampleRate)
+        const mockResampledBuffer = createMockAudioBuffer(targetSampleRate)
+        const mockClose = vi.fn()
+        const mockDecodeAudioData = vi.fn().mockResolvedValue(mockAudioBuffer)
+
+        const MockAudioContext = vi.fn().mockImplementation(() => ({
+          close: mockClose,
+          decodeAudioData: mockDecodeAudioData,
+        }))
+        window.AudioContext =
+          MockAudioContext as unknown as typeof AudioContext
+
+        const mockStartRendering = vi
+          .fn()
+          .mockResolvedValue(mockResampledBuffer)
+        const mockCreateBufferSource = vi.fn().mockReturnValue({
+          buffer: null,
+          connect: vi.fn(),
+          start: vi.fn(),
+        })
+
+        const MockOfflineAudioContext = vi.fn().mockImplementation(() => ({
+          createBufferSource: mockCreateBufferSource,
+          destination: {},
+          startRendering: mockStartRendering,
+        }))
+        window.OfflineAudioContext =
+          MockOfflineAudioContext as unknown as typeof OfflineAudioContext
+
+        const blob = createMockBlob()
+        const result = await convertFileToWav(blob, targetSampleRate)
+
+        expect(result).toBeInstanceOf(Blob)
+        expect(result?.type).toBe("audio/wav")
+        expect(MockOfflineAudioContext).toHaveBeenCalledWith(
+          2,
+          expect.any(Number),
+          targetSampleRate
+        )
+        expect(mockStartRendering).toHaveBeenCalled()
+      }
+    )
+
+    it("should fallback to original sample rate if OfflineAudioContext is not supported", async () => {
+      const mockAudioBuffer = createMockAudioBuffer(22050)
+      const mockClose = vi.fn()
+      const mockDecodeAudioData = vi.fn().mockResolvedValue(mockAudioBuffer)
+
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+        decodeAudioData: mockDecodeAudioData,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      // Remove OfflineAudioContext completely
+      delete (
+        window as Window & { OfflineAudioContext?: typeof OfflineAudioContext }
+      ).OfflineAudioContext
+
+      const blob = createMockBlob()
+      const result = await convertFileToWav(blob, 44100)
+
+      expect(result).toBeInstanceOf(Blob)
+      expect(result?.type).toBe("audio/wav")
+      // Should return original audio without resampling
+    })
+
+    it("should handle startRendering failure gracefully", async () => {
+      const mockAudioBuffer = createMockAudioBuffer(22050)
+      const mockClose = vi.fn()
+      const mockDecodeAudioData = vi.fn().mockResolvedValue(mockAudioBuffer)
+
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+        decodeAudioData: mockDecodeAudioData,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      const mockStartRendering = vi
+        .fn()
+        .mockRejectedValue(new Error("Rendering failed"))
+      const mockCreateBufferSource = vi.fn().mockReturnValue({
+        buffer: null,
+        connect: vi.fn(),
+        start: vi.fn(),
+      })
+
+      const MockOfflineAudioContext = vi.fn().mockImplementation(() => ({
+        createBufferSource: mockCreateBufferSource,
+        destination: {},
+        startRendering: mockStartRendering,
+      }))
+      window.OfflineAudioContext =
+        MockOfflineAudioContext as unknown as typeof OfflineAudioContext
+
+      const blob = createMockBlob()
+      const result = await convertFileToWav(blob, 44100)
+
+      // Should fallback to original audio
+      expect(result).toBeInstanceOf(Blob)
+      expect(result?.type).toBe("audio/wav")
+      expect(mockStartRendering).toHaveBeenCalled()
+    })
+  })
+
+  describe("WAV encoding", () => {
+    it("should handle mono audio correctly", async () => {
+      const mockAudioBuffer = {
+        sampleRate: 44100,
+        numberOfChannels: 1,
+        length: 100,
+        duration: 100 / 44100,
+        getChannelData: vi
+          .fn()
+          .mockReturnValue(new Float32Array(100).fill(0.5)),
+      }
+
+      const mockClose = vi.fn()
+      const mockDecodeAudioData = vi.fn().mockResolvedValue(mockAudioBuffer)
+
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+        decodeAudioData: mockDecodeAudioData,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      const blob = createMockBlob()
+      const result = await convertFileToWav(blob)
+
+      expect(result).toBeInstanceOf(Blob)
+      expect(result?.type).toBe("audio/wav")
+      expect(result?.size).toBeGreaterThan(44) // WAV header is 44 bytes
+    })
+
+    it("should handle stereo audio correctly", async () => {
+      const mockAudioBuffer = {
+        sampleRate: 44100,
+        numberOfChannels: 2,
+        length: 100,
+        duration: 100 / 44100,
+        getChannelData: vi
+          .fn()
+          .mockImplementation((channel: number): Float32Array => {
+            return new Float32Array(100).fill(channel === 0 ? 0.5 : -0.5)
+          }),
+      }
+
+      const mockClose = vi.fn()
+      const mockDecodeAudioData = vi.fn().mockResolvedValue(mockAudioBuffer)
+
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+        decodeAudioData: mockDecodeAudioData,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      const blob = createMockBlob()
+      const result = await convertFileToWav(blob)
+
+      expect(result).toBeInstanceOf(Blob)
+      expect(result?.type).toBe("audio/wav")
+      expect(result?.size).toBeGreaterThan(44) // WAV header is 44 bytes
+    })
+
+    it("should clamp audio samples that exceed [-1, 1] range", async () => {
+      const mockAudioBuffer = {
+        sampleRate: 44100,
+        numberOfChannels: 1,
+        length: 3,
+        duration: 3 / 44100,
+        getChannelData: vi
+          .fn()
+          .mockReturnValue(new Float32Array([2.0, -2.0, 0.5])),
+      }
+
+      const mockClose = vi.fn()
+      const mockDecodeAudioData = vi.fn().mockResolvedValue(mockAudioBuffer)
+
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+        decodeAudioData: mockDecodeAudioData,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      const blob = createMockBlob()
+      const result = await convertFileToWav(blob)
+
+      expect(result).toBeInstanceOf(Blob)
+      expect(result?.type).toBe("audio/wav")
+      // Values should be clamped during encoding
+      expect(result?.size).toBe(44 + 3 * 2) // 44 byte header + 3 samples * 2 bytes per sample
+    })
+
+    it("should use original sample rate when no target is specified", async () => {
+      const mockAudioBuffer = {
+        sampleRate: 22050,
+        numberOfChannels: 1,
+        length: 100,
+        duration: 100 / 22050,
+        getChannelData: vi.fn().mockReturnValue(new Float32Array(100).fill(0)),
+      }
+
+      const mockClose = vi.fn()
+      const mockDecodeAudioData = vi.fn().mockResolvedValue(mockAudioBuffer)
+
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+        decodeAudioData: mockDecodeAudioData,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      const blob = createMockBlob()
+      const result = await convertFileToWav(blob)
+
+      expect(result).toBeInstanceOf(Blob)
+      expect(result?.type).toBe("audio/wav")
+      // Should use original sample rate
+    })
+  })
+
+  describe("different audio channel configurations", () => {
+    it.each([
+      { channels: 1, samples: 100, description: "mono audio" },
+      { channels: 2, samples: 100, description: "stereo audio" },
+      { channels: 4, samples: 50, description: "4-channel audio" },
+      { channels: 6, samples: 25, description: "5.1 surround audio" },
+    ])("should handle $description", async ({ channels, samples }) => {
+      const mockAudioBuffer = {
+        sampleRate: 44100,
+        numberOfChannels: channels,
+        length: samples,
+        duration: samples / 44100,
+        getChannelData: vi
+          .fn()
+          .mockImplementation((channel: number): Float32Array => {
+            return new Float32Array(samples).fill(0.25 * channel)
+          }),
+      }
+
+      const mockClose = vi.fn()
+      const mockDecodeAudioData = vi.fn().mockResolvedValue(mockAudioBuffer)
+
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+        decodeAudioData: mockDecodeAudioData,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      const blob = createMockBlob()
+      const result = await convertFileToWav(blob)
+
+      expect(result).toBeInstanceOf(Blob)
+      expect(result?.type).toBe("audio/wav")
+      // WAV header is 44 bytes + (samples * channels * 2 bytes per sample)
+      expect(result?.size).toBe(44 + samples * channels * 2)
+      expect(mockAudioBuffer.getChannelData).toHaveBeenCalledTimes(
+        samples * channels
+      )
+    })
+
+    it.each([
+      { length: 1, description: "single sample" },
+      { length: 10, description: "very short buffer" },
+      { length: 100000, description: "large buffer" },
+    ])("should handle $description", async ({ length }) => {
+      const mockAudioBuffer = {
+        sampleRate: 44100,
+        numberOfChannels: 1,
+        length,
+        duration: length / 44100,
+        getChannelData: vi
+          .fn()
+          .mockReturnValue(new Float32Array(length).fill(0.5)),
+      }
+
+      const mockClose = vi.fn()
+      const mockDecodeAudioData = vi.fn().mockResolvedValue(mockAudioBuffer)
+
+      const MockAudioContext = vi.fn().mockImplementation(() => ({
+        close: mockClose,
+        decodeAudioData: mockDecodeAudioData,
+      }))
+      window.AudioContext = MockAudioContext as unknown as typeof AudioContext
+
+      const blob = createMockBlob()
+      const result = await convertFileToWav(blob)
+
+      expect(result).toBeInstanceOf(Blob)
+      expect(result?.type).toBe("audio/wav")
+      expect(result?.size).toBe(44 + length * 2) // 44 byte header + length * 2 bytes
+    })
+  })
+})
diff --git a/frontend/lib/src/components/widgets/AudioInput/convertAudioToWav.ts b/frontend/lib/src/components/widgets/AudioInput/convertAudioToWav.ts
index 5e11fb173..0868ecf75 100644
--- a/frontend/lib/src/components/widgets/AudioInput/convertAudioToWav.ts
+++ b/frontend/lib/src/components/widgets/AudioInput/convertAudioToWav.ts
@@ -20,60 +20,145 @@ import { getLogger } from "loglevel"
 const LOG = getLogger("convertAudioToWav")
 
 /**
- * Converts a file Blob (audio/video) to a WAV Blob.
- * @param fileBlob - The input file as a Blob.
- * @returns - A Promise resolving with the WAV file as a Blob.
+ * Converts an audio blob to WAV format with high-quality resampling.
+ * Uses the Web Audio API's OfflineAudioContext for professional-quality
+ * resampling with proper anti-aliasing filters instead of linear interpolation.
+ *
+ * @param fileBlob - The input audio blob to convert
+ * @param targetSampleRate - Optional target sample rate for the output WAV file
+ * @returns A Promise resolving to the WAV file as a Blob, or undefined on error
  */
-async function convertFileToWav(fileBlob: Blob): Promise<Blob | undefined> {
-  const audioContext = new window.AudioContext()
-  const arrayBuffer = await fileBlob.arrayBuffer()
+async function convertFileToWav(
+  fileBlob: Blob,
+  targetSampleRate?: number
+): Promise<Blob | undefined> {
+  if (!fileBlob || fileBlob.size === 0) {
+    LOG.error("Invalid or empty blob provided")
+    return undefined
+  }
+
+  if (!window.AudioContext) {
+    LOG.error("AudioContext not supported in this browser")
+    return undefined
+  }
+
+  const audioContext = new AudioContext()
+
+  let arrayBuffer: ArrayBuffer
+  try {
+    arrayBuffer = await fileBlob.arrayBuffer()
+  } catch (error) {
+    LOG.error("Failed to read blob as ArrayBuffer", error)
+    void audioContext.close()
+    return undefined
+  }
 
   let audioBuffer: AudioBuffer
   try {
     audioBuffer = await audioContext.decodeAudioData(arrayBuffer)
   } catch (error) {
-    LOG.error(error)
-    return undefined // Return undefined if decoding fails
+    LOG.error("Failed to decode audio data", error)
+    void audioContext.close()
+    return undefined
+  } finally {
+    void audioContext.close()
   }
 
-  const HEADER_HEIGHT = 44
+  const outputSampleRate = targetSampleRate || audioBuffer.sampleRate
+
+  if (outputSampleRate === audioBuffer.sampleRate) {
+    LOG.debug(
+      `No resampling needed, sample rate is already ${outputSampleRate}Hz`
+    )
+    return encodeWAV(audioBuffer, outputSampleRate)
+  }
+
+  LOG.debug(
+    `Resampling from ${audioBuffer.sampleRate}Hz to ${outputSampleRate}Hz`
+  )
+
+  const { duration, numberOfChannels } = audioBuffer
+  const frameCount = Math.ceil(duration * outputSampleRate)
+
+  if (!window.OfflineAudioContext) {
+    LOG.error(
+      "OfflineAudioContext not supported, falling back to no resampling"
+    )
+    return encodeWAV(audioBuffer, audioBuffer.sampleRate)
+  }
+
+  const offlineContext = new OfflineAudioContext(
+    numberOfChannels,
+    frameCount,
+    outputSampleRate
+  )
+
+  const source = offlineContext.createBufferSource()
+  source.buffer = audioBuffer
+  source.connect(offlineContext.destination)
+  source.start(0)
+
+  try {
+    const resampledBuffer = await offlineContext.startRendering()
+    return encodeWAV(resampledBuffer, outputSampleRate)
+  } catch (error) {
+    LOG.error("Failed to resample audio using OfflineAudioContext", error)
+    return encodeWAV(audioBuffer, audioBuffer.sampleRate)
+  }
+}
+
+/**
+ * Encodes an AudioBuffer as a WAV file blob.
+ * Separated from the main function for better modularity and testability.
+ *
+ * @param audioBuffer - The AudioBuffer containing the audio data to encode.
+ *   - Each channel in the buffer will be interleaved in the output WAV file.
+ *   - The buffer should contain PCM float samples in the range [-1, 1].
+ * @param sampleRate - The sample rate (in Hz) to use for the WAV file header.
+ *   - This determines the playback rate of the resulting WAV file.
+ * @returns A Blob containing a WAV file encoded according to the RIFF/WAVE specification:
+ *   - 44-byte header (RIFF, WAVE, fmt, data chunks)
+ *   - 16-bit PCM samples, interleaved for multiple channels
+ *   - Audio data starts at byte 44
+ *   - MIME type is "audio/wav"
+ */
+function encodeWAV(audioBuffer: AudioBuffer, sampleRate: number): Blob {
+  const HEADER_SIZE = 44
   const numOfChan = audioBuffer.numberOfChannels
-  const sampleRate = audioBuffer.sampleRate
-  const length = audioBuffer.length * numOfChan * 2 + HEADER_HEIGHT
+  const length = audioBuffer.length * numOfChan * 2 + HEADER_SIZE
   const buffer = new ArrayBuffer(length)
   const view = new DataView(buffer)
 
-  // WAV header metadata
-  const wavHeader = {
-    0: { type: "string", value: "RIFF" },
-    4: { type: "uint32", value: length - 8 },
-    8: { type: "string", value: "WAVE" },
-    12: { type: "string", value: "fmt " },
-    16: { type: "uint32", value: 16 }, // PCM format
-    20: { type: "uint16", value: 1 }, // PCM format code
-    22: { type: "uint16", value: numOfChan }, // Number of channels
-    24: { type: "uint32", value: sampleRate }, // Sample rate
-    28: { type: "uint32", value: sampleRate * numOfChan * 2 }, // Byte rate
-    32: { type: "uint16", value: numOfChan * 2 }, // Block align
-    34: { type: "uint16", value: 16 }, // Bits per sample (16-bit)
-    36: { type: "string", value: "data" },
-    40: { type: "uint32", value: audioBuffer.length * numOfChan * 2 }, // Data chunk length
+  /**
+   * Helper function to write a string to the DataView
+   */
+  const writeString = (offset: number, string: string): void => {
+    for (let i = 0; i < string.length; i++) {
+      view.setUint8(offset + i, string.charCodeAt(i))
+    }
   }
 
-  // Write WAV header from the dictionary using Object.entries
-  Object.entries(wavHeader).forEach(([offset, { type, value }]) => {
-    const intOffset = parseInt(offset, 10)
-    if (type === "string") {
-      writeString(view, intOffset, value as string)
-    } else if (type === "uint32") {
-      view.setUint32(intOffset, value as number, true)
-    } else if (type === "uint16") {
-      view.setUint16(intOffset, value as number, true)
-    }
-  })
+  // Write RIFF chunk descriptor
+  writeString(0, "RIFF")
+  view.setUint32(4, length - 8, true) // File size minus RIFF header
+  writeString(8, "WAVE")
+
+  // Write fmt sub-chunk
+  writeString(12, "fmt ")
+  view.setUint32(16, 16, true) // SubChunk1Size (16 for PCM)
+  view.setUint16(20, 1, true) // AudioFormat (1 for PCM)
+  view.setUint16(22, numOfChan, true) // NumChannels
+  view.setUint32(24, sampleRate, true) // SampleRate
+  view.setUint32(28, sampleRate * numOfChan * 2, true) // ByteRate
+  view.setUint16(32, numOfChan * 2, true) // BlockAlign
+  view.setUint16(34, 16, true) // BitsPerSample
 
-  // Write PCM data
-  let offset = HEADER_HEIGHT
+  // Write data sub-chunk
+  writeString(36, "data")
+  view.setUint32(40, length - HEADER_SIZE, true) // SubChunk2Size
+
+  // Write interleaved PCM samples
+  let offset = HEADER_SIZE
   for (let i = 0; i < audioBuffer.length; i++) {
     for (let channel = 0; channel < numOfChan; channel++) {
       const sample = Math.max(
@@ -85,20 +170,7 @@ async function convertFileToWav(fileBlob: Blob): Promise<Blob | undefined> {
     }
   }
 
-  const wavArray = new Uint8Array(buffer)
-  return new Blob([wavArray], { type: "audio/wav" })
-}
-
-/**
- * Writes a string to a DataView at the specified offset.
- * @param view - The DataView to write to.
- * @param offset - The starting offset in the DataView.
- * @param string - The string to write.
- */
-function writeString(view: DataView, offset: number, string: string): void {
-  for (let i = 0; i < string.length; i++) {
-    view.setUint8(offset + i, string.charCodeAt(i))
-  }
+  return new Blob([buffer], { type: "audio/wav" })
 }
 
 export default convertFileToWav
diff --git a/lib/streamlit/elements/widgets/audio_input.py b/lib/streamlit/elements/widgets/audio_input.py
index 67fffa42f..d1c52cba3 100644
--- a/lib/streamlit/elements/widgets/audio_input.py
+++ b/lib/streamlit/elements/widgets/audio_input.py
@@ -35,6 +35,7 @@ from streamlit.elements.lib.utils import (
     to_key,
 )
 from streamlit.elements.widgets.file_uploader import _get_upload_files
+from streamlit.errors import StreamlitAPIException
 from streamlit.proto.AudioInput_pb2 import AudioInput as AudioInputProto
 from streamlit.proto.Common_pb2 import FileUploaderState as FileUploaderStateProto
 from streamlit.proto.Common_pb2 import UploadedFileInfo as UploadedFileInfoProto
@@ -54,6 +55,9 @@ if TYPE_CHECKING:
 
 SomeUploadedAudioFile: TypeAlias = Union[UploadedFile, DeletedFile, None]
 
+# Allowed sample rates for audio recording
+ALLOWED_SAMPLE_RATES = {8000, 11025, 16000, 22050, 24000, 32000, 44100, 48000}
+
 
 @dataclass
 class AudioInputSerde:
@@ -90,6 +94,7 @@ class AudioInputMixin:
         self,
         label: str,
         *,
+        sample_rate: int | None = 16000,
         key: Key | None = None,
         help: str | None = None,
         on_change: WidgetCallback | None = None,
@@ -125,6 +130,12 @@ class AudioInputMixin:
             .. |st.markdown| replace:: ``st.markdown``
             .. _st.markdown: https://docs.streamlit.io/develop/api-reference/text/st.markdown
 
+        sample_rate : int or None
+            The target sample rate for the audio recording in Hz. If specified,
+            must be one of: 8000, 11025, 16000, 22050, 24000, 32000, 44100, or 48000.
+            Default is 16000 Hz (optimal for speech recognition). Pass ``None`` to
+            use the browser's default sample rate (typically 44100 or 48000 Hz).
+
         key : str or int
             An optional string or integer to use as the unique key for the widget.
             If this is omitted, a key will be generated for the widget
@@ -187,19 +198,34 @@ class AudioInputMixin:
         --------
         >>> import streamlit as st
         >>>
+        >>> # Record with default 16 kHz sample rate (optimal for speech)
         >>> audio_value = st.audio_input("Record a voice message")
         >>>
         >>> if audio_value:
         ...     st.audio(audio_value)
 
+        >>> # Record with browser's default sample rate (44.1/48 kHz)
+        >>> audio_hq = st.audio_input("Record high quality audio", sample_rate=None)
+
+        >>> # Record at 24 kHz for real-time transcription
+        >>> audio_rt = st.audio_input("Record for transcription", sample_rate=24000)
+
         .. output::
            https://doc-audio-input.streamlit.app/
            height: 260px
 
         """
+        # Validate sample_rate parameter
+        if sample_rate is not None and sample_rate not in ALLOWED_SAMPLE_RATES:
+            raise StreamlitAPIException(
+                f"Invalid sample_rate: {sample_rate}. "
+                f"Must be one of {sorted(ALLOWED_SAMPLE_RATES)} Hz, or None for browser default."
+            )
+
         ctx = get_script_run_ctx()
         return self._audio_input(
             label=label,
+            sample_rate=sample_rate,
             key=key,
             help=help,
             on_change=on_change,
@@ -214,6 +240,7 @@ class AudioInputMixin:
     def _audio_input(
         self,
         label: str,
+        sample_rate: int | None = 16000,
         key: Key | None = None,
         help: str | None = None,
         on_change: WidgetCallback | None = None,
@@ -255,6 +282,10 @@ class AudioInputMixin:
             label_visibility
         )
 
+        # Set sample_rate in protobuf if specified
+        if sample_rate is not None:
+            audio_input_proto.sample_rate = sample_rate
+
         if label and help is not None:
             audio_input_proto.help = dedent(help)
 
diff --git a/lib/tests/streamlit/elements/audio_input_test.py b/lib/tests/streamlit/elements/audio_input_test.py
index 6926ff9a2..fd816f25b 100644
--- a/lib/tests/streamlit/elements/audio_input_test.py
+++ b/lib/tests/streamlit/elements/audio_input_test.py
@@ -39,6 +39,8 @@ class AudioInputTest(DeltaGeneratorTestCase):
             c.label_visibility.value
             == LabelVisibilityMessage.LabelVisibilityOptions.VISIBLE
         )
+        # Default sample_rate should be 16000
+        assert c.sample_rate == 16000
 
     @parameterized.expand(
         [
@@ -109,6 +111,48 @@ class AudioInputTest(DeltaGeneratorTestCase):
         with pytest.raises(StreamlitInvalidWidthError):
             st.audio_input("the label", width=invalid_width)
 
+    @parameterized.expand(
+        [
+            (8000,),
+            (11025,),
+            (16000,),
+            (22050,),
+            (24000,),
+            (32000,),
+            (44100,),
+            (48000,),
+        ]
+    )
+    def test_valid_sample_rates(self, sample_rate):
+        """Test that valid sample rates are accepted and properly set in the protobuf."""
+        st.audio_input("the label", sample_rate=sample_rate)
+
+        c = self.get_delta_from_queue().new_element.audio_input
+        assert c.sample_rate == sample_rate
+
+    def test_sample_rate_none(self):
+        """Test that None sample_rate means browser default."""
+        st.audio_input("the label", sample_rate=None)
+
+        c = self.get_delta_from_queue().new_element.audio_input
+        # When sample_rate is None, the field should not be set in protobuf
+        assert not c.HasField("sample_rate")
+
+    @parameterized.expand(
+        [
+            (12345,),
+            (9000,),
+            (50000,),
+            (100000,),
+        ]
+    )
+    def test_invalid_sample_rates(self, sample_rate):
+        """Test that invalid sample rates raise an exception."""
+        with pytest.raises(StreamlitAPIException) as e:
+            st.audio_input("the label", sample_rate=sample_rate)
+        assert "Invalid sample_rate" in str(e.value)
+        assert "Must be one of" in str(e.value)
+
     @patch("streamlit.elements.widgets.audio_input._get_upload_files")
     def test_not_allowed_file_extension_raise_an_exception_for_camera_input(
         self, get_upload_files_patch
diff --git a/proto/streamlit/proto/AudioInput.proto b/proto/streamlit/proto/AudioInput.proto
index effa061b4..0f8fe0642 100644
--- a/proto/streamlit/proto/AudioInput.proto
+++ b/proto/streamlit/proto/AudioInput.proto
@@ -28,4 +28,5 @@ message AudioInput {
   string form_id = 4;
   bool disabled = 5;
   LabelVisibilityMessage label_visibility = 6;
+  optional int32 sample_rate = 7;
 }
