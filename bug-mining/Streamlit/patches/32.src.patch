diff --git a/e2e_playwright/lazy_loaded_modules.py b/e2e_playwright/lazy_loaded_modules.py
index 2bdebadcf..fd4622fc0 100644
--- a/e2e_playwright/lazy_loaded_modules.py
+++ b/e2e_playwright/lazy_loaded_modules.py
@@ -23,6 +23,10 @@ lazy_loaded_modules = [
     "graphviz",
     "matplotlib",
     "numpy",
+    # There is currently a 10% probability that we check for new
+    # versions on streamlit start-up. This is using the packaging module.
+    # So, we cannot check this without it being flaky.
+    # "packaging",
     "pandas",
     # Pillow is lazy-loaded, but it gets imported by plotly,
     # which we have to import in case it is installed to correctly
@@ -35,9 +39,11 @@ lazy_loaded_modules = [
     # toml is automatically loaded if there is a secret.toml, config.toml or
     # a local credentials.toml file.
     "toml",
+    "unittest",
     # Internal modules:
     "streamlit.emojis",
     "streamlit.external",
+    "streamlit.proto.openmetrics_data_model_pb2",
     "streamlit.vendor.pympler",
     # Requires `server.fileWatcherType` to be configured with `none` or `poll`:
     "watchdog",
diff --git a/e2e_playwright/lazy_loaded_modules_test.py b/e2e_playwright/lazy_loaded_modules_test.py
index 9f7a6d32e..2993e0399 100644
--- a/e2e_playwright/lazy_loaded_modules_test.py
+++ b/e2e_playwright/lazy_loaded_modules_test.py
@@ -20,6 +20,6 @@ from playwright.sync_api import Page, expect
 def test_lazy_loaded_modules_are_not_imported(app: Page):
     """Test that lazy loaded modules are not imported when the page is loaded."""
     markdown_elements = app.get_by_test_id("stMarkdown")
-    expect(markdown_elements).to_have_count(16)
+    expect(markdown_elements).to_have_count(18)
     for element in markdown_elements.all():
         expect(element).to_have_text(re.compile(r".*not loaded.*"))
diff --git a/lib/streamlit/runtime/caching/hashing.py b/lib/streamlit/runtime/caching/hashing.py
index fd49e893d..8b8d3b523 100644
--- a/lib/streamlit/runtime/caching/hashing.py
+++ b/lib/streamlit/runtime/caching/hashing.py
@@ -13,6 +13,9 @@
 # limitations under the License.
 
 """Hashing for st.cache_data and st.cache_resource."""
+
+from __future__ import annotations
+
 import collections
 import dataclasses
 import datetime
@@ -22,15 +25,15 @@ import inspect
 import io
 import os
 import pickle
-import struct
 import sys
 import tempfile
 import threading
-import unittest.mock
 import uuid
 import weakref
 from enum import Enum
-from typing import Any, Callable, Dict, List, Optional, Pattern, Type, Union
+from typing import Any, Callable, Dict, Final, Pattern, Type, Union
+
+from typing_extensions import TypeAlias
 
 from streamlit import type_util, util
 from streamlit.errors import StreamlitAPIException
@@ -40,18 +43,20 @@ from streamlit.runtime.uploaded_file_manager import UploadedFile
 from streamlit.util import HASHLIB_KWARGS
 
 # If a dataframe has more than this many rows, we consider it large and hash a sample.
-_PANDAS_ROWS_LARGE = 100000
-_PANDAS_SAMPLE_SIZE = 10000
+_PANDAS_ROWS_LARGE: Final = 100000
+_PANDAS_SAMPLE_SIZE: Final = 10000
 
 # Similar to dataframes, we also sample large numpy arrays.
-_NP_SIZE_LARGE = 1000000
-_NP_SAMPLE_SIZE = 100000
+_NP_SIZE_LARGE: Final = 1000000
+_NP_SAMPLE_SIZE: Final = 100000
 
-HashFuncsDict = Dict[Union[str, Type[Any]], Callable[[Any], Any]]
+HashFuncsDict: TypeAlias = Dict[Union[str, Type[Any]], Callable[[Any], Any]]
 
 # Arbitrary item to denote where we found a cycle in a hashed object.
 # This allows us to hash self-referencing lists, dictionaries, etc.
-_CYCLE_PLACEHOLDER = b"streamlit-57R34ML17-hesamagicalponyflyingthroughthesky-CYCLE"
+_CYCLE_PLACEHOLDER: Final = (
+    b"streamlit-57R34ML17-hesamagicalponyflyingthroughthesky-CYCLE"
+)
 
 
 class UserHashError(StreamlitAPIException):
@@ -60,7 +65,7 @@ class UserHashError(StreamlitAPIException):
         orig_exc,
         object_to_hash,
         hash_func,
-        cache_type: Optional[CacheType] = None,
+        cache_type: CacheType | None = None,
     ):
         self.alternate_name = type(orig_exc).__name__
         self.hash_func = hash_func
@@ -101,7 +106,7 @@ If you think this is actually a Streamlit bug, please
         self,
         orig_exc: BaseException,
         failed_obj: Any,
-    ) -> Dict[str, Any]:
+    ) -> dict[str, Any]:
         hash_source = hash_stacks.current.hash_source
 
         failed_obj_type_str = type_util.get_fqn_type(failed_obj)
@@ -139,8 +144,8 @@ def update_hash(
     val: Any,
     hasher,
     cache_type: CacheType,
-    hash_source: Optional[Callable[..., Any]] = None,
-    hash_funcs: Optional[HashFuncsDict] = None,
+    hash_source: Callable[..., Any] | None = None,
+    hash_funcs: HashFuncsDict | None = None,
 ) -> None:
     """Updates a hashlib hasher with the hash of val.
 
@@ -166,10 +171,10 @@ class _HashStack:
     """
 
     def __init__(self):
-        self._stack: collections.OrderedDict[int, List[Any]] = collections.OrderedDict()
+        self._stack: collections.OrderedDict[int, list[Any]] = collections.OrderedDict()
         # A function that we decorate with streamlit cache
         # primitive (st.cache_data or st.cache_resource).
-        self.hash_source: Optional[Callable[..., Any]] = None
+        self.hash_source: Callable[..., Any] | None = None
 
     def __repr__(self) -> str:
         return util.repr_(self)
@@ -226,11 +231,14 @@ def _int_to_bytes(i: int) -> bytes:
 
 
 def _float_to_bytes(f: float) -> bytes:
+    # Lazy-load for performance reasons.
+    import struct
+
     # Floats are 64bit in Python, so we need to use the "d" format.
     return struct.pack("<d", f)
 
 
-def _key(obj: Optional[Any]) -> Any:
+def _key(obj: Any | None) -> Any:
     """Return key for memoization."""
 
     if obj is None:
@@ -268,9 +276,7 @@ def _key(obj: Optional[Any]) -> Any:
 class _CacheFuncHasher:
     """A hasher that can hash objects with cycles."""
 
-    def __init__(
-        self, cache_type: CacheType, hash_funcs: Optional[HashFuncsDict] = None
-    ):
+    def __init__(self, cache_type: CacheType, hash_funcs: HashFuncsDict | None = None):
         # Can't use types as the keys in the internal _hash_funcs because
         # we always remove user-written modules from memory when rerunning a
         # script in order to reload it and grab the latest code changes.
@@ -286,7 +292,7 @@ class _CacheFuncHasher:
             }
         else:
             self._hash_funcs = {}
-        self._hashes: Dict[Any, bytes] = {}
+        self._hashes: dict[Any, bytes] = {}
 
         # The number of the bytes in the hash.
         self.size = 0
@@ -344,7 +350,9 @@ class _CacheFuncHasher:
 
         h = hashlib.new("md5", **HASHLIB_KWARGS)
 
-        if isinstance(obj, unittest.mock.Mock):
+        if type_util.is_type(obj, "unittest.mock.Mock") or type_util.is_type(
+            obj, "unittest.mock.MagicMock"
+        ):
             # Mock objects can appear to be infinitely
             # deep, so we don't try to hash them at all.
             return self.to_bytes(id(obj))
diff --git a/lib/streamlit/runtime/legacy_caching/hashing.py b/lib/streamlit/runtime/legacy_caching/hashing.py
index ce9ef4976..a425f1170 100644
--- a/lib/streamlit/runtime/legacy_caching/hashing.py
+++ b/lib/streamlit/runtime/legacy_caching/hashing.py
@@ -14,12 +14,12 @@
 
 """A hashing utility for code."""
 
+from __future__ import annotations
+
 import collections
-import dis
 import enum
 import functools
 import hashlib
-import importlib
 import inspect
 import io
 import os
@@ -28,9 +28,8 @@ import sys
 import tempfile
 import textwrap
 import threading
-import unittest.mock
 import weakref
-from typing import Any, Callable, Dict, List, Optional, Pattern, Type, Union
+from typing import Any, Callable, Dict, Pattern, Type, Union
 
 from streamlit import config, file_util, type_util, util
 from streamlit.errors import MarkdownFormattedException, StreamlitAPIException
@@ -95,8 +94,8 @@ def update_hash(
     hasher,
     hash_reason: HashReason,
     hash_source: Callable[..., Any],
-    context: Optional[Context] = None,
-    hash_funcs: Optional[HashFuncsDict] = None,
+    context: Context | None = None,
+    hash_funcs: HashFuncsDict | None = None,
 ) -> None:
     """Updates a hashlib hasher with the hash of val.
 
@@ -122,15 +121,15 @@ class _HashStack:
     """
 
     def __init__(self):
-        self._stack: collections.OrderedDict[int, List[Any]] = collections.OrderedDict()
+        self._stack: collections.OrderedDict[int, list[Any]] = collections.OrderedDict()
 
         # The reason why we're doing this hashing, for debug purposes.
-        self.hash_reason: Optional[HashReason] = None
+        self.hash_reason: HashReason | None = None
 
         # Either a function or a code block, depending on whether the reason is
         # due to hashing part of a function (i.e. body, args, output) or an
         # st.Cache codeblock.
-        self.hash_source: Optional[Callable[..., Any]] = None
+        self.hash_source: Callable[..., Any] | None = None
 
     def __repr__(self) -> str:
         return util.repr_(self)
@@ -147,7 +146,7 @@ class _HashStack:
     def pretty_print(self):
         def to_str(v):
             try:
-                return "Object of type %s: %s" % (type_util.get_fqn_type(v), str(v))
+                return "Object of type {}: {}".format(type_util.get_fqn_type(v), str(v))
             except Exception:
                 return "<Unable to convert item to string>"
 
@@ -272,7 +271,7 @@ def _int_to_bytes(i: int) -> bytes:
     return i.to_bytes(num_bytes, "little", signed=True)
 
 
-def _key(obj: Optional[Any]) -> Any:
+def _key(obj: Any | None) -> Any:
     """Return key for memoization."""
 
     if obj is None:
@@ -315,7 +314,7 @@ def _key(obj: Optional[Any]) -> Any:
 class _CodeHasher:
     """A hasher that can hash code objects including dependencies."""
 
-    def __init__(self, hash_funcs: Optional[HashFuncsDict] = None):
+    def __init__(self, hash_funcs: HashFuncsDict | None = None):
         # Can't use types as the keys in the internal _hash_funcs because
         # we always remove user-written modules from memory when rerunning a
         # script in order to reload it and grab the latest code changes.
@@ -332,7 +331,7 @@ class _CodeHasher:
         else:
             self._hash_funcs = {}
 
-        self._hashes: Dict[Any, bytes] = {}
+        self._hashes: dict[Any, bytes] = {}
 
         # The number of the bytes in the hash.
         self.size = 0
@@ -340,7 +339,7 @@ class _CodeHasher:
     def __repr__(self) -> str:
         return util.repr_(self)
 
-    def to_bytes(self, obj: Any, context: Optional[Context] = None) -> bytes:
+    def to_bytes(self, obj: Any, context: Context | None = None) -> bytes:
         """Add memoization to _to_bytes and protect against cycles in data structures."""
         tname = type(obj).__qualname__.encode()
         key = (tname, _key(obj))
@@ -381,7 +380,7 @@ class _CodeHasher:
 
         return b
 
-    def update(self, hasher, obj: Any, context: Optional[Context] = None) -> None:
+    def update(self, hasher, obj: Any, context: Context | None = None) -> None:
         """Update the provided hasher with the hash of an object."""
         b = self.to_bytes(obj, context)
         hasher.update(b)
@@ -403,7 +402,7 @@ class _CodeHasher:
             filepath, self._get_main_script_directory()
         ) or file_util.file_in_pythonpath(filepath)
 
-    def _to_bytes(self, obj: Any, context: Optional[Context]) -> bytes:
+    def _to_bytes(self, obj: Any, context: Context | None) -> bytes:
         """Hash objects to bytes, including code with dependencies.
 
         Python's built in `hash` does not produce consistent results across
@@ -412,7 +411,9 @@ class _CodeHasher:
 
         h = hashlib.new("md5", **HASHLIB_KWARGS)
 
-        if isinstance(obj, unittest.mock.Mock):
+        if type_util.is_type(obj, "unittest.mock.Mock") or type_util.is_type(
+            obj, "unittest.mock.MagicMock"
+        ):
             # Mock objects can appear to be infinitely
             # deep, so we don't try to hash them at all.
             return self.to_bytes(id(obj))
@@ -615,7 +616,7 @@ class _CodeHasher:
             if obj.__module__.startswith("streamlit"):
                 # Ignore streamlit modules even if they are in the CWD
                 # (e.g. during development).
-                return self.to_bytes("%s.%s" % (obj.__module__, obj.__name__))
+                return self.to_bytes("{}.{}".format(obj.__module__, obj.__name__))
 
             code = getattr(obj, "__code__", None)
             assert code is not None
@@ -710,11 +711,11 @@ class _CodeHasher:
         return str(abs_main_path.parent)
 
 
-def get_referenced_objects(code, context: Context) -> List[Any]:
+def get_referenced_objects(code, context: Context) -> list[Any]:
     # Top of the stack
     tos: Any = None
     lineno = None
-    refs: List[Any] = []
+    refs: list[Any] = []
 
     def set_tos(t):
         nonlocal tos
@@ -729,6 +730,7 @@ def get_referenced_objects(code, context: Context) -> List[Any]:
     # code reads `bar` of `foo`. We are going over the bytecode to resolve
     # from which object an attribute is requested.
     # Read more about bytecode at https://docs.python.org/3/library/dis.html
+    import dis
 
     for op in dis.get_instructions(code):
         try:
@@ -747,6 +749,8 @@ def get_referenced_objects(code, context: Context) -> List[Any]:
                 set_tos(context.cells.values[op.argval])
             elif op.opname == "IMPORT_NAME":
                 try:
+                    import importlib
+
                     set_tos(importlib.import_module(op.argval))
                 except ImportError:
                     set_tos(op.argval)
@@ -785,7 +789,7 @@ class NoResult:
 class UnhashableTypeError(StreamlitAPIException):
     def __init__(self, orig_exc, failed_obj):
         msg = self._get_message(orig_exc, failed_obj)
-        super(UnhashableTypeError, self).__init__(msg)
+        super().__init__(msg)
         self.with_traceback(orig_exc.__traceback__)
 
     def _get_message(self, orig_exc, failed_obj):
@@ -834,7 +838,7 @@ class UserHashError(StreamlitAPIException):
         else:
             msg = self._get_message_from_code(orig_exc, cached_func_or_code, lineno)
 
-        super(UserHashError, self).__init__(msg)
+        super().__init__(msg)
         self.with_traceback(orig_exc.__traceback__)
 
     def _get_message_from_func(self, orig_exc, cached_func, hash_func):
@@ -906,7 +910,7 @@ class InternalHashError(MarkdownFormattedException):
 
     def __init__(self, orig_exc: BaseException, failed_obj: Any):
         msg = self._get_message(orig_exc, failed_obj)
-        super(InternalHashError, self).__init__(msg)
+        super().__init__(msg)
         self.with_traceback(orig_exc.__traceback__)
 
     def _get_message(self, orig_exc: BaseException, failed_obj: Any) -> str:
@@ -949,7 +953,7 @@ for more details.
         ).strip("\n")
 
 
-def _get_error_message_args(orig_exc: BaseException, failed_obj: Any) -> Dict[str, Any]:
+def _get_error_message_args(orig_exc: BaseException, failed_obj: Any) -> dict[str, Any]:
     hash_reason = hash_stacks.current.hash_reason
     hash_source = hash_stacks.current.hash_source
 
@@ -984,7 +988,7 @@ def _get_error_message_args(orig_exc: BaseException, failed_obj: Any) -> Dict[st
     }
 
 
-def _get_failing_lines(code, lineno: int) -> List[str]:
+def _get_failing_lines(code, lineno: int) -> list[str]:
     """Get list of strings (lines of code) from lineno to lineno+3.
 
     Ideally we'd return the exact line where the error took place, but there
diff --git a/lib/streamlit/runtime/stats.py b/lib/streamlit/runtime/stats.py
index 8663ac826..92c60adf9 100644
--- a/lib/streamlit/runtime/stats.py
+++ b/lib/streamlit/runtime/stats.py
@@ -11,13 +11,15 @@
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
 # limitations under the License.
+
+from __future__ import annotations
+
 import itertools
 from abc import abstractmethod
-from typing import List, NamedTuple
+from typing import TYPE_CHECKING, NamedTuple, Protocol, runtime_checkable
 
-from typing_extensions import Protocol, runtime_checkable
-
-from streamlit.proto.openmetrics_data_model_pb2 import Metric as MetricProto
+if TYPE_CHECKING:
+    from streamlit.proto.openmetrics_data_model_pb2 import Metric as MetricProto
 
 
 class CacheStat(NamedTuple):
@@ -42,11 +44,7 @@ class CacheStat(NamedTuple):
     byte_length: int
 
     def to_metric_str(self) -> str:
-        return 'cache_memory_bytes{cache_type="%s",cache="%s"} %s' % (
-            self.category_name,
-            self.cache_name,
-            self.byte_length,
-        )
+        return f'cache_memory_bytes{{cache_type="{self.category_name}",cache="{self.cache_name}"}} {self.byte_length}'
 
     def marshall_metric_proto(self, metric: MetricProto) -> None:
         """Fill an OpenMetrics `Metric` protobuf object."""
@@ -62,13 +60,13 @@ class CacheStat(NamedTuple):
         metric_point.gauge_value.int_value = self.byte_length
 
 
-def group_stats(stats: List[CacheStat]) -> List[CacheStat]:
+def group_stats(stats: list[CacheStat]) -> list[CacheStat]:
     """Group a list of CacheStats by category_name and cache_name and sum byte_length"""
 
     def key_function(individual_stat):
         return individual_stat.category_name, individual_stat.cache_name
 
-    result: List[CacheStat] = []
+    result: list[CacheStat] = []
 
     sorted_stats = sorted(stats, key=key_function)
     grouped_stats = itertools.groupby(sorted_stats, key=key_function)
@@ -87,13 +85,13 @@ def group_stats(stats: List[CacheStat]) -> List[CacheStat]:
 @runtime_checkable
 class CacheStatsProvider(Protocol):
     @abstractmethod
-    def get_stats(self) -> List[CacheStat]:
+    def get_stats(self) -> list[CacheStat]:
         raise NotImplementedError
 
 
 class StatsManager:
     def __init__(self):
-        self._cache_stats_providers: List[CacheStatsProvider] = []
+        self._cache_stats_providers: list[CacheStatsProvider] = []
 
     def register_provider(self, provider: CacheStatsProvider) -> None:
         """Register a CacheStatsProvider with the manager.
@@ -102,9 +100,9 @@ class StatsManager:
         """
         self._cache_stats_providers.append(provider)
 
-    def get_stats(self) -> List[CacheStat]:
+    def get_stats(self) -> list[CacheStat]:
         """Return a list containing all stats from each registered provider."""
-        all_stats: List[CacheStat] = []
+        all_stats: list[CacheStat] = []
         for provider in self._cache_stats_providers:
             all_stats.extend(provider.get_stats())
 
diff --git a/lib/streamlit/testing/v1/util.py b/lib/streamlit/testing/v1/util.py
index 30343c6ea..bf29474bf 100644
--- a/lib/streamlit/testing/v1/util.py
+++ b/lib/streamlit/testing/v1/util.py
@@ -12,15 +12,16 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
+from __future__ import annotations
+
 from contextlib import contextmanager
-from typing import Any, Dict
-from unittest.mock import patch
+from typing import Any
 
 from streamlit import config
 
 
 @contextmanager
-def patch_config_options(config_overrides: Dict[str, Any]):
+def patch_config_options(config_overrides: dict[str, Any]):
     """A context manager that overrides config options. It can
     also be used as a function decorator.
 
@@ -33,6 +34,9 @@ def patch_config_options(config_overrides: Dict[str, Any]):
     ... def test_my_thing():
     ...   assert(config.get_option("server.headless") is True)
     """
+    # Lazy-load for performance reasons.
+    from unittest.mock import patch
+
     mock_get_option = build_mock_config_get_option(config_overrides)
     with patch.object(config, "get_option", new=mock_get_option):
         yield
diff --git a/lib/streamlit/version.py b/lib/streamlit/version.py
index 546551e0d..9473c52ad 100644
--- a/lib/streamlit/version.py
+++ b/lib/streamlit/version.py
@@ -13,15 +13,20 @@
 # limitations under the License.
 
 """Streamlit version utilities."""
+
+from __future__ import annotations
+
 import random
 from importlib.metadata import version as _version
-
-import packaging.version
-from typing_extensions import Final
+from typing import TYPE_CHECKING, Final
 
 import streamlit.logger as logger
 
-_LOGGER = logger.get_logger(__name__)
+if TYPE_CHECKING:
+    from packaging.version import Version
+
+
+_LOGGER: Final = logger.get_logger(__name__)
 
 PYPI_STREAMLIT_URL = "https://pypi.org/pypi/streamlit/json"
 
@@ -33,11 +38,13 @@ CHECK_PYPI_PROBABILITY = 0.10
 STREAMLIT_VERSION_STRING: Final[str] = _version("streamlit")
 
 
-def _version_str_to_obj(version_str) -> packaging.version.Version:
-    return packaging.version.Version(version_str)
+def _version_str_to_obj(version_str: str) -> Version:
+    from packaging.version import Version
+
+    return Version(version_str)
 
 
-def _get_installed_streamlit_version() -> packaging.version.Version:
+def _get_installed_streamlit_version() -> Version:
     """Return the streamlit version string from setup.py.
 
     Returns
@@ -49,7 +56,7 @@ def _get_installed_streamlit_version() -> packaging.version.Version:
     return _version_str_to_obj(STREAMLIT_VERSION_STRING)
 
 
-def _get_latest_streamlit_version(timeout=None):
+def _get_latest_streamlit_version(timeout: float | None = None) -> Version:
     """Request the latest streamlit version string from PyPI.
 
     NB: this involves a network call, so it could raise an error
@@ -77,7 +84,7 @@ def _get_latest_streamlit_version(timeout=None):
     return _version_str_to_obj(version_str)
 
 
-def should_show_new_version_notice():
+def should_show_new_version_notice() -> bool:
     """True if streamlit should show a 'new version!' notice to the user.
 
     We need to make a network call to PyPI to determine the latest streamlit
diff --git a/lib/streamlit/web/server/stats_request_handler.py b/lib/streamlit/web/server/stats_request_handler.py
index 94e476e5c..c318381b8 100644
--- a/lib/streamlit/web/server/stats_request_handler.py
+++ b/lib/streamlit/web/server/stats_request_handler.py
@@ -12,15 +12,18 @@
 # See the License for the specific language governing permissions and
 # limitations under the License.
 
-from typing import List
+from __future__ import annotations
+
+from typing import TYPE_CHECKING
 
 import tornado.web
 
-from streamlit.proto.openmetrics_data_model_pb2 import GAUGE
-from streamlit.proto.openmetrics_data_model_pb2 import MetricSet as MetricSetProto
 from streamlit.runtime.stats import CacheStat, StatsManager
 from streamlit.web.server.server_util import emit_endpoint_deprecation_notice
 
+if TYPE_CHECKING:
+    from streamlit.proto.openmetrics_data_model_pb2 import MetricSet as MetricSetProto
+
 
 class StatsRequestHandler(tornado.web.RequestHandler):
     def initialize(self, stats_manager: StatsManager) -> None:
@@ -56,7 +59,7 @@ class StatsRequestHandler(tornado.web.RequestHandler):
             self.set_status(200)
 
     @staticmethod
-    def _stats_to_text(stats: List[CacheStat]) -> str:
+    def _stats_to_text(stats: list[CacheStat]) -> str:
         metric_type = "# TYPE cache_memory_bytes gauge"
         metric_unit = "# UNIT cache_memory_bytes bytes"
         metric_help = "# HELP Total memory consumed by a cache."
@@ -70,7 +73,13 @@ class StatsRequestHandler(tornado.web.RequestHandler):
         return "\n".join(result)
 
     @staticmethod
-    def _stats_to_proto(stats: List[CacheStat]) -> MetricSetProto:
+    def _stats_to_proto(stats: list[CacheStat]) -> MetricSetProto:
+        # Lazy load the import of this proto message for better performance:
+        from streamlit.proto.openmetrics_data_model_pb2 import GAUGE
+        from streamlit.proto.openmetrics_data_model_pb2 import (
+            MetricSet as MetricSetProto,
+        )
+
         metric_set = MetricSetProto()
 
         metric_family = metric_set.metric_families.add()
