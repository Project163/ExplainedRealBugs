{"url":"https://api.github.com/repos/redis/redis/issues/13018","repository_url":"https://api.github.com/repos/redis/redis","labels_url":"https://api.github.com/repos/redis/redis/issues/13018/labels{/name}","comments_url":"https://api.github.com/repos/redis/redis/issues/13018/comments","events_url":"https://api.github.com/repos/redis/redis/issues/13018/events","html_url":"https://github.com/redis/redis/issues/13018","id":2109453249,"node_id":"I_kwDOAAJhcs59u7PB","number":13018,"title":"Preventing temporary circular replication and slot loss in Redis Cluster Failover","user":{"login":"MagicalLas","id":35015234,"node_id":"MDQ6VXNlcjM1MDE1MjM0","avatar_url":"https://avatars.githubusercontent.com/u/35015234?v=4","gravatar_id":"","url":"https://api.github.com/users/MagicalLas","html_url":"https://github.com/MagicalLas","followers_url":"https://api.github.com/users/MagicalLas/followers","following_url":"https://api.github.com/users/MagicalLas/following{/other_user}","gists_url":"https://api.github.com/users/MagicalLas/gists{/gist_id}","starred_url":"https://api.github.com/users/MagicalLas/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MagicalLas/subscriptions","organizations_url":"https://api.github.com/users/MagicalLas/orgs","repos_url":"https://api.github.com/users/MagicalLas/repos","events_url":"https://api.github.com/users/MagicalLas/events{/privacy}","received_events_url":"https://api.github.com/users/MagicalLas/received_events","type":"User","user_view_type":"public","site_admin":false},"labels":[],"state":"closed","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2024-01-31T07:26:15Z","updated_at":"2024-03-05T01:32:26Z","closed_at":"2024-03-05T01:32:26Z","author_association":"NONE","type":null,"active_lock_reason":null,"sub_issues_summary":{"total":0,"completed":0,"percent_completed":0},"issue_dependencies_summary":{"blocked_by":0,"total_blocked_by":0,"blocking":0,"total_blocking":0},"body":"I've encountered an issue with some nodes in a Redis cluster during a manual failover, where the state seen by certain nodes becomes incorrect.\r\n\r\nThe normal scenario for a failover is as follows:\r\n\r\n```mermaid\r\nsequenceDiagram\r\nactor user\r\nuser->>NodeA: redis-cli cluster failover\r\nNodeA->>NodeB: manual failover start\r\nactivate NodeB\r\nNodeB->>NodeA: ping with offset\r\nNodeB->>NodeA: ping with offset\r\nNodeA->>NodeC: auth failover\r\nNodeC->>NodeA: vote\r\nNote over NodeA: cluster nodes<br/>NodeA: master, 0-100<br/>NodeB: master, 0-0<br/>NodeC: master, 101-200\r\nNodeA->>NodeB: PONG, i'm master, slot 0-100\r\nNote over NodeB: cluster nodes<br/>NodeA: master, 0-100<br/>NodeB: replica of NodeA<br/>NodeC: master, 101-200\r\ndeactivate NodeB\r\nNodeA->>NodeC: PONG, i'm master, slot 0-100\r\nNote over NodeC: cluster nodes<br/>NodeA: master, 0-100<br/>NodeB: master, 0-0<br/>NodeC: master, 101-200\r\nNodeB->>NodeA: PONG, i'm replica\r\nNote over NodeA: cluster nodes<br/>NodeA: master, 0-100<br/>NodeB: replica of NodeA<br/>NodeC: master, 101-200\r\nNodeB->>NodeC: PONG, i'm replica\r\nNote over NodeC: cluster nodes<br/>NodeA: master, 0-100<br/>NodeB: replica of NodeA<br/>NodeC: master, 101-200\r\n```\r\n\r\nHowever, if NodeA's message is not delivered due to network latency or other reasons, the state viewed by NodeC becomes incorrect:\r\n\r\n```mermaid\r\nsequenceDiagram\r\nactor user\r\nuser->>NodeA: redis-cli cluster failover\r\nNodeA->>NodeB: manual failover start\r\nactivate NodeB\r\nNodeB->>NodeA: ping with offset\r\nNodeB->>NodeA: ping with offset\r\nNodeA->>NodeC: auth failover\r\nNodeC->>NodeA: vote\r\nNote over NodeA: cluster nodes<br/>NodeA: master, 0-100<br/>NodeB: master, 0-0<br/>NodeC: master, 101-200\r\nNodeA->>NodeB: PONG, i'm master, slot 0-100\r\nNote over NodeB: cluster nodes<br/>NodeA: master, 0-100<br/>NodeB: replica of NodeA<br/>NodeC: master, 101-200\r\ndeactivate NodeB\r\nNodeB->>NodeA: PONG, i'm replica\r\nNote over NodeA: cluster nodes<br/>NodeA: master, 0-100<br/>NodeB: replica of NodeA<br/>NodeC: master, 101-200\r\nNodeB->>NodeC: PONG, i'm replica\r\nNote over NodeC: cluster nodes<br/>NodeA: replica of NodeB<br/>NodeB: replica of NodeA<br/>NodeC: master, 101-200\r\nNote over NodeA: delayed some reason...\r\nNodeA->>NodeC: PONG, i'm master, slot 0-100\r\nNote over NodeC: cluster nodes<br/>NodeA: master, 0-100<br/>NodeB: replica of NodeA<br/>NodeC: master, 101-200\r\n```\r\n\r\nIn this case, NodeC recognizes NodeA and NodeB as being in a circular replication state, and some slots are lost. This state persists until NodeA sends a PONG to NodeC. This situation can be easily reproduced by dropping packets from NodeA to NodeC using iptables.\r\n\r\nI propose a solution that involves delaying the transition to an incorrect state when a node's status changes are detected. Specifically, if a sender is to become a replica, and the sender still owns slots while the new master is a replica of the sender, then the process of turning the sender into a replica should be delayed. This approach can prevent temporary circular replication and slot loss, as well as avoid additional problems(eg: https://github.com/redis/redis/pull/10489#issuecomment-1728593084 , https://github.com/lettuce-io/lettuce-core/issues/2578). (not sure...)\r\n\r\nThe proposed behavior involves a delay in the transition of a master to a replica in the event of a network partition. However, the scenario where the old master receives the message to become a replica before the message promoting a new master is very rare and unlikely to occur in most situations. Additionally, experiencing 1-2 extra 'moved' errors due to this delay is safer than not being able to find a node at all.\r\n\r\nIf need any further explanation or details about the situation, please let me know.","closed_by":{"login":"madolson","id":34459052,"node_id":"MDQ6VXNlcjM0NDU5MDUy","avatar_url":"https://avatars.githubusercontent.com/u/34459052?v=4","gravatar_id":"","url":"https://api.github.com/users/madolson","html_url":"https://github.com/madolson","followers_url":"https://api.github.com/users/madolson/followers","following_url":"https://api.github.com/users/madolson/following{/other_user}","gists_url":"https://api.github.com/users/madolson/gists{/gist_id}","starred_url":"https://api.github.com/users/madolson/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/madolson/subscriptions","organizations_url":"https://api.github.com/users/madolson/orgs","repos_url":"https://api.github.com/users/madolson/repos","events_url":"https://api.github.com/users/madolson/events{/privacy}","received_events_url":"https://api.github.com/users/madolson/received_events","type":"User","user_view_type":"public","site_admin":false},"reactions":{"url":"https://api.github.com/repos/redis/redis/issues/13018/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/redis/redis/issues/13018/timeline","performed_via_github_app":null,"state_reason":"completed"}