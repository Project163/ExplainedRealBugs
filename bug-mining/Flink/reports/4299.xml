<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:47:41 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-17327] Kafka unavailability could cause Flink TM shutdown</title>
                <link>https://issues.apache.org/jira/browse/FLINK-17327</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;Steps to reproduce:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Start a Flink 1.10 standalone cluster&lt;/li&gt;
	&lt;li&gt;Run a Flink job which reads from one Kafka topic and writes to another topic, with exactly-once checkpointing enabled&lt;/li&gt;
	&lt;li&gt;Stop all Kafka Brokers after a few successful checkpoints&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;When Kafka brokers are down:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;tt&gt;org.apache.kafka.clients.NetworkClient&lt;/tt&gt; reported connection to broker could not be established&lt;/li&gt;
	&lt;li&gt;Then, Flink could not complete snapshot due to &lt;tt&gt;Timeout expired while initializing transactional state in 60000ms&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;After several snapshot failures, Flink reported&#160;&lt;tt&gt;Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;Eventually, Flink tried to cancel the task which did not succeed within 3 min. According to logs, consumer was cancelled, but producer is still running&lt;/li&gt;
	&lt;li&gt;Then &lt;tt&gt;Fatal error occurred while executing the TaskManager. Shutting it down...&lt;/tt&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I will attach the logs to show the details. &#160;Worth to note that if there would be no consumer but producer only in the task, the behavior is different:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;&lt;tt&gt;org.apache.kafka.clients.NetworkClient&lt;/tt&gt; reported connection to broker could not be established&lt;/li&gt;
	&lt;li&gt;after&#160;&lt;tt&gt;delivery.timeout.ms&lt;/tt&gt;&#160;(2min by default), producer reports:&#160;&lt;tt&gt;FlinkKafkaException: Failed to send data to Kafka: Expiring 4 record(s) for output-topic-0:120001 ms has passed since batch creation&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;Flink tried to cancel the upstream tasks and created a new producer&lt;/li&gt;
	&lt;li&gt;The new producer obviously reported connectivity issue to brokers&lt;/li&gt;
	&lt;li&gt;This continues till Kafka brokers are back.&#160;&lt;/li&gt;
	&lt;li&gt;Flink reported&#160;&lt;tt&gt;Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;Flink cancelled the tasks and restarted them&lt;/li&gt;
	&lt;li&gt;The job continues, and new checkpoint succeeded.&#160;&lt;/li&gt;
	&lt;li&gt;TM runs all the time in this scenario&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I set Kafka transaction time out to 1 hour just to avoid transaction timeout during the test.&lt;/p&gt;

&lt;p&gt;To get a producer only task, I called &lt;tt&gt;env.disableOperatorChaining();&lt;/tt&gt; in the second scenario.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13300291">FLINK-17327</key>
            <summary>Kafka unavailability could cause Flink TM shutdown</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="aljoscha">Aljoscha Krettek</assignee>
                                    <reporter username="qinjunjerry">Jun Qin</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Wed, 22 Apr 2020 14:51:23 +0000</created>
                <updated>Fri, 26 Feb 2021 13:42:52 +0000</updated>
                            <resolved>Mon, 15 Jun 2020 17:14:58 +0000</resolved>
                                    <version>1.10.0</version>
                                    <fixVersion>1.11.0</fixVersion>
                                    <component>Connectors / Kafka</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>11</watches>
                                                                                                                <comments>
                            <comment id="17089755" author="qinjunjerry" created="Wed, 22 Apr 2020 15:07:08 +0000"  >&lt;p&gt;&lt;tt&gt;TM.log&lt;/tt&gt; is for a job where Kafka Source and Kafka Sink are in a single task thread&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;TM_producer_only_task.log&lt;/tt&gt; is for a job with three task threads: Kafka Source, map, Kafka Sink&lt;/p&gt;</comment>
                            <comment id="17090018" author="qinjunjerry" created="Wed, 22 Apr 2020 20:31:16 +0000"  >&lt;p&gt;Added also the Standalonesession logs which covers the time frame of both tests. My local standalone cluster is basically one TM with one slot.&#160;&lt;/p&gt;</comment>
                            <comment id="17090378" author="aljoscha" created="Thu, 23 Apr 2020 08:07:45 +0000"  >&lt;p&gt;Isn&apos;t this the expected behaviour? If Flink cannot write to Kafka then the job will fail. Is that what you&apos;re observing?&lt;/p&gt;</comment>
                            <comment id="17090402" author="pnowojski" created="Thu, 23 Apr 2020 08:42:12 +0000"  >&lt;p&gt;I&apos;m analysing this &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt; and there are at least couple of issues here that we need to fix. Once I&apos;m done I will post my findings.&lt;/p&gt;</comment>
                            <comment id="17090576" author="pnowojski" created="Thu, 23 Apr 2020 12:31:36 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=qinjunjerry&quot; class=&quot;user-hover&quot; rel=&quot;qinjunjerry&quot;&gt;qinjunjerry&lt;/a&gt; for submitting the issue. There are multiple issues that contributed together to the symptoms that you have reported. I will list them in the descending priority order:&lt;/p&gt;

&lt;p&gt;1. &lt;b&gt;CRITICAL bug&lt;/b&gt;: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-17350&quot; title=&quot;StreamTask should always fail immediately on failures in synchronous part of a checkpoint&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-17350&quot;&gt;&lt;del&gt;FLINK-17350&lt;/del&gt;&lt;/a&gt; &lt;tt&gt;setTolerableCheckpointFailureNumber(...)&lt;/tt&gt; and its deprecated &lt;tt&gt;setFailTaskOnCheckpointError(...)&lt;/tt&gt; predecessor are implemented incorrectly. Since Flink 1.5 (&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4809&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-4809&lt;/a&gt;) they can lead to operators (and especially sinks with an external state) end up in an inconsistent state. That&apos;s also true even if they are not used, because...&lt;/p&gt;

&lt;p&gt;2. &lt;b&gt;CRITICAL bug&lt;/b&gt;: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-17351&quot; title=&quot;CheckpointCoordinator and CheckpointFailureManager ignores checkpoint timeouts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-17351&quot;&gt;&lt;del&gt;FLINK-17351&lt;/del&gt;&lt;/a&gt; The logic in how &lt;tt&gt;CheckpointCoordinator&lt;/tt&gt; handles checkpoint timeouts is broken. In your &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=qinjunjerry&quot; class=&quot;user-hover&quot; rel=&quot;qinjunjerry&quot;&gt;qinjunjerry&lt;/a&gt; examples, your job should have failed after first checkpoint failure, but checkpoints were time outing on CheckpointCoordinator after 5 seconds, before &lt;tt&gt;FlinkKafkaProducer&lt;/tt&gt; was detecting Kafka failure after 2 minutes. Those timeouts were not checked against &lt;tt&gt;setTolerableCheckpointFailureNumber(...)&lt;/tt&gt; limit, so the job was keep going with many timed out checkpoints. Now funny thing happens: FlinkKafkaProducer detects Kafka failure. Funny thing is that it depends where the failure was detected:&lt;/p&gt;

&lt;p&gt;a) on processing record? no problem, job will failover immediately once failure is detected (in this example after 2 minutes)&lt;br/&gt;
b) on checkpoint? heh, the failure is reported to &lt;tt&gt;CheckpointCoordinator&lt;/tt&gt; &lt;b&gt;and gets ignored, as PendingCheckpoint has already been discarded 2 minutes ago&lt;/b&gt; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; So theoretically the checkpoints can keep failing forever and the job will not restart automatically, unless something else fails.&lt;/p&gt;

&lt;p&gt;Even more funny things can happen if we mix 1. or 2b) with intermittent external system failure. Sink reports an exception, transaction was lost/aborted, Sink is in failed state, but if there will be a happy coincidence that it manages to accept further records, this exception can be lost and all of the records in those failed checkpoints will be lost forever as well. In all of the examples that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=qinjunjerry&quot; class=&quot;user-hover&quot; rel=&quot;qinjunjerry&quot;&gt;qinjunjerry&lt;/a&gt; posted it hasn&#8217;t happened. &lt;tt&gt;FlinkKafkaProducer&lt;/tt&gt; was not able to recover after the initial failure and it was keep throwing exceptions until the job finally failed (but much later then it should have). And that&#8217;s not guaranteed anywhere.&lt;/p&gt;

&lt;p&gt;3. &lt;b&gt;non critical bug&lt;/b&gt;: previously reported &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-16482&quot; title=&quot;Flink Job throw CloseException when call the FlinkKafkaConsumer cancel function&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-16482&quot;&gt;&lt;del&gt;FLINK-16482&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;tt&gt;FlinkKafkaConsumer&lt;/tt&gt; is not gracefully closing:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2020-04-21 17:17:50,612 INFO  org.apache.flink.runtime.taskmanager.Task                     - Attempting to cancel task Source: Custom Source -&amp;gt; Sink: Unnamed (1/1) (8928563344a077ee98377721a2f22790).
2020-04-21 17:17:50,612 INFO  org.apache.flink.runtime.taskmanager.Task                     - Source: Custom Source -&amp;gt; Sink: Unnamed (1/1) (8928563344a077ee98377721a2f22790) switched from RUNNING to CANCELING.
2020-04-21 17:17:50,612 INFO  org.apache.flink.runtime.taskmanager.Task                     - Triggering cancellation of task code Source: Custom Source -&amp;gt; Sink: Unnamed (1/1) (8928563344a077ee98377721a2f22790).
2020-04-21 17:17:50,614 WARN  org.apache.flink.streaming.runtime.tasks.StreamTask           - Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; canceling task.
org.apache.flink.streaming.connectors.kafka.internal.Handover$ClosedException
	at org.apache.flink.streaming.connectors.kafka.internal.Handover.close(Handover.java:182)
	at org.apache.flink.streaming.connectors.kafka.internal.KafkaFetcher.cancel(KafkaFetcher.java:175)
	at org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumerBase.cancel(FlinkKafkaConsumerBase.java:818)
	at org.apache.flink.streaming.api.operators.StreamSource.cancel(StreamSource.java:147)
	at org.apache.flink.streaming.runtime.tasks.SourceStreamTask.cancelTask(SourceStreamTask.java:136)
	at org.apache.flink.streaming.runtime.tasks.StreamTask.cancel(StreamTask.java:602)
	at org.apache.flink.runtime.taskmanager.Task$TaskCanceler.run(Task.java:1355)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;That could be analysed why is it happening, as this is what brought the cluster down (caused TaskExecutor to restart)&lt;/p&gt;

&lt;p&gt;4. You configuration doesn&#8217;t make much sense. You are time outing checkpoints in 5-10s, with similar checkpoint interval, but timeouts in Kafka are probably still on default values, like 120 or 60 seconds:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2020-04-21 17:15:50,585 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator  - Could not complete snapshot 7 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;operator&lt;/span&gt; Source: Custom Source -&amp;gt; Sink: Unnamed (1/1).
org.apache.kafka.common.errors.TimeoutException: Timeout expired &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; initializing transactional state in 60000ms.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;That is not a big issue on it&#8217;s own, it&#8217;s just one of the things that triggered this whole complicated crash (&lt;tt&gt;CheckpointCoordinator&lt;/tt&gt; timeouting checkpoint, which triggered other bugs)&lt;/p&gt;

&lt;p&gt;5. The reason why splitting the job into multiple tasks helped, is probably just a pure lack. In this case FlinkKafkaProducer was failing, triggering cancelation on other tasks which actually completed successfully.&lt;/p&gt;

&lt;p&gt;6.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Too many ongoing snapshots. Increase kafka producers pool size or decrease number of concurrent checkpoints.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Is just an after shock of &lt;tt&gt;FlinkKafkaProducer&lt;/tt&gt; ending up in broken/inconsistent state because of previously ignored errors (points 1. and 2.). Task should have failed much sooner, after first snapshotting failure. But it kept going until it run out of producers in the pool, but that&#8217;s pure coincidence. After the first failure, there was no way to keep going without a data loss, and we were actually lucky that it didn&#8217;t recover and there was some terminal failure after all. (edited) &lt;/p&gt;


&lt;p&gt;As there are 3 different bugs, I will create new tickets for those issues.&lt;/p&gt;</comment>
                            <comment id="17090602" author="pnowojski" created="Thu, 23 Apr 2020 13:07:15 +0000"  >&lt;p&gt;I&apos;m closing this, as it&apos;s a duplicate of an existing bug:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-16482&quot; title=&quot;Flink Job throw CloseException when call the FlinkKafkaConsumer cancel function&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-16482&quot;&gt;&lt;del&gt;FLINK-16482&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
and I&apos;ve created two different tickets&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-17350&quot; title=&quot;StreamTask should always fail immediately on failures in synchronous part of a checkpoint&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-17350&quot;&gt;&lt;del&gt;FLINK-17350&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-17351&quot; title=&quot;CheckpointCoordinator and CheckpointFailureManager ignores checkpoint timeouts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-17351&quot;&gt;&lt;del&gt;FLINK-17351&lt;/del&gt;&lt;/a&gt;&lt;br/&gt;
to track remaining issues.&lt;/p&gt;

&lt;p&gt;If I have missed anything, please feel to re-open this bug report.&lt;/p&gt;</comment>
                            <comment id="17098999" author="aljoscha" created="Mon, 4 May 2020 14:45:58 +0000"  >&lt;p&gt;I&apos;m re-opening for now since I think the KafkaConsumer is working as designed, i.e. &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-16482&quot; title=&quot;Flink Job throw CloseException when call the FlinkKafkaConsumer cancel function&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-16482&quot;&gt;&lt;del&gt;FLINK-16482&lt;/del&gt;&lt;/a&gt; is not a bug (though I don&apos;t like the exception throwing behaviour).&lt;/p&gt;

&lt;p&gt;Btw, the Kafka Producer is stuck on a lock, that&apos;s why the TM is eventually killed:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2020-05-04 16:43:21,297 WARN  org.apache.flink.runtime.taskmanager.Task                     - Task &lt;span class=&quot;code-quote&quot;&gt;&apos;Map -&amp;gt; Sink: Unnamed (1/1)&apos;&lt;/span&gt; did not react to cancelling signal &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; 30 seconds, but is stuck in method:
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
org.apache.kafka.clients.producer.internals.TransactionalRequestResult.await(TransactionalRequestResult.java:50)
org.apache.kafka.clients.producer.KafkaProducer.commitTransaction(KafkaProducer.java:698)
org.apache.flink.streaming.connectors.kafka.internal.FlinkKafkaInternalProducer.commitTransaction(FlinkKafkaInternalProducer.java:103)
org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.recoverAndCommit(FlinkKafkaProducer.java:920)
org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.recoverAndCommit(FlinkKafkaProducer.java:98)
org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.recoverAndCommitInternal(TwoPhaseCommitSinkFunction.java:405)
org.apache.flink.streaming.api.functions.sink.TwoPhaseCommitSinkFunction.initializeState(TwoPhaseCommitSinkFunction.java:358)
org.apache.flink.streaming.connectors.kafka.FlinkKafkaProducer.initializeState(FlinkKafkaProducer.java:1042)
org.apache.flink.streaming.util.functions.StreamingFunctionUtils.tryRestoreFunction(StreamingFunctionUtils.java:178)
org.apache.flink.streaming.util.functions.StreamingFunctionUtils.restoreFunctionState(StreamingFunctionUtils.java:160)
org.apache.flink.streaming.api.operators.AbstractUdfStreamOperator.initializeState(AbstractUdfStreamOperator.java:96)
org.apache.flink.streaming.api.operators.AbstractStreamOperator.initializeState(AbstractStreamOperator.java:284)
org.apache.flink.streaming.runtime.tasks.StreamTask.initializeStateAndOpen(StreamTask.java:989)
org.apache.flink.streaming.runtime.tasks.StreamTask.lambda$beforeInvoke$0(StreamTask.java:453)
org.apache.flink.streaming.runtime.tasks.StreamTask$$Lambda$122/1846623322.run(Unknown Source)
org.apache.flink.streaming.runtime.tasks.StreamTaskActionExecutor$SynchronizedStreamTaskActionExecutor.runThrowing(StreamTaskActionExecutor.java:94)
org.apache.flink.streaming.runtime.tasks.StreamTask.beforeInvoke(StreamTask.java:448)
org.apache.flink.streaming.runtime.tasks.StreamTask.invoke(StreamTask.java:460)
org.apache.flink.runtime.taskmanager.Task.doRun(Task.java:707)
org.apache.flink.runtime.taskmanager.Task.run(Task.java:532)
java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:748)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17099045" author="aljoscha" created="Mon, 4 May 2020 15:31:59 +0000"  >&lt;p&gt;I believe that &lt;tt&gt;TransactionalRequestResult.await()&lt;/tt&gt; is the culprit for the indefinite blocking, the latch is not counted down in the failure case: &lt;a href=&quot;https://github.com/apache/kafka/blob/2.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionalRequestResult.java#L38&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/blob/2.2/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionalRequestResult.java#L38&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I also believe that this bug in Kafka was fixed here as an unrelated change: &lt;a href=&quot;https://github.com/apache/kafka/commit/df13fc93d0aebfe0ecc40dd4af3c5fb19b35f710#diff-8a2c4f47dcec247ce2ecebf082b3d0b1R42&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/kafka/commit/df13fc93d0aebfe0ecc40dd4af3c5fb19b35f710#diff-8a2c4f47dcec247ce2ecebf082b3d0b1R42&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="17099608" author="aljoscha" created="Tue, 5 May 2020 07:37:33 +0000"  >&lt;p&gt;The fix I mentioned is only available on Kafka 2.5.x, so to fix it we should open Kafka Issues and fix it also for earlier versions.&lt;/p&gt;</comment>
                            <comment id="17099611" author="qinjunjerry" created="Tue, 5 May 2020 07:45:28 +0000"  >&lt;p&gt;The scenario I described in the first entry of this Jira&#160;was tested with&#160;kafka_2.12-2.5.0&lt;/p&gt;</comment>
                            <comment id="17099625" author="pnowojski" created="Tue, 5 May 2020 07:59:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=qinjunjerry&quot; class=&quot;user-hover&quot; rel=&quot;qinjunjerry&quot;&gt;qinjunjerry&lt;/a&gt; the bug that &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aljoscha&quot; class=&quot;user-hover&quot; rel=&quot;aljoscha&quot;&gt;aljoscha&lt;/a&gt; is talking about, that blocked the &lt;tt&gt;FlinkKafkaProducer&lt;/tt&gt;, is on the Kafka client&apos;s side, not on the brokers. To fix it, we would have to upgrade Kafka dependencies in our connector to 2.5.0 (currently it&apos;s 2.2.x), which in turn is blocked by &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-15362&quot; title=&quot;Bump Kafka client version to 2.4.1 for universal Kafka connector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-15362&quot;&gt;&lt;del&gt;FLINK-15362&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="17099751" author="aljoscha" created="Tue, 5 May 2020 10:16:28 +0000"  >&lt;p&gt;This is another fix we need: &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7763&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-7763&lt;/a&gt;. It&apos;s only available from 2.3.x onwards.&lt;/p&gt;</comment>
                            <comment id="17099775" author="aljoscha" created="Tue, 5 May 2020 11:00:12 +0000"  >&lt;p&gt;For testing I tried this against Kafka 2.5.0 (which we can&apos;t really do, see &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-15362&quot; title=&quot;Bump Kafka client version to 2.4.1 for universal Kafka connector&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-15362&quot;&gt;&lt;del&gt;FLINK-15362&lt;/del&gt;&lt;/a&gt;). Here the job fails successfully, because we don&apos;t wait indefinitely for the result. However, now we have a leftover Kafka &lt;tt&gt;NetworkClient&lt;/tt&gt; which logs indefinitely:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2020-05-05 12:59:03,465 WARN  org.apache.kafka.clients.NetworkClient                       [] - [Producer clientId=producer-Map -&amp;gt; Sink: Unnamed-c09dc291fad93d575e015871097bfc60-4, transactionalId=Map -&amp;gt; Sink: Unnamed-c09dc291fad93d575e015871097bfc60-4] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also not good, because then your &lt;tt&gt;TaskManagers&lt;/tt&gt; will eventually be full of leftover Kafka threads.&lt;/p&gt;</comment>
                            <comment id="17099777" author="aljoscha" created="Tue, 5 May 2020 11:00:36 +0000"  >&lt;p&gt;I think the Kafka code doesn&apos;t like being interrupted, which Flink does when cancelling.&lt;/p&gt;</comment>
                            <comment id="17099782" author="qinjunjerry" created="Tue, 5 May 2020 11:09:15 +0000"  >&lt;p&gt;What do you mean by &quot;job fails successfully&quot;?&#160;With Flink 1.10, I believe you will get the following exception (after 1 min) which will then trigger the job cancellation:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2020-05-04 16:34:36,262 INFO  org.apache.flink.streaming.api.operators.AbstractStreamOperator  - Could not complete snapshot 2 &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;operator&lt;/span&gt; Source: Custom Source -&amp;gt; Sink: Unnamed (1/1).
org.apache.kafka.common.errors.TimeoutException: Timeout expired &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; initializing transactional state in 60000ms.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17104242" author="aljoscha" created="Mon, 11 May 2020 09:01:01 +0000"  >&lt;p&gt;Btw, this can be reproduced even simpler. A job with just a synthetic source and a &lt;tt&gt;FlinkKafkaProducer&lt;/tt&gt; will also be stuck in cancelling and eventually kill the TM. So it has nothing to do with &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-16482&quot; title=&quot;Flink Job throw CloseException when call the FlinkKafkaConsumer cancel function&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-16482&quot;&gt;&lt;del&gt;FLINK-16482&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="17128272" author="aljoscha" created="Mon, 8 Jun 2020 12:48:41 +0000"  >&lt;p&gt;&lt;b&gt;&lt;font color=&quot;red&quot;&gt;edit: this description is not fully accurate anymore, we don&apos;t need to patch Kafka but can instead call close with a zero timeout. I&apos;m adding a better description below.&lt;/font&gt;&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;I managed to find a fix for this:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;change our code to always use &lt;tt&gt;close()&lt;/tt&gt; with a timeout on the Kafka Producer, if not, we might leave lingering threads&lt;/li&gt;
	&lt;li&gt;this alone does not work because of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7763&quot; title=&quot;KafkaProducer with transactionId endless waits when network is disconnection for 10-20s&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7763&quot;&gt;&lt;del&gt;KAFKA-7763&lt;/del&gt;&lt;/a&gt;, i.e. on shutdown requests are not properly cancelled, which leaves lingering threads&lt;br/&gt;
this alone does not work because of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6635&quot; title=&quot;Producer close does not await pending transaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6635&quot;&gt;&lt;del&gt;KAFKA-6635&lt;/del&gt;&lt;/a&gt;, i.e. on shutdown requests are not properly cancelled, which leaves lingering threads&lt;/li&gt;
	&lt;li&gt;the fix &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6635&quot; title=&quot;Producer close does not await pending transaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6635&quot;&gt;&lt;del&gt;KAFKA-6635&lt;/del&gt;&lt;/a&gt; also introduces code that aborts outstanding transactions when cancelling. This doesn&apos;t work together with our exactly-once Kafka Producer&lt;/li&gt;
	&lt;li&gt;you need a patched Kafka that includes the fix part of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6635&quot; title=&quot;Producer close does not await pending transaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6635&quot;&gt;&lt;del&gt;KAFKA-6635&lt;/del&gt;&lt;/a&gt;, without the code that aborts transactions, I&apos;m attaching a patch for that against the Kafka 2.4 branch&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The changes needed in Flink are here: &lt;a href=&quot;https://github.com/aljoscha/flink/tree/flink-17327-kafka-clean-shutdown-2.4&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/aljoscha/flink/tree/flink-17327-kafka-clean-shutdown-2.4&lt;/a&gt;. Patch for Kafka is attached. I don&apos;t think the Kafka project will like that patch, though, because aborting outstanding transactions is valid for Kafka Streams/KSQL where pending transactions that are not cancelled with block downstream consumption. &lt;/p&gt;</comment>
                            <comment id="17129590" author="aljoscha" created="Tue, 9 Jun 2020 16:44:59 +0000"  >&lt;p&gt;I was mixing up issues before, &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6635&quot; title=&quot;Producer close does not await pending transaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6635&quot;&gt;&lt;del&gt;KAFKA-6635&lt;/del&gt;&lt;/a&gt; has a fix but also introduces the &quot;feature&quot; that transactions are aborted on shutdown.&lt;/p&gt;</comment>
                            <comment id="17130745" author="aljoscha" created="Wed, 10 Jun 2020 14:40:50 +0000"  >&lt;p&gt;For posterity, here&apos;s a summary of the issue:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;our code calls &lt;tt&gt;close()&lt;/tt&gt; on the &lt;tt&gt;KafkaProducer&lt;/tt&gt; (Kafka code), which is equivalent to calling &lt;tt&gt;close&lt;/tt&gt; with a timeout of &lt;tt&gt;Long.MAX_VALUE&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;this means threads will leak when a failure happens, for example because of Broker downtime&lt;/li&gt;
	&lt;li&gt;the Flink Task Watchdog will kill the Task Manager because of these threads after a timeout&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The fix is to always call &lt;tt&gt;close()&lt;/tt&gt; with a reasonable timeout.&lt;/p&gt;

&lt;p&gt;The fix also requires a Kafka version bump because of &lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-6635&quot; title=&quot;Producer close does not await pending transaction&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-6635&quot;&gt;&lt;del&gt;KAFKA-6635&lt;/del&gt;&lt;/a&gt;/&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-7763&quot; title=&quot;KafkaProducer with transactionId endless waits when network is disconnection for 10-20s&quot; class=&quot;issue-link&quot; data-issue-key=&quot;KAFKA-7763&quot;&gt;&lt;del&gt;KAFKA-7763&lt;/del&gt;&lt;/a&gt;, which mean that resources still leak even when closing with a timeout. Additionally, we need to close with exactly zero as timeout, because otherwise in-flight transactions will be aborted.&lt;/p&gt;</comment>
                            <comment id="17136030" author="aljoscha" created="Mon, 15 Jun 2020 17:14:58 +0000"  >&lt;p&gt;release-1.11: 7d4041250dfefeb919b5d2854e993434e8b68401&lt;br/&gt;
master: 711f619d3f61b7cf37b13fefb820edff2199b19d&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                            <outwardlinks description="causes">
                                        <issuelink>
            <issuekey id="13311597">FLINK-18311</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is caused by">
                                        <issuelink>
            <issuekey id="13300558">FLINK-17350</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13300559">FLINK-17351</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13276035">FLINK-15362</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="13005114" name="0001-Change-version-to-2.4.2-ALJOSCHA.patch" size="705" author="aljoscha" created="Mon, 8 Jun 2020 12:49:09 +0000"/>
                            <attachment id="13005115" name="0002-Don-t-abort-in-flight-transactions.patch" size="1577" author="aljoscha" created="Mon, 8 Jun 2020 12:49:10 +0000"/>
                            <attachment id="13000880" name="Standalonesession.log" size="973863" author="qinjunjerry" created="Wed, 22 Apr 2020 20:29:50 +0000"/>
                            <attachment id="13000862" name="TM.log" size="821553" author="qinjunjerry" created="Wed, 22 Apr 2020 15:05:06 +0000"/>
                            <attachment id="13000863" name="TM_produer_only_task.log" size="759976" author="qinjunjerry" created="Wed, 22 Apr 2020 15:05:07 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>5.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            5 years, 22 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0dy8w:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>