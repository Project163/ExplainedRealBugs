<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:27:23 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-3679] Allow Kafka consumer to skip corrupted messages</title>
                <link>https://issues.apache.org/jira/browse/FLINK-3679</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;There are a couple of issues with the DeserializationSchema API that I think should be improved.  This request has come to me via an existing Flink user.&lt;/p&gt;

&lt;p&gt;The main issue is simply that the API assumes that there is a one-to-one mapping between input and outputs.  In reality there are scenarios where one input message (say from Kafka) might actually map to zero or more logical elements in the pipeline.&lt;/p&gt;

&lt;p&gt;Particularly important here is the case where you receive a message from a source (such as Kafka) and say the raw bytes don&apos;t deserialize properly.  Right now the only recourse is to throw IOException and therefore fail the job.  &lt;/p&gt;

&lt;p&gt;This is definitely not good since bad data is a reality and failing the job is not the right option.  If the job fails we&apos;ll just end up replaying the bad data and the whole thing will start again.&lt;/p&gt;

&lt;p&gt;Instead in this case it would be best if the user could just return the empty set.&lt;/p&gt;

&lt;p&gt;The other case is where one input message should logically be multiple output messages.  This case is probably less important since there are other ways to do this but in general it might be good to make the DeserializationSchema.deserialize() method return a collection rather than a single element.&lt;/p&gt;

&lt;p&gt;Maybe we need to support a DeserializationSchema variant that has semantics more like that of FlatMap.&lt;/p&gt;


</description>
                <environment></environment>
        <key id="12954416">FLINK-3679</key>
            <summary>Allow Kafka consumer to skip corrupted messages</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="wheat9">Haohui Mai</assignee>
                                    <reporter username="jgrier">Jamie Grier</reporter>
                        <labels>
                    </labels>
                <created>Tue, 29 Mar 2016 20:36:52 +0000</created>
                <updated>Wed, 2 Oct 2019 17:43:06 +0000</updated>
                            <resolved>Thu, 9 Mar 2017 06:11:51 +0000</resolved>
                                                    <fixVersion>1.3.0</fixVersion>
                                    <component>API / DataStream</component>
                    <component>Connectors / Kafka</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="15224182" author="rmetzger" created="Mon, 4 Apr 2016 14:03:43 +0000"  >&lt;p&gt;I had a quick offline chat about this with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt;. Changing the semantics of the DeserializationSchema to use an OutputCollector would be possible, but it would break existing code, introduce a new class and make the locking / operator chaining of the Kafka consumer code more complicated.&lt;br/&gt;
I wonder if the problems you&apos;ve mentioned can&apos;t be solved with a flatMap() operator. When the Kafka consumer and the flatMap() are executed with the same parallelism, they&apos;ll be chained together and then executed in the same thread with almost no overhead.&lt;br/&gt;
If one Kafka message results in two or more logical messages, that &quot;splitting&quot; can be done in the flatMap() as well. For invalid records, this can also be reflected in the returned record (with a failure flag (some id set to -1 or a bool set to false), or a special field in a JSON record), ...) and then treated accordingly in the flatMap() call.&lt;/p&gt;

&lt;p&gt;If you want, we can keep the JIRA issue open and see if more users run into this. If so, we can reconsider fixing it (I&apos;m not saying I&apos;ve decided against fixing it)&lt;/p&gt;</comment>
                            <comment id="15224692" author="jgrier" created="Mon, 4 Apr 2016 18:12:10 +0000"  >&lt;p&gt;I&apos;m not sure about the locking and operator chaining issues so I would say if that&apos;s unduly complicated because of this change maybe it&apos;s not worth it.  However, a DeserializationSchema with more flatMap() like semantics would certainly be the better API given that bad data issues are a reality.  It also seems we could provide this without breaking existing code, but certainly it would add a bit more complexity to the API (having multiple variants for this).&lt;/p&gt;

&lt;p&gt;Anyway, I agree you can work around this issue my making a special &quot;sentinel&quot; value and dealing with all of this is in a chained flatMap() operator.  I imagine that&apos;s exactly the approach that people are already using.&lt;/p&gt;
</comment>
                            <comment id="15445795" author="rmetzger" created="Mon, 29 Aug 2016 12:56:40 +0000"  >&lt;p&gt;Two users were affected by this recently:&lt;br/&gt;
&lt;a href=&quot;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Handle-deserialization-error-td8724.html#a8725&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Handle-deserialization-error-td8724.html#a8725&lt;/a&gt;&lt;br/&gt;
&lt;a href=&quot;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Handling-Kafka-DeserializationSchema-exceptions-td8700.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Handling-Kafka-DeserializationSchema-exceptions-td8700.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I think we need to fix this issue.&lt;/p&gt;</comment>
                            <comment id="15456204" author="tzulitai" created="Thu, 1 Sep 2016 18:11:24 +0000"  >&lt;p&gt;+1 to fix the issue, the proposed changes seem reasonable and heads towards a better API. The Kinesis consumer will need to adapt to this change as well, as it also accepts DeserializationSchema.&lt;/p&gt;</comment>
                            <comment id="15849633" author="tzulitai" created="Thu, 2 Feb 2017 08:15:08 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wheat9&quot; class=&quot;user-hover&quot; rel=&quot;wheat9&quot;&gt;wheat9&lt;/a&gt;!&lt;br/&gt;
Thank you for picking this JIRA up. How are you doing with this work?&lt;/p&gt;

&lt;p&gt;&lt;del&gt;Since the previous discussion didn&apos;t really come to a conclusion on the API changes for this feature yet, can you briefly describe how you plan to add this? We might need to be extra careful in how the new API works with the state update locking in the consumer.&lt;/del&gt;&lt;br/&gt;
(Sorry, I just realized you have some proposals already in &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5583&quot; title=&quot;Support flexible error handling in the Kafka consumer&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5583&quot;&gt;&lt;del&gt;FLINK-5583&lt;/del&gt;&lt;/a&gt;. I&apos;ll move the API discussion there.)&lt;/p&gt;

&lt;p&gt;Please also feel free to call out to us if you want to jump around some ideas or bump into any problems for this.&lt;/p&gt;</comment>
                            <comment id="15866919" author="githubbot" created="Tue, 14 Feb 2017 23:29:45 +0000"  >&lt;p&gt;GitHub user haohui opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-3679&quot; title=&quot;Allow Kafka consumer to skip corrupted messages&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-3679&quot;&gt;&lt;del&gt;FLINK-3679&lt;/del&gt;&lt;/a&gt; DeserializationSchema should handle zero or more outputs&lt;/p&gt;

&lt;p&gt;    This PR adds a new interface, `RichKeyedDeserializationSchema`, to enable the deserializer to produce zero or more outputs. The main use case is that skipping corrupted messages in the Kafka stream.&lt;/p&gt;

&lt;p&gt;    Feedbacks (especially on backward compatibility) are highly appreciated.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/haohui/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/haohui/flink&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-3679&quot; title=&quot;Allow Kafka consumer to skip corrupted messages&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-3679&quot;&gt;&lt;del&gt;FLINK-3679&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #3314&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 7728acb3bc00a12a7552706be569710fbfdbd200&lt;br/&gt;
Author: Haohui Mai &amp;lt;wheat9@apache.org&amp;gt;&lt;br/&gt;
Date:   2017-02-14T22:19:29Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-3679&quot; title=&quot;Allow Kafka consumer to skip corrupted messages&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-3679&quot;&gt;&lt;del&gt;FLINK-3679&lt;/del&gt;&lt;/a&gt; DeserializationSchema should handle zero or more outputs for every input.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15872506" author="wheat9" created="Fri, 17 Feb 2017 20:54:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tzulitai&quot; class=&quot;user-hover&quot; rel=&quot;tzulitai&quot;&gt;tzulitai&lt;/a&gt; &amp;#8211; would you mind taking a look?&lt;/p&gt;</comment>
                            <comment id="15873009" author="tzulitai" created="Sat, 18 Feb 2017 05:48:13 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wheat9&quot; class=&quot;user-hover&quot; rel=&quot;wheat9&quot;&gt;wheat9&lt;/a&gt;, sure! I&apos;ve noticed your PR, and will schedule some time next week to review it &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
Thank you for the reminder.&lt;/p&gt;</comment>
                            <comment id="15874739" author="githubbot" created="Mon, 20 Feb 2017 16:02:56 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102048475&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102048475&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -373,16 +370,28 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								keyPayload.get(keyBytes);
     							}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final T value = deserializer.deserialize(keyBytes, valueBytes,&lt;/li&gt;
	&lt;li&gt;currentPartition.getTopic(), currentPartition.getPartition(), offset);&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -								// remove partition from subscribed partitions.
    -								partitionsIterator.remove();
    -								continue partitionsLoop;
    -							}&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;owner.emitRecord(value, currentPartition, offset);&lt;br/&gt;
    +							final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I&apos;m not sure of the performance implications for this. The JVM will create a Collector instance for each record read from Kafka.&lt;br/&gt;
    I wonder if we can re-use one collector instance here.&lt;/p&gt;


&lt;p&gt;    Also, I wonder if we need to use this `Collector` implementation, with a `close()` method we are not using and an exception we are turning into a `RuntimeException`. Maybe we should let the collect throw an exception?&lt;/p&gt;</comment>
                            <comment id="15874745" author="githubbot" created="Mon, 20 Feb 2017 16:03:53 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102048656&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102048656&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -142,25 +141,38 @@ public void runFetchLoop() throws Exception {&lt;br/&gt;
     				final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records = handover.pollNext();&lt;/p&gt;

&lt;p&gt;     				// get the records for each topic partition&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition : subscribedPartitions()) {&lt;br/&gt;
    +				for (final KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition : subscribedPartitions()) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     					List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; partitionRecords =&lt;br/&gt;
     							records.records(partition.getKafkaPartitionHandle());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; record : partitionRecords) {&lt;/li&gt;
	&lt;li&gt;final T value = deserializer.deserialize(&lt;/li&gt;
	&lt;li&gt;record.key(), record.value(),&lt;/li&gt;
	&lt;li&gt;record.topic(), record.partition(), record.offset());&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -							// end of stream signaled
    -							running = false;
    -							break;
    -						}
&lt;p&gt;    -&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// emit the actual record. this also updates offset state atomically&lt;/li&gt;
	&lt;li&gt;// and deals with timestamps and watermark generation&lt;/li&gt;
	&lt;li&gt;emitRecord(value, partition, record.offset(), record);&lt;br/&gt;
    +					for (final ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; record : partitionRecords) {&lt;br/&gt;
    +						final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Same question as in the Kafka 0.8 impl&lt;/p&gt;</comment>
                            <comment id="15874748" author="githubbot" created="Mon, 20 Feb 2017 16:04:42 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thank you for opening a pull request.&lt;br/&gt;
    I think the change is missing an update to the documentation. I did a very very superficial review of the change &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt; This needs a more thorough check.&lt;/p&gt;</comment>
                            <comment id="15876573" author="githubbot" created="Tue, 21 Feb 2017 19:43:52 +0000"  >&lt;p&gt;Github user haohui commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102299038&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102299038&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -373,16 +370,28 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								keyPayload.get(keyBytes);
     							}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final T value = deserializer.deserialize(keyBytes, valueBytes,&lt;/li&gt;
	&lt;li&gt;currentPartition.getTopic(), currentPartition.getPartition(), offset);&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -								// remove partition from subscribed partitions.
    -								partitionsIterator.remove();
    -								continue partitionsLoop;
    -							}&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;owner.emitRecord(value, currentPartition, offset);&lt;br/&gt;
    +							final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Totally agree. Playing around a little bit and it might require some trade-offs here.&lt;/p&gt;

&lt;p&gt;    The problem is that `emitRecord()` needs the state for each records (e.g., topic partition, offset, etc.). The state can be either passed inside a closure (like the new instance for the `Collector`) or passed through arguments. I see there are three possibilities here:&lt;/p&gt;

&lt;p&gt;    1. Create a new instance of `Collector` for every record. The JVM may or may not be able to optimize it. Trace-based JVM should be able to but I&apos;m not sure about classed-based JVM.&lt;/p&gt;

&lt;p&gt;    2. Expose the internal state in the `collect()` call. The `collect()` call takes additional parameters such as offset and partition state. It reduces the GC overheads but also hinders changing the implementation.&lt;/p&gt;

&lt;p&gt;    3. Create a new interface like `Optional&amp;lt;T&amp;gt; deserialize(byte[] messageKey, ...)` (or&lt;br/&gt;
    `void deserialize(byte[] messageKey, ..., AtomicReference&amp;lt;T&amp;gt; result)` to optimize away the cost of the `Optional` class). It results in a slightly more complex APIs but it probably has the best trade-offs between performances and API compatibility.&lt;/p&gt;

&lt;p&gt;    What do you think?&lt;/p&gt;
</comment>
                            <comment id="15878971" author="githubbot" created="Wed, 22 Feb 2017 18:52:55 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102542018&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102542018&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -373,16 +370,28 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								keyPayload.get(keyBytes);
     							}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final T value = deserializer.deserialize(keyBytes, valueBytes,&lt;/li&gt;
	&lt;li&gt;currentPartition.getTopic(), currentPartition.getPartition(), offset);&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -								// remove partition from subscribed partitions.
    -								partitionsIterator.remove();
    -								continue partitionsLoop;
    -							}&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;owner.emitRecord(value, currentPartition, offset);&lt;br/&gt;
    +							final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    @StephanEwen What is your opinion on solving this problem?&lt;/p&gt;
</comment>
                            <comment id="15880142" author="githubbot" created="Thu, 23 Feb 2017 08:59:52 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102665687&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102665687&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -373,16 +370,28 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								keyPayload.get(keyBytes);
     							}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final T value = deserializer.deserialize(keyBytes, valueBytes,&lt;/li&gt;
	&lt;li&gt;currentPartition.getTopic(), currentPartition.getPartition(), offset);&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -								// remove partition from subscribed partitions.
    -								partitionsIterator.remove();
    -								continue partitionsLoop;
    -							}&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;owner.emitRecord(value, currentPartition, offset);&lt;br/&gt;
    +							final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Moving the discussion back a bit:&lt;/p&gt;

&lt;p&gt;    I don&apos;t think this implementation works correctly with exactly-once and how we checkpoint the consumer&apos;s partition offset state.&lt;/p&gt;

&lt;p&gt;    The problem is that, in `emitRecord`, we will be updating the offset state. In the changes here, what this means is that we will be considering a record to have been fully processed as soon as the collector collects something.&lt;/p&gt;

&lt;p&gt;    For example, lets say the serializer will call `collect` 3 times for elements deserialized from record R before `deserialize` returns. R has offset 100L. As soon as the first element is collected, the state will be updated to `finished processing offset 100L`. If now checkpointing is triggered, and we use that checkpoint to restore, we will be skipping the remaining 2 elements that were yet to be collected.&lt;br/&gt;
    Once &lt;/p&gt;


</comment>
                            <comment id="15880154" author="githubbot" created="Thu, 23 Feb 2017 09:13:30 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102668004&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102668004&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -373,16 +370,28 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								keyPayload.get(keyBytes);
     							}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final T value = deserializer.deserialize(keyBytes, valueBytes,&lt;/li&gt;
	&lt;li&gt;currentPartition.getTopic(), currentPartition.getPartition(), offset);&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -								// remove partition from subscribed partitions.
    -								partitionsIterator.remove();
    -								continue partitionsLoop;
    -							}&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;owner.emitRecord(value, currentPartition, offset);&lt;br/&gt;
    +							final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    What I think we should do to solve this correctly:&lt;/p&gt;

&lt;p&gt;    Buffer the elements collected from the `deserialize` call. The `Collector.collect` implementation should simply add the collected element to the buffer, and not emit it immediately.&lt;/p&gt;

&lt;p&gt;    After `deserialize` returns, call `emitRecord` once with all the elements in the buffer and the original record&apos;s offset. This, of course, would mean we need to slightly change the `emitRecord` implementation a bit to something like:&lt;br/&gt;
    ```&lt;br/&gt;
    void emitRecord(List&amp;lt;T&amp;gt; records, KafkaTopicPartitionState&amp;lt;KPH&amp;gt; partitionState, long offset) {&lt;br/&gt;
        synchronized (checkpointLock) {&lt;br/&gt;
            for (T record : records) &lt;/p&gt;
{
                sourceContext.collect(record);
            }
&lt;p&gt;            partitionState.setOffset(offset);&lt;br/&gt;
        }&lt;br/&gt;
    }&lt;br/&gt;
    ```&lt;/p&gt;

&lt;p&gt;    After this, we proceed with the next record and repeat. Note that the emitting of all produced elements from record at offset 100L and the update to the offset state to 100L happens atomically synchronized on the checkpoint lock,  so we can make sure that a checkpoint barrier will only come either after or before all the produced records of offset 100, and not in-between.&lt;/p&gt;

&lt;p&gt;    I think we should also be able to avoid a per-record `Collector` with this solution. We can reuse a `Collector` and provide it to the `deserializer` for every record, because it&apos;s simply only a means to collect elements to the internal buffer and we&apos;re not calling `emitRecords` in it.&lt;/p&gt;</comment>
                            <comment id="15881376" author="githubbot" created="Thu, 23 Feb 2017 22:03:25 +0000"  >&lt;p&gt;Github user haohui commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102830609&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102830609&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -373,16 +370,28 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								keyPayload.get(keyBytes);
     							}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final T value = deserializer.deserialize(keyBytes, valueBytes,&lt;/li&gt;
	&lt;li&gt;currentPartition.getTopic(), currentPartition.getPartition(), offset);&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -								// remove partition from subscribed partitions.
    -								partitionsIterator.remove();
    -								continue partitionsLoop;
    -							}&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;owner.emitRecord(value, currentPartition, offset);&lt;br/&gt;
    +							final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Good catch, @tzulitai !&lt;/p&gt;

&lt;p&gt;    I tried the buffer approach and had no luck. The problem is that calling `emitRecord`needs to pass in both the offset and the record itself &amp;#8211; The record is used to extract the timestamp in the Kafka 0.10 consumers. The buffer itself needs to buffer the deserialized value and the record itself &amp;#8211; it cannot solve the problem of having a collector per record.&lt;/p&gt;</comment>
                            <comment id="15881986" author="githubbot" created="Fri, 24 Feb 2017 05:20:43 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102881092&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102881092&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -373,16 +370,28 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								keyPayload.get(keyBytes);
     							}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final T value = deserializer.deserialize(keyBytes, valueBytes,&lt;/li&gt;
	&lt;li&gt;currentPartition.getTopic(), currentPartition.getPartition(), offset);&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -								// remove partition from subscribed partitions.
    -								partitionsIterator.remove();
    -								continue partitionsLoop;
    -							}&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;owner.emitRecord(value, currentPartition, offset);&lt;br/&gt;
    +							final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    @haohui hmm this seems a bit odd to me. I think it should be achievable.&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    // the buffer; this can be shared&lt;br/&gt;
    final List&amp;lt;T&amp;gt; bufferedElements = new LinkedList&amp;lt;&amp;gt;();&lt;br/&gt;
    // BufferCollector is an implementation of Collector that adds collected elements to bufferedElements; this can be shared&lt;br/&gt;
    final BufferCollector collector = new BufferCollector&amp;lt;T&amp;gt;(bufferedElements);&lt;/p&gt;

&lt;p&gt;    ...&lt;/p&gt;

&lt;p&gt;    for (final ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; record : partitionRecords) &lt;/p&gt;
{
        deserializer.deserialize(
            record.key(), record.value(), record.topic(),
            record.partition(), record.offset(), collector);
    
        emitRecords(bufferedElements, partitionState, record.offset(), record);
    
        bufferedElements.clear(); // after the elements for the record have been emitted, empty out the buffer
    }
&lt;p&gt;    ```&lt;/p&gt;

&lt;p&gt;    Doesn&apos;t this work? I haven&apos;t really tried this hands-on, so I might be overlooking something. Let me know what you think &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15881989" author="githubbot" created="Fri, 24 Feb 2017 05:23:51 +0000"  >&lt;p&gt;Github user haohui commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102881264&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102881264&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -373,16 +370,28 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								keyPayload.get(keyBytes);
     							}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final T value = deserializer.deserialize(keyBytes, valueBytes,&lt;/li&gt;
	&lt;li&gt;currentPartition.getTopic(), currentPartition.getPartition(), offset);&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -								// remove partition from subscribed partitions.
    -								partitionsIterator.remove();
    -								continue partitionsLoop;
    -							}&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;owner.emitRecord(value, currentPartition, offset);&lt;br/&gt;
    +							final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I see what you are saying. The trade off here is handing offs the objects another time, but I think it&apos;s okay. I&apos;ll update the PR accordingly.&lt;/p&gt;</comment>
                            <comment id="15881992" author="githubbot" created="Fri, 24 Feb 2017 05:28:35 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102881632&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102881632&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -373,16 +370,28 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								keyPayload.get(keyBytes);
     							}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final T value = deserializer.deserialize(keyBytes, valueBytes,&lt;/li&gt;
	&lt;li&gt;currentPartition.getTopic(), currentPartition.getPartition(), offset);&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -								// remove partition from subscribed partitions.
    -								partitionsIterator.remove();
    -								continue partitionsLoop;
    -							}&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;owner.emitRecord(value, currentPartition, offset);&lt;br/&gt;
    +							final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    @haohui, if you don&apos;t mind, I would also wait for @rmetzger to take another look at the new proposals here, before you jump back again into the code.&lt;br/&gt;
    This part is quite critical for Flink Kafka&apos;s exacty-once guarantee, so another pair of eyes on this will be safer.&lt;/p&gt;

&lt;p&gt;    I would also like to do a thorough pass on your code and see if there are other problems, so you work on those all-together.&lt;/p&gt;

&lt;p&gt;    Is that ok for you? Sorry for some more waiting.&lt;/p&gt;</comment>
                            <comment id="15882258" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102901299&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102901299&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaConsumerTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -1236,10 +1237,11 @@ public Tuple2WithTopicSchema(ExecutionConfig ec) {&lt;br/&gt;
     		}&lt;/p&gt;

&lt;p&gt;     		@Override&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public Tuple3&amp;lt;Integer, Integer, String&amp;gt; deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset) throws IOException {&lt;br/&gt;
    +		public void deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset,&lt;br/&gt;
    +								Collector&amp;lt;Tuple3&amp;lt;Integer, Integer, String&amp;gt;&amp;gt; collector) throws IOException {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Same here: the indentation formatting seems off.&lt;/p&gt;</comment>
                            <comment id="15882259" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102898788&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102898788&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/util/serialization/RichKeyedDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,54 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +package org.apache.flink.streaming.util.serialization;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ResultTypeQueryable;&lt;br/&gt;
    +import org.apache.flink.util.Collector;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.IOException;&lt;br/&gt;
    +import java.io.Serializable;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * RichDeserializationSchema describes how to turn byte key / value messages into zero or more messages into data types.&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The name of the class is `RichKeyedDeserializationSchema `, but in the Javadocs it mentions `RichDeserializationSchema `.&lt;/p&gt;</comment>
                            <comment id="15882260" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102900238&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102900238&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/util/serialization/RichKeyedDeserializationSchemaWrapper.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,50 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +package org.apache.flink.streaming.util.serialization;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.TypeInformation;&lt;br/&gt;
    +import org.apache.flink.util.Collector;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.IOException;&lt;br/&gt;
    +&lt;br/&gt;
    +public class RichKeyedDeserializationSchemaWrapper&amp;lt;T&amp;gt; implements RichKeyedDeserializationSchema&amp;lt;T&amp;gt; {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Can you also include Javadocs for this class?&lt;/p&gt;</comment>
                            <comment id="15882261" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102898901&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102898901&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/util/serialization/RichKeyedDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,54 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +package org.apache.flink.streaming.util.serialization;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ResultTypeQueryable;&lt;br/&gt;
    +import org.apache.flink.util.Collector;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.IOException;&lt;br/&gt;
    +import java.io.Serializable;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * RichDeserializationSchema describes how to turn byte key / value messages into zero or more messages into data types.&lt;br/&gt;
    + * &lt;/p&gt;
{@see KeyedSerializationSchema}
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I&apos;m not sure why we need to link to `KeyedSerializationSchema` in the Javadocs for the new serialization schema.&lt;br/&gt;
    From what I know, we&apos;re going to completely replace it, correct?&lt;/p&gt;</comment>
                            <comment id="15882263" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102899179&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102899179&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/util/serialization/RichKeyedDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,54 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +package org.apache.flink.streaming.util.serialization;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ResultTypeQueryable;&lt;br/&gt;
    +import org.apache.flink.util.Collector;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.IOException;&lt;br/&gt;
    +import java.io.Serializable;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * RichDeserializationSchema describes how to turn byte key / value messages into zero or more messages into data types.&lt;br/&gt;
    + * &lt;/p&gt;
{@see KeyedSerializationSchema}
&lt;p&gt;    + *&lt;br/&gt;
    + * @param &amp;lt;T&amp;gt; The type created by the keyed deserialization schema.&lt;br/&gt;
    + */&lt;br/&gt;
    +public interface RichKeyedDeserializationSchema&amp;lt;T&amp;gt; extends Serializable, ResultTypeQueryable&amp;lt;T&amp;gt; {&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Deserializes the byte message.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param messageKey the key as a byte array (null if no key has been set)&lt;br/&gt;
    +	 * @param message The message, as a byte array. (null if the message was empty or deleted)&lt;br/&gt;
    +	 * @param partition The partition the message has originated from&lt;br/&gt;
    +	 * @param offset the offset of the message in the original source (for example the Kafka offset)&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @return The deserialized message as an object.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	void deserialize(byte[] messageKey, byte[] message, String topic, int partition, long offset,&lt;br/&gt;
    +						Collector&amp;lt;T&amp;gt; collector) throws IOException;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The indentation of the parameters here seems a bit off.&lt;br/&gt;
    Now with the number of parameters to be quite lengthy, it might be a good style to have one parameter per line.&lt;/p&gt;</comment>
                            <comment id="15882262" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102900986&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102900986&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -142,25 +141,38 @@ public void runFetchLoop() throws Exception {&lt;br/&gt;
     				final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records = handover.pollNext();&lt;/p&gt;

&lt;p&gt;     				// get the records for each topic partition&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition : subscribedPartitions()) {&lt;br/&gt;
    +				for (final KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition : subscribedPartitions()) {&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     					List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; partitionRecords =&lt;br/&gt;
     							records.records(partition.getKafkaPartitionHandle());&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;for (ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; record : partitionRecords) {&lt;/li&gt;
	&lt;li&gt;final T value = deserializer.deserialize(&lt;/li&gt;
	&lt;li&gt;record.key(), record.value(),&lt;/li&gt;
	&lt;li&gt;record.topic(), record.partition(), record.offset());&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;if (deserializer.isEndOfStream(value)) 
{
    -							// end of stream signaled
    -							running = false;
    -							break;
    -						}
&lt;p&gt;    -&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// emit the actual record. this also updates offset state atomically&lt;/li&gt;
	&lt;li&gt;// and deals with timestamps and watermark generation&lt;/li&gt;
	&lt;li&gt;emitRecord(value, partition, record.offset(), record);&lt;br/&gt;
    +					for (final ConsumerRecord&amp;lt;byte[], byte[]&amp;gt; record : partitionRecords) {&lt;br/&gt;
    +						final Collector&amp;lt;T&amp;gt; collector = new Collector&amp;lt;T&amp;gt;() {&lt;br/&gt;
    +							@Override&lt;br/&gt;
    +							public void collect(T value) {&lt;br/&gt;
    +								if (deserializer.isEndOfStream(value)) 
{
    +									// end of stream signaled
    +									running = false;
    +								}
&lt;p&gt; else &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +									// emit the actual record. this also updates offset state atomically    +									// and deals with timestamps and watermark generation    +									try {
    +										emitRecord(value, partition, record.offset(), record);
    +									} catch (Exception e) {
    +										throw new RuntimeException(e);
    +									}    +								}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;    +							}&lt;br/&gt;
    +&lt;br/&gt;
    +							@Override&lt;br/&gt;
    +							public void close() &lt;/p&gt;
{
    +
    +							}
&lt;p&gt;    +						};&lt;br/&gt;
    +&lt;br/&gt;
    +						deserializer.deserialize(&lt;br/&gt;
    +							record.key(), record.value(),&lt;br/&gt;
    +							record.topic(), record.partition(), record.offset(), collector);&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    The formatting for the list of arguments here could be nicer. Perhaps one argument per line?&lt;/p&gt;</comment>
                            <comment id="15882264" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102896783&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102896783&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer08.java &amp;#8212;&lt;br/&gt;
    @@ -176,7 +177,7 @@ public FlinkKafkaConsumer08(List&amp;lt;String&amp;gt; topics, DeserializationSchema&amp;lt;T&amp;gt; deseri&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param props&lt;/li&gt;
	&lt;li&gt;The properties that are used to configure both the fetcher and the offset handler.&lt;br/&gt;
     	 */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public FlinkKafkaConsumer08(List&amp;lt;String&amp;gt; topics, KeyedDeserializationSchema&amp;lt;T&amp;gt; deserializer, Properties props) {&lt;br/&gt;
    +	public FlinkKafkaConsumer08(List&amp;lt;String&amp;gt; topics, RichKeyedDeserializationSchema&amp;lt;T&amp;gt; deserializer, Properties props) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    This will break user-code. We&apos;ll need proper usage migration here.&lt;/p&gt;

&lt;p&gt;    We have a separate JIRA that aims at deprecating the current Kafka Consumer constructors: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5704&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-5704&lt;/a&gt;. The migration to use the new flat-map deserialzer can be included there.&lt;/p&gt;

&lt;p&gt;    Perhaps for this PR, we should just use your `RichKeyedDeserializationSchemaWrapper` as &quot;behaviour bridges&quot; for the original deserialization schema to the new one, and don&apos;t change the original constructor / include new constructors yet, so that we don&apos;t overlap and complicate things for &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5704&quot; title=&quot;Deprecate FlinkKafkaConsumer constructors in favor of improvements to decoupling from Kafka offset committing&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5704&quot;&gt;&lt;del&gt;FLINK-5704&lt;/del&gt;&lt;/a&gt;. &lt;/p&gt;</comment>
                            <comment id="15882265" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102898307&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102898307&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -422,6 +429,99 @@ public void run() &lt;/p&gt;
{
     		assertFalse(&quot;fetcher threads did not properly finish&quot;, sourceContext.isStillBlocking());
     	}

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testRichDeserializationSchema() throws Exception {&lt;br/&gt;
    +		final String topic = &quot;test-topic&quot;;&lt;br/&gt;
    +		final int partition = 3;&lt;br/&gt;
    +		final byte[] payload = new byte[] &lt;/p&gt;
{1, 2, 3, 4}
&lt;p&gt;;&lt;br/&gt;
    +		final byte[] endPayload = &quot;end&quot;.getBytes(StandardCharsets.UTF_8);&lt;br/&gt;
    +&lt;br/&gt;
    +		final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records = Arrays.asList(&lt;br/&gt;
    +			new ConsumerRecord&amp;lt;&amp;gt;(topic, partition, 15, payload, payload),&lt;br/&gt;
    +			new ConsumerRecord&amp;lt;&amp;gt;(topic, partition, 16, payload, payload),&lt;br/&gt;
    +			new ConsumerRecord&amp;lt;&amp;gt;(topic, partition, 17, payload, endPayload));&lt;br/&gt;
    +&lt;br/&gt;
    +		final Map&amp;lt;TopicPartition, List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt;&amp;gt; data = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		data.put(new TopicPartition(topic, partition), records);&lt;br/&gt;
    +&lt;br/&gt;
    +		final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; consumerRecords = new ConsumerRecords&amp;lt;&amp;gt;(data);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- the test consumer -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final KafkaConsumer&amp;lt;?, ?&amp;gt; mockConsumer = mock(KafkaConsumer.class);&lt;br/&gt;
    +		when(mockConsumer.poll(anyLong())).thenAnswer(new Answer&amp;lt;ConsumerRecords&amp;lt;?, ?&amp;gt;&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public ConsumerRecords&amp;lt;?, ?&amp;gt; answer(InvocationOnMock invocation) &lt;/p&gt;
{
    +				return consumerRecords;
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		whenNew(KafkaConsumer.class).withAnyArguments().thenReturn(mockConsumer);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- build a fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		ArrayList&amp;lt;String&amp;gt; results = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
    +		SourceContext&amp;lt;String&amp;gt; sourceContext = new CollectingSourceContext&amp;lt;&amp;gt;(results, results);&lt;br/&gt;
    +		List&amp;lt;KafkaTopicPartition&amp;gt; topics = Collections.singletonList(new KafkaTopicPartition(topic, partition));&lt;br/&gt;
    +		RichKeyedDeserializationSchema&amp;lt;String&amp;gt; schema = new RichKeyedDeserializationSchema&amp;lt;String&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void deserialize(&lt;br/&gt;
    +				byte[] messageKey, byte[] message, String topic, int partition,&lt;br/&gt;
    +				long offset, Collector&amp;lt;String&amp;gt; collector) throws IOException {&lt;br/&gt;
    +				if (offset != 16) &lt;/p&gt;
{
    +					collector.collect(new String(message));
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public boolean isEndOfStream(String nextElement) &lt;/p&gt;
{
    +				return nextElement.equals(&quot;end&quot;);
    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public TypeInformation&amp;lt;String&amp;gt; getProducedType() &lt;/p&gt;
{
    +				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt;    +		};&lt;br/&gt;
    +&lt;br/&gt;
    +		final Kafka09Fetcher&amp;lt;String&amp;gt; fetcher = new Kafka09Fetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +			sourceContext,&lt;br/&gt;
    +			topics,&lt;br/&gt;
    +			null, /* no restored state */&lt;br/&gt;
    +			null, /* periodic watermark extractor */&lt;br/&gt;
    +			null, /* punctuated watermark extractor */&lt;br/&gt;
    +			new TestProcessingTimeService(),&lt;br/&gt;
    +			10, /* watermark interval */&lt;br/&gt;
    +			this.getClass().getClassLoader(),&lt;br/&gt;
    +			false, /* checkpointing */&lt;br/&gt;
    +			&quot;task_name&quot;,&lt;br/&gt;
    +			new UnregisteredMetricsGroup(),&lt;br/&gt;
    +			schema,&lt;br/&gt;
    +			new Properties(),&lt;br/&gt;
    +			0L,&lt;br/&gt;
    +			StartupMode.GROUP_OFFSETS,&lt;br/&gt;
    +			false);&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final AtomicReference&amp;lt;Throwable&amp;gt; error = new AtomicReference&amp;lt;&amp;gt;();&lt;br/&gt;
    +		final Thread fetcherRunner = new Thread(&quot;fetcher runner&quot;) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We have a nice utility `CheckedThread` that serves for the tested purpose here (catching errors and storing its reference).&lt;/p&gt;</comment>
                            <comment id="15882266" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102896891&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102896891&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer010.java &amp;#8212;&lt;br/&gt;
    @@ -121,7 +122,7 @@ public FlinkKafkaConsumer010(List&amp;lt;String&amp;gt; topics, DeserializationSchema&amp;lt;T&amp;gt; deser&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param props&lt;/li&gt;
	&lt;li&gt;The properties that are used to configure both the fetcher and the offset handler.&lt;br/&gt;
     	 */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public FlinkKafkaConsumer010(List&amp;lt;String&amp;gt; topics, KeyedDeserializationSchema&amp;lt;T&amp;gt; deserializer, Properties props) {&lt;br/&gt;
    +	public FlinkKafkaConsumer010(List&amp;lt;String&amp;gt; topics, RichKeyedDeserializationSchema&amp;lt;T&amp;gt; deserializer, Properties props) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Same as in the comment in `FlinkKafkaConsumer08`: this breaks user code.&lt;/p&gt;</comment>
                            <comment id="15882267" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102900064&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102900064&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/util/serialization/RichKeyedDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,54 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +package org.apache.flink.streaming.util.serialization;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Since this is now a very Kafka-specific class, I think this is good timing to change to the package path `org.apache.flink.streaming.kafka.serialization` now.&lt;/p&gt;

&lt;p&gt;    The original `KeyedDeserializationSchema` was placed under `o.a.f.s.util.serialization` because it was wrongly packaged in another module before, and moved to `flink-connector-kafka-base` under the same package path to avoid breaking user code.&lt;/p&gt;</comment>
                            <comment id="15882268" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102901123&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102901123&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumerBaseMigrationTest.java &amp;#8212;&lt;br/&gt;
    @@ -171,8 +171,9 @@ private static String getResourceFilename(String filename) {&lt;br/&gt;
     		private final List&amp;lt;KafkaTopicPartition&amp;gt; partitions;&lt;/p&gt;

&lt;p&gt;     		@SuppressWarnings(&quot;unchecked&quot;)&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;DummyFlinkKafkaConsumer(List&amp;lt;KafkaTopicPartition&amp;gt; partitions) {&lt;/li&gt;
	&lt;li&gt;super(Arrays.asList(&quot;dummy-topic&quot;), (KeyedDeserializationSchema&amp;lt; T &amp;gt;) mock(KeyedDeserializationSchema.class));&lt;br/&gt;
    +		DummyFlinkKafkaConsumer(&lt;br/&gt;
    +				List&amp;lt;KafkaTopicPartition&amp;gt; partitions) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    If its just one parameter, I don&apos;t think we need a new line.&lt;/p&gt;</comment>
                            <comment id="15882269" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102897911&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102897911&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -422,6 +429,99 @@ public void run() &lt;/p&gt;
{
     		assertFalse(&quot;fetcher threads did not properly finish&quot;, sourceContext.isStillBlocking());
     	}

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testRichDeserializationSchema() throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I think we should enhance this test to test the behaviour with multiple `collect`s per record also.&lt;/p&gt;</comment>
                            <comment id="15882270" author="githubbot" created="Fri, 24 Feb 2017 09:04:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102900820&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102900820&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaConsumer08.java &amp;#8212;&lt;br/&gt;
    @@ -176,7 +177,7 @@ public FlinkKafkaConsumer08(List&amp;lt;String&amp;gt; topics, DeserializationSchema&amp;lt;T&amp;gt; deseri&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;@param props&lt;/li&gt;
	&lt;li&gt;The properties that are used to configure both the fetcher and the offset handler.&lt;br/&gt;
     	 */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public FlinkKafkaConsumer08(List&amp;lt;String&amp;gt; topics, KeyedDeserializationSchema&amp;lt;T&amp;gt; deserializer, Properties props) {&lt;br/&gt;
    +	public FlinkKafkaConsumer08(List&amp;lt;String&amp;gt; topics, RichKeyedDeserializationSchema&amp;lt;T&amp;gt; deserializer, Properties props) {&lt;br/&gt;
     		super(topics, deserializer);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    So, instead of changing the constructor, we should still do&lt;br/&gt;
    `super(topics, new RickKeyedDeserializationSchemaWrapper(deserializer))`&lt;br/&gt;
    here.&lt;/p&gt;</comment>
                            <comment id="15882276" author="githubbot" created="Fri, 24 Feb 2017 09:07:09 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r102902685&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r102902685&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka010FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -119,7 +119,7 @@ public Void answer(InvocationOnMock invocation) {&lt;br/&gt;
             @SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
             SourceContext&amp;lt;String&amp;gt; sourceContext = mock(SourceContext.class);&lt;br/&gt;
             List&amp;lt;KafkaTopicPartition&amp;gt; topics = Collections.singletonList(new KafkaTopicPartition(&quot;test&quot;, 42));&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KeyedDeserializationSchema&amp;lt;String&amp;gt; schema = new KeyedDeserializationSchemaWrapper&amp;lt;&amp;gt;(new SimpleStringSchema());&lt;br/&gt;
    +        RichKeyedDeserializationSchemaWrapper&amp;lt;String&amp;gt; schema = new RichKeyedDeserializationSchemaWrapper&amp;lt;&amp;gt;(new SimpleStringSchema());
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    This file will have conflict with the current `master`, because I recently pushed a hotfix to `master` to fix the indentation of this file (previously, it&apos;s incorrectly using spaces to indent instead of tabs). Sorry about this!&lt;/p&gt;</comment>
                            <comment id="15886445" author="githubbot" created="Mon, 27 Feb 2017 20:09:42 +0000"  >&lt;p&gt;Github user haohui commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @rmetzger ping...&lt;br/&gt;
    just wondering what do you think about all the approaches we have discussed here? Your comments are appreciated.&lt;/p&gt;</comment>
                            <comment id="15886750" author="githubbot" created="Mon, 27 Feb 2017 23:24:42 +0000"  >&lt;p&gt;Github user haohui commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r103339489&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r103339489&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/util/serialization/RichKeyedDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,54 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +package org.apache.flink.streaming.util.serialization;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Can you please suggest where it should be put?&lt;/p&gt;</comment>
                            <comment id="15887548" author="githubbot" created="Tue, 28 Feb 2017 08:31:47 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r103399810&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r103399810&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/util/serialization/RichKeyedDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,54 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +package org.apache.flink.streaming.util.serialization;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I would put it perhaps in `org.apache.flink.streaming.kafka.serialization` under `flink-connector-kafka-base`.&lt;/p&gt;</comment>
                            <comment id="15887647" author="githubbot" created="Tue, 28 Feb 2017 09:24:32 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r103405663&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r103405663&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/main/java/org/apache/flink/streaming/util/serialization/RichKeyedDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,61 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +package org.apache.flink.streaming.util.serialization;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ResultTypeQueryable;&lt;br/&gt;
    +import org.apache.flink.util.Collector;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.IOException;&lt;br/&gt;
    +import java.io.Serializable;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * RichKeyedDeserializationSchema describes how to turn byte key / value messages into zero or more messages into data types.&lt;br/&gt;
    + * &lt;/p&gt;
{@see KeyedSerializationSchema}
&lt;p&gt;    + *&lt;br/&gt;
    + * @param &amp;lt;T&amp;gt; The type created by the keyed deserialization schema.&lt;br/&gt;
    + */&lt;br/&gt;
    +public interface RichKeyedDeserializationSchema&amp;lt;T&amp;gt; extends Serializable, ResultTypeQueryable&amp;lt;T&amp;gt; {&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Deserializes the byte message.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param messageKey the key as a byte array (null if no key has been set)&lt;br/&gt;
    +	 * @param message The message, as a byte array. (null if the message was empty or deleted)&lt;br/&gt;
    +	 * @param partition The partition the message has originated from&lt;br/&gt;
    +	 * @param offset the offset of the message in the original source (for example the Kafka offset)&lt;br/&gt;
    +	 * @param collector the user-provided collector that deserializes the bytes into zero or more&lt;br/&gt;
    +	 *                  records.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @return The deserialized message as an object.&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The method doesn&apos;t return anything.&lt;/p&gt;</comment>
                            <comment id="15887668" author="githubbot" created="Tue, 28 Feb 2017 09:40:31 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @StephanEwen and I just had an offline discussion about the change, and we came up with the following thoughts:&lt;/p&gt;

&lt;p&gt;    Using an `ArrayList` for buffering elements is an &quot;anti-pattern&quot; in Flink, because it is not a robust solution. Users could theoretically run into the size limit of an array list, and unnesting large messages (in multiple threads in the Kafka 0.8 case) can put pressure on the GC. We think that we should try to avoid that approach if possible.&lt;/p&gt;

&lt;p&gt;    Alternative approaches we considered (ordered by preference):&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Define the DeserializationSchema so that users can return `null` if the user doesn&apos;t want to emit a record.&lt;br/&gt;
    This approach would not change the current approach, and is pretty minimal. Of course, it would not allow for the &quot;unnesting&quot; use case, where you want to emit multiple records from one Kafka message. Users would need to deserialize into a nested structure and use a flatMap afterwards to do the un-nesting.&lt;/li&gt;
	&lt;li&gt;Move the deserialization into the checkpoint lock. This would allow us to collect elements into our internal collector from the user collector while still preserving exactly once semantics.&lt;br/&gt;
    This change would probably be a bit more involved code-wise, as we need to rearrange some parts (maybe moving the deserialization schema instance into the emitRecord() method, change of some method signatures).&lt;br/&gt;
    A downside of this approach would be that the Kafka 0.8 consumer threads would deserialize records in a sequential order (since only one consumer thread can hold the lock at a time). For Kafka 0.9 this is already the case. I think we can live with that, because the majority of users moved away from kafka 0.8 by now.&lt;/li&gt;
	&lt;li&gt;Use the `ArrayList` approach. Users would potentially run into issues and we would loose some of Flink&apos;s robustness.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    @jgrier since you&apos;ve opened the original JIRA back then, what&apos;s your take on the discussion? How bad would it be for users to just allow the `null` or record approach? (Other opinions are of course also appreciated)&lt;/p&gt;
</comment>
                            <comment id="15889101" author="githubbot" created="Tue, 28 Feb 2017 23:25:47 +0000"  >&lt;p&gt;Github user jgrier commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I think it would be just fine if we allowed a null return given the tradeoffs discussed here.  The main thing was to allow users a way to deal with bad data with minimal effort and without throwing an exception and causing their job to restart.&lt;/p&gt;</comment>
                            <comment id="15891810" author="githubbot" created="Thu, 2 Mar 2017 08:39:31 +0000"  >&lt;p&gt;Github user haohui commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks for the comments. Allowing `DeserializationSchema` to return `null` sounds good to me. I&apos;ll update the PR accordingly.&lt;/p&gt;</comment>
                            <comment id="15891895" author="githubbot" created="Thu, 2 Mar 2017 09:20:13 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks a lot for your understanding @haohui. &lt;br/&gt;
    Let us know once you&apos;ve updated the PR so that we can review and merge it.&lt;/p&gt;</comment>
                            <comment id="15895722" author="githubbot" created="Sat, 4 Mar 2017 15:32:36 +0000"  >&lt;p&gt;Github user rmetzger commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The change looks good to merge in my opinion.&lt;br/&gt;
    @tzulitai can you also have a quick look?&lt;/p&gt;</comment>
                            <comment id="15895723" author="githubbot" created="Sat, 4 Mar 2017 15:32:44 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r104227652&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r104227652&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/dev/connectors/kafka.md &amp;#8212;&lt;br/&gt;
    @@ -146,6 +146,10 @@ The Flink Kafka Consumer needs to know how to turn the binary data in Kafka into&lt;br/&gt;
     `DeserializationSchema` allows users to specify such a schema. The `T deserialize(byte[] message)`&lt;br/&gt;
     method gets called for each Kafka message, passing the value from Kafka.&lt;/p&gt;

&lt;p&gt;    +There are two possible design choice when the `DeserializationSchema` encounters a corrupted message. It can&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    choices&lt;/p&gt;</comment>
                            <comment id="15896252" author="githubbot" created="Sun, 5 Mar 2017 13:01:58 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r104312079&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r104312079&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -419,6 +424,164 @@ public void run() &lt;/p&gt;
{
     		assertFalse(&quot;fetcher threads did not properly finish&quot;, sourceContext.isStillBlocking());
     	}

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testSkipCorruptedMessage() throws Exception {&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- some test data -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final String topic = &quot;test-topic&quot;;&lt;br/&gt;
    +		final int partition = 3;&lt;br/&gt;
    +		final byte[] payload = new byte[] &lt;/p&gt;
{1, 2, 3, 4}
&lt;p&gt;;&lt;br/&gt;
    +&lt;br/&gt;
    +		final List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt; records = Arrays.asList(&lt;br/&gt;
    +			new ConsumerRecord&amp;lt;&amp;gt;(topic, partition, 15, payload, payload),&lt;br/&gt;
    +			new ConsumerRecord&amp;lt;&amp;gt;(topic, partition, 16, payload, payload),&lt;br/&gt;
    +			new ConsumerRecord&amp;lt;&amp;gt;(topic, partition, 17, payload, &quot;end&quot;.getBytes()));&lt;br/&gt;
    +&lt;br/&gt;
    +		final Map&amp;lt;TopicPartition, List&amp;lt;ConsumerRecord&amp;lt;byte[], byte[]&amp;gt;&amp;gt;&amp;gt; data = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +		data.put(new TopicPartition(topic, partition), records);&lt;br/&gt;
    +&lt;br/&gt;
    +		final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; consumerRecords = new ConsumerRecords&amp;lt;&amp;gt;(data);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- the test consumer -----&lt;br/&gt;
    +&lt;br/&gt;
    +		final KafkaConsumer&amp;lt;?, ?&amp;gt; mockConsumer = mock(KafkaConsumer.class);&lt;br/&gt;
    +		when(mockConsumer.poll(anyLong())).thenAnswer(new Answer&amp;lt;ConsumerRecords&amp;lt;?, ?&amp;gt;&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public ConsumerRecords&amp;lt;?, ?&amp;gt; answer(InvocationOnMock invocation) &lt;/p&gt;
{
    +				return consumerRecords;
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		whenNew(KafkaConsumer.class).withAnyArguments().thenReturn(mockConsumer);&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- build a fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		ArrayList&amp;lt;String&amp;gt; results = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
    +		SourceContext&amp;lt;String&amp;gt; sourceContext = new CollectingSourceContext&amp;lt;&amp;gt;(results, results);&lt;br/&gt;
    +		Map&amp;lt;KafkaTopicPartition, Long&amp;gt; partitionsWithInitialOffsets =&lt;br/&gt;
    +			Collections.singletonMap(new KafkaTopicPartition(topic, partition), KafkaTopicPartitionStateSentinel.GROUP_OFFSET);&lt;br/&gt;
    +		KeyedDeserializationSchema&amp;lt;String&amp;gt; schema = new KeyedDeserializationSchema&amp;lt;String&amp;gt;() {&lt;br/&gt;
    +&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public String deserialize(byte[] messageKey, byte[] message,&lt;br/&gt;
    +									  String topic, int partition, long offset) throws IOException &lt;/p&gt;
{
    +				return offset == 15 ? null : new String(message);
    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public boolean isEndOfStream(String nextElement) &lt;/p&gt;
{
    +				return &quot;end&quot;.equals(nextElement);
    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public TypeInformation&amp;lt;String&amp;gt; getProducedType() &lt;/p&gt;
{
    +				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt;    +		};&lt;br/&gt;
    +&lt;br/&gt;
    +		final Kafka09Fetcher&amp;lt;String&amp;gt; fetcher = new Kafka09Fetcher&amp;lt;&amp;gt;(&lt;br/&gt;
    +			sourceContext,&lt;br/&gt;
    +			partitionsWithInitialOffsets,&lt;br/&gt;
    +			null, /* periodic watermark extractor */&lt;br/&gt;
    +			null, /* punctuated watermark extractor */&lt;br/&gt;
    +			new TestProcessingTimeService(),&lt;br/&gt;
    +			10, /* watermark interval */&lt;br/&gt;
    +			this.getClass().getClassLoader(),&lt;br/&gt;
    +			true, /* checkpointing */&lt;br/&gt;
    +			&quot;task_name&quot;,&lt;br/&gt;
    +			new UnregisteredMetricsGroup(),&lt;br/&gt;
    +			schema,&lt;br/&gt;
    +			new Properties(),&lt;br/&gt;
    +			0L,&lt;br/&gt;
    +			false);&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +		// ----- run the fetcher -----&lt;br/&gt;
    +&lt;br/&gt;
    +		fetcher.runFetchLoop();&lt;br/&gt;
    +		assertEquals(1, results.size());&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testNullAsEOF() throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I&apos;m not sure if this test is necessary. It&apos;s essentially just testing that `isEndOfStream` works when `isEndOfStream` is `true`. Whether or not the condition is `element == null` seems irrelevant to what&apos;s been tested.&lt;/p&gt;

&lt;p&gt;    We also already have a `runEndOfStreamTest` in `KafkaConsumerTestBase`.&lt;/p&gt;

</comment>
                            <comment id="15896253" author="githubbot" created="Sun, 5 Mar 2017 13:01:58 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314#discussion_r104311996&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314#discussion_r104311996&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/main/java/org/apache/flink/streaming/connectors/kafka/internals/SimpleConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -381,6 +381,10 @@ else if (partitionsRemoved) &lt;/p&gt;
{
     								partitionsIterator.remove();
     								continue partitionsLoop;
     							}
&lt;p&gt;    +&lt;br/&gt;
    +							if (value == null) &lt;/p&gt;
{
    +								continue;
    +							}
&lt;p&gt;    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Would it make sense to do the `null` checking inside `emitRecord(...)`?&lt;br/&gt;
    Otherwise, we wouldn&apos;t be updating the state for skipped records, and therefore not accounting it as &quot;already processed&quot;.&lt;/p&gt;

&lt;p&gt;    I don&apos;t think it really matters, since we aren&apos;t outputting anything anyway, but I see at least one minor advantage that might deserve changing it: If we fail during a series of continuous skipped records, we won&apos;t be wasting any overhead re-processing them on restore.&lt;/p&gt;</comment>
                            <comment id="15900035" author="githubbot" created="Tue, 7 Mar 2017 19:49:10 +0000"  >&lt;p&gt;Github user haohui commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    CI failed in one of the group as the group was timed out. The specific group was not timed out in the last run.&lt;/p&gt;

&lt;p&gt;    @tzulitai can you please take another look? Thanks&lt;/p&gt;</comment>
                            <comment id="15900735" author="githubbot" created="Wed, 8 Mar 2017 05:26:52 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    LGTM, I&apos;ll proceed to merge this later today.&lt;br/&gt;
    One minor problem: the offset state still isn&apos;t updated if `record == null`. We need to do the checking in the synchronize block in the `emitRecord*` methods.&lt;/p&gt;

&lt;p&gt;    It&apos;s a simple fix, so I can do it while merging &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15901624" author="githubbot" created="Wed, 8 Mar 2017 17:33:31 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I&apos;ve made some final general improvements in &lt;a href=&quot;https://github.com/tzulitai/flink/tree/PR-FLINK-3679&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tzulitai/flink/tree/PR-FLINK-3679&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;    Doing a Travis run before merging:&lt;br/&gt;
    &lt;a href=&quot;https://travis-ci.org/tzulitai/flink/builds/209054624&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://travis-ci.org/tzulitai/flink/builds/209054624&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15902546" author="githubbot" created="Thu, 9 Mar 2017 06:08:21 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15902549" author="tzulitai" created="Thu, 9 Mar 2017 06:11:51 +0000"  >&lt;p&gt;Resolved for &lt;tt&gt;master&lt;/tt&gt; with &lt;a href=&quot;http://git-wip-us.apache.org/repos/asf/flink/commit/c39ad31&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://git-wip-us.apache.org/repos/asf/flink/commit/c39ad31&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Thanks a lot for your contribution &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wheat9&quot; class=&quot;user-hover&quot; rel=&quot;wheat9&quot;&gt;wheat9&lt;/a&gt;!&lt;/p&gt;</comment>
                            <comment id="15902554" author="tzulitai" created="Thu, 9 Mar 2017 06:15:28 +0000"  >&lt;p&gt;I&apos;m a bit hesitant whether or not we want to backport this fix to &lt;tt&gt;release-1.1&lt;/tt&gt; and &lt;tt&gt;release-1.2&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;I think it&apos;s ok, since it doesn&apos;t break any user code.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rmetzger&quot; class=&quot;user-hover&quot; rel=&quot;rmetzger&quot;&gt;rmetzger&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jgrier&quot; class=&quot;user-hover&quot; rel=&quot;jgrier&quot;&gt;jgrier&lt;/a&gt; what do you think?&lt;/p&gt;</comment>
                            <comment id="15902563" author="githubbot" created="Thu, 9 Mar 2017 06:29:03 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @haohui - one suggestion for future contributions for easier reviews:&lt;br/&gt;
    We usually use follow-up commits that addresses review comments, instead of force pushing the whole branch. For reviewers, this allows easier tracking of history of what has been addressed and fixed from the original PR.&lt;/p&gt;</comment>
                            <comment id="15902597" author="githubbot" created="Thu, 9 Mar 2017 07:15:18 +0000"  >&lt;p&gt;Github user haohui commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Yes totally agree. Thanks very much for taking the time to review the PRs. Will do it next time.&lt;/p&gt;</comment>
                            <comment id="16414130" author="githubbot" created="Mon, 26 Mar 2018 16:59:26 +0000"  >&lt;p&gt;Github user InfinitiesLoop commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/3314&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/3314&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Sorry for necro&apos;ing this thread, but where does the community land on the multiple record per kafka payload idea this PR originally intended to solve?&lt;/p&gt;

&lt;p&gt;    I have this scenario, where a single payload in kafka can represent hundreds of logical records. It&apos;s fine to just flatMap() them out after the deserialization schema, but that does not let me deal with timestamps and watermarks correctly. It&apos;s possible the source is reading from 2 partitions that are out of sync with each other, but I can&apos;t assign a timestamp and watermark for a single message that contains many records that might span multiple timestamps. So I&apos;m just using a timestamp and watermark extractor on the stream separate from the source, and just hoping that I never have out of sync partitions. If a solution is still desired I&apos;d love to contribute, otherwise it looks like I will end up having to write my own custom kafka source..&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13036323">FLINK-5583</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13040063">FLINK-5704</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 34 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2vd5b:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>