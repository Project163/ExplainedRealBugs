<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:40:01 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-12852] [hotfix timeout] Deadlock occurs when requiring exclusive buffer for RemoteInputChannel</title>
                <link>https://issues.apache.org/jira/browse/FLINK-12852</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;When running tests with an upstream vertex and downstream vertex, deadlock occurs when submitting the job:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-quote&quot;&gt;&quot;Sink: Unnamed (3/500)&quot;&lt;/span&gt; #136 prio=5 os_prio=0 tid=0x00007f2cca81b000 nid=0x38845 waiting on condition [0x00007f2cbe9fe000]
java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.State: TIMED_WAITING (parking)
at sun.misc.Unsafe.park(Native Method)
- parking to wait &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; &amp;lt;0x000000073ed6b6f0&amp;gt; (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:233)
at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2078)
at java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:418)
at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestMemorySegments(NetworkBufferPool.java:180)
at org.apache.flink.runtime.io.network.buffer.NetworkBufferPool.requestMemorySegments(NetworkBufferPool.java:54)
at org.apache.flink.runtime.io.network.partition.consumer.RemoteInputChannel.assignExclusiveSegments(RemoteInputChannel.java:139)
at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.assignExclusiveSegments(SingleInputGate.java:312)
- locked &amp;lt;0x000000073fbc81f0&amp;gt; (a java.lang.&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt;)
at org.apache.flink.runtime.io.network.partition.consumer.SingleInputGate.setup(SingleInputGate.java:220)
at org.apache.flink.runtime.taskmanager.Task.setupPartionsAndGates(Task.java:836)
at org.apache.flink.runtime.taskmanager.Task.run(Task.java:598)
at java.lang.&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.run(&lt;span class=&quot;code-object&quot;&gt;Thread&lt;/span&gt;.java:834)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;This is due to the required and max of local buffer pool is not the same and there may be over-allocation, when assignExclusiveSegments there are no available memory.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;The detail of the scenarios is as follows: The parallelism of both upstream vertex and downstream vertex are 1500 and 500 respectively. There are 200 TM and each TM has 10696 buffers( in total and has 10 slots. For a TM that runs 9 upstream tasks and 1 downstream task, the 9 upstream tasks start first with local buffer pool {required = 500, max = 2 * 500 + 8 = 1008}, it produces data quickly and each occupy about 990 buffers. Then the DownStream task starts and try to assigning exclusive buffers for 1500 -9 = 1491 InputChannels. It requires 2981 buffers but only 1786 left. Since not all downstream tasks&#160;can start, the job will be blocked finally and no buffer can be released, and the deadlock finally occurred.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;I think although increasing the network memory solves the problem, the deadlock may not be acceptable.&#160;&#160;Fined grained resource management&#160;&#160;Flink-12761 can solve this problem, but AFAIK in 1.9 it will not include the network memory into the ResourceProfile.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13239507">FLINK-12852</key>
            <summary>[hotfix timeout] Deadlock occurs when requiring exclusive buffer for RemoteInputChannel</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="gaoyunhaii">Yun Gao</assignee>
                                    <reporter username="gaoyunhaii">Yun Gao</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Fri, 14 Jun 2019 10:20:04 +0000</created>
                <updated>Tue, 31 Aug 2021 07:17:47 +0000</updated>
                            <resolved>Thu, 11 Jul 2019 07:26:16 +0000</resolved>
                                    <version>1.7.2</version>
                    <version>1.8.1</version>
                    <version>1.9.0</version>
                                    <fixVersion>1.9.0</fixVersion>
                                    <component>Runtime / Network</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>10</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="1200">20m</timespent>
                                <comments>
                            <comment id="16874667" author="gaoyunhaii" created="Fri, 28 Jun 2019 04:38:24 +0000"  >&lt;p&gt;For all the possible methods to solve this problem come to me:&#160;&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Reserver buffers for exclusive buffers: It is hard to know how much required to reserve for the exclusive buffers, especially it is hard to know how many tasks will be scheduled to this TM.&lt;/li&gt;
	&lt;li&gt;Make the required buffers and the max buffers the same for all the local buffer pools: It may cause previous running jobs unable to run due to the total buffers is less than the sum of the updated required buffers.&lt;/li&gt;
	&lt;li&gt;Postpone the acquirement of exclusive buffers: It does not solves the deadlock problem, since the downstream still may not acquired any buffers to make progress.&lt;/li&gt;
	&lt;li&gt;Add a timeout for the &lt;em&gt;requestMemorySegments.&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;Therefore, I think&#160;currently we need to use the last&#160;method. I&apos;d like to&#160;&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Add an option for the timeout of requestMemorySegment for each channel. The default timeout is 30s. This option will be marked as undocumented since it may be removed within the future implementation.&lt;/li&gt;
	&lt;li&gt;Transfer the timeout to NetworkBufferPool.&lt;/li&gt;
	&lt;li&gt;RequestMemorySegments will throw IOException(&quot;Insufficient buffer&quot;)&#160; if not all segments acquired after timeout.&lt;/li&gt;
&lt;/ol&gt;
</comment>
                            <comment id="16876842" author="stephanewen" created="Tue, 2 Jul 2019 10:14:47 +0000"  >&lt;p&gt;Thanks for reporting this. This is a pretty critical bug!&lt;br/&gt;
Definitely a blocker for the 1.9 release, if we want to make that the &quot;batch works well&quot; release.&lt;/p&gt;
</comment>
                            <comment id="16876855" author="stephanewen" created="Tue, 2 Jul 2019 10:37:18 +0000"  >&lt;p&gt;The fix suggested here is a quick band aid to not get a deadlock and rather fail.&lt;br/&gt;
That is an improvement, but arguable not the best user experience.&lt;/p&gt;

&lt;p&gt;In your option (2) - I think I do not fully understand. Why would that break existing jobs?&lt;/p&gt;

&lt;p&gt;I cannot think of a very good short term solution for this at the moment.&lt;br/&gt;
A long-term solution could be:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;We make memory buffers a resource per slot - slots can not eat into each others&apos; network buffers.&lt;/li&gt;
	&lt;li&gt;The new batch scheduler sets slot sharing so that it can only share slots within a pipelined region (also helps with managed memory fragmentation). We thus know how many tasks will be in a slot at most (and it should not be too many).&lt;/li&gt;
	&lt;li&gt;we compute the minimum number of buffers per gate (&quot;per-channel x num-channels + floating&quot;) and derive the total number of needed buffers for all tasks that might run concurrently.&lt;/li&gt;
	&lt;li&gt;we parameterize the output gates to know the total number of needed buffers and allow them request more only from the buffers beyond that number.&lt;/li&gt;
	&lt;li&gt;input gates take only the &quot;per-channel * num-channels + floating&quot; and never more (I think that even is the case right now)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16876889" author="stephanewen" created="Tue, 2 Jul 2019 11:37:51 +0000"  >&lt;p&gt;Quick comment to understand the implications of this:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The blocking subpartition implementation should not be affected, because it eagerly releases resources.&lt;/li&gt;
	&lt;li&gt;If bounded shuffles and rebalancing were always blocking, then this could only happen in FORWARD channels.&lt;/li&gt;
	&lt;li&gt;At least in the DataSet API, the FORWARD channels are almost always slot-local, so realized via local channels, which are not affected&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;That means a workaround for DataSet / Table Flink runner could be to have all non-FORWARD strategies as blocking shuffles&lt;/p&gt;

&lt;p&gt;For the Table Blink runner, we need to think a bit more...&lt;/p&gt;</comment>
                            <comment id="16876895" author="gaoyunhaii" created="Tue, 2 Jul 2019 11:50:15 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt;, very thanks for the explanation!&lt;/p&gt;

&lt;p&gt;For the option 2, I thought it might break the existing jobs since it increases the &lt;em&gt;required buffers&lt;/em&gt; if we increase the number of required buffers to the number of max buffers. For example, if we have a TM with 10 buffers, and we have two local buffer pools on this TM with [required, max) equals to [2, 8). Before the increasing, the total required buffers is 4 and both the two local buffer pools can be created. But after the increasing, the total required buffers become 16, and creating the second local buffer pool will fail with InsufficientNumberOfBuffers.&lt;/p&gt;

&lt;p&gt;On the other side, if we want to totally avoid the problems brought by increasing the number of required buffers, we can only decrease the number of max buffers to the number of required buffers, but this might decrease the performance because the local buffer pool could not apply as many buffers as before.&lt;/p&gt;

&lt;p&gt;I&apos;m also a bit worry that the above problem might also exist if we want to limit the network memory per slot for the long run: increasing the current required buffers of local buffer pool might be not compatible with existing jobs and reduces the number of tasks that can be scheduled to the same TaskManager, while decreasing the max buffer of local buffer pool might decrease the performance of the network layer.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16876919" author="gaoyunhaii" created="Tue, 2 Jul 2019 12:35:26 +0000"  >&lt;p&gt;Hi&#160;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt;,&#160;I think except for the batch scenarios, this issue also affects the stream jobs. In fact, the issue was also detected in a stream job. For stream jobs, the upstream&#160;tasks&#160;may start first, and they then start produce data. This piece of data might occupied all the buffers of the TM. Then the downstream tasks start and they cannot acquired the exclusive buffers required. Since the downstream tasks can not consume buffers from the upstream tasks, the occupied buffers of the upstream task cannot be recycled, then the deadlock occurs.&lt;/p&gt;</comment>
                            <comment id="16877073" author="zjwang" created="Tue, 2 Jul 2019 15:40:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt; thanks for concerning this issue and sharing the suggestions.&lt;/p&gt;

&lt;p&gt;The root reason of this issue is the mechanism of&#160;distributing&#160;global buffers among LocalBufferPools. As we know the local pool has core size and max size for partition/gate. The core size must be satisfied otherwise an exception would be thrown.&#160;Every local pool could use max size at most if there are enough buffers in global pool to make better use of resource. But the precondition is the extra buffers beyond core size could be returned finally after redistribution.&#160; This precondition/assumption is not always&#160;satisfied especially in credit-based mode for the following reasons:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;In non-credit mode, the producer could always push data to network until backpressure, so the extra used buffers in partition&apos;s local pool could be recycled in time. But now the producer could not send buffer to network until the&#160;consumer requested the exclusive buffers as initial credits.&lt;/li&gt;
	&lt;li&gt;In non-credit mode, the gate&apos;s local pool only needs the core size equal to numChannels, but now we need 2*numChannels as exclusive core size by default in credit-based mode. So the probability is higher than before.&lt;/li&gt;
	&lt;li&gt;In non-credit mode, the buffer request from global pool is lazy by data driven, that means after consumer receives data from network, then the local pool would request buffer from global pool. But now the exclusive buffer request from global pool is eager during startup. So the probability is also higher than before. If we make exclusive request also lazy, then it might also relieve the deadlock issue.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;I think the previous non-credit mode could not avoid deadlock completely in theory, not very sure, especially for some corner cases. But in credit-based mode,&#160;the&#160;above factors would make the probability of deadlock much higher than before.&#160;&lt;/p&gt;

&lt;p&gt;I agree with the&#160;implications mentioned&#160;by Stephan, the current blocking partition would not exist this issue. For streaming job it still exists this probability, but i think it should not be a blocker for release-1.9.&lt;/p&gt;

&lt;p&gt;The current proposed PR makes the deadlock to fail instead. It seems a bit better than now to tell users what happens, but it does not solve this issue in root. And users might still have a bad experience. We ever&#160;avoided this issue in alibaba via network resource matching in ResourceProfile and fixed size in local pool.&lt;/p&gt;

&lt;p&gt;I also agree with the general ideas Stephan proposed for solving this issue. The slot resource isolation (including shuffle resource) might be a right way to go in future. ATM we could make some improvements to decrease this probability, such as making exclusive size as 1 by default which Jira was already created before and making exclusive request lazy as floating buffers.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16877524" author="maguowei" created="Wed, 3 Jul 2019 06:44:31 +0000"  >&lt;p&gt;In long run there may be another way to avoid the dead lock. Task could spill some buffer to the disk and return the buffer to the local buffer pool. I think this could&#160;increase the utilization of memory in most time. What do you guys think?&lt;/p&gt;</comment>
                            <comment id="16877556" author="gaoyunhaii" created="Wed, 3 Jul 2019 07:17:23 +0000"  >&lt;p&gt;Hi all, very thanks for the discussion and all the valuable suggestions. I&apos;d like to have a short summarization of the possible solutions to this issue based on the above discussion:&lt;/p&gt;

&lt;p&gt;For the short term, we may also&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Use blocking shuffle instead of the the pipeline shuffle for the affected partitioner in batch jobs.&lt;/li&gt;
	&lt;li&gt;Decrease the exclusive buffers per channel to 1, and postpone the request of exclusive buffers till data is received. They decreases the probability of this issue.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;For the long run, this issue may be also related with the fine-grained resource management, we may&#160;&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Make memory buffers a resource per slot and disallow slots to eat into each others&apos; network buffers.&#160;&lt;/li&gt;
	&lt;li&gt;Keep the required/max mechanism, but to fix the deadlock problem, the local buffer pool need be able to return the overused buffers when there are not enough required buffers for the other channels/local buffer pool, like spill to disk.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;If there are misunderstands and missing points, let me know and I&#160;would update this comment. &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16877605" author="zjwang" created="Wed, 3 Jul 2019 07:52:15 +0000"  >&lt;p&gt;The spill way could solve the deadlock issue in function, which is very similar with previous SpillableSubpartition&apos;s &#160;behavior. If&#160;we only&#160;want to spill the buffers between core size and max size after redistribution, it still could not solve another existing exception&#160;of &#160;`insufficient number of network buffers&apos; which was always experienced in Flink for large-scale job. If we also spill some buffers within core size to avoid that `IOException`, the performance regression&#160;might be serious but users are not aware of it. Users might prefer to increase buffer options&#160;to avoid performance regression if they know. Especially for streaming job, it is better not to touch disk unless necessary.&lt;/p&gt;

&lt;p&gt;In contrast, if we agree that the slot resource matching would be the final way in future, then it could resolve both deadlock and `insufficient number of network buffers&apos; issues. And users could decide to adjust the relevant buffer configs to make a tradeoff between performance and total resource usage. And we might further improve the internal mechanism for decreasing the requirements of core buffer sizes(including exclusive and core size in local buffer) to make job still run when given limited resource.&lt;/p&gt;</comment>
                            <comment id="16877616" author="zjwang" created="Wed, 3 Jul 2019 08:09:53 +0000"  >&lt;p&gt;Hey &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gaoyunhaii&quot; class=&quot;user-hover&quot; rel=&quot;gaoyunhaii&quot;&gt;gaoyunhaii&lt;/a&gt;, considering the second point for long run, I have not thought it throughly.&lt;/p&gt;

&lt;p&gt;I am not very sure whether it is still worth&#160;keeping dynamic local pool between core and max size based on slot resource matching. If this dynamic&#160;would bring more troubles in practice, another option is adjusting it to a fix-size local pool instead. The system could calculate a reasonable default size for the local pool as now, and users could also tune it to any size they want. E.g. only 2 total buffers in local pool could also work for 100 subpartitions if users&#160;are not caring about the performance.&lt;/p&gt;</comment>
                            <comment id="16877680" author="gaoyunhaii" created="Wed, 3 Jul 2019 09:34:22 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt;&#160;Thanks a lot for the explanation. I&apos;m also still thinking on it currently, but one point is that compared with the previous required/max mechanism, we may not easily find a suitable fixed buffer pool size. If we&#160;use&#160;a size that is less than the number of max buffers before, we may not use as many buffers as before, which may decrease the performance. If we instead use a size equals to the number of max buffers before, then with the same number of buffers we may run less number of tasks compared with before, and the buffers may not be fully utilized if the task is not busy (Previously these buffers may be shared by others). On the other side, I totally agree with that the fixed buffer size solves the deadlock problem and should be easier to implement.&lt;/p&gt;</comment>
                            <comment id="16877737" author="zjwang" created="Wed, 3 Jul 2019 10:47:19 +0000"  >&lt;p&gt;It is hardly to say whether we could get performance benefits via&#160;overusing the extra buffers beyond core size temporarily.&lt;/p&gt;

&lt;p&gt;In general the tasks would be deployed into one TM by sequence in very short internal time. If the first task occupies some&#160;buffers actually belong to other following tasks, it would also bring some&#160;cost/overhead to recycle these extra buffers afterwards. Especially this greedy mechanism&#160;might only have limited&#160;benefits in special cases, such as backpressure. Overall&#160;it is hard to evaluate&#160;the final benefits in different scenarios.&lt;/p&gt;</comment>
                            <comment id="16877779" author="pnowojski" created="Wed, 3 Jul 2019 11:53:29 +0000"  >&lt;p&gt;Couple of comments from my side.&#160;&lt;/p&gt;

&lt;p&gt;How does the resource allocation per slot solves the problem if we take into the account slot sharing?&lt;/p&gt;

&lt;p&gt;Keeping resource sharing might give us some benefits in the future, although I think it doesn&apos;t help us now - at least not in streaming.&lt;/p&gt;

&lt;p&gt;Maybe a solution could be&#160;that task initially allocates bare minimum of the resources for him to progress, and is only allowed to grab extra resources once it knows that it can make a progress (for broadcast if there is at least one down stream reader, for others if each partition/sub-partition has at least one reader).&lt;/p&gt;</comment>
                            <comment id="16878129" author="stephanewen" created="Wed, 3 Jul 2019 19:50:43 +0000"  >&lt;p&gt;I think Piotr&apos;s suggestion goes in a good direction.&lt;/p&gt;

&lt;p&gt;Even simpler, we could say that a pipelined partition cannot accept records (and thus allocate more than the minimum buffers) before it&apos;s reader (SubpartitionView) has not been created. By the time that connection is made, the receiver will have allocated its buffers.&lt;/p&gt;

&lt;p&gt;This is effectively one way to make sure that the allocated buffers in a pipelined result partition can can only increase beyond the minimum once a partition knows that it can make progress through sending.&lt;/p&gt;

&lt;p&gt;The price might me a slightly higher time to ramp up to full throughput after recovery.&lt;br/&gt;
We would have to investigate what the practical impact of that would be (hopefully not really anything in practice).&lt;/p&gt;</comment>
                            <comment id="16878256" author="gaoyunhaii" created="Thu, 4 Jul 2019 01:13:43 +0000"  >&lt;p&gt;Hi all, very thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pnowojski&quot; class=&quot;user-hover&quot; rel=&quot;pnowojski&quot;&gt;pnowojski&lt;/a&gt; and &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt; for the valuable suggestions, I also&#160;agree with&#160;that postponing allocation till the readers have been created makes it more easier to solve this issue. On the other side, to complement, I think we may also need to limit the orders of &lt;em&gt;requestPartitions&lt;/em&gt; among tasks while allow the ResultPartition only to grab the (extra) buffers after the readers have been created. If not, for jobs like A -&amp;gt; B -&amp;gt; C and A/B/C are mixed on TaskManagers, B may first starts and requests result partitions to A, then A starts sending records and grabbing extra buffers, then C may fail to start and finally cause a deadlock. To solve this issue, we may also need to require that the &lt;em&gt;requestPartition&lt;/em&gt; happens from Sinks towards Sources.&lt;/p&gt;

&lt;p&gt;With postponing the allocation of (extra) buffers and requesting partitions in order, I think the deadlock could also be avoided for clusters with multiple jobs, since now&#160; it ensures all the over-allocated buffers can be recycled finally. I have not found other counter-examples currently.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16878289" author="zjwang" created="Thu, 4 Jul 2019 03:25:35 +0000"  >&lt;p&gt;Network resource matching in slot has many unsetting issues which would&#160;be further discussed future, so we could not make it effect in short time.&lt;/p&gt;

&lt;p&gt;Lazy allocation buffers on producer side seems&#160;a feasible way atm. It could still retain the current core and maximum mechanism in local pool. But it brings another two effects:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;Higher time to ramp up to full throughput as Stephan mentioned, especially for some very short-time jobs (several seconds finish) and I remembered there exists such cases in Kurt&apos;s benchmark before. We change the previous concurrent production and consumption to sequential way. For short-time job, before&#160;the consumer requests partition, all the data set might already be emitted&#160;and cached in partition&#160;pool on producer side before.&lt;/li&gt;
	&lt;li&gt;We rely on&#160;another&#160;assumption that produced buffers could be recycled&#160;finally once subpartition view is established. This assumption might limit our new features/improvements future. ATM we need to adjust the action to trigger partition request, that means RemoteInputChannel could only send partition request if the correspond task&#160;has no result partition or the partition&apos;s view has already been established. In future the InputSelection might also destroy the above assumption. Although the partition was requested, but the OP could select not to consumer&#160;that partition long time.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16878461" author="pnowojski" created="Thu, 4 Jul 2019 09:20:01 +0000"  >&lt;p&gt;Just for the record &amp;amp; FYI,&#160;I&apos;m currently working in parallel on changing when &lt;tt&gt;requestPartitions&lt;/tt&gt; call is happening (I&apos;m trying to move it from &lt;tt&gt;SingleInputGate#get/poll&lt;/tt&gt; to &lt;tt&gt;SingleInputGate#setup()&lt;/tt&gt;&#160;)&#160;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-13013&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-13013&lt;/a&gt;&#160;.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&amp;gt;&#160;&#160;is only allowed to grab extra resources once it knows that it can make a progress&#160;&lt;/p&gt;

&lt;p&gt;When&#160;writing this I forgot that it indeed requires not only the immediate successor operator to be ready, but for all of&#160;the downstream operators be ready as well as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gaoyunhaii&quot; class=&quot;user-hover&quot; rel=&quot;gaoyunhaii&quot;&gt;gaoyunhaii&lt;/a&gt; wrote. So we would have to indeed initialize/grab resources in the order from the sinks up to the sources.&lt;/p&gt;

&lt;p&gt;Do I understand it correctly, that we can have three solutions:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;Grab only required resources from sinks up to the sources and once that completes grab additional resources.&lt;/li&gt;
	&lt;li&gt;Make buffers completely not shared between tasks - for example disable slot sharing and make&#160;buffer pools per slot&lt;/li&gt;
	&lt;li&gt;Make optional buffers&#160;&quot;revocable&quot;, for example by spilling.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt;&#160;which one of those three were you referring to when writing &quot;Network resource matching in slot&quot; and &quot;Lazy allocation buffers on producer side&quot;?&lt;/p&gt;</comment>
                            <comment id="16878516" author="zjwang" created="Thu, 4 Jul 2019 10:08:38 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pnowojski&quot; class=&quot;user-hover&quot; rel=&quot;pnowojski&quot;&gt;pnowojski&lt;/a&gt;&#160;&quot;Network resource matching in slot&quot; refers to the second one you mentioned, and &#160;&quot;Lazy allocation buffers on producer side&quot; refers to the first one.&lt;/p&gt;</comment>
                            <comment id="16878518" author="zjwang" created="Thu, 4 Jul 2019 10:19:29 +0000"  >&lt;p&gt;We have two directions in general: buffers sharing between tasks or not.&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The sharing direction&#160;seems more flexible&#160;and has more&#160;possibilities, but we need to confirm the buffers revocable mechanism to not bring downsides. ATM we have above&#160;the first and third ways for it.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;The not-sharing direction could avoid potential issues&#160;completely, but it might bring resource waste in some cases.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16878609" author="aitozi" created="Thu, 4 Jul 2019 12:26:17 +0000"  >&lt;p&gt;Hi&#65292; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gaoyunhaii&quot; class=&quot;user-hover&quot; rel=&quot;gaoyunhaii&quot;&gt;gaoyunhaii&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I check the allocation logic again, I found that during createBufferPool for inputGate/resultPartition it will increase the variable  &lt;em&gt;numTotalRequiredBuffers&lt;/em&gt; .  If the &lt;em&gt;numTotalRequiredBuffers + numRequiredBuffers &amp;gt; totalNumberOfMemorySegments&lt;/em&gt; it will throw the network buffer not  enough exception. And the inputGate assignExclusiveBuffers will do the same thing. But during the localBufferPool#requestMemorySegment it will ask for buffer from the global buffer pool but this do not increase the &lt;em&gt;numTotalRequiredBuffers&lt;/em&gt; (this method will request extra buffers larger than the core size). So although the downstream have not enough buffer to assign, but it can also pass the &lt;em&gt;numTotalRequiredBuffers + numRequiredBuffers &amp;gt; totalNumberOfMemorySegments&lt;/em&gt; check in the &lt;em&gt;NetworkBufferPool#requestMemorySegments&lt;/em&gt; method. &lt;/p&gt;

&lt;p&gt;Although this can not indeed solve the deadlock problem, but i think this can partly hide this situation, otherwise it will fail with the check.&lt;/p&gt;
</comment>
                            <comment id="16878692" author="zjwang" created="Thu, 4 Jul 2019 14:10:20 +0000"  >&lt;p&gt;Thanks for concerning this issue &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aitozi&quot; class=&quot;user-hover&quot; rel=&quot;aitozi&quot;&gt;aitozi&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;We already have the check `numTotalRequiredBuffers + numberOfSegmentsToRequest &amp;gt; totalNumberOfMemorySegments` during `NetworkBufferPool#requestMemorySegments` and this check is mainly used for&#160;judging whether the global size could satisfy the total core size. If we replace the `numberOfSegmentsToRequest` with `numRequiredBuffers` as you suggested above, what is the value for `numRequiredBuffers` here?&lt;/p&gt;

&lt;p&gt;During&#160;`NetworkBufferPool#createBufferPool`, the `numRequiredBuffers` in check is for the core size in local pool. I guess your suggestion is also throwing the IOException(&quot;Insufficient buffers...&quot;) after allocating the exclusive buffers failed. There are two concerns for this solution:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;How long do you want to wait before throwing the exception. The extra buffers recycle from other local pool might take some time after redistribution, but we could not estimate the rough time to throw this exception properly.&lt;/li&gt;
	&lt;li&gt;This seems a bit tricky/hacky to reuse the check here. If the exception tells users the message of insufficient buffers, users might be confused because the total buffers are actually enough. If the exception tells users the message of deadlock, users might still do not know what to do. The only benefit is identifying the deadlock easily, not via jstack.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16878727" author="aitozi" created="Thu, 4 Jul 2019 14:57:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt; &lt;br/&gt;
I&apos;am sorry for confusing you that the variable name &lt;em&gt;numRequiredBuffers&lt;/em&gt; is wrong because I copy from another code branch. &lt;br/&gt;
My mean is that &lt;em&gt;numTotalRequiredBuffers&lt;/em&gt; is not calculated correctly because it do not increase 1 when invoke &lt;em&gt;requestMemorySegment&lt;/em&gt;.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
@Nullable
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; MemorySegment requestMemorySegment() {
	&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; availableMemorySegments.poll();
}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt; So the check &lt;em&gt;numTotalRequiredBuffers + numberOfSegmentsToRequest &amp;gt; totalNumberOfMemorySegments&lt;/em&gt; may pass because the &lt;em&gt;numTotalRequiredBuffers&lt;/em&gt; is less than the real required buffer . &lt;/p&gt;

&lt;p&gt;If it is correctly calculated, the check in requestMemorySegments method will fail directly and will not need the timeout mechanism to prevent the deadlock.&lt;/p&gt;</comment>
                            <comment id="16878958" author="gaoyunhaii" created="Fri, 5 Jul 2019 04:11:03 +0000"  >&lt;p&gt;Hi&#160;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=aitozi&quot; class=&quot;user-hover&quot; rel=&quot;aitozi&quot;&gt;aitozi&lt;/a&gt;, very thanks for concerning this issue. In my opinion, currently local buffer pool is configured by [required buffers, max buffers). The required buffers indicates how many buffers that ensures to be allocated and the max buffers indicates how many buffers to occupy at most. The requested buffers may be larger than the required buffers and it cannot exceed the max buffers.&lt;/p&gt;

&lt;p&gt;I think if we increase the number of required buffers when &lt;em&gt;requestMemorySegment&lt;/em&gt;, the effect is&#160;equivalent to judge whether a local buffer pool can be created via requested buffers. Furthermore, I think it would be also equivalent to the timeout method in the original PR with &lt;em&gt;timeout = 0&lt;/em&gt;: when we create a new local buffer pool, we see how many buffers are left and if it is not sufficient, the creation will fail. Since in many cases the over-allocated buffers of other local buffers can be recycled later, I think judging according to current remaining buffers might cause additional unnecessary failures.&lt;/p&gt;</comment>
                            <comment id="16878996" author="aitozi" created="Fri, 5 Jul 2019 06:29:52 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gaoyunhaii&quot; class=&quot;user-hover&quot; rel=&quot;gaoyunhaii&quot;&gt;gaoyunhaii&lt;/a&gt; for your explanation, I think over this today, it&apos;s indeed equivalent to your original PR with &lt;em&gt;timeout = 0&lt;/em&gt; and it&apos;s not a good solution. &lt;/p&gt;</comment>
                            <comment id="16879173" author="stephanewen" created="Fri, 5 Jul 2019 11:56:39 +0000"  >&lt;p&gt;So, in summary we have these options.&lt;/p&gt;

&lt;p&gt;  1. Only allocate the minimum per producer, &lt;b&gt;which is one buffer per channel&lt;/b&gt;. This would be needed to keep the requirement similar to what we have at the moment, but it is much less than we recommend for the credit-based network data exchange (2* channels + floating)&lt;br/&gt;
    ==&amp;gt; &lt;b&gt;Not a good option in my opinion&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;  2. Coordinate the deployment sink-to-source such that receivers always have their buffers first. This will be complex to implement and coordinate and break with many assumptions about tasks being independent (coordination wise) on the TaskManagers. Giving that assumption up will be a pretty big step and cause lot&apos;s of complexity in the future.&lt;br/&gt;
      It will also increase deployment delays. Low deployment delays should be a design goal in my opinion, as it will enable other features more easily, like low-disruption upgrades, etc.&lt;br/&gt;
   ==&amp;gt; &lt;b&gt;Not a good option either, in my opinion&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;  3. Make buffers always revokable, by spilling.&lt;br/&gt;
      This is tricky to implement very efficiently, especially because&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;there is the logic that slices buffers for early sends for the low-latency streaming stuff&lt;/li&gt;
	&lt;li&gt;the spilling request will come from an asynchronous call.  That will probably stay like that even with the mailbox, because the main thread will be frequently blocked on buffer allocation when this request comes.&lt;br/&gt;
   ==&amp;gt; I think this would explode in complexity and bugs, unless we rewrite other parts significantly. &lt;b&gt;Not a good option either&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;  4. We allocate the recommended number for good throughput (2*numChannels + floating) per consumer and per producer.&lt;br/&gt;
      No dynamic rebalancing any more. This would increase the number of required network buffers in certain high-parallelism scenarios quite a bit with the default config. Users can down-configure this by setting the per-channel buffers lower. But it would break user setups and require them to adjust the config when upgrading.&lt;br/&gt;
    ==&amp;gt; &lt;b&gt;Not a great option, in my opinion&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;  5. We make the network resource per slot and ask the scheduler to attach information about how many producers and how many consumers will be in the slot, worst case. We use that to pre-compute how many excess buffers the producers may take.&lt;br/&gt;
     This will also break with some assumptions and lead us to the point that we have to pre-compute network buffers in the same way as managed memory. Seeing how much pain it is with the managed memory, this seems not so great.&lt;br/&gt;
   ==&amp;gt; &lt;b&gt;Not a great option either&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;This is tricky. At this point it looks like option (4) is the only one that is feasible without severe performance issues or an explosion of complexity.&lt;/p&gt;</comment>
                            <comment id="16879272" author="pnowojski" created="Fri, 5 Jul 2019 13:42:07 +0000"  >&lt;p&gt;For option number #2, we would need a new RPC call from JobManager to TaskManager, &quot;task X is unblocked&quot;. Initial version could just send it once &quot;all tasks are running&quot;, which later can be refined to &quot;once all downstream tasks are running&quot;.&#160;Handling of&#160;&quot;task X is unblocked&quot; on the task managers&#160;maybe&#160;is not that difficult, considering that we already have the buffers rebalancing code.&#160;I think in the long term this would be the best solution if we want to keep min/optimal buffers range.&#160;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt; how is the yours number #4 different to what is proposed in the current PR? It sounds like the current proposal is a superset of functionality that you described. If buffer requirements are unable to be met, fail the task with a timeout while we try to keep min required buffers vs optimal number of buffers constraint. If that&apos;s annoyance for the user, he should adjust the network config,&#160;without any changes in the behaviour (except of failure instead of deadlock).&lt;/p&gt;</comment>
                            <comment id="16879328" author="stephanewen" created="Fri, 5 Jul 2019 14:32:53 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pnowojski&quot; class=&quot;user-hover&quot; rel=&quot;pnowojski&quot;&gt;pnowojski&lt;/a&gt; I think in the current PR, senders can still exceed that number of buffers (2*numChannels + floating) and thus cause deadlocks even all task could get that number of buffers (guarded by a timeout/fail). The suggestion would be to avoid dynamic rebalancing and fail eagerly if you cannot get that exact number.&lt;/p&gt;</comment>
                            <comment id="16879396" author="zjwang" created="Fri, 5 Jul 2019 15:58:12 +0000"  >&lt;p&gt;Thanks for above detail conclusions &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt;!&lt;/p&gt;

&lt;p&gt;I agree&#160;that the option 4 is the most feasible way ATM. The logic of fixed-size pool is easy and direct to avoid sharing buffers among tasks&#160;compared with dynamic size as now, so tasks are independent with each other, not relying on any assumptions. We ever realized the fixed-size local pool to avoid the buffers redistribution in early Blink version.&lt;/p&gt;

&lt;p&gt;I understand the option 1 has the same direction with option 4, both with fixed-size local pool. The difference is that option 4 takes the (2*numChannels + floating) as the fixed size&#160;of pool for better performance, and the option 1 takes the numChannels as the fixed size of pool. The option 1 is&#160;compatible with users when upgrading, but it might bring performance regression. The option 4 could keep the performance but might need user to adjust the config when upgrade failure.&lt;/p&gt;

&lt;p&gt;The other three options are&#160;indeed&#160;complex to implement and might break/rely on some assumptions to bring&#160;potential risks.&lt;/p&gt;</comment>
                            <comment id="16879607" author="gaoyunhaii" created="Sat, 6 Jul 2019 06:13:50 +0000"  >&lt;p&gt;Very thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt; for the detailed comparison of the different options. I also agree with that ATM option (4) should be the one method with the least modification. For option (4), we might&#160;also need to give users explicit guideline on the semantic changes of the buffer size.&#160;&lt;/p&gt;

&lt;p&gt;On the other side, I still think that the core/max semantics of the local buffer pool might be more flexible, and might have performance gain for cases&#160;like the ability of the downstream tasks is not stable. The&#160;core/max semantics might still be an option if other conditions are satisfied in the more longer future.&#160;&lt;/p&gt;

&lt;p&gt;At last, in considering of the advantages and disadvantages of the current options, and the publish timeline of 1.9, now do we still plan&#160;to have this issue fixed (to some extend) with some method in 1.9.0 ?&lt;/p&gt;</comment>
                            <comment id="16882068" author="stephanewen" created="Wed, 10 Jul 2019 13:41:17 +0000"  >&lt;p&gt;All the options above are very invasive or may break user setups. At the same time, the bug seems to occur rarely.&lt;/p&gt;

&lt;p&gt;I would suggest to actually not rush anything for 1.9 but go with the initially proposed solution to catch this situation (via a timeout) and trigger a recovery.&lt;/p&gt;

&lt;p&gt;It&apos;s not the perfect solution, but the least invasive for now and we can think about how to properly fix this in the future.&lt;/p&gt;

&lt;p&gt;So I am giving +1 for the original solution in the PR from Yun.&lt;/p&gt;</comment>
                            <comment id="16882701" author="pnowojski" created="Thu, 11 Jul 2019 07:26:16 +0000"  >&lt;p&gt;I&apos;m closing this ticket as a timeout hotfix and I&apos;m setting a proper release note. I&apos;ve created a new ticket &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-13203&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-13203&lt;/a&gt; for a proper fix. &lt;/p&gt;

&lt;p&gt;Merged to master as cf884899c72410e8365c2e5637374b82d846eb4e&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13244241">FLINK-13203</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13397927">FLINK-24035</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13271878">FLINK-15031</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 18 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z03qug:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>To avoid a potential deadlock, this adds a timeout (default value of 30 seconds, configurable via {{taskmanager.network.memory.exclusive-buffers-request-timeout-ms}}) for how long Task will be waiting for assignment of exclusive memory segments. However it is possible that for some previously working deployments this default timeout value is too low and might has to be increased.</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>