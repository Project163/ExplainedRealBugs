<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 21:19:43 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-34007] Flink Job stuck in suspend state after losing leadership in HA Mode</title>
                <link>https://issues.apache.org/jira/browse/FLINK-34007</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;The observation is that Job manager goes to suspend state with a failed container not able to register itself to resource manager after timeout.&lt;/p&gt;

&lt;p&gt;JM Log, see attached&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13563759">FLINK-34007</key>
            <summary>Flink Job stuck in suspend state after losing leadership in HA Mode</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="mapohl">Matthias Pohl</assignee>
                                    <reporter username="ZhenqiuHuang">Zhenqiu Huang</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Sat, 6 Jan 2024 03:35:48 +0000</created>
                <updated>Mon, 5 Feb 2024 22:10:44 +0000</updated>
                            <resolved>Mon, 5 Feb 2024 22:10:10 +0000</resolved>
                                    <version>1.19.0</version>
                    <version>1.18.1</version>
                    <version>1.18.2</version>
                                    <fixVersion>1.19.0</fixVersion>
                                    <component>Runtime / Coordination</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>9</watches>
                                                                                                                <comments>
                            <comment id="17804840" author="zhenqiuhuang" created="Tue, 9 Jan 2024 18:45:03 +0000"  >&lt;p&gt;From initial investigation, the job manager is initially lose the leadership, then goes to SUSPENDED status. Shouldn&apos;t the job manager exit directly rather than goes to SUSPENDED status?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17805146" author="mapohl" created="Wed, 10 Jan 2024 13:39:11 +0000"  >&lt;p&gt;Thanks for sharing this issue with us, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ZhenqiuHuang&quot; class=&quot;user-hover&quot; rel=&quot;ZhenqiuHuang&quot;&gt;ZhenqiuHuang&lt;/a&gt;. In an offline conversation with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gyfora&quot; class=&quot;user-hover&quot; rel=&quot;gyfora&quot;&gt;gyfora&lt;/a&gt; it was said that you observed this issue not only with Flink 1.18 but also 1.17 and 1.16. Can you confirm that?&lt;/p&gt;</comment>
                            <comment id="17805148" author="gyfora" created="Wed, 10 Jan 2024 13:42:38 +0000"  >&lt;p&gt;Yes we can confirm &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mapohl&quot; class=&quot;user-hover&quot; rel=&quot;mapohl&quot;&gt;mapohl&lt;/a&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17805157" author="mapohl" created="Wed, 10 Jan 2024 14:12:21 +0000"  >&lt;p&gt;Ok, I went through the log file you shared. AFAIS, suspending the JobManager worked as expected:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The Job with the ID &lt;tt&gt;217cee964b2cfdc3115fb74cac0ec550&lt;/tt&gt; was suspended due to the leadership loss for session ID &lt;tt&gt;9987190b-35f4-4238-b317-057dc3615e4d&lt;/tt&gt;.&lt;/li&gt;
	&lt;li&gt;The ResourceManager and the Dispatcher got their leadership revoked as well.&lt;/li&gt;
	&lt;li&gt;The ResourceManager is not shut down.&lt;/li&gt;
	&lt;li&gt;The Dispatcher is stopped but the corresponding DispatcherLeaderProcess keeps running. That&apos;s the process that should trigger another Dispatcher initialization if it picks up leadership again.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;The &lt;tt&gt;RecipientUnreachableException&lt;/tt&gt; appears because there&apos;s no leader being re-elected, I guess. Does this match your findings?&lt;/p&gt;

&lt;p&gt;You&apos;re not having any other standby JM running in the Flink cluster as far as I understand? We would expect this very same JobManager to pick up leadership again. Do we have some logs from the Kubernetes cluster that we could investigate?&lt;/p&gt;</comment>
                            <comment id="17805203" author="zhenqiuhuang" created="Wed, 10 Jan 2024 15:53:43 +0000"  >&lt;p&gt;I have been investigating the io.fabric8.kubernetes.client.extended.leaderelection.LeaderElector. Looks like there is a bug in the version 5.12.4, when the leader lose leadership, the loop executor will be shutdown. That is why the leader is not acquired again. Thus, bump the kubernetest-client version will probably help.&lt;/p&gt;

&lt;p&gt;There is a fix in this PR.&lt;br/&gt;
&lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/commit/042c77b360e77dfaac4ae713518b684dcd0d985b&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/fabric8io/kubernetes-client/commit/042c77b360e77dfaac4ae713518b684dcd0d985b&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17805217" author="mapohl" created="Wed, 10 Jan 2024 16:30:18 +0000"  >&lt;p&gt;The fix you shared ended up in 6.0.0. That would mean that we shouldn&apos;t experience the issue in a Flink 1.18 cluster. The fabric8 client was updated to 6.6.2 in 1.18.0 (&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-31997&quot; title=&quot;Update to Fabric8 6.5.1+ in flink-kubernetes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-31997&quot;&gt;&lt;del&gt;FLINK-31997&lt;/del&gt;&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="17805294" author="zhenqiuhuang" created="Wed, 10 Jan 2024 21:02:05 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mapohl&quot; class=&quot;user-hover&quot; rel=&quot;mapohl&quot;&gt;mapohl&lt;/a&gt;&lt;br/&gt;
Yes, I mistakenly looked into the flink 1.17 source code. I uploaded another debug log above. The KubernetesLeaderElector check the annotation &quot;control-plane.alpha.kubernetes.io/leader&quot; and whether the lockIdentity exists in content. Given this job only has 1 job manager, there should be no other job manager instance try to acquire the lock. The only possibility is that somehow the cluster config map is returned incorrectly.&lt;/p&gt;

&lt;p&gt;In this case, even fabric8 LeaderElector will continue to try to acquire leadership (If it can get without exceed deadline), flink will not able to restart services (such RM and dispatcher) as DefaultLeaderRetrievalService is stopped also. To resolve the issue for now, should we focus on gracefully shutdown Job Manager rather than move job to Suspended status?  &lt;/p&gt;</comment>
                            <comment id="17805411" author="mapohl" created="Thu, 11 Jan 2024 07:36:28 +0000"  >&lt;p&gt;I still don&apos;t fully understand the error you shared: Shouldn&apos;t the KubernetesClientException resolve itself because the logic runs in a loop? Is this stacktrace you shared only a one-time thing or does it reoccur (which would confirm the execution in the loop and indicate that the ConfigMap is in some odd state)? Another thing I&apos;m wondering is why the ConfigMap was concurrently updated (which caused the KubernetesClientException as far as I understand) when there&apos;s only one JM running. Are there other processes accessing the ConfigMap?&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt; flink will not able to restart services (such RM and dispatcher) as DefaultLeaderRetrievalService is stopped also &lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt;&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;The DefaultLeaderRetrievalService is not in charge of restarting any services. The LeaderElectionService will trigger the restart of any shut down services (in our case the SessionDispatcherLeaderProcess which would be started by the DefaultDispatcherRunner; the latter one maintains the Dispatcher&apos;s leader election) as soon as the JobManager gets the leadership again.&lt;/p&gt;</comment>
                            <comment id="17805487" author="fly_in_gis" created="Thu, 11 Jan 2024 10:15:37 +0000"  >&lt;p&gt;Do you mean the &lt;tt&gt;KubernetesLeaderElector&lt;/tt&gt; could not obtain the leadership due to continuous resource conflicts? I am not sure of this because you only share one line DEBUG log.&lt;/p&gt;</comment>
                            <comment id="17805495" author="davidmoravek" created="Thu, 11 Jan 2024 10:38:59 +0000"  >&lt;p&gt;If there would have been a continuous conflict, I&apos;d assume the job would fail sooner.&lt;/p&gt;

&lt;p&gt;Can you check whether you&apos;re not rate-limited by the k8s apiserver? We&apos;ve seen similar issues in the past. The &quot;multicomponent leader election&quot; was one efforts to lower the pressure there, do you have it enabled?&lt;/p&gt;</comment>
                            <comment id="17805540" author="mapohl" created="Thu, 11 Jan 2024 12:02:32 +0000"  >&lt;p&gt;fyi: multi-component leader election is enabled/used by default since 1.16 (&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-25806&quot; title=&quot;Remove legacy high availability services&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-25806&quot;&gt;&lt;del&gt;FLINK-25806&lt;/del&gt;&lt;/a&gt;). So, all the deployments &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ZhenqiuHuang&quot; class=&quot;user-hover&quot; rel=&quot;ZhenqiuHuang&quot;&gt;ZhenqiuHuang&lt;/a&gt; talked about should have been using multi-component leader election.&lt;/p&gt;</comment>
                            <comment id="17805871" author="fly_in_gis" created="Fri, 12 Jan 2024 04:02:48 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mapohl&quot; class=&quot;user-hover&quot; rel=&quot;mapohl&quot;&gt;mapohl&lt;/a&gt; Could you please confirm that whether &quot;multi-component leader election&quot; will clean up the leader annotation on the ConfigMap when lost leadership?&lt;/p&gt;

&lt;p&gt;It seems that the fabric8 Kubernetes client leader elector will not work properly by &lt;tt&gt;run()&lt;/tt&gt; more than once if we do not clean up the leader annotation.&lt;/p&gt;</comment>
                            <comment id="17805960" author="mapohl" created="Fri, 12 Jan 2024 09:19:21 +0000"  >&lt;p&gt;I couldn&apos;t find anything related to ConfigMap cleanup being triggered during leadership loss in the Flink code (Flink&apos;s &lt;a href=&quot;https://github.com/apache/flink/blob/c5808b04fdce9ca0b705b6cc7a64666ab6426875/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L82&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KubernetesLeaderElector:82&lt;/a&gt; sets up the &lt;a href=&quot;https://github.com/apache/flink/blob/c5808b04fdce9ca0b705b6cc7a64666ab6426875/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L124&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;callback for the leadership loss&lt;/a&gt; which is implemented by &lt;a href=&quot;https://github.com/apache/flink/blob/11259ef52466889157ef473f422ecced72bab169/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesLeaderElectionDriver.java#L212&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KubernetesLeaderElectionDriver#LeaderCallbackHandlerImpl&lt;/a&gt;). This behavior is on-par with the old (i.e. pre-&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-24038&quot; title=&quot;DispatcherResourceManagerComponent fails to deregister application if no leading ResourceManager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-24038&quot;&gt;&lt;del&gt;FLINK-24038&lt;/del&gt;&lt;/a&gt;) version of the Flink 1.15 codebase (see 1.15 class &lt;a href=&quot;https://github.com/apache/flink/blob/bba7c417217be878fffb12efedeac50dec2a7459/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesLeaderElectionDriver.java#L202&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KubernetesLeaderElectionDriver:202&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;That&apos;s also not something I would expect. It should be handled by the LeaderElector, instead, because the LeaderElector knows the state of the leader election and can trigger a clean up before the new leader information is written to the ConfigMap entry. Flink shouldn&apos;t trigger a clean up because it doesn&apos;t know whether a new leader was already elected (in which case cleaning up the ConfigMap entry would result in losing the leadership information of the new leader). And the Flink process wouldn&apos;t be able to clean it up, anyway, because the process isn&apos;t the leader anymore. Or am I missing something here?&lt;/p&gt;

&lt;p&gt;On another note: I came across &lt;a href=&quot;https://github.com/apache/flink/pull/22540/files#diff-0e859df42954459619211d2ec60957742b24c9fc6ce55526616fddc540f0f8ffL59-R60&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;this change&lt;/a&gt; in the &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-31997&quot; title=&quot;Update to Fabric8 6.5.1+ in flink-kubernetes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-31997&quot;&gt;&lt;del&gt;FLINK-31997&lt;/del&gt;&lt;/a&gt; PR (k8s client update to 6.6.2): We&apos;re changing the thread pool size from 1 to 3 essentially allowing the same internal LeaderElector being executed multiple times (because we trigger another &lt;tt&gt;KubernetesLeaderElector#run&lt;/tt&gt; call when the leadership is revoked). The old version of the code uses a single thread which would mean that the run call would get queued until the previous &lt;tt&gt;LeaderElector#run&lt;/tt&gt; failed for whatever reason. That change sounds strange but shouldn&apos;t be the cause of this Jira issue because the change only went into 1.18 and we&apos;re experiencing this also in older versions of Flink.&lt;/p&gt;</comment>
                            <comment id="17806410" author="zhenqiuhuang" created="Sun, 14 Jan 2024 00:23:59 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mapohl&quot; class=&quot;user-hover&quot; rel=&quot;mapohl&quot;&gt;mapohl&lt;/a&gt;&lt;br/&gt;
Yes, from the observation on the failure case, the ConfigMap was not cleanup when job manager lose the leadership. Even the renewTime field is no longer upgraded by leader elector, it means leader elector already goes out of its run loop. If look into the fabric8 leader elector source code, it looks like only when renew deadline expired, LeaderElector will abort from its run loop. Even through I don&apos;t know why  renew deadline expired, enlarge the high-availability.kubernetes.leader-election.renew-deadline value could isolate some transient issues. &lt;/p&gt;

&lt;p&gt;I have started a testing job with debug log of both io.fabric8.kubernetes.client.extended.leaderelection and flink kubernetes leader election modules two days ago. If the job fail, I will post new logs in this thread.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wangyang0918&quot; class=&quot;user-hover&quot; rel=&quot;wangyang0918&quot;&gt;wangyang0918&lt;/a&gt;&lt;br/&gt;
Would you please elaborate a little bit why &quot;It seems that the fabric8 Kubernetes client leader elector will not work properly by run() more than once if we do not clean up the leader annotation.&quot;?&lt;/p&gt;
</comment>
                            <comment id="17806600" author="fly_in_gis" created="Mon, 15 Jan 2024 02:50:17 +0000"  >&lt;p&gt;Maybe I did not make myself clear. I mean the old leader JM should try to remove the annotation of HA ConfigMap &lt;tt&gt;control-plane.alpha.kubernetes.io/leader&lt;/tt&gt; when lost leadership. From the fabric8 K8s client impl&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, &lt;tt&gt;isLeader&lt;/tt&gt; callback will be executed only when the holder identity changed.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;. &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/v6.6.2/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L232&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/fabric8io/kubernetes-client/blob/v6.6.2/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L232&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17806699" author="mapohl" created="Mon, 15 Jan 2024 08:56:07 +0000"  >&lt;p&gt;Sorry, I misunderstood you initially. If the annotation is the problem, I would assume the issue being somewhere within the k8s client library. The annotation was never touched by the Flink code (there&apos;s only one read access in &lt;a href=&quot;https://github.com/apache/flink/blob/c5808b04fdce9ca0b705b6cc7a64666ab6426875/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L115&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KubernetesLeaderElector#hasLeadership&lt;/a&gt; to verify the leadership). ...even before the changes of &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-24038&quot; title=&quot;DispatcherResourceManagerComponent fails to deregister application if no leading ResourceManager&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-24038&quot;&gt;&lt;del&gt;FLINK-24038&lt;/del&gt;&lt;/a&gt;. The lifecycle of the annotation is handled within the fabric8.io client&apos;s &lt;tt&gt;LeaderElector&lt;/tt&gt;. I&apos;m gonna do some more investigation of that code segment. But having more debug k8s/kubernetes-client logs would help, I guess.&lt;/p&gt;</comment>
                            <comment id="17806714" author="fly_in_gis" created="Mon, 15 Jan 2024 09:34:55 +0000"  >&lt;p&gt;Maybe this issue is related with fabric8 K8s client behavior change. In v5.12.4&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;, the &lt;tt&gt;onStartLeading&lt;/tt&gt; callback will always be executed when acquired the leadership. But in v6.6.2&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;, the &lt;tt&gt;onStartingLeading&lt;/tt&gt; callback will be executed only when the holder identity changed. In this case, the JobManager does not crash while the holder identity keeps same.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;. &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/v5.12.4/kubernetes-client/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L71&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/fabric8io/kubernetes-client/blob/v5.12.4/kubernetes-client/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L71&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;. &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/v6.6.2/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L232&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/fabric8io/kubernetes-client/blob/v6.6.2/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L232&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="17806759" author="mapohl" created="Mon, 15 Jan 2024 11:21:52 +0000"  >&lt;p&gt;But it should be an issue that is available in different k8s client version (not only 6.6.2):&lt;/p&gt;
&lt;div class=&apos;table-wrap&apos;&gt;
&lt;table class=&apos;confluenceTable&apos;&gt;&lt;tbody&gt;
&lt;tr&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Flink&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;k8s client&lt;/th&gt;
&lt;th class=&apos;confluenceTh&apos;&gt;Jira issue&lt;/th&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.18&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;6.6.2&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-31997&quot; title=&quot;Update to Fabric8 6.5.1+ in flink-kubernetes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-31997&quot;&gt;&lt;del&gt;FLINK-31997&lt;/del&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.17&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.12.4&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-30231&quot; title=&quot;Update to Fabric8 Kubernetes Client to a version that has automatic renewal of service account tokens&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-30231&quot;&gt;&lt;del&gt;FLINK-30231&lt;/del&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.16&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.12.3&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-28481&quot; title=&quot;Bump the fabric8 kubernetes-client to 5.12.3&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-28481&quot;&gt;&lt;del&gt;FLINK-28481&lt;/del&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;1.14-1.15&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;5.5.0&lt;/td&gt;
&lt;td class=&apos;confluenceTd&apos;&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-22802&quot; title=&quot;Bump Fabric8 Kubernetes Client to &amp;gt;= 5.X&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-22802&quot;&gt;&lt;del&gt;FLINK-22802&lt;/del&gt;&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;&lt;/table&gt;
&lt;/div&gt;


&lt;p&gt;At least based on the reports of this Jira issue, there must have been an incident (which caused the lease to not be renewed) in a k8s cluster that triggered the same failure in multiple Flink clusters (with versions of 1.18, 1.17 and 1.16 at least) that triggered the same issue in all of those deployments. ...if I understand it correctly.&lt;/p&gt;

&lt;p&gt;Therefore, the issue should exist in the entire version range &lt;span class=&quot;error&quot;&gt;&amp;#91;5.12.3, 6.6.2&amp;#93;&lt;/span&gt;. But the change you&apos;re referring to (&lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/pull/4125&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;PR #4152&lt;/a&gt;) seems to be the only bigger change in &lt;tt&gt;LeaderElector&lt;/tt&gt;, indeed.&lt;/p&gt;

&lt;p&gt;&#8212;&lt;/p&gt;

&lt;p&gt;On another note: I remembered that there is a slight difference in the revocation protocol in the &lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLINK/FLIP-285%3A+Refactoring+LeaderElection+to+make+Flink+support+multi-component+leader+election+out-of-the-box&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;FLIP-285&lt;/a&gt; changes:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;The old implementation (see &lt;a href=&quot;https://github.com/apache/flink/blob/6e1caa390882996bf2d602951b54e4bb2d9c90dc/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L238&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;1.15 DefaultLeaderElectionService:238&lt;/a&gt;) did try to clear the leader information from the ConfigMap.&lt;/li&gt;
	&lt;li&gt;The new implementation (see &lt;a href=&quot;https://github.com/apache/flink/blob/773feebbb2426ab1a8f7684f59b9a73db8f6a613/flink-runtime/src/main/java/org/apache/flink/runtime/leaderelection/DefaultLeaderElectionService.java#L484&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;1.18+ DefaultLeaderElectionService:484&lt;/a&gt;) doesn&apos;t clear the component leader information, anymore. Here, the reasoning was that the data wouldn&apos;t be able to be updated, anymore, because the leadership is already lost.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;But that change still seems to be reasonable based on my findings: In the k8s client 6.6.2 codebase, &lt;tt&gt;stopLeading&lt;/tt&gt; is either called after noticing the change in the lock identity (&lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/f91e0bd8e364f9a3758af0b90b9c661d0fc0a9eb/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L238&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LeaderElector:L238&lt;/a&gt;; the lock identity change would prevent the clearing of the data) or when the lease wasn&apos;t renewed (&lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/f91e0bd8e364f9a3758af0b90b9c661d0fc0a9eb/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L95&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LeaderElector:95&lt;/a&gt;) where we would have to assume that other leader information is already written. And this change shouldn&apos;t be related to the issues with the lock lifecycle in general because it only affects metadata and not the lock annotation itself, should it? WDYT?&lt;/p&gt;</comment>
                            <comment id="17806782" author="mapohl" created="Mon, 15 Jan 2024 12:33:02 +0000"  >&lt;blockquote&gt;
&lt;p&gt;At least based on the reports of this Jira issue, there must have been an incident (which caused the lease to not be renewed) in a k8s cluster that triggered the same failure in multiple Flink clusters (with versions of 1.18, 1.17 and 1.16 at least) that triggered the same issue in all of those deployments. ...if I understand it correctly.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ZhenqiuHuang&quot; class=&quot;user-hover&quot; rel=&quot;ZhenqiuHuang&quot;&gt;ZhenqiuHuang&lt;/a&gt; can you elaborate a bit on the incident itself. ...just to get a bit more context. Did I understand it correctly that there were different Flink versions deployed in a single Kubernetes cluster which run independently and all of them ran into the same issue around the same time (indicating that would have been caused by the same event). Or did the failures in the different Flink clusters happen independently from each other over a longer stretch of time?&lt;/p&gt;</comment>
                            <comment id="17807009" author="fly_in_gis" created="Tue, 16 Jan 2024 02:13:31 +0000"  >&lt;blockquote&gt;&lt;p&gt;At least based on the reports of this Jira issue, there must have been an incident (which caused the lease to not be renewed)&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;I am afraid we could not get this conclusion before we have the K8s APIServer audit logs to verify that the lease annotation did not get renewed. Because it could also happen that the lease annotation get renewed normally while the onStartLeading callback is not executed somehow.&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Therefore, the issue should exist in the entire version range&#160;&lt;span class=&quot;error&quot;&gt;&amp;#91;5.12.3, 6.6.2&amp;#93;&lt;/span&gt;.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;If this issue only happened in the Flink 1.18, then it should be related with the fabric8 K8s client 6.6.2 behavior change. Otherwise, we still have not find the root cause.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;You are right. The slight difference in the revocation protocol in the&#160;&lt;a href=&quot;https://cwiki.apache.org/confluence/display/FLINK/FLIP-285%3A+Refactoring+LeaderElection+to+make+Flink+support+multi-component+leader+election+out-of-the-box&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;FLIP-285&lt;/a&gt; changes about clear the leader information in ConfigMap is not related with this issue.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;BTW, if we know how to reproduce this issue, it will be easier to find the root cause. Because we might also need the K8s APIServer audit log to do some deep analysis.&lt;/p&gt;</comment>
                            <comment id="17807017" author="zhenqiuhuang" created="Tue, 16 Jan 2024 03:34:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mapohl&quot; class=&quot;user-hover&quot; rel=&quot;mapohl&quot;&gt;mapohl&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wangyang0918&quot; class=&quot;user-hover&quot; rel=&quot;wangyang0918&quot;&gt;wangyang0918&lt;/a&gt;&lt;br/&gt;
I am intensively testing flink 1.18. Within two days, there are users reported the job manager stuck issue in 1.17 and 1.16. 1.18 and 1.17 job instances are running in the same cluster. 1.16 is in different cluster.&lt;/p&gt;

&lt;p&gt;I attached another LeaderElector-Debug.json file that contains debug log of a flink 1.18 app. The issue happened several times:&lt;br/&gt;
1. due to the configmap not accessible from api sever then renew timeout exceeded. &lt;br/&gt;
2. a failure on patch on a updated configmap&lt;/p&gt;


&lt;p&gt;The interesting part of the behavior of last several days is that job manager was not stuck but exit directly. Then, new job manager pod started correctly that is why new leader is selected in the log above. Hopefully, it is useful for your diagnosis.&lt;/p&gt;


&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wangyang0918&quot; class=&quot;user-hover&quot; rel=&quot;wangyang0918&quot;&gt;wangyang0918&lt;/a&gt;&lt;br/&gt;
From my initial observation (before creating the jira), the leader annotation update stopped when job manager was stuck. &lt;/p&gt;



</comment>
                            <comment id="17807229" author="mapohl" created="Tue, 16 Jan 2024 13:01:04 +0000"  >&lt;p&gt;The timeout of the renew operation leads to a &lt;tt&gt;stopLeading&lt;/tt&gt; call in &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L119&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LeaderElector:96&lt;/a&gt; which results in LeaderElector being stopped (the &lt;tt&gt;stopped&lt;/tt&gt; flag is set to true in &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L119&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LeaderElector:119&lt;/a&gt; and is never disabled again). Any later call of &lt;tt&gt;tryAcquireOrRenew&lt;/tt&gt; will not perform any action because of the &lt;tt&gt;stopped&lt;/tt&gt; flag being true (see &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L211&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LeaderElector#tryAcquireOrRenew:211&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Flink doesn&apos;t re-instantiate the &lt;tt&gt;LeaderElector&lt;/tt&gt; but calls &lt;tt&gt;LeaderElector#run&lt;/tt&gt; on the same (now stopped) instance in the &lt;a href=&quot;https://github.com/apache/flink/blob/11259ef52466889157ef473f422ecced72bab169/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/highavailability/KubernetesLeaderElectionDriver.java#L214&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KubernetesLeaderElectionDriver#notLeader&lt;/a&gt; callback.&lt;/p&gt;

&lt;p&gt;That explains the behavior in the Flink 1.18 deployments, if I didn&apos;t miss anything. It sounds like a bug in the fabric8io k8s client implementation which we should be able to workaround by recreating a new LeaderElector on leadership loss. I replied to &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/issues/5635&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;fabric8io:kubernetes-client#5635&lt;/a&gt; because the bug report provides the same stacktrace. That also explains why restarting the JobManager fixes the issue. Because that results in a new (not-stopped) LeaderElector being instantiated.&lt;/p&gt;

&lt;p&gt;I&apos;m still puzzled about the issues in other Flink versions, though.&lt;/p&gt;</comment>
                            <comment id="17807538" author="fly_in_gis" created="Wed, 17 Jan 2024 04:10:07 +0000"  >&lt;p&gt;I think you are right that &lt;tt&gt;stopped&lt;/tt&gt; flag in &lt;tt&gt;LeaderElector&lt;/tt&gt; never reset after lost leadership. However, I am afraid even though we recreate a new LeaderElector it still not works unless we have a different &lt;tt&gt;HolderIdentity&lt;/tt&gt;. From &lt;a href=&quot;#L232]&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;LeaderElector:232&lt;/a&gt;, &lt;tt&gt;onStartLeading&lt;/tt&gt; only happens&#160;when the holder identity changes.&lt;/p&gt;</comment>
                            <comment id="17807612" author="mapohl" created="Wed, 17 Jan 2024 07:47:05 +0000"  >&lt;p&gt;good point, a the holder identity would have to be reset as well.&lt;/p&gt;

&lt;p&gt;I&apos;m also wondering whether we should enable &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L124&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;isReleaseOnCancel()&lt;/a&gt;. That config property was never used in the kubernetes-client v5.12.4 codebase and seems to be disabled by default. We&apos;re not enabling it in &lt;a href=&quot;https://github.com/apache/flink/blob/c5808b04fdce9ca0b705b6cc7a64666ab6426875/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L69&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KubernetesLeaderElector:69&lt;/a&gt; on the Flink side. Releasing the lock identify might help in certain situation where we don&apos;t want to wait for the lease to timeout. But that&apos;s just a minor improvement that I came across while looking through the v6.6.2 code.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ZhenqiuHuang&quot; class=&quot;user-hover&quot; rel=&quot;ZhenqiuHuang&quot;&gt;ZhenqiuHuang&lt;/a&gt; But that finding still doesn&apos;t explain that you&apos;re seeing the same/similar issues with Flink 1.17- deployments. Because the update of the kubernetes-client to v6.6.2 only happened in 1.18.0.&lt;/p&gt;</comment>
                            <comment id="17807700" author="fly_in_gis" created="Wed, 17 Jan 2024 11:35:11 +0000"  >&lt;p&gt;I agree with you that we could enable &lt;tt&gt;isReleaseOnCancel&lt;/tt&gt;, which will set the holder identity to empty.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;If this issue also existed in 1.17-, then we might need to get the jstack of JobManager to see where the &lt;tt&gt;LeaderElector&lt;/tt&gt; stuck.&lt;/p&gt;</comment>
                            <comment id="17808237" author="mapohl" created="Thu, 18 Jan 2024 14:28:01 +0000"  >&lt;p&gt;I checked the implementation (since we&apos;re getting close to the 1.19 feature freeze). We have the following options:&lt;/p&gt;
&lt;ol&gt;
	&lt;li&gt;We could downgrade the fabric8io kubernetes client dependency back to v5.12.4 (essentially reverting &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-31997&quot; title=&quot;Update to Fabric8 6.5.1+ in flink-kubernetes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-31997&quot;&gt;&lt;del&gt;FLINK-31997&lt;/del&gt;&lt;/a&gt;).&lt;/li&gt;
	&lt;li&gt;We could fix the issue in the fabric8io kubernetes client and update the dependency as soon as the fix is released. Here, I&apos;m also not that confident that we would be able to bring the fix into Flink before 1.19 should be released. ...because we&apos;re relying on the release of another project.&lt;/li&gt;
	&lt;li&gt;Refactor the k8s implementation to allow the restart of the KubernetesLeaderElector within the KubernetesLeaderElectionDriver. That would require updating the lockIdentify as well. The problem is that the lockIdentity is actually not owned by the KubernetesLeaderElector but by the k8s HighAvailabilityServices (because it&apos;s also used by the KubernetesStateHandleStore when checking the leadership). Even though moving the lockIdentity into the KubernetesLeaderElector makes sense (the KubernetesStateHandleStore should rely on the LeaderElectionService to detect whether leadership is acquired, instead), it is a larger effort and I am hesitant to work on that one that closely to the 1.19 feature freeze.&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;I feel like we should apply all the three options in the above order. Option #1 would end up in 1.19.0 and 1.18.3 with option #2 being the follow-up. Option #3 could be considered as a dedicated refactoring effort in 1.20 or later. What&apos;s your view on that?&lt;/p&gt;

&lt;p&gt;My proposal only covers the issue we identified in k8s client v6.6.2. I ignored the fact that there are issues observed in deployments with Flink 1.17-. Any investigation around Flink 1.17- leadership issues should be moved into a separate Jira issue.&lt;/p&gt;</comment>
                            <comment id="17808252" author="gyfora" created="Thu, 18 Jan 2024 14:42:44 +0000"  >&lt;p&gt;The fabric8 community is super response and agile so fixing bugs and getting a release is not an issue. If we feel that there is a bug there we should report it immediately and it may get fixed in a few days.&lt;/p&gt;

&lt;p&gt;I personally don&apos;t like the idea of reverting to the old fabric8 version. It has a few very serious issues and this can also cause issues for dependent projects like the Kubernetes operator.&lt;/p&gt;</comment>
                            <comment id="17808257" author="mapohl" created="Thu, 18 Jan 2024 14:53:47 +0000"  >&lt;p&gt;Fair enough. Or we could come up with a Flink-local implementation of the fabric8io LeaderElector including a possible fix, if a release of the fabric8io kubernetes client takes too long.&lt;/p&gt;

&lt;p&gt;I raised the priority for the issue to Blocker for now. I guess we should get this one fixed before the 1.19 release. Additionally, I removed the affected versions 1.17 and 1.16 (&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ZhenqiuHuang&quot; class=&quot;user-hover&quot; rel=&quot;ZhenqiuHuang&quot;&gt;ZhenqiuHuang&lt;/a&gt; please create another Jira issue if we have more insights into what&apos;s causing the leader election issues in the Flink 1.17-)&lt;/p&gt;</comment>
                            <comment id="17808334" author="zhenqiuhuang" created="Thu, 18 Jan 2024 18:03:50 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mapohl&quot; class=&quot;user-hover&quot; rel=&quot;mapohl&quot;&gt;mapohl&lt;/a&gt; Ack. There are no new observations from last 2 days&apos; testing result. The only thing that probably worth to mention is that when the LeaderElector (3 thread executor) exit from renew deadline out, it is actually one of the thread exit from the loop. From the debug log, I can still observe 2 thread consistently failed to acquire the leadership due to it the stop flag.&lt;/p&gt;


&lt;p&gt;For the 1.17, I will create an instance for testing in the same cluster today. Let&apos;s see what&apos;s the result.&lt;/p&gt;</comment>
                            <comment id="17808441" author="fly_in_gis" created="Fri, 19 Jan 2024 03:15:20 +0000"  >&lt;p&gt;I also remember that the fabric8 kubernetes-client community has a very good response. If the &lt;tt&gt;LeaderElector&lt;/tt&gt; is designed for only run once, though I do not think this is the reasonable behavior, then we need to create a new &lt;tt&gt;LeaderElector&lt;/tt&gt; when lost leadership.&lt;/p&gt;

&lt;p&gt;For option #3, it might be unnecessary because &lt;tt&gt;LeaderElector&lt;/tt&gt; could work as expected when creating a new instance with same lock identity. It is a larger effort to do such refactor without additional benefits.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;BTW, maybe I miss some background. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gyfora&quot; class=&quot;user-hover&quot; rel=&quot;gyfora&quot;&gt;gyfora&lt;/a&gt; Could you please share me why we need to change the thread pool to 3 in &lt;tt&gt;KubernetesLeaderElector&lt;/tt&gt;?&lt;/p&gt;</comment>
                            <comment id="17808485" author="gyfora" created="Fri, 19 Jan 2024 06:41:17 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wangyang0918&quot; class=&quot;user-hover&quot; rel=&quot;wangyang0918&quot;&gt;wangyang0918&lt;/a&gt; the tests failed. The executor service (single threaded) previously was only used to execute Flink side logic and now we had to pass it to the LeaderElector itself as well so a single thread kind of deadlocked it somehow.&lt;/p&gt;

&lt;p&gt;So I increased to 3 and it made it work. Yesterday I started to think that it may actually be a reason why we see ConfigMap version conficts (and lost leaderships) in the first place.&lt;/p&gt;

&lt;p&gt;This is probably unrelated to why it cannot recover the leadership but I am going to try to change back to 1 or use 2 different single threaded executors.&lt;/p&gt;</comment>
                            <comment id="17808872" author="mapohl" created="Sat, 20 Jan 2024 06:11:46 +0000"  >&lt;p&gt;Increasing the thread count appeared to be necessary because the old Flink code executed the fabric8io &lt;tt&gt;LeaderElector#run&lt;/tt&gt; command in &lt;a href=&quot;https://github.com/apache/flink/blob/c5808b04fdce9ca0b705b6cc7a64666ab6426875/flink-kubernetes/src/main/java/org/apache/flink/kubernetes/kubeclient/resources/KubernetesLeaderElector.java#L103&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;KubernetesLeaderElector#run&lt;/a&gt; in the same thread pool that is used in the fabric8io LeaderElector for running the CompetableFutures in the loop:&lt;/p&gt;

&lt;p&gt;The &lt;tt&gt;LeaderElector#run&lt;/tt&gt; call waits for the CompletableFuture returned by &lt;tt&gt;LeaderElector#acquire&lt;/tt&gt; to complete (see &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L70&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LeaderElector:70&lt;/a&gt;). &lt;tt&gt;LeaderElector#acquire&lt;/tt&gt; will trigger an asynchronous call on the &lt;tt&gt;executorService&lt;/tt&gt; which wouldn&apos;t pick up the task because the single thread is waiting for the acquire call to complete. This deadlock situation is reproduced by Flink&apos;s &lt;tt&gt;KubernetesLeaderElectionITCase&lt;/tt&gt;. I would imagine that this is the timeout, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=gyfora&quot; class=&quot;user-hover&quot; rel=&quot;gyfora&quot;&gt;gyfora&lt;/a&gt; was experiencing when upgrading to v6.6.2.&lt;/p&gt;

&lt;p&gt;The 3 threads that were introduced by &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-31997&quot; title=&quot;Update to Fabric8 6.5.1+ in flink-kubernetes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-31997&quot;&gt;&lt;del&gt;FLINK-31997&lt;/del&gt;&lt;/a&gt; shouldn&apos;t have caused any issues because the LeaderElector only writes to the ConfigMap in &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L210&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LeaderElector#tryAcquireOrRenew&lt;/a&gt; and &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/main/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElector.java#L147&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;LeaderElector#release&lt;/a&gt;. Both methods are synchronized. Hence, they shouldn&apos;t cause a race condition in any way as far as I can see.&lt;/p&gt;

&lt;p&gt;I created a &lt;a href=&quot;https://github.com/apache/flink/pull/24132&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;draft PR with a fix&lt;/a&gt; that recreates the fabric8io &lt;tt&gt;LeaderElector&lt;/tt&gt; instead of reusing it. The fix also covers reverts the thread pool size from 3 to 1 with &lt;tt&gt;KubernetesLeaderElectionITCase&lt;/tt&gt; passing again. I still have to think of a way to test Flink&apos;s KubernetesLeaderElector on the issue that caused the failure of this Jira. The only way I could think of is doing something similar to what&apos;s done in the &lt;a href=&quot;https://github.com/fabric8io/kubernetes-client/blob/0f6c696509935a6a86fdb4620caea023d8e680f1/kubernetes-client-api/src/test/java/io/fabric8/kubernetes/client/extended/leaderelection/LeaderElectorTest.java#L63&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;fabric8io codebase&lt;/a&gt; with Mockito. Any other ideas are appreciated. I would like to avoid Mockito.&lt;/p&gt;</comment>
                            <comment id="17809858" author="fly_in_gis" created="Tue, 23 Jan 2024 09:55:54 +0000"  >&lt;p&gt;It seems that the fabric8 K8s client community leans towards to make the &lt;tt&gt;LeaderElector&lt;/tt&gt; non-restartable. And using a same lock identity should also work for us when creating a new leader-elector instance. So your PR looks good to me.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;About the test:&lt;/p&gt;

&lt;p&gt;We just need to have a test to guard that a new leader-elector instance should be created instead of reusing the existing one. Right?&lt;/p&gt;</comment>
                            <comment id="17810038" author="mapohl" created="Tue, 23 Jan 2024 17:01:50 +0000"  >&lt;p&gt;Just checking whether a new instance of &lt;tt&gt;LeaderElector&lt;/tt&gt; was created is tricky because &lt;tt&gt;LeaderElector&lt;/tt&gt; is coupled with the k8s backend. We would either have to mock &lt;tt&gt;LeaderElector&lt;/tt&gt; or have to work with some backend to work properly within Flink&apos;s &lt;tt&gt;KubernetesLeaderElector&lt;/tt&gt;. Anyway, I came up with a quite straight-forward ITCase. I added it to the PR which is ready to be reviewed now.&lt;/p&gt;</comment>
                            <comment id="17813134" author="mapohl" created="Thu, 1 Feb 2024 09:45:23 +0000"  >&lt;ul&gt;
	&lt;li&gt;master
	&lt;ul&gt;
		&lt;li&gt;&lt;a href=&quot;https://github.com/apache/flink/commit/95417a4857ec87a349c0fa9f4d3951f7d3807844&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;95417a4857ec87a349c0fa9f4d3951f7d3807844&lt;/a&gt;&lt;/li&gt;
		&lt;li&gt;&lt;a href=&quot;https://github.com/apache/flink/commit/927972ff4ad6252fd933fcc627c7d95dbbdae431&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;927972ff4ad6252fd933fcc627c7d95dbbdae431&lt;/a&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;1.18: will be handled in &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-34333&quot; title=&quot;Fix FLINK-34007 LeaderElector bug in 1.18&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-34333&quot;&gt;&lt;del&gt;FLINK-34333&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17813138" author="yunta" created="Thu, 1 Feb 2024 09:51:27 +0000"  >&lt;p&gt;It seems we have had a long discussion, does this problem also exist in Flink-1.17? &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=wangyang0918&quot; class=&quot;user-hover&quot; rel=&quot;wangyang0918&quot;&gt;wangyang0918&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mapohl&quot; class=&quot;user-hover&quot; rel=&quot;mapohl&quot;&gt;mapohl&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17813150" author="mapohl" created="Thu, 1 Feb 2024 10:12:32 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yunta&quot; class=&quot;user-hover&quot; rel=&quot;yunta&quot;&gt;yunta&lt;/a&gt; we&apos;re not aware of any issues related to &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-34007&quot; title=&quot;Flink Job stuck in suspend state after losing leadership in HA Mode&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-34007&quot;&gt;&lt;del&gt;FLINK-34007&lt;/del&gt;&lt;/a&gt; in Flink 1.17. The issue started to appear with the upgrade of the k8s client dependency to v6.6.2 in &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-31997&quot; title=&quot;Update to Fabric8 6.5.1+ in flink-kubernetes&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-31997&quot;&gt;&lt;del&gt;FLINK-31997&lt;/del&gt;&lt;/a&gt; (which ended up in Flink 1.18).&lt;/p&gt;

&lt;p&gt;That being said, &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ZhenqiuHuang&quot; class=&quot;user-hover&quot; rel=&quot;ZhenqiuHuang&quot;&gt;ZhenqiuHuang&lt;/a&gt; reported similar errors in Flink 1.17 and 1.16 deployments as well which we cannot explain. We were not able to investigate the cause due to missing logs. We agreed to cover any other problems in a separate Jira issue if &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ZhenqiuHuang&quot; class=&quot;user-hover&quot; rel=&quot;ZhenqiuHuang&quot;&gt;ZhenqiuHuang&lt;/a&gt; comes up with new information (see his comment above).&lt;/p&gt;</comment>
                            <comment id="17813536" author="mapohl" created="Fri, 2 Feb 2024 07:43:35 +0000"  >&lt;p&gt;I&apos;m reopening the issue because we&apos;re seeing test instabilities now:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;&lt;a href=&quot;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57203&amp;amp;view=logs&amp;amp;j=64debf87-ecdb-5aef-788d-8720d341b5cb&amp;amp;t=2302fb98-0839-5df2-3354-bbae636f81a7&amp;amp;l=8066&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57203&amp;amp;view=logs&amp;amp;j=64debf87-ecdb-5aef-788d-8720d341b5cb&amp;amp;t=2302fb98-0839-5df2-3354-bbae636f81a7&amp;amp;l=8066&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;&lt;a href=&quot;https://github.com/XComp/flink/actions/runs/7745486791/job/21121947504#step:14:7309&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/XComp/flink/actions/runs/7745486791/job/21121947504#step:14:7309&lt;/a&gt; (this one happened on the &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-34333&quot; title=&quot;Fix FLINK-34007 LeaderElector bug in 1.18&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-34333&quot;&gt;&lt;del&gt;FLINK-34333&lt;/del&gt;&lt;/a&gt; 1.18 backport which covers the same change)&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17813626" author="mapohl" created="Fri, 2 Feb 2024 11:33:19 +0000"  >&lt;p&gt;The Azure test failure seems to be unrelated. I created &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-34343&quot; title=&quot;ResourceManager registration is not completed when registering the JobMaster&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-34343&quot;&gt;&lt;del&gt;FLINK-34343&lt;/del&gt;&lt;/a&gt; to cover this one separately. The GitHub Actions failures is still concerning, though.&lt;/p&gt;</comment>
                            <comment id="17814201" author="mapohl" created="Mon, 5 Feb 2024 07:34:42 +0000"  >&lt;p&gt;&lt;a href=&quot;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57270&amp;amp;view=logs&amp;amp;j=bbb1e2a2-a43c-55c8-fb48-5cfe7a8a0ca6&amp;amp;t=a69a379d-ca44-5937-4e62-0ce084a23679&amp;amp;l=7935&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://dev.azure.com/apache-flink/apache-flink/_build/results?buildId=57270&amp;amp;view=logs&amp;amp;j=bbb1e2a2-a43c-55c8-fb48-5cfe7a8a0ca6&amp;amp;t=a69a379d-ca44-5937-4e62-0ce084a23679&amp;amp;l=7935&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="17814524" author="mapohl" created="Mon, 5 Feb 2024 22:10:10 +0000"  >&lt;ul&gt;
	&lt;li&gt;master
	&lt;ul&gt;
		&lt;li&gt;&lt;a href=&quot;https://github.com/apache/flink/commit/79cccd7103a304bfa07104dcafd1f65a032c88ce&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;79cccd7103a304bfa07104dcafd1f65a032c88ce&lt;/a&gt;&lt;/li&gt;
		&lt;li&gt;&lt;a href=&quot;https://github.com/apache/flink/commit/95417a4857ec87a349c0fa9f4d3951f7d3807844&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;95417a4857ec87a349c0fa9f4d3951f7d3807844&lt;/a&gt;&lt;/li&gt;
		&lt;li&gt;&lt;a href=&quot;https://github.com/apache/flink/commit/927972ff4ad6252fd933fcc627c7d95dbbdae431&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;927972ff4ad6252fd933fcc627c7d95dbbdae431&lt;/a&gt;&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
	&lt;li&gt;1.18: will be handled in &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-34333&quot; title=&quot;Fix FLINK-34007 LeaderElector bug in 1.18&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-34333&quot;&gt;&lt;del&gt;FLINK-34333&lt;/del&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310761">
                    <name>Issue split</name>
                                            <outwardlinks description="split to">
                                        <issuelink>
            <issuekey id="13567014">FLINK-34333</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                            <outwardlinks description="causes">
                                        <issuelink>
            <issuekey id="13566311">FLINK-34243</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="12310760">
                    <name>Testing</name>
                                            <outwardlinks description="Testing discovered">
                                        <issuelink>
            <issuekey id="13567177">FLINK-34343</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="13065881" name="Debug.log" size="8828" author="ZhenqiuHuang" created="Wed, 10 Jan 2024 20:02:08 +0000"/>
                            <attachment id="13066005" name="LeaderElector-Debug.json" size="103236" author="ZhenqiuHuang" created="Tue, 16 Jan 2024 03:21:32 +0000"/>
                            <attachment id="13065848" name="job-manager.log" size="10490305" author="ZhenqiuHuang" created="Tue, 9 Jan 2024 20:55:42 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            1 year, 40 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1ml9s:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                            <customfield id="customfield_12310192" key="com.atlassian.jira.plugin.system.customfieldtypes:textarea">
                        <customfieldname>Release Note</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>Fixes a bug where the leader election wasn&amp;#39;t able to pick up leadership again after renewing the lease token caused a leadership loss. This required fabric8io:kubernetes-client to be upgraded from v6.6.2 to v6.9.0.</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                </customfields>
    </item>
</channel>
</rss>