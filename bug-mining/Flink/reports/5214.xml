<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:55:17 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-21942] KubernetesLeaderRetrievalDriver not closed after terminated which lead to connection leak</title>
                <link>https://issues.apache.org/jira/browse/FLINK-21942</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;Looks like KubernetesLeaderRetrievalDriver is not closed even if the KubernetesLeaderElectionDriver is closed and job reach globally terminated.&lt;br/&gt;
This will lead to many configmap watching be still active with connections to K8s.&lt;/p&gt;

&lt;p&gt;When the connections exceeds max concurrent requests, those new configmap watching can not be started. Finally leads to all new jobs submitted timeout.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fly_in_gis&quot; class=&quot;user-hover&quot; rel=&quot;fly_in_gis&quot;&gt;fly_in_gis&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=trohrmann&quot; class=&quot;user-hover&quot; rel=&quot;trohrmann&quot;&gt;trohrmann&lt;/a&gt; This may be related to &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-20695&quot; title=&quot;Zookeeper node under leader and leaderlatch is not deleted after job finished&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-20695&quot;&gt;&lt;del&gt;FLINK-20695&lt;/del&gt;&lt;/a&gt;, could you confirm this issue?&lt;br/&gt;
But when many jobs are running in same session cluster, the config map watching is required to be active. Maybe we should merge all config maps watching?&lt;/p&gt;</description>
                <environment></environment>
        <key id="13367137">FLINK-21942</key>
            <summary>KubernetesLeaderRetrievalDriver not closed after terminated which lead to connection leak</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="wangyang0918">Yang Wang</assignee>
                                    <reporter username="yittg">Yi Tang</reporter>
                        <labels>
                            <label>k8s-ha</label>
                            <label>pull-request-available</label>
                    </labels>
                <created>Wed, 24 Mar 2021 03:57:26 +0000</created>
                <updated>Sat, 28 Aug 2021 11:13:27 +0000</updated>
                            <resolved>Thu, 1 Apr 2021 17:25:59 +0000</resolved>
                                    <version>1.12.2</version>
                    <version>1.13.0</version>
                                    <fixVersion>1.13.0</fixVersion>
                    <fixVersion>1.12.3</fixVersion>
                                    <component>Runtime / Coordination</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="17307627" author="fly_in_gis" created="Wed, 24 Mar 2021 07:48:40 +0000"  >&lt;p&gt;I do not think we have Kubernetes watch leak currently. When a Flink job reached to terminal state(e.g. canceled), &lt;tt&gt;KubernetesLeaderRetrievalDriver&lt;/tt&gt; could be stopped. And I have verified in the application mode and session mode. When I cancel the job, I could find the following logs in the JobManager log.&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
2021-03-24 06:49:11,958 INFO  org.apache.flink.runtime.leaderretrieval.DefaultLeaderRetrievalService [] - Stopping DefaultLeaderRetrievalService.2021-03-24 06:49:11,958 INFO  org.apache.flink.kubernetes.highavailability.KubernetesLeaderRetrievalDriver [] - Stopping KubernetesLeaderRetrievalDriver{configMapName=&lt;span class=&quot;code-quote&quot;&gt;&apos;standalone-k8s-ha-session-resourcemanager-leader&apos;&lt;/span&gt;}.&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Could you please double check for that?&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Currently, in Kubernetes HA service, each job will have the dedicated leader election and retrieval service. It is clearer and also aligned with ZooKeeper HA service. AFAIK, I do not think this will take a&#160;burden to the APIServer, which could be started with multiple replicas. Have you found that the APIServer become the bottleneck because of Flink watch?&lt;/p&gt;</comment>
                            <comment id="17307635" author="yittg" created="Wed, 24 Mar 2021 08:07:35 +0000"  >&lt;p&gt;Did you find any log like &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Stopping KubernetesLeaderRetrievalDriver{configMapName=&lt;span class=&quot;code-quote&quot;&gt;&apos;xxxx-{jobid}-jobmanager-leader&apos;&lt;/span&gt;}.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;excepts for those timeout jobs.&lt;/p&gt;

&lt;p&gt;Furthermore, if many jobs are running (exceeds the max concurrent requests 64 which looks like can not be configured), the API server is OK. The problem is that the new watch for new job(election and retrieve) can not be established.&lt;/p&gt;

&lt;p&gt;I&apos;ll try to provide a detailed steps to reproduce it.&lt;/p&gt;</comment>
                            <comment id="17307734" author="yittg" created="Wed, 24 Mar 2021 10:14:36 +0000"  >&lt;p&gt;kubernetes session cluster with options:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
cluster.io-pool.size=16
kubernetes.jobmanager.cpu=2
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;h2&gt;&lt;a name=&quot;thesimplestscenario&quot;&gt;&lt;/a&gt;the simplest scenario&lt;/h2&gt;

&lt;p&gt;run batch job one by one, e.g. examples/batch/WordCount.jar&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13022893/13022893_image-2021-03-24-18-08-30-196.png&quot; height=&quot;629&quot; width=&quot;990&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In this scenario, the latest job is created and waiting to be assigned, the previous one failed after waiting 5m. The FINISHED jobs only occupy one config map watch, so almost 60 jobs can be run successfully.&lt;/p&gt;


&lt;h2&gt;&lt;a name=&quot;anotherscenario%3A&quot;&gt;&lt;/a&gt;another scenario:&lt;/h2&gt;

&lt;p&gt;since resource limit set like following:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
kubernetes.taskmanager.cpu=2
taskmanager.numberOfTaskSlots: 10
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;run streaming job, examples/streaming/StateMachineExample.jar --error-rate 0.5 --sleep 100&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;image-wrap&quot; style=&quot;&quot;&gt;&lt;img src=&quot;https://issues.apache.org/jira/secure/attachment/13022894/13022894_image-2021-03-24-18-08-42-116.png&quot; height=&quot;629&quot; width=&quot;990&quot; style=&quot;border: 0px solid black&quot; /&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;In this scenario, the latest job is always initializing, and the previous one is created and waiting to be assigned. The RUNNING jobs occupy three config map watch, so almost 20 jobs can be running successfully.&lt;/p&gt;</comment>
                            <comment id="17307767" author="fly_in_gis" created="Wed, 24 Mar 2021 11:59:59 +0000"  >&lt;p&gt;Could you please increase &lt;tt&gt;cluster.io-pool.size&lt;/tt&gt; bigger or share the jstack when the job submission is blocking?&lt;/p&gt;</comment>
                            <comment id="17308299" author="yittg" created="Thu, 25 Mar 2021 02:33:29 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=fly_in_gis&quot; class=&quot;user-hover&quot; rel=&quot;fly_in_gis&quot;&gt;fly_in_gis&lt;/a&gt; Sure, I&apos;ve uploaded a ( &lt;span class=&quot;nobr&quot;&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/attachment/13022919/13022919_jstack.l&quot; title=&quot;jstack.l attached to FLINK-21942&quot;&gt;jstack.l&lt;sup&gt;&lt;img class=&quot;rendericon&quot; src=&quot;https://issues.apache.org/jira/images/icons/link_attachment_7.gif&quot; height=&quot;7&quot; width=&quot;7&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/sup&gt;&lt;/a&gt;&lt;/span&gt; ) corresponding the first scenario. &lt;br/&gt;
You can focus on those OkHttp And OkHttp Websocket threads first.&lt;br/&gt;
I don&apos;t think add more cluster.io-pool.size will help, so it&apos;s still 16 as shown in stack.&lt;/p&gt;</comment>
                            <comment id="17308456" author="fly_in_gis" created="Thu, 25 Mar 2021 07:45:12 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=yittg&quot; class=&quot;user-hover&quot; rel=&quot;yittg&quot;&gt;yittg&lt;/a&gt;&#160;Thanks for providing the information.&lt;/p&gt;

&lt;p&gt;After more investigation, I have to admit that we have the leader ConfigMap watch leak currently. When the job reaches to terminal state, the jobmanager leader retrieval service in ResourceManager is not stopped correctly. We start the &quot;job leader id monitoring&quot; in &lt;tt&gt;ResourceManager#registerJobManager&lt;/tt&gt;, but we do not stop it when we &lt;tt&gt;disconnectJobManager&lt;/tt&gt;. cc &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=trohrmann&quot; class=&quot;user-hover&quot; rel=&quot;trohrmann&quot;&gt;trohrmann&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;For the second problem(could not start more than 60 batch jobs or 20 streaming jobs in a session), I am trying to reproduce it.&lt;/p&gt;</comment>
                            <comment id="17308488" author="till.rohrmann" created="Thu, 25 Mar 2021 08:35:33 +0000"  >&lt;p&gt;I think the closing of the &lt;tt&gt;LeaderRetrievalService&lt;/tt&gt; happens when &lt;tt&gt;ResourceManager.removeJob&lt;/tt&gt; is called. This happens currently only after a job has no leader for at least 5 minutes (configurable via &lt;tt&gt;resourcemanager.job.timeout&lt;/tt&gt;). With &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-21751&quot; title=&quot;Improve handling of freed slots if final requirement message is in flight&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-21751&quot;&gt;&lt;del&gt;FLINK-21751&lt;/del&gt;&lt;/a&gt; we added the state of the job when &lt;tt&gt;disconnectJobManager&lt;/tt&gt; is called. We could use this to call &lt;tt&gt;removeJob&lt;/tt&gt; when the job status is globally terminal. This could solve the problem.&lt;/p&gt;</comment>
                            <comment id="17308515" author="yittg" created="Thu, 25 Mar 2021 09:09:36 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=trohrmann&quot; class=&quot;user-hover&quot; rel=&quot;trohrmann&quot;&gt;trohrmann&lt;/a&gt; Absolutely yes, this could solve the leak of globally terminated jobs.&lt;/p&gt;

&lt;p&gt;In addition, I think we need more discussion about the watching mechanism in K8s HA.&lt;/p&gt;</comment>
                            <comment id="17310438" author="fly_in_gis" created="Mon, 29 Mar 2021 06:21:38 +0000"  >&lt;p&gt;I will attach a PR for this issue. It will solve the Kubernetes ConfigMap watch leak, as well as the ZooKeeper connection.&lt;/p&gt;

&lt;p&gt;Actually, we have the same issue for ZooKeeper HA service for long time.&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;Unfortunately, I still have not enough time to find out the root cause why we only could run about 20 jobs in a Kubernetes session cluster with HA enabled. Since it is a separate issue, I will create another ticket.&lt;/p&gt;</comment>
                            <comment id="17313335" author="till.rohrmann" created="Thu, 1 Apr 2021 17:25:59 +0000"  >&lt;p&gt;Fixed via&lt;/p&gt;

&lt;p&gt;1.13.0:&lt;br/&gt;
8aa510b705bdcfe5b8ff69bc0e294a56b437f53e&lt;br/&gt;
6b40ff1f384c5a2253c8393c3612d3384ae6bfc5&lt;br/&gt;
2eb5d1ce886824fb9eb61847ab56ffba4223a2bf&lt;/p&gt;

&lt;p&gt;1.12.3:&lt;br/&gt;
3409e7f7e52d1dcb70ce238177bcd837f9bb15d3&lt;br/&gt;
8c475b3f0e40be34325a7b37a5b4dbbca738b55d&lt;br/&gt;
c25dc3f83e07adf4f0788d09201b03bfc8e92801&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13347101">FLINK-20695</issuekey>
        </issuelink>
                            </outwardlinks>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="13368291">FLINK-22006</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13368761">FLINK-22054</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                            <attachment id="13022893" name="image-2021-03-24-18-08-30-196.png" size="368401" author="yittg" created="Wed, 24 Mar 2021 10:08:33 +0000"/>
                            <attachment id="13022894" name="image-2021-03-24-18-08-42-116.png" size="371566" author="yittg" created="Wed, 24 Mar 2021 10:08:43 +0000"/>
                            <attachment id="13022919" name="jstack.l" size="310421" author="yittg" created="Thu, 25 Mar 2021 02:27:40 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>3.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 32 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0p460:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>