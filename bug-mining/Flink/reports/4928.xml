<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:51:42 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-20419] Insert fails due to failure to generate execution plan</title>
                <link>https://issues.apache.org/jira/browse/FLINK-20419</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;Test case to reproduce:&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
	@Test
	&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; void test() &lt;span class=&quot;code-keyword&quot;&gt;throws&lt;/span&gt; Exception {
		tableEnv.executeSql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table src(x &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;)&quot;&lt;/span&gt;);
		tableEnv.executeSql(&lt;span class=&quot;code-quote&quot;&gt;&quot;create table dest(x &lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt;) partitioned by (p string,q string)&quot;&lt;/span&gt;);
		tableEnv.executeSql(&lt;span class=&quot;code-quote&quot;&gt;&quot;insert into dest select x,&lt;span class=&quot;code-quote&quot;&gt;&apos;0&apos;&lt;/span&gt;,&lt;span class=&quot;code-quote&quot;&gt;&apos;0&apos;&lt;/span&gt; from src order by x&quot;&lt;/span&gt;).await();
	}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</description>
                <environment></environment>
        <key id="13343276">FLINK-20419</key>
            <summary>Insert fails due to failure to generate execution plan</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="lirui">Rui Li</assignee>
                                    <reporter username="lirui">Rui Li</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Mon, 30 Nov 2020 09:44:51 +0000</created>
                <updated>Wed, 9 Dec 2020 04:28:29 +0000</updated>
                            <resolved>Wed, 9 Dec 2020 04:27:05 +0000</resolved>
                                                    <fixVersion>1.12.1</fixVersion>
                    <fixVersion>1.13.0</fixVersion>
                                    <component>Table SQL / Planner</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="17240611" author="lirui" created="Mon, 30 Nov 2020 09:45:48 +0000"  >&lt;p&gt;Detailed failure is:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;org.apache.flink.table.api.TableException: Cannot generate a valid execution plan for the given query: 

FlinkLogicalSink(table=[test-catalog.default.dest], fields=[x, EXPR$1, EXPR$2])
+- FlinkLogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
   +- FlinkLogicalCalc(select=[x, _UTF-16LE&apos;0&apos; AS EXPR$1, _UTF-16LE&apos;0&apos; AS EXPR$2])
      +- FlinkLogicalTableSourceScan(table=[[test-catalog, default, src]], fields=[x])

This exception indicates that the query uses an unsupported SQL feature.
Please check the documentation for the set of currently supported SQL features.

	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:72)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:62)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram$$anonfun$optimize$1.apply(FlinkChainedProgram.scala:58)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:157)
	at scala.collection.Iterator$class.foreach(Iterator.scala:891)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1334)
	at scala.collection.IterableLike$class.foreach(IterableLike.scala:72)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:54)
	at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:157)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:104)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.optimizeTree(BatchCommonSubGraphBasedOptimizer.scala:86)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.org$apache$flink$table$planner$plan$optimize$BatchCommonSubGraphBasedOptimizer$$optimizeBlock(BatchCommonSubGraphBasedOptimizer.scala:57)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer$$anonfun$doOptimize$1.apply(BatchCommonSubGraphBasedOptimizer.scala:45)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.flink.table.planner.plan.optimize.BatchCommonSubGraphBasedOptimizer.doOptimize(BatchCommonSubGraphBasedOptimizer.scala:45)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:77)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:286)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:165)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1267)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:675)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeOperation(TableEnvironmentImpl.java:759)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:665)
	at org.apache.flink.connectors.hive.TableEnvHiveConnectorITCase.test(TableEnvHiveConnectorITCase.java:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:363)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:69)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:220)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:53)
Caused by: org.apache.calcite.plan.RelOptPlanner$CannotPlanException: There are not enough rules to produce a node with desired properties: convention=BATCH_PHYSICAL, FlinkRelDistributionTraitDef=any, sort=[].
Missing conversion is FlinkLogicalSort[convention: LOGICAL -&amp;gt; BATCH_PHYSICAL, sort: [0 ASC-nulls-first] -&amp;gt; [1 ASC-nulls-first, 2 ASC-nulls-first]]
There is 1 empty subset: rel#219:RelSubset#6.BATCH_PHYSICAL.any.[1 ASC-nulls-first, 2 ASC-nulls-first], the relevant part of the original plan is as follows
205:FlinkLogicalSort(sort0=[$0], dir0=[ASC-nulls-first])
  203:FlinkLogicalCalc(subset=[rel#204:RelSubset#5.LOGICAL.any.[]], select=[x, _UTF-16LE&apos;0&apos; AS EXPR$1, _UTF-16LE&apos;0&apos; AS EXPR$2])
    175:FlinkLogicalTableSourceScan(subset=[rel#202:RelSubset#4.LOGICAL.any.[]], table=[[test-catalog, default, src]], fields=[x])

Root: rel#209:RelSubset#7.BATCH_PHYSICAL.any.[]
Original rel:
FlinkLogicalSink(subset=[rel#173:RelSubset#3.LOGICAL.any.[]], table=[test-catalog.default.dest], fields=[x, EXPR$1, EXPR$2]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 186
  FlinkLogicalSort(subset=[rel#185:RelSubset#1.LOGICAL.any.[0 ASC-nulls-first]], sort0=[$0], dir0=[ASC-nulls-first]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 184
    FlinkLogicalCalc(subset=[rel#183:RelSubset#1.LOGICAL.any.[]], select=[x, _UTF-16LE&apos;0&apos; AS EXPR$1, _UTF-16LE&apos;0&apos; AS EXPR$2]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 0.0 cpu, 0.0 io, 0.0 network, 0.0 memory}, id = 187
      FlinkLogicalTableSourceScan(subset=[rel#176:RelSubset#0.LOGICAL.any.[]], table=[[test-catalog, default, src]], fields=[x]): rowcount = 1.0E8, cumulative cost = {1.0E8 rows, 1.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}, id = 175

Sets:
Set#4, type: RecordType(INTEGER x)
	rel#202:RelSubset#4.LOGICAL.any.[], best=rel#175
		rel#175:FlinkLogicalTableSourceScan.LOGICAL.any.[](table=[test-catalog, default, src],fields=x), rowcount=1.0E8, cumulative cost={1.0E8 rows, 1.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
	rel#212:RelSubset#4.BATCH_PHYSICAL.any.[], best=rel#211
		rel#211:BatchExecTableSourceScan.BATCH_PHYSICAL.any.[](table=[test-catalog, default, src],fields=x), rowcount=1.0E8, cumulative cost={1.0E8 rows, 0.0 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
		rel#224:AbstractConverter.BATCH_PHYSICAL.single.[](input=RelSubset#212,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#227:BatchExecExchange.BATCH_PHYSICAL.single.[](input=RelSubset#212,distribution=single), rowcount=1.0E8, cumulative cost={2.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}
	rel#223:RelSubset#4.BATCH_PHYSICAL.single.[], best=rel#227
		rel#224:AbstractConverter.BATCH_PHYSICAL.single.[](input=RelSubset#212,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#227:BatchExecExchange.BATCH_PHYSICAL.single.[](input=RelSubset#212,distribution=single), rowcount=1.0E8, cumulative cost={2.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}
Set#5, type: RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)
	rel#204:RelSubset#5.LOGICAL.any.[], best=rel#203
		rel#203:FlinkLogicalCalc.LOGICAL.any.[](input=RelSubset#202,select=x, _UTF-16LE&apos;0&apos; AS EXPR$1, _UTF-16LE&apos;0&apos; AS EXPR$2), rowcount=1.0E8, cumulative cost={2.0E8 rows, 1.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
	rel#214:RelSubset#5.BATCH_PHYSICAL.any.[], best=rel#213
		rel#213:BatchExecCalc.BATCH_PHYSICAL.any.[](input=RelSubset#212,select=x, _UTF-16LE&apos;0&apos; AS EXPR$1, _UTF-16LE&apos;0&apos; AS EXPR$2), rowcount=1.0E8, cumulative cost={2.0E8 rows, 0.0 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
		rel#216:AbstractConverter.BATCH_PHYSICAL.single.[](input=RelSubset#214,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#222:BatchExecExchange.BATCH_PHYSICAL.single.[](input=RelSubset#214,distribution=single), rowcount=1.0E8, cumulative cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 8.0E8 network, 0.0 memory}
		rel#225:BatchExecCalc.BATCH_PHYSICAL.single.[](input=RelSubset#223,select=x, _UTF-16LE&apos;0&apos; AS EXPR$1, _UTF-16LE&apos;0&apos; AS EXPR$2), rowcount=1.0E8, cumulative cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}
	rel#215:RelSubset#5.BATCH_PHYSICAL.single.[], best=rel#225
		rel#216:AbstractConverter.BATCH_PHYSICAL.single.[](input=RelSubset#214,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#222:BatchExecExchange.BATCH_PHYSICAL.single.[](input=RelSubset#214,distribution=single), rowcount=1.0E8, cumulative cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 8.0E8 network, 0.0 memory}
		rel#225:BatchExecCalc.BATCH_PHYSICAL.single.[](input=RelSubset#223,select=x, _UTF-16LE&apos;0&apos; AS EXPR$1, _UTF-16LE&apos;0&apos; AS EXPR$2), rowcount=1.0E8, cumulative cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}
Set#6, type: RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)
	rel#206:RelSubset#6.LOGICAL.any.[0 ASC-nulls-first], best=rel#205
		rel#205:FlinkLogicalSort.LOGICAL.any.[0 ASC-nulls-first](input=RelSubset#204,sort0=$0,dir0=ASC-nulls-first), rowcount=1.0E8, cumulative cost={3.0E8 rows, 2.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
	rel#218:RelSubset#6.BATCH_PHYSICAL.single.[0 ASC-nulls-first], best=rel#217
		rel#217:BatchExecSort.BATCH_PHYSICAL.single.[0 ASC-nulls-first](input=RelSubset#215,orderBy=x ASC), rowcount=1.0E8, cumulative cost={4.0E8 rows, 2.3468272297580948E10 cpu, 4.0E8 io, 4.0E8 network, 6.4E9 memory}
	rel#219:RelSubset#6.BATCH_PHYSICAL.any.[1 ASC-nulls-first, 2 ASC-nulls-first], best=null
Set#7, type: RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)
	rel#208:RelSubset#7.LOGICAL.any.[], best=rel#207
		rel#207:FlinkLogicalSink.LOGICAL.any.[](input=RelSubset#206,table=test-catalog.default.dest,fields=x, EXPR$1, EXPR$2), rowcount=1.0E8, cumulative cost={4.0E8 rows, 3.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}
	rel#209:RelSubset#7.BATCH_PHYSICAL.any.[], best=null
		rel#210:AbstractConverter.BATCH_PHYSICAL.any.[](input=RelSubset#208,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=any,sort=[]), rowcount=1.0E8, cumulative cost={inf}
		rel#220:BatchExecSink.BATCH_PHYSICAL.any.[](input=RelSubset#219,table=test-catalog.default.dest,fields=x, EXPR$1, EXPR$2), rowcount=1.0E8, cumulative cost={inf}

Graphviz:
digraph G {
	root [style=filled,label=&quot;Root&quot;];
	subgraph cluster4{
		label=&quot;Set 4 RecordType(INTEGER x)&quot;;
		rel175 [label=&quot;rel#175:FlinkLogicalTableSourceScan\ntable=[test-catalog, default, src],fields=x\nrows=1.0E8, cost={1.0E8 rows, 1.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}&quot;,color=blue,shape=box]
		rel211 [label=&quot;rel#211:BatchExecTableSourceScan\ntable=[test-catalog, default, src],fields=x\nrows=1.0E8, cost={1.0E8 rows, 0.0 cpu, 4.0E8 io, 0.0 network, 0.0 memory}&quot;,color=blue,shape=box]
		rel224 [label=&quot;rel#224:AbstractConverter\ninput=RelSubset#212,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]\nrows=1.0E8, cost={inf}&quot;,shape=box]
		rel227 [label=&quot;rel#227:BatchExecExchange\ninput=RelSubset#212,distribution=single\nrows=1.0E8, cost={2.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}&quot;,color=blue,shape=box]
		subset202 [label=&quot;rel#202:RelSubset#4.LOGICAL.any.[]&quot;]
		subset212 [label=&quot;rel#212:RelSubset#4.BATCH_PHYSICAL.any.[]&quot;]
		subset223 [label=&quot;rel#223:RelSubset#4.BATCH_PHYSICAL.single.[]&quot;]
		subset212 -&amp;gt; subset223;	}
	subgraph cluster5{
		label=&quot;Set 5 RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)&quot;;
		rel203 [label=&quot;rel#203:FlinkLogicalCalc\ninput=RelSubset#202,select=x, _UTF-16LE&apos;0&apos; AS EXPR$1, _UTF-16LE&apos;0&apos; AS EXPR$2\nrows=1.0E8, cost={2.0E8 rows, 1.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}&quot;,color=blue,shape=box]
		rel213 [label=&quot;rel#213:BatchExecCalc\ninput=RelSubset#212,select=x, _UTF-16LE&apos;0&apos; AS EXPR$1, _UTF-16LE&apos;0&apos; AS EXPR$2\nrows=1.0E8, cost={2.0E8 rows, 0.0 cpu, 4.0E8 io, 0.0 network, 0.0 memory}&quot;,color=blue,shape=box]
		rel216 [label=&quot;rel#216:AbstractConverter\ninput=RelSubset#214,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=single,sort=[]\nrows=1.0E8, cost={inf}&quot;,shape=box]
		rel222 [label=&quot;rel#222:BatchExecExchange\ninput=RelSubset#214,distribution=single\nrows=1.0E8, cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 8.0E8 network, 0.0 memory}&quot;,shape=box]
		rel225 [label=&quot;rel#225:BatchExecCalc\ninput=RelSubset#223,select=x, _UTF-16LE&apos;0&apos; AS EXPR$1, _UTF-16LE&apos;0&apos; AS EXPR$2\nrows=1.0E8, cost={3.0E8 rows, 1.61E10 cpu, 4.0E8 io, 4.0E8 network, 0.0 memory}&quot;,color=blue,shape=box]
		subset204 [label=&quot;rel#204:RelSubset#5.LOGICAL.any.[]&quot;]
		subset214 [label=&quot;rel#214:RelSubset#5.BATCH_PHYSICAL.any.[]&quot;]
		subset215 [label=&quot;rel#215:RelSubset#5.BATCH_PHYSICAL.single.[]&quot;]
		subset214 -&amp;gt; subset215;	}
	subgraph cluster6{
		label=&quot;Set 6 RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)&quot;;
		rel205 [label=&quot;rel#205:FlinkLogicalSort\ninput=RelSubset#204,sort0=$0,dir0=ASC-nulls-first\nrows=1.0E8, cost={3.0E8 rows, 2.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}&quot;,color=blue,shape=box]
		rel217 [label=&quot;rel#217:BatchExecSort\ninput=RelSubset#215,orderBy=x ASC\nrows=1.0E8, cost={4.0E8 rows, 2.3468272297580948E10 cpu, 4.0E8 io, 4.0E8 network, 6.4E9 memory}&quot;,color=blue,shape=box]
		subset206 [label=&quot;rel#206:RelSubset#6.LOGICAL.any.[0 ASC-nulls-first]&quot;]
		subset218 [label=&quot;rel#218:RelSubset#6.BATCH_PHYSICAL.single.[0 ASC-nulls-first]&quot;]
		subset219 [label=&quot;rel#219:RelSubset#6.BATCH_PHYSICAL.any.[1 ASC-nulls-first, 2 ASC-nulls-first]&quot;,color=red]
	}
	subgraph cluster7{
		label=&quot;Set 7 RecordType(INTEGER x, CHAR(1) EXPR$1, CHAR(1) EXPR$2)&quot;;
		rel207 [label=&quot;rel#207:FlinkLogicalSink\ninput=RelSubset#206,table=test-catalog.default.dest,fields=x, EXPR$1, EXPR$2\nrows=1.0E8, cost={4.0E8 rows, 3.0E8 cpu, 4.0E8 io, 0.0 network, 0.0 memory}&quot;,color=blue,shape=box]
		rel210 [label=&quot;rel#210:AbstractConverter\ninput=RelSubset#208,convention=BATCH_PHYSICAL,FlinkRelDistributionTraitDef=any,sort=[]\nrows=1.0E8, cost={inf}&quot;,shape=box]
		rel220 [label=&quot;rel#220:BatchExecSink\ninput=RelSubset#219,table=test-catalog.default.dest,fields=x, EXPR$1, EXPR$2\nrows=1.0E8, cost={inf}&quot;,shape=box]
		subset208 [label=&quot;rel#208:RelSubset#7.LOGICAL.any.[]&quot;]
		subset209 [label=&quot;rel#209:RelSubset#7.BATCH_PHYSICAL.any.[]&quot;]
	}
	root -&amp;gt; subset209;
	subset202 -&amp;gt; rel175[color=blue];
	subset212 -&amp;gt; rel211[color=blue];
	subset223 -&amp;gt; rel224; rel224 -&amp;gt; subset212;
	subset223 -&amp;gt; rel227[color=blue]; rel227 -&amp;gt; subset212[color=blue];
	subset204 -&amp;gt; rel203[color=blue]; rel203 -&amp;gt; subset202[color=blue];
	subset214 -&amp;gt; rel213[color=blue]; rel213 -&amp;gt; subset212[color=blue];
	subset215 -&amp;gt; rel216; rel216 -&amp;gt; subset214;
	subset215 -&amp;gt; rel222; rel222 -&amp;gt; subset214;
	subset215 -&amp;gt; rel225[color=blue]; rel225 -&amp;gt; subset223[color=blue];
	subset206 -&amp;gt; rel205[color=blue]; rel205 -&amp;gt; subset204[color=blue];
	subset218 -&amp;gt; rel217[color=blue]; rel217 -&amp;gt; subset215[color=blue];
	subset208 -&amp;gt; rel207[color=blue]; rel207 -&amp;gt; subset206[color=blue];
	subset209 -&amp;gt; rel210; rel210 -&amp;gt; subset208;
	subset209 -&amp;gt; rel220; rel220 -&amp;gt; subset219;
}
	at org.apache.calcite.plan.volcano.RelSubset$CheapestPlanReplacer.visit(RelSubset.java:742)
	at org.apache.calcite.plan.volcano.RelSubset.buildCheapestPlan(RelSubset.java:365)
	at org.apache.calcite.plan.volcano.VolcanoPlanner.findBestExp(VolcanoPlanner.java:520)
	at org.apache.calcite.tools.Programs$RuleSetProgram.run(Programs.java:312)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkVolcanoProgram.optimize(FlinkVolcanoProgram.scala:64)
	... 48 more
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="17240691" author="jark" created="Mon, 30 Nov 2020 11:31:28 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, are you in streaming mode? Order by regular columns is not supported in streaming mode. Do you mean we should improve this error message?&lt;/p&gt;</comment>
                            <comment id="17240698" author="lirui" created="Mon, 30 Nov 2020 11:41:12 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jark&quot; class=&quot;user-hover&quot; rel=&quot;jark&quot;&gt;jark&lt;/a&gt;, I was running in batch mode. So I guess this is could be a bug, although I haven&apos;t dug deep into it.&lt;/p&gt;</comment>
                            <comment id="17240715" author="jark" created="Mon, 30 Nov 2020 12:06:31 +0000"  >&lt;p&gt;Got it &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="17242367" author="lirui" created="Wed, 2 Dec 2020 13:41:24 +0000"  >&lt;p&gt;I suspect the issue is related to dynamic partition grouping. &lt;tt&gt;BatchExecSinkRule&lt;/tt&gt; automatically adds field collations for dynamic partitions by calling &lt;tt&gt;RelTraitSet::plus&lt;/tt&gt;, which would replace previous collation &lt;tt&gt;RelTrait&lt;/tt&gt;. This error message seems to imply the issue:&lt;/p&gt;
&lt;div class=&quot;preformatted panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;preformattedContent panelContent&quot;&gt;
&lt;pre&gt;Missing conversion is FlinkLogicalSort[convention: LOGICAL -&amp;gt; BATCH_PHYSICAL, sort: [0 ASC-nulls-first] -&amp;gt; [1 ASC-nulls-first, 2 ASC-nulls-first]]
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I tried to disable partition grouping and the query passed.&lt;br/&gt;
Perhaps we shouldn&apos;t do partition grouping if the input already defined collation trait.&lt;/p&gt;</comment>
                            <comment id="17242372" author="lirui" created="Wed, 2 Dec 2020 13:47:40 +0000"  >&lt;p&gt;Hi &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jark&quot; class=&quot;user-hover&quot; rel=&quot;jark&quot;&gt;jark&lt;/a&gt;, I think you implemented &lt;tt&gt;BatchExecSinkRule&lt;/tt&gt;, any thoughts about my above proposal?&lt;/p&gt;</comment>
                            <comment id="17242986" author="jark" created="Thu, 3 Dec 2020 08:02:04 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=lirui&quot; class=&quot;user-hover&quot; rel=&quot;lirui&quot;&gt;lirui&lt;/a&gt;, the logic of &lt;tt&gt;BatchExecSinkRule&lt;/tt&gt; is copied from &lt;tt&gt;BatchExecLegacySinkRule&lt;/tt&gt;.&lt;/p&gt;</comment>
                            <comment id="17245830" author="jark" created="Tue, 8 Dec 2020 11:26:57 +0000"  >&lt;p&gt;Fixed in &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;master: c4a4d6290daf2199bfdfb00a92ac464c72c93876, a40b261d0cb77420fca6eed17952677aa49c788b&lt;/li&gt;
	&lt;li&gt;release-1.12: 5a55f0173a26dfc7af250d7aed55f81a00660978, 87d8358e8d2e88a032c851a517eb677709c98234&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="17245884" author="lirui" created="Tue, 8 Dec 2020 13:28:22 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=jark&quot; class=&quot;user-hover&quot; rel=&quot;jark&quot;&gt;jark&lt;/a&gt; Thanks for reviewing and merging! I have opened PR for 1.12 here:&lt;br/&gt;
&lt;a href=&quot;https://github.com/apache/flink/pull/14339&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/14339&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            4 years, 49 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z0l1l4:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>