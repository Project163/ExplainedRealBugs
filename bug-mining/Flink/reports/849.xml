<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:22:50 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-3418] RocksDB HDFSCopyFromLocal util doesn&apos;t respect our Hadoop security configuration</title>
                <link>https://issues.apache.org/jira/browse/FLINK-3418</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;As you can see for example in the &lt;tt&gt;YARNTaskManagerRunner&lt;/tt&gt;, our TaskManagers are running in a special UserGroupInformation.doAs(); call. &lt;/p&gt;

&lt;p&gt;With that call, we are manually changing the user from the user starting the YARN NodeManager (our containers are part of that process tree) to the user who submitted the job.&lt;/p&gt;

&lt;p&gt;For example on my cluster, the NodeManager runs as &quot;yarn&quot;, but &quot;robert&quot; submits the job. For regular file access, &quot;robert&quot; is accessing the files in HDFS, even though &quot;yarn&quot; runs the process.&lt;/p&gt;

&lt;p&gt;The &lt;tt&gt;HDFSCopyFromLocal&lt;/tt&gt; does not properly initialize these settings, hence &quot;yarn&quot; tries to access the files, leading to the following exception:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;Caused by: java.lang.RuntimeException: Error &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; copying to remote FileSystem: SLF4J: &lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt; path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/yarn/nm/usercache/robert/appcache/application_1455632128025_0010/filecache/17/slf4j-log4j12-1.7.7.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/opt/cloudera/parcels/CDH-5.4.5-1.cdh5.4.5.p0.7/jars/slf4j-log4j12-1.7.5.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http:&lt;span class=&quot;code-comment&quot;&gt;//www.slf4j.org/codes.html#multiple_bindings &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; an explanation.
&lt;/span&gt;SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; org.apache.hadoop.security.AccessControlException: Permission denied: user=yarn, access=WRITE, inode=&lt;span class=&quot;code-quote&quot;&gt;&quot;/user/robert/rocksdb/5b7ad8b04048e894ef7bf341856681bf&quot;&lt;/span&gt;:robert:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:257)
	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:238)
	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:216)
	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:145)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:138)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6599)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6581)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6533)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4337)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4307)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4280)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:853)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:321)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:601)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2755)
	at org.apache.hadoop.hdfs.DFSClient.mkdirs(DFSClient.java:2724)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:870)
	at org.apache.hadoop.hdfs.DistributedFileSystem$17.doCall(DistributedFileSystem.java:866)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirsInternal(DistributedFileSystem.java:866)
	at org.apache.hadoop.hdfs.DistributedFileSystem.mkdirs(DistributedFileSystem.java:859)
	at org.apache.hadoop.fs.FileSystem.mkdirs(FileSystem.java:1817)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:351)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:338)
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1905)
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1873)
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1838)
	at org.apache.flink.contrib.streaming.state.HDFSCopyFromLocal.main(HDFSCopyFromLocal.java:47)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.security.AccessControlException): Permission denied: user=yarn, access=WRITE, inode=&lt;span class=&quot;code-quote&quot;&gt;&quot;/user/robert/rocksdb/5b7ad8b04048e894ef7bf341856681bf&quot;&lt;/span&gt;:robert:supergroup:drwxr-xr-x
	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkFsPermission(DefaultAuthorizationProvider.java:257)
	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:238)
	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.check(DefaultAuthorizationProvider.java:216)
	at org.apache.hadoop.hdfs.server.namenode.DefaultAuthorizationProvider.checkPermission(DefaultAuthorizationProvider.java:145)
	at org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:138)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6599)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkPermission(FSNamesystem.java:6581)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAncestorAccess(FSNamesystem.java:6533)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInternal(FSNamesystem.java:4337)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirsInt(FSNamesystem.java:4307)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.mkdirs(FSNamesystem.java:4280)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.mkdirs(NameNodeRpcServer.java:853)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.mkdirs(AuthorizationProviderProxyClientProtocol.java:321)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.mkdirs(ClientNamenodeProtocolServerSideTranslatorPB.java:601)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:619)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1060)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2044)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2040)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2038)

	at org.apache.hadoop.ipc.Client.call(Client.java:1468)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy9.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.mkdirs(ClientNamenodeProtocolTranslatorPB.java:539)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy10.mkdirs(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.primitiveMkdir(DFSClient.java:2753)
	... 13 more

	at org.apache.flink.contrib.streaming.state.HDFSCopyFromLocal.copyFromLocal(HDFSCopyFromLocal.java:54)
	at org.apache.flink.contrib.streaming.state.AbstractRocksDBState$AsyncRocksDBSnapshot.materialize(AbstractRocksDBState.java:454)
	at org.apache.flink.streaming.runtime.tasks.StreamTask$1.run(StreamTask.java:531)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I think we need to fix this before the 1.0.0 release.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12939543">FLINK-3418</key>
            <summary>RocksDB HDFSCopyFromLocal util doesn&apos;t respect our Hadoop security configuration</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="aljoscha">Aljoscha Krettek</assignee>
                                    <reporter username="rmetzger">Robert Metzger</reporter>
                        <labels>
                    </labels>
                <created>Tue, 16 Feb 2016 16:34:46 +0000</created>
                <updated>Tue, 23 Feb 2016 16:52:32 +0000</updated>
                            <resolved>Tue, 23 Feb 2016 16:52:32 +0000</resolved>
                                                                    <component>Runtime / State Backends</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>5</watches>
                                                                                                                <comments>
                            <comment id="15148878" author="rmetzger" created="Tue, 16 Feb 2016 16:41:35 +0000"  >&lt;p&gt;Currently, even making the directory accessible for the user running the NM (&quot;yarn&quot;) doesn&apos;t solve the problem because &lt;tt&gt;initializeForJob()&lt;/tt&gt; creates a directory with the user who submitted the job &quot;robert&quot;.&lt;/p&gt;</comment>
                            <comment id="15148896" author="aljoscha" created="Tue, 16 Feb 2016 16:50:37 +0000"  >&lt;p&gt;Where is the logic that does the user change? Could this simply be added to the copy utilities?&lt;/p&gt;</comment>
                            <comment id="15148906" author="stephanewen" created="Tue, 16 Feb 2016 16:56:07 +0000"  >&lt;p&gt;Something like this needs to be added:&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/apache/flink/blob/master/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala#L1370&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/blob/master/flink-runtime/src/main/scala/org/apache/flink/runtime/taskmanager/TaskManager.scala#L1370&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This runs code as privileged code under the authenticated user .&lt;/p&gt;</comment>
                            <comment id="15157012" author="githubbot" created="Mon, 22 Feb 2016 14:02:59 +0000"  >&lt;p&gt;GitHub user aljoscha opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-3418&quot; title=&quot;RocksDB HDFSCopyFromLocal util doesn&amp;#39;t respect our Hadoop security configuration&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-3418&quot;&gt;&lt;del&gt;FLINK-3418&lt;/del&gt;&lt;/a&gt; Don&apos;t run RocksDB copy utils in external process&lt;/p&gt;

&lt;p&gt;    This was causing to many problems with security tokens and yarn. Now,&lt;br/&gt;
    let the RocksDB backup run in a thread but&lt;br/&gt;
    don&apos;t interrupt these Threads anymore on closing. The Threads will close&lt;br/&gt;
    themselves because the copy operation will fail because of a&lt;br/&gt;
    FileNotFoundException when the state directories are being cleaned up.&lt;/p&gt;

&lt;p&gt;    This also removes the ExternalProcessRunner because it is not needed&lt;br/&gt;
    anymore and using it causes too many headaches.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/aljoscha/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/aljoscha/flink&lt;/a&gt; rocksdb-security-fix&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1687&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 421e851f0da752da6ec9d14988759b06680ced03&lt;br/&gt;
Author: Aljoscha Krettek &amp;lt;aljoscha.krettek@gmail.com&amp;gt;&lt;br/&gt;
Date:   2016-02-17T11:34:51Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-3418&quot; title=&quot;RocksDB HDFSCopyFromLocal util doesn&amp;#39;t respect our Hadoop security configuration&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-3418&quot;&gt;&lt;del&gt;FLINK-3418&lt;/del&gt;&lt;/a&gt; Don&apos;t run RocksDB copy utils in external process&lt;/p&gt;

&lt;p&gt;    This was causing to many problems with security tokens and yarn. Now,&lt;br/&gt;
    let the RocksDB backup run in a thread but&lt;br/&gt;
    don&apos;t interrupt these Threads anymore on closing. The Threads will close&lt;br/&gt;
    themselves because the copy operation will fail because of a&lt;br/&gt;
    FileNotFoundException when the state directories are being cleaned up.&lt;/p&gt;

&lt;p&gt;    This also removes the ExternalProcessRunner because it is not needed&lt;br/&gt;
    anymore and using it causes too many headaches.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15157026" author="githubbot" created="Mon, 22 Feb 2016 14:10:13 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687#discussion_r53628814&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687#discussion_r53628814&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyFromLocal.java &amp;#8212;&lt;br/&gt;
    @@ -26,32 +25,46 @@&lt;br/&gt;
     import java.io.File;&lt;br/&gt;
     import java.io.FileInputStream;&lt;br/&gt;
     import java.net.URI;&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Utility for copying from local file system to a HDFS 
{@link FileSystem} in an external process.&lt;br/&gt;
    - * This is required since {@code FileSystem.copyFromLocalFile} does not like being interrupted.&lt;br/&gt;
    + * Utility for copying from local file system to a HDFS {@link FileSystem}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
     public class HDFSCopyFromLocal {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static void main(String[] args) throws Exception {&lt;/li&gt;
	&lt;li&gt;String hadoopConfPath = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;String localBackupPath = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;String backupUri = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;Configuration hadoopConf = new Configuration();&lt;/li&gt;
	&lt;li&gt;try (DataInputStream in = new DataInputStream(new FileInputStream(hadoopConfPath))) 
{
    -			hadoopConf.readFields(in);
    -		}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FileSystem fs = FileSystem.get(new URI(backupUri), hadoopConf);&lt;br/&gt;
    +	public static void copyFromLocal(final File hadoopConfPath, final File localPath, final URI remotePath) throws Exception 
{
    +		// Do it in another Thread because HDFS can deadlock if being interrupted while copying
     
    -		fs.copyFromLocalFile(new Path(localBackupPath), new Path(backupUri));
    -	}
&lt;p&gt;    +		String threadName = &quot;HDFS Copy from &quot; + localPath + &quot; to &quot; + remotePath;&lt;br/&gt;
    +&lt;br/&gt;
    +		final List&amp;lt;Exception&amp;gt; asyncException = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
    +&lt;br/&gt;
    +		Thread copyThread = new Thread(threadName) {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try {&lt;br/&gt;
    +					Configuration hadoopConf = new Configuration();&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    This is only loading the Hadoop configuration from the classpath, not from the Flink configuration or environment variables&lt;/p&gt;

&lt;p&gt;    I think our filesystem code has a method to try the environment variables and the config as well.&lt;/p&gt;</comment>
                            <comment id="15157029" author="githubbot" created="Mon, 22 Feb 2016 14:11:39 +0000"  >&lt;p&gt;Github user aljoscha commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687#discussion_r53629014&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687#discussion_r53629014&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyFromLocal.java &amp;#8212;&lt;br/&gt;
    @@ -26,32 +25,46 @@&lt;br/&gt;
     import java.io.File;&lt;br/&gt;
     import java.io.FileInputStream;&lt;br/&gt;
     import java.net.URI;&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Utility for copying from local file system to a HDFS 
{@link FileSystem} in an external process.&lt;br/&gt;
    - * This is required since {@code FileSystem.copyFromLocalFile} does not like being interrupted.&lt;br/&gt;
    + * Utility for copying from local file system to a HDFS {@link FileSystem}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
     public class HDFSCopyFromLocal {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static void main(String[] args) throws Exception {&lt;/li&gt;
	&lt;li&gt;String hadoopConfPath = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;String localBackupPath = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;String backupUri = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;Configuration hadoopConf = new Configuration();&lt;/li&gt;
	&lt;li&gt;try (DataInputStream in = new DataInputStream(new FileInputStream(hadoopConfPath))) 
{
    -			hadoopConf.readFields(in);
    -		}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FileSystem fs = FileSystem.get(new URI(backupUri), hadoopConf);&lt;br/&gt;
    +	public static void copyFromLocal(final File hadoopConfPath, final File localPath, final URI remotePath) throws Exception 
{
    +		// Do it in another Thread because HDFS can deadlock if being interrupted while copying
     
    -		fs.copyFromLocalFile(new Path(localBackupPath), new Path(backupUri));
    -	}
&lt;p&gt;    +		String threadName = &quot;HDFS Copy from &quot; + localPath + &quot; to &quot; + remotePath;&lt;br/&gt;
    +&lt;br/&gt;
    +		final List&amp;lt;Exception&amp;gt; asyncException = new ArrayList&amp;lt;&amp;gt;();&lt;br/&gt;
    +&lt;br/&gt;
    +		Thread copyThread = new Thread(threadName) {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void run() {&lt;br/&gt;
    +				try {&lt;br/&gt;
    +					Configuration hadoopConf = new Configuration();&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Ah dammit, I pushed to wrong stuff. Give me a sec.&lt;/p&gt;</comment>
                            <comment id="15157046" author="githubbot" created="Mon, 22 Feb 2016 14:26:09 +0000"  >&lt;p&gt;Github user aljoscha commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687#issuecomment-187202268&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687#issuecomment-187202268&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Should be the correct code now. :smile: &lt;/p&gt;</comment>
                            <comment id="15157502" author="githubbot" created="Mon, 22 Feb 2016 19:09:41 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687#issuecomment-187323846&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687#issuecomment-187323846&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Fails on checkstyle, otherwise looks good...&lt;/p&gt;</comment>
                            <comment id="15157523" author="githubbot" created="Mon, 22 Feb 2016 19:19:28 +0000"  >&lt;p&gt;Github user aljoscha commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687#issuecomment-187328780&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687#issuecomment-187328780&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Ahh, stupid beginners mistake. Fixed it.&lt;/p&gt;</comment>
                            <comment id="15158843" author="githubbot" created="Tue, 23 Feb 2016 13:11:50 +0000"  >&lt;p&gt;Github user rmetzger commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687#discussion_r53777248&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687#discussion_r53777248&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyFromLocal.java &amp;#8212;&lt;br/&gt;
    @@ -17,41 +17,50 @@&lt;br/&gt;
      */&lt;br/&gt;
     package org.apache.flink.streaming.util;&lt;/p&gt;

&lt;p&gt;    -import org.apache.flink.util.ExternalProcessRunner;&lt;br/&gt;
    +import org.apache.flink.runtime.fs.hdfs.HadoopFileSystem;&lt;br/&gt;
     import org.apache.hadoop.conf.Configuration;&lt;br/&gt;
     import org.apache.hadoop.fs.FileSystem;&lt;br/&gt;
     import org.apache.hadoop.fs.Path;&lt;/p&gt;

&lt;p&gt;    -import java.io.DataInputStream;&lt;br/&gt;
     import java.io.File;&lt;br/&gt;
    -import java.io.FileInputStream;&lt;br/&gt;
     import java.net.URI;&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Utility for copying from local file system to a HDFS 
{@link FileSystem} in an external process.&lt;br/&gt;
    - * This is required since {@code FileSystem.copyFromLocalFile} does not like being interrupted.&lt;br/&gt;
    + * Utility for copying from local file system to a HDFS {@link FileSystem}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
     public class HDFSCopyFromLocal {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static void main(String[] args) throws Exception {&lt;/li&gt;
	&lt;li&gt;String hadoopConfPath = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;String localBackupPath = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;String backupUri = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;Configuration hadoopConf = new Configuration();&lt;/li&gt;
	&lt;li&gt;try (DataInputStream in = new DataInputStream(new FileInputStream(hadoopConfPath))) 
{
    -			hadoopConf.readFields(in);
    -		}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FileSystem fs = FileSystem.get(new URI(backupUri), hadoopConf);&lt;br/&gt;
    +	public static void copyFromLocal(final File localPath,&lt;br/&gt;
    +			final URI remotePath) throws Exception 
{
    +		// Do it in another Thread because HDFS can deadlock if being interrupted while copying
    +		String threadName = &quot;HDFS Copy from &quot; + localPath + &quot; to &quot; + remotePath;
     
    -		fs.copyFromLocalFile(new Path(localBackupPath), new Path(backupUri));
    -	}
&lt;p&gt;    +		final List&amp;lt;Exception&amp;gt; asyncException = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    If you have cases like this again in the future: I use Flink&apos;s `Tuple1` for those cases. Its probably cheaper than creating a new ArrayList.&lt;/p&gt;</comment>
                            <comment id="15158845" author="githubbot" created="Tue, 23 Feb 2016 13:13:13 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687#issuecomment-187692550&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687#issuecomment-187692550&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    +1 to merge this change&lt;/p&gt;</comment>
                            <comment id="15158884" author="githubbot" created="Tue, 23 Feb 2016 13:43:56 +0000"  >&lt;p&gt;Github user tillrohrmann commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687#discussion_r53780485&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687#discussion_r53780485&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-java/src/main/java/org/apache/flink/streaming/util/HDFSCopyFromLocal.java &amp;#8212;&lt;br/&gt;
    @@ -17,41 +17,50 @@&lt;br/&gt;
      */&lt;br/&gt;
     package org.apache.flink.streaming.util;&lt;/p&gt;

&lt;p&gt;    -import org.apache.flink.util.ExternalProcessRunner;&lt;br/&gt;
    +import org.apache.flink.runtime.fs.hdfs.HadoopFileSystem;&lt;br/&gt;
     import org.apache.hadoop.conf.Configuration;&lt;br/&gt;
     import org.apache.hadoop.fs.FileSystem;&lt;br/&gt;
     import org.apache.hadoop.fs.Path;&lt;/p&gt;

&lt;p&gt;    -import java.io.DataInputStream;&lt;br/&gt;
     import java.io.File;&lt;br/&gt;
    -import java.io.FileInputStream;&lt;br/&gt;
     import java.net.URI;&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Utility for copying from local file system to a HDFS 
{@link FileSystem} in an external process.&lt;br/&gt;
    - * This is required since {@code FileSystem.copyFromLocalFile} does not like being interrupted.&lt;br/&gt;
    + * Utility for copying from local file system to a HDFS {@link FileSystem}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
     public class HDFSCopyFromLocal {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;public static void main(String[] args) throws Exception {&lt;/li&gt;
	&lt;li&gt;String hadoopConfPath = args&lt;span class=&quot;error&quot;&gt;&amp;#91;0&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;String localBackupPath = args&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;String backupUri = args&lt;span class=&quot;error&quot;&gt;&amp;#91;2&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;Configuration hadoopConf = new Configuration();&lt;/li&gt;
	&lt;li&gt;try (DataInputStream in = new DataInputStream(new FileInputStream(hadoopConfPath))) 
{
    -			hadoopConf.readFields(in);
    -		}&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;FileSystem fs = FileSystem.get(new URI(backupUri), hadoopConf);&lt;br/&gt;
    +	public static void copyFromLocal(final File localPath,&lt;br/&gt;
    +			final URI remotePath) throws Exception 
{
    +		// Do it in another Thread because HDFS can deadlock if being interrupted while copying
    +		String threadName = &quot;HDFS Copy from &quot; + localPath + &quot; to &quot; + remotePath;
     
    -		fs.copyFromLocalFile(new Path(localBackupPath), new Path(backupUri));
    -	}
&lt;p&gt;    +		final List&amp;lt;Exception&amp;gt; asyncException = new ArrayList&amp;lt;&amp;gt;();&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Agreed. `ArrayList` will allocate an array of 10 elements when you initialize the ArrayList without an initial capacity. So either initialize the ArrayList with a size of `1` or use Robert&apos;s approach.&lt;/p&gt;</comment>
                            <comment id="15158888" author="githubbot" created="Tue, 23 Feb 2016 13:48:12 +0000"  >&lt;p&gt;Github user tillrohrmann commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687#issuecomment-187705826&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687#issuecomment-187705826&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks for your work @aljoscha. Changes look good to me. I&apos;ll address Robert&apos;s concern and then merge the PR.&lt;/p&gt;</comment>
                            <comment id="15159172" author="githubbot" created="Tue, 23 Feb 2016 16:52:00 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1687&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1687&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15159175" author="till.rohrmann" created="Tue, 23 Feb 2016 16:52:32 +0000"  >&lt;p&gt;Fixed via 7ddc078e68d645bac2829de9634e3373a59be551&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 39 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2swcv:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>