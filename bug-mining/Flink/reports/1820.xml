<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:29:05 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-6996] FlinkKafkaProducer010 doesn&apos;t guarantee at-least-once semantic</title>
                <link>https://issues.apache.org/jira/browse/FLINK-6996</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;FlinkKafkaProducer010 doesn&apos;t implement CheckpointedFunction interface. This means, when it&apos;s used like a &quot;regular sink function&quot; (option a from &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-master/api/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;the java doc&lt;/a&gt;) it will not flush the data on &quot;snapshotState&quot;  as it is supposed to.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13082150">FLINK-6996</key>
            <summary>FlinkKafkaProducer010 doesn&apos;t guarantee at-least-once semantic</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="pnowojski">Piotr Nowojski</assignee>
                                    <reporter username="pnowojski">Piotr Nowojski</reporter>
                        <labels>
                    </labels>
                <created>Fri, 23 Jun 2017 15:05:44 +0000</created>
                <updated>Wed, 2 Aug 2017 07:15:28 +0000</updated>
                            <resolved>Wed, 2 Aug 2017 07:15:28 +0000</resolved>
                                    <version>1.2.0</version>
                    <version>1.2.1</version>
                    <version>1.3.0</version>
                    <version>1.3.1</version>
                                    <fixVersion>1.3.2</fixVersion>
                    <fixVersion>1.4.0</fixVersion>
                                    <component>Connectors / Kafka</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="16061091" author="tzulitai" created="Fri, 23 Jun 2017 15:12:12 +0000"  >&lt;p&gt;I think its the other way around.&lt;br/&gt;
In approach (b), i.e. &lt;tt&gt;FlinkKafkaProducer010.writeToKafkaWithTimestamps(inStream, schema, config)&lt;/tt&gt;, flushing works.&lt;br/&gt;
It&apos;s in approach (a) where its used as a regular sink UDF &lt;tt&gt;stream.addSink(new FlinkKafkaProducer010(...))&lt;/tt&gt;, since it doesn&apos;t implement the &lt;tt&gt;CheckpointedFunction&lt;/tt&gt; interface, there&apos;s no flushing happening.&lt;/p&gt;</comment>
                            <comment id="16061168" author="pnowojski" created="Fri, 23 Jun 2017 16:19:10 +0000"  >&lt;p&gt;Yes, thanks, you are correct. Fixed description.&lt;/p&gt;</comment>
                            <comment id="16066021" author="pnowojski" created="Wed, 28 Jun 2017 07:01:10 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=tzulitai&quot; class=&quot;user-hover&quot; rel=&quot;tzulitai&quot;&gt;tzulitai&lt;/a&gt; I guess we were both wrong. I added tests for this issue for both (a) and (b), and both were failing (not flushing the data) before fix.&lt;/p&gt;</comment>
                            <comment id="16066023" author="githubbot" created="Wed, 28 Jun 2017 07:02:35 +0000"  >&lt;p&gt;GitHub user pnowojski opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6996&quot; title=&quot;FlinkKafkaProducer010 doesn&amp;#39;t guarantee at-least-once semantic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6996&quot;&gt;&lt;del&gt;FLINK-6996&lt;/del&gt;&lt;/a&gt; FlinkKafkaProducer010 doesn&apos;t guarantee at-least-once semantic&lt;/p&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/pnowojski/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/pnowojski/flink&lt;/a&gt; at-least-once&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4206&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit b05b72a2baab8656787e2020120750e780b37621&lt;br/&gt;
Author: Piotr Nowojski &amp;lt;piotr.nowojski@gmail.com&amp;gt;&lt;br/&gt;
Date:   2017-06-26T09:28:51Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6996&quot; title=&quot;FlinkKafkaProducer010 doesn&amp;#39;t guarantee at-least-once semantic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6996&quot;&gt;&lt;del&gt;FLINK-6996&lt;/del&gt;&lt;/a&gt; Refactor and automaticall inherit KafkaProducer integration tests&lt;/p&gt;

&lt;p&gt;commit 62b553503964230d8af6d7d79054721060da8061&lt;br/&gt;
Author: Piotr Nowojski &amp;lt;piotr.nowojski@gmail.com&amp;gt;&lt;br/&gt;
Date:   2017-06-26T10:20:36Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6996&quot; title=&quot;FlinkKafkaProducer010 doesn&amp;#39;t guarantee at-least-once semantic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6996&quot;&gt;&lt;del&gt;FLINK-6996&lt;/del&gt;&lt;/a&gt; Fix formatting in KafkaConsumerTestBase and KafkaProducerTestBase&lt;/p&gt;

&lt;p&gt;commit 34ba4b74f0c5c6b915695ab8bf7bda5b40955d5b&lt;br/&gt;
Author: Piotr Nowojski &amp;lt;piotr.nowojski@gmail.com&amp;gt;&lt;br/&gt;
Date:   2017-06-26T10:36:40Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6996&quot; title=&quot;FlinkKafkaProducer010 doesn&amp;#39;t guarantee at-least-once semantic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6996&quot;&gt;&lt;del&gt;FLINK-6996&lt;/del&gt;&lt;/a&gt; Fix at-least-once semantic for FlinkKafkaProducer010&lt;/p&gt;

&lt;p&gt;    Add tests coverage for Kafka 0.10 and 0.9&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16066046" author="githubbot" created="Wed, 28 Jun 2017 07:27:45 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r124468424&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r124468424&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaTestEnvironmentImpl.java &amp;#8212;&lt;br/&gt;
    @@ -116,6 +120,30 @@ public String getVersion() {&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
    +	public &amp;lt;K, V&amp;gt; Collection&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt; getAllRecordsFromTopic(Properties properties, String topic, int partition) {&lt;br/&gt;
    +		ImmutableList.Builder&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt; result = ImmutableList.builder();&lt;br/&gt;
    +		KafkaConsumer&amp;lt;K, V&amp;gt; consumer = new KafkaConsumer&amp;lt;&amp;gt;(properties);&lt;br/&gt;
    +		consumer.assign(ImmutableList.of(new TopicPartition(topic, partition)));&lt;br/&gt;
    +&lt;br/&gt;
    +		while (true) {&lt;br/&gt;
    +			boolean processedAtLeastOneRecord = false;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I&apos;m a bit confused by this flag.&lt;br/&gt;
    The method name is `getAllRecordsFromTopic`, but it seems like we&apos;re escaping the loop once some record is fetched.&lt;/p&gt;</comment>
                            <comment id="16066047" author="githubbot" created="Wed, 28 Jun 2017 07:27:45 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r124466733&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r124466733&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaTestEnvironmentImpl.java &amp;#8212;&lt;br/&gt;
    @@ -26,6 +26,7 @@&lt;br/&gt;
     import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
     import org.apache.flink.util.NetUtils;&lt;/p&gt;

&lt;p&gt;    +import com.google.common.collect.ImmutableList;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    In Flink we usually try to avoid Guava usages. Would it be easy to switch to `Collections.unmodifiableList`?&lt;/p&gt;</comment>
                            <comment id="16066048" author="githubbot" created="Wed, 28 Jun 2017 07:27:45 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r124467541&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r124467541&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka08ProducerITCase.java &amp;#8212;&lt;br/&gt;
    @@ -18,17 +18,19 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.streaming.connectors.kafka;&lt;/p&gt;

&lt;p&gt;    -import org.junit.Test;&lt;br/&gt;
    -&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;IT cases for the 
{@link FlinkKafkaProducer08}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
     @SuppressWarnings(&quot;serial&quot;)&lt;br/&gt;
     public class Kafka08ProducerITCase extends KafkaProducerTestBase {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testCustomPartitioning() {&lt;/li&gt;
	&lt;li&gt;runCustomPartitioningTest();&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceRegularSink() throws Exception 
{
    +		// TODO: enable this for Kafka 0.8 - now it hangs indefinitely
     	}&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    +	@Override&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {&lt;br/&gt;
    +		// Disable this test since FlinkKafka08Producer doesn&apos;t support writing timestamps&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I would perhaps rephrase this comment a bit:&lt;br/&gt;
    it&apos;s disabled because FlinkKafka08Producer doesn&apos;t run in the custom operator mode (to be coherent with the test case name)&lt;/p&gt;</comment>
                            <comment id="16066049" author="githubbot" created="Wed, 28 Jun 2017 07:27:45 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r124466523&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r124466523&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/FlinkKafkaProducer010.java &amp;#8212;&lt;br/&gt;
    @@ -411,6 +414,18 @@ public void processElement(StreamRecord&amp;lt;T&amp;gt; element) throws Exception &lt;/p&gt;
{
     		invokeInternal(element.getValue(), element.getTimestamp());
     	}

&lt;p&gt;    +	@Override&lt;br/&gt;
    +	public void snapshotState(FunctionSnapshotContext context) throws Exception &lt;/p&gt;
{
    +		final FlinkKafkaProducerBase&amp;lt;T&amp;gt; internalProducer = (FlinkKafkaProducerBase&amp;lt;T&amp;gt;) userFunction;
    +		internalProducer.snapshotState(context);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void initializeState(FunctionInitializationContext context) throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    nit: I would declare `initializeState` before `snapshotState`, just for the sake of a better logic flow.&lt;/p&gt;</comment>
                            <comment id="16066050" author="githubbot" created="Wed, 28 Jun 2017 07:27:45 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r124467315&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r124467315&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka08ProducerITCase.java &amp;#8212;&lt;br/&gt;
    @@ -18,17 +18,19 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.streaming.connectors.kafka;&lt;/p&gt;

&lt;p&gt;    -import org.junit.Test;&lt;br/&gt;
    -&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;IT cases for the 
{@link FlinkKafkaProducer08}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
     @SuppressWarnings(&quot;serial&quot;)&lt;br/&gt;
     public class Kafka08ProducerITCase extends KafkaProducerTestBase {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testCustomPartitioning() {&lt;/li&gt;
	&lt;li&gt;runCustomPartitioningTest();&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceRegularSink() throws Exception {&lt;br/&gt;
    +		// TODO: enable this for Kafka 0.8 - now it hangs indefinitely
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    If this a pending fix?&lt;/p&gt;</comment>
                            <comment id="16066051" author="githubbot" created="Wed, 28 Jun 2017 07:27:45 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r124467740&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r124467740&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09ProducerITCase.java &amp;#8212;&lt;br/&gt;
    @@ -18,17 +18,13 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.streaming.connectors.kafka;&lt;/p&gt;

&lt;p&gt;    -import org.junit.Test;&lt;br/&gt;
    -&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;IT cases for the 
{@link FlinkKafkaProducer09}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
     @SuppressWarnings(&quot;serial&quot;)&lt;br/&gt;
     public class Kafka09ProducerITCase extends KafkaProducerTestBase {&lt;br/&gt;
    -&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testCustomPartitioning() {&lt;/li&gt;
	&lt;li&gt;runCustomPartitioningTest();&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceCustomOperator() throws Exception {&lt;br/&gt;
    +		// Disable this test since FlinkKafka09Producer doesn&apos;t support writing timestamps
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Same here.&lt;/p&gt;</comment>
                            <comment id="16066052" author="githubbot" created="Wed, 28 Jun 2017 07:27:45 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r124467929&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r124467929&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaTestEnvironmentImpl.java &amp;#8212;&lt;br/&gt;
    @@ -28,6 +28,7 @@&lt;br/&gt;
     import org.apache.flink.streaming.util.serialization.KeyedSerializationSchema;&lt;br/&gt;
     import org.apache.flink.util.NetUtils;&lt;/p&gt;

&lt;p&gt;    +import com.google.common.collect.ImmutableList;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Avoid Guava&lt;/p&gt;</comment>
                            <comment id="16066103" author="githubbot" created="Wed, 28 Jun 2017 08:04:45 +0000"  >&lt;p&gt;Github user pnowojski commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r124476235&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r124476235&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaTestEnvironmentImpl.java &amp;#8212;&lt;br/&gt;
    @@ -116,6 +120,30 @@ public String getVersion() {&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
    +	public &amp;lt;K, V&amp;gt; Collection&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt; getAllRecordsFromTopic(Properties properties, String topic, int partition) {&lt;br/&gt;
    +		ImmutableList.Builder&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt; result = ImmutableList.builder();&lt;br/&gt;
    +		KafkaConsumer&amp;lt;K, V&amp;gt; consumer = new KafkaConsumer&amp;lt;&amp;gt;(properties);&lt;br/&gt;
    +		consumer.assign(ImmutableList.of(new TopicPartition(topic, partition)));&lt;br/&gt;
    +&lt;br/&gt;
    +		while (true) {&lt;br/&gt;
    +			boolean processedAtLeastOneRecord = false;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    No, it&apos;s other way around. We are braking the loop if after pooling for 1 second for next records we did get an empty response. Added comment.&lt;/p&gt;</comment>
                            <comment id="16066110" author="githubbot" created="Wed, 28 Jun 2017 08:09:20 +0000"  >&lt;p&gt;Github user pnowojski commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r124476992&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r124476992&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-0.8/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka08ProducerITCase.java &amp;#8212;&lt;br/&gt;
    @@ -18,17 +18,19 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.streaming.connectors.kafka;&lt;/p&gt;

&lt;p&gt;    -import org.junit.Test;&lt;br/&gt;
    -&lt;br/&gt;
     /**&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;IT cases for the 
{@link FlinkKafkaProducer08}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
     @SuppressWarnings(&quot;serial&quot;)&lt;br/&gt;
     public class Kafka08ProducerITCase extends KafkaProducerTestBase {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;public void testCustomPartitioning() {&lt;/li&gt;
	&lt;li&gt;runCustomPartitioningTest();&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceRegularSink() throws Exception {&lt;br/&gt;
    +		// TODO: enable this for Kafka 0.8 - now it hangs indefinitely
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I will not fix this test (I&apos;m pretty sure this is a test issue) within the scope of this ticket. I even think that it&apos;s not worth the effort to investigate it at all - it is difficult to debug those failure tests and Kafka 0.8 is pretty old.&lt;/p&gt;</comment>
                            <comment id="16069471" author="githubbot" created="Fri, 30 Jun 2017 04:46:50 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @pnowojski related test `Kafka010ProducerITCase&amp;gt;KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink` is failing in Travis. It seems like the fetched records from Kafka is empty?&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    Failed tests: &lt;br/&gt;
      Kafka09ProducerITCase&amp;gt;KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink:202-&amp;gt;KafkaProducerTestBase.testOneToOneAtLeastOnce:282-&amp;gt;KafkaProducerTestBase.assertAtLeastOnceForTopic:298 expected:&amp;lt;&lt;span class=&quot;error&quot;&gt;&amp;#91;0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269&amp;#93;&lt;/span&gt;&amp;gt; but was:&amp;lt;[]&amp;gt;&lt;br/&gt;
    ```&lt;/p&gt;</comment>
                            <comment id="16069657" author="githubbot" created="Fri, 30 Jun 2017 07:50:34 +0000"  >&lt;p&gt;Github user pnowojski commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hmm, this is disturbing. Locally it works for me always. I have rewritten test so that it should be less prone to intermittent failures (longer reading from Kafka timeout). Hopefully that will solve this issue, otherwise we still have some bug. &lt;/p&gt;</comment>
                            <comment id="16072510" author="githubbot" created="Mon, 3 Jul 2017 14:17:44 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r125295964&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r125295964&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaProducerTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -172,6 +194,118 @@ public void cancel() {&lt;br/&gt;
     		}&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	/**&lt;br/&gt;
    +	 * Tests the at-least-once semantic for the simple writes into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceRegularSink() throws Exception &lt;/p&gt;
{
    +		testOneToOneAtLeastOnce(true);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Tests the at-least-once semantic for the simple writes into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceCustomOperator() throws Exception &lt;/p&gt;
{
    +		testOneToOneAtLeastOnce(false);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * This test sets KafkaProducer so that it will not automatically flush the data and&lt;br/&gt;
    +	 * and fails the broker to check whether FlinkKafkaProducer flushed records manually on snapshotState.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {&lt;br/&gt;
    +		final String topic = regularSink ? &quot;oneToOneTopicRegularSink&quot; : &quot;oneToOneTopicCustomOperator&quot;;&lt;br/&gt;
    +		final int partition = 0;&lt;br/&gt;
    +		final int numElements = 1000;&lt;br/&gt;
    +		final int failAfterElements = 333;&lt;br/&gt;
    +&lt;br/&gt;
    +		createTestTopic(topic, 1, 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		TypeInformationSerializationSchema&amp;lt;Integer&amp;gt; schema = new TypeInformationSerializationSchema&amp;lt;&amp;gt;(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig());&lt;br/&gt;
    +		KeyedSerializationSchema&amp;lt;Integer&amp;gt; keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema);&lt;br/&gt;
    +&lt;br/&gt;
    +		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&lt;br/&gt;
    +		env.enableCheckpointing(500);&lt;br/&gt;
    +		env.setParallelism(1);&lt;br/&gt;
    +		env.setRestartStrategy(RestartStrategies.noRestart());&lt;br/&gt;
    +		env.getConfig().disableSysoutLogging();&lt;br/&gt;
    +&lt;br/&gt;
    +		Properties properties = new Properties();&lt;br/&gt;
    +		properties.putAll(standardProps);&lt;br/&gt;
    +		properties.putAll(secureProps);&lt;br/&gt;
    +		// decrease timeout and block time from 60s down to 10s - this is how long KafkaProducer will try send pending (not flushed) data on close()&lt;br/&gt;
    +		properties.setProperty(&quot;timeout.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +		properties.setProperty(&quot;max.block.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +		// increase batch.size and linger.ms - this tells KafkaProducer to batch produced events instead of flushing them immediately&lt;br/&gt;
    +		properties.setProperty(&quot;batch.size&quot;, &quot;10240000&quot;);&lt;br/&gt;
    +		properties.setProperty(&quot;linger.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +&lt;br/&gt;
    +		int leaderId = kafkaServer.getLeaderToShutDown(topic);&lt;br/&gt;
    +		BrokerRestartingMapper.resetState();&lt;br/&gt;
    +&lt;br/&gt;
    +		// process exactly failAfterElements number of elements and then shutdown Kafka broker and fail application&lt;br/&gt;
    +		DataStream&amp;lt;Integer&amp;gt; inputStream = env&lt;br/&gt;
    +			.fromCollection(getIntegersSequence(numElements))&lt;br/&gt;
    +			.map(new BrokerRestartingMapper&amp;lt;Integer&amp;gt;(leaderId, failAfterElements));&lt;br/&gt;
    +&lt;br/&gt;
    +		StreamSink&amp;lt;Integer&amp;gt; kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) &lt;/p&gt;
{
    +				return partition;
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		if (regularSink) &lt;/p&gt;
{
    +			inputStream.addSink(kafkaSink.getUserFunction());
    +		}
&lt;p&gt;    +		else {&lt;br/&gt;
    +			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
    +				@Override&lt;br/&gt;
    +				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) &lt;/p&gt;
{
    +					return partition;
    +				}
&lt;p&gt;    +			});&lt;br/&gt;
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		FailingIdentityMapper.failedBefore = false;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Why do we need this here? I don&apos;t see that the `FailingIdentityMapper` is used elsewhere in the pipeline.&lt;/p&gt;</comment>
                            <comment id="16072511" author="githubbot" created="Mon, 3 Jul 2017 14:18:08 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    It seems like the tests are still failing on Travis:&lt;/p&gt;

&lt;p&gt;    &amp;gt;Failed tests: &lt;br/&gt;
      Kafka09ProducerITCase&amp;gt;KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink:202-&amp;gt;KafkaProducerTestBase.testOneToOneAtLeastOnce:282-&amp;gt;KafkaProducerTestBase.assertAtLeastOnceForTopic:298 expected:&amp;lt;&lt;span class=&quot;error&quot;&gt;&amp;#91;0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 331&amp;#93;&lt;/span&gt;&amp;gt; but was:&amp;lt;[]&amp;gt;&lt;/p&gt;</comment>
                            <comment id="16072512" author="githubbot" created="Mon, 3 Jul 2017 14:19:02 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I wonder if we can actually replace this validation step with a validating map function after the Kafka producer sink. e.g. use a Flink Kafka consumer to read the results, followed by a validating flat map function?&lt;/p&gt;</comment>
                            <comment id="16072544" author="githubbot" created="Mon, 3 Jul 2017 14:44:56 +0000"  >&lt;p&gt;Github user pnowojski commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Sorry, those tests were passing before rebase - after rebase I have accidentally reverted this previous fixup. I have re-introduced it now as a separate commit&lt;/p&gt;</comment>
                            <comment id="16072558" author="githubbot" created="Mon, 3 Jul 2017 14:52:17 +0000"  >&lt;p&gt;Github user pnowojski commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r125306972&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r125306972&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaProducerTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -172,6 +194,118 @@ public void cancel() {&lt;br/&gt;
     		}&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	/**&lt;br/&gt;
    +	 * Tests the at-least-once semantic for the simple writes into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceRegularSink() throws Exception &lt;/p&gt;
{
    +		testOneToOneAtLeastOnce(true);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Tests the at-least-once semantic for the simple writes into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceCustomOperator() throws Exception &lt;/p&gt;
{
    +		testOneToOneAtLeastOnce(false);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * This test sets KafkaProducer so that it will not automatically flush the data and&lt;br/&gt;
    +	 * and fails the broker to check whether FlinkKafkaProducer flushed records manually on snapshotState.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {&lt;br/&gt;
    +		final String topic = regularSink ? &quot;oneToOneTopicRegularSink&quot; : &quot;oneToOneTopicCustomOperator&quot;;&lt;br/&gt;
    +		final int partition = 0;&lt;br/&gt;
    +		final int numElements = 1000;&lt;br/&gt;
    +		final int failAfterElements = 333;&lt;br/&gt;
    +&lt;br/&gt;
    +		createTestTopic(topic, 1, 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		TypeInformationSerializationSchema&amp;lt;Integer&amp;gt; schema = new TypeInformationSerializationSchema&amp;lt;&amp;gt;(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig());&lt;br/&gt;
    +		KeyedSerializationSchema&amp;lt;Integer&amp;gt; keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema);&lt;br/&gt;
    +&lt;br/&gt;
    +		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&lt;br/&gt;
    +		env.enableCheckpointing(500);&lt;br/&gt;
    +		env.setParallelism(1);&lt;br/&gt;
    +		env.setRestartStrategy(RestartStrategies.noRestart());&lt;br/&gt;
    +		env.getConfig().disableSysoutLogging();&lt;br/&gt;
    +&lt;br/&gt;
    +		Properties properties = new Properties();&lt;br/&gt;
    +		properties.putAll(standardProps);&lt;br/&gt;
    +		properties.putAll(secureProps);&lt;br/&gt;
    +		// decrease timeout and block time from 60s down to 10s - this is how long KafkaProducer will try send pending (not flushed) data on close()&lt;br/&gt;
    +		properties.setProperty(&quot;timeout.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +		properties.setProperty(&quot;max.block.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +		// increase batch.size and linger.ms - this tells KafkaProducer to batch produced events instead of flushing them immediately&lt;br/&gt;
    +		properties.setProperty(&quot;batch.size&quot;, &quot;10240000&quot;);&lt;br/&gt;
    +		properties.setProperty(&quot;linger.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +&lt;br/&gt;
    +		int leaderId = kafkaServer.getLeaderToShutDown(topic);&lt;br/&gt;
    +		BrokerRestartingMapper.resetState();&lt;br/&gt;
    +&lt;br/&gt;
    +		// process exactly failAfterElements number of elements and then shutdown Kafka broker and fail application&lt;br/&gt;
    +		DataStream&amp;lt;Integer&amp;gt; inputStream = env&lt;br/&gt;
    +			.fromCollection(getIntegersSequence(numElements))&lt;br/&gt;
    +			.map(new BrokerRestartingMapper&amp;lt;Integer&amp;gt;(leaderId, failAfterElements));&lt;br/&gt;
    +&lt;br/&gt;
    +		StreamSink&amp;lt;Integer&amp;gt; kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) &lt;/p&gt;
{
    +				return partition;
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		if (regularSink) &lt;/p&gt;
{
    +			inputStream.addSink(kafkaSink.getUserFunction());
    +		}
&lt;p&gt;    +		else {&lt;br/&gt;
    +			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
    +				@Override&lt;br/&gt;
    +				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) &lt;/p&gt;
{
    +					return partition;
    +				}
&lt;p&gt;    +			});&lt;br/&gt;
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		FailingIdentityMapper.failedBefore = false;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    This is static variable and `FialingIdentityMapper` is used twice. First to test regular sink and then custom sink operator. Without reseting this state second test run would fail.&lt;/p&gt;</comment>
                            <comment id="16073276" author="githubbot" created="Tue, 4 Jul 2017 08:02:55 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r125411227&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r125411227&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaProducerTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -172,6 +195,144 @@ public void cancel() {&lt;br/&gt;
     		}&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	/**&lt;br/&gt;
    +	 * Tests the at-least-once semantic for the simple writes into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceRegularSink() throws Exception &lt;/p&gt;
{
    +		testOneToOneAtLeastOnce(true);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Tests the at-least-once semantic for the simple writes into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceCustomOperator() throws Exception &lt;/p&gt;
{
    +		testOneToOneAtLeastOnce(false);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * This test sets KafkaProducer so that it will not automatically flush the data and&lt;br/&gt;
    +	 * and fails the broker to check whether FlinkKafkaProducer flushed records manually on snapshotState.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {&lt;br/&gt;
    +		final String topic = regularSink ? &quot;oneToOneTopicRegularSink&quot; : &quot;oneToOneTopicCustomOperator&quot;;&lt;br/&gt;
    +		final int partition = 0;&lt;br/&gt;
    +		final int numElements = 1000;&lt;br/&gt;
    +		final int failAfterElements = 333;&lt;br/&gt;
    +&lt;br/&gt;
    +		createTestTopic(topic, 1, 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		TypeInformationSerializationSchema&amp;lt;Integer&amp;gt; schema = new TypeInformationSerializationSchema&amp;lt;&amp;gt;(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig());&lt;br/&gt;
    +		KeyedSerializationSchema&amp;lt;Integer&amp;gt; keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema);&lt;br/&gt;
    +&lt;br/&gt;
    +		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&lt;br/&gt;
    +		env.enableCheckpointing(500);&lt;br/&gt;
    +		env.setParallelism(1);&lt;br/&gt;
    +		env.setRestartStrategy(RestartStrategies.noRestart());&lt;br/&gt;
    +		env.getConfig().disableSysoutLogging();&lt;br/&gt;
    +&lt;br/&gt;
    +		Properties properties = new Properties();&lt;br/&gt;
    +		properties.putAll(standardProps);&lt;br/&gt;
    +		properties.putAll(secureProps);&lt;br/&gt;
    +		// decrease timeout and block time from 60s down to 10s - this is how long KafkaProducer will try send pending (not flushed) data on close()&lt;br/&gt;
    +		properties.setProperty(&quot;timeout.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +		properties.setProperty(&quot;max.block.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +		// increase batch.size and linger.ms - this tells KafkaProducer to batch produced events instead of flushing them immediately&lt;br/&gt;
    +		properties.setProperty(&quot;batch.size&quot;, &quot;10240000&quot;);&lt;br/&gt;
    +		properties.setProperty(&quot;linger.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +&lt;br/&gt;
    +		int leaderId = kafkaServer.getLeaderToShutDown(topic);&lt;br/&gt;
    +		BrokerRestartingMapper.resetState();&lt;br/&gt;
    +&lt;br/&gt;
    +		// process exactly failAfterElements number of elements and then shutdown Kafka broker and fail application&lt;br/&gt;
    +		DataStream&amp;lt;Integer&amp;gt; inputStream = env&lt;br/&gt;
    +			.fromCollection(getIntegersSequence(numElements))&lt;br/&gt;
    +			.map(new BrokerRestartingMapper&amp;lt;Integer&amp;gt;(leaderId, failAfterElements));&lt;br/&gt;
    +&lt;br/&gt;
    +		StreamSink&amp;lt;Integer&amp;gt; kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) &lt;/p&gt;
{
    +				return partition;
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		if (regularSink) &lt;/p&gt;
{
    +			inputStream.addSink(kafkaSink.getUserFunction());
    +		}
&lt;p&gt;    +		else {&lt;br/&gt;
    +			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
    +				@Override&lt;br/&gt;
    +				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) &lt;/p&gt;
{
    +					return partition;
    +				}
&lt;p&gt;    +			});&lt;br/&gt;
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		FailingIdentityMapper.failedBefore = false;&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			env.execute(&quot;One-to-one at least once test&quot;);
    +			fail(&quot;Job should fail!&quot;);
    +		}
&lt;p&gt;    +		catch (Exception ex) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I think we need a more specific exception here.&lt;br/&gt;
    There may be actual exceptions thrown by Flink that would be masked by this assumption.&lt;/p&gt;</comment>
                            <comment id="16073280" author="githubbot" created="Tue, 4 Jul 2017 08:05:33 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r125411707&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r125411707&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaProducerTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -172,6 +194,118 @@ public void cancel() {&lt;br/&gt;
     		}&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	/**&lt;br/&gt;
    +	 * Tests the at-least-once semantic for the simple writes into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceRegularSink() throws Exception &lt;/p&gt;
{
    +		testOneToOneAtLeastOnce(true);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Tests the at-least-once semantic for the simple writes into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceCustomOperator() throws Exception &lt;/p&gt;
{
    +		testOneToOneAtLeastOnce(false);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * This test sets KafkaProducer so that it will not automatically flush the data and&lt;br/&gt;
    +	 * and fails the broker to check whether FlinkKafkaProducer flushed records manually on snapshotState.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {&lt;br/&gt;
    +		final String topic = regularSink ? &quot;oneToOneTopicRegularSink&quot; : &quot;oneToOneTopicCustomOperator&quot;;&lt;br/&gt;
    +		final int partition = 0;&lt;br/&gt;
    +		final int numElements = 1000;&lt;br/&gt;
    +		final int failAfterElements = 333;&lt;br/&gt;
    +&lt;br/&gt;
    +		createTestTopic(topic, 1, 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		TypeInformationSerializationSchema&amp;lt;Integer&amp;gt; schema = new TypeInformationSerializationSchema&amp;lt;&amp;gt;(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig());&lt;br/&gt;
    +		KeyedSerializationSchema&amp;lt;Integer&amp;gt; keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema);&lt;br/&gt;
    +&lt;br/&gt;
    +		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&lt;br/&gt;
    +		env.enableCheckpointing(500);&lt;br/&gt;
    +		env.setParallelism(1);&lt;br/&gt;
    +		env.setRestartStrategy(RestartStrategies.noRestart());&lt;br/&gt;
    +		env.getConfig().disableSysoutLogging();&lt;br/&gt;
    +&lt;br/&gt;
    +		Properties properties = new Properties();&lt;br/&gt;
    +		properties.putAll(standardProps);&lt;br/&gt;
    +		properties.putAll(secureProps);&lt;br/&gt;
    +		// decrease timeout and block time from 60s down to 10s - this is how long KafkaProducer will try send pending (not flushed) data on close()&lt;br/&gt;
    +		properties.setProperty(&quot;timeout.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +		properties.setProperty(&quot;max.block.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +		// increase batch.size and linger.ms - this tells KafkaProducer to batch produced events instead of flushing them immediately&lt;br/&gt;
    +		properties.setProperty(&quot;batch.size&quot;, &quot;10240000&quot;);&lt;br/&gt;
    +		properties.setProperty(&quot;linger.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +&lt;br/&gt;
    +		int leaderId = kafkaServer.getLeaderToShutDown(topic);&lt;br/&gt;
    +		BrokerRestartingMapper.resetState();&lt;br/&gt;
    +&lt;br/&gt;
    +		// process exactly failAfterElements number of elements and then shutdown Kafka broker and fail application&lt;br/&gt;
    +		DataStream&amp;lt;Integer&amp;gt; inputStream = env&lt;br/&gt;
    +			.fromCollection(getIntegersSequence(numElements))&lt;br/&gt;
    +			.map(new BrokerRestartingMapper&amp;lt;Integer&amp;gt;(leaderId, failAfterElements));&lt;br/&gt;
    +&lt;br/&gt;
    +		StreamSink&amp;lt;Integer&amp;gt; kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) &lt;/p&gt;
{
    +				return partition;
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		if (regularSink) &lt;/p&gt;
{
    +			inputStream.addSink(kafkaSink.getUserFunction());
    +		}
&lt;p&gt;    +		else {&lt;br/&gt;
    +			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
    +				@Override&lt;br/&gt;
    +				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) &lt;/p&gt;
{
    +					return partition;
    +				}
&lt;p&gt;    +			});&lt;br/&gt;
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		FailingIdentityMapper.failedBefore = false;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I see. Perhaps we can make this more explicit by following the same pattern as `BrokerRestartingMapper.resetState()`?&lt;/p&gt;</comment>
                            <comment id="16073283" author="githubbot" created="Tue, 4 Jul 2017 08:07:00 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r125411974&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r125411974&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaTestEnvironment.java &amp;#8212;&lt;br/&gt;
    @@ -80,6 +82,12 @@ public void createTestTopic(String topic, int numberOfPartitions, int replicatio&lt;/p&gt;

&lt;p&gt;     	public abstract &amp;lt;T&amp;gt; FlinkKafkaConsumerBase&amp;lt;T&amp;gt; getConsumer(List&amp;lt;String&amp;gt; topics, KeyedDeserializationSchema&amp;lt;T&amp;gt; readSchema, Properties props);&lt;/p&gt;

&lt;p&gt;    +	public abstract &amp;lt;K, V&amp;gt; Collection&amp;lt;ConsumerRecord&amp;lt;K, V&amp;gt;&amp;gt; getAllRecordsFromTopic(&lt;br/&gt;
    +		Properties properties,&lt;br/&gt;
    +		String topic,&lt;br/&gt;
    +		int partition,&lt;br/&gt;
    +		long timeout);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    nit: the indentation pattern is inconsistent with the other abstract method declarations here.&lt;/p&gt;</comment>
                            <comment id="16073284" author="githubbot" created="Tue, 4 Jul 2017 08:07:57 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks for the fixups @pnowojski!&lt;br/&gt;
    I have some final minor comments, other than that this LGTM.&lt;/p&gt;</comment>
                            <comment id="16073313" author="githubbot" created="Tue, 4 Jul 2017 08:29:41 +0000"  >&lt;p&gt;Github user pnowojski commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @tzulitai applied changes. &lt;/p&gt;</comment>
                            <comment id="16073315" author="githubbot" created="Tue, 4 Jul 2017 08:30:59 +0000"  >&lt;p&gt;Github user pnowojski commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206#discussion_r125416757&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206#discussion_r125416757&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaProducerTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -172,6 +195,144 @@ public void cancel() {&lt;br/&gt;
     		}&lt;br/&gt;
     	}&lt;/p&gt;

&lt;p&gt;    +	/**&lt;br/&gt;
    +	 * Tests the at-least-once semantic for the simple writes into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceRegularSink() throws Exception &lt;/p&gt;
{
    +		testOneToOneAtLeastOnce(true);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Tests the at-least-once semantic for the simple writes into Kafka.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testOneToOneAtLeastOnceCustomOperator() throws Exception &lt;/p&gt;
{
    +		testOneToOneAtLeastOnce(false);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * This test sets KafkaProducer so that it will not automatically flush the data and&lt;br/&gt;
    +	 * and fails the broker to check whether FlinkKafkaProducer flushed records manually on snapshotState.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	protected void testOneToOneAtLeastOnce(boolean regularSink) throws Exception {&lt;br/&gt;
    +		final String topic = regularSink ? &quot;oneToOneTopicRegularSink&quot; : &quot;oneToOneTopicCustomOperator&quot;;&lt;br/&gt;
    +		final int partition = 0;&lt;br/&gt;
    +		final int numElements = 1000;&lt;br/&gt;
    +		final int failAfterElements = 333;&lt;br/&gt;
    +&lt;br/&gt;
    +		createTestTopic(topic, 1, 1);&lt;br/&gt;
    +&lt;br/&gt;
    +		TypeInformationSerializationSchema&amp;lt;Integer&amp;gt; schema = new TypeInformationSerializationSchema&amp;lt;&amp;gt;(BasicTypeInfo.INT_TYPE_INFO, new ExecutionConfig());&lt;br/&gt;
    +		KeyedSerializationSchema&amp;lt;Integer&amp;gt; keyedSerializationSchema = new KeyedSerializationSchemaWrapper(schema);&lt;br/&gt;
    +&lt;br/&gt;
    +		StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();&lt;br/&gt;
    +		env.enableCheckpointing(500);&lt;br/&gt;
    +		env.setParallelism(1);&lt;br/&gt;
    +		env.setRestartStrategy(RestartStrategies.noRestart());&lt;br/&gt;
    +		env.getConfig().disableSysoutLogging();&lt;br/&gt;
    +&lt;br/&gt;
    +		Properties properties = new Properties();&lt;br/&gt;
    +		properties.putAll(standardProps);&lt;br/&gt;
    +		properties.putAll(secureProps);&lt;br/&gt;
    +		// decrease timeout and block time from 60s down to 10s - this is how long KafkaProducer will try send pending (not flushed) data on close()&lt;br/&gt;
    +		properties.setProperty(&quot;timeout.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +		properties.setProperty(&quot;max.block.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +		// increase batch.size and linger.ms - this tells KafkaProducer to batch produced events instead of flushing them immediately&lt;br/&gt;
    +		properties.setProperty(&quot;batch.size&quot;, &quot;10240000&quot;);&lt;br/&gt;
    +		properties.setProperty(&quot;linger.ms&quot;, &quot;10000&quot;);&lt;br/&gt;
    +&lt;br/&gt;
    +		int leaderId = kafkaServer.getLeaderToShutDown(topic);&lt;br/&gt;
    +		BrokerRestartingMapper.resetState();&lt;br/&gt;
    +&lt;br/&gt;
    +		// process exactly failAfterElements number of elements and then shutdown Kafka broker and fail application&lt;br/&gt;
    +		DataStream&amp;lt;Integer&amp;gt; inputStream = env&lt;br/&gt;
    +			.fromCollection(getIntegersSequence(numElements))&lt;br/&gt;
    +			.map(new BrokerRestartingMapper&amp;lt;Integer&amp;gt;(leaderId, failAfterElements));&lt;br/&gt;
    +&lt;br/&gt;
    +		StreamSink&amp;lt;Integer&amp;gt; kafkaSink = kafkaServer.getProducerSink(topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) &lt;/p&gt;
{
    +				return partition;
    +			}
&lt;p&gt;    +		});&lt;br/&gt;
    +&lt;br/&gt;
    +		if (regularSink) &lt;/p&gt;
{
    +			inputStream.addSink(kafkaSink.getUserFunction());
    +		}
&lt;p&gt;    +		else {&lt;br/&gt;
    +			kafkaServer.produceIntoKafka(inputStream, topic, keyedSerializationSchema, properties, new FlinkKafkaPartitioner&amp;lt;Integer&amp;gt;() {&lt;br/&gt;
    +				@Override&lt;br/&gt;
    +				public int partition(Integer record, byte[] key, byte[] value, String targetTopic, int[] partitions) &lt;/p&gt;
{
    +					return partition;
    +				}
&lt;p&gt;    +			});&lt;br/&gt;
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		FailingIdentityMapper.failedBefore = false;&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			env.execute(&quot;One-to-one at least once test&quot;);
    +			fail(&quot;Job should fail!&quot;);
    +		}
&lt;p&gt;    +		catch (Exception ex) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    FYI `getCause` Exception is type of `java.lang.Exception`, so there is no point in making an assertion on that.&lt;/p&gt;</comment>
                            <comment id="16073441" author="githubbot" created="Tue, 4 Jul 2017 10:28:02 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    LGTM, merging &#128077; &lt;/p&gt;</comment>
                            <comment id="16075906" author="githubbot" created="Thu, 6 Jul 2017 04:24:52 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16075919" author="tzulitai" created="Thu, 6 Jul 2017 04:32:27 +0000"  >&lt;p&gt;Fixed for master via 0e7a53189991f7615891e9a168c747f43d1b13c3.&lt;br/&gt;
Fixed for 1.3 via 6630dfdd748dee9c2fa6a0993497dcf3468a0948.&lt;/p&gt;</comment>
                            <comment id="16076051" author="githubbot" created="Thu, 6 Jul 2017 06:57:18 +0000"  >&lt;p&gt;Github user pnowojski commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4206&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4206&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    thanks!&lt;/p&gt;</comment>
                            <comment id="16105176" author="till.rohrmann" created="Fri, 28 Jul 2017 16:13:08 +0000"  >&lt;p&gt;There seems to be a test instability with &lt;tt&gt;Kafka010ProducerITCase&amp;gt;KafkaProducerTestBase.testOneToOneAtLeastOnceRegularSink&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://s3.amazonaws.com/archive.travis-ci.org/jobs/258538641/log.txt&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://s3.amazonaws.com/archive.travis-ci.org/jobs/258538641/log.txt&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16106034" author="aljoscha" created="Sat, 29 Jul 2017 05:18:19 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=till.rohrmann&quot; class=&quot;user-hover&quot; rel=&quot;till.rohrmann&quot;&gt;till.rohrmann&lt;/a&gt; The log is not accessible. (at least for me)&lt;/p&gt;</comment>
                            <comment id="16107029" author="till.rohrmann" created="Mon, 31 Jul 2017 09:28:33 +0000"  >&lt;p&gt;True. I hope this link works now: &lt;a href=&quot;https://travis-ci.org/tillrohrmann/flink/jobs/258538641&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://travis-ci.org/tillrohrmann/flink/jobs/258538641&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16108748" author="nicok" created="Tue, 1 Aug 2017 11:38:41 +0000"  >&lt;p&gt;I got another incarnation (seen only once) with a different failure (only change in there is switching from &lt;tt&gt;HeapMemorySegment&lt;/tt&gt; to &lt;tt&gt;HybridMemorySegment&lt;/tt&gt; but since the memory type was not changed (still at on-heap by default) this should not be related.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;09:02:49,616 ERROR org.apache.flink.streaming.connectors.kafka.Kafka010ProducerITCase  - 
--------------------------------------------------------------------------------
Test testOneToOneAtLeastOnceCustomOperator(org.apache.flink.streaming.connectors.kafka.Kafka010ProducerITCase) failed with:
java.lang.OutOfMemoryError: Java heap space
	at java.nio.HeapByteBuffer.&amp;lt;init&amp;gt;(HeapByteBuffer.java:57)
	at java.nio.ByteBuffer.allocate(ByteBuffer.java:335)
	at kafka.log.SkimpyOffsetMap.&amp;lt;init&amp;gt;(OffsetMap.scala:44)
	at kafka.log.LogCleaner$CleanerThread.&amp;lt;init&amp;gt;(LogCleaner.scala:198)
	at kafka.log.LogCleaner$$anonfun$2.apply(LogCleaner.scala:89)
	at kafka.log.LogCleaner$$anonfun$2.apply(LogCleaner.scala:89)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)
	at scala.collection.immutable.Range.foreach(Range.scala:160)
	at scala.collection.TraversableLike$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;map(TraversableLike.scala:234)
	at scala.collection.AbstractTraversable.map(Traversable.scala:104)
	at kafka.log.LogCleaner.&amp;lt;init&amp;gt;(LogCleaner.scala:89)
	at kafka.log.LogManager.&amp;lt;init&amp;gt;(LogManager.scala:72)
	at kafka.server.KafkaServer.createLogManager(KafkaServer.scala:648)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:208)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.getKafkaServer(KafkaTestEnvironmentImpl.java:433)
	at org.apache.flink.streaming.connectors.kafka.KafkaTestEnvironmentImpl.restartBroker(KafkaTestEnvironmentImpl.java:181)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnce(KafkaProducerTestBase.java:282)
	at org.apache.flink.streaming.connectors.kafka.KafkaProducerTestBase.testOneToOneAtLeastOnceCustomOperator(KafkaProducerTestBase.java:212)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:55)
	at org.junit.rules.RunRules.evaluate(RunRules.java:20)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;a href=&quot;https://transfer.sh/H7pW5/369.3.tar.gz&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://transfer.sh/H7pW5/369.3.tar.gz&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16108974" author="githubbot" created="Tue, 1 Aug 2017 14:15:29 +0000"  >&lt;p&gt;GitHub user pnowojski opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4456&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4456&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6996&quot; title=&quot;FlinkKafkaProducer010 doesn&amp;#39;t guarantee at-least-once semantic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6996&quot;&gt;&lt;del&gt;FLINK-6996&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Increase Xmx for tests&lt;/p&gt;

&lt;p&gt;    As reported by @NicoK, sometimes 1000m was not enough memory to run at-least-once tests with broker failures on Travis. I remember having the same issue in #4239 where I have set this same value to `2048`. Hopefully it will solve the problems.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/pnowojski/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/pnowojski/flink&lt;/a&gt; kafka010&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4456.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4456.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #4456&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 8c211b8fd975af441c5a762ee45a62a7dd44f173&lt;br/&gt;
Author: Piotr Nowojski &amp;lt;piotr.nowojski@gmail.com&amp;gt;&lt;br/&gt;
Date:   2017-08-01T13:02:56Z&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;docs&amp;#93;&lt;/span&gt; Add section in docs about writing unit and integration tests&lt;/p&gt;

&lt;p&gt;commit dd7060497454c2450be3f33a4cf7bdf8cc854f14&lt;br/&gt;
Author: Piotr Nowojski &amp;lt;piotr.nowojski@gmail.com&amp;gt;&lt;br/&gt;
Date:   2017-08-01T14:05:49Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-6996&quot; title=&quot;FlinkKafkaProducer010 doesn&amp;#39;t guarantee at-least-once semantic&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-6996&quot;&gt;&lt;del&gt;FLINK-6996&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;kafka&amp;#93;&lt;/span&gt; Increase Xmx for tests&lt;/p&gt;

&lt;p&gt;    Sometimes 1000m was not enough memory to run at-least-once tests with broker failures on Travis&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16109608" author="githubbot" created="Tue, 1 Aug 2017 19:39:13 +0000"  >&lt;p&gt;Github user greghogan commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/4456&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/4456&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    +1 this matches the parent `pom.xml`. Wondering if the same change would fix `flink-hbase` always failing for me when running `mvn verify`.&lt;/p&gt;</comment>
                            <comment id="16110443" author="pnowojski" created="Wed, 2 Aug 2017 07:15:28 +0000"  >&lt;p&gt;Those test failures most likely are not caused by actual bug in Flink, but a intermittent tests. I have created separate issue for fixing those tests: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-7343&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-7343&lt;/a&gt; . Closing this issue.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 15 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3go5r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>