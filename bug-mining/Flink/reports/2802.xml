<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:38:00 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-11044] RegisterTableSink docs incorrect</title>
                <link>https://issues.apache.org/jira/browse/FLINK-11044</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;Parameter order and types incorrect for RegisterTableSink here: &lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/table/connect.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/table/connect.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;It&apos;s correct here:&#160;&lt;a href=&quot;https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/table/common.html#register-a-tablesink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://ci.apache.org/projects/flink/flink-docs-release-1.7/dev/table/common.html#register-a-tablesink&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</description>
                <environment></environment>
        <key id="13201719">FLINK-11044</key>
            <summary>RegisterTableSink docs incorrect</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="-1">Unassigned</assignee>
                                    <reporter username="kgorman">Kenny Gorman</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Fri, 30 Nov 2018 21:32:54 +0000</created>
                <updated>Fri, 15 Mar 2019 12:27:29 +0000</updated>
                            <resolved>Tue, 4 Dec 2018 09:15:44 +0000</resolved>
                                    <version>1.6.2</version>
                    <version>1.7.0</version>
                                    <fixVersion>1.7.1</fixVersion>
                    <fixVersion>1.8.0</fixVersion>
                                    <component>Documentation</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>3</watches>
                                                                                                                <comments>
                            <comment id="16705361" author="githubbot" created="Fri, 30 Nov 2018 22:33:30 +0000"  >&lt;p&gt;kgorman opened a new pull request #7208: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-11044&quot; title=&quot;RegisterTableSink docs incorrect&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-11044&quot;&gt;&lt;del&gt;FLINK-11044&lt;/del&gt;&lt;/a&gt; connect docs fix for registerTableSink&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7208&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7208&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   &amp;lt;!--&lt;br/&gt;
   &lt;b&gt;Thank you very much for contributing to Apache Flink - we are happy that you want to help us improve Flink. To help the community review your contribution in the best possible way, please go through the checklist below, which will get the contribution into a shape in which it can be best reviewed.&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;   &lt;b&gt;Please understand that we do not do this to make contributions to Flink a hassle. In order to uphold a high standard of quality for code contributions, while at the same time managing a large number of contributions, we need contributors to prepare the contributions well, and give reviewers enough contextual information for the review. Please also understand that contributions that do not follow this guide will take longer to review and thus typically be picked up with lower priority by the community.&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Contribution Checklist&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Make sure that the pull request corresponds to a &lt;span class=&quot;error&quot;&gt;&amp;#91;JIRA issue&amp;#93;&lt;/span&gt;(&lt;a href=&quot;https://issues.apache.org/jira/projects/FLINK/issues&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/projects/FLINK/issues&lt;/a&gt;). Exceptions are made for typos in JavaDoc or documentation files, which need no JIRA issue.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Name the pull request in the form &quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;FLINK-XXXX&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;component&amp;#93;&lt;/span&gt; Title of the pull request&quot;, where &lt;b&gt;FLINK-XXXX&lt;/b&gt; should be replaced by the actual issue number. Skip &lt;b&gt;component&lt;/b&gt; if you are unsure about which is the best component.&lt;br/&gt;
     Typo fixes that have no associated JIRA issue should be named following this pattern: `&lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;docs&amp;#93;&lt;/span&gt; Fix typo in event time introduction` or `&lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;javadocs&amp;#93;&lt;/span&gt; Expand JavaDoc for PuncuatedWatermarkGenerator`.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Fill out the template below to describe the changes contributed by the pull request. That will give reviewers the context they need to do the review.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Make sure that the change passes the automated tests, i.e., `mvn clean verify` passes. You can set up Travis CI to do that following &lt;span class=&quot;error&quot;&gt;&amp;#91;this guide&amp;#93;&lt;/span&gt;(&lt;a href=&quot;http://flink.apache.org/contribute-code.html#best-practices&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://flink.apache.org/contribute-code.html#best-practices&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Each pull request should address only one issue, not mix up code from multiple issues.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Each commit in the pull request has a meaningful commit message (including the JIRA id)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Once all items of the checklist are addressed, remove the above text and this checklist, leaving only the filled out template below.&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;   *&lt;b&gt;(The sections below can be removed for hotfixes of typos)&lt;/b&gt;*&lt;br/&gt;
   --&amp;gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   &lt;b&gt;(For example: This pull request makes task deployment go through the blob server, rather than through RPC. That way we avoid re-transferring them on each deployment (during recovery).)&lt;/b&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Brief change log&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   &lt;b&gt;(for example&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/b&gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;b&gt;The TaskInfo is stored in the blob store on job creation time as a persistent artifact&lt;/b&gt;&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Deployments RPC transmits only the blob storage reference&lt;/b&gt;&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;TaskManagers retrieve the TaskInfo from the blob cache&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;



&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   &lt;b&gt;(Please pick either of the following options)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;   This change is a trivial rework / code cleanup without any test coverage.&lt;/p&gt;

&lt;p&gt;   &lt;b&gt;(or)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;   This change is already covered by existing tests, such as &lt;b&gt;(please describe tests)&lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;   &lt;b&gt;(or)&lt;/b&gt;&lt;/p&gt;

&lt;p&gt;   This change added tests and can be verified as follows:&lt;/p&gt;

&lt;p&gt;   &lt;b&gt;(example&lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/b&gt;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;b&gt;Added integration tests for end-to-end deployment with large payloads (100MB)&lt;/b&gt;&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Extended integration test for recovery after master (JobManager) failure&lt;/b&gt;&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Added test that validates that TaskInfo is transferred only once across recoveries&lt;/b&gt;&lt;/li&gt;
	&lt;li&gt;&lt;b&gt;Manually verified the change by running a 4 node cluser with 2 JobManagers and 4 TaskManagers, a stateful streaming program, and killing one JobManager and two TaskManagers during the execution, verifying that recovery happens correctly.&lt;/b&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): (yes / no)&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (yes / no)&lt;/li&gt;
	&lt;li&gt;The serializers: (yes / no / don&apos;t know)&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): (yes / no / don&apos;t know)&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (yes / no / don&apos;t know)&lt;/li&gt;
	&lt;li&gt;The S3 file system connector: (yes / no / don&apos;t know)&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? (yes / no)&lt;br/&gt;
   No&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? (not applicable / docs / JavaDocs / not documented)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16708436" author="githubbot" created="Tue, 4 Dec 2018 09:09:49 +0000"  >&lt;p&gt;asfgit closed pull request #7208: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-11044&quot; title=&quot;RegisterTableSink docs incorrect&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-11044&quot;&gt;&lt;del&gt;FLINK-11044&lt;/del&gt;&lt;/a&gt; connect docs fix for registerTableSink&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/7208&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/7208&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/docs/dev/table/connect.md b/docs/dev/table/connect.md&lt;br/&gt;
index effd913707e..d8677714fa8 100644&lt;br/&gt;
&amp;#8212; a/docs/dev/table/connect.md&lt;br/&gt;
+++ b/docs/dev/table/connect.md&lt;br/&gt;
@@ -312,7 +312,7 @@ The following timestamp extractors are supported:&lt;br/&gt;
     .timestampsFromField(&quot;ts_field&quot;)    // required: original field name in the input&lt;br/&gt;
 )&lt;/p&gt;

&lt;p&gt;-// Converts the assigned timestamps from a DataStream API record into the rowtime attribute &lt;br/&gt;
+// Converts the assigned timestamps from a DataStream API record into the rowtime attribute&lt;br/&gt;
 // and thus preserves the assigned timestamps from the source.&lt;br/&gt;
 // This requires a source that assigns timestamps (e.g., Kafka 0.10+).&lt;br/&gt;
 .rowtime(&lt;br/&gt;
@@ -337,7 +337,7 @@ rowtime:&lt;br/&gt;
     type: from-field&lt;br/&gt;
     from: &quot;ts_field&quot;                 # required: original field name in the input&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Converts the assigned timestamps from a DataStream API record into the rowtime attribute&lt;br/&gt;
+# Converts the assigned timestamps from a DataStream API record into the rowtime attribute&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;
&lt;ol&gt;
	&lt;li&gt;and thus preserves the assigned timestamps from the source.&lt;br/&gt;
 rowtime:&lt;br/&gt;
   timestamps:&lt;br/&gt;
@@ -351,7 +351,7 @@ The following watermark strategies are supported:&lt;br/&gt;
 &amp;lt;div class=&quot;codetabs&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
 &amp;lt;div data-lang=&quot;Java/Scala&quot; markdown=&quot;1&quot;&amp;gt;
 {% highlight java %}&lt;br/&gt;
-// Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum &lt;br/&gt;
+// Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum&lt;br/&gt;
 // observed timestamp so far minus 1. Rows that have a timestamp equal to the max timestamp&lt;br/&gt;
 // are not late.&lt;br/&gt;
 .rowtime(&lt;br/&gt;
@@ -377,7 +377,7 @@ The following watermark strategies are supported:&lt;br/&gt;
 &lt;br/&gt;
 &amp;lt;div data-lang=&quot;YAML&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
 {% highlight yaml %}&lt;br/&gt;
-# Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum &lt;br/&gt;
+# Sets a watermark strategy for ascending rowtime attributes. Emits a watermark of the maximum&lt;br/&gt;
 # observed timestamp so far minus 1. Rows that have a timestamp equal to the max timestamp&lt;br/&gt;
 # are not late.&lt;br/&gt;
 rowtime:&lt;br/&gt;
@@ -695,7 +695,7 @@ connector:&lt;br/&gt;
 &lt;br/&gt;
 *&lt;b&gt;Key extraction:&lt;/b&gt;* Flink automatically extracts valid keys from a query. For example, a query `SELECT a, b, c FROM t GROUP BY a, b` defines a composite key of the fields `a` and `b`. The Elasticsearch connector generates a document ID string for every row by concatenating all key fields in the order defined in the query using a key delimiter. A custom representation of null literals for key fields can be defined.&lt;br/&gt;
 &lt;br/&gt;
-&amp;lt;span class=&quot;label label-danger&quot;&amp;gt;Attention&amp;lt;/span&amp;gt; A JSON format defines how to encode documents for the external system, therefore, it must be added as a &lt;span class=&quot;error&quot;&gt;&amp;#91;dependency&amp;#93;&lt;/span&gt;(connect.html#formats). &lt;br/&gt;
+&amp;lt;span class=&quot;label label-danger&quot;&amp;gt;Attention&amp;lt;/span&amp;gt; A JSON format defines how to encode documents for the external system, therefore, it must be added as a &lt;span class=&quot;error&quot;&gt;&amp;#91;dependency&amp;#93;&lt;/span&gt;(connect.html#formats).&lt;br/&gt;
 &lt;br/&gt;
 {% top %}&lt;br/&gt;
 &lt;br/&gt;
@@ -717,8 +717,8 @@ The CSV format allows to read and write comma-separated rows.&lt;br/&gt;
   new Csv()&lt;br/&gt;
     .field(&quot;field1&quot;, Types.STRING)    // required: ordered format fields&lt;br/&gt;
     .field(&quot;field2&quot;, Types.TIMESTAMP)&lt;br/&gt;
-    .fieldDelimiter(&quot;,&quot;)              // optional: string delimiter &quot;,&quot; by default &lt;br/&gt;
-    .lineDelimiter(&quot;\n&quot;)              // optional: string delimiter &quot;\n&quot; by default &lt;br/&gt;
+    .fieldDelimiter(&quot;,&quot;)              // optional: string delimiter &quot;,&quot; by default&lt;br/&gt;
+    .lineDelimiter(&quot;\n&quot;)              // optional: string delimiter &quot;\n&quot; by default&lt;br/&gt;
     .quoteCharacter(&apos;&quot;&apos;)              // optional: single character for string values, empty by default&lt;br/&gt;
     .commentPrefix(&apos;#&apos;)               // optional: string to indicate comments, empty by default&lt;br/&gt;
     .ignoreFirstLine()                // optional: ignore the first line, by default it is not skipped&lt;br/&gt;
@@ -736,8 +736,8 @@ format:&lt;br/&gt;
       type: VARCHAR&lt;br/&gt;
     - name: field2&lt;br/&gt;
       type: TIMESTAMP&lt;br/&gt;
-  field-delimiter: &quot;,&quot;       # optional: string delimiter &quot;,&quot; by default &lt;br/&gt;
-  line-delimiter: &quot;\n&quot;       # optional: string delimiter &quot;\n&quot; by default &lt;br/&gt;
+  field-delimiter: &quot;,&quot;       # optional: string delimiter &quot;,&quot; by default&lt;br/&gt;
+  line-delimiter: &quot;\n&quot;       # optional: string delimiter &quot;\n&quot; by default&lt;br/&gt;
   quote-character: &apos;&quot;&apos;       # optional: single character for string values, empty by default&lt;br/&gt;
   comment-prefix: &apos;#&apos;        # optional: string to indicate comments, empty by default&lt;br/&gt;
   ignore-first-line: false   # optional: boolean flag to ignore the first line, by default it is not skipped&lt;br/&gt;
@@ -992,7 +992,7 @@ These are the additional `TableSink`s which are provided with Flink:&lt;br/&gt;
 | *&lt;b&gt;Class name&lt;/b&gt;* | *&lt;b&gt;Maven dependency&lt;/b&gt;* | *&lt;b&gt;Batch?&lt;/b&gt;* | *&lt;b&gt;Streaming?&lt;/b&gt;* | *&lt;b&gt;Description&lt;/b&gt;*&lt;br/&gt;
 | `CsvTableSink` | `flink-table` | Y | Append | A simple sink for CSV files.&lt;br/&gt;
 | `JDBCAppendTableSink` | `flink-jdbc` | Y | Append | Writes a Table to a JDBC table.&lt;br/&gt;
-| `CassandraAppendTableSink` | `flink-connector-cassandra` | N | Append | Writes a Table to a Cassandra table. &lt;br/&gt;
+| `CassandraAppendTableSink` | `flink-connector-cassandra` | N | Append | Writes a Table to a Cassandra table.&lt;br/&gt;
 &lt;br/&gt;
 ### OrcTableSource&lt;br/&gt;
 &lt;br/&gt;
@@ -1044,7 +1044,7 @@ val orcTableSource = OrcTableSource.builder()&lt;br/&gt;
 &lt;br/&gt;
 ### CsvTableSink&lt;br/&gt;
 &lt;br/&gt;
-The `CsvTableSink` emits a `Table` to one or more CSV files. &lt;br/&gt;
+The `CsvTableSink` emits a `Table` to one or more CSV files.&lt;br/&gt;
 &lt;br/&gt;
 The sink only supports append-only streaming tables. It cannot be used to emit a `Table` that is continuously updated. See the &lt;span class=&quot;error&quot;&gt;&amp;#91;documentation on Table to Stream conversions&amp;#93;&lt;/span&gt;(./streaming/dynamic_tables.html#table-to-stream-conversion) for details. When emitting a streaming table, rows are written at least once (if checkpointing is enabled) and the `CsvTableSink` does not split output files into bucket files but continuously writes to the same files.&lt;br/&gt;
 &lt;br/&gt;
@@ -1053,17 +1053,17 @@ The sink only supports append-only streaming tables. It cannot be used to emit a&lt;br/&gt;
 {% highlight java %}&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt; CsvTableSink sink = new CsvTableSink(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;path,                  // output path&lt;br/&gt;
+    path,                  // output path&lt;br/&gt;
     &quot;|&quot;,                   // optional: delimit files by &apos;|&apos;&lt;br/&gt;
     1,                     // optional: write to a single file&lt;br/&gt;
     WriteMode.OVERWRITE);  // optional: override existing files&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; tableEnv.registerTableSink(&lt;br/&gt;
   &quot;csvOutputTable&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sink,&lt;br/&gt;
   // specify table schema&lt;br/&gt;
   new String[]
{&quot;f0&quot;, &quot;f1&quot;}
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;new TypeInformation[]
{Types.STRING, Types.INT});&lt;br/&gt;
+  new TypeInformation[]{Types.STRING, Types.INT}
&lt;p&gt;,&lt;br/&gt;
+  sink);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; Table table = ...&lt;br/&gt;
 table.insertInto(&quot;csvOutputTable&quot;);&lt;br/&gt;
@@ -1074,17 +1074,17 @@ table.insertInto(&quot;csvOutputTable&quot;);&lt;/p&gt;
 {% highlight scala %}

&lt;p&gt; val sink: CsvTableSink = new CsvTableSink(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;path,                             // output path&lt;br/&gt;
+    path,                             // output path&lt;br/&gt;
     fieldDelim = &quot;|&quot;,                 // optional: delimit files by &apos;|&apos;&lt;br/&gt;
     numFiles = 1,                     // optional: write to a single file&lt;br/&gt;
     writeMode = WriteMode.OVERWRITE)  // optional: override existing files&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; tableEnv.registerTableSink(&lt;br/&gt;
   &quot;csvOutputTable&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sink,&lt;br/&gt;
   // specify table schema&lt;br/&gt;
   Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;(&quot;f0&quot;, &quot;f1&quot;),&lt;/li&gt;
	&lt;li&gt;Array[TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;](Types.STRING, Types.INT))&lt;br/&gt;
+  Array[TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;](Types.STRING, Types.INT),&lt;br/&gt;
+  sink)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; val table: Table = ???&lt;br/&gt;
 table.insertInto(&quot;csvOutputTable&quot;)&lt;br/&gt;
@@ -1113,10 +1113,10 @@ JDBCAppendTableSink sink = JDBCAppendTableSink.builder()&lt;/p&gt;

&lt;p&gt; tableEnv.registerTableSink(&lt;br/&gt;
   &quot;jdbcOutputTable&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sink,&lt;br/&gt;
   // specify table schema&lt;br/&gt;
   new String[]
{&quot;id&quot;}
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;new TypeInformation[]
{Types.INT});&lt;br/&gt;
+  new TypeInformation[]{Types.INT}
&lt;p&gt;,&lt;br/&gt;
+  sink);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; Table table = ...&lt;br/&gt;
 table.insertInto(&quot;jdbcOutputTable&quot;);&lt;br/&gt;
@@ -1134,10 +1134,10 @@ val sink: JDBCAppendTableSink = JDBCAppendTableSink.builder()&lt;/p&gt;

&lt;p&gt; tableEnv.registerTableSink(&lt;br/&gt;
   &quot;jdbcOutputTable&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sink,&lt;br/&gt;
   // specify table schema&lt;br/&gt;
   Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;(&quot;id&quot;),&lt;/li&gt;
	&lt;li&gt;Array[TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;](Types.INT))&lt;br/&gt;
+  Array[TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;](Types.INT),&lt;br/&gt;
+  sink)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; val table: Table = ???&lt;br/&gt;
 table.insertInto(&quot;jdbcOutputTable&quot;)&lt;br/&gt;
@@ -1145,7 +1145,7 @@ table.insertInto(&quot;jdbcOutputTable&quot;)&lt;br/&gt;
 &amp;lt;/div&amp;gt;&lt;br/&gt;
 &amp;lt;/div&amp;gt;&lt;/p&gt;

&lt;p&gt;-Similar to using &amp;lt;code&amp;gt;JDBCOutputFormat&amp;lt;/code&amp;gt;, you have to explicitly specify the name of the JDBC driver, the JDBC URL, the query to be executed, and the field types of the JDBC table. &lt;br/&gt;
+Similar to using &amp;lt;code&amp;gt;JDBCOutputFormat&amp;lt;/code&amp;gt;, you have to explicitly specify the name of the JDBC driver, the JDBC URL, the query to be executed, and the field types of the JDBC table.&lt;/p&gt;

 {% top %}

&lt;p&gt;@@ -1164,16 +1164,16 @@ To use the `CassandraAppendTableSink`, you have to add the Cassandra connector d&lt;br/&gt;
 ClusterBuilder builder = ... // configure Cassandra cluster connection&lt;/p&gt;

&lt;p&gt; CassandraAppendTableSink sink = new CassandraAppendTableSink(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;builder,&lt;br/&gt;
+  builder,&lt;br/&gt;
   // the query must match the schema of the table&lt;br/&gt;
   INSERT INTO flink.myTable (id, name, value) VALUES (?, ?, ?));&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; tableEnv.registerTableSink(&lt;br/&gt;
   &quot;cassandraOutputTable&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sink,&lt;br/&gt;
   // specify table schema&lt;br/&gt;
   new String[]
{&quot;id&quot;, &quot;name&quot;, &quot;value&quot;}
&lt;p&gt;,&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;new TypeInformation[]
{Types.INT, Types.STRING, Types.DOUBLE});&lt;br/&gt;
+  new TypeInformation[]{Types.INT, Types.STRING, Types.DOUBLE}
&lt;p&gt;,&lt;br/&gt;
+  sink);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; Table table = ...&lt;br/&gt;
 table.insertInto(cassandraOutputTable);&lt;br/&gt;
@@ -1185,16 +1185,16 @@ table.insertInto(cassandraOutputTable);&lt;br/&gt;
 val builder: ClusterBuilder = ... // configure Cassandra cluster connection&lt;/p&gt;

&lt;p&gt; val sink: CassandraAppendTableSink = new CassandraAppendTableSink(&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;builder,&lt;br/&gt;
+  builder,&lt;br/&gt;
   // the query must match the schema of the table&lt;br/&gt;
   INSERT INTO flink.myTable (id, name, value) VALUES (?, ?, ?))&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; tableEnv.registerTableSink(&lt;br/&gt;
   &quot;cassandraOutputTable&quot;,&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;sink,&lt;br/&gt;
   // specify table schema&lt;br/&gt;
   Array&lt;span class=&quot;error&quot;&gt;&amp;#91;String&amp;#93;&lt;/span&gt;(&quot;id&quot;, &quot;name&quot;, &quot;value&quot;),&lt;/li&gt;
	&lt;li&gt;Array[TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;](Types.INT, Types.STRING, Types.DOUBLE))&lt;br/&gt;
+  Array[TypeInformation&lt;span class=&quot;error&quot;&gt;&amp;#91;_&amp;#93;&lt;/span&gt;](Types.INT, Types.STRING, Types.DOUBLE),&lt;br/&gt;
+  sink)&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt; val table: Table = ???&lt;br/&gt;
 table.insertInto(cassandraOutputTable)&lt;/p&gt;




&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16708450" author="twalthr" created="Tue, 4 Dec 2018 09:15:44 +0000"  >&lt;p&gt;Fixed in master: b0d1d33f4bad12ea79954bb7735081b04b441837&lt;br/&gt;
Fixed in 1.7: 1034f3e920df88bee75ff36a3d8732a1bd7b2913&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 50 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|s0124o:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>