<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:35:03 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-10259] Key validation for GroupWindowAggregate is broken</title>
                <link>https://issues.apache.org/jira/browse/FLINK-10259</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;WindowGroups have multiple equivalent keys (start, end) that should be handled differently from other keys. The &lt;tt&gt;UpdatingPlanChecker&lt;/tt&gt; uses equivalence groups to identify equivalent keys but the keys of WindowGroups are not correctly assigned to groups.&lt;/p&gt;

&lt;p&gt;This means that we cannot correctly extract keys from queries that use group windows.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13182129">FLINK-10259</key>
            <summary>Key validation for GroupWindowAggregate is broken</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="fhueske">Fabian Hueske</assignee>
                                    <reporter username="fhueske">Fabian Hueske</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Thu, 30 Aug 2018 13:38:33 +0000</created>
                <updated>Fri, 19 Oct 2018 09:10:19 +0000</updated>
                            <resolved>Fri, 19 Oct 2018 09:10:19 +0000</resolved>
                                                    <fixVersion>1.5.5</fixVersion>
                    <fixVersion>1.6.2</fixVersion>
                    <fixVersion>1.7.0</fixVersion>
                                    <component>Table SQL / API</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="16598371" author="githubbot" created="Fri, 31 Aug 2018 07:35:17 +0000"  >&lt;p&gt;fhueske opened a new pull request #6641: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10259&quot; title=&quot;Key validation for GroupWindowAggregate is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10259&quot;&gt;&lt;del&gt;FLINK-10259&lt;/del&gt;&lt;/a&gt; Fix key extraction for GroupWindows.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6641&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6641&lt;/a&gt;&lt;/p&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;   Fixes the key validation for SQL queries with group window aggregates that are emitted to an `UpsertStreamTableSink`.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Brief change log&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul&gt;
	&lt;li&gt;Handle window properties (start, end, rowtime) in one equivalence group because all property attributes identify the same window.&lt;/li&gt;
	&lt;li&gt;Add tests for SQL `INSERT INTO` of different result types to different types of table sinks.&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul&gt;
	&lt;li&gt;Run the added tests.&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): *&lt;b&gt;no&lt;/b&gt;*&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: *&lt;b&gt;no&lt;/b&gt;*&lt;/li&gt;
	&lt;li&gt;The serializers: *&lt;b&gt;no&lt;/b&gt;*&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): *&lt;b&gt;no&lt;/b&gt;*&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: *&lt;b&gt;no&lt;/b&gt;*&lt;/li&gt;
	&lt;li&gt;The S3 file system connector: *&lt;b&gt;no&lt;/b&gt;*&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? *&lt;b&gt;no&lt;/b&gt;*&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? *&lt;b&gt;n/a&lt;/b&gt;*&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16601235" author="githubbot" created="Sun, 2 Sep 2018 15:12:24 +0000"  >&lt;p&gt;hequn8128 commented on a change in pull request #6641: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10259&quot; title=&quot;Key validation for GroupWindowAggregate is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10259&quot;&gt;&lt;del&gt;FLINK-10259&lt;/del&gt;&lt;/a&gt; Fix key extraction for GroupWindows.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6641#discussion_r214543996&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6641#discussion_r214543996&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/InsertIntoITCase.scala&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -0,0 +1,406 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.stream.sql&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.flink.api.common.typeinfo.TypeInformation&lt;br/&gt;
+import org.apache.flink.streaming.api.TimeCharacteristic&lt;br/&gt;
+import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment&lt;br/&gt;
+import org.apache.flink.table.api.scala._&lt;br/&gt;
+import org.apache.flink.table.api.&lt;/p&gt;
{TableEnvironment, Types}
&lt;p&gt;+import org.apache.flink.table.runtime.stream.table.&lt;/p&gt;
{RowCollector, TestRetractSink, TestUpsertSink}
&lt;p&gt;+import org.apache.flink.table.runtime.utils.StreamTestData&lt;br/&gt;
+import org.apache.flink.table.utils.MemoryTableSourceSinkUtil&lt;br/&gt;
+import org.apache.flink.test.util.&lt;/p&gt;
{AbstractTestBase, TestBaseUtils}
&lt;p&gt;+import org.junit.Assert._&lt;br/&gt;
+import org.junit.Test&lt;br/&gt;
+&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+class InsertIntoITCase extends AbstractTestBase {&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendStreamToTableSink(): Unit = {&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   Since we have this test case, should we remove the `testInsertIntoMemoryTable` in `SqlITCase` ? It seems the two test cases are same.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16601236" author="githubbot" created="Sun, 2 Sep 2018 15:12:24 +0000"  >&lt;p&gt;hequn8128 commented on a change in pull request #6641: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10259&quot; title=&quot;Key validation for GroupWindowAggregate is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10259&quot;&gt;&lt;del&gt;FLINK-10259&lt;/del&gt;&lt;/a&gt; Fix key extraction for GroupWindows.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6641#discussion_r214543970&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6641#discussion_r214543970&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/InsertIntoITCase.scala&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -0,0 +1,406 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.stream.sql&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.flink.api.common.typeinfo.TypeInformation&lt;br/&gt;
+import org.apache.flink.streaming.api.TimeCharacteristic&lt;br/&gt;
+import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment&lt;br/&gt;
+import org.apache.flink.table.api.scala._&lt;br/&gt;
+import org.apache.flink.table.api.&lt;/p&gt;
{TableEnvironment, Types}
&lt;p&gt;+import org.apache.flink.table.runtime.stream.table.&lt;/p&gt;
{RowCollector, TestRetractSink, TestUpsertSink}
&lt;p&gt;+import org.apache.flink.table.runtime.utils.StreamTestData&lt;br/&gt;
+import org.apache.flink.table.utils.MemoryTableSourceSinkUtil&lt;br/&gt;
+import org.apache.flink.test.util.&lt;/p&gt;
{AbstractTestBase, TestBaseUtils}
&lt;p&gt;+import org.junit.Assert._&lt;br/&gt;
+import org.junit.Test&lt;br/&gt;
+&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+class InsertIntoITCase extends AbstractTestBase {&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   extends StreamingWithStateTestBase? &lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16601237" author="githubbot" created="Sun, 2 Sep 2018 15:12:25 +0000"  >&lt;p&gt;hequn8128 commented on a change in pull request #6641: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10259&quot; title=&quot;Key validation for GroupWindowAggregate is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10259&quot;&gt;&lt;del&gt;FLINK-10259&lt;/del&gt;&lt;/a&gt; Fix key extraction for GroupWindows.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6641#discussion_r214544003&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6641#discussion_r214544003&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/InsertIntoITCase.scala&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -0,0 +1,406 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.stream.sql&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.flink.api.common.typeinfo.TypeInformation&lt;br/&gt;
+import org.apache.flink.streaming.api.TimeCharacteristic&lt;br/&gt;
+import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment&lt;br/&gt;
+import org.apache.flink.table.api.scala._&lt;br/&gt;
+import org.apache.flink.table.api.&lt;/p&gt;
{TableEnvironment, Types}
&lt;p&gt;+import org.apache.flink.table.runtime.stream.table.&lt;/p&gt;
{RowCollector, TestRetractSink, TestUpsertSink}
&lt;p&gt;+import org.apache.flink.table.runtime.utils.StreamTestData&lt;br/&gt;
+import org.apache.flink.table.utils.MemoryTableSourceSinkUtil&lt;br/&gt;
+import org.apache.flink.test.util.&lt;/p&gt;
{AbstractTestBase, TestBaseUtils}
&lt;p&gt;+import org.junit.Assert._&lt;br/&gt;
+import org.junit.Test&lt;br/&gt;
+&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+class InsertIntoITCase extends AbstractTestBase {&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendStreamToTableSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    MemoryTableSourceSinkUtil.clear()
+
+    val input = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(r =&amp;gt; r._2)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, input, &apos;a, &apos;b, &apos;c, &apos;t.rowtime)
+
+    val fieldNames = Array(&quot;d&quot;, &quot;e&quot;, &quot;t&quot;)
+    val fieldTypes: Array[TypeInformation[_]] = Array(Types.STRING, Types.SQL_TIMESTAMP, Types.LONG)
+    val sink = new MemoryTableSourceSinkUtil.UnsafeMemoryAppendTableSink
+
+    tEnv.registerTableSink(&quot;targetTable&quot;, fieldNames, fieldTypes, sink)
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT c, t, b
+         |FROM sourceTable
+         |WHERE a &amp;lt; 3 OR a &amp;gt; 19
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+
+    val expected = Seq(
+      &quot;Hi,1970-01-01 00:00:00.001,1&quot;,
+      &quot;Hello,1970-01-01 00:00:00.002,2&quot;,
+      &quot;Comment#14,1970-01-01 00:00:00.006,6&quot;,
+      &quot;Comment#15,1970-01-01 00:00:00.006,6&quot;).mkString(&quot;\n&quot;)
+
+    TestBaseUtils.compareResultAsText(MemoryTableSourceSinkUtil.tableData.asJava, expected)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoUpdatingTableToRetractSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+
+    val t = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(_._1.toLong)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text)
+    tEnv.registerTableSink(
+      &quot;targetTable&quot;,
+      Array(&quot;len&quot;, &quot;cntid&quot;, &quot;sumnum&quot;),
+      Array(Types.INT, Types.LONG, Types.LONG),
+      new TestRetractSink)
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT len, COUNT(id) AS cntid, SUM(num) AS sumnum
+         |FROM (SELECT id, num, CHAR_LENGTH(text) AS len FROM sourceTable)
+         |GROUP BY len
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+    val results = RowCollector.getAndClearValues
+
+    val retracted = RowCollector.retractResults(results).sorted
+    val expected = List(
+      &quot;2,1,1&quot;,
+      &quot;5,1,2&quot;,
+      &quot;11,1,2&quot;,
+      &quot;25,1,3&quot;,
+      &quot;10,7,39&quot;,
+      &quot;14,1,3&quot;,
+      &quot;9,9,41&quot;).sorted
+    assertEquals(expected, retracted)
+
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendTableToRetractSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+
+    val t = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(_._1.toLong)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text, &apos;rowtime.rowtime)
+    tEnv.registerTableSink(
+      &quot;targetTable&quot;,
+      Array(&quot;wend&quot;, &quot;cntid&quot;, &quot;sumnum&quot;),
+      Array(Types.SQL_TIMESTAMP, Types.LONG, Types.LONG),
+      new TestRetractSink
+    )
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT
+         |  TUMBLE_END(rowtime, INTERVAL &apos;0.005&apos; SECOND) AS wend,
+         |  COUNT(id) AS cntid,
+         |  SUM(num) AS sumnum
+         |FROM sourceTable
+         |GROUP BY TUMBLE(rowtime, INTERVAL &apos;0.005&apos; SECOND)
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+    val results = RowCollector.getAndClearValues
+
+    assertFalse(
+      &quot;Received retraction messages for append only table&quot;,
+      results.exists(!_.f0))
+
+    val retracted = RowCollector.retractResults(results).sorted
+    val expected = List(
+      &quot;1970-01-01 00:00:00.005,4,8&quot;,
+      &quot;1970-01-01 00:00:00.01,5,18&quot;,
+      &quot;1970-01-01 00:00:00.015,5,24&quot;,
+      &quot;1970-01-01 00:00:00.02,5,29&quot;,
+      &quot;1970-01-01 00:00:00.025,2,12&quot;)
+      .sorted
+    assertEquals(expected, retracted)
+
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoUpdatingTableWithFullKeyToUpsertSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+
+    val t = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(_._1.toLong)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text)
+    tEnv.registerTableSink(
+      &quot;targetTable&quot;,
+      Array(&quot;cnt&quot;, &quot;cntid&quot;, &quot;cTrue&quot;),
+      Array(Types.LONG, Types.LONG, Types.BOOLEAN),
+      new TestUpsertSink(Array(&quot;cnt&quot;, &quot;cTrue&quot;), false)
+    )
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT cnt, COUNT(len) AS cntid, cTrue
+         |FROM
+         |  (SELECT CHAR_LENGTH(text) AS len, (id &amp;gt; 0) AS cTrue, COUNT(id) AS cnt
+         |   FROM sourceTable
+         |   GROUP BY CHAR_LENGTH(text), (id &amp;gt; 0)
+         |   )
+         |GROUP BY cnt, cTrue
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+    val results = RowCollector.getAndClearValues
+
+    assertTrue(
+      &quot;Results must include delete messages&quot;,
+      results.exists(_.f0 == false)
+    )
+
+    val retracted = RowCollector.upsertResults(results, Array(0, 2)).sorted
+    val expected = List(
+      &quot;1,5,true&quot;,
+      &quot;7,1,true&quot;,
+      &quot;9,1,true&quot;).sorted
+    assertEquals(expected, retracted)
+
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendingTableWithFullKey1ToUpsertSink(): Unit = {&lt;br/&gt;
+    val env = StreamExecutionEnvironment.getExecutionEnvironment&lt;br/&gt;
+    env.getConfig.enableObjectReuse()&lt;br/&gt;
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)&lt;br/&gt;
+    val tEnv = TableEnvironment.getTableEnvironment(env)&lt;br/&gt;
+&lt;br/&gt;
+    val t = StreamTestData.get3TupleDataStream(env)&lt;br/&gt;
+      .assignAscendingTimestamps(_._1.toLong)&lt;br/&gt;
+&lt;br/&gt;
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text, &apos;rowtime.rowtime)&lt;br/&gt;
+    tEnv.registerTableSink(&lt;br/&gt;
+      &quot;targetTable&quot;,&lt;br/&gt;
+      Array(&quot;num&quot;, &quot;wend&quot;, &quot;cntid&quot;),&lt;br/&gt;
+      Array(Types.LONG, Types.SQL_TIMESTAMP, Types.LONG),&lt;br/&gt;
+      new TestUpsertSink(Array(&quot;wend&quot;, &quot;num&quot;), true)&lt;br/&gt;
+    )&lt;br/&gt;
+&lt;br/&gt;
+    tEnv.sqlUpdate(&lt;br/&gt;
+      s&quot;&quot;&quot;INSERT INTO targetTable&lt;br/&gt;
+         |SELECT&lt;br/&gt;
+         |  num,&lt;br/&gt;
+         |  TUMBLE_END(rowtime, INTERVAL &apos;0.005&apos; SECOND) AS wend,&lt;br/&gt;
+         |  COUNT(id) AS cntid&lt;br/&gt;
+         |FROM sourceTable&lt;br/&gt;
+         |GROUP BY TUMBLE(rowtime, INTERVAL &apos;0.005&apos; SECOND), num&lt;br/&gt;
+       &quot;&quot;&quot;.stripMargin)&lt;br/&gt;
+&lt;br/&gt;
+    env.execute()&lt;br/&gt;
+    val results = RowCollector.getAndClearValues&lt;br/&gt;
+&lt;br/&gt;
+    assertFalse(&lt;br/&gt;
+      &quot;Received retraction messages for append only table&quot;,&lt;br/&gt;
+      results.exists(!_.f0))&lt;br/&gt;
+&lt;br/&gt;
+    val retracted = RowCollector.upsertResults(results, Array(0, 1, 2)).sorted&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   Array(0, 1, 2) -&amp;gt; Array(0, 1)&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16601238" author="githubbot" created="Sun, 2 Sep 2018 15:12:45 +0000"  >&lt;p&gt;hequn8128 commented on a change in pull request #6641: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10259&quot; title=&quot;Key validation for GroupWindowAggregate is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10259&quot;&gt;&lt;del&gt;FLINK-10259&lt;/del&gt;&lt;/a&gt; Fix key extraction for GroupWindows.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6641#discussion_r214543996&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6641#discussion_r214543996&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/InsertIntoITCase.scala&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -0,0 +1,406 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.stream.sql&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.flink.api.common.typeinfo.TypeInformation&lt;br/&gt;
+import org.apache.flink.streaming.api.TimeCharacteristic&lt;br/&gt;
+import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment&lt;br/&gt;
+import org.apache.flink.table.api.scala._&lt;br/&gt;
+import org.apache.flink.table.api.&lt;/p&gt;
{TableEnvironment, Types}
&lt;p&gt;+import org.apache.flink.table.runtime.stream.table.&lt;/p&gt;
{RowCollector, TestRetractSink, TestUpsertSink}
&lt;p&gt;+import org.apache.flink.table.runtime.utils.StreamTestData&lt;br/&gt;
+import org.apache.flink.table.utils.MemoryTableSourceSinkUtil&lt;br/&gt;
+import org.apache.flink.test.util.&lt;/p&gt;
{AbstractTestBase, TestBaseUtils}
&lt;p&gt;+import org.junit.Assert._&lt;br/&gt;
+import org.junit.Test&lt;br/&gt;
+&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+class InsertIntoITCase extends AbstractTestBase {&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendStreamToTableSink(): Unit = {&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   Since we have this test case, should we remove the `testInsertIntoMemoryTable` in `SqlITCase` ? It seems the two test cases are very similar.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16616938" author="githubbot" created="Sun, 16 Sep 2018 23:11:47 +0000"  >&lt;p&gt;fhueske commented on a change in pull request #6641: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10259&quot; title=&quot;Key validation for GroupWindowAggregate is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10259&quot;&gt;&lt;del&gt;FLINK-10259&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix key extraction for GroupWindows.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6641#discussion_r217931616&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6641#discussion_r217931616&lt;/a&gt;&lt;/p&gt;



&lt;p&gt; ##########&lt;br/&gt;
 File path: flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/InsertIntoITCase.scala&lt;br/&gt;
 ##########&lt;br/&gt;
 @@ -0,0 +1,406 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.stream.sql&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.flink.api.common.typeinfo.TypeInformation&lt;br/&gt;
+import org.apache.flink.streaming.api.TimeCharacteristic&lt;br/&gt;
+import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment&lt;br/&gt;
+import org.apache.flink.table.api.scala._&lt;br/&gt;
+import org.apache.flink.table.api.&lt;/p&gt;
{TableEnvironment, Types}
&lt;p&gt;+import org.apache.flink.table.runtime.stream.table.&lt;/p&gt;
{RowCollector, TestRetractSink, TestUpsertSink}
&lt;p&gt;+import org.apache.flink.table.runtime.utils.StreamTestData&lt;br/&gt;
+import org.apache.flink.table.utils.MemoryTableSourceSinkUtil&lt;br/&gt;
+import org.apache.flink.test.util.&lt;/p&gt;
{AbstractTestBase, TestBaseUtils}
&lt;p&gt;+import org.junit.Assert._&lt;br/&gt;
+import org.junit.Test&lt;br/&gt;
+&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+class InsertIntoITCase extends AbstractTestBase {&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendStreamToTableSink(): Unit = {&lt;/p&gt;

&lt;p&gt; Review comment:&lt;br/&gt;
   Good point!&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16616941" author="githubbot" created="Sun, 16 Sep 2018 23:22:08 +0000"  >&lt;p&gt;fhueske commented on issue #6641: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10259&quot; title=&quot;Key validation for GroupWindowAggregate is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10259&quot;&gt;&lt;del&gt;FLINK-10259&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix key extraction for GroupWindows.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6641#issuecomment-421851590&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6641#issuecomment-421851590&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Thanks for the review @hequn8128!&lt;br/&gt;
   I&apos;ve updated the PR.&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16618355" author="githubbot" created="Tue, 18 Sep 2018 01:13:53 +0000"  >&lt;p&gt;hequn8128 commented on issue #6641: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10259&quot; title=&quot;Key validation for GroupWindowAggregate is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10259&quot;&gt;&lt;del&gt;FLINK-10259&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix key extraction for GroupWindows.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6641#issuecomment-422220796&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6641#issuecomment-422220796&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Thanks for the update @fhueske . Looks good to me. +1 to merge&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16619619" author="githubbot" created="Tue, 18 Sep 2018 19:37:21 +0000"  >&lt;p&gt;fhueske commented on issue #6641: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10259&quot; title=&quot;Key validation for GroupWindowAggregate is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10259&quot;&gt;&lt;del&gt;FLINK-10259&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix key extraction for GroupWindows.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6641#issuecomment-422521853&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6641#issuecomment-422521853&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   Thanks for the review @hequn8128! Merging&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16619825" author="githubbot" created="Tue, 18 Sep 2018 22:40:07 +0000"  >&lt;p&gt;asfgit closed pull request #6641: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-10259&quot; title=&quot;Key validation for GroupWindowAggregate is broken&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-10259&quot;&gt;&lt;del&gt;FLINK-10259&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; Fix key extraction for GroupWindows.&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6641&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6641&lt;/a&gt;&lt;/p&gt;




&lt;p&gt;This is a PR merged from a forked repository.&lt;br/&gt;
As GitHub hides the original diff on merge, it is displayed below for&lt;br/&gt;
the sake of provenance:&lt;/p&gt;

&lt;p&gt;As this is a foreign pull request (from a fork), the diff is supplied&lt;br/&gt;
below (as it won&apos;t show otherwise due to GitHub magic):&lt;/p&gt;

&lt;p&gt;diff --git a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/util/UpdatingPlanChecker.scala b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/util/UpdatingPlanChecker.scala&lt;br/&gt;
index 4b7d0ed681c..c47898730b2 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/util/UpdatingPlanChecker.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/main/scala/org/apache/flink/table/plan/util/UpdatingPlanChecker.scala&lt;br/&gt;
@@ -142,7 +142,8 @@ object UpdatingPlanChecker {&lt;br/&gt;
             .map(_.name)&lt;br/&gt;
           // we have only a unique key if at least one window property is selected&lt;br/&gt;
           if (windowProperties.nonEmpty) &lt;/p&gt;
{
-            Some(groupKeys.map(e =&amp;gt; (e, e)) ++ windowProperties.map(e =&amp;gt; (e, e)))
+            val windowId = windowProperties.min
+            Some(groupKeys.map(e =&amp;gt; (e, e)) ++ windowProperties.map(e =&amp;gt; (e, windowId)))
           }
&lt;p&gt; else &lt;/p&gt;
{
             None
           }
&lt;p&gt;diff --git a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/InsertIntoITCase.scala b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/InsertIntoITCase.scala&lt;br/&gt;
new file mode 100644&lt;br/&gt;
index 00000000000..efba0265d3c&lt;br/&gt;
&amp;#8212; /dev/null&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/InsertIntoITCase.scala&lt;br/&gt;
@@ -0,0 +1,406 @@&lt;br/&gt;
+/*&lt;br/&gt;
+ * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
+ * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
+ * distributed with this work for additional information&lt;br/&gt;
+ * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
+ * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
+ * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
+ * with the License.  You may obtain a copy of the License at&lt;br/&gt;
+ *&lt;br/&gt;
+ *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
+ *&lt;br/&gt;
+ * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
+ * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
+ * See the License for the specific language governing permissions and&lt;br/&gt;
+ * limitations under the License.&lt;br/&gt;
+ */&lt;br/&gt;
+&lt;br/&gt;
+package org.apache.flink.table.runtime.stream.sql&lt;br/&gt;
+&lt;br/&gt;
+import org.apache.flink.api.common.typeinfo.TypeInformation&lt;br/&gt;
+import org.apache.flink.streaming.api.TimeCharacteristic&lt;br/&gt;
+import org.apache.flink.streaming.api.scala.StreamExecutionEnvironment&lt;br/&gt;
+import org.apache.flink.table.api.scala._&lt;br/&gt;
+import org.apache.flink.table.api.&lt;/p&gt;
{TableEnvironment, Types}
&lt;p&gt;+import org.apache.flink.table.runtime.stream.table.&lt;/p&gt;
{RowCollector, TestRetractSink, TestUpsertSink}
&lt;p&gt;+import org.apache.flink.table.runtime.utils.&lt;/p&gt;
{StreamTestData, StreamingWithStateTestBase}
&lt;p&gt;+import org.apache.flink.table.utils.MemoryTableSourceSinkUtil&lt;br/&gt;
+import org.apache.flink.test.util.TestBaseUtils&lt;br/&gt;
+import org.junit.Assert._&lt;br/&gt;
+import org.junit.Test&lt;br/&gt;
+&lt;br/&gt;
+import scala.collection.JavaConverters._&lt;br/&gt;
+&lt;br/&gt;
+class InsertIntoITCase extends StreamingWithStateTestBase {&lt;br/&gt;
+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendStreamToTableSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+    MemoryTableSourceSinkUtil.clear()
+
+    val input = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(r =&amp;gt; r._2)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, input, &apos;a, &apos;b, &apos;c, &apos;t.rowtime)
+
+    val fieldNames = Array(&quot;d&quot;, &quot;e&quot;, &quot;t&quot;)
+    val fieldTypes: Array[TypeInformation[_]] = Array(Types.STRING, Types.SQL_TIMESTAMP, Types.LONG)
+    val sink = new MemoryTableSourceSinkUtil.UnsafeMemoryAppendTableSink
+
+    tEnv.registerTableSink(&quot;targetTable&quot;, fieldNames, fieldTypes, sink)
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT c, t, b
+         |FROM sourceTable
+         |WHERE a &amp;lt; 3 OR a &amp;gt; 19
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+
+    val expected = Seq(
+      &quot;Hi,1970-01-01 00:00:00.001,1&quot;,
+      &quot;Hello,1970-01-01 00:00:00.002,2&quot;,
+      &quot;Comment#14,1970-01-01 00:00:00.006,6&quot;,
+      &quot;Comment#15,1970-01-01 00:00:00.006,6&quot;).mkString(&quot;\n&quot;)
+
+    TestBaseUtils.compareResultAsText(MemoryTableSourceSinkUtil.tableData.asJava, expected)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoUpdatingTableToRetractSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+
+    val t = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(_._1.toLong)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text)
+    tEnv.registerTableSink(
+      &quot;targetTable&quot;,
+      Array(&quot;len&quot;, &quot;cntid&quot;, &quot;sumnum&quot;),
+      Array(Types.INT, Types.LONG, Types.LONG),
+      new TestRetractSink)
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT len, COUNT(id) AS cntid, SUM(num) AS sumnum
+         |FROM (SELECT id, num, CHAR_LENGTH(text) AS len FROM sourceTable)
+         |GROUP BY len
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+    val results = RowCollector.getAndClearValues
+
+    val retracted = RowCollector.retractResults(results).sorted
+    val expected = List(
+      &quot;2,1,1&quot;,
+      &quot;5,1,2&quot;,
+      &quot;11,1,2&quot;,
+      &quot;25,1,3&quot;,
+      &quot;10,7,39&quot;,
+      &quot;14,1,3&quot;,
+      &quot;9,9,41&quot;).sorted
+    assertEquals(expected, retracted)
+
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendTableToRetractSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+
+    val t = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(_._1.toLong)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text, &apos;rowtime.rowtime)
+    tEnv.registerTableSink(
+      &quot;targetTable&quot;,
+      Array(&quot;wend&quot;, &quot;cntid&quot;, &quot;sumnum&quot;),
+      Array(Types.SQL_TIMESTAMP, Types.LONG, Types.LONG),
+      new TestRetractSink
+    )
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT
+         |  TUMBLE_END(rowtime, INTERVAL &apos;0.005&apos; SECOND) AS wend,
+         |  COUNT(id) AS cntid,
+         |  SUM(num) AS sumnum
+         |FROM sourceTable
+         |GROUP BY TUMBLE(rowtime, INTERVAL &apos;0.005&apos; SECOND)
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+    val results = RowCollector.getAndClearValues
+
+    assertFalse(
+      &quot;Received retraction messages for append only table&quot;,
+      results.exists(!_.f0))
+
+    val retracted = RowCollector.retractResults(results).sorted
+    val expected = List(
+      &quot;1970-01-01 00:00:00.005,4,8&quot;,
+      &quot;1970-01-01 00:00:00.01,5,18&quot;,
+      &quot;1970-01-01 00:00:00.015,5,24&quot;,
+      &quot;1970-01-01 00:00:00.02,5,29&quot;,
+      &quot;1970-01-01 00:00:00.025,2,12&quot;)
+      .sorted
+    assertEquals(expected, retracted)
+
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoUpdatingTableWithFullKeyToUpsertSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+
+    val t = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(_._1.toLong)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text)
+    tEnv.registerTableSink(
+      &quot;targetTable&quot;,
+      Array(&quot;cnt&quot;, &quot;cntid&quot;, &quot;cTrue&quot;),
+      Array(Types.LONG, Types.LONG, Types.BOOLEAN),
+      new TestUpsertSink(Array(&quot;cnt&quot;, &quot;cTrue&quot;), false)
+    )
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT cnt, COUNT(len) AS cntid, cTrue
+         |FROM
+         |  (SELECT CHAR_LENGTH(text) AS len, (id &amp;gt; 0) AS cTrue, COUNT(id) AS cnt
+         |   FROM sourceTable
+         |   GROUP BY CHAR_LENGTH(text), (id &amp;gt; 0)
+         |   )
+         |GROUP BY cnt, cTrue
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+    val results = RowCollector.getAndClearValues
+
+    assertTrue(
+      &quot;Results must include delete messages&quot;,
+      results.exists(_.f0 == false)
+    )
+
+    val retracted = RowCollector.upsertResults(results, Array(0, 2)).sorted
+    val expected = List(
+      &quot;1,5,true&quot;,
+      &quot;7,1,true&quot;,
+      &quot;9,1,true&quot;).sorted
+    assertEquals(expected, retracted)
+
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendingTableWithFullKey1ToUpsertSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+
+    val t = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(_._1.toLong)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text, &apos;rowtime.rowtime)
+    tEnv.registerTableSink(
+      &quot;targetTable&quot;,
+      Array(&quot;num&quot;, &quot;wend&quot;, &quot;cntid&quot;),
+      Array(Types.LONG, Types.SQL_TIMESTAMP, Types.LONG),
+      new TestUpsertSink(Array(&quot;wend&quot;, &quot;num&quot;), true)
+    )
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT
+         |  num,
+         |  TUMBLE_END(rowtime, INTERVAL &apos;0.005&apos; SECOND) AS wend,
+         |  COUNT(id) AS cntid
+         |FROM sourceTable
+         |GROUP BY TUMBLE(rowtime, INTERVAL &apos;0.005&apos; SECOND), num
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+    val results = RowCollector.getAndClearValues
+
+    assertFalse(
+      &quot;Received retraction messages for append only table&quot;,
+      results.exists(!_.f0))
+
+    val retracted = RowCollector.upsertResults(results, Array(0, 1)).sorted
+    val expected = List(
+      &quot;1,1970-01-01 00:00:00.005,1&quot;,
+      &quot;2,1970-01-01 00:00:00.005,2&quot;,
+      &quot;3,1970-01-01 00:00:00.005,1&quot;,
+      &quot;3,1970-01-01 00:00:00.01,2&quot;,
+      &quot;4,1970-01-01 00:00:00.01,3&quot;,
+      &quot;4,1970-01-01 00:00:00.015,1&quot;,
+      &quot;5,1970-01-01 00:00:00.015,4&quot;,
+      &quot;5,1970-01-01 00:00:00.02,1&quot;,
+      &quot;6,1970-01-01 00:00:00.02,4&quot;,
+      &quot;6,1970-01-01 00:00:00.025,2&quot;).sorted
+    assertEquals(expected, retracted)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendingTableWithFullKey2ToUpsertSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+
+    val t = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(_._1.toLong)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text, &apos;rowtime.rowtime)
+    tEnv.registerTableSink(
+      &quot;targetTable&quot;,
+      Array(&quot;wstart&quot;, &quot;wend&quot;, &quot;num&quot;, &quot;cntid&quot;),
+      Array(Types.SQL_TIMESTAMP, Types.SQL_TIMESTAMP, Types.LONG, Types.LONG),
+      new TestUpsertSink(Array(&quot;wstart&quot;, &quot;wend&quot;, &quot;num&quot;), true)
+    )
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT
+         |  TUMBLE_START(rowtime, INTERVAL &apos;0.005&apos; SECOND) AS wstart,
+         |  TUMBLE_END(rowtime, INTERVAL &apos;0.005&apos; SECOND) AS wend,
+         |  num,
+         |  COUNT(id) AS cntid
+         |FROM sourceTable
+         |GROUP BY TUMBLE(rowtime, INTERVAL &apos;0.005&apos; SECOND), num
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+    val results = RowCollector.getAndClearValues
+
+    assertFalse(
+      &quot;Received retraction messages for append only table&quot;,
+      results.exists(!_.f0))
+
+    val retracted = RowCollector.upsertResults(results, Array(0, 1, 2)).sorted
+    val expected = List(
+      &quot;1970-01-01 00:00:00.0,1970-01-01 00:00:00.005,1,1&quot;,
+      &quot;1970-01-01 00:00:00.0,1970-01-01 00:00:00.005,2,2&quot;,
+      &quot;1970-01-01 00:00:00.0,1970-01-01 00:00:00.005,3,1&quot;,
+      &quot;1970-01-01 00:00:00.005,1970-01-01 00:00:00.01,3,2&quot;,
+      &quot;1970-01-01 00:00:00.005,1970-01-01 00:00:00.01,4,3&quot;,
+      &quot;1970-01-01 00:00:00.01,1970-01-01 00:00:00.015,4,1&quot;,
+      &quot;1970-01-01 00:00:00.01,1970-01-01 00:00:00.015,5,4&quot;,
+      &quot;1970-01-01 00:00:00.015,1970-01-01 00:00:00.02,5,1&quot;,
+      &quot;1970-01-01 00:00:00.015,1970-01-01 00:00:00.02,6,4&quot;,
+      &quot;1970-01-01 00:00:00.02,1970-01-01 00:00:00.025,6,2&quot;).sorted
+    assertEquals(expected, retracted)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendingTableWithoutFullKey1ToUpsertSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+
+    val t = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(_._1.toLong)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text, &apos;rowtime.rowtime)
+    tEnv.registerTableSink(
+      &quot;targetTable&quot;,
+      Array(&quot;wend&quot;, &quot;cntid&quot;),
+      Array(Types.SQL_TIMESTAMP, Types.LONG),
+      new TestUpsertSink(null, true)
+    )
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT
+         |  TUMBLE_END(rowtime, INTERVAL &apos;0.005&apos; SECOND) AS wend,
+         |  COUNT(id) AS cntid
+         |FROM sourceTable
+         |GROUP BY TUMBLE(rowtime, INTERVAL &apos;0.005&apos; SECOND), num
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+    val results = RowCollector.getAndClearValues
+
+    assertFalse(
+      &quot;Received retraction messages for append only table&quot;,
+      results.exists(!_.f0))
+
+    val retracted = results.map(_.f1.toString).sorted
+    val expected = List(
+      &quot;1970-01-01 00:00:00.005,1&quot;,
+      &quot;1970-01-01 00:00:00.005,2&quot;,
+      &quot;1970-01-01 00:00:00.005,1&quot;,
+      &quot;1970-01-01 00:00:00.01,2&quot;,
+      &quot;1970-01-01 00:00:00.01,3&quot;,
+      &quot;1970-01-01 00:00:00.015,1&quot;,
+      &quot;1970-01-01 00:00:00.015,4&quot;,
+      &quot;1970-01-01 00:00:00.02,1&quot;,
+      &quot;1970-01-01 00:00:00.02,4&quot;,
+      &quot;1970-01-01 00:00:00.025,2&quot;).sorted
+    assertEquals(expected, retracted)
+  }
&lt;p&gt;+&lt;br/&gt;
+  @Test&lt;br/&gt;
+  def testInsertIntoAppendingTableWithoutFullKey2ToUpsertSink(): Unit = &lt;/p&gt;
{
+    val env = StreamExecutionEnvironment.getExecutionEnvironment
+    env.getConfig.enableObjectReuse()
+    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
+    val tEnv = TableEnvironment.getTableEnvironment(env)
+
+    val t = StreamTestData.get3TupleDataStream(env)
+      .assignAscendingTimestamps(_._1.toLong)
+
+    tEnv.registerDataStream(&quot;sourceTable&quot;, t, &apos;id, &apos;num, &apos;text, &apos;rowtime.rowtime)
+    tEnv.registerTableSink(
+      &quot;targetTable&quot;,
+      Array(&quot;num&quot;, &quot;cntid&quot;),
+      Array(Types.LONG, Types.LONG),
+      new TestUpsertSink(null, true)
+    )
+
+    tEnv.sqlUpdate(
+      s&quot;&quot;&quot;INSERT INTO targetTable
+         |SELECT
+         |  num,
+         |  COUNT(id) AS cntid
+         |FROM sourceTable
+         |GROUP BY TUMBLE(rowtime, INTERVAL &apos;0.005&apos; SECOND), num
+       &quot;&quot;&quot;.stripMargin)
+
+    env.execute()
+    val results = RowCollector.getAndClearValues
+
+    assertFalse(
+      &quot;Received retraction messages for append only table&quot;,
+      results.exists(!_.f0))
+
+    val retracted = results.map(_.f1.toString).sorted
+    val expected = List(
+      &quot;1,1&quot;,
+      &quot;2,2&quot;,
+      &quot;3,1&quot;,
+      &quot;3,2&quot;,
+      &quot;4,3&quot;,
+      &quot;4,1&quot;,
+      &quot;5,4&quot;,
+      &quot;5,1&quot;,
+      &quot;6,4&quot;,
+      &quot;6,2&quot;).sorted
+    assertEquals(expected, retracted)
+  }
&lt;p&gt;+}&lt;br/&gt;
diff --git a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/SqlITCase.scala b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/SqlITCase.scala&lt;br/&gt;
index a2d9bb26c4c..51bea2cdf20 100644&lt;br/&gt;
&amp;#8212; a/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/SqlITCase.scala&lt;br/&gt;
+++ b/flink-libraries/flink-table/src/test/scala/org/apache/flink/table/runtime/stream/sql/SqlITCase.scala&lt;br/&gt;
@@ -713,35 +713,6 @@ class SqlITCase extends StreamingWithStateTestBase &lt;/p&gt;
{
     assertEquals(expected.sorted, StreamITCase.testResults.sorted)
   }

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;@Test&lt;/li&gt;
	&lt;li&gt;def testInsertIntoMemoryTable(): Unit = 
{
-    val env = StreamExecutionEnvironment.getExecutionEnvironment
-    env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)
-    val tEnv = TableEnvironment.getTableEnvironment(env)
-    MemoryTableSourceSinkUtil.clear()
-
-    val t = StreamTestData.getSmall3TupleDataStream(env)
-        .assignAscendingTimestamps(x =&amp;gt; x._2)
-      .toTable(tEnv, &apos;a, &apos;b, &apos;c, &apos;rowtime.rowtime)
-    tEnv.registerTable(&quot;sourceTable&quot;, t)
-
-    val fieldNames = Array(&quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;t&quot;)
-    val fieldTypes = Array(Types.INT, Types.LONG, Types.STRING, Types.SQL_TIMESTAMP)
-      .asInstanceOf[Array[TypeInformation[_]]]
-    val sink = new MemoryTableSourceSinkUtil.UnsafeMemoryAppendTableSink
-    tEnv.registerTableSink(&quot;targetTable&quot;, fieldNames, fieldTypes, sink)
-
-    val sql = &quot;INSERT INTO targetTable SELECT a, b, c, rowtime FROM sourceTable&quot;
-    tEnv.sqlUpdate(sql)
-    env.execute()
-
-    val expected = List(
-      &quot;1,1,Hi,1970-01-01 00:00:00.001&quot;,
-      &quot;2,2,Hello,1970-01-01 00:00:00.002&quot;,
-      &quot;3,2,Hello world,1970-01-01 00:00:00.002&quot;)
-    assertEquals(expected.sorted, MemoryTableSourceSinkUtil.tableDataStrings.sorted)
-  }
&lt;p&gt;-&lt;br/&gt;
   @Test&lt;br/&gt;
   def testWriteReadTableSourceSink(): Unit = {&lt;br/&gt;
     val env = StreamExecutionEnvironment.getExecutionEnvironment&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;





&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                            <comment id="16619867" author="fhueske" created="Tue, 18 Sep 2018 23:11:35 +0000"  >&lt;p&gt;Fixed for 1.5.5 with 5852a25d8d9a4defa70c31ae61231471dc484bc1&lt;br/&gt;
Fixed for 1.6.2 with 290d96e50f864468b6b2d2a3b741e0fbdaee5df0&lt;br/&gt;
Fixed for 1.7.0 with f28b82909c3c6bcbe0436cae41af9a3c001f1c36&lt;/p&gt;</comment>
                            <comment id="16655330" author="fhueske" created="Thu, 18 Oct 2018 14:36:08 +0000"  >&lt;p&gt;Not sure why I reopened this issue. I guess that was accidentally. Otherwise, I&apos;d have left a comment.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 4 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3xl9r:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>