<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 21:22:21 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-37691] Can&apos;t consume changelog stream with upsert-kafka connector</title>
                <link>https://issues.apache.org/jira/browse/FLINK-37691</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;I read from a Kafka topic with data change events using the Kafka SQL connector using changelog semantics and write those events to another Kafka topic using the Upsert Kafka SQL connector. This works as expected with Flink 1.20 and 2.0.0 (the Debezium events on the source topic are emitted as flat upsert-style records on the sink topic), but fails as of 2.1-SNAPSHOT:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
Exception in thread &lt;span class=&quot;code-quote&quot;&gt;&quot;main&quot;&lt;/span&gt; java.lang.UnsupportedOperationException: Unsupported to visit node StreamPhysicalDropUpdateBefore. The node either should not be pushed through the changelog normalize or is not supported yet.
	at org.apache.flink.table.planner.plan.optimize.ChangelogNormalizeRequirementResolver.visit(ChangelogNormalizeRequirementResolver.java:119)
	at org.apache.flink.table.planner.plan.optimize.ChangelogNormalizeRequirementResolver.visit(ChangelogNormalizeRequirementResolver.java:90)
	at org.apache.flink.table.planner.plan.optimize.ChangelogNormalizeRequirementResolver.isRequired(ChangelogNormalizeRequirementResolver.java:74)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyDeleteKindTraitVisitor.visit(FlinkChangelogModeInferenceProgram.scala:1164)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyDeleteKindTraitVisitor.$anonfun$visitChildren$4(FlinkChangelogModeInferenceProgram.scala:1208)
	at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableLike.map(TraversableLike.scala:286)
	at scala.collection.TraversableLike.map$(TraversableLike.scala:279)
	at scala.collection.AbstractTraversable.map(Traversable.scala:108)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyDeleteKindTraitVisitor.visitChildren(FlinkChangelogModeInferenceProgram.scala:1207)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyDeleteKindTraitVisitor.$anonfun$visitSink$2(FlinkChangelogModeInferenceProgram.scala:1253)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyDeleteKindTraitVisitor.visitSink(FlinkChangelogModeInferenceProgram.scala:1253)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram$SatisfyDeleteKindTraitVisitor.visit(FlinkChangelogModeInferenceProgram.scala:1031)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.$anonfun$optimize$2(FlinkChangelogModeInferenceProgram.scala:103)
	at scala.collection.immutable.List.flatMap(List.scala:366)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.optimize(FlinkChangelogModeInferenceProgram.scala:101)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChangelogModeInferenceProgram.optimize(FlinkChangelogModeInferenceProgram.scala:47)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$2(FlinkGroupProgram.scala:59)
	at scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)
	at scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$1(FlinkGroupProgram.scala:56)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.$anonfun$optimize$1$adapted(FlinkGroupProgram.scala:51)
	at scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)
	at scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)
	at scala.collection.immutable.Range.foreach(Range.scala:158)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkGroupProgram.optimize(FlinkGroupProgram.scala:51)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.$anonfun$optimize$1(FlinkChainedProgram.scala:59)
	at scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:196)
	at scala.collection.TraversableOnce$folder$1.apply(TraversableOnce.scala:194)
	at scala.collection.Iterator.foreach(Iterator.scala:943)
	at scala.collection.Iterator.foreach$(Iterator.scala:943)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)
	at scala.collection.IterableLike.foreach(IterableLike.scala:74)
	at scala.collection.IterableLike.foreach$(IterableLike.scala:73)
	at scala.collection.AbstractIterable.foreach(Iterable.scala:56)
	at scala.collection.TraversableOnce.foldLeft(TraversableOnce.scala:199)
	at scala.collection.TraversableOnce.foldLeft$(TraversableOnce.scala:192)
	at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:108)
	at org.apache.flink.table.planner.plan.optimize.program.FlinkChainedProgram.optimize(FlinkChainedProgram.scala:55)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeTree(StreamCommonSubGraphBasedOptimizer.scala:196)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.optimizeSinkBlocks(StreamCommonSubGraphBasedOptimizer.scala:83)
	at org.apache.flink.table.planner.plan.optimize.StreamCommonSubGraphBasedOptimizer.doOptimize(StreamCommonSubGraphBasedOptimizer.scala:118)
	at org.apache.flink.table.planner.plan.optimize.CommonSubGraphBasedOptimizer.optimize(CommonSubGraphBasedOptimizer.scala:87)
	at org.apache.flink.table.planner.delegation.PlannerBase.optimize(PlannerBase.scala:395)
	at org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:183)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1373)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:951)
	at org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:1181)
	at org.apache.flink.table.api.internal.TablePipelineImpl.execute(TablePipelineImpl.java:59)
	at dev.morling.demos.cdcingest.KafkaChangelogToUpsertJob.main(KafkaChangelogToUpsertJob.java:59)
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here&apos;s my job definition:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;class &lt;/span&gt;KafkaChangelogToUpsertJob {

	&lt;span class=&quot;code-keyword&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;static&lt;/span&gt; void main(&lt;span class=&quot;code-object&quot;&gt;String&lt;/span&gt;[] args) {
		Configuration configuration = Configuration.fromMap(Map.of(&lt;span class=&quot;code-quote&quot;&gt;&quot;table.exec.source.cdc-events-duplicate&quot;&lt;/span&gt;, &lt;span class=&quot;code-quote&quot;&gt;&quot;&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;&quot;&lt;/span&gt;));

		EnvironmentSettings settings = EnvironmentSettings
				.newInstance()
				.inStreamingMode()
				.withConfiguration(configuration)
				.build();

		TableEnvironment tableEnv = TableEnvironment.create(settings);

		tableEnv.executeSql(&quot;&quot;&quot;
				CREATE TABLE authors_source (
					id BIGINT,
					first_name STRING,
					last_name STRING,
					biography STRING,
					registered BIGINT,
					PRIMARY KEY (id) NOT ENFORCED
				) WITH (
				  &lt;span class=&quot;code-quote&quot;&gt;&apos;connector&apos;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&apos;kafka&apos;&lt;/span&gt;,
				  &lt;span class=&quot;code-quote&quot;&gt;&apos;topic&apos;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&apos;dbserver1.inventory.authors&apos;&lt;/span&gt;,
				  &lt;span class=&quot;code-quote&quot;&gt;&apos;properties.bootstrap.servers&apos;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&apos;localhost:9092&apos;&lt;/span&gt;,
				  &lt;span class=&quot;code-quote&quot;&gt;&apos;scan.startup.mode&apos;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&apos;earliest-offset&apos;&lt;/span&gt;,
				  &lt;span class=&quot;code-quote&quot;&gt;&apos;value.format&apos;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&apos;debezium-json&apos;&lt;/span&gt;

				);
				&quot;&quot;&quot;);

		tableEnv.executeSql(&quot;&quot;&quot;
				CREATE TABLE authors_sink (
					id BIGINT,
					first_name STRING,
					last_name STRING,
					biography STRING,
					registered BIGINT,
					PRIMARY KEY (id) NOT ENFORCED
				) WITH (
				  &lt;span class=&quot;code-quote&quot;&gt;&apos;connector&apos;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&apos;upsert-kafka&apos;&lt;/span&gt;,
				  &lt;span class=&quot;code-quote&quot;&gt;&apos;topic&apos;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&apos;authors_processed&apos;&lt;/span&gt;,
				  &lt;span class=&quot;code-quote&quot;&gt;&apos;properties.bootstrap.servers&apos;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&apos;localhost:9092&apos;&lt;/span&gt;,
				  &lt;span class=&quot;code-quote&quot;&gt;&apos;key.format&apos;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&apos;json&apos;&lt;/span&gt;,
				  &lt;span class=&quot;code-quote&quot;&gt;&apos;value.format&apos;&lt;/span&gt; = &lt;span class=&quot;code-quote&quot;&gt;&apos;json&apos;&lt;/span&gt;
				);
				&quot;&quot;&quot;);

		Table authors = tableEnv.sqlQuery(&lt;span class=&quot;code-quote&quot;&gt;&quot;SELECT id, first_name, last_name, biography, registered FROM authors_source&quot;&lt;/span&gt;);
		authors.insertInto(&lt;span class=&quot;code-quote&quot;&gt;&quot;authors_sink&quot;&lt;/span&gt;).execute();
		authors.execute().print();
	}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This might be a regression caused by &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-37475&quot; title=&quot;Drop ChangelogNormalize for piping from upsert source to sink&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-37475&quot;&gt;&lt;del&gt;FLINK-37475&lt;/del&gt;&lt;/a&gt;.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13615512">FLINK-37691</key>
            <summary>Can&apos;t consume changelog stream with upsert-kafka connector</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="3" iconUrl="https://issues.apache.org/jira/images/icons/priorities/major.svg">Major</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="dwysakowicz">Dawid Wysakowicz</assignee>
                                    <reporter username="gunnar.morling">Gunnar Morling</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Wed, 16 Apr 2025 12:57:11 +0000</created>
                <updated>Wed, 23 Apr 2025 14:34:31 +0000</updated>
                            <resolved>Wed, 23 Apr 2025 14:34:31 +0000</resolved>
                                    <version>2.1.0</version>
                                                    <component>Table SQL / Runtime</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="17946729" author="dawidwys" created="Wed, 23 Apr 2025 14:34:31 +0000"  >&lt;p&gt;Fixed in 4b6ea1eecd152e85c969eef377135de623cdf4c6&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                                                <inwardlinks description="is caused by">
                                        <issuelink>
            <issuekey id="13611882">FLINK-37475</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            28 weeks, 6 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|z1vem0:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>