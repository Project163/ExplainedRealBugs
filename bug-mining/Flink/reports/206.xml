<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:18:10 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-1396] Add hadoop input formats directly to the user API.</title>
                <link>https://issues.apache.org/jira/browse/FLINK-1396</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description></description>
                <environment></environment>
        <key id="12767113">FLINK-1396</key>
            <summary>Add hadoop input formats directly to the user API.</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="4" iconUrl="https://issues.apache.org/jira/images/icons/priorities/minor.svg">Minor</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="aljoscha">Aljoscha Krettek</assignee>
                                    <reporter username="rmetzger">Robert Metzger</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Tue, 13 Jan 2015 12:57:26 +0000</created>
                <updated>Thu, 7 Nov 2019 03:28:25 +0000</updated>
                            <resolved>Mon, 9 Feb 2015 13:44:59 +0000</resolved>
                                                    <fixVersion>0.9</fixVersion>
                    <fixVersion>0.8.1</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>4</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="600">10m</timespent>
                                                        <aggregatetimeremainingestimate seconds="0">0h</aggregatetimeremainingestimate>
                                        <aggregatetimespent seconds="600">10m</aggregatetimespent>
                                    <comments>
                            <comment id="14293458" author="aljoscha" created="Tue, 27 Jan 2015 12:57:12 +0000"  >&lt;p&gt;For this to work I have to move the Hadoop Formats from addons to the java (resp. scala) packages. Should I move the input formats and leave the rest intact, or should I duplicate the input formats and leave the addons package as it is?&lt;/p&gt;

&lt;p&gt;Also, I should probably add direct methods for both the old and the new API.&lt;/p&gt;

&lt;p&gt;What are your thoughts on this?&lt;/p&gt;</comment>
                            <comment id="14294517" author="fhueske" created="Wed, 28 Jan 2015 00:54:34 +0000"  >&lt;p&gt;+1 for having direct API methods for HadoopInputFormats. If you do it manually, its quite a few lines of ugly boilerplate code.&lt;br/&gt;
I am also +1 for moving the hadoop-compat code to flink-java. Are we talking about all wrappers or only IFs and OFs, btw? &lt;/p&gt;</comment>
                            <comment id="14305494" author="githubbot" created="Wed, 4 Feb 2015 16:54:32 +0000"  >&lt;p&gt;GitHub user aljoscha opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-1396&quot; title=&quot;Add hadoop input formats directly to the user API.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-1396&quot;&gt;&lt;del&gt;FLINK-1396&lt;/del&gt;&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-1303&quot; title=&quot;HadoopInputFormat does not work with Scala API&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-1303&quot;&gt;&lt;del&gt;FLINK-1303&lt;/del&gt;&lt;/a&gt; Hadoop Input/Output directly in API&lt;/p&gt;

&lt;p&gt;    This adds methods on ExecutionEnvironment for reading with Hadoop&lt;br/&gt;
    Input/OutputFormat.&lt;/p&gt;

&lt;p&gt;    This also adds support in the Scala API for Hadoop Input/OutputFormats.&lt;/p&gt;

&lt;p&gt;    I also added tests and updated the documentation.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/aljoscha/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/aljoscha/flink&lt;/a&gt; hadoop-in-api&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #363&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 94376ce914c740e9880bf161e90ae92a0ced39ed&lt;br/&gt;
Author: Aljoscha Krettek &amp;lt;aljoscha.krettek@gmail.com&amp;gt;&lt;br/&gt;
Date:   2015-01-28T14:13:30Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-1396&quot; title=&quot;Add hadoop input formats directly to the user API.&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-1396&quot;&gt;&lt;del&gt;FLINK-1396&lt;/del&gt;&lt;/a&gt;&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-1303&quot; title=&quot;HadoopInputFormat does not work with Scala API&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-1303&quot;&gt;&lt;del&gt;FLINK-1303&lt;/del&gt;&lt;/a&gt; Hadoop Input/Output directly in API&lt;/p&gt;

&lt;p&gt;    This adds methods on ExecutionEnvironment for reading with Hadoop&lt;br/&gt;
    Input/OutputFormat.&lt;/p&gt;

&lt;p&gt;    This also adds support in the Scala API for Hadoop Input/OutputFormats.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14305905" author="githubbot" created="Wed, 4 Feb 2015 20:43:32 +0000"  >&lt;p&gt;Github user fhueske commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#discussion_r24117603&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#discussion_r24117603&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/hadoop_compatibility.md &amp;#8212;&lt;br/&gt;
    @@ -38,9 +39,19 @@ This document shows how to use existing Hadoop MapReduce code with Flink. Please&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Project Configuration&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    -The Hadoop Compatibility Layer is part of the `flink-addons` Maven module. All relevant classes are located in the `org.apache.flink.hadoopcompatibility` package. It includes separate packages and classes for the Hadoop `mapred` and `mapreduce` APIs.&lt;br/&gt;
    +Support for Haddop input/output formats is part of the `flink-java` and&lt;br/&gt;
    +`flink-scala` Maven modules that are always required when writing Flink jobs.&lt;br/&gt;
    +The code is located in `org.apache.flink.api.java.hadoop` and&lt;br/&gt;
    +`org.apache.flink.api.scala.hadoop` in an additional sub-package for the&lt;br/&gt;
    +`mapred` and `mapreduce` API.&lt;/p&gt;

&lt;p&gt;    -Add the following dependency to your `pom.xml` to use the Hadoop Compatibility Layer.&lt;br/&gt;
    +Support for Hadoop Mappers and Reducers is contained in the `flink-addons`&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    `flink-staging` is the new `flink-addons` &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="14305910" author="githubbot" created="Wed, 4 Feb 2015 20:45:15 +0000"  >&lt;p&gt;Github user fhueske commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#discussion_r24117714&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#discussion_r24117714&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/hadoop_compatibility.md &amp;#8212;&lt;br/&gt;
    @@ -52,56 +63,70 @@ Add the following dependency to your `pom.xml` to use the Hadoop Compatibility L&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Using Hadoop Data Types&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    -Flink supports all Hadoop `Writable` and `WritableComparable` data types out-of-the-box. You do not need to include the Hadoop Compatibility dependency, if you only want to use your Hadoop data types. See the &lt;span class=&quot;error&quot;&gt;&amp;#91;Programming Guide&amp;#93;&lt;/span&gt;(programming_guide.html#data-types) for more details.&lt;br/&gt;
    +Flink supports all Hadoop `Writable` and `WritableComparable` data types&lt;br/&gt;
    +out-of-the-box. You do not need to include the Hadoop Compatibility dependency,&lt;br/&gt;
    +if you only want to use your Hadoop data types. See the&lt;br/&gt;
    +&lt;span class=&quot;error&quot;&gt;&amp;#91;Programming Guide&amp;#93;&lt;/span&gt;(programming_guide.html#data-types) for more details.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Using Hadoop InputFormats&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    -Flink provides a compatibility wrapper for Hadoop `InputFormats`. Any class that implements `org.apache.hadoop.mapred.InputFormat` or extends `org.apache.hadoop.mapreduce.InputFormat` is supported. Thus, Flink can handle Hadoop built-in formats such as `TextInputFormat` as well as external formats such as Hive&apos;s `HCatInputFormat`. Data read from Hadoop InputFormats is converted into a `DataSet&amp;lt;Tuple2&amp;lt;KEY,VALUE&amp;gt;&amp;gt;` where `KEY` is the key and `VALUE` is the value of the original Hadoop key-value pair.&lt;br/&gt;
    -&lt;br/&gt;
    -Flink&apos;s InputFormat wrappers are &lt;br/&gt;
    -&lt;br/&gt;
    &amp;#8211; `org.apache.flink.hadoopcompatibility.mapred.HadoopInputFormat` and &lt;br/&gt;
    &amp;#8211; `org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat`&lt;br/&gt;
    +Hadoop input formats can be used to create a data source by using&lt;br/&gt;
    +on of the methods `readHadoopFile` or `createHadoopInput` of the&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    on -&amp;gt; one&lt;/p&gt;</comment>
                            <comment id="14305914" author="githubbot" created="Wed, 4 Feb 2015 20:47:02 +0000"  >&lt;p&gt;Github user fhueske commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#discussion_r24117856&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#discussion_r24117856&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/hadoop_compatibility.md &amp;#8212;&lt;br/&gt;
    @@ -52,56 +63,70 @@ Add the following dependency to your `pom.xml` to use the Hadoop Compatibility L&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Using Hadoop Data Types&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    -Flink supports all Hadoop `Writable` and `WritableComparable` data types out-of-the-box. You do not need to include the Hadoop Compatibility dependency, if you only want to use your Hadoop data types. See the &lt;span class=&quot;error&quot;&gt;&amp;#91;Programming Guide&amp;#93;&lt;/span&gt;(programming_guide.html#data-types) for more details.&lt;br/&gt;
    +Flink supports all Hadoop `Writable` and `WritableComparable` data types&lt;br/&gt;
    +out-of-the-box. You do not need to include the Hadoop Compatibility dependency,&lt;br/&gt;
    +if you only want to use your Hadoop data types. See the&lt;br/&gt;
    +&lt;span class=&quot;error&quot;&gt;&amp;#91;Programming Guide&amp;#93;&lt;/span&gt;(programming_guide.html#data-types) for more details.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;Using Hadoop InputFormats&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    -Flink provides a compatibility wrapper for Hadoop `InputFormats`. Any class that implements `org.apache.hadoop.mapred.InputFormat` or extends `org.apache.hadoop.mapreduce.InputFormat` is supported. Thus, Flink can handle Hadoop built-in formats such as `TextInputFormat` as well as external formats such as Hive&apos;s `HCatInputFormat`. Data read from Hadoop InputFormats is converted into a `DataSet&amp;lt;Tuple2&amp;lt;KEY,VALUE&amp;gt;&amp;gt;` where `KEY` is the key and `VALUE` is the value of the original Hadoop key-value pair.&lt;br/&gt;
    -&lt;br/&gt;
    -Flink&apos;s InputFormat wrappers are &lt;br/&gt;
    -&lt;br/&gt;
    &amp;#8211; `org.apache.flink.hadoopcompatibility.mapred.HadoopInputFormat` and &lt;br/&gt;
    &amp;#8211; `org.apache.flink.hadoopcompatibility.mapreduce.HadoopInputFormat`&lt;br/&gt;
    +Hadoop input formats can be used to create a data source by using&lt;br/&gt;
    +on of the methods `readHadoopFile` or `createHadoopInput` of the&lt;br/&gt;
    +`ExecutionEnvironment`. The former is used for input formats derived&lt;br/&gt;
    +from `FileInputFormat` while the latter has to be used for general purpose&lt;br/&gt;
    +input formats.&lt;/p&gt;

&lt;p&gt;    -and can be used as regular Flink &lt;span class=&quot;error&quot;&gt;&amp;#91;InputFormats&amp;#93;&lt;/span&gt;(programming_guide.html#data-sources).&lt;br/&gt;
    +The resulting `DataSet` contains 2-tuples where the first field&lt;br/&gt;
    +is the key and the second field is the value retrieved from the Hadoop&lt;br/&gt;
    +InputFormat.&lt;/p&gt;

&lt;p&gt;     The following example shows how to use Hadoop&apos;s `TextInputFormat`.&lt;/p&gt;

&lt;p&gt;    +&amp;lt;div class=&quot;codetabs&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;java&quot; markdown=&quot;1&quot;&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
     ~~~java&lt;br/&gt;
     ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;&lt;p&gt;    -// Set up the Hadoop TextInputFormat.&lt;br/&gt;
    -Job job = Job.getInstance();&lt;br/&gt;
    -HadoopInputFormat&amp;lt;LongWritable, Text&amp;gt; hadoopIF = &lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;// create the Flink wrapper.&lt;/li&gt;
	&lt;li&gt;new HadoopInputFormat&amp;lt;LongWritable, Text&amp;gt;(&lt;/li&gt;
	&lt;li&gt;// create the Hadoop InputFormat, specify key and value type, and job.&lt;/li&gt;
	&lt;li&gt;new TextInputFormat(), LongWritable.class, Text.class, job&lt;/li&gt;
	&lt;li&gt;);&lt;br/&gt;
    -TextInputFormat.addInputPath(job, new Path(inputPath));&lt;/li&gt;
	&lt;li&gt;&lt;p&gt;    -// Read data using the Hadoop TextInputFormat.&lt;br/&gt;
    -DataSet&amp;lt;Tuple2&amp;lt;LongWritable, Text&amp;gt;&amp;gt; text = env.createInput(hadoopIF);&lt;br/&gt;
    +&lt;br/&gt;
    +DataSet&amp;lt;Tuple2&amp;lt;LongWritable, Text&amp;gt;&amp;gt; input =&lt;br/&gt;
    +    env.readHadoopFile(new TextInputFormat(), LongWritable.class, Text.class, textPath);&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     // Do something with the data.&lt;br/&gt;
     &lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt;&lt;br/&gt;
     ~~~&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;
		&lt;ol&gt;
			&lt;li&gt;
			&lt;ol&gt;
				&lt;li&gt;Using Hadoop OutputFormats&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&amp;lt;div data-lang=&quot;scala&quot; markdown=&quot;1&quot;&amp;gt;&lt;/li&gt;
			&lt;/ol&gt;
			&lt;/li&gt;
		&lt;/ol&gt;
		&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    -Flink provides a compatibility wrapper for Hadoop `OutputFormats`. Any class that implements `org.apache.hadoop.mapred.OutputFormat` or extends `org.apache.hadoop.mapreduce.OutputFormat` is supported. The OutputFormat wrapper expects its input data to be a `DataSet&amp;lt;Tuple2&amp;lt;KEY,VALUE&amp;gt;&amp;gt;` where `KEY` is the key and `VALUE` is the value of the Hadoop key-value pair that is processed by the Hadoop OutputFormat.&lt;br/&gt;
    +~~~scala&lt;br/&gt;
    +val env = ExecutionEnvironment.getExecutionEnvironment&lt;br/&gt;
    +		&lt;br/&gt;
    +val input: DataSet&lt;span class=&quot;error&quot;&gt;&amp;#91;(LongWritable, Text)&amp;#93;&lt;/span&gt; =&lt;br/&gt;
    +  env.readHadoopFile(new TextInputFormat, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;LongWritable&amp;#93;&lt;/span&gt;, classOf&lt;span class=&quot;error&quot;&gt;&amp;#91;Text&amp;#93;&lt;/span&gt;, textPath)&lt;/p&gt;

&lt;p&gt;    -Flink&apos;s OUtputFormat wrappers are&lt;br/&gt;
    +// Do something with the data.&lt;br/&gt;
    +&lt;span class=&quot;error&quot;&gt;&amp;#91;...&amp;#93;&lt;/span&gt;&lt;br/&gt;
    +~~~&lt;br/&gt;
    +&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8211; `org.apache.flink.hadoopcompatibility.mapred.HadoopOutputFormat` and &lt;br/&gt;
    &amp;#8211; `org.apache.flink.hadoopcompatibility.mapreduce.HadoopOutputFormat`&lt;br/&gt;
    +&amp;lt;/div&amp;gt;&lt;br/&gt;
    +&lt;br/&gt;
    +### Using Hadoop OutputFormats&lt;/p&gt;

&lt;p&gt;    -and can be used as regular Flink &lt;span class=&quot;error&quot;&gt;&amp;#91;OutputFormats&amp;#93;&lt;/span&gt;(programming_guide.html#data-sinks).&lt;br/&gt;
    +Flink provides a compatibility wrapper for Hadoop `OutputFormats`. Any class&lt;br/&gt;
    +that implements `org.apache.hadoop.mapred.OutputFormat` or extend&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    extend -&amp;gt; extends&lt;/p&gt;</comment>
                            <comment id="14305921" author="githubbot" created="Wed, 4 Feb 2015 20:50:56 +0000"  >&lt;p&gt;Github user fhueske commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#discussion_r24118164&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#discussion_r24118164&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-java/src/main/java/org/apache/flink/api/java/ExecutionEnvironment.java &amp;#8212;&lt;br/&gt;
    @@ -458,6 +461,67 @@ public CsvReader readCsvFile(String filePath) &lt;/p&gt;
{
     		
     		return new DataSource&amp;lt;X&amp;gt;(this, inputFormat, producedType, Utils.getCallLocationName());
     	}
&lt;p&gt;    +&lt;br/&gt;
    +	// ----------------------------------- Hadoop Input Format ---------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a &lt;/p&gt;
{@link DataSet} from the given {@link org.apache.hadoop.mapred.FileInputFormat}. The&lt;br/&gt;
    +	 * given inputName is set on the given job.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public &amp;lt;K,V&amp;gt; DataSource&amp;lt;Tuple2&amp;lt;K, V&amp;gt;&amp;gt; readHadoopFile(org.apache.hadoop.mapred.FileInputFormat&amp;lt;K,V&amp;gt; mapredInputFormat, Class&amp;lt;K&amp;gt; key, Class&amp;lt;V&amp;gt; value, String inputPath, JobConf job) {
    +		DataSource&amp;lt;Tuple2&amp;lt;K, V&amp;gt;&amp;gt; result = createHadoopInput(mapredInputFormat, key, value, job);
    +
    +		org.apache.hadoop.mapred.FileInputFormat.addInputPath(job, new org.apache.hadoop.fs.Path(inputPath));
    +
    +		return result;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a {@link DataSet}
&lt;p&gt; from the given &lt;/p&gt;
{@link org.apache.hadoop.mapred.FileInputFormat}
&lt;p&gt;. A&lt;br/&gt;
    +	 * &lt;/p&gt;
{@link org.apache.hadoop.mapred.JobConf}
&lt;p&gt; with the given inputPath is created.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public &amp;lt;K,V&amp;gt; DataSource&amp;lt;Tuple2&amp;lt;K, V&amp;gt;&amp;gt; readHadoopFile(org.apache.hadoop.mapred.FileInputFormat&amp;lt;K,V&amp;gt; mapredInputFormat, Class&amp;lt;K&amp;gt; key, Class&amp;lt;V&amp;gt; value, String inputPath) &lt;/p&gt;
{
    +		return readHadoopFile(mapredInputFormat, key, value, inputPath, new JobConf());
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a &lt;/p&gt;
{@link DataSet}
&lt;p&gt; from the given &lt;/p&gt;
{@link org.apache.hadoop.mapred.InputFormat}
&lt;p&gt;.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public &amp;lt;K,V&amp;gt; DataSource&amp;lt;Tuple2&amp;lt;K, V&amp;gt;&amp;gt; createHadoopInput(org.apache.hadoop.mapred.InputFormat&amp;lt;K,V&amp;gt; mapredInputFormat, Class&amp;lt;K&amp;gt; key, Class&amp;lt;V&amp;gt; value, JobConf job) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Why no &quot;shortcut&quot; without `JobConf`?&lt;/p&gt;</comment>
                            <comment id="14305956" author="githubbot" created="Wed, 4 Feb 2015 21:06:33 +0000"  >&lt;p&gt;Github user fhueske commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#issuecomment-72939364&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#issuecomment-72939364&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Looks good. &lt;br/&gt;
    Besides the typos and inline comments, you could also move the `HadoopInputFormatTest` and the `HadoopIOFormatsITCase` to flink-java and flink-tests. &lt;/p&gt;

&lt;p&gt;    We could even integrate HadoopIFs even more, if we overload the &quot;regular&quot; Flink input functions `ExecutionEnvironment.readFile()` and `ExecutionEnvironment.createInput()` instead of using &quot;special&quot; HadoopIF functions. What do you think?&lt;/p&gt;</comment>
                            <comment id="14306878" author="githubbot" created="Thu, 5 Feb 2015 09:02:39 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#issuecomment-73014361&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#issuecomment-73014361&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Looks good.&lt;/p&gt;

&lt;p&gt;    I have one suggestion concerning the Hadoop dependencies: The `flink-java` project depends on the Hadoop API, for the `Writable` interface, the `NullValue` and the InputFormat classes. &lt;/p&gt;

&lt;p&gt;    We should be able to exclude all transitive dependencies from the Hadoop dependency in `flink-java`, making the project more lightweight, so that someone that only writes against the flink API does not have the long tail of transitive hadoop dependencies.&lt;/p&gt;

&lt;p&gt;    Whenever we really execute Hadoop code, we have the flink runtime involved, which then has the necessary dependencies.&lt;/p&gt;

&lt;p&gt;    To exclude all transitive dependencies, use&lt;br/&gt;
    ```&lt;br/&gt;
    &amp;lt;exclusions&amp;gt;&lt;br/&gt;
      &amp;lt;exclusion&amp;gt;&lt;br/&gt;
        &amp;lt;groupId&amp;gt;*&amp;lt;/groupId&amp;gt;&lt;br/&gt;
        &amp;lt;artifactId&amp;gt;*&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
      &amp;lt;/exclusion&amp;gt;&lt;br/&gt;
    &amp;lt;/exclusions&amp;gt;&lt;br/&gt;
    ```&lt;/p&gt;
</comment>
                            <comment id="14306928" author="githubbot" created="Thu, 5 Feb 2015 09:51:19 +0000"  >&lt;p&gt;Github user aljoscha commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#issuecomment-73020223&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#issuecomment-73020223&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I addressed the comments. What do the others think about overloading readFile()? I made it like this on purpose. So that the user sees in the API that they are using Hadoop input formats or that they can be used.&lt;/p&gt;</comment>
                            <comment id="14306943" author="githubbot" created="Thu, 5 Feb 2015 10:09:33 +0000"  >&lt;p&gt;Github user fhueske commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#issuecomment-73022621&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#issuecomment-73022621&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hmm, yes. That&apos;s also a valid point. &lt;br/&gt;
    But on the other hand, new users might not even be aware of the different types of InputFormats. It all would look &quot;natural&quot; &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;

&lt;p&gt;    I am more leaning towards overloading, but would be fine with having separate functions as well.&lt;/p&gt;</comment>
                            <comment id="14306959" author="githubbot" created="Thu, 5 Feb 2015 10:21:55 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#issuecomment-73024197&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#issuecomment-73024197&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I vote for keeping @aljoscha&apos;s original approach.&lt;br/&gt;
    Users might not notice the different interfaces there, so the &quot;Hadoop&quot; in the method name makes it more explicit.&lt;br/&gt;
    Also, it could lead to confusions because Flink&apos;s and Hadoop&apos;s InputFormats have pretty similar names (they actually only differ in the package names).&lt;br/&gt;
    Lastly, it would cause some work on Aljoscha&apos;s side to update the code and the documentation.&lt;/p&gt;</comment>
                            <comment id="14307147" author="githubbot" created="Thu, 5 Feb 2015 12:41:23 +0000"  >&lt;p&gt;Github user aljoscha commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#issuecomment-73039917&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#issuecomment-73039917&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @StephanEwen  If I add the exclusions then users that just add flink-java as a dependency will get weird errors when using Hadoop InputFormats. &lt;/p&gt;
</comment>
                            <comment id="14307154" author="githubbot" created="Thu, 5 Feb 2015 12:43:45 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#issuecomment-73040193&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#issuecomment-73040193&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Does this occur during local execution, or collection execution? The dependencies are not covered by the runtime dependencies?&lt;/p&gt;</comment>
                            <comment id="14307162" author="githubbot" created="Thu, 5 Feb 2015 12:49:03 +0000"  >&lt;p&gt;Github user aljoscha commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#issuecomment-73040767&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#issuecomment-73040767&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I think if executing it in an IDE the dependencies are not there. Since flink-java does not depend on flink-runtime, which has the hadoop dependencies.&lt;/p&gt;</comment>
                            <comment id="14311965" author="githubbot" created="Mon, 9 Feb 2015 09:22:23 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363#issuecomment-73478491&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363#issuecomment-73478491&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Looks good. We are getting into very long package names here &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;br/&gt;
    `org.apache.flink.api.java.hadoop.mapred.wrapper.*`&lt;/p&gt;</comment>
                            <comment id="14312233" author="githubbot" created="Mon, 9 Feb 2015 13:42:42 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/363&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/363&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14312237" author="aljoscha" created="Mon, 9 Feb 2015 13:44:59 +0000"  >&lt;p&gt;Resolved in &lt;a href=&quot;https://github.com/apache/flink/commit/8b3805ba5905c3d84f3e0631bc6090a618df8e90&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/commit/8b3805ba5905c3d84f3e0631bc6090a618df8e90&lt;/a&gt;&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                            <subtask id="12759705">FLINK-1303</subtask>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 41 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i24b0n:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>