<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:24:38 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-4094] Off heap memory deallocation might not properly work</title>
                <link>https://issues.apache.org/jira/browse/FLINK-4094</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;A user reported that off-heap memory is not properly deallocated when setting &lt;tt&gt;taskmanager.memory.preallocate:false&lt;/tt&gt; (per default) &lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt;. This can cause the TaskManager process being killed by the OS.&lt;/p&gt;

&lt;p&gt;It should be possible to execute multiple batch jobs with preallocation turned off. No longer used direct memory buffers should be properly garbage collected so that the JVM process does not exceed it&apos;s maximum memory bounds.&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;1&amp;#93;&lt;/span&gt; &lt;a href=&quot;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/offheap-memory-allocation-and-memory-leak-bug-td12154.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-flink-mailing-list-archive.1008284.n3.nabble.com/offheap-memory-allocation-and-memory-leak-bug-td12154.html&lt;/a&gt;&lt;/p&gt;</description>
                <environment></environment>
        <key id="12980800">FLINK-4094</key>
            <summary>Off heap memory deallocation might not properly work</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="ram_krish">ramkrishna.s.vasudevan</assignee>
                                    <reporter username="trohrmann">Till Rohrmann</reporter>
                        <labels>
                    </labels>
                <created>Mon, 20 Jun 2016 10:30:54 +0000</created>
                <updated>Wed, 2 Oct 2019 17:42:24 +0000</updated>
                            <resolved>Mon, 24 Apr 2017 07:36:39 +0000</resolved>
                                    <version>1.1.0</version>
                                                    <component>Runtime / Task</component>
                        <due></due>
                            <votes>2</votes>
                                    <watches>10</watches>
                                                                                                                <comments>
                            <comment id="15339331" author="ram_krish" created="Mon, 20 Jun 2016 10:50:57 +0000"  >&lt;p&gt;Offheap direct memory cannot be cleared unless a full GC happens. If the allocated memory happens to be in the active working set always (like this preallocate does) then there is always the chance of reusing the existing allocated direct memory and things should be fine. &lt;/p&gt;</comment>
                            <comment id="15339332" author="ram_krish" created="Mon, 20 Jun 2016 10:51:29 +0000"  >&lt;p&gt;So if the direct memory cannot be reused its better onheap buffers are created so that it is under the disposal of java gc.&lt;/p&gt;</comment>
                            <comment id="15340447" author="capacman" created="Mon, 20 Jun 2016 21:34:27 +0000"  >&lt;p&gt;&lt;a href=&quot;https://github.com/capacman/flinkoffheaptest&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/capacman/flinkoffheaptest&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Above github repository contains batch job and example data i used to test the issue. I also put my flink conf yaml as well. In my tests i used jvm 1.8.0_66 on ubuntu 16.04 with flink 1.0.3. Also if you use taskmanager.sh file as is(leaving MaxDirectMemorySize=8388607T) you will spot the problem more easily. In my tests i also checked whether limiting -XX:MaxDirectMemorySize to intended memory usage helps or not. Usually it defer the problem a little bit since ,as i understand, hitting limit of MaxDirectMemorySize can cause full GC and as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ram_krish&lt;/a&gt; said it clear some memory. Usually problem become more visible when you repeat test with a machine which have more than 24 GB memory.&lt;/p&gt;</comment>
                            <comment id="15397634" author="ram_krish" created="Thu, 28 Jul 2016 14:47:55 +0000"  >&lt;p&gt;Can I work on this? I saw the code - if preallocation is false and the memory type is OFFHEAP we should not be allowing that config. May be we can even disallow that combination or if OFFHEAP is the type we will always allow preallocation. Or add our own internal offheap buffer management that is not exposed to user? May be that is redundant.&lt;br/&gt;
Not allowing preallocation and creating offheap buffers is very dangerous.&lt;/p&gt;</comment>
                            <comment id="15399038" author="mxm" created="Fri, 29 Jul 2016 09:35:03 +0000"  >&lt;p&gt;I don&apos;t think just disallowing preallocation:false is a good fix. We should rather fix the underlying issue with the memory management and freeing of memory.&lt;/p&gt;

&lt;p&gt;I can&apos;t asses the problem at the moment. As far as I know &apos;preallocation: false&apos; never releases memory. It only acquires memory dynamically and returns it to the memory manager if it is not needed anymore. Only when shutting down the TaskManager the MemoryManager releases the acquired memory.&lt;/p&gt;

&lt;p&gt;Please correct me if I&apos;m wrong.&lt;/p&gt;</comment>
                            <comment id="15399085" author="ram_krish" created="Fri, 29 Jul 2016 10:07:30 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=mxm&quot; class=&quot;user-hover&quot; rel=&quot;mxm&quot;&gt;mxm&lt;/a&gt;&lt;br/&gt;
Thanks for the comment. My comment was bit vague as I  added them while on travel.&lt;br/&gt;
Let me try to explain based on what I see in code. If am missing something or wrong, pls do correct me.&lt;br/&gt;
The MemoryManager manages both Heap and offheap memory segment. &lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;		@Override
		HybridMemorySegment allocateNewSegment(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; owner) {
			ByteBuffer memory = ByteBuffer.allocateDirect(segmentSize);
			&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; HybridMemorySegment.FACTORY.wrapPooledOffHeapMemory(memory, owner);
		}

		@Override
		HybridMemorySegment requestSegmentFromPool(&lt;span class=&quot;code-object&quot;&gt;Object&lt;/span&gt; owner) {
			ByteBuffer buf = availableMemory.remove();
			&lt;span class=&quot;code-keyword&quot;&gt;return&lt;/span&gt; HybridMemorySegment.FACTORY.wrapPooledOffHeapMemory(buf, owner);
		}

		@Override
		void returnSegmentToPool(MemorySegment segment) {
			&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (segment.getClass() == HybridMemorySegment.class) {
				HybridMemorySegment hybridSegment = (HybridMemorySegment) segment;
				ByteBuffer buf = hybridSegment.getOffHeapBuffer();
				availableMemory.add(buf);
				hybridSegment.free();
			}
			&lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
				&lt;span class=&quot;code-keyword&quot;&gt;throw&lt;/span&gt; &lt;span class=&quot;code-keyword&quot;&gt;new&lt;/span&gt; IllegalArgumentException(&lt;span class=&quot;code-quote&quot;&gt;&quot;Memory segment is not a &quot;&lt;/span&gt; + HeapMemorySegment.&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;getSimpleName());
			}
		}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;If you see the usage of the above APIs&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;			&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (isPreAllocated) {
				&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = numPages; i &amp;gt; 0; i--) {
					MemorySegment segment = memoryPool.requestSegmentFromPool(owner);
					target.add(segment);
					segmentsForOwner.add(segment);
				}
			}
			&lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
				&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = numPages; i &amp;gt; 0; i--) {
					MemorySegment segment = memoryPool.allocateNewSegment(owner);
					target.add(segment);
					segmentsForOwner.add(segment);
				}
				numNonAllocatedPages -= numPages;
			}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;So if there is preAllocation enabled the memory buffer is requested from the pool or every time there is newsegment allocated.&lt;br/&gt;
Coming to the release of these buffers&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (isPreAllocated) {
					&lt;span class=&quot;code-comment&quot;&gt;// release the memory in any &lt;span class=&quot;code-keyword&quot;&gt;case&lt;/span&gt;
&lt;/span&gt;					memoryPool.returnSegmentToPool(segment);
				}
				&lt;span class=&quot;code-keyword&quot;&gt;else&lt;/span&gt; {
					segment.free();
					numNonAllocatedPages++;
				}
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Again only if preAllocation is enabled we are returning to pool. Ya as you clearly pointed out it is just dynamic allocation that we do and on memory manager shutdown we clear the allocated buffers. But for offheap this will not be enough as the GC will not be able to garbage collect them unless the fullGC happens. &lt;br/&gt;
I would rather say that it is better we do internal management of offheap buffers. We should create a pool from which the buffers are allocated and if the pool is of fixed size and we have requests for more buffers than the size of the pool we should allocate them onheap only. (if that is acceptable).&lt;/p&gt;

&lt;p&gt;Currently the memory management pool is done by ArrayDeque. We only allow initialSize and I think it can grow beyond too. So for offheap buffers we should have a fixed size pool and as and when the demand grows we should allocate few buffers onheap and once the pool is again able to offer buffers we use them. &lt;/p&gt;

&lt;blockquote&gt;&lt;p&gt;I don&apos;t think just disallowing preallocation:false is a good fix.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes. I agree. That is a hacky one.&lt;/p&gt;</comment>
                            <comment id="15402004" author="till.rohrmann" created="Mon, 1 Aug 2016 13:04:46 +0000"  >&lt;p&gt;I think that having in both cases (preallocate and no preallocation) a memory pool which manages the off heap buffers (allocation and caching) with eager/lazy allocation of buffers at start up is a good idea to solve the problem.&lt;/p&gt;</comment>
                            <comment id="15402231" author="mxm" created="Mon, 1 Aug 2016 15:17:30 +0000"  >&lt;p&gt;Thanks for elaborating on your thoughts &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ram_krish&lt;/a&gt;! I was assuming that the memory segments actually would be managed by a pool regardless of preallocation set to false or true. You&apos;re right that without preallocation, segments are requested and released as they are needed. This DOES NOT work for the offheap memory segments (at least not how it is implemented now).&lt;/p&gt;

&lt;p&gt;Two possible fixes that come to my mind:&lt;/p&gt;

&lt;p&gt;1) Implement proper clearing of the offheap memory segments and keeping the preallocation:false behavior: &lt;a href=&quot;http://stackoverflow.com/a/8462690/2225100&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://stackoverflow.com/a/8462690/2225100&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2) Use pooling even with preallocation:false which only differs to preallocation:true that the memory is allocated lazily over time. &lt;/p&gt;

&lt;p&gt;I would say, let&apos;s go with 2) which should be easily solvable.&lt;/p&gt;


&lt;blockquote&gt;
&lt;p&gt;I would rather say that it is better we do internal management of offheap buffers. We should create a pool from which the buffers are allocated and if the pool is of fixed size and we have requests for more buffers than the size of the pool we should allocate them onheap only. (if that is acceptable).&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;In the case of offheap managed memory, the upper bound of the memory should be respected in the same way as for heap memory. No additional memory should be allocated on the heap instead. The user has to increase the memory size of the job fails.&lt;/p&gt;</comment>
                            <comment id="15402572" author="stephanewen" created="Mon, 1 Aug 2016 18:25:27 +0000"  >&lt;p&gt;If we pool the off-heap byte buffers, we effectively make a behavior very similar to pre-allocation for the off heap case.&lt;/p&gt;

&lt;p&gt;The crux here is that there is no point when the released off-heap memory is reclaimed before new off-heap memory is allocated.&lt;br/&gt;
The JVM has the parameter &lt;tt&gt;MaxDirectMemorySize&lt;/tt&gt; to limit the direct memory, and force a garbage collection (releasing off heap memory) when too much would be otherwise allocated.&lt;/p&gt;

&lt;p&gt;So, another option to fix this would be to set the &lt;tt&gt;MaxDirectMemorySize&lt;/tt&gt; parameter properly.&lt;/p&gt;

&lt;p&gt;Other than that, we can only pool. We cannot really manually release the memory when freeing the segment, because the ByteBuffer wrapper object may still exist. Freeing the memory too early will result in segmentation faults.&lt;/p&gt;</comment>
                            <comment id="15403533" author="ram_krish" created="Tue, 2 Aug 2016 07:26:53 +0000"  >&lt;blockquote&gt;&lt;p&gt;So, another option to fix this would be to set the MaxDirectMemorySize parameter properly.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Yes. I agree. But when the job runs in a multi tenant system where there are other process running and they are also memory intensive configuring this may always not be easy. I agree it is a direct way to solve the problem if one really knows his memory needs and requirements.&lt;br/&gt;
Regarding Pooling, some techniques that can be followed ( am saying from the we have used it in our projects)&lt;br/&gt;
-&amp;gt; Just pool the offheap byte buffers (all are fixed sized buffers). Once the usage is over put them back to pool. If the pool is empty we need to wait (blocking call - which may not be accepted). So either create onheap buffers which may not be right in this use case (but it is ideally safe). Or allocate offheap buffers dynamically and warn the user that  his pool size has to be increased because he is frequently allocating dynamic offheap buffers. &lt;br/&gt;
-&amp;gt; Another way to avoid segementation could be like Chunking. I can see that by default we create 32K sized buffers (page size). Instead we could create say 2MB sized offheap buffers and keep allocating 32K sized offset on every request. Again all the 2MB sized buffers will be pooled but once a buffer is requested from the pool we try to allocate 32K offsets. Once a buffer is full or the next request cannot be contained in it then move on to the next buffer. In turn we can pool these chunks also so that once a chunk is done we put them back to  a chunk pool and reuse it once that portion of the chunk is done. But this needs some knowledge of when the task has exactly completed the usage of that chunk. There should not be any references to it.&lt;/p&gt;</comment>
                            <comment id="15403775" author="stephanewen" created="Tue, 2 Aug 2016 11:00:56 +0000"  >&lt;p&gt;If we use pooling, I think we should just follow the exact same pooling pattern as the &quot;preallocation&quot; case, only make the allocation lazy.&lt;br/&gt;
That minimizes added complexity, and the pre-allocation pooling is quite proven.&lt;/p&gt;</comment>
                            <comment id="15403843" author="ram_krish" created="Tue, 2 Aug 2016 11:56:49 +0000"  >&lt;p&gt;Ok.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;If we use pooling, I think we should just follow the exact same pooling pattern as the &quot;preallocation&quot; case, only make the allocation lazy.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Am fine with the simple pattern. Can I have a go at it?&lt;/p&gt;</comment>
                            <comment id="15403961" author="mxm" created="Tue, 2 Aug 2016 13:33:03 +0000"  >&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt; for clyariying that we can&apos;t release the memory ourselves. It seems like the only two options we have is to&lt;/p&gt;

&lt;p&gt;1) Pool off-heap memory regardless of the preallocation mode. That would change the semantics of preallocation because it is supposed to eventually return memory.&lt;br/&gt;
2) Set the DirectMaxMemorySize limit correctly. This has been hard to do in the past because calculatating how much memory Netty uses, is not so easy to do. Also we have two versions of Netty, one included with Akka, one for the Flink network stack.&lt;/p&gt;

&lt;p&gt;Both options seem not really viable. I think before we change memory allocation behavior, we should discuss that on the Flink mailing list. &lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=ram_krish&quot; class=&quot;user-hover&quot; rel=&quot;ram_krish&quot;&gt;ram_krish&lt;/a&gt; What we can do now, is to discourage the use of off-heap memory with preallocation set to false. For example, print a prominent warning and add a hint to the documentation.&lt;/p&gt;
</comment>
                            <comment id="15404095" author="till.rohrmann" created="Tue, 2 Aug 2016 14:35:47 +0000"  >&lt;p&gt;Is &lt;tt&gt;preallocate == false&lt;/tt&gt; really supposed to return the memory eventually? Given the documentation it simply says that memory is allocated lazily but nothing about releasing. Was this an undocumented behaviour?&lt;/p&gt;</comment>
                            <comment id="15404120" author="mxm" created="Tue, 2 Aug 2016 14:49:15 +0000"  >&lt;p&gt;Yes, it was undocumented. Initially, I thought preallocation:false would just lazily pool when, in fact, it actually returned memory. In the offheap case this memory was never freed because the garbage collection of offheap memory only kicks in when the direct memory size limit is reached (or when a full garbage collection occurs in the meantime). For the regular heap case, it actually frees memory during normal garbage collection cycles.&lt;/p&gt;</comment>
                            <comment id="15404270" author="stephanewen" created="Tue, 2 Aug 2016 16:08:31 +0000"  >&lt;p&gt;True, it is not documented the &lt;tt&gt;preallocate == false&lt;/tt&gt; returns the memory, but is is quite a desirable behavior, actually.&lt;/p&gt;</comment>
                            <comment id="15404378" author="ram_krish" created="Tue, 2 Aug 2016 16:55:59 +0000"  >&lt;blockquote&gt;&lt;p&gt;We cannot really manually release the memory when freeing the segment, because the ByteBuffer wrapper object may still exist. &lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ideally when we are going to pool we won&apos;t try to free the memory - so the ByteBuffer wrapper will exist and that is what we will pool. I think once we do this we wont do segment.free() on that buffer and we will allow the address to be valid - if am not wrong.&lt;br/&gt;
Just a question, In case of &lt;/p&gt;
{ preallocation = true }
&lt;p&gt;, what does happen if the number of requests is more than the initial size? So we consume all the buffers in the pool but new requets won&apos;t be served?&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;What we can do now, is to discourage the use of off-heap memory with preallocation set to false. For example, print a prominent warning and add a hint to the documentation.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;May be for now we can do it. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I think before we change memory allocation behavior, we should discuss that on the Flink mailing list.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Ok sounds like a plan. So once we discuss I think we can go with the lazy allocation pooling model and that should be beneficial. Because anyway current pooling is with a unbounded queue and similarly it can be done here too. &lt;br/&gt;
One thing to note is that even with pooling if the MaxDirectMemory is still not configured right we will not be able to work with offheap buffers. The only thing is we won&apos;t grow infinitely. &lt;/p&gt;</comment>
                            <comment id="15404384" author="ram_krish" created="Tue, 2 Aug 2016 16:58:33 +0000"  >&lt;p&gt;I can do this Log update and document update tomorrow and open a PR for the same.&lt;/p&gt;</comment>
                            <comment id="15405836" author="stephanewen" created="Wed, 3 Aug 2016 12:25:48 +0000"  >&lt;p&gt;If a task requests more memory than the system is configured for, it will throw an error.&lt;br/&gt;
The program translation process calculates the memory budget for each operator to make sure they don&apos;t allocate too much.&lt;/p&gt;</comment>
                            <comment id="15408274" author="ram_krish" created="Thu, 4 Aug 2016 18:27:56 +0000"  >&lt;p&gt;I created the log related msg and the doc. So what info should we give in the doc. Will the following be enough?&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;+- `taskmanager.memory.preallocate`: Can be either of `&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;` or `&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;`. Specifies whether task managers should allocate all managed memory when starting up. (DEFAULT: &lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;). When `taskmanager.memory.off-heap` is set to `&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;`, then it is advised that &lt;span class=&quot;code-keyword&quot;&gt;this&lt;/span&gt; configuration is also set to `&lt;span class=&quot;code-keyword&quot;&gt;true&lt;/span&gt;`, because when set to
+`&lt;span class=&quot;code-keyword&quot;&gt;false&lt;/span&gt;`, cleaning up of the allocated offheap memory kicks up only when the configured JVM parameter MaxDirectMemorySize is reached by triggering a full GC.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;I can submit a PR after this.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;If a task requests more memory than the system is configured for, it will throw an error.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;Thanks &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt;.&lt;/p&gt;</comment>
                            <comment id="15408925" author="githubbot" created="Fri, 5 Aug 2016 05:57:12 +0000"  >&lt;p&gt;GitHub user ramkrish86 opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2336&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2336&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4094&quot; title=&quot;Off heap memory deallocation might not properly work&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4094&quot;&gt;&lt;del&gt;FLINK-4094&lt;/del&gt;&lt;/a&gt; Off heap memory deallocation might not properly work (Ram)&lt;/p&gt;

&lt;p&gt;    Thanks for contributing to Apache Flink. Before you open your pull request, please take the following check list into consideration.&lt;br/&gt;
    If your changes take all of the items into account, feel free to open your pull request. For more information and/or questions please refer to the &lt;span class=&quot;error&quot;&gt;&amp;#91;How To Contribute guide&amp;#93;&lt;/span&gt;(&lt;a href=&quot;http://flink.apache.org/how-to-contribute.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://flink.apache.org/how-to-contribute.html&lt;/a&gt;).&lt;br/&gt;
    In addition to going through the list, please provide a meaningful description of your changes.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] General&lt;/li&gt;
	&lt;li&gt;The pull request references the related JIRA issue (&quot;&lt;span class=&quot;error&quot;&gt;&amp;#91;FLINK-XXX&amp;#93;&lt;/span&gt; Jira title text&quot;)&lt;/li&gt;
	&lt;li&gt;The pull request addresses only one issue&lt;/li&gt;
	&lt;li&gt;Each commit in the PR has a meaningful commit message (including the JIRA id)&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Documentation&lt;/li&gt;
	&lt;li&gt;Documentation has been added for new functionality&lt;/li&gt;
	&lt;li&gt;Old documentation affected by the pull request has been updated&lt;/li&gt;
	&lt;li&gt;JavaDoc for public methods has been added&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;[ ] Tests &amp;amp; Build&lt;/li&gt;
	&lt;li&gt;Functionality added by the pull request is covered by tests&lt;/li&gt;
	&lt;li&gt;`mvn clean verify` has been executed successfully locally or a Travis build has passed&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Just added a log msg warning the user about using OFFHEAP and PREALLOCATION together. Same comment is added over in the doc also. But not sure if that is enough or we need more information to be added there. But for a user it may not be relevant. Any thoughts?&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/ramkrish86/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/ramkrish86/flink&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4094&quot; title=&quot;Off heap memory deallocation might not properly work&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4094&quot;&gt;&lt;del&gt;FLINK-4094&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2336.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2336.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #2336&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 4e51aaffebeb3ccf530bff9e67c1acd63d90672d&lt;br/&gt;
Author: Ramkrishna &amp;lt;ramkrishna.s.vasudevan@intel.com&amp;gt;&lt;br/&gt;
Date:   2016-08-05T05:55:07Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-4094&quot; title=&quot;Off heap memory deallocation might not properly work&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-4094&quot;&gt;&lt;del&gt;FLINK-4094&lt;/del&gt;&lt;/a&gt; Off heap memory deallocation might not properly work (Ram)&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15409276" author="githubbot" created="Fri, 5 Aug 2016 10:12:16 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2336#discussion_r73670793&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2336#discussion_r73670793&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: docs/setup/config.md &amp;#8212;&lt;br/&gt;
    @@ -83,7 +83,8 @@ The default fraction for managed memory can be adjusted using the `taskmanager.m&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;`taskmanager.memory.segment-size`: The size of memory buffers used by the memory manager and the network stack in bytes (DEFAULT: 32768 (= 32 KiBytes)).&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    &amp;#8211; `taskmanager.memory.preallocate`: Can be either of `true` or `false`. Specifies whether task managers should allocate all managed memory when starting up. (DEFAULT: false)&lt;br/&gt;
    +- `taskmanager.memory.preallocate`: Can be either of `true` or `false`. Specifies whether task managers should allocate all managed memory when starting up. (DEFAULT: false). When `taskmanager.memory.off-heap` is set to `true`, then it is advised that this configuration is also set to `true`, because when set to&lt;br/&gt;
    +`false`, cleaning up of the allocated offheap memory kicks up only when the configured JVM parameter MaxDirectMemorySize is reached by triggering a full GC.&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    &quot;kicks up&quot; --&amp;gt; &quot;happens&quot;.&lt;/p&gt;

&lt;p&gt;    The reading flow is easier if this is split into two sentences (I would split at &quot;, because&quot;)&lt;/p&gt;</comment>
                            <comment id="15409277" author="githubbot" created="Fri, 5 Aug 2016 10:12:35 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2336#discussion_r73670834&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2336#discussion_r73670834&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-runtime/src/main/java/org/apache/flink/runtime/memory/MemoryManager.java &amp;#8212;&lt;br/&gt;
    @@ -53,6 +55,7 @@&lt;br/&gt;
      */&lt;br/&gt;
     public class MemoryManager {&lt;/p&gt;

&lt;p&gt;    +	static final Logger LOG = LoggerFactory.getLogger(MemoryManager.class);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Would make this `private`&lt;/p&gt;</comment>
                            <comment id="15409278" author="githubbot" created="Fri, 5 Aug 2016 10:12:48 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2336&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2336&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Two small comments, otherwise this looks good.&lt;/p&gt;</comment>
                            <comment id="15409350" author="ram_krish" created="Fri, 5 Aug 2016 11:44:05 +0000"  >&lt;p&gt;Updated the PR. Pls check.&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt;&lt;br/&gt;
Thanks for the comments. &lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I think before we change memory allocation behavior, we should discuss that on the Flink mailing list.&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;ARe we going to send a mail to the community and decide if we can allow preAllocation for OFFHEAP buffers? &lt;/p&gt;</comment>
                            <comment id="15409387" author="githubbot" created="Fri, 5 Aug 2016 12:29:51 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2336&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2336&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Looks good, will merge this...&lt;/p&gt;</comment>
                            <comment id="15409487" author="githubbot" created="Fri, 5 Aug 2016 14:05:40 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2336&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2336&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15409491" author="stephanewen" created="Fri, 5 Aug 2016 14:06:49 +0000"  >&lt;p&gt;The warning has been added in 119364c6e38aedf7e6ebf8b2e84632a0a3697377&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 15 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2zqef:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>