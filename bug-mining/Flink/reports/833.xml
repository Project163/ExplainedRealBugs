<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:22:43 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-3368] Kafka 0.8 consumer fails to recover from broker shutdowns</title>
                <link>https://issues.apache.org/jira/browse/FLINK-3368</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;It seems that the Kafka Consumer (0.8) fails to restart a job after it failed due to a Kafka broker shutdown.&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;java.lang.Exception: Unable to get last offset &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partitions [FetchPartition {topic=a, partition=13, offset=-915623761776}, FetchPartition {topic=b, partition=13, offset=-915623761776}, FetchPartition {topic=c, partition=13, offset=-915623761776}, FetchPartition {topic=d, partition=13, offset=-915623761776}, FetchPartition {topic=e, partition=13, offset=-915623761776}, FetchPartition {topic=f, partition=13, offset=-915623761776}, FetchPartition {topic=g, partition=13, offset=-915623761776}].
Exception &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; partition 13: kafka.common.NotLeaderForPartitionException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at java.lang.&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.newInstance(&lt;span class=&quot;code-object&quot;&gt;Class&lt;/span&gt;.java:442)
	at kafka.common.ErrorMapping$.exceptionFor(ErrorMapping.scala:86)
	at kafka.common.ErrorMapping.exceptionFor(ErrorMapping.scala)
	at org.apache.flink.streaming.connectors.kafka.internals.LegacyFetcher$SimpleConsumerThread.getLastOffset(LegacyFetcher.java:551)
	at org.apache.flink.streaming.connectors.kafka.internals.LegacyFetcher$SimpleConsumerThread.run(LegacyFetcher.java:379)

&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I haven&apos;t understood the cause of this issue, but I&apos;ll investigate it.&lt;/p&gt;</description>
                <environment></environment>
        <key id="12937535">FLINK-3368</key>
            <summary>Kafka 0.8 consumer fails to recover from broker shutdowns</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rmetzger">Robert Metzger</assignee>
                                    <reporter username="rmetzger">Robert Metzger</reporter>
                        <labels>
                    </labels>
                <created>Mon, 8 Feb 2016 15:38:05 +0000</created>
                <updated>Tue, 19 Apr 2016 10:25:34 +0000</updated>
                            <resolved>Fri, 19 Feb 2016 18:50:15 +0000</resolved>
                                                    <fixVersion>1.0.0</fixVersion>
                                    <component>Connectors / Kafka</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                                                            <comments>
                            <comment id="15137138" author="stephanewen" created="Mon, 8 Feb 2016 16:05:10 +0000"  >&lt;p&gt;I think this is a blocker&lt;/p&gt;</comment>
                            <comment id="15137767" author="rmetzger" created="Mon, 8 Feb 2016 21:51:15 +0000"  >&lt;p&gt;It seems that the issue depends on the number of partitions subscribed by the FlinkKafkaConsumer.&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;with 2 topics, 4 partitions each: on failure, 1 restart needed)&lt;/li&gt;
	&lt;li&gt;with 10 topics, 4 partitions each: on failure, 2 restarts needed&lt;/li&gt;
	&lt;li&gt;with 100 topics, 4 partitions the restarts are going on forever.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="15140540" author="rmetzger" created="Wed, 10 Feb 2016 09:12:13 +0000"  >&lt;p&gt;Flink&apos;s mechanism to detect the leading brokers and the assignment to the TopicPartitions works properly.&lt;br/&gt;
Also, no leader information is kept across restarts.&lt;/p&gt;

&lt;p&gt;I think the issue is that Kafka needs a lot of time to until all broker return up-to-date metadata. In my experiment (with 400 partitions on 7 brokers and a replication of 2) it took 13 minutes for all brokers returning valid information again.&lt;br/&gt;
There are also some issues in Kafka which are not fixed in Kafka 0.8, relating to metadata information in the brokers:&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-1367&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-1367&lt;/a&gt; (Broker topic metadata not kept in sync with ZooKeeper)&lt;br/&gt;
&lt;a href=&quot;https://issues.apache.org/jira/browse/KAFKA-972&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/KAFKA-972&lt;/a&gt; (MetadataRequest returns stale list of brokers)&lt;br/&gt;
(Cloudera has &lt;a href=&quot;http://www.cloudera.com/documentation/kafka/latest/topics/kafka_fi_131.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;backported&lt;/a&gt; the fixes in their distribution)&lt;/p&gt;

&lt;p&gt;Why are the high level java kafka consumers are not suffering from this?&lt;br/&gt;
Kafka is randomly choosing the broker to query for the metadata. Flink is using always the first one. By randomly asking around, they&apos;ll eventually hit a broker with valid information.&lt;br/&gt;
Also, Kafka is not failing all consumers if one is trying to pull data from the wrong broker. They&apos;ll probably query again for metadata and then reconnect to the new leader, without failing everything.&lt;/p&gt;

&lt;p&gt;Therefore, I suggest to implement the mechanisms to handle leader changes into the LegacyFetcher of our Flink Kafka 0.8 consumer.&lt;/p&gt;</comment>
                            <comment id="15140731" author="gyfora" created="Wed, 10 Feb 2016 12:57:02 +0000"  >&lt;p&gt;Thanks Robert for the detailed explanation. It sucks that they did not backport this fix into Kafka &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/sad.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15143135" author="githubbot" created="Thu, 11 Feb 2016 17:47:25 +0000"  >&lt;p&gt;GitHub user rmetzger opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-3368&quot; title=&quot;Kafka 0.8 consumer fails to recover from broker shutdowns&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-3368&quot;&gt;&lt;del&gt;FLINK-3368&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Kafka 0.8&amp;#93;&lt;/span&gt; Handle leader changes in Kafka Consumer.&lt;/p&gt;

&lt;p&gt;    Please see the JIRA for an explanation of the problems.&lt;br/&gt;
    tl;dr: The Kafka 0.8 consumer now handles broker failures internally, without relying on Flink&apos;s checkpointing / job recovery.&lt;/p&gt;

&lt;p&gt;    The test case which was previously relying on a topology restart is now expecting the consumers to handle the failure.&lt;/p&gt;

&lt;p&gt;    I also tested this change on a 7 nodes cluster. The job was running for 30 minutes, surviving 4 broker shutdowns.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/rmetzger/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/rmetzger/flink&lt;/a&gt; flink3368&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1623&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 5e3bc92a0e6b1250a8ae0f8d898296586851dea1&lt;br/&gt;
Author: Robert Metzger &amp;lt;rmetzger@apache.org&amp;gt;&lt;br/&gt;
Date:   2016-02-09T11:04:45Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-3368&quot; title=&quot;Kafka 0.8 consumer fails to recover from broker shutdowns&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-3368&quot;&gt;&lt;del&gt;FLINK-3368&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Kafka 0.8&amp;#93;&lt;/span&gt; Handle leader changes in Kafka Consumer.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15143862" author="githubbot" created="Fri, 12 Feb 2016 01:36:22 +0000"  >&lt;p&gt;Github user ndimiduk commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-183143954&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-183143954&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I just ran into this as well. Do you mind backporting to 0.10 branch too?&lt;/p&gt;</comment>
                            <comment id="15144544" author="githubbot" created="Fri, 12 Feb 2016 12:56:23 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-183314810&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-183314810&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Once this PR has been merged (I expect that some adoptions might still be required) I can try and see if its straightforward to backport. Are you planning to stick to Flink 0.10 for a longer period after 1.0 has been released?&lt;br/&gt;
     (I&apos;m asking because so far most users were migrating to the latest release)&lt;/p&gt;</comment>
                            <comment id="15144906" author="githubbot" created="Fri, 12 Feb 2016 17:30:51 +0000"  >&lt;p&gt;Github user ndimiduk commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-183421908&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-183421908&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    My timeline for adopting Flink 1.0 depends heavily on the amount of code&lt;br/&gt;
    change required to adopt. I haven&apos;t looked closely at my exposure to API&lt;br/&gt;
    and dependency changes. I also usually avoid jumping directly to production&lt;br/&gt;
    with Apache X.0.0 community releases, preferring to cut their teeth in a&lt;br/&gt;
    stage env while community can further stabilize.&lt;/p&gt;

&lt;p&gt;    On Friday, February 12, 2016, Robert Metzger &amp;lt;notifications@github.com&amp;gt;&lt;br/&gt;
    wrote:&lt;/p&gt;

&lt;p&gt;    &amp;gt; Once this PR has been merged (I expect that some adoptions might still be&lt;br/&gt;
    &amp;gt; required) I can try and see if its straightforward to backport. Are you&lt;br/&gt;
    &amp;gt; planning to stick to Flink 0.10 for a longer period after 1.0 has been&lt;br/&gt;
    &amp;gt; released?&lt;br/&gt;
    &amp;gt; (I&apos;m asking because so far most users were migrating to the latest release)&lt;br/&gt;
    &amp;gt;&lt;br/&gt;
    &amp;gt; &#8212;&lt;br/&gt;
    &amp;gt; Reply to this email directly or view it on GitHub&lt;br/&gt;
    &amp;gt; &amp;lt;&lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-183314810&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-183314810&lt;/a&gt;&amp;gt;.&lt;br/&gt;
    &amp;gt;&lt;/p&gt;
</comment>
                            <comment id="15150652" author="githubbot" created="Wed, 17 Feb 2016 15:39:58 +0000"  >&lt;p&gt;Github user gyfora commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185260566&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185260566&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    We have tested this and it worked correctly. Should we go ahead a merge it?&lt;/p&gt;</comment>
                            <comment id="15150659" author="githubbot" created="Wed, 17 Feb 2016 15:45:51 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185264548&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185264548&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Awesome. Thanks a lot for testing it! @StephanEwen wanted to take a look at the PR as well.&lt;/p&gt;</comment>
                            <comment id="15150660" author="githubbot" created="Wed, 17 Feb 2016 15:46:53 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185265002&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185265002&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Taking a look right now. Give me a bit to double check...&lt;/p&gt;</comment>
                            <comment id="15150803" author="githubbot" created="Wed, 17 Feb 2016 17:15:24 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185307346&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185307346&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I have some comments, here is a first batch:&lt;/p&gt;

&lt;p&gt;    The `ZooKeeperStringSerializer` seems to be only used in tests, but adds a hard compile-scope dependency to `org.I0Itec:ZkClient`. Moving this to test scope gets rid of this.&lt;/p&gt;

&lt;p&gt;    There are some old functions that seem to have no more use:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;KafkaTopicPartitionLeader.replaceIgnoringLeader()&lt;/li&gt;
	&lt;li&gt;KafkaTopicPartition.isContained()&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;    In the `OffsetHandler` / `ZookeeperOffsetHandler`, the `getOffsets(...)` methods can probably get rid of the `Fetcher` (since it does not position the fetcher any more.&lt;/p&gt;


&lt;p&gt;    There is a lot of option parsing with no checks and proper exceptions. If someone puts in a wrong value, all they see is an exception in `Integer.valueOf()`. In most other places of the code, there is a lot of effort to give good messages on parse errors.&lt;/p&gt;

&lt;p&gt;    Note: Given all the discussion about dependency clashes / shading and resulting issues, I would like to reduce dependencies as much as possible. The Kafka code adds the Guava dependency simply for the `checkNotNull` method. I would simpyl use `java.util.Object.requiteNotNull` instead and get rid of the dependency.&lt;/p&gt;</comment>
                            <comment id="15151290" author="githubbot" created="Wed, 17 Feb 2016 22:13:50 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185430953&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185430953&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thank you for the review. I&apos;ll address them soon!&lt;/p&gt;</comment>
                            <comment id="15152351" author="githubbot" created="Thu, 18 Feb 2016 14:02:05 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185735036&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185735036&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The `getPartitionsForTopic()` method fails hard immediately, when it does not find the Topic name or partition. There will be no retries. Is that desirable, or can it be that some nodes to not know the topic name, because Kafka&apos;s meta data has not properly synced?&lt;/p&gt;</comment>
                            <comment id="15152393" author="githubbot" created="Thu, 18 Feb 2016 14:36:30 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185748371&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185748371&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    So far nobody had this as an issue and during the integration tests and my cluster tests, this issue never occurred. But I agree, there is no good reason to fail hard here. I&apos;ll remove the exception and just log the error, so that the retry can happen.&lt;/p&gt;</comment>
                            <comment id="15153030" author="githubbot" created="Thu, 18 Feb 2016 20:32:10 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185903144&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185903144&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I think this is in okay shape, quite an improvement over the current version.&lt;/p&gt;

&lt;p&gt;    There are some things I suggest to re-examine:&lt;/p&gt;


&lt;p&gt;    Method `commitOffsets()` in &quot;KlinkKafkaConsumer08:397&quot; misses `@Override` annotation.&lt;/p&gt;

&lt;p&gt;    The &quot;TODO: maybe add a check again here if we are still running.&quot; seems worth doing&lt;/p&gt;

&lt;p&gt;    In many places, you do `if (!closableQueue.addIfOpen(...)) &lt;/p&gt;
{ error }
&lt;p&gt;`. Simply use the `add()`&#180; method, which errors if not open. Also (LegacyFetcher:120): There can never be an IllegalStateException while initially loading in the constructor.&lt;/p&gt;

&lt;p&gt;    I do not see the use of the list `deadBrokerThreads` in the legacy fetcher. Unless I am overlooking something, I would remove it (simplify the code). Since they will eventually shut down anyways, I think you need not sync on them when leaving the `run()` method of the legacy fetcher.&lt;/p&gt;

&lt;p&gt;    The check whether a fetcher thread is alive should probably be `thread.getNewPartitionsQueue().isOpen()`, rather than `thread.isAlive()`. That is the flag that is atomically changed and checked with the adding of partitions. &lt;/p&gt;

&lt;p&gt;    The fetcher&apos;s main thread always blocks 5 seconds before it can notice that the broker threads shut down.&lt;br/&gt;
    I am wondering if we can make that more snappy, by waking the main thread up when a fetcher thread terminates (by adding a marker element to the queue).&lt;/p&gt;


&lt;p&gt;    Method `findLeaderForPartitions(...)` in the legacy fetcher also fails hard once it cannot find the leader for a partition (no retries). Is that intended?&lt;/p&gt;

&lt;p&gt;    The code uses everywhere `Integer.valueOf()`, creating a boxed integer, and then unboxes it. `Integer.parseInt(...)` is the preferrable choice (almost always, I would completely drop using `Integer.valueOf()` in any place).&lt;/p&gt;

&lt;p&gt;    This loop (in `findLeaderForPartitions(...)`) is either to nifty for me to comprehend, or bogus:&lt;/p&gt;

&lt;p&gt;    ```java&lt;br/&gt;
    List&amp;lt;KafkaTopicPartitionLeader&amp;gt; topicPartitionWithLeaderList = infoFetcher.getPartitions();&lt;br/&gt;
    List&amp;lt;FetchPartition&amp;gt; partitionsToAssignInternal = new ArrayList&amp;lt;&amp;gt;(partitionsToAssign);&lt;/p&gt;

&lt;p&gt;    Map&amp;lt;Node, List&amp;lt;FetchPartition&amp;gt;&amp;gt; leaderToPartitions = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;    for(KafkaTopicPartitionLeader partitionLeader: topicPartitionWithLeaderList) {&lt;br/&gt;
    	if (partitionsToAssignInternal.size() == 0) &lt;/p&gt;
{
    		// we are done: all partitions are assigned
    		break;
    	}
&lt;p&gt;    	Iterator&amp;lt;FetchPartition&amp;gt; fpIter = partitionsToAssignInternal.iterator();&lt;br/&gt;
    	while (fpIter.hasNext()) {&lt;br/&gt;
    		FetchPartition fp = fpIter.next();&lt;br/&gt;
    		if (fp.topic.equals(partitionLeader.getTopicPartition().getTopic())&lt;br/&gt;
    				&amp;amp;&amp;amp; fp.partition == partitionLeader.getTopicPartition().getPartition()) {&lt;/p&gt;

&lt;p&gt;    			// we found the leader for one of the fetch partitions&lt;br/&gt;
    			Node leader = partitionLeader.getLeader();&lt;br/&gt;
    			List&amp;lt;FetchPartition&amp;gt; partitionsOfLeader = leaderToPartitions.get(leader);&lt;br/&gt;
    			if (partitionsOfLeader == null) &lt;/p&gt;
{
    				partitionsOfLeader = new ArrayList&amp;lt;&amp;gt;();
    				leaderToPartitions.put(leader, partitionsOfLeader);
    			}
&lt;p&gt;    			partitionsOfLeader.add(fp);&lt;br/&gt;
    			fpIter.remove();&lt;br/&gt;
    			break;&lt;br/&gt;
    		}&lt;br/&gt;
    	}&lt;br/&gt;
    }&lt;br/&gt;
    ```&lt;br/&gt;
    Does this do anything different than the version below? (The internal iteration always finds the&lt;br/&gt;
    exact same element at the first position and breaks).&lt;br/&gt;
    ```java&lt;br/&gt;
    List&amp;lt;KafkaTopicPartitionLeader&amp;gt; topicPartitionWithLeaderList = infoFetcher.getPartitions();&lt;br/&gt;
    Map&amp;lt;Node, List&amp;lt;FetchPartition&amp;gt;&amp;gt; leaderToPartitions = new HashMap&amp;lt;&amp;gt;();&lt;/p&gt;

&lt;p&gt;    for (KafkaTopicPartitionLeader partitionLeader: topicPartitionWithLeaderList) {&lt;br/&gt;
    	Node leader = partitionLeader.getLeader();&lt;br/&gt;
    	List&amp;lt;FetchPartition&amp;gt; partitionsOfLeader = leaderToPartitions.get(leader);&lt;br/&gt;
    	if (partitionsOfLeader == null) &lt;/p&gt;
{
    		partitionsOfLeader = new ArrayList&amp;lt;&amp;gt;();
    		leaderToPartitions.put(leader, partitionsOfLeader);
    	}
&lt;p&gt;    	partitionsOfLeader.add(fp);&lt;br/&gt;
    }&lt;br/&gt;
    ```&lt;/p&gt;</comment>
                            <comment id="15153031" author="githubbot" created="Thu, 18 Feb 2016 20:32:21 +0000"  >&lt;p&gt;GitHub user StephanEwen opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1672&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1672&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-3368&quot; title=&quot;Kafka 0.8 consumer fails to recover from broker shutdowns&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-3368&quot;&gt;&lt;del&gt;FLINK-3368&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Kafka 0.8&amp;#93;&lt;/span&gt; Improvements to the Kafka Consumer changes&lt;/p&gt;

&lt;p&gt;    This makes some changes on top of #1623 , as remarked in the comments.&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/StephanEwen/incubator-flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/StephanEwen/incubator-flink&lt;/a&gt; kafka&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1672.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1672.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1672&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 982d68554b5e373500fff4522e03f721486ce86b&lt;br/&gt;
Author: Robert Metzger &amp;lt;rmetzger@apache.org&amp;gt;&lt;br/&gt;
Date:   2016-02-09T11:04:45Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-3368&quot; title=&quot;Kafka 0.8 consumer fails to recover from broker shutdowns&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-3368&quot;&gt;&lt;del&gt;FLINK-3368&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Kafka 0.8&amp;#93;&lt;/span&gt; Handle leader changes in Kafka Consumer.&lt;/p&gt;

&lt;p&gt;commit 7a118c71a8a0db999cd0890769fec5cabc607e64&lt;br/&gt;
Author: Stephan Ewen &amp;lt;sewen@apache.org&amp;gt;&lt;br/&gt;
Date:   2016-02-18T20:12:03Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-3368&quot; title=&quot;Kafka 0.8 consumer fails to recover from broker shutdowns&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-3368&quot;&gt;&lt;del&gt;FLINK-3368&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;Kafka 0.8&amp;#93;&lt;/span&gt; Some improvements to the Legacy Fetcher&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="15153034" author="githubbot" created="Thu, 18 Feb 2016 20:33:01 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185903526&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185903526&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I have made a few changes on top of this, which address some of the comments above. Have a look at #1672&lt;/p&gt;</comment>
                            <comment id="15153038" author="githubbot" created="Thu, 18 Feb 2016 20:35:45 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185905274&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185905274&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Awesome, thank you. I&apos;ll take your changes from the pull request and address the remaining issues.&lt;/p&gt;</comment>
                            <comment id="15153404" author="githubbot" created="Fri, 19 Feb 2016 00:06:26 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-185987092&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-185987092&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The comments are mostly simplifications and faster response. I did not find a critical issue, so this should be good to go, actually.&lt;/p&gt;

</comment>
                            <comment id="15154178" author="githubbot" created="Fri, 19 Feb 2016 12:48:19 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-186201689&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-186201689&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thank you for the good review. I adopted my IntelliJ inspections to find some of the mentioned issues myself in the future (forgotten @Override, unused methods).&lt;/p&gt;

&lt;p&gt;    I&apos;ll address all your comments and then merge the pull request.&lt;/p&gt;

&lt;p&gt;    Regarding the `findLeaderForPartitions` method: Its not very well documented and quite hidden in the code, but the `topicPartitionWithLeaderList` list contains the TopicPartitions for ALL requested topics.&lt;/p&gt;

&lt;p&gt;    From a high level, the method does the following:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Get a list of FetchPartitions (usually only a few partitions)&lt;/li&gt;
	&lt;li&gt;Get the list of topics from the FetchPartitions list and request the partitions for the topics. (Kafka doesn&apos;t support getting leaders for a set of partitions)&lt;/li&gt;
	&lt;li&gt;Build a Map&amp;lt;Leader, List&amp;lt;FetchPartition&amp;gt;&amp;gt; where only the requested partitions are contained.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I added some comments into the code and renamed some variables to make it more obvious.&lt;/p&gt;
</comment>
                            <comment id="15154633" author="githubbot" created="Fri, 19 Feb 2016 18:41:59 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15154632" author="githubbot" created="Fri, 19 Feb 2016 18:41:59 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1672&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1672&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15154653" author="rmetzger" created="Fri, 19 Feb 2016 18:50:15 +0000"  >&lt;p&gt;Fixed in &lt;a href=&quot;http://git-wip-us.apache.org/repos/asf/flink/commit/d48bb59a&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://git-wip-us.apache.org/repos/asf/flink/commit/d48bb59a&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15154902" author="githubbot" created="Fri, 19 Feb 2016 21:38:23 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1623#issuecomment-186418102&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1623#issuecomment-186418102&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks! And I finally got the meaning of that method with the nested loops &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15172109" author="ndimiduk" created="Mon, 29 Feb 2016 16:41:16 +0000"  >&lt;p&gt;My streaming jobs are continuing to die when Kafka rebalances. Any chance on a backport to 0.10? Maybe it&apos;s there and I&apos;m looking at the wrong branch? release-0.10 is the place it would land, correct?&lt;/p&gt;</comment>
                            <comment id="15173899" author="stephanewen" created="Tue, 1 Mar 2016 15:28:52 +0000"  >&lt;p&gt;It has not yet been backported to the 0.10 branch, as far as I know. In 0.10, the entire job needs to go into recovery for that.&lt;/p&gt;

&lt;p&gt;Seems these days people are all very busy with 1.0 release testing. If there is some breathing room after that, we could look into this.&lt;/p&gt;</comment>
                            <comment id="15174151" author="ndimiduk" created="Tue, 1 Mar 2016 18:34:35 +0000"  >&lt;p&gt;Kafka rebalances and the flink job must &quot;go into recovery&quot;? Hopefully that&apos;s not as drastic an action as it sounds. I was expecting something more like a back-off retry loop with a counter that resets after a period; that&apos;s what some of our internal kafka clients do. Seems I&apos;m asking for more than I realized.&lt;/p&gt;</comment>
                            <comment id="15174251" author="stephanewen" created="Tue, 1 Mar 2016 19:25:48 +0000"  >&lt;p&gt;In Flink 1.0, that is pretty much how it works: If brokers rebalance, the partition handling marked for re-assignment and and picked up by a different broker connection.&lt;br/&gt;
Works pretty quick and the job does not notice anything.&lt;/p&gt;

&lt;p&gt;We tested it also on Kafka installations where the brokers were out of sync concerning their metadata, so the cluster &quot;rebalanced&quot; many times very quickly.&lt;/p&gt;</comment>
                            <comment id="15174258" author="stephanewen" created="Tue, 1 Mar 2016 19:27:29 +0000"  >&lt;p&gt;One thing that you can try is to us the 1.0-RC3 FlnkKafkaConsumer with your 0.10.2 Flink installation. It might actually work, because since 0.10.2 the interface did not break (IIRC).&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                            <subtask id="12938197">FLINK-3384</subtask>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            9 years, 38 weeks ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2sk87:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>