<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:25:46 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-5048] Kafka Consumer (0.9/0.10) threading model leads problematic cancellation behavior</title>
                <link>https://issues.apache.org/jira/browse/FLINK-5048</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;The &lt;tt&gt;FLinkKafkaConsumer&lt;/tt&gt; (0.9 / 0.10) spawns a separate thread that operates the KafkaConsumer. That thread is shielded from interrupts, because the Kafka Consumer has not been handling thread interrupts well.&lt;/p&gt;

&lt;p&gt;Since that thread is also the thread that emits records, it may block in the network stack (backpressure) or in chained operators. The later case leads to situations where cancellations get very slow unless that thread would be interrupted (which it cannot be).&lt;/p&gt;

&lt;p&gt;I propose to change the thread model as follows:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A spawned consumer thread pull from the KafkaConsumer and pushes its pulled batch of records into a blocking queue (size one)&lt;/li&gt;
	&lt;li&gt;The main thread of the task will pull the record batches from the blocking queue and emit the records.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;This allows actually for some additional I/O overlay while limiting the additional memory consumption - only two batches are ever held, one being fetched and one being emitted.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13019793">FLINK-5048</key>
            <summary>Kafka Consumer (0.9/0.10) threading model leads problematic cancellation behavior</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="sewen">Stephan Ewen</assignee>
                                    <reporter username="sewen">Stephan Ewen</reporter>
                        <labels>
                    </labels>
                <created>Thu, 10 Nov 2016 10:30:32 +0000</created>
                <updated>Thu, 16 Mar 2017 12:45:25 +0000</updated>
                            <resolved>Thu, 16 Mar 2017 03:45:23 +0000</resolved>
                                    <version>1.1.3</version>
                                    <fixVersion>1.2.0</fixVersion>
                                    <component>Connectors / Kafka</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>8</watches>
                                                                                                                <comments>
                            <comment id="15657219" author="githubbot" created="Fri, 11 Nov 2016 14:38:51 +0000"  >&lt;p&gt;GitHub user StephanEwen opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5048&quot; title=&quot;Kafka Consumer (0.9/0.10) threading model leads problematic cancellation behavior&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5048&quot;&gt;&lt;del&gt;FLINK-5048&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Kafka Consumer&amp;#93;&lt;/span&gt; Change thread model of FlinkKafkaConsumer to better handel shutdown/interrupt situations&lt;/p&gt;

&lt;p&gt;    *&lt;b&gt;NOTE:&lt;/b&gt;* Only the second commit is relevant, the first commit only prepares by cleaning up some code in the Flink Kafka Consumers for 0.9 and 0.10&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Rational&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    Prior to this commit, the FlinkKafkaConsumer (0.9 / 0.10) spawns a separate thread that operates Kafka&apos;s consumer. That thread was shielded from interrupts, because the Kafka Consumer has not been handling thread interrupts well.&lt;/p&gt;

&lt;p&gt;    Since that thread was also the thread that emitted records, it would block in the network stack (backpressure) or in chained operators. The later case lead to situations where cancellations got very slow unless that thread would be interrupted (which it could not be).&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Core changes&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    This commit changes the thread model:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A spawned consumer thread polls a batch or records from the KafkaConsumer and pushes the batch of records into a sort of blocking queue&lt;/li&gt;
	&lt;li&gt;The main thread of the task will pull the record batches from the blocking queue and emit the records.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    The &quot;batches&quot; are the fetch batches from Kafka&apos;s consumer, there is no additional buffering or so that would impact latency.&lt;/p&gt;

&lt;p&gt;    The thread-to-thread handover of the records batches is handled by a class `Handover` which is a size-one blocking queue with the additional ability to gracefully wake up the consumer thread if the main thread decided to shut down. That way we need no interrupts on the KafkaConsumerThread.&lt;/p&gt;

&lt;p&gt;    This also pulls the KafkaConsumerThread out of the fetcher class for some code cleanup (scope simplifications).&lt;br/&gt;
    The method calls that were broken between Kafka 0.9 and 0.10 are handled via a &quot;call bridge&quot;, which leads to fewer code changes in the fetchers for each method that needs to be adapted.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Tests&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    This adjusts some tests, but it removes the &quot;short retention IT Cases&quot; for Kafka 0.9 and 0.10 consumers.&lt;br/&gt;
    While that type of test makes sense for the 0.8 consumer, for the newer ones the tests actually test purely Kafka and no Flink code.&lt;/p&gt;

&lt;p&gt;    In addition, they are virtually impossible to run stable and fast, because they rely on an artificial slowdown in the KafkaConsumer threads. That type of unhealthy interference is exactly what this patch here prevents &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/StephanEwen/incubator-flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/StephanEwen/incubator-flink&lt;/a&gt; kafka_consumer&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #2789&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit f6cd417cdf37213f88c62e9342206e249402eac6&lt;br/&gt;
Author: Stephan Ewen &amp;lt;sewen@apache.org&amp;gt;&lt;br/&gt;
Date:   2016-11-09T16:58:54Z&lt;/p&gt;

&lt;p&gt;    &lt;span class=&quot;error&quot;&gt;&amp;#91;hotfix&amp;#93;&lt;/span&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;Kafka Consumer&amp;#93;&lt;/span&gt; Clean up some code confusion and style in the Fetchers for Kafka 0.9/0.10&lt;/p&gt;

&lt;p&gt;commit 9a0786508b9a13cd986de593c6bdb2ecdb1737a8&lt;br/&gt;
Author: Stephan Ewen &amp;lt;sewen@apache.org&amp;gt;&lt;br/&gt;
Date:   2016-11-10T10:13:43Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-5048&quot; title=&quot;Kafka Consumer (0.9/0.10) threading model leads problematic cancellation behavior&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-5048&quot;&gt;&lt;del&gt;FLINK-5048&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;kafka consumer&amp;#93;&lt;/span&gt; Change thread model of FlinkKafkaConsumer to better handel shutdown/interrupt situations&lt;/p&gt;

&lt;p&gt;    Prior to this commit, the FlinkKafkaConsumer (0.9 / 0.10) spawns a separate thread that operates Kafka&apos;s consumer.&lt;br/&gt;
    That thread ws shielded from interrupts, because the Kafka Consumer has not been handling thread interrupts well.&lt;br/&gt;
    Since that thread was also the thread that emitted records, it would block in the network stack (backpressure) or in chained operators.&lt;br/&gt;
    The later case lead to situations where cancellations got very slow unless that thread would be interrupted (which it could not be).&lt;/p&gt;

&lt;p&gt;    This commit changes the thread model:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;A spawned consumer thread polls a batch or records from the KafkaConsumer and pushes the&lt;br/&gt;
        batch of records into a blocking queue (size one)&lt;/li&gt;
	&lt;li&gt;The main thread of the task will pull the record batches from the blocking queue and&lt;br/&gt;
        emit the records.&lt;/li&gt;
&lt;/ul&gt;


&lt;hr /&gt;</comment>
                            <comment id="15657652" author="githubbot" created="Fri, 11 Nov 2016 17:57:11 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87621534&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87621534&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Handover.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,136 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.annotation.VisibleForTesting;&lt;br/&gt;
    +import org.apache.flink.util.ExceptionUtils;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +&lt;br/&gt;
    +import javax.annotation.Nonnull;&lt;br/&gt;
    +import java.io.Closeable;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +public final class Handover implements Closeable {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Would be great if this class has some Javadoc too &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/wink.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15657653" author="githubbot" created="Fri, 11 Nov 2016 17:57:11 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87624670&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87624670&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Handover.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,136 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.annotation.VisibleForTesting;&lt;br/&gt;
    +import org.apache.flink.util.ExceptionUtils;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +&lt;br/&gt;
    +import javax.annotation.Nonnull;&lt;br/&gt;
    +import java.io.Closeable;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +public final class Handover implements Closeable {&lt;br/&gt;
    +&lt;br/&gt;
    +	private final Object lock = new Object();&lt;br/&gt;
    +&lt;br/&gt;
    +	private ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; next;&lt;br/&gt;
    +	private Throwable error;&lt;br/&gt;
    +	private boolean wakeup;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Can we rename this to perhaps `producerWakeup` ? It&apos;ll be less confusing if it only affects the producer side of the handover.&lt;/p&gt;</comment>
                            <comment id="15657654" author="githubbot" created="Fri, 11 Nov 2016 17:57:11 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87621606&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87621606&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Handover.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,136 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.annotation.VisibleForTesting;&lt;br/&gt;
    +import org.apache.flink.util.ExceptionUtils;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +&lt;br/&gt;
    +import javax.annotation.Nonnull;&lt;br/&gt;
    +import java.io.Closeable;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +public final class Handover implements Closeable {&lt;br/&gt;
    +&lt;br/&gt;
    +	private final Object lock = new Object();&lt;br/&gt;
    +&lt;br/&gt;
    +	private ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; next;&lt;br/&gt;
    +	private Throwable error;&lt;br/&gt;
    +	private boolean wakeup;&lt;br/&gt;
    +&lt;br/&gt;
    +	@Nonnull&lt;br/&gt;
    +	public ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; pollNext() throws Exception {&lt;br/&gt;
    +		synchronized (lock) {&lt;br/&gt;
    +			while (next == null &amp;amp;&amp;amp; error == null) &lt;/p&gt;
{
    +				lock.wait();
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; n = next;&lt;br/&gt;
    +			if (n != null) {
    +				next = null;
    +				lock.notifyAll();
    +				return n;
    +			}&lt;br/&gt;
    +			else {
    +				ExceptionUtils.rethrowException(error, error.getMessage());
    +
    +				// this statement cannot be reached since the above method always throws an exception
    +				// this is only here to silence the compiler and any warnings
    +				return ConsumerRecords.empty(); 
    +			}&lt;br/&gt;
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	public void produce(final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; element)&lt;br/&gt;
    +			throws InterruptedException, WakeupException, ClosedException {&lt;br/&gt;
    +&lt;br/&gt;
    +		checkNotNull(element);&lt;br/&gt;
    +&lt;br/&gt;
    +		synchronized (lock) {&lt;br/&gt;
    +			while (next != null &amp;amp;&amp;amp; !wakeup) {    +				lock.wait();    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			wakeup = false;&lt;br/&gt;
    +&lt;br/&gt;
    +			// if there is still an element, we must have been woken up&lt;br/&gt;
    +			if (next != null) &lt;/p&gt;
{
    +				throw new WakeupException();
    +			}
&lt;p&gt;    +			// if there is no error, then this is open and can accept this element&lt;br/&gt;
    +			else if (error == null) &lt;/p&gt;
{
    +				next = element;
    +				lock.notifyAll();
    +			}
&lt;p&gt;    +			// an error marks this as closed for the producer&lt;br/&gt;
    +			else &lt;/p&gt;
{
    +				throw new ClosedException();
    +			}
&lt;p&gt;    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	public void reportError(Throwable t) {&lt;br/&gt;
    +		checkNotNull(t);&lt;br/&gt;
    +&lt;br/&gt;
    +		synchronized (lock) {&lt;br/&gt;
    +			// do not override the initial exception&lt;br/&gt;
    +			if (error == null) &lt;/p&gt;
{
    +				error = t;
    +			}
&lt;p&gt;    +			next = null;&lt;br/&gt;
    +			lock.notifyAll();&lt;br/&gt;
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void close() {&lt;br/&gt;
    +		synchronized (lock) {&lt;br/&gt;
    +			next = null;&lt;br/&gt;
    +			wakeup = false;&lt;br/&gt;
    +&lt;br/&gt;
    +			if (error == null) &lt;/p&gt;
{
    +				error = new ClosedException();
    +			}
&lt;p&gt;    +			lock.notifyAll();&lt;br/&gt;
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	public void wakeupProducer() {&lt;br/&gt;
    +		synchronized (lock) &lt;/p&gt;
{
    +			wakeup = true;
    +			lock.notifyAll();
    +		}
&lt;p&gt;    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@VisibleForTesting&lt;br/&gt;
    +	Object getLock() {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    This method doesn&apos;t seem to be used, even in the tests.&lt;/p&gt;</comment>
                            <comment id="15657655" author="githubbot" created="Fri, 11 Nov 2016 17:57:11 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87626228&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87626228&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -66,36 +57,15 @@&lt;br/&gt;
     	/** The schema to convert between Kafka&apos;s byte messages, and Flink&apos;s objects */&lt;br/&gt;
     	private final KeyedDeserializationSchema&amp;lt;T&amp;gt; deserializer;&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/** The configuration for the Kafka consumer */&lt;/li&gt;
	&lt;li&gt;private final Properties kafkaProperties;&lt;br/&gt;
    +	/** The handover of data and exceptions between the consumer thread and the task thread */&lt;br/&gt;
    +	private final Handover handover;&lt;/li&gt;
&lt;/ul&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;/** The maximum number of milliseconds to wait for a fetch batch */&lt;/li&gt;
	&lt;li&gt;private final long pollTimeout;&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;/** The next offsets that the main thread should commit */&lt;/li&gt;
	&lt;li&gt;private final AtomicReference&amp;lt;Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt;&amp;gt; nextOffsetsToCommit;&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;/** The callback invoked by Kafka once an offset commit is complete */&lt;/li&gt;
	&lt;li&gt;private final OffsetCommitCallback offsetCommitCallback;&lt;br/&gt;
    -&lt;/li&gt;
	&lt;li&gt;/** Reference to the Kafka consumer, once it is created */&lt;/li&gt;
	&lt;li&gt;private volatile KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;/li&gt;
	&lt;li&gt;&lt;/li&gt;
	&lt;li&gt;/** Reference to the proxy, forwarding exceptions from the fetch thread to the main thread */&lt;/li&gt;
	&lt;li&gt;private volatile ExceptionProxy errorHandler;&lt;br/&gt;
    +	/** The thread that runs the proper KafkaConsumer and hand the record batches to this fetcher */
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    nit: &apos;proper&apos; confused me a  bit at first. Perhaps &apos;actual&apos;?&lt;/p&gt;</comment>
                            <comment id="15657656" author="githubbot" created="Fri, 11 Nov 2016 17:57:11 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87621040&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87621040&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,325 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.metrics.MetricGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricWrapper;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.Metric;&lt;br/&gt;
    +import org.apache.kafka.common.MetricName;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +import org.apache.kafka.common.errors.WakeupException;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.slf4j.Logger;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The thread the runs the &lt;/p&gt;
{@link KafkaConsumer}
&lt;p&gt;, connecting to the brokers and polling records.&lt;br/&gt;
    + * The thread pushes the data into a &lt;/p&gt;
{@link Handover}
&lt;p&gt; to be picked up by the fetcher that will&lt;br/&gt;
    + * deserialize and emit the records.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;IMPORTANT:&amp;lt;/b&amp;gt; This thread must not be interrupted when attempting to shut it down.&lt;br/&gt;
    + * The Kafka consumer code was found to not always handle interrupts well, and to even&lt;br/&gt;
    + * deadlock in certain situations.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;Implementation Note: This code is written to be reusable in later versions of the KafkaConsumer.&lt;br/&gt;
    + * Because Kafka is not maintaining binary compatibility, we use a &quot;call bridge&quot; as an indirection&lt;br/&gt;
    + * to the KafkaConsumer calls that change signature.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaConsumerThread extends Thread {&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Logger for this consumer */&lt;br/&gt;
    +	final Logger log;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Private?&lt;/p&gt;</comment>
                            <comment id="15657657" author="githubbot" created="Fri, 11 Nov 2016 17:57:11 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87625136&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87625136&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Handover.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,136 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.annotation.VisibleForTesting;&lt;br/&gt;
    +import org.apache.flink.util.ExceptionUtils;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +&lt;br/&gt;
    +import javax.annotation.Nonnull;&lt;br/&gt;
    +import java.io.Closeable;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +public final class Handover implements Closeable {&lt;br/&gt;
    +&lt;br/&gt;
    +	private final Object lock = new Object();&lt;br/&gt;
    +&lt;br/&gt;
    +	private ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; next;&lt;br/&gt;
    +	private Throwable error;&lt;br/&gt;
    +	private boolean wakeup;&lt;br/&gt;
    +&lt;br/&gt;
    +	@Nonnull&lt;br/&gt;
    +	public ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; pollNext() throws Exception {&lt;br/&gt;
    +		synchronized (lock) {&lt;br/&gt;
    +			while (next == null &amp;amp;&amp;amp; error == null) &lt;/p&gt;
{
    +				lock.wait();
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; n = next;&lt;br/&gt;
    +			if (n != null) {
    +				next = null;
    +				lock.notifyAll();
    +				return n;
    +			}&lt;br/&gt;
    +			else {
    +				ExceptionUtils.rethrowException(error, error.getMessage());
    +
    +				// this statement cannot be reached since the above method always throws an exception
    +				// this is only here to silence the compiler and any warnings
    +				return ConsumerRecords.empty(); 
    +			}&lt;br/&gt;
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	public void produce(final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; element)&lt;br/&gt;
    +			throws InterruptedException, WakeupException, ClosedException {&lt;br/&gt;
    +&lt;br/&gt;
    +		checkNotNull(element);&lt;br/&gt;
    +&lt;br/&gt;
    +		synchronized (lock) {&lt;br/&gt;
    +			while (next != null &amp;amp;&amp;amp; !wakeup) {    +				lock.wait();    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			wakeup = false;&lt;br/&gt;
    +&lt;br/&gt;
    +			// if there is still an element, we must have been woken up&lt;br/&gt;
    +			if (next != null) &lt;/p&gt;
{
    +				throw new WakeupException();
    +			}
&lt;p&gt;    +			// if there is no error, then this is open and can accept this element&lt;br/&gt;
    +			else if (error == null) &lt;/p&gt;
{
    +				next = element;
    +				lock.notifyAll();
    +			}
&lt;p&gt;    +			// an error marks this as closed for the producer&lt;br/&gt;
    +			else &lt;/p&gt;
{
    +				throw new ClosedException();
    +			}
&lt;p&gt;    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	public void reportError(Throwable t) {&lt;br/&gt;
    +		checkNotNull(t);&lt;br/&gt;
    +&lt;br/&gt;
    +		synchronized (lock) {&lt;br/&gt;
    +			// do not override the initial exception&lt;br/&gt;
    +			if (error == null) &lt;/p&gt;
{
    +				error = t;
    +			}
&lt;p&gt;    +			next = null;&lt;br/&gt;
    +			lock.notifyAll();&lt;br/&gt;
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void close() {&lt;br/&gt;
    +		synchronized (lock) {&lt;br/&gt;
    +			next = null;&lt;br/&gt;
    +			wakeup = false;&lt;br/&gt;
    +&lt;br/&gt;
    +			if (error == null) &lt;/p&gt;
{
    +				error = new ClosedException();
    +			}
&lt;p&gt;    +			lock.notifyAll();&lt;br/&gt;
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	public void wakeupProducer() {&lt;br/&gt;
    +		synchronized (lock) &lt;/p&gt;
{
    +			wakeup = true;
    +			lock.notifyAll();
    +		}
&lt;p&gt;    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@VisibleForTesting&lt;br/&gt;
    +	Object getLock() &lt;/p&gt;
{
    +		return lock;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	public static final class ClosedException extends IllegalStateException {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Not really sure if extending `IllegalStateException` is good here, because `ClosedException` will be rethrown to the fetcher even on a normal call to `Handover#close()`.&lt;/p&gt;

&lt;p&gt;    I understand it&apos;s to allow the cancellation process be faster, but somehow I think a normal `close()` &lt;em&gt;after&lt;/em&gt; `poll()` was called doesn&apos;t add up to me as an illegal state.&lt;/p&gt;</comment>
                            <comment id="15662974" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87744290&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87744290&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,325 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.metrics.MetricGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricWrapper;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.Metric;&lt;br/&gt;
    +import org.apache.kafka.common.MetricName;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +import org.apache.kafka.common.errors.WakeupException;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.slf4j.Logger;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The thread the runs the &lt;/p&gt;
{@link KafkaConsumer}
&lt;p&gt;, connecting to the brokers and polling records.&lt;br/&gt;
    + * The thread pushes the data into a &lt;/p&gt;
{@link Handover}
&lt;p&gt; to be picked up by the fetcher that will&lt;br/&gt;
    + * deserialize and emit the records.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;IMPORTANT:&amp;lt;/b&amp;gt; This thread must not be interrupted when attempting to shut it down.&lt;br/&gt;
    + * The Kafka consumer code was found to not always handle interrupts well, and to even&lt;br/&gt;
    + * deadlock in certain situations.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;Implementation Note: This code is written to be reusable in later versions of the KafkaConsumer.&lt;br/&gt;
    + * Because Kafka is not maintaining binary compatibility, we use a &quot;call bridge&quot; as an indirection&lt;br/&gt;
    + * to the KafkaConsumer calls that change signature.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaConsumerThread extends Thread {&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Logger for this consumer */&lt;br/&gt;
    +	final Logger log;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The handover of data and exceptions between the consumer thread and the task thread */&lt;br/&gt;
    +	private final Handover handover;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The next offsets that the main thread should commit */&lt;br/&gt;
    +	private final AtomicReference&amp;lt;Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt;&amp;gt; nextOffsetsToCommit;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The configuration for the Kafka consumer */&lt;br/&gt;
    +	private final Properties kafkaProperties;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The partitions that this consumer reads from */ &lt;br/&gt;
    +	private final KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt;[] subscribedPartitions;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** We get this from the outside to publish metrics. **/&lt;br/&gt;
    +	private final MetricGroup kafkaMetricGroup;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The indirections on KafkaConsumer methods, for cases where KafkaConsumer compatibility is broken */&lt;br/&gt;
    +	private final KafkaConsumerCallBridge consumerCallBridge;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The maximum number of milliseconds to wait for a fetch batch */&lt;br/&gt;
    +	private final long pollTimeout;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag whether to add Kafka&apos;s metrics to the Flink metrics */&lt;br/&gt;
    +	private final boolean useMetrics;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Reference to the Kafka consumer, once it is created */&lt;br/&gt;
    +	private volatile KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag to mark the main work loop as alive */&lt;br/&gt;
    +	private volatile boolean running;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag tracking whether the latest commit request has completed */&lt;br/&gt;
    +	private volatile boolean commitInProgress;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +	public KafkaConsumerThread(&lt;br/&gt;
    +			Logger log,&lt;br/&gt;
    +			Handover handover,&lt;br/&gt;
    +			Properties kafkaProperties,&lt;br/&gt;
    +			KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt;[] subscribedPartitions,&lt;br/&gt;
    +			MetricGroup kafkaMetricGroup,&lt;br/&gt;
    +			KafkaConsumerCallBridge consumerCallBridge,&lt;br/&gt;
    +			String threadName,&lt;br/&gt;
    +			long pollTimeout,&lt;br/&gt;
    +			boolean useMetrics) &lt;/p&gt;
{
    +
    +		super(threadName);
    +		setDaemon(true);
    +
    +		this.log = checkNotNull(log);
    +		this.handover = checkNotNull(handover);
    +		this.kafkaProperties = checkNotNull(kafkaProperties);
    +		this.subscribedPartitions = checkNotNull(subscribedPartitions);
    +		this.kafkaMetricGroup = checkNotNull(kafkaMetricGroup);
    +		this.consumerCallBridge = checkNotNull(consumerCallBridge);
    +		this.pollTimeout = pollTimeout;
    +		this.useMetrics = useMetrics;
    +
    +		this.nextOffsetsToCommit = new AtomicReference&amp;lt;&amp;gt;();
    +		this.running = true;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void run() {&lt;br/&gt;
    +		// early exit check&lt;br/&gt;
    +		if (!running) &lt;/p&gt;
{
    +			return;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		// this is the means to talk to FlinkKafkaConsumer&apos;s main thread&lt;br/&gt;
    +		final Handover handover = this.handover;&lt;br/&gt;
    +&lt;br/&gt;
    +		// This method initializes the KafkaConsumer and guarantees it is torn down properly.&lt;br/&gt;
    +		// This is important, because the consumer has multi-threading issues,&lt;br/&gt;
    +		// including concurrent &apos;close()&apos; calls.&lt;br/&gt;
    +		final KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			consumer = new KafkaConsumer&amp;lt;&amp;gt;(kafkaProperties);
    +		}
&lt;p&gt;    +		catch (Throwable t) &lt;/p&gt;
{
    +			handover.reportError(t);
    +			return;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		// from here on, the consumer is guaranteed to be closed properly&lt;br/&gt;
    +		try {&lt;br/&gt;
    +			// The callback invoked by Kafka once an offset commit is complete&lt;br/&gt;
    +			final OffsetCommitCallback offsetCommitCallback = new CommitCallback();&lt;br/&gt;
    +&lt;br/&gt;
    +			// tell the consumer which partitions to work with&lt;br/&gt;
    +			consumerCallBridge.assignPartitions(consumer, convertKafkaPartitions(subscribedPartitions));&lt;br/&gt;
    +&lt;br/&gt;
    +			// register Kafka&apos;s very own metrics in Flink&apos;s metric reporters&lt;br/&gt;
    +			if (useMetrics) {&lt;br/&gt;
    +				// register Kafka metrics to Flink&lt;br/&gt;
    +				Map&amp;lt;MetricName, ? extends Metric&amp;gt; metrics = consumer.metrics();&lt;br/&gt;
    +				if (metrics == null) &lt;/p&gt;
{
    +					// MapR&apos;s Kafka implementation returns null here.
    +					log.info(&quot;Consumer implementation does not support metrics&quot;);
    +				}
&lt;p&gt; else {&lt;br/&gt;
    +					// we have Kafka metrics, register them&lt;br/&gt;
    +					for (Map.Entry&amp;lt;MetricName, ? extends Metric&amp;gt; metric: metrics.entrySet()) &lt;/p&gt;
{
    +						kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()));
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			// early exit check&lt;br/&gt;
    +			if (!running) &lt;/p&gt;
{
    +				return;
    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			// seek the consumer to the initial offsets&lt;br/&gt;
    +			for (KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition : subscribedPartitions) {&lt;br/&gt;
    +				if (partition.isOffsetDefined()) {&lt;br/&gt;
    +					log.info(&quot;Partition {} has restored initial offsets {} from checkpoint / savepoint; &quot; +&lt;br/&gt;
    +							&quot;seeking the consumer to position {}&quot;,&lt;br/&gt;
    +							partition.getKafkaPartitionHandle(), partition.getOffset(), partition.getOffset() + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +					consumer.seek(partition.getKafkaPartitionHandle(), partition.getOffset() + 1);&lt;br/&gt;
    +				}&lt;br/&gt;
    +				else {&lt;br/&gt;
    +					// for partitions that do not have offsets restored from a checkpoint/savepoint,&lt;br/&gt;
    +					// we need to define our internal offset state for them using the initial offsets retrieved from Kafka&lt;br/&gt;
    +					// by the KafkaConsumer, so that they are correctly checkpointed and committed on the next checkpoint&lt;br/&gt;
    +&lt;br/&gt;
    +					long fetchedOffset = consumer.position(partition.getKafkaPartitionHandle());&lt;br/&gt;
    +&lt;br/&gt;
    +					log.info(&quot;Partition {} has no initial offset; the consumer has position {}, &quot; +&lt;br/&gt;
    +							&quot;so the initial offset will be set to {}&quot;,&lt;br/&gt;
    +							partition.getKafkaPartitionHandle(), fetchedOffset, fetchedOffset - 1);&lt;br/&gt;
    +&lt;br/&gt;
    +					// the fetched offset represents the next record to process, so we need to subtract it by 1&lt;br/&gt;
    +					partition.setOffset(fetchedOffset - 1);&lt;br/&gt;
    +				}&lt;br/&gt;
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			// from now on, external operations may call the consumer&lt;br/&gt;
    +			this.consumer = consumer;&lt;br/&gt;
    +&lt;br/&gt;
    +			// the latest bulk of records. may carry across the loop if the thread is woken up&lt;br/&gt;
    +			// from blocking on the handover&lt;br/&gt;
    +			ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records = null;&lt;br/&gt;
    +&lt;br/&gt;
    +			// main fetch loop&lt;br/&gt;
    +			while (running) {&lt;br/&gt;
    +&lt;br/&gt;
    +				// check if there is something to commit&lt;br/&gt;
    +				if (!commitInProgress) {&lt;br/&gt;
    +					// get and reset the work-to-be committed, so we don&apos;t repeatedly commit the same&lt;br/&gt;
    +					final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; toCommit = nextOffsetsToCommit.getAndSet(null);&lt;br/&gt;
    +&lt;br/&gt;
    +					if (toCommit != null) &lt;/p&gt;
{
    +						log.debug(&quot;Sending async offset commit request to Kafka broker&quot;);
    +
    +						// also record that a commit is already in progress
    +						// the order here matters! first set the flag, then send the commit command.
    +						commitInProgress = true;
    +						consumer.commitAsync(toCommit, offsetCommitCallback);
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				// get the next batch of records, unless we did not manage to hand the old batch over&lt;br/&gt;
    +				if (records == null) {&lt;br/&gt;
    +					try &lt;/p&gt;
{
    +						records = consumer.poll(pollTimeout);
    +					}
&lt;p&gt;    +					catch (WakeupException we) &lt;/p&gt;
{
    +						continue;
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					handover.produce(records);
    +					records = null;
    +				}
&lt;p&gt;    +				catch (Handover.WakeupException e) &lt;/p&gt;
{
    +					// fall through the loop
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +			// end main fetch loop&lt;br/&gt;
    +		}&lt;br/&gt;
    +		catch (Throwable t) &lt;/p&gt;
{
    +			// let the main thread know and exit
    +			// it may be that this exception comes because the main thread closed the handover, in
    +			// which case the below reporting is irrelevant, but does not hurt either
    +			handover.reportError(t);
    +		}
&lt;p&gt;    +		finally {&lt;br/&gt;
    +			try &lt;/p&gt;
{
    +				consumer.close();
    +			}
&lt;p&gt;    +			catch (Throwable t) &lt;/p&gt;
{
    +				log.warn(&quot;Error while closing Kafka consumer&quot;, t);
    +			}
&lt;p&gt;    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Shuts this thread down, waking up the thread gracefully if blocked.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public void shutdown() {&lt;br/&gt;
    +		running = false;&lt;br/&gt;
    +&lt;br/&gt;
    +		// this wakes up the consumer if it is blocked handing over records&lt;br/&gt;
    +		handover.close();&lt;br/&gt;
    +&lt;br/&gt;
    +		// this wakes up the consumer if it is blocked in a kafka poll &lt;br/&gt;
    +		if (consumer != null) {&lt;br/&gt;
    +			consumer.wakeup();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    There was a notice comment before in the code, on calling `close()` on the consumer:&lt;/p&gt;

&lt;p&gt;    &amp;gt; //   - We cannot call close() on the consumer, because it will actually throw&lt;br/&gt;
    &amp;gt; //     an exception if a concurrent call is in progress&lt;/p&gt;

&lt;p&gt;    Perhaps it is reasonable to note that here too, to only use `wakeup`, and let the consumer safely close in the `finally` clause, to let others know so we don&apos;t get confusing exceptions rethrown to the fetcher thread via the handover.&lt;/p&gt;</comment>
                            <comment id="15662975" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87739024&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87739024&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,325 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.metrics.MetricGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricWrapper;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.Metric;&lt;br/&gt;
    +import org.apache.kafka.common.MetricName;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +import org.apache.kafka.common.errors.WakeupException;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.slf4j.Logger;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The thread the runs the &lt;/p&gt;
{@link KafkaConsumer}
&lt;p&gt;, connecting to the brokers and polling records.&lt;br/&gt;
    + * The thread pushes the data into a &lt;/p&gt;
{@link Handover}
&lt;p&gt; to be picked up by the fetcher that will&lt;br/&gt;
    + * deserialize and emit the records.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;IMPORTANT:&amp;lt;/b&amp;gt; This thread must not be interrupted when attempting to shut it down.&lt;br/&gt;
    + * The Kafka consumer code was found to not always handle interrupts well, and to even&lt;br/&gt;
    + * deadlock in certain situations.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;Implementation Note: This code is written to be reusable in later versions of the KafkaConsumer.&lt;br/&gt;
    + * Because Kafka is not maintaining binary compatibility, we use a &quot;call bridge&quot; as an indirection&lt;br/&gt;
    + * to the KafkaConsumer calls that change signature.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaConsumerThread extends Thread {&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Logger for this consumer */&lt;br/&gt;
    +	final Logger log;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The handover of data and exceptions between the consumer thread and the task thread */&lt;br/&gt;
    +	private final Handover handover;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The next offsets that the main thread should commit */&lt;br/&gt;
    +	private final AtomicReference&amp;lt;Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt;&amp;gt; nextOffsetsToCommit;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The configuration for the Kafka consumer */&lt;br/&gt;
    +	private final Properties kafkaProperties;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The partitions that this consumer reads from */ &lt;br/&gt;
    +	private final KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt;[] subscribedPartitions;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** We get this from the outside to publish metrics. **/&lt;br/&gt;
    +	private final MetricGroup kafkaMetricGroup;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The indirections on KafkaConsumer methods, for cases where KafkaConsumer compatibility is broken */&lt;br/&gt;
    +	private final KafkaConsumerCallBridge consumerCallBridge;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The maximum number of milliseconds to wait for a fetch batch */&lt;br/&gt;
    +	private final long pollTimeout;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag whether to add Kafka&apos;s metrics to the Flink metrics */&lt;br/&gt;
    +	private final boolean useMetrics;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Reference to the Kafka consumer, once it is created */&lt;br/&gt;
    +	private volatile KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag to mark the main work loop as alive */&lt;br/&gt;
    +	private volatile boolean running;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag tracking whether the latest commit request has completed */&lt;br/&gt;
    +	private volatile boolean commitInProgress;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +	public KafkaConsumerThread(&lt;br/&gt;
    +			Logger log,&lt;br/&gt;
    +			Handover handover,&lt;br/&gt;
    +			Properties kafkaProperties,&lt;br/&gt;
    +			KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt;[] subscribedPartitions,&lt;br/&gt;
    +			MetricGroup kafkaMetricGroup,&lt;br/&gt;
    +			KafkaConsumerCallBridge consumerCallBridge,&lt;br/&gt;
    +			String threadName,&lt;br/&gt;
    +			long pollTimeout,&lt;br/&gt;
    +			boolean useMetrics) &lt;/p&gt;
{
    +
    +		super(threadName);
    +		setDaemon(true);
    +
    +		this.log = checkNotNull(log);
    +		this.handover = checkNotNull(handover);
    +		this.kafkaProperties = checkNotNull(kafkaProperties);
    +		this.subscribedPartitions = checkNotNull(subscribedPartitions);
    +		this.kafkaMetricGroup = checkNotNull(kafkaMetricGroup);
    +		this.consumerCallBridge = checkNotNull(consumerCallBridge);
    +		this.pollTimeout = pollTimeout;
    +		this.useMetrics = useMetrics;
    +
    +		this.nextOffsetsToCommit = new AtomicReference&amp;lt;&amp;gt;();
    +		this.running = true;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void run() {&lt;br/&gt;
    +		// early exit check&lt;br/&gt;
    +		if (!running) &lt;/p&gt;
{
    +			return;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		// this is the means to talk to FlinkKafkaConsumer&apos;s main thread&lt;br/&gt;
    +		final Handover handover = this.handover;&lt;br/&gt;
    +&lt;br/&gt;
    +		// This method initializes the KafkaConsumer and guarantees it is torn down properly.&lt;br/&gt;
    +		// This is important, because the consumer has multi-threading issues,&lt;br/&gt;
    +		// including concurrent &apos;close()&apos; calls.&lt;br/&gt;
    +		final KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			consumer = new KafkaConsumer&amp;lt;&amp;gt;(kafkaProperties);
    +		}
&lt;p&gt;    +		catch (Throwable t) &lt;/p&gt;
{
    +			handover.reportError(t);
    +			return;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		// from here on, the consumer is guaranteed to be closed properly&lt;br/&gt;
    +		try {&lt;br/&gt;
    +			// The callback invoked by Kafka once an offset commit is complete&lt;br/&gt;
    +			final OffsetCommitCallback offsetCommitCallback = new CommitCallback();&lt;br/&gt;
    +&lt;br/&gt;
    +			// tell the consumer which partitions to work with&lt;br/&gt;
    +			consumerCallBridge.assignPartitions(consumer, convertKafkaPartitions(subscribedPartitions));&lt;br/&gt;
    +&lt;br/&gt;
    +			// register Kafka&apos;s very own metrics in Flink&apos;s metric reporters&lt;br/&gt;
    +			if (useMetrics) {&lt;br/&gt;
    +				// register Kafka metrics to Flink&lt;br/&gt;
    +				Map&amp;lt;MetricName, ? extends Metric&amp;gt; metrics = consumer.metrics();&lt;br/&gt;
    +				if (metrics == null) &lt;/p&gt;
{
    +					// MapR&apos;s Kafka implementation returns null here.
    +					log.info(&quot;Consumer implementation does not support metrics&quot;);
    +				}
&lt;p&gt; else {&lt;br/&gt;
    +					// we have Kafka metrics, register them&lt;br/&gt;
    +					for (Map.Entry&amp;lt;MetricName, ? extends Metric&amp;gt; metric: metrics.entrySet()) &lt;/p&gt;
{
    +						kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()));
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			// early exit check&lt;br/&gt;
    +			if (!running) &lt;/p&gt;
{
    +				return;
    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			// seek the consumer to the initial offsets&lt;br/&gt;
    +			for (KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition : subscribedPartitions) {&lt;br/&gt;
    +				if (partition.isOffsetDefined()) {&lt;br/&gt;
    +					log.info(&quot;Partition {} has restored initial offsets {} from checkpoint / savepoint; &quot; +&lt;br/&gt;
    +							&quot;seeking the consumer to position {}&quot;,&lt;br/&gt;
    +							partition.getKafkaPartitionHandle(), partition.getOffset(), partition.getOffset() + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +					consumer.seek(partition.getKafkaPartitionHandle(), partition.getOffset() + 1);&lt;br/&gt;
    +				}&lt;br/&gt;
    +				else {&lt;br/&gt;
    +					// for partitions that do not have offsets restored from a checkpoint/savepoint,&lt;br/&gt;
    +					// we need to define our internal offset state for them using the initial offsets retrieved from Kafka&lt;br/&gt;
    +					// by the KafkaConsumer, so that they are correctly checkpointed and committed on the next checkpoint&lt;br/&gt;
    +&lt;br/&gt;
    +					long fetchedOffset = consumer.position(partition.getKafkaPartitionHandle());&lt;br/&gt;
    +&lt;br/&gt;
    +					log.info(&quot;Partition {} has no initial offset; the consumer has position {}, &quot; +&lt;br/&gt;
    +							&quot;so the initial offset will be set to {}&quot;,&lt;br/&gt;
    +							partition.getKafkaPartitionHandle(), fetchedOffset, fetchedOffset - 1);&lt;br/&gt;
    +&lt;br/&gt;
    +					// the fetched offset represents the next record to process, so we need to subtract it by 1&lt;br/&gt;
    +					partition.setOffset(fetchedOffset - 1);&lt;br/&gt;
    +				}&lt;br/&gt;
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			// from now on, external operations may call the consumer&lt;br/&gt;
    +			this.consumer = consumer;&lt;br/&gt;
    +&lt;br/&gt;
    +			// the latest bulk of records. may carry across the loop if the thread is woken up&lt;br/&gt;
    +			// from blocking on the handover&lt;br/&gt;
    +			ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records = null;&lt;br/&gt;
    +&lt;br/&gt;
    +			// main fetch loop&lt;br/&gt;
    +			while (running) {&lt;br/&gt;
    +&lt;br/&gt;
    +				// check if there is something to commit&lt;br/&gt;
    +				if (!commitInProgress) {&lt;br/&gt;
    +					// get and reset the work-to-be committed, so we don&apos;t repeatedly commit the same&lt;br/&gt;
    +					final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; toCommit = nextOffsetsToCommit.getAndSet(null);&lt;br/&gt;
    +&lt;br/&gt;
    +					if (toCommit != null) &lt;/p&gt;
{
    +						log.debug(&quot;Sending async offset commit request to Kafka broker&quot;);
    +
    +						// also record that a commit is already in progress
    +						// the order here matters! first set the flag, then send the commit command.
    +						commitInProgress = true;
    +						consumer.commitAsync(toCommit, offsetCommitCallback);
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				// get the next batch of records, unless we did not manage to hand the old batch over&lt;br/&gt;
    +				if (records == null) {&lt;br/&gt;
    +					try &lt;/p&gt;
{
    +						records = consumer.poll(pollTimeout);
    +					}
&lt;p&gt;    +					catch (WakeupException we) &lt;/p&gt;
{
    +						continue;
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					handover.produce(records);
    +					records = null;
    +				}
&lt;p&gt;    +				catch (Handover.WakeupException e) &lt;/p&gt;
{
    +					// fall through the loop
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +			// end main fetch loop&lt;br/&gt;
    +		}&lt;br/&gt;
    +		catch (Throwable t) &lt;/p&gt;
{
    +			// let the main thread know and exit
    +			// it may be that this exception comes because the main thread closed the handover, in
    +			// which case the below reporting is irrelevant, but does not hurt either
    +			handover.reportError(t);
    +		}
&lt;p&gt;    +		finally {&lt;br/&gt;
    +			try {&lt;br/&gt;
    +				consumer.close();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I suggest to do `handover.close()` here instead of in `shutdown()`, for clearer shutdown flow. Please see the inline comment in the `shutdown()` method.&lt;/p&gt;</comment>
                            <comment id="15662976" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87735824&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87735824&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaShortRetention010ITCase.java &amp;#8212;&lt;br/&gt;
    @@ -1,34 +0,0 @@&lt;br/&gt;
    -/*&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements.  See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License.  You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
    -&lt;br/&gt;
    -package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    -&lt;br/&gt;
    -import org.junit.Test;&lt;br/&gt;
    -&lt;br/&gt;
    -@SuppressWarnings(&quot;serial&quot;)&lt;br/&gt;
    -public class KafkaShortRetention010ITCase extends KafkaShortRetentionTestBase {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    &lt;ins&gt;1 to remove these tests for 0.9&lt;/ins&gt; connectors.&lt;/p&gt;</comment>
                            <comment id="15662977" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87745778&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87745778&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaShortRetentionTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -172,8 +173,14 @@ public void cancel() {&lt;/p&gt;

&lt;p&gt;     		// ----------- add consumer dataflow ----------&lt;/p&gt;

&lt;p&gt;    +		// the consumer should only poll very small chunks&lt;br/&gt;
    +		Properties consumerProps = new Properties();&lt;br/&gt;
    +		consumerProps.putAll(standardProps);&lt;br/&gt;
    +		consumerProps.putAll(secureProps);&lt;br/&gt;
    +		consumerProps.setProperty(&quot;fetch.message.max.bytes&quot;, &quot;100&quot;);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I think we shouldn&apos;t be setting `fetch.message.max.bytes` here. The config key for this setting has changed across Kafka versions (for 0.9+ it&apos;s `max.partition.fetch.bytes`). The version-specific `standardProps` already set values for this config.&lt;/p&gt;

&lt;p&gt;    So, the original `props` that only contains `standardProps` and `secureProps` should be enough for the test to work.&lt;/p&gt;</comment>
                            <comment id="15662978" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87734223&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87734223&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerCallBridge010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,40 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The ConsumerCallBridge simply calls the &lt;/p&gt;
{@link KafkaConsumer#assign(java.util.Collection)}
&lt;p&gt; method.&lt;br/&gt;
    + * &lt;br/&gt;
    + * This indirection is necessary, because Kafka broke binary compatibility between 0.9 and 0.10,&lt;br/&gt;
    + * changing &lt;/p&gt;
{@code assign(List)}
&lt;p&gt; to &lt;/p&gt;
{@code assign(Collection)}
&lt;p&gt;.&lt;br/&gt;
    + * &lt;br/&gt;
    + * Because of that, we need to two versions whose compiled code goes against different method signatures.&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    nit: we need &quot;to&quot; two versions &amp;lt;-- redundant &quot;to&quot;.&lt;/p&gt;</comment>
                            <comment id="15662979" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87746504&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87746504&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/HandoverTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,387 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internal.Handover.WakeupException;&lt;br/&gt;
    +import org.apache.flink.util.ExceptionUtils;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.IOException;&lt;br/&gt;
    +import java.util.Random;&lt;br/&gt;
    +import java.util.concurrent.TimeoutException;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.*;&lt;br/&gt;
    +import static org.mockito.Mockito.*;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Tests for the &lt;/p&gt;
{@link Handover}
&lt;p&gt; between Kafka Consumer Thread and the fetcher&apos;s main thread. &lt;br/&gt;
    + */&lt;br/&gt;
    +public class HandoverTest {&lt;br/&gt;
    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +	//  test produce / consumer&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testWithVariableProducer() throws Exception &lt;/p&gt;
{
    +		runProducerConsumerTest(500, 2, 0);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testWithVariableConsumer() throws Exception &lt;/p&gt;
{
    +		runProducerConsumerTest(500, 0, 2);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testWithVariableBoth() throws Exception &lt;/p&gt;
{
    +		runProducerConsumerTest(500, 2, 2);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	private void runProducerConsumerTest(int numRecords, int maxProducerDelay, int maxConsumerDelay) throws Exception {&lt;br/&gt;
    +		// generate test data&lt;br/&gt;
    +		@SuppressWarnings(&lt;/p&gt;
{&quot;unchecked&quot;, &quot;rawtypes&quot;}
&lt;p&gt;)&lt;br/&gt;
    +		final ConsumerRecords&amp;lt;byte[], byte[]&amp;gt;[] data = new ConsumerRecords&lt;span class=&quot;error&quot;&gt;&amp;#91;numRecords&amp;#93;&lt;/span&gt;;&lt;br/&gt;
    +		for (int i = 0; i &amp;lt; numRecords; i++) &lt;/p&gt;
{
    +			data[i] = createTestRecords();
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		final Handover handover = new Handover();&lt;br/&gt;
    +&lt;br/&gt;
    +		ProducerThread producer = new ProducerThread(handover, data, maxProducerDelay);&lt;br/&gt;
    +		ConsumerThread consumer = new ConsumerThread(handover, data, maxConsumerDelay);&lt;br/&gt;
    +&lt;br/&gt;
    +		consumer.start();&lt;br/&gt;
    +		producer.start();&lt;br/&gt;
    +&lt;br/&gt;
    +		// sync first on the consumer, so it propagates assertion errors&lt;br/&gt;
    +		consumer.sync();&lt;br/&gt;
    +		producer.sync();&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +	//  test error propagation&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testPublishErrorOnEmptyHandover() throws Exception {&lt;br/&gt;
    +		final Handover handover = new Handover();&lt;br/&gt;
    +&lt;br/&gt;
    +		Exception error = new Exception();&lt;br/&gt;
    +		handover.reportError(error);&lt;br/&gt;
    +&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			handover.pollNext();
    +			fail(&quot;should throw an exception&quot;);
    +		}&lt;br/&gt;
    +		catch (Exception e) {
    +			assertEquals(error, e);
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testPublishErrorOnFullHandover() throws Exception {&lt;br/&gt;
    +		final Handover handover = new Handover();&lt;br/&gt;
    +		handover.produce(createTestRecords());&lt;br/&gt;
    +&lt;br/&gt;
    +		IOException error = new IOException();&lt;br/&gt;
    +		handover.reportError(error);&lt;br/&gt;
    +&lt;br/&gt;
    +		try {    +			handover.pollNext();    +			fail(&quot;should throw an exception&quot;);    +		}
&lt;p&gt;    +		catch (Exception e) &lt;/p&gt;
{
    +			assertEquals(error, e);
    +		}
&lt;p&gt;    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testExceptionMarksClosedOnEmpty() throws Exception {&lt;br/&gt;
    +		final Handover handover = new Handover();&lt;br/&gt;
    +&lt;br/&gt;
    +		IllegalStateException error = new IllegalStateException();&lt;br/&gt;
    +		handover.reportError(error);&lt;br/&gt;
    +&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			handover.produce(createTestRecords());
    +			fail(&quot;should throw an exception&quot;);
    +		}&lt;br/&gt;
    +		catch (Handover.ClosedException e) {
    +			// expected
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testExceptionMarksClosedOnFull() throws Exception {&lt;br/&gt;
    +		final Handover handover = new Handover();&lt;br/&gt;
    +		handover.produce(createTestRecords());&lt;br/&gt;
    +&lt;br/&gt;
    +		LinkageError error = new LinkageError();&lt;br/&gt;
    +		handover.reportError(error);&lt;br/&gt;
    +&lt;br/&gt;
    +		try {    +			handover.produce(createTestRecords());    +			fail(&quot;should throw an exception&quot;);    +		}
&lt;p&gt;    +		catch (Handover.ClosedException e) &lt;/p&gt;
{
    +			// expected
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +	//  test closing behavior&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testCloseEmptyForConsumer() throws Exception {&lt;br/&gt;
    +		final Handover handover = new Handover();&lt;br/&gt;
    +		handover.close();&lt;br/&gt;
    +&lt;br/&gt;
    +		try {
    +			handover.pollNext();
    +			fail(&quot;should throw an exception&quot;);
    +		}&lt;br/&gt;
    +		catch (Handover.ClosedException e) {    +			// expected    +		}
&lt;p&gt;    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testCloseFullForConsumer() throws Exception {&lt;br/&gt;
    +		final Handover handover = new Handover();&lt;br/&gt;
    +		handover.produce(createTestRecords());&lt;br/&gt;
    +		handover.close();&lt;br/&gt;
    +&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			handover.pollNext();
    +			fail(&quot;should throw an exception&quot;);
    +		}
&lt;p&gt;    +		catch (Handover.ClosedException e) &lt;/p&gt;
{
    +			// expected
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testCloseEmptyForProducer() throws Exception {&lt;br/&gt;
    +		final Handover handover = new Handover();&lt;br/&gt;
    +		handover.close();&lt;br/&gt;
    +&lt;br/&gt;
    +		try {
    +			handover.produce(createTestRecords());
    +			fail(&quot;should throw an exception&quot;);
    +		}&lt;br/&gt;
    +		catch (Handover.ClosedException e) {    +			// expected    +		}
&lt;p&gt;    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testCloseFullForProducer() throws Exception {&lt;br/&gt;
    +		final Handover handover = new Handover();&lt;br/&gt;
    +		handover.produce(createTestRecords());&lt;br/&gt;
    +		handover.close();&lt;br/&gt;
    +&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			handover.produce(createTestRecords());
    +			fail(&quot;should throw an exception&quot;);
    +		}&lt;br/&gt;
    +		catch (Handover.ClosedException e) {
    +			// expected
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +	//  test wake up behavior&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testWakeupDoesNotWakeWhenEmpty() throws Exception {&lt;br/&gt;
    +		Handover handover = new Handover();&lt;br/&gt;
    +		handover.wakeupProducer();&lt;br/&gt;
    +&lt;br/&gt;
    +		// produce into a woken but empty handover&lt;br/&gt;
    +		try {
    +			handover.produce(createTestRecords());
    +		}&lt;br/&gt;
    +		catch (Handover.WakeupException e) {
    +			fail();
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		// handover now has records, next time we wakeup and produce it needs&lt;br/&gt;
    +		// to throw an exception&lt;br/&gt;
    +		handover.wakeupProducer();&lt;br/&gt;
    +		try {    +			handover.produce(createTestRecords());    +			fail(&quot;should throw an exception&quot;);    +		}
&lt;p&gt;    +		catch (Handover.WakeupException e) &lt;/p&gt;
{
    +			// expected
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		// empty the handover&lt;br/&gt;
    +		assertNotNull(handover.pollNext());&lt;br/&gt;
    +		&lt;br/&gt;
    +		// producing into an empty handover should work&lt;br/&gt;
    +		try {
    +			handover.produce(createTestRecords());
    +		}&lt;br/&gt;
    +		catch (Handover.WakeupException e) {
    +			fail();
    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testWakeupWakesOnlyOnce() throws Exception {&lt;br/&gt;
    +		// create a full handover&lt;br/&gt;
    +		final Handover handover = new Handover();&lt;br/&gt;
    +		handover.produce(createTestRecords());&lt;br/&gt;
    +&lt;br/&gt;
    +		handover.wakeupProducer();&lt;br/&gt;
    +&lt;br/&gt;
    +		try {
    +			handover.produce(createTestRecords());
    +			fail();
    +		} catch (WakeupException e) {    +			// expected    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		CheckedThread producer = new CheckedThread() {&lt;br/&gt;
    +			@Override&lt;br/&gt;
    +			public void go() throws Exception &lt;/p&gt;
{
    +				handover.produce(createTestRecords());
    +			}
&lt;p&gt;    +		};&lt;br/&gt;
    +		producer.start();&lt;br/&gt;
    +&lt;br/&gt;
    +		// the producer must go blocking&lt;br/&gt;
    +		producer.waitUntilThreadHoldsLock(10000);&lt;br/&gt;
    +&lt;br/&gt;
    +		// release the thread by consuming something&lt;br/&gt;
    +		assertNotNull(handover.pollNext());&lt;br/&gt;
    +		producer.sync();&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +	//  utilities&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    +	static ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; createTestRecords() {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Might as well make this private.&lt;/p&gt;</comment>
                            <comment id="15662980" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87745858&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87745858&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaShortRetentionTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -122,14 +122,15 @@ public static void shutDownServices() {&lt;br/&gt;
     	 *&lt;br/&gt;
     	 */&lt;br/&gt;
     	private static boolean stopProducer = false;&lt;br/&gt;
    +&lt;br/&gt;
     	public void runAutoOffsetResetTest() throws Exception {&lt;br/&gt;
     		final String topic = &quot;auto-offset-reset-test&quot;;&lt;/p&gt;

&lt;p&gt;     		final int parallelism = 1;&lt;br/&gt;
     		final int elementsPerPartition = 50000;&lt;/p&gt;

&lt;p&gt;     		Properties tprops = new Properties();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;tprops.setProperty(&quot;retention.ms&quot;, &quot;250&quot;);&lt;br/&gt;
    +		tprops.setProperty(&quot;retention.ms&quot;, &quot;100&quot;);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Is this change necessary?&lt;/p&gt;</comment>
                            <comment id="15662985" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87738353&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87738353&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -143,133 +123,26 @@ public Kafka09Fetcher(&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void runFetchLoop() throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We will be throwing all exceptions, even if it&apos;s a `Handover.ClosedException`, correct?&lt;/p&gt;

&lt;p&gt;    I wonder if it makes sense to suppress `Handover.ClosedException`s to not reach the main task thread, and only restore the interruption state that follows `cancel()`? So basically, we catch `InterruptedException` on the whole `runFetchLoop()` scope.&lt;/p&gt;

&lt;p&gt;    This was what the exception passing behaviour was like before. Before, when `cancel()` was called on the fetcher, we won&apos;t be throwing any other exceptions, only restoring the interruption state to the main task thread.&lt;/p&gt;</comment>
                            <comment id="15662984" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87744744&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87744744&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/Kafka09FetcherTest.java &amp;#8212;&lt;br/&gt;
    @@ -323,8 +329,154 @@ else if (partition.topic().equals(&quot;another&quot;)) {&lt;/p&gt;

&lt;p&gt;     		// check that there were no errors in the fetcher&lt;br/&gt;
     		final Throwable caughtError = error.get();&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;if (caughtError != null) {&lt;br/&gt;
    +		if (caughtError != null &amp;amp;&amp;amp; !(caughtError instanceof Handover.ClosedException)) {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Perhaps we should be suppressing the fetcher of throwing `Handover.ClosedException`, as it doesn&apos;t really make sense to the main thread. Please see my above comments.&lt;/p&gt;</comment>
                            <comment id="15662981" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87736264&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87736264&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaShortRetention010ITCase.java &amp;#8212;&lt;br/&gt;
    @@ -1,34 +0,0 @@&lt;br/&gt;
    -/*&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Licensed to the Apache Software Foundation (ASF) under one or more&lt;/li&gt;
	&lt;li&gt;* contributor license agreements.  See the NOTICE file distributed with&lt;/li&gt;
	&lt;li&gt;* this work for additional information regarding copyright ownership.&lt;/li&gt;
	&lt;li&gt;* The ASF licenses this file to You under the Apache License, Version 2.0&lt;/li&gt;
	&lt;li&gt;* (the &quot;License&quot;); you may not use this file except in compliance with&lt;/li&gt;
	&lt;li&gt;* the License.  You may obtain a copy of the License at&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;*    &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;/li&gt;
	&lt;li&gt;*&lt;/li&gt;
	&lt;li&gt;* Unless required by applicable law or agreed to in writing, software&lt;/li&gt;
	&lt;li&gt;* distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;/li&gt;
	&lt;li&gt;* WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;/li&gt;
	&lt;li&gt;* See the License for the specific language governing permissions and&lt;/li&gt;
	&lt;li&gt;* limitations under the License.&lt;/li&gt;
	&lt;li&gt;*/&lt;br/&gt;
    -&lt;br/&gt;
    -package org.apache.flink.streaming.connectors.kafka;&lt;br/&gt;
    -&lt;br/&gt;
    -import org.junit.Test;&lt;br/&gt;
    -&lt;br/&gt;
    -@SuppressWarnings(&quot;serial&quot;)&lt;br/&gt;
    -public class KafkaShortRetention010ITCase extends KafkaShortRetentionTestBase {
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I think with this removal, we can also completely remove the `runFailOnAutoOffsetResetNone()` from the `KafkaShortRetentionTestBase`.&lt;/p&gt;

&lt;p&gt;    The 0.8 connector runs `runFailOnAutoOffsetResetNoneEager()` instead of `runFailOnAutoOffsetResetNone()`. I think this is what we actually should also be doing for 0.9+ connectors, testing only the eager version, because that&apos;s a Flink-specific behaviour (just pointing this out, we can add this as a separate future task as this probably requires some some work on 0.9+).&lt;/p&gt;</comment>
                            <comment id="15662982" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87738700&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87738700&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,325 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.metrics.MetricGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricWrapper;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.Metric;&lt;br/&gt;
    +import org.apache.kafka.common.MetricName;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +import org.apache.kafka.common.errors.WakeupException;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.slf4j.Logger;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The thread the runs the &lt;/p&gt;
{@link KafkaConsumer}
&lt;p&gt;, connecting to the brokers and polling records.&lt;br/&gt;
    + * The thread pushes the data into a &lt;/p&gt;
{@link Handover}
&lt;p&gt; to be picked up by the fetcher that will&lt;br/&gt;
    + * deserialize and emit the records.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;IMPORTANT:&amp;lt;/b&amp;gt; This thread must not be interrupted when attempting to shut it down.&lt;br/&gt;
    + * The Kafka consumer code was found to not always handle interrupts well, and to even&lt;br/&gt;
    + * deadlock in certain situations.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;Implementation Note: This code is written to be reusable in later versions of the KafkaConsumer.&lt;br/&gt;
    + * Because Kafka is not maintaining binary compatibility, we use a &quot;call bridge&quot; as an indirection&lt;br/&gt;
    + * to the KafkaConsumer calls that change signature.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaConsumerThread extends Thread {&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Logger for this consumer */&lt;br/&gt;
    +	final Logger log;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The handover of data and exceptions between the consumer thread and the task thread */&lt;br/&gt;
    +	private final Handover handover;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The next offsets that the main thread should commit */&lt;br/&gt;
    +	private final AtomicReference&amp;lt;Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt;&amp;gt; nextOffsetsToCommit;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The configuration for the Kafka consumer */&lt;br/&gt;
    +	private final Properties kafkaProperties;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The partitions that this consumer reads from */ &lt;br/&gt;
    +	private final KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt;[] subscribedPartitions;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** We get this from the outside to publish metrics. **/&lt;br/&gt;
    +	private final MetricGroup kafkaMetricGroup;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The indirections on KafkaConsumer methods, for cases where KafkaConsumer compatibility is broken */&lt;br/&gt;
    +	private final KafkaConsumerCallBridge consumerCallBridge;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The maximum number of milliseconds to wait for a fetch batch */&lt;br/&gt;
    +	private final long pollTimeout;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag whether to add Kafka&apos;s metrics to the Flink metrics */&lt;br/&gt;
    +	private final boolean useMetrics;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Reference to the Kafka consumer, once it is created */&lt;br/&gt;
    +	private volatile KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag to mark the main work loop as alive */&lt;br/&gt;
    +	private volatile boolean running;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag tracking whether the latest commit request has completed */&lt;br/&gt;
    +	private volatile boolean commitInProgress;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +	public KafkaConsumerThread(&lt;br/&gt;
    +			Logger log,&lt;br/&gt;
    +			Handover handover,&lt;br/&gt;
    +			Properties kafkaProperties,&lt;br/&gt;
    +			KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt;[] subscribedPartitions,&lt;br/&gt;
    +			MetricGroup kafkaMetricGroup,&lt;br/&gt;
    +			KafkaConsumerCallBridge consumerCallBridge,&lt;br/&gt;
    +			String threadName,&lt;br/&gt;
    +			long pollTimeout,&lt;br/&gt;
    +			boolean useMetrics) &lt;/p&gt;
{
    +
    +		super(threadName);
    +		setDaemon(true);
    +
    +		this.log = checkNotNull(log);
    +		this.handover = checkNotNull(handover);
    +		this.kafkaProperties = checkNotNull(kafkaProperties);
    +		this.subscribedPartitions = checkNotNull(subscribedPartitions);
    +		this.kafkaMetricGroup = checkNotNull(kafkaMetricGroup);
    +		this.consumerCallBridge = checkNotNull(consumerCallBridge);
    +		this.pollTimeout = pollTimeout;
    +		this.useMetrics = useMetrics;
    +
    +		this.nextOffsetsToCommit = new AtomicReference&amp;lt;&amp;gt;();
    +		this.running = true;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void run() {&lt;br/&gt;
    +		// early exit check&lt;br/&gt;
    +		if (!running) &lt;/p&gt;
{
    +			return;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		// this is the means to talk to FlinkKafkaConsumer&apos;s main thread&lt;br/&gt;
    +		final Handover handover = this.handover;&lt;br/&gt;
    +&lt;br/&gt;
    +		// This method initializes the KafkaConsumer and guarantees it is torn down properly.&lt;br/&gt;
    +		// This is important, because the consumer has multi-threading issues,&lt;br/&gt;
    +		// including concurrent &apos;close()&apos; calls.&lt;br/&gt;
    +		final KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			consumer = new KafkaConsumer&amp;lt;&amp;gt;(kafkaProperties);
    +		}
&lt;p&gt;    +		catch (Throwable t) &lt;/p&gt;
{
    +			handover.reportError(t);
    +			return;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		// from here on, the consumer is guaranteed to be closed properly&lt;br/&gt;
    +		try {&lt;br/&gt;
    +			// The callback invoked by Kafka once an offset commit is complete&lt;br/&gt;
    +			final OffsetCommitCallback offsetCommitCallback = new CommitCallback();&lt;br/&gt;
    +&lt;br/&gt;
    +			// tell the consumer which partitions to work with&lt;br/&gt;
    +			consumerCallBridge.assignPartitions(consumer, convertKafkaPartitions(subscribedPartitions));&lt;br/&gt;
    +&lt;br/&gt;
    +			// register Kafka&apos;s very own metrics in Flink&apos;s metric reporters&lt;br/&gt;
    +			if (useMetrics) {&lt;br/&gt;
    +				// register Kafka metrics to Flink&lt;br/&gt;
    +				Map&amp;lt;MetricName, ? extends Metric&amp;gt; metrics = consumer.metrics();&lt;br/&gt;
    +				if (metrics == null) &lt;/p&gt;
{
    +					// MapR&apos;s Kafka implementation returns null here.
    +					log.info(&quot;Consumer implementation does not support metrics&quot;);
    +				}
&lt;p&gt; else {&lt;br/&gt;
    +					// we have Kafka metrics, register them&lt;br/&gt;
    +					for (Map.Entry&amp;lt;MetricName, ? extends Metric&amp;gt; metric: metrics.entrySet()) &lt;/p&gt;
{
    +						kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()));
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			// early exit check&lt;br/&gt;
    +			if (!running) &lt;/p&gt;
{
    +				return;
    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			// seek the consumer to the initial offsets&lt;br/&gt;
    +			for (KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition : subscribedPartitions) {&lt;br/&gt;
    +				if (partition.isOffsetDefined()) {&lt;br/&gt;
    +					log.info(&quot;Partition {} has restored initial offsets {} from checkpoint / savepoint; &quot; +&lt;br/&gt;
    +							&quot;seeking the consumer to position {}&quot;,&lt;br/&gt;
    +							partition.getKafkaPartitionHandle(), partition.getOffset(), partition.getOffset() + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +					consumer.seek(partition.getKafkaPartitionHandle(), partition.getOffset() + 1);&lt;br/&gt;
    +				}&lt;br/&gt;
    +				else {&lt;br/&gt;
    +					// for partitions that do not have offsets restored from a checkpoint/savepoint,&lt;br/&gt;
    +					// we need to define our internal offset state for them using the initial offsets retrieved from Kafka&lt;br/&gt;
    +					// by the KafkaConsumer, so that they are correctly checkpointed and committed on the next checkpoint&lt;br/&gt;
    +&lt;br/&gt;
    +					long fetchedOffset = consumer.position(partition.getKafkaPartitionHandle());&lt;br/&gt;
    +&lt;br/&gt;
    +					log.info(&quot;Partition {} has no initial offset; the consumer has position {}, &quot; +&lt;br/&gt;
    +							&quot;so the initial offset will be set to {}&quot;,&lt;br/&gt;
    +							partition.getKafkaPartitionHandle(), fetchedOffset, fetchedOffset - 1);&lt;br/&gt;
    +&lt;br/&gt;
    +					// the fetched offset represents the next record to process, so we need to subtract it by 1&lt;br/&gt;
    +					partition.setOffset(fetchedOffset - 1);&lt;br/&gt;
    +				}&lt;br/&gt;
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			// from now on, external operations may call the consumer&lt;br/&gt;
    +			this.consumer = consumer;&lt;br/&gt;
    +&lt;br/&gt;
    +			// the latest bulk of records. may carry across the loop if the thread is woken up&lt;br/&gt;
    +			// from blocking on the handover&lt;br/&gt;
    +			ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records = null;&lt;br/&gt;
    +&lt;br/&gt;
    +			// main fetch loop&lt;br/&gt;
    +			while (running) {&lt;br/&gt;
    +&lt;br/&gt;
    +				// check if there is something to commit&lt;br/&gt;
    +				if (!commitInProgress) {&lt;br/&gt;
    +					// get and reset the work-to-be committed, so we don&apos;t repeatedly commit the same&lt;br/&gt;
    +					final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; toCommit = nextOffsetsToCommit.getAndSet(null);&lt;br/&gt;
    +&lt;br/&gt;
    +					if (toCommit != null) &lt;/p&gt;
{
    +						log.debug(&quot;Sending async offset commit request to Kafka broker&quot;);
    +
    +						// also record that a commit is already in progress
    +						// the order here matters! first set the flag, then send the commit command.
    +						commitInProgress = true;
    +						consumer.commitAsync(toCommit, offsetCommitCallback);
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				// get the next batch of records, unless we did not manage to hand the old batch over&lt;br/&gt;
    +				if (records == null) {&lt;br/&gt;
    +					try &lt;/p&gt;
{
    +						records = consumer.poll(pollTimeout);
    +					}
&lt;p&gt;    +					catch (WakeupException we) &lt;/p&gt;
{
    +						continue;
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					handover.produce(records);
    +					records = null;
    +				}
&lt;p&gt;    +				catch (Handover.WakeupException e) &lt;/p&gt;
{
    +					// fall through the loop
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +			// end main fetch loop&lt;br/&gt;
    +		}&lt;br/&gt;
    +		catch (Throwable t) &lt;/p&gt;
{
    +			// let the main thread know and exit
    +			// it may be that this exception comes because the main thread closed the handover, in
    +			// which case the below reporting is irrelevant, but does not hurt either
    +			handover.reportError(t);
    +		}
&lt;p&gt;    +		finally {&lt;br/&gt;
    +			try &lt;/p&gt;
{
    +				consumer.close();
    +			}
&lt;p&gt;    +			catch (Throwable t) &lt;/p&gt;
{
    +				log.warn(&quot;Error while closing Kafka consumer&quot;, t);
    +			}
&lt;p&gt;    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Shuts this thread down, waking up the thread gracefully if blocked.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public void shutdown() {&lt;br/&gt;
    +		running = false;&lt;br/&gt;
    +&lt;br/&gt;
    +		// this wakes up the consumer if it is blocked handing over records&lt;br/&gt;
    +		handover.close();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Can we actually call `handover.wakeupProducer()` here, and call `handover.close()` in the `finally` clause of the run() loop?&lt;/p&gt;

&lt;p&gt;    I don&apos;t think it really matters that much on our case, but IMO, this way the cancellation flow between the fetcher loop and the consumer thread will be clearer.&lt;/p&gt;

&lt;p&gt;    My thinking is that, only the `KafkaConsumerThread` actually calls `close()` on the handover and immediately rethrow a `Handover.ClosedException` to the fetcher thread on blocking `handover.pollNext()`s. The fetcher thread only calls `shutdown()` on `KafkaConsumerThread`, either on cancellation (in which case the `pollNext()` can still immediately be rethrown either a `Handover.ClosedException` or `InterruptedException`, depending on which arrives first) or normal clean exit.&lt;/p&gt;</comment>
                            <comment id="15662983" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87746033&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87746033&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/test/java/org/apache/flink/streaming/connectors/kafka/internal/HandoverTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,387 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internal.Handover.WakeupException;&lt;br/&gt;
    +import org.apache.flink.util.ExceptionUtils;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.IOException;&lt;br/&gt;
    +import java.util.Random;&lt;br/&gt;
    +import java.util.concurrent.TimeoutException;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.*;&lt;br/&gt;
    +import static org.mockito.Mockito.*;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Tests for the &lt;/p&gt;
{@link Handover}
&lt;p&gt; between Kafka Consumer Thread and the fetcher&apos;s main thread. &lt;br/&gt;
    + */&lt;br/&gt;
    +public class HandoverTest {&lt;br/&gt;
    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +	//  test produce / consumer&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testWithVariableProducer() throws Exception &lt;/p&gt;
{
    +		runProducerConsumerTest(500, 2, 0);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testWithVariableConsumer() throws Exception &lt;/p&gt;
{
    +		runProducerConsumerTest(500, 0, 2);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testWithVariableBoth() throws Exception &lt;/p&gt;
{
    +		runProducerConsumerTest(500, 2, 2);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	private void runProducerConsumerTest(int numRecords, int maxProducerDelay, int maxConsumerDelay) throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    nit: Can we move this private method down to the bottom of the file? Not entirely necessary, just that I have a preference of keeping private methods after the public ones.&lt;/p&gt;</comment>
                            <comment id="15662986" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87742870&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87742870&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -279,32 +152,37 @@ public void run() &lt;/p&gt;
{
     							break;
     						}

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// emit the actual record. this also update offset state atomically&lt;br/&gt;
    +						// emit the actual record. this also updates offset state atomically&lt;br/&gt;
     						// and deals with timestamps and watermark generation&lt;br/&gt;
     						emitRecord(value, partition, record.offset(), record);&lt;br/&gt;
     					}&lt;br/&gt;
     				}&lt;br/&gt;
     			}&lt;/li&gt;
	&lt;li&gt;// end main fetch loop&lt;/li&gt;
	&lt;li&gt;}&lt;/li&gt;
	&lt;li&gt;catch (Throwable t) {&lt;/li&gt;
	&lt;li&gt;if (running) 
{
    -				running = false;
    -				errorHandler.reportError(t);
    -			}
&lt;p&gt; else &lt;/p&gt;
{
    -				LOG.debug(&quot;Stopped ConsumerThread threw exception&quot;, t);
    -			}
&lt;p&gt;     		}&lt;br/&gt;
     		finally {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;try 
{
    -				consumer.close();
    -			}&lt;/li&gt;
	&lt;li&gt;catch (Throwable t) 
{
    -				LOG.warn(&quot;Error while closing Kafka 0.9 consumer&quot;, t);
    -			}
&lt;p&gt;    +			// this signals the consumer thread that no more work is to be done&lt;br/&gt;
    +			consumerThread.shutdown();&lt;br/&gt;
    +		}&lt;br/&gt;
    +&lt;br/&gt;
    +		// on a clean exit, wait for the runner thread&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			consumerThread.join();
    +		}
&lt;p&gt;    +		catch (InterruptedException e) &lt;/p&gt;
{
    +			// may be the result of a wake-up interruption after an exception.
    +			// we ignore this here and only restore the interruption state
    +			Thread.currentThread().interrupt();
     		}
&lt;p&gt;     	}&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    +	@Override&lt;br/&gt;
    +	public void cancel() {&lt;br/&gt;
    +		// flag the main thread to exit. A thread interrupt will come anyways.&lt;br/&gt;
    +		running = false;&lt;br/&gt;
    +		handover.close();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We might not need to call `close()` on the handover here. Please see my above comments.&lt;/p&gt;</comment>
                            <comment id="15662987" author="githubbot" created="Mon, 14 Nov 2016 06:59:38 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r87734293&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r87734293&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerCallBridge010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,40 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The ConsumerCallBridge simply calls the &lt;/p&gt;
{@link KafkaConsumer#assign(java.util.Collection)}
&lt;p&gt; method.&lt;br/&gt;
    + * &lt;br/&gt;
    + * This indirection is necessary, because Kafka broke binary compatibility between 0.9 and 0.10,&lt;br/&gt;
    + * changing &lt;/p&gt;
{@code assign(List)}
&lt;p&gt; to &lt;/p&gt;
{@code assign(Collection)}
&lt;p&gt;.&lt;br/&gt;
    + * &lt;br/&gt;
    + * Because of that, we need to two versions whose compiled code goes against different method signatures.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaConsumerCallBridge010 extends KafkaConsumerCallBridge {&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void assignPartitions(KafkaConsumer&amp;lt;?, ?&amp;gt; consumer, List&amp;lt;TopicPartition&amp;gt; topicPartitions) throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Does the type parameters for key / value of `KafkaConsumer` need to be generic? Seems like we will only be using `&amp;lt;byte[], byte[]&amp;gt;` anyway.&lt;/p&gt;</comment>
                            <comment id="15670496" author="githubbot" created="Wed, 16 Nov 2016 14:11:12 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks for the review, @tzulitai &lt;/p&gt;

&lt;p&gt;    I would go ahead and merge this, addressing the comments.&lt;/p&gt;</comment>
                            <comment id="15670534" author="githubbot" created="Wed, 16 Nov 2016 14:23:47 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r88245373&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r88245373&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,325 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.metrics.MetricGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricWrapper;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.Metric;&lt;br/&gt;
    +import org.apache.kafka.common.MetricName;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +import org.apache.kafka.common.errors.WakeupException;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.slf4j.Logger;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The thread the runs the &lt;/p&gt;
{@link KafkaConsumer}
&lt;p&gt;, connecting to the brokers and polling records.&lt;br/&gt;
    + * The thread pushes the data into a &lt;/p&gt;
{@link Handover}
&lt;p&gt; to be picked up by the fetcher that will&lt;br/&gt;
    + * deserialize and emit the records.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;IMPORTANT:&amp;lt;/b&amp;gt; This thread must not be interrupted when attempting to shut it down.&lt;br/&gt;
    + * The Kafka consumer code was found to not always handle interrupts well, and to even&lt;br/&gt;
    + * deadlock in certain situations.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;Implementation Note: This code is written to be reusable in later versions of the KafkaConsumer.&lt;br/&gt;
    + * Because Kafka is not maintaining binary compatibility, we use a &quot;call bridge&quot; as an indirection&lt;br/&gt;
    + * to the KafkaConsumer calls that change signature.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaConsumerThread extends Thread {&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Logger for this consumer */&lt;br/&gt;
    +	final Logger log;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I left this package-private, because it is accessed by the nested class for the commit callback.&lt;br/&gt;
    If I make it private, the compiler has to inject a bridge method.&lt;/p&gt;

&lt;p&gt;    I guess making it private is correct, though, it better documents how it should be used.&lt;/p&gt;</comment>
                            <comment id="15670707" author="githubbot" created="Wed, 16 Nov 2016 15:26:17 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r88257338&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r88257338&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.10/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerCallBridge010.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,40 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The ConsumerCallBridge simply calls the &lt;/p&gt;
{@link KafkaConsumer#assign(java.util.Collection)}
&lt;p&gt; method.&lt;br/&gt;
    + * &lt;br/&gt;
    + * This indirection is necessary, because Kafka broke binary compatibility between 0.9 and 0.10,&lt;br/&gt;
    + * changing &lt;/p&gt;
{@code assign(List)}
&lt;p&gt; to &lt;/p&gt;
{@code assign(Collection)}
&lt;p&gt;.&lt;br/&gt;
    + * &lt;br/&gt;
    + * Because of that, we need to two versions whose compiled code goes against different method signatures.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaConsumerCallBridge010 extends KafkaConsumerCallBridge {&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void assignPartitions(KafkaConsumer&amp;lt;?, ?&amp;gt; consumer, List&amp;lt;TopicPartition&amp;gt; topicPartitions) throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I think generic is nice, because for this method, the key/value types do not matter. That way it is more future proof.&lt;/p&gt;</comment>
                            <comment id="15670717" author="githubbot" created="Wed, 16 Nov 2016 15:29:57 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r88258273&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r88258273&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaShortRetentionTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -172,8 +173,14 @@ public void cancel() {&lt;/p&gt;

&lt;p&gt;     		// ----------- add consumer dataflow ----------&lt;/p&gt;

&lt;p&gt;    +		// the consumer should only poll very small chunks&lt;br/&gt;
    +		Properties consumerProps = new Properties();&lt;br/&gt;
    +		consumerProps.putAll(standardProps);&lt;br/&gt;
    +		consumerProps.putAll(secureProps);&lt;br/&gt;
    +		consumerProps.setProperty(&quot;fetch.message.max.bytes&quot;, &quot;100&quot;);&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    leftover from getting the &quot;short retention&quot; tests to run with the modified source. will undo.&lt;/p&gt;</comment>
                            <comment id="15670743" author="githubbot" created="Wed, 16 Nov 2016 15:37:30 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r88260220&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r88260220&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,325 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.metrics.MetricGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricWrapper;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.Metric;&lt;br/&gt;
    +import org.apache.kafka.common.MetricName;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +import org.apache.kafka.common.errors.WakeupException;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.slf4j.Logger;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The thread the runs the &lt;/p&gt;
{@link KafkaConsumer}
&lt;p&gt;, connecting to the brokers and polling records.&lt;br/&gt;
    + * The thread pushes the data into a &lt;/p&gt;
{@link Handover}
&lt;p&gt; to be picked up by the fetcher that will&lt;br/&gt;
    + * deserialize and emit the records.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;IMPORTANT:&amp;lt;/b&amp;gt; This thread must not be interrupted when attempting to shut it down.&lt;br/&gt;
    + * The Kafka consumer code was found to not always handle interrupts well, and to even&lt;br/&gt;
    + * deadlock in certain situations.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;Implementation Note: This code is written to be reusable in later versions of the KafkaConsumer.&lt;br/&gt;
    + * Because Kafka is not maintaining binary compatibility, we use a &quot;call bridge&quot; as an indirection&lt;br/&gt;
    + * to the KafkaConsumer calls that change signature.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaConsumerThread extends Thread {&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Logger for this consumer */&lt;br/&gt;
    +	final Logger log;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The handover of data and exceptions between the consumer thread and the task thread */&lt;br/&gt;
    +	private final Handover handover;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The next offsets that the main thread should commit */&lt;br/&gt;
    +	private final AtomicReference&amp;lt;Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt;&amp;gt; nextOffsetsToCommit;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The configuration for the Kafka consumer */&lt;br/&gt;
    +	private final Properties kafkaProperties;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The partitions that this consumer reads from */ &lt;br/&gt;
    +	private final KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt;[] subscribedPartitions;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** We get this from the outside to publish metrics. **/&lt;br/&gt;
    +	private final MetricGroup kafkaMetricGroup;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The indirections on KafkaConsumer methods, for cases where KafkaConsumer compatibility is broken */&lt;br/&gt;
    +	private final KafkaConsumerCallBridge consumerCallBridge;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The maximum number of milliseconds to wait for a fetch batch */&lt;br/&gt;
    +	private final long pollTimeout;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag whether to add Kafka&apos;s metrics to the Flink metrics */&lt;br/&gt;
    +	private final boolean useMetrics;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Reference to the Kafka consumer, once it is created */&lt;br/&gt;
    +	private volatile KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag to mark the main work loop as alive */&lt;br/&gt;
    +	private volatile boolean running;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag tracking whether the latest commit request has completed */&lt;br/&gt;
    +	private volatile boolean commitInProgress;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +	public KafkaConsumerThread(&lt;br/&gt;
    +			Logger log,&lt;br/&gt;
    +			Handover handover,&lt;br/&gt;
    +			Properties kafkaProperties,&lt;br/&gt;
    +			KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt;[] subscribedPartitions,&lt;br/&gt;
    +			MetricGroup kafkaMetricGroup,&lt;br/&gt;
    +			KafkaConsumerCallBridge consumerCallBridge,&lt;br/&gt;
    +			String threadName,&lt;br/&gt;
    +			long pollTimeout,&lt;br/&gt;
    +			boolean useMetrics) &lt;/p&gt;
{
    +
    +		super(threadName);
    +		setDaemon(true);
    +
    +		this.log = checkNotNull(log);
    +		this.handover = checkNotNull(handover);
    +		this.kafkaProperties = checkNotNull(kafkaProperties);
    +		this.subscribedPartitions = checkNotNull(subscribedPartitions);
    +		this.kafkaMetricGroup = checkNotNull(kafkaMetricGroup);
    +		this.consumerCallBridge = checkNotNull(consumerCallBridge);
    +		this.pollTimeout = pollTimeout;
    +		this.useMetrics = useMetrics;
    +
    +		this.nextOffsetsToCommit = new AtomicReference&amp;lt;&amp;gt;();
    +		this.running = true;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void run() {&lt;br/&gt;
    +		// early exit check&lt;br/&gt;
    +		if (!running) &lt;/p&gt;
{
    +			return;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		// this is the means to talk to FlinkKafkaConsumer&apos;s main thread&lt;br/&gt;
    +		final Handover handover = this.handover;&lt;br/&gt;
    +&lt;br/&gt;
    +		// This method initializes the KafkaConsumer and guarantees it is torn down properly.&lt;br/&gt;
    +		// This is important, because the consumer has multi-threading issues,&lt;br/&gt;
    +		// including concurrent &apos;close()&apos; calls.&lt;br/&gt;
    +		final KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			consumer = new KafkaConsumer&amp;lt;&amp;gt;(kafkaProperties);
    +		}
&lt;p&gt;    +		catch (Throwable t) &lt;/p&gt;
{
    +			handover.reportError(t);
    +			return;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		// from here on, the consumer is guaranteed to be closed properly&lt;br/&gt;
    +		try {&lt;br/&gt;
    +			// The callback invoked by Kafka once an offset commit is complete&lt;br/&gt;
    +			final OffsetCommitCallback offsetCommitCallback = new CommitCallback();&lt;br/&gt;
    +&lt;br/&gt;
    +			// tell the consumer which partitions to work with&lt;br/&gt;
    +			consumerCallBridge.assignPartitions(consumer, convertKafkaPartitions(subscribedPartitions));&lt;br/&gt;
    +&lt;br/&gt;
    +			// register Kafka&apos;s very own metrics in Flink&apos;s metric reporters&lt;br/&gt;
    +			if (useMetrics) {&lt;br/&gt;
    +				// register Kafka metrics to Flink&lt;br/&gt;
    +				Map&amp;lt;MetricName, ? extends Metric&amp;gt; metrics = consumer.metrics();&lt;br/&gt;
    +				if (metrics == null) &lt;/p&gt;
{
    +					// MapR&apos;s Kafka implementation returns null here.
    +					log.info(&quot;Consumer implementation does not support metrics&quot;);
    +				}
&lt;p&gt; else {&lt;br/&gt;
    +					// we have Kafka metrics, register them&lt;br/&gt;
    +					for (Map.Entry&amp;lt;MetricName, ? extends Metric&amp;gt; metric: metrics.entrySet()) &lt;/p&gt;
{
    +						kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()));
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			// early exit check&lt;br/&gt;
    +			if (!running) &lt;/p&gt;
{
    +				return;
    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			// seek the consumer to the initial offsets&lt;br/&gt;
    +			for (KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition : subscribedPartitions) {&lt;br/&gt;
    +				if (partition.isOffsetDefined()) {&lt;br/&gt;
    +					log.info(&quot;Partition {} has restored initial offsets {} from checkpoint / savepoint; &quot; +&lt;br/&gt;
    +							&quot;seeking the consumer to position {}&quot;,&lt;br/&gt;
    +							partition.getKafkaPartitionHandle(), partition.getOffset(), partition.getOffset() + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +					consumer.seek(partition.getKafkaPartitionHandle(), partition.getOffset() + 1);&lt;br/&gt;
    +				}&lt;br/&gt;
    +				else {&lt;br/&gt;
    +					// for partitions that do not have offsets restored from a checkpoint/savepoint,&lt;br/&gt;
    +					// we need to define our internal offset state for them using the initial offsets retrieved from Kafka&lt;br/&gt;
    +					// by the KafkaConsumer, so that they are correctly checkpointed and committed on the next checkpoint&lt;br/&gt;
    +&lt;br/&gt;
    +					long fetchedOffset = consumer.position(partition.getKafkaPartitionHandle());&lt;br/&gt;
    +&lt;br/&gt;
    +					log.info(&quot;Partition {} has no initial offset; the consumer has position {}, &quot; +&lt;br/&gt;
    +							&quot;so the initial offset will be set to {}&quot;,&lt;br/&gt;
    +							partition.getKafkaPartitionHandle(), fetchedOffset, fetchedOffset - 1);&lt;br/&gt;
    +&lt;br/&gt;
    +					// the fetched offset represents the next record to process, so we need to subtract it by 1&lt;br/&gt;
    +					partition.setOffset(fetchedOffset - 1);&lt;br/&gt;
    +				}&lt;br/&gt;
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			// from now on, external operations may call the consumer&lt;br/&gt;
    +			this.consumer = consumer;&lt;br/&gt;
    +&lt;br/&gt;
    +			// the latest bulk of records. may carry across the loop if the thread is woken up&lt;br/&gt;
    +			// from blocking on the handover&lt;br/&gt;
    +			ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records = null;&lt;br/&gt;
    +&lt;br/&gt;
    +			// main fetch loop&lt;br/&gt;
    +			while (running) {&lt;br/&gt;
    +&lt;br/&gt;
    +				// check if there is something to commit&lt;br/&gt;
    +				if (!commitInProgress) {&lt;br/&gt;
    +					// get and reset the work-to-be committed, so we don&apos;t repeatedly commit the same&lt;br/&gt;
    +					final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; toCommit = nextOffsetsToCommit.getAndSet(null);&lt;br/&gt;
    +&lt;br/&gt;
    +					if (toCommit != null) &lt;/p&gt;
{
    +						log.debug(&quot;Sending async offset commit request to Kafka broker&quot;);
    +
    +						// also record that a commit is already in progress
    +						// the order here matters! first set the flag, then send the commit command.
    +						commitInProgress = true;
    +						consumer.commitAsync(toCommit, offsetCommitCallback);
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				// get the next batch of records, unless we did not manage to hand the old batch over&lt;br/&gt;
    +				if (records == null) {&lt;br/&gt;
    +					try &lt;/p&gt;
{
    +						records = consumer.poll(pollTimeout);
    +					}
&lt;p&gt;    +					catch (WakeupException we) &lt;/p&gt;
{
    +						continue;
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					handover.produce(records);
    +					records = null;
    +				}
&lt;p&gt;    +				catch (Handover.WakeupException e) &lt;/p&gt;
{
    +					// fall through the loop
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +			// end main fetch loop&lt;br/&gt;
    +		}&lt;br/&gt;
    +		catch (Throwable t) &lt;/p&gt;
{
    +			// let the main thread know and exit
    +			// it may be that this exception comes because the main thread closed the handover, in
    +			// which case the below reporting is irrelevant, but does not hurt either
    +			handover.reportError(t);
    +		}
&lt;p&gt;    +		finally {&lt;br/&gt;
    +			try {&lt;br/&gt;
    +				consumer.close();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    We need to call `handover.close()` in the `shutdown()` method, otherwise it will not properly wake up the thread.&lt;/p&gt;

&lt;p&gt;    We could rely on someone else calling it, but that would not make the KafkaConsumerThread&apos;s logic self-contained (it would rely on implicit behavior of another class), which I would like to avoid.&lt;/p&gt;</comment>
                            <comment id="15670757" author="githubbot" created="Wed, 16 Nov 2016 15:41:28 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I would actually like to not change how/when `handover.close()` is called. It is called more often that necessary (probably), but since it is an idempotent operation, it does not matter.&lt;/p&gt;

&lt;p&gt;    The code is designed to lead to the quickest wakeup/termination possible in all cases:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;cancellation&lt;/li&gt;
	&lt;li&gt;end of stream&lt;/li&gt;
	&lt;li&gt;error in the fetcher&lt;/li&gt;
	&lt;li&gt;error in the consumer&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Also note that errors/close do not overwrite each other, which makes it fine if the other is called afterwards in addition.&lt;/p&gt;

&lt;p&gt;    Also, both the fetcher and the KafkaConsumerThread are written to encapsulate all necessary logic self contained. That means they do not rely on each other to call `handover.close()` in any situation - that makes the design more robust.&lt;/p&gt;</comment>
                            <comment id="15670798" author="githubbot" created="Wed, 16 Nov 2016 15:54:55 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r88264489&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r88264489&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/KafkaConsumerThread.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,325 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.streaming.connectors.kafka.internal;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.metrics.MetricGroup;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.KafkaTopicPartitionState;&lt;br/&gt;
    +import org.apache.flink.streaming.connectors.kafka.internals.metrics.KafkaMetricWrapper;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.ConsumerRecords;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.KafkaConsumer;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetAndMetadata;&lt;br/&gt;
    +import org.apache.kafka.clients.consumer.OffsetCommitCallback;&lt;br/&gt;
    +import org.apache.kafka.common.Metric;&lt;br/&gt;
    +import org.apache.kafka.common.MetricName;&lt;br/&gt;
    +import org.apache.kafka.common.TopicPartition;&lt;br/&gt;
    +import org.apache.kafka.common.errors.WakeupException;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.slf4j.Logger;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.util.ArrayList;&lt;br/&gt;
    +import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.Properties;&lt;br/&gt;
    +import java.util.concurrent.atomic.AtomicReference;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.apache.flink.util.Preconditions.checkNotNull;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * The thread the runs the &lt;/p&gt;
{@link KafkaConsumer}
&lt;p&gt;, connecting to the brokers and polling records.&lt;br/&gt;
    + * The thread pushes the data into a &lt;/p&gt;
{@link Handover}
&lt;p&gt; to be picked up by the fetcher that will&lt;br/&gt;
    + * deserialize and emit the records.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;&amp;lt;b&amp;gt;IMPORTANT:&amp;lt;/b&amp;gt; This thread must not be interrupted when attempting to shut it down.&lt;br/&gt;
    + * The Kafka consumer code was found to not always handle interrupts well, and to even&lt;br/&gt;
    + * deadlock in certain situations.&lt;br/&gt;
    + * &lt;br/&gt;
    + * &amp;lt;p&amp;gt;Implementation Note: This code is written to be reusable in later versions of the KafkaConsumer.&lt;br/&gt;
    + * Because Kafka is not maintaining binary compatibility, we use a &quot;call bridge&quot; as an indirection&lt;br/&gt;
    + * to the KafkaConsumer calls that change signature.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class KafkaConsumerThread extends Thread {&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Logger for this consumer */&lt;br/&gt;
    +	final Logger log;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The handover of data and exceptions between the consumer thread and the task thread */&lt;br/&gt;
    +	private final Handover handover;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The next offsets that the main thread should commit */&lt;br/&gt;
    +	private final AtomicReference&amp;lt;Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt;&amp;gt; nextOffsetsToCommit;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The configuration for the Kafka consumer */&lt;br/&gt;
    +	private final Properties kafkaProperties;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The partitions that this consumer reads from */ &lt;br/&gt;
    +	private final KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt;[] subscribedPartitions;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** We get this from the outside to publish metrics. **/&lt;br/&gt;
    +	private final MetricGroup kafkaMetricGroup;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The indirections on KafkaConsumer methods, for cases where KafkaConsumer compatibility is broken */&lt;br/&gt;
    +	private final KafkaConsumerCallBridge consumerCallBridge;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** The maximum number of milliseconds to wait for a fetch batch */&lt;br/&gt;
    +	private final long pollTimeout;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag whether to add Kafka&apos;s metrics to the Flink metrics */&lt;br/&gt;
    +	private final boolean useMetrics;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Reference to the Kafka consumer, once it is created */&lt;br/&gt;
    +	private volatile KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag to mark the main work loop as alive */&lt;br/&gt;
    +	private volatile boolean running;&lt;br/&gt;
    +&lt;br/&gt;
    +	/** Flag tracking whether the latest commit request has completed */&lt;br/&gt;
    +	private volatile boolean commitInProgress;&lt;br/&gt;
    +&lt;br/&gt;
    +&lt;br/&gt;
    +	public KafkaConsumerThread(&lt;br/&gt;
    +			Logger log,&lt;br/&gt;
    +			Handover handover,&lt;br/&gt;
    +			Properties kafkaProperties,&lt;br/&gt;
    +			KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt;[] subscribedPartitions,&lt;br/&gt;
    +			MetricGroup kafkaMetricGroup,&lt;br/&gt;
    +			KafkaConsumerCallBridge consumerCallBridge,&lt;br/&gt;
    +			String threadName,&lt;br/&gt;
    +			long pollTimeout,&lt;br/&gt;
    +			boolean useMetrics) &lt;/p&gt;
{
    +
    +		super(threadName);
    +		setDaemon(true);
    +
    +		this.log = checkNotNull(log);
    +		this.handover = checkNotNull(handover);
    +		this.kafkaProperties = checkNotNull(kafkaProperties);
    +		this.subscribedPartitions = checkNotNull(subscribedPartitions);
    +		this.kafkaMetricGroup = checkNotNull(kafkaMetricGroup);
    +		this.consumerCallBridge = checkNotNull(consumerCallBridge);
    +		this.pollTimeout = pollTimeout;
    +		this.useMetrics = useMetrics;
    +
    +		this.nextOffsetsToCommit = new AtomicReference&amp;lt;&amp;gt;();
    +		this.running = true;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	// ------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	@Override&lt;br/&gt;
    +	public void run() {&lt;br/&gt;
    +		// early exit check&lt;br/&gt;
    +		if (!running) &lt;/p&gt;
{
    +			return;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		// this is the means to talk to FlinkKafkaConsumer&apos;s main thread&lt;br/&gt;
    +		final Handover handover = this.handover;&lt;br/&gt;
    +&lt;br/&gt;
    +		// This method initializes the KafkaConsumer and guarantees it is torn down properly.&lt;br/&gt;
    +		// This is important, because the consumer has multi-threading issues,&lt;br/&gt;
    +		// including concurrent &apos;close()&apos; calls.&lt;br/&gt;
    +		final KafkaConsumer&amp;lt;byte[], byte[]&amp;gt; consumer;&lt;br/&gt;
    +		try &lt;/p&gt;
{
    +			consumer = new KafkaConsumer&amp;lt;&amp;gt;(kafkaProperties);
    +		}
&lt;p&gt;    +		catch (Throwable t) &lt;/p&gt;
{
    +			handover.reportError(t);
    +			return;
    +		}
&lt;p&gt;    +&lt;br/&gt;
    +		// from here on, the consumer is guaranteed to be closed properly&lt;br/&gt;
    +		try {&lt;br/&gt;
    +			// The callback invoked by Kafka once an offset commit is complete&lt;br/&gt;
    +			final OffsetCommitCallback offsetCommitCallback = new CommitCallback();&lt;br/&gt;
    +&lt;br/&gt;
    +			// tell the consumer which partitions to work with&lt;br/&gt;
    +			consumerCallBridge.assignPartitions(consumer, convertKafkaPartitions(subscribedPartitions));&lt;br/&gt;
    +&lt;br/&gt;
    +			// register Kafka&apos;s very own metrics in Flink&apos;s metric reporters&lt;br/&gt;
    +			if (useMetrics) {&lt;br/&gt;
    +				// register Kafka metrics to Flink&lt;br/&gt;
    +				Map&amp;lt;MetricName, ? extends Metric&amp;gt; metrics = consumer.metrics();&lt;br/&gt;
    +				if (metrics == null) &lt;/p&gt;
{
    +					// MapR&apos;s Kafka implementation returns null here.
    +					log.info(&quot;Consumer implementation does not support metrics&quot;);
    +				}
&lt;p&gt; else {&lt;br/&gt;
    +					// we have Kafka metrics, register them&lt;br/&gt;
    +					for (Map.Entry&amp;lt;MetricName, ? extends Metric&amp;gt; metric: metrics.entrySet()) &lt;/p&gt;
{
    +						kafkaMetricGroup.gauge(metric.getKey().name(), new KafkaMetricWrapper(metric.getValue()));
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			// early exit check&lt;br/&gt;
    +			if (!running) &lt;/p&gt;
{
    +				return;
    +			}
&lt;p&gt;    +&lt;br/&gt;
    +			// seek the consumer to the initial offsets&lt;br/&gt;
    +			for (KafkaTopicPartitionState&amp;lt;TopicPartition&amp;gt; partition : subscribedPartitions) {&lt;br/&gt;
    +				if (partition.isOffsetDefined()) {&lt;br/&gt;
    +					log.info(&quot;Partition {} has restored initial offsets {} from checkpoint / savepoint; &quot; +&lt;br/&gt;
    +							&quot;seeking the consumer to position {}&quot;,&lt;br/&gt;
    +							partition.getKafkaPartitionHandle(), partition.getOffset(), partition.getOffset() + 1);&lt;br/&gt;
    +&lt;br/&gt;
    +					consumer.seek(partition.getKafkaPartitionHandle(), partition.getOffset() + 1);&lt;br/&gt;
    +				}&lt;br/&gt;
    +				else {&lt;br/&gt;
    +					// for partitions that do not have offsets restored from a checkpoint/savepoint,&lt;br/&gt;
    +					// we need to define our internal offset state for them using the initial offsets retrieved from Kafka&lt;br/&gt;
    +					// by the KafkaConsumer, so that they are correctly checkpointed and committed on the next checkpoint&lt;br/&gt;
    +&lt;br/&gt;
    +					long fetchedOffset = consumer.position(partition.getKafkaPartitionHandle());&lt;br/&gt;
    +&lt;br/&gt;
    +					log.info(&quot;Partition {} has no initial offset; the consumer has position {}, &quot; +&lt;br/&gt;
    +							&quot;so the initial offset will be set to {}&quot;,&lt;br/&gt;
    +							partition.getKafkaPartitionHandle(), fetchedOffset, fetchedOffset - 1);&lt;br/&gt;
    +&lt;br/&gt;
    +					// the fetched offset represents the next record to process, so we need to subtract it by 1&lt;br/&gt;
    +					partition.setOffset(fetchedOffset - 1);&lt;br/&gt;
    +				}&lt;br/&gt;
    +			}&lt;br/&gt;
    +&lt;br/&gt;
    +			// from now on, external operations may call the consumer&lt;br/&gt;
    +			this.consumer = consumer;&lt;br/&gt;
    +&lt;br/&gt;
    +			// the latest bulk of records. may carry across the loop if the thread is woken up&lt;br/&gt;
    +			// from blocking on the handover&lt;br/&gt;
    +			ConsumerRecords&amp;lt;byte[], byte[]&amp;gt; records = null;&lt;br/&gt;
    +&lt;br/&gt;
    +			// main fetch loop&lt;br/&gt;
    +			while (running) {&lt;br/&gt;
    +&lt;br/&gt;
    +				// check if there is something to commit&lt;br/&gt;
    +				if (!commitInProgress) {&lt;br/&gt;
    +					// get and reset the work-to-be committed, so we don&apos;t repeatedly commit the same&lt;br/&gt;
    +					final Map&amp;lt;TopicPartition, OffsetAndMetadata&amp;gt; toCommit = nextOffsetsToCommit.getAndSet(null);&lt;br/&gt;
    +&lt;br/&gt;
    +					if (toCommit != null) &lt;/p&gt;
{
    +						log.debug(&quot;Sending async offset commit request to Kafka broker&quot;);
    +
    +						// also record that a commit is already in progress
    +						// the order here matters! first set the flag, then send the commit command.
    +						commitInProgress = true;
    +						consumer.commitAsync(toCommit, offsetCommitCallback);
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				// get the next batch of records, unless we did not manage to hand the old batch over&lt;br/&gt;
    +				if (records == null) {&lt;br/&gt;
    +					try &lt;/p&gt;
{
    +						records = consumer.poll(pollTimeout);
    +					}
&lt;p&gt;    +					catch (WakeupException we) &lt;/p&gt;
{
    +						continue;
    +					}
&lt;p&gt;    +				}&lt;br/&gt;
    +&lt;br/&gt;
    +				try &lt;/p&gt;
{
    +					handover.produce(records);
    +					records = null;
    +				}
&lt;p&gt;    +				catch (Handover.WakeupException e) &lt;/p&gt;
{
    +					// fall through the loop
    +				}
&lt;p&gt;    +			}&lt;br/&gt;
    +			// end main fetch loop&lt;br/&gt;
    +		}&lt;br/&gt;
    +		catch (Throwable t) &lt;/p&gt;
{
    +			// let the main thread know and exit
    +			// it may be that this exception comes because the main thread closed the handover, in
    +			// which case the below reporting is irrelevant, but does not hurt either
    +			handover.reportError(t);
    +		}
&lt;p&gt;    +		finally {&lt;br/&gt;
    +			try &lt;/p&gt;
{
    +				consumer.close();
    +			}
&lt;p&gt;    +			catch (Throwable t) &lt;/p&gt;
{
    +				log.warn(&quot;Error while closing Kafka consumer&quot;, t);
    +			}
&lt;p&gt;    +		}&lt;br/&gt;
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Shuts this thread down, waking up the thread gracefully if blocked.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public void shutdown() {&lt;br/&gt;
    +		running = false;&lt;br/&gt;
    +&lt;br/&gt;
    +		// this wakes up the consumer if it is blocked handing over records&lt;br/&gt;
    +		handover.close();&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I followed this partly, but kept an eager call to `handover.close()` just to make the consumer thread cancellation double safe.&lt;/p&gt;</comment>
                            <comment id="15670803" author="githubbot" created="Wed, 16 Nov 2016 15:56:37 +0000"  >&lt;p&gt;Github user StephanEwen commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r88264932&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r88264932&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -143,133 +123,26 @@ public Kafka09Fetcher(&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void runFetchLoop() throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    To be safe, I think the `CloseExceptions` should be re-thrown, as should all others.&lt;br/&gt;
    Just for the case when we overlook something and the consumer thread could close the handover by itself or so. Any abnormal termination of the fetch loop should result in an exception - that is the safest we can do.&lt;/p&gt;</comment>
                            <comment id="15670854" author="githubbot" created="Wed, 16 Nov 2016 16:12:35 +0000"  >&lt;p&gt;Github user tzulitai commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;gt; Also, both the fetcher and the KafkaConsumerThread are written to encapsulate all necessary logic self contained. That means they do not rely on each other to call handover.close() in any situation - that makes the design more robust.&lt;/p&gt;

&lt;p&gt;    I think that makes sense. My suggestions will definitely make the fetcher thread rely on the KafkaConsumerThread to do correct calls.&lt;br/&gt;
    Agree to keep it as is &lt;img class=&quot;emoticon&quot; src=&quot;https://issues.apache.org/jira/images/icons/emoticons/smile.png&quot; height=&quot;16&quot; width=&quot;16&quot; align=&quot;absmiddle&quot; alt=&quot;&quot; border=&quot;0&quot;/&gt;&lt;/p&gt;</comment>
                            <comment id="15670911" author="githubbot" created="Wed, 16 Nov 2016 16:31:11 +0000"  >&lt;p&gt;Github user tzulitai commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789#discussion_r88273352&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789#discussion_r88273352&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-streaming-connectors/flink-connector-kafka-0.9/src/main/java/org/apache/flink/streaming/connectors/kafka/internal/Kafka09Fetcher.java &amp;#8212;&lt;br/&gt;
    @@ -143,133 +123,26 @@ public Kafka09Fetcher(&lt;/p&gt;

&lt;p&gt;     	@Override&lt;br/&gt;
     	public void runFetchLoop() throws Exception {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Ok, I agree to be safe.&lt;/p&gt;

&lt;p&gt;    Also, I just realized that &quot;end of stream&quot; shouldn&apos;t lead to the `ClosedException`, only &quot;cancellation&quot;, &quot;fetcher error&quot;, &quot;consumer error&quot;, and (hopefully not) any other stuff we overlooked will. So, basically, like what you said, only abnormal terminations. In that case, let&apos;s keep it this way.&lt;/p&gt;</comment>
                            <comment id="15671423" author="githubbot" created="Wed, 16 Nov 2016 19:51:49 +0000"  >&lt;p&gt;Github user StephanEwen commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Manually merged in a66e7ad14e41fa07737f447d68920ad5cc4ed6d3&lt;/p&gt;</comment>
                            <comment id="15671424" author="githubbot" created="Wed, 16 Nov 2016 19:51:50 +0000"  >&lt;p&gt;Github user StephanEwen closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/2789&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/2789&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="15671432" author="stephanewen" created="Wed, 16 Nov 2016 19:53:32 +0000"  >&lt;p&gt;Fixed in 1.2.0 via a66e7ad14e41fa07737f447d68920ad5cc4ed6d3&lt;/p&gt;</comment>
                            <comment id="15714813" author="rmetzger" created="Fri, 2 Dec 2016 11:04:11 +0000"  >&lt;p&gt;Do we want to backport the changes to 1.1.4 or can I close the issue? (I would like to get rid of all blocker issues)&lt;/p&gt;</comment>
                            <comment id="15715278" author="uce" created="Fri, 2 Dec 2016 14:34:00 +0000"  >&lt;p&gt;I would like to have this in 1.1.4, if feasible.&lt;/p&gt;</comment>
                            <comment id="15721966" author="uce" created="Mon, 5 Dec 2016 11:00:14 +0000"  >&lt;p&gt;Talked to &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rmetzger&quot; class=&quot;user-hover&quot; rel=&quot;rmetzger&quot;&gt;rmetzger&lt;/a&gt; about this and we would like to not block RC2 on this since no user reported it yet. Moving to 1.1.5&lt;/p&gt;</comment>
                            <comment id="15925810" author="tzulitai" created="Wed, 15 Mar 2017 09:45:46 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=uce&quot; class=&quot;user-hover&quot; rel=&quot;uce&quot;&gt;uce&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=rmetzger&quot; class=&quot;user-hover&quot; rel=&quot;rmetzger&quot;&gt;rmetzger&lt;/a&gt; It seems like the change for this in 1.2.0 has been quite stable (at least no users came up with problems about it). Since 1.1.5 is coming up, I think we should try to finally backport this for 1.1.x. What do you think?&lt;/p&gt;</comment>
                            <comment id="15926820" author="rmetzger" created="Wed, 15 Mar 2017 19:33:46 +0000"  >&lt;p&gt;I&apos;m not aware of any user on 1.1 affected by this. The change is quite involved and could potentially break existing code. Therefore, I would not backport that change.&lt;/p&gt;</comment>
                            <comment id="15927427" author="tzulitai" created="Thu, 16 Mar 2017 03:43:29 +0000"  >&lt;p&gt;Agreed, let&apos;s mark this issue as resolved and remove &lt;tt&gt;1.1.5&lt;/tt&gt; from the fix versions then.&lt;/p&gt;</comment>
                            <comment id="15927941" author="stephanewen" created="Thu, 16 Mar 2017 12:45:25 +0000"  >&lt;p&gt;Sorry to come late here. The fix did not change user-facing behavior - it is completely internal to the KafkaConsumer. It also proved pretty good and stable.&lt;/p&gt;

&lt;p&gt;On the other hand, I think users can simply use the 1.2.0 kafka consumer with Flink 1.1.x if there is an issue.&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            8 years, 35 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3662v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>