<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:40:21 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-13245] Network stack is leaking files</title>
                <link>https://issues.apache.org/jira/browse/FLINK-13245</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;There&apos;s file leak in the network stack / shuffle service.&lt;/p&gt;

&lt;p&gt;When running the &lt;tt&gt;SlotCountExceedingParallelismTest&lt;/tt&gt; on Windows a large number of &lt;tt&gt;.channel&lt;/tt&gt; files continue to reside in a &lt;tt&gt;flink-netty-shuffle-XXX&lt;/tt&gt; directory.&lt;/p&gt;

&lt;p&gt;From what I&apos;ve gathered so far these files are still being used by a &lt;tt&gt;BoundedBlockingSubpartition&lt;/tt&gt;. The cleanup logic in this class uses ref-counting to ensure we don&apos;t release data while a reader is still present. However, at the end of the job this count has not reached 0, and thus nothing is being released.&lt;/p&gt;

&lt;p&gt;The same issue is also present on the &lt;tt&gt;ResultPartition&lt;/tt&gt; level; the &lt;tt&gt;ReleaseOnConsumptionResultPartition&lt;/tt&gt; also are being released while the ref-count is greater than 0.&lt;/p&gt;

&lt;p&gt;Overall it appears like there&apos;s some issue with the notifications for partitions being consumed.&lt;/p&gt;

&lt;p&gt;It is feasible that this issue has recently caused issues on Travis where the build were failing due to a lack of disk space.&lt;/p&gt;</description>
                <environment></environment>
        <key id="13244579">FLINK-13245</key>
            <summary>Network stack is leaking files</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="6" iconUrl="https://issues.apache.org/jira/images/icons/statuses/closed.png" description="The issue is considered finished, the resolution is correct. Issues which are not closed can be reopened.">Closed</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="zjwang">Zhijiang</assignee>
                                    <reporter username="chesnay">Chesnay Schepler</reporter>
                        <labels>
                            <label>pull-request-available</label>
                    </labels>
                <created>Fri, 12 Jul 2019 10:52:55 +0000</created>
                <updated>Wed, 2 Oct 2019 17:50:03 +0000</updated>
                            <resolved>Mon, 29 Jul 2019 10:27:13 +0000</resolved>
                                    <version>1.9.0</version>
                                    <fixVersion>1.9.0</fixVersion>
                                    <component>Runtime / Network</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>7</watches>
                                                    <progress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </progress>
                                    <aggregateprogress percentage="100">
                                    <originalProgress>
                                                    <row percentage="0" backgroundColor="#89afd7"/>
                                                    <row percentage="100" backgroundColor="transparent"/>
                                            </originalProgress>
                                                    <currentProgress>
                                                    <row percentage="100" backgroundColor="#51a825"/>
                                                    <row percentage="0" backgroundColor="#ec8e00"/>
                                            </currentProgress>
                            </aggregateprogress>
                                            <timeestimate seconds="0">0h</timeestimate>
                            <timespent seconds="1200">20m</timespent>
                                <comments>
                            <comment id="16883829" author="zentol" created="Fri, 12 Jul 2019 13:50:13 +0000"  >&lt;p&gt;With the extensive help of &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=azagrebin&quot; class=&quot;user-hover&quot; rel=&quot;azagrebin&quot;&gt;azagrebin&lt;/a&gt; we tracked this down to an issue in the &lt;tt&gt;PartitionRequestQueue&lt;/tt&gt;, where readers are only properly released if they are marked as available (i.e., having credit). If a consumer attempted to close a connection for which the producer has no credit the actual release was skipped.&lt;/p&gt;

&lt;p&gt;This issue only surfaced since the new &lt;tt&gt;BoundedBlockingSubpartition&lt;/tt&gt; only releases data when all readers have been released, contrary to the previous implementation.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=pnowojski&quot; class=&quot;user-hover&quot; rel=&quot;pnowojski&quot;&gt;pnowojski&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=NicoK&quot; class=&quot;user-hover&quot; rel=&quot;NicoK&quot;&gt;NicoK&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Our proposal would be to modify &lt;tt&gt;PartitionRequestQueue#userEventTriggered&lt;/tt&gt; as follows:&lt;/p&gt;

&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;
&lt;span class=&quot;code-comment&quot;&gt;// Cancel the request &lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; the input channel
&lt;/span&gt;&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; size = availableReaders.size();
&lt;span class=&quot;code-keyword&quot;&gt;final&lt;/span&gt; NetworkSequenceViewReader toRelease = allReaders.remove(toCancel);
&lt;span class=&quot;code-comment&quot;&gt;// remove reader from queue of available readers
&lt;/span&gt;&lt;span class=&quot;code-keyword&quot;&gt;for&lt;/span&gt; (&lt;span class=&quot;code-object&quot;&gt;int&lt;/span&gt; i = 0; i &amp;lt; size; i++) {
	NetworkSequenceViewReader reader = pollAvailableReader();
	&lt;span class=&quot;code-keyword&quot;&gt;if&lt;/span&gt; (reader != toRelease) {
		registerAvailableReader(reader);
	}
}
toRelease.releaseAllResources();
markAsReleased(toRelease.getReceiverId());
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;</comment>
                            <comment id="16883968" author="pnowojski" created="Fri, 12 Jul 2019 16:35:27 +0000"  >&lt;p&gt;Thanks for investigating this and this indeed seems wrong. I think the change make sense (at least after a quick look from my side). As I&apos;m away next week, can you &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt; take a look at this (either review &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=azagrebin&quot; class=&quot;user-hover&quot; rel=&quot;azagrebin&quot;&gt;azagrebin&lt;/a&gt;&apos;s/&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chesnay&quot; class=&quot;user-hover&quot; rel=&quot;chesnay&quot;&gt;chesnay&lt;/a&gt;&apos;s fix or implement the fix/tests)?&lt;/p&gt;</comment>
                            <comment id="16885361" author="azagrebin" created="Mon, 15 Jul 2019 16:14:12 +0000"  >&lt;p&gt;Here&#160;is some more clarification about our debugging with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chesnay&quot; class=&quot;user-hover&quot; rel=&quot;chesnay&quot;&gt;chesnay&lt;/a&gt;. We basically found two problems:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;In case of&#160;RemoteInputChannel, producer&apos;s&#160;PartitionRequestQueue#userEventTriggered always gets a&#160;CancelPartitionRequest from each consumer&apos;s&#160;RemoteInputChannel except one before&#160;CloseRequest comes and triggers&#160;PartitionRequestQueue#close. The problem is that&#160;PartitionRequestQueue#userEventTriggered releases the&#160;toCancel reader only if it is in&#160;availableReaders but always removes it from&#160;allReaders. This means that when&#160;PartitionRequestQueue#close is called only not canceled readers&#160;are released, others are leaked in&#160;PartitionRequestQueue#userEventTriggered. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chesnay&quot; class=&quot;user-hover&quot; rel=&quot;chesnay&quot;&gt;chesnay&lt;/a&gt;&#160;has suggested a fix for it but the files are still not deleted when the test job shuts down. They are deleted only when the&#160;TE and network shut down at the end of the test. To fix this, the next point has to be resolved.&lt;/li&gt;
	&lt;li&gt;After our further investigation another problem was found. The final release of&#160;ReleaseOnConsumptionResultPartition happens only if&#160;pendingReferences is zero. This happens only if&#160;notifySubpartitionConsumed is called for all readers in&#160;PartitionRequestQueue.&#160;notifySubpartitionConsumed is currently called only in&#160;PartitionRequestQueue#close but not in case of&#160;CancelPartitionRequest,&#160;handleException and&#160;channelInactive. It means that the partition will linger in those cases and will be released only when the TE and network shut down where&#160;pendingReferences is not checked. It leads to the question of why we have 2 separate reader methods: notifySubpartitionConsumed and&#160;releaseAllResources if they both are&#160;basically parts of partition release. Why do we need&#160;notifySubpartitionConsumed, what does it mean and how is the fact of &apos;being Consumed&apos; used except releasing? It seems both methods&#160;need to be called in all cases:&#160;CancelPartitionRequest,&#160;handleException, channelInactive and&#160;CloseRequest when the partition is not needed any more in PartitionRequestQueue.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16885371" author="zjwang" created="Mon, 15 Jul 2019 16:22:22 +0000"  >&lt;p&gt;Thanks for finding this potential issue and the investigation! &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chesnay&quot; class=&quot;user-hover&quot; rel=&quot;chesnay&quot;&gt;chesnay&lt;/a&gt;&#160; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=azagrebin&quot; class=&quot;user-hover&quot; rel=&quot;azagrebin&quot;&gt;azagrebin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I think the idea of above modifications makes sense, because the `availableReader` is not always equivalent to `allReaders`, then it is proper to find the canceled view reader from `allReaders` instead.&lt;/p&gt;

&lt;p&gt;This issue also exists in previous &lt;tt&gt;SpillableSubpartition&lt;/tt&gt;&#160;which actually uses memory type in &lt;tt&gt;SlotCountExceedingParallelismTest,&lt;/tt&gt;&#160;so we could not find this potential bug then.&lt;/p&gt;

&lt;p&gt;In detail, we should also call `toRelease.notifySubpartitionConsumed` before calling `toRelease.releaseAllResources` in above modifications. Otherwise the reference counter in &lt;tt&gt;ReleaseOnConsumptionResultPartition&lt;/tt&gt;&#160;would not decrease to zero and really release partition via &lt;tt&gt;ResultPartitionManager&lt;/tt&gt;.&lt;/p&gt;

&lt;p&gt;I would submit the PR and add some unite tests later tomorrow.&lt;/p&gt;</comment>
                            <comment id="16885832" author="zjwang" created="Tue, 16 Jul 2019 04:15:48 +0000"  >&lt;p&gt;We have some previous assumptions that ResultSubpartitionView could be released individually, but all the&#160;subpartitions are released together via `ResultPartition/ResultPartitionManager`.&lt;/p&gt;

&lt;p&gt;After thinking it through, it might be reasonable to have both methods as &lt;tt&gt;ResultSubpartitionView#notifySubpartitionConsumed&lt;/tt&gt;&#160;and &lt;tt&gt;ResultSubpartitionView#releaseAllResources&lt;/tt&gt;, because&#160;they describe the different semantics.&#160;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;`releaseAllResources` is used for releasing resources from&#160;ResultSubpartitionView aspect. The view is created by netty stack&#160;which is also responsible for triggering the release. In detail it has two scenarios to trigger release: One case is that netty channel inactive/exception as current &lt;tt&gt;PartitonRequestQueue#channelInactive&lt;/tt&gt;&#160;and &lt;tt&gt;PartitionRequestQueue#exceptionCaught&lt;/tt&gt;&#160;done. The other case is that when ResultSubpartitionView&#160;is actually consumed via &lt;tt&gt;NettyMessage#CannelPartitionRequest&lt;/tt&gt;.&lt;/li&gt;
&lt;/ul&gt;


&lt;ul&gt;
	&lt;li&gt;`notifySubpartitionConsumed` only indicates the ResultSubpartition/View&#160;consumed via &lt;tt&gt;CannelPartitionRequest&lt;/tt&gt;, so we should call this method&#160;while handling the cancel&#160;message. For the case of channel exception/inactive, it does not always indicate the consumption semantic, so we should not call this method as current done in &lt;tt&gt;PartitionRequestQueue&lt;/tt&gt;. It is up to &lt;tt&gt;JobMaster&lt;/tt&gt;&#160;whether to release partition in the case of channel inactive/exception. For the streaming job if the consumer fails, the &lt;tt&gt;JobMaster&lt;/tt&gt;&#160;would also cancel the producer task to release the whole &lt;tt&gt;ResultPartition&lt;/tt&gt;. For the batch job of blocking partition, if the consumer TM exits to cause channel inactive, the &lt;tt&gt;ResultPartition&lt;/tt&gt;&#160;might not need to be released.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Overall, these two methods seem to decouple the release between &lt;tt&gt;ResultPartition&lt;/tt&gt;&#160;and &lt;tt&gt;ResultSubpartitionView&lt;/tt&gt;.&#160;So it makes sense to keep them as now, as long as we could handle the &lt;tt&gt;CannelPartitionRequest&lt;/tt&gt;&#160;message correctly based on above modifications. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=azagrebin&quot; class=&quot;user-hover&quot; rel=&quot;azagrebin&quot;&gt;azagrebin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&#160;&lt;/p&gt;</comment>
                            <comment id="16886116" author="azagrebin" created="Tue, 16 Jul 2019 13:38:57 +0000"  >&lt;p&gt;Thanks for the clarification &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt;!&#160;I have some more questions.&lt;/p&gt;

&lt;p&gt;Firstly, some&#160;comments about the second point&#160;`notifySubpartitionConsumed`. At the moment, the semantics for the partition lifecycle is that there are two cases: release or not on consumption:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;If&#160;release on consumption is set to shuffle service then job master never tries to reuse any partitions. This means that the shuffle service should just monitor the first consumption attempt for each subpartition and independently how it ends (consumed or failed), it should release all subpartitions after one attempt is done for each of them. Job Master will always restart everything and not released partitions from previous attempts will just linger around, even successfully produced ones.&lt;/li&gt;
	&lt;li&gt;If&#160;release is done outside then again&#160;independently from how&#160;any consumption attempt&#160;ends, Job Master will&#160;decide&#160;when to release the&#160;successfully produced partitions. The partition should be auto-released only if the production fails which happens in Task.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;Basically, it looks like (sub)partition needs only some kind of `View is released/done for any reason notification` to clean up readers and count consumption attempts&#160;for auto release.&lt;/p&gt;

&lt;p&gt;Secondly, some questions about the described difference between &apos;releaseAllResources\notifySubpartitionConsumed&apos; in&#160;PartitionRequestQueue if&#160;it is still needed.&lt;/p&gt;

&lt;p&gt;I think the confusion comes&#160;from the name of&#160;&lt;em&gt;CancelPartitionRequest&lt;/em&gt;&#160;which seems to be actually a confirmation of consumption from the consumer to producer at the same time (&lt;em&gt;NettyPartitionRequestClient#close&lt;/em&gt;). Then we should notify it as an end of consumption to the whole partition and it is expected always to happen, right?&#160;This sounds more like &lt;em&gt;Acknowledge-&lt;/em&gt; or&#160;&lt;em&gt;ConfirmPartitionRequest&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;At the same time, it also serves as a cancelation of producer/consumer communication in case of consumer internal channel failure.&#160;At least as I see it in &lt;em&gt;PartitionRequestClientHandler#decodeMsg&lt;/em&gt; calling &lt;em&gt;cancelRequestFor&lt;/em&gt;. This case sounds more like &apos;&lt;em&gt;cancelation&apos;&lt;/em&gt;. But&#160;does it actually mean that we should notify the end of consumption to the whole partition on the producer side? Is it not the&#160;similar case as&#160;channel inactive/exception? The consumption&#160;might have been&#160;not successful but&#160;the&#160;end of consumption notification will lead to the subpartition full release. Could the job master reuse this sub-partition again for the recovered consumer if it tried?&lt;/p&gt;

&lt;p&gt;Also looking more into `CloseRequest`, it will release and confirm&#160;consumption for all channels but does it actually mean that the consumption is done for all of them? Could it be that some of them failed internally in the mean time?&lt;/p&gt;</comment>
                            <comment id="16888788" author="zjwang" created="Fri, 19 Jul 2019 11:01:22 +0000"  >&lt;p&gt;Thanks for these further thoughts and I think it would make things more clearly after&#160;discussion. &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=azagrebin&quot; class=&quot;user-hover&quot; rel=&quot;azagrebin&quot;&gt;azagrebin&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;I could understand your above concerns. I agree that&#160;the current semantics of `CancelPartitionRequest/CloseRequest` are not very accurate, because they could&#160;indicate either successful consumption on consumer side or consumer task fails /any exceptions during consumption.&lt;/p&gt;

&lt;p&gt;Considering the concern of&#160;when to call `notifySubpartitionConsumed`, I think the current implementation is based on whether the producer receives the confirmable notification(Cancel/CloseRequest) from consumer side. If it receives any messages then it would call `notifySubpartitionConsumed` no matter with consumer finishes/fails. In the case of handling channel exception, it only happens in consumer locally, so it would not call `notifySubpartitionConsumed`. Also for the case of channel inactive, the producer could not distinguish whether it is caused by&#160;initiative close connection on consumer side or TM lost exceptionally, so it would not call ` notifySubpartitionConsumed`.&lt;/p&gt;

&lt;p&gt;For the first concern, we could provide the more definitely messages for clearly semantics of consumption successful/failed instead of current `Cancel/ClosePartition`. For the second concern we could also consider the proper way for handling messages/exception/inactive. But one precondition is that we should confirm the specific semantic of releasing partition based on consumption in partition management feature.&lt;/p&gt;

&lt;p&gt;Currently there are three strategies&#160;which could release partition:&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;partition release based on consumers confirmation via network&lt;/li&gt;
	&lt;li&gt;partition release based on JM notification&lt;/li&gt;
	&lt;li&gt;partition release when disconnection between TM/JM&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;For the first strategy (partition release based on consumers confirmation):&#160;&lt;/p&gt;
&lt;ul&gt;
	&lt;li&gt;We could define&#160;the network message as `ReleasePartition` instead of current `Cancel/ClosePartition`. Then&#160;it might not care&#160;about whether the consumer finishes/fails during consumption. The precondition for this way is reliable network notification, but actually we have no ack mechanism for such message in application layer. Even&#160;if consumer task fails before establishing the connection with producer, we still&#160;need rely on JM notification of releasing partitions of producers.&lt;/li&gt;
	&lt;li&gt;We could not provide specific semantic as now, and the current strategy is only coupling with existing mechanism in network stack.&lt;/li&gt;
	&lt;li&gt;The semantic actually could be defined clearly as partition release based on&#160;one successful consumption. And considering the implementation it could be done by both consumer notification and JM notification.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;In general I think we should consider how to define different release strategies which should provide specific semantics, and&#160;not caring about implementations when thinking about strategy. Actually any strategy could be implemented in multiple ways. E.g. the semantics might be divided into at-least once consumption, exactly-once consumption and at-most once consumption. After we confirm the specific semantics, then we would know how to refactor the current network stack considering implementation for certain strategy.&lt;/p&gt;

&lt;p&gt;It might need worth further re-architecture the partition release strategy in release-1.10, because the feature of interactive queries is difficult to expand&#160;another strategy based on current architecture. In order not to block current release, the existing modifications could solve the file leak issue I think.&lt;/p&gt;</comment>
                            <comment id="16888982" author="nicok" created="Fri, 19 Jul 2019 15:42:33 +0000"  >&lt;p&gt;I agree with &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt; - changing the semantics should be tackled separately, not necessarily as part of this bug fix. I&apos;ll see when I have time to look at the PR so we can get this merged&lt;/p&gt;</comment>
                            <comment id="16889022" author="zjwang" created="Fri, 19 Jul 2019 16:31:18 +0000"  >&lt;p&gt;After confirming the comments from &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=chesnay&quot; class=&quot;user-hover&quot; rel=&quot;chesnay&quot;&gt;chesnay&lt;/a&gt; in PR, I found that for the case of `SlotCountExceedingParallelismTest` it would not generate `ReleaseOnConsumptionResultPartition` because the partition is blocking type. So&#160;the reference counter would not be used in `ResultPartition`, and the files for bounded blocking partition could be released finally via calling `TaskExecutorGateway#releasePartitions` based on `RegionPartitionReleaseStrategy`.&lt;/p&gt;

&lt;p&gt;The&#160;description of this jira&#160;ticket might&#160;not be&#160;accurate. In my local running this test in Mac system, it has no file leaks after finished. I am not sure why it has file leaks in windows system and I guess it might be relevant with mmap internal mechanism in different systems. I would double verify this test in windows system.&lt;/p&gt;

&lt;p&gt;My PR modifications seems only for the case of pipelined partition which is using `ReleaseOnConsumptionResultPartition`, then the call of `notifySubpartitionConsumed` would make the reference counter become 0 finally to trigger release. But for the pipelined partition it is no issues for persistent file.&lt;/p&gt;</comment>
                            <comment id="16890145" author="azagrebin" created="Mon, 22 Jul 2019 13:00:07 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt;&#160;true, to reproduce the failure,&#160;JobManagerOptions.FORCE_PARTITION_RELEASE_ON_CONSUMPTION has to be true (it has been recently changed to false by default).&lt;/p&gt;</comment>
                            <comment id="16890272" author="azagrebin" created="Mon, 22 Jul 2019 15:42:39 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=NicoK&quot; class=&quot;user-hover&quot; rel=&quot;NicoK&quot;&gt;NicoK&lt;/a&gt;&lt;br/&gt;
I agree that we should address the semantics of partition lifecycle separately. But the fine grained recovery is already implemented and planned for the release. I just wanted to make sure that network stack and this fix&#160;are in sync with the lifecycle semantics and the&#160;fine grained recovery effort.&lt;/p&gt;

&lt;p&gt;`release on consumption` notion was an optimisation to save RPC release calls from JM and fallback to the previous behaviour, but as &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt; pointed out this internal release&#160;is unreliable atm. If activated, it&#160;may always be done now, also in case of consumer failure (both notify/release), but only on a best effort. It means we have to adjust JM and send&#160;RPC release at least in case of consumer failure or just producer restart (&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-13371&quot; title=&quot;Release partitions in JM if producer restarts&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-13371&quot;&gt;&lt;del&gt;FLINK-13371&lt;/del&gt;&lt;/a&gt;). When&#160;the fine-grained recovery is stable we might not need this option at all and can simplify network stack later if needed.&lt;/p&gt;</comment>
                            <comment id="16890630" author="zjwang" created="Tue, 23 Jul 2019 02:50:03 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=azagrebin&quot; class=&quot;user-hover&quot; rel=&quot;azagrebin&quot;&gt;azagrebin&lt;/a&gt;&#160;I totally agree with your above point. The first priority should make the current fine grained recovery work in release-1.9. As we confirmed before, the consumption notification via network is just best-effort atm, not always reliable especially when the network connection is not established during consumer failed.&lt;/p&gt;

&lt;p&gt;I remembered that the JM would always release partitions while restarting producer tasks before, maybe I missed some parts while reviewing the relevant PRs of partition lifecycle feature. I am sorry for not giving this potential issue from network stack before.&lt;/p&gt;

&lt;p&gt;We could solve the current issue in &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-13771&quot; title=&quot;Support kqueue Netty transports (MacOS)&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-13771&quot;&gt;FLINK-13771&lt;/a&gt; now, and further address the semantics of partition release and refactor&#160;the network behavior if necessary in release-1.10.&lt;/p&gt;</comment>
                            <comment id="16890921" author="azagrebin" created="Tue, 23 Jul 2019 11:54:44 +0000"  >&lt;p&gt;&lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=zjwang&quot; class=&quot;user-hover&quot; rel=&quot;zjwang&quot;&gt;zjwang&lt;/a&gt;&#160;Thanks for the confirmation! I guess we all overlooked the&#160;potential issue.&lt;/p&gt;</comment>
                            <comment id="16892005" author="stephanewen" created="Wed, 24 Jul 2019 17:02:30 +0000"  >&lt;p&gt;Thanks for this discussion. I commented on the PR suggestion to always call &lt;tt&gt;notifySubpartitionConsumed()&lt;/tt&gt; when releasing a reader.&lt;/p&gt;

&lt;p&gt;My suggestion for Flink 1.10 would be:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Drop &lt;tt&gt;notifySubpartitionConsumed()&lt;/tt&gt; completely&lt;/li&gt;
	&lt;li&gt;Drop the &lt;tt&gt;ReleaseOnConsumptionResultPartition&lt;/tt&gt;&lt;/li&gt;
	&lt;li&gt;For bounded blocking partitions, the release happens always from the scheduler (no &lt;tt&gt;JobManagerOptions.FORCE_PARTITION_RELEASE_ON_CONSUMPTION&lt;/tt&gt; any more)&lt;/li&gt;
	&lt;li&gt;Pipelined subpartitions are released when the one and only reader/view is released. There can be no further reader, so might as well immediate release it. The result partition as whole is released when all subpartitions are released.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16892023" author="stephanewen" created="Wed, 24 Jul 2019 17:24:17 +0000"  >&lt;p&gt;An observation on the current state of the implementation of &lt;tt&gt;ReleaseOnConsumptionResultPartition&lt;/tt&gt;&lt;/p&gt;

&lt;p&gt;We have a weird inconsistent situation where we&lt;br/&gt;
  1. have a count down on the ResultPartition level about the &lt;tt&gt;notifySubpartitionConsumed()&lt;/tt&gt; calls from the subpartitions, to release the partition when all subpartitions are released.&lt;br/&gt;
  2. have results that can have multiple readers/views and can hence receive multiple release/consumed calls.&lt;/p&gt;

&lt;p&gt;This can probably lead to counting two notifications from one subpartition, and then releasing too early, unless there is super careful accounting when to notify about consumption and when not to.&lt;br/&gt;
This careful accounting seems super fragile to me, given the state of the netty stack. I expect that we will have issues were we either notify too often (early release) or not often enough (lingering files).&lt;/p&gt;

&lt;p&gt;I would suggest to do the following: &lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Change &lt;tt&gt;ReleaseOnConsumptionResultPartition&lt;/tt&gt; to have a flag per subpartition that tracks whether there was a &lt;tt&gt;notifySubpartitionConsumed()&lt;/tt&gt; call or not, so that multiple calls are idempotent.&lt;/li&gt;
	&lt;li&gt;Always send &lt;tt&gt;notifySubpartitionConsumed()&lt;/tt&gt; calls with every &lt;tt&gt;releaseAllResources()&lt;/tt&gt; call.&lt;/li&gt;
&lt;/ul&gt;
</comment>
                            <comment id="16893427" author="zjwang" created="Fri, 26 Jul 2019 07:55:12 +0000"  >&lt;p&gt;Thanks for joining this discussion and giving helpful suggestions &lt;a href=&quot;https://issues.apache.org/jira/secure/ViewProfile.jspa?name=sewen&quot; class=&quot;user-hover&quot; rel=&quot;sewen&quot;&gt;sewen&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I agree with the further refactoring of above relevant&#160;processes for partition release in release-1.10.&lt;/p&gt;

&lt;p&gt;Considering the inconsistent situation mentioned above, actually it could work correctly by default now. Because the reference counter in `ReleaseOnConsumptionResultPartition` would only be used for the case of pipelined partitions by default. For the blocking partitions which would be consumed multiple times via creating multiple readers/views, the release should be controlled by the scheduler.&lt;/p&gt;

&lt;p&gt;But I agree the above logic is fragile and might cause inconsistent easily if not setting correctly. We could further refactor this issue later.&lt;/p&gt;</comment>
                            <comment id="16895133" author="stephanewen" created="Mon, 29 Jul 2019 10:27:13 +0000"  >&lt;p&gt;Fixed in&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;1.9.0 as of 7116ab71edc183d34d128453e06a3efc15ad8905&lt;/li&gt;
	&lt;li&gt;master as of 71a53d49d1c6ce0e5f840e1b528cb75323dc2665&lt;/li&gt;
&lt;/ul&gt;
</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="12310560">
                    <name>Problem/Incident</name>
                                            <outwardlinks description="causes">
                                        <issuelink>
            <issuekey id="13236024">FLINK-12655</issuekey>
        </issuelink>
            <issuelink>
            <issuekey id="13235939">FLINK-12645</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                            <outwardlinks description="relates to">
                                        <issuelink>
            <issuekey id="13246357">FLINK-13371</issuekey>
        </issuelink>
                            </outwardlinks>
                                                        </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            6 years, 16 weeks, 1 day ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i31e3o:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>