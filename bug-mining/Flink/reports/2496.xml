<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:34:00 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-9444] Add full SQL support for Avro formats</title>
                <link>https://issues.apache.org/jira/browse/FLINK-9444</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;When some Avro schema has map/array fields and the corresponding TableSchema declares&#160;&lt;b&gt;MapTypeInfo/ListTypeInfo&lt;/b&gt;&#160;for these fields, an exception will be thrown when registering the&#160;&lt;b&gt;KafkaAvroTableSource&lt;/b&gt;, complaining like:&lt;/p&gt;

&lt;p&gt;Exception in thread &quot;main&quot; org.apache.flink.table.api.ValidationException: Type Map&amp;lt;String, Integer&amp;gt; of table field &apos;event&apos; does not match with type GenericType&amp;lt;java.util.Map&amp;gt; of the field &apos;event&apos; of the TableSource return type.&lt;br/&gt;
 at org.apache.flink.table.api.ValidationException$.apply(exceptions.scala:74)&lt;br/&gt;
 at org.apache.flink.table.sources.TableSourceUtil$$anonfun$validateTableSource$1.apply(TableSourceUtil.scala:92)&lt;br/&gt;
 at org.apache.flink.table.sources.TableSourceUtil$$anonfun$validateTableSource$1.apply(TableSourceUtil.scala:71)&lt;br/&gt;
 at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&lt;br/&gt;
 at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)&lt;br/&gt;
 at org.apache.flink.table.sources.TableSourceUtil$.validateTableSource(TableSourceUtil.scala:71)&lt;br/&gt;
 at org.apache.flink.table.plan.schema.StreamTableSourceTable.&amp;lt;init&amp;gt;(StreamTableSourceTable.scala:33)&lt;br/&gt;
 at org.apache.flink.table.api.StreamTableEnvironment.registerTableSourceInternal(StreamTableEnvironment.scala:124)&lt;br/&gt;
 at org.apache.flink.table.api.TableEnvironment.registerTableSource(TableEnvironment.scala:438)&lt;/p&gt;</description>
                <environment></environment>
        <key id="13162224">FLINK-9444</key>
            <summary>Add full SQL support for Avro formats</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="1" iconUrl="https://issues.apache.org/jira/images/icons/priorities/blocker.svg">Blocker</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="junz">Jun Zhang</assignee>
                                    <reporter username="junz">Jun Zhang</reporter>
                        <labels>
                            <label>patch</label>
                            <label>pull-request-available</label>
                    </labels>
                <created>Sat, 26 May 2018 12:25:07 +0000</created>
                <updated>Fri, 3 Aug 2018 07:55:15 +0000</updated>
                            <resolved>Tue, 3 Jul 2018 15:18:37 +0000</resolved>
                                    <version>1.6.0</version>
                                    <fixVersion>1.6.0</fixVersion>
                                    <component>Connectors / Kafka</component>
                    <component>Table SQL / API</component>
                        <due></due>
                            <votes>0</votes>
                                    <watches>2</watches>
                                                                                                                <comments>
                            <comment id="16491635" author="junz" created="Sat, 26 May 2018 12:28:17 +0000"  >&lt;p&gt;I&apos;ve already fixed it and attached the patch.&lt;/p&gt;</comment>
                            <comment id="16491644" author="githubbot" created="Sat, 26 May 2018 12:50:40 +0000"  >&lt;p&gt;GitHub user tragicjun opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9444&quot; title=&quot;Add full SQL support for Avro formats&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9444&quot;&gt;&lt;del&gt;FLINK-9444&lt;/del&gt;&lt;/a&gt; KafkaAvroTableSource failed to work for map fields&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    Once some Avro schema has map fields, an exception will be thrown when registering the KafkaAvroTableSource, complaining like:&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    Exception in thread &quot;main&quot; org.apache.flink.table.api.ValidationException: Type Map&amp;lt;String, String&amp;gt; of table field &apos;event&apos; does not match with type GenericType&amp;lt;java.util.Map&amp;gt; of the field &apos;event&apos; of the TableSource return type.&lt;br/&gt;
    at org.apache.flink.table.api.ValidationException$.apply(exceptions.scala:74)&lt;br/&gt;
    at org.apache.flink.table.sources.TableSourceUtil$$anonfun$validateTableSource$1.apply(TableSourceUtil.scala:92)&lt;br/&gt;
    at org.apache.flink.table.sources.TableSourceUtil$$anonfun$validateTableSource$1.apply(TableSourceUtil.scala:71)&lt;br/&gt;
    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)&lt;br/&gt;
    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)&lt;br/&gt;
    at org.apache.flink.table.sources.TableSourceUtil$.validateTableSource(TableSourceUtil.scala:71)&lt;br/&gt;
    at org.apache.flink.table.plan.schema.StreamTableSourceTable.&amp;lt;init&amp;gt;(StreamTableSourceTable.scala:33)&lt;br/&gt;
    at org.apache.flink.table.api.StreamTableEnvironment.registerTableSourceInternal(StreamTableEnvironment.scala:124)&lt;br/&gt;
    at org.apache.flink.table.api.TableEnvironment.registerTableSource(TableEnvironment.scala:438)&lt;br/&gt;
    ```&lt;/p&gt;

&lt;p&gt;    This pull request adds a new unit test to expose the issue and then fixes it. &lt;/p&gt;

&lt;p&gt;    &lt;b&gt;Note: In this implementation, following Avro primitive value types are supported: string, int, long, float, double and boolean, which should cater for most use cases.&lt;/b&gt;&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Brief change log&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Add a new unit test &quot;testHasMapFieldsAvroClass()&quot; in KafkaAvroTableSourceTestBase&lt;/li&gt;
	&lt;li&gt;Add some logic in &quot;AvroTestUtils.createFlatAvroSchema()&quot; to create Avro MapSchema&lt;/li&gt;
	&lt;li&gt;Add some logic in &quot;AvroRecordClassConverter.convertType()&quot; to convert &quot;GenericType&amp;lt;java.util.Map&amp;gt;&quot;  into &quot;MapTypeInfo&quot; with matching value types.&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    This change can be verified as follows:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Run the unit test &quot;testHasMapFieldsAvroClass()&quot;  added in KafkaAvroTableSourceTestBase by this fix.&lt;/li&gt;
	&lt;li&gt;The unit test would fail with similar exceptions thrown described above.&lt;/li&gt;
	&lt;li&gt;Merge this fix and run the unit test again, it should pass&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): (no)&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: (no)&lt;/li&gt;
	&lt;li&gt;The serializers: ( no)&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): (no)&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: (no)&lt;/li&gt;
	&lt;li&gt;The S3 file system connector: (no)&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? (no)&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? (not applicable)&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/tragicjun/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/tragicjun/flink&lt;/a&gt; master&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #6082&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 802e7e211b7bea6fd17b88a058591272f0fb215f&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-16T16:27:32Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9384&quot; title=&quot;KafkaAvroTableSource failed to work due to type mismatch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9384&quot;&gt;&lt;del&gt;FLINK-9384&lt;/del&gt;&lt;/a&gt;KafkaAvroTableSource failed to work due to type mismatch&lt;/p&gt;

&lt;p&gt;commit b731f98ff3ca920883bc3c9daebb599c25049c0d&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-17T03:00:58Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9384&quot; title=&quot;KafkaAvroTableSource failed to work due to type mismatch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9384&quot;&gt;&lt;del&gt;FLINK-9384&lt;/del&gt;&lt;/a&gt;KafkaAvroTableSource failed to work due to type mismatch&lt;/p&gt;

&lt;p&gt;commit f291a34debca992ea675b75ffdb4358dfbfa3b47&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-19T07:06:24Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9384&quot; title=&quot;KafkaAvroTableSource failed to work due to type mismatch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9384&quot;&gt;&lt;del&gt;FLINK-9384&lt;/del&gt;&lt;/a&gt;KafkaAvroTableSource failed to work due to type mismatch&lt;/p&gt;

&lt;p&gt;commit 61d2081ef7f8aa3669d9774da6149d4020d9581c&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-19T07:34:20Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9384&quot; title=&quot;KafkaAvroTableSource failed to work due to type mismatch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9384&quot;&gt;&lt;del&gt;FLINK-9384&lt;/del&gt;&lt;/a&gt;KafkaAvroTableSource failed to work due to type mismatch&lt;/p&gt;

&lt;p&gt;commit d3d1afb710858b8f3cce988541ef3e805bd75b03&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-23T15:01:10Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9384&quot; title=&quot;KafkaAvroTableSource failed to work due to type mismatch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9384&quot;&gt;&lt;del&gt;FLINK-9384&lt;/del&gt;&lt;/a&gt;KafkaAvroTableSource failed to work due to type mismatch&lt;/p&gt;

&lt;p&gt;commit 112873fd9cca097db7948d8454a3d66c5dd2b32f&lt;br/&gt;
Author: tragicjun &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-23T15:06:32Z&lt;/p&gt;

&lt;p&gt;    Merge branch &apos;master&apos; into master&lt;/p&gt;

&lt;p&gt;commit e450e8b64c066339331e158e6d599b2599636d55&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-23T15:55:51Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9384&quot; title=&quot;KafkaAvroTableSource failed to work due to type mismatch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9384&quot;&gt;&lt;del&gt;FLINK-9384&lt;/del&gt;&lt;/a&gt;KafkaAvroTableSource failed to work due to type mismatch&lt;/p&gt;

&lt;p&gt;commit 33349a82b3547e09e845f3d1d844d80a0ed0c091&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-24T13:22:13Z&lt;/p&gt;

&lt;p&gt;    revert &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9384&quot; title=&quot;KafkaAvroTableSource failed to work due to type mismatch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9384&quot;&gt;&lt;del&gt;FLINK-9384&lt;/del&gt;&lt;/a&gt; changes&lt;/p&gt;

&lt;p&gt;commit 8cc12a112ea673c3ec2794949b1a6ab63e855195&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-24T13:24:49Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9384&quot; title=&quot;KafkaAvroTableSource failed to work due to type mismatch&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9384&quot;&gt;&lt;del&gt;FLINK-9384&lt;/del&gt;&lt;/a&gt;KafkaAvroTableSource failed to work due to type mismatch&lt;/p&gt;

&lt;p&gt;commit d91a475a328e051f4717ec8b95be7adff92a3913&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-26T08:15:17Z&lt;/p&gt;

&lt;p&gt;    Sync with upstream&lt;/p&gt;

&lt;p&gt;commit 84ac010f0480342fa5fdf912d7fb10ff1f444900&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-26T08:17:03Z&lt;/p&gt;

&lt;p&gt;    Sync with upstream&lt;/p&gt;

&lt;p&gt;commit 5940fcbf7988a898a3e961f65d34f5711c17a5c4&lt;br/&gt;
Author: jerryjzhang &amp;lt;zhangjun2915@...&amp;gt;&lt;br/&gt;
Date:   2018-05-26T12:29:05Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9444&quot; title=&quot;Add full SQL support for Avro formats&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9444&quot;&gt;&lt;del&gt;FLINK-9444&lt;/del&gt;&lt;/a&gt;KafkaAvroTableSource failed to work for map fields&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16491646" author="githubbot" created="Sat, 26 May 2018 12:57:47 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Please ignore previous commits, only the latest commit is relevant to this issue.&lt;/p&gt;</comment>
                            <comment id="16495255" author="githubbot" created="Wed, 30 May 2018 14:43:58 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I encountered another exception working with string type in Avro map/array, any advice whether I should open a separate issue or just reusing this one.&lt;/p&gt;</comment>
                            <comment id="16496732" author="githubbot" created="Thu, 31 May 2018 15:44:42 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @twalthr @suez1224 would you please review on this?&lt;/p&gt;</comment>
                            <comment id="16497530" author="githubbot" created="Fri, 1 Jun 2018 04:05:28 +0000"  >&lt;p&gt;Github user suez1224 commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks for the PR, @tragicjun. I will take a look in the next few days.&lt;/p&gt;</comment>
                            <comment id="16501331" author="githubbot" created="Tue, 5 Jun 2018 06:12:27 +0000"  >&lt;p&gt;Github user suez1224 commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r192955182&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r192955182&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) &lt;/p&gt;
{
    +				// avro map key is always string
    +				return Types.MAP(Types.STRING,
    +					convertPrimitiveType(schema.getValueType().getType()));
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == List.class &amp;amp;&amp;amp;&lt;br/&gt;
    +				schema.getType() == Schema.Type.ARRAY) {&lt;br/&gt;
    +				return Types.LIST(convertPrimitiveType(schema.getElementType().getType()));&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I dont think Flink Table &amp;amp; SQL support LIST, please see org.apache.flink.table.api.Types.&lt;/p&gt;</comment>
                            <comment id="16501379" author="githubbot" created="Tue, 5 Jun 2018 06:59:57 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r192771502&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r192771502&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) &lt;/p&gt;
{
    +				// avro map key is always string
    +				return Types.MAP(Types.STRING,
    +					convertPrimitiveType(schema.getValueType().getType()));
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == List.class &amp;amp;&amp;amp;&lt;br/&gt;
    +				schema.getType() == Schema.Type.ARRAY) {&lt;br/&gt;
    +				return Types.LIST(convertPrimitiveType(schema.getElementType().getType()));&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Call this function recursively. Btw also update the method docs about this behavior.&lt;/p&gt;</comment>
                            <comment id="16501380" author="githubbot" created="Tue, 5 Jun 2018 06:59:57 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r192768664&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r192768664&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaAvroTableSourceTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -128,6 +130,82 @@ public void testDifferentFieldsAvroClass() &lt;/p&gt;
{
     			source.getDataStream(StreamExecutionEnvironment.getExecutionEnvironment()).getType());
     	}

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testHasMapFieldsAvroClass() {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I think we don&apos;t need changes in Kafka-related classes. This is an issue with the `AvroRowDeserializationSchema` and should be covered by the `AvroRowDeSerializationSchemaTest`.&lt;/p&gt;</comment>
                            <comment id="16501381" author="githubbot" created="Tue, 5 Jun 2018 06:59:57 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r192770567&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r192770567&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) {&lt;br/&gt;
    +				// avro map key is always string&lt;br/&gt;
    +				return Types.MAP(Types.STRING,&lt;br/&gt;
    +					convertPrimitiveType(schema.getValueType().getType()));&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The value must not be primitive. Call this function recursively instead?&lt;/p&gt;</comment>
                            <comment id="16501382" author="githubbot" created="Tue, 5 Jun 2018 06:59:57 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r192965275&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r192965275&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    If you update this converter class, you should also update the corresponding runtime classes in `org.apache.flink.formats.avro.AvroRow(De)SerializationSchema`&lt;/p&gt;</comment>
                            <comment id="16501383" author="githubbot" created="Tue, 5 Jun 2018 06:59:58 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r192771700&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r192771700&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) &lt;/p&gt;
{
    +				// avro map key is always string
    +				return Types.MAP(Types.STRING,
    +					convertPrimitiveType(schema.getValueType().getType()));
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == List.class &amp;amp;&amp;amp;&lt;br/&gt;
    +				schema.getType() == Schema.Type.ARRAY) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Is this check necessary? If yes, why is it not necessary for Maps?&lt;/p&gt;</comment>
                            <comment id="16501625" author="githubbot" created="Tue, 5 Jun 2018 11:39:05 +0000"  >&lt;p&gt;Github user tragicjun commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r193039834&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r193039834&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) &lt;/p&gt;
{
    +				// avro map key is always string
    +				return Types.MAP(Types.STRING,
    +					convertPrimitiveType(schema.getValueType().getType()));
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == List.class &amp;amp;&amp;amp;&lt;br/&gt;
    +				schema.getType() == Schema.Type.ARRAY) {&lt;br/&gt;
    +				return Types.LIST(convertPrimitiveType(schema.getElementType().getType()));&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    yes, org.apache.flink.table.api.Types doesn&apos;t support LIST, but org.apache.flink.api.common.typeinfo.Types does. The Avro array type would be converted to java List type. Can we add LIST in org.apache.flink.table.api.Types to support Avro arrays?&lt;/p&gt;</comment>
                            <comment id="16501842" author="githubbot" created="Tue, 5 Jun 2018 14:04:19 +0000"  >&lt;p&gt;Github user tragicjun commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r193084082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r193084082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaAvroTableSourceTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -128,6 +130,82 @@ public void testDifferentFieldsAvroClass() &lt;/p&gt;
{
     			source.getDataStream(StreamExecutionEnvironment.getExecutionEnvironment()).getType());
     	}

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testHasMapFieldsAvroClass() {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The issue is exposed when using KafkaAvroTableSource, but moving the unit tests to AvroRowDeSerializationSchemaTest should be fine.&lt;/p&gt;</comment>
                            <comment id="16501858" author="githubbot" created="Tue, 5 Jun 2018 14:25:27 +0000"  >&lt;p&gt;Github user tragicjun commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r193092151&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r193092151&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) {&lt;br/&gt;
    +				// avro map key is always string&lt;br/&gt;
    +				return Types.MAP(Types.STRING,&lt;br/&gt;
    +					convertPrimitiveType(schema.getValueType().getType()));&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    This function requires &quot;TypeInformation&amp;lt;?&amp;gt; extracted, Schema schema&quot; , but we can only get &quot;org.apache.avro.Schema.Type&quot; from Avro MapSchema (value type) and ArraySchema (element type). &lt;/p&gt;</comment>
                            <comment id="16501867" author="githubbot" created="Tue, 5 Jun 2018 14:31:19 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thans @suez1224 @twalthr for reviewing. I&apos;ve moved the unit tests from KafkaAvroTableSourceTestBase to AvroRowDeSerializationSchemaTest. As for other comments, plz see my comments.&lt;/p&gt;</comment>
                            <comment id="16501890" author="githubbot" created="Tue, 5 Jun 2018 14:48:23 +0000"  >&lt;p&gt;Github user tragicjun commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r193101536&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r193101536&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) {&lt;br/&gt;
    +				// avro map key is always string&lt;br/&gt;
    +				return Types.MAP(Types.STRING,&lt;br/&gt;
    +					convertPrimitiveType(schema.getValueType().getType()));&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    do you actually mean the value *&lt;b&gt;might&lt;/b&gt;* not be primitive?&lt;/p&gt;</comment>
                            <comment id="16501896" author="githubbot" created="Tue, 5 Jun 2018 14:49:58 +0000"  >&lt;p&gt;Github user tragicjun commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r193102162&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r193102162&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) &lt;/p&gt;
{
    +				// avro map key is always string
    +				return Types.MAP(Types.STRING,
    +					convertPrimitiveType(schema.getValueType().getType()));
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == List.class &amp;amp;&amp;amp;&lt;br/&gt;
    +				schema.getType() == Schema.Type.ARRAY) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    it is necessary because List.class doesn&apos;t mean the Schema.Type must be ARRAY. But I think it should be better use Schema.Type to do it.&lt;/p&gt;</comment>
                            <comment id="16502016" author="githubbot" created="Tue, 5 Jun 2018 16:09:10 +0000"  >&lt;p&gt;Github user tragicjun commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r193132257&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r193132257&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) {&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    could you explain more about this? I didn&apos;t find any coupling between AvroRecordClassConverter and AvroRow(De)SerializationSchema. But I did encounter &quot;UTF8&amp;lt;-&amp;gt;String&quot; cast problem during my integration which I was not sure if I should open a separate issue.&lt;/p&gt;</comment>
                            <comment id="16502043" author="githubbot" created="Tue, 5 Jun 2018 16:26:57 +0000"  >&lt;p&gt;Github user tragicjun commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r193137905&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r193137905&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) {&lt;br/&gt;
    +				// avro map key is always string&lt;br/&gt;
    +				return Types.MAP(Types.STRING,&lt;br/&gt;
    +					convertPrimitiveType(schema.getValueType().getType()));&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    if the value is not primitive, say another record, how could we get the *&lt;b&gt;TypeInformation extracted&lt;/b&gt;&lt;b&gt;? One solution is to get full class name of the map value type and then use reflection to get the class type of it and pass the class type to **convert(Class&amp;lt;T&amp;gt; avroClass)&lt;/b&gt;*. Any better idea?&lt;/p&gt;</comment>
                            <comment id="16502215" author="githubbot" created="Tue, 5 Jun 2018 18:04:08 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks @suez1224 @twalthr for reviewing. Plz see my comments and latest commits as per your comments.&lt;/p&gt;</comment>
                            <comment id="16502216" author="githubbot" created="Tue, 5 Jun 2018 18:04:12 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks @suez1224 @twalthr for reviewing. Plz see my comments and latest commits as per your comments.&lt;/p&gt;</comment>
                            <comment id="16502220" author="githubbot" created="Tue, 5 Jun 2018 18:06:43 +0000"  >&lt;p&gt;Github user tragicjun commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082#discussion_r193168591&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#discussion_r193168591&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/typeutils/AvroRecordClassConverter.java &amp;#8212;&lt;br/&gt;
    @@ -73,9 +75,37 @@ private AvroRecordClassConverter() {&lt;br/&gt;
     			final GenericTypeInfo&amp;lt;?&amp;gt; genericTypeInfo = (GenericTypeInfo&amp;lt;?&amp;gt;) extracted;&lt;br/&gt;
     			if (genericTypeInfo.getTypeClass() == Utf8.class) &lt;/p&gt;
{
     				return BasicTypeInfo.STRING_TYPE_INFO;
    +			}
&lt;p&gt; else if (genericTypeInfo.getTypeClass() == Map.class) {&lt;br/&gt;
    +				// avro map key is always string&lt;br/&gt;
    +				return Types.MAP(Types.STRING,&lt;br/&gt;
    +					convertPrimitiveType(schema.getValueType().getType()));&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I&apos;ve implemented a reflection version, which now supports record type within map/array.&lt;/p&gt;</comment>
                            <comment id="16502225" author="githubbot" created="Tue, 5 Jun 2018 18:09:43 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Particularly I suggest that we add LIST in org.apache.flink.table.api.Types to support Avro array types. I can submit it in next commits if you guys think the same.&lt;/p&gt;</comment>
                            <comment id="16503120" author="githubbot" created="Wed, 6 Jun 2018 10:50:47 +0000"  >&lt;p&gt;Github user fhueske commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    We treat sequences of values as arrays in SQL and the Table API. There are no built-in functions to handle lists. So we should return the values as an array, and hence don&apos;t need a List type.&lt;/p&gt;</comment>
                            <comment id="16503559" author="githubbot" created="Wed, 6 Jun 2018 16:38:06 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Hi @fhueske , &lt;/p&gt;

&lt;p&gt;    Avro array type actually is mapped to Java List type, specifically the class *&lt;b&gt;org.apache.avro.generic.GenericData.Array&lt;/b&gt;* extends *&lt;b&gt;java.util.AbstractList&lt;/b&gt;&lt;b&gt;. I tried to convert the List  to an Array in **AvroRowDeserializationSchema&lt;/b&gt;&lt;b&gt;, but to make it generic an **Object []&lt;/b&gt;* must be returned, which would then lead to a cast problem when passing the *&lt;b&gt;Object []&lt;/b&gt;* to TypeSerializer.copy().&lt;/p&gt;

&lt;p&gt;    I tried using *&lt;b&gt;ListTypeInfo&lt;/b&gt;* to declare corresponding Avro array type, it was just working fine, as we already have *&lt;b&gt;ListSerializer&lt;/b&gt;* to support it.&lt;/p&gt;</comment>
                            <comment id="16504037" author="githubbot" created="Wed, 6 Jun 2018 23:22:14 +0000"  >&lt;p&gt;Github user fhueske commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I think we have to return an typed array here. A `List` won&apos;t be supported by the built-in SQL functions. &lt;/p&gt;

&lt;p&gt;    There are a few tricks on can play to create typed arrays, even in static code like&lt;/p&gt;

&lt;p&gt;    ```&lt;br/&gt;
    Object[] array = (Object[]) Array.newInstance(clazz, length);&lt;br/&gt;
    ```&lt;/p&gt;

&lt;p&gt;    Have a look at the code of the ORC InputFormat that had to solve a similar challenge: &lt;span class=&quot;error&quot;&gt;&amp;#91;OrcBatchReader.java&amp;#93;&lt;/span&gt;(&lt;a href=&quot;https://github.com/apache/flink/blob/master/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/blob/master/flink-connectors/flink-orc/src/main/java/org/apache/flink/orc/OrcBatchReader.java&lt;/a&gt;).&lt;/p&gt;</comment>
                            <comment id="16504249" author="githubbot" created="Thu, 7 Jun 2018 05:04:56 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @fhueske Great, let me take a look and commit another version later. &lt;/p&gt;</comment>
                            <comment id="16504685" author="githubbot" created="Thu, 7 Jun 2018 13:35:04 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The latest commit uses *&lt;b&gt;Types.OBJECT_ARRAY&lt;/b&gt;* to map Avro array type. Hence, Avro *&lt;b&gt;GenericData.Array&lt;/b&gt;* has to be converted into regular java arrays back (see *&lt;b&gt;AvroRowSerializationSchema&lt;/b&gt;&lt;b&gt;) and forth(see **AvroRowDeserializationSchema&lt;/b&gt;*). Moreover, nested record within Avro map/array is also supported.&lt;/p&gt;

&lt;p&gt;    The unit tests and my local integration tests have passed. Would you please review? @fhueske @twalthr @suez1224 &lt;/p&gt;</comment>
                            <comment id="16510559" author="githubbot" created="Wed, 13 Jun 2018 02:55:49 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @suez1224 @twalthr any update please?&lt;/p&gt;</comment>
                            <comment id="16511144" author="githubbot" created="Wed, 13 Jun 2018 13:32:59 +0000"  >&lt;p&gt;Github user twalthr commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks for the update @tragicjun. I had a look at the changes. I think the logic does still not cover all cases that we want to cover. Avro has the following types:&lt;br/&gt;
    `RECORD, ENUM, ARRAY, MAP, UNION, FIXED, STRING, BYTES, INT, LONG, FLOAT, DOUBLE, BOOLEAN, NULL`&lt;br/&gt;
    And they should all be covered in the converter and ser/deser schemes. Would it be ok for you if I try to simplify the logic a bit and build on top of your commits?&lt;/p&gt;</comment>
                            <comment id="16511388" author="githubbot" created="Wed, 13 Jun 2018 16:28:24 +0000"  >&lt;p&gt;Github user tragicjun commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @twalthr Sure, please go ahead and let me know if anything I can help further. &lt;/p&gt;</comment>
                            <comment id="16525103" author="githubbot" created="Wed, 27 Jun 2018 13:39:20 +0000"  >&lt;p&gt;GitHub user twalthr opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9444&quot; title=&quot;Add full SQL support for Avro formats&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9444&quot;&gt;&lt;del&gt;FLINK-9444&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;formats&amp;#93;&lt;/span&gt; Add full SQL support for Avro formats&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;What is the purpose of the change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;p&gt;    This PR adds full support of Apache Avro records for the Table API &amp;amp; SQL. It adds (de)serialization schemas to the row type for both specific and generic records. It converts all Avro types to Flink types and vice versa. It supports both physical and logical Avro types. Both an Avro class or a Avro schema string can be used for format initialization.&lt;/p&gt;

&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Brief change log&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Rework of SerializationSchema and DeserializationSchema for Avro&lt;/li&gt;
	&lt;li&gt;Update old tests for new Avro types introduced with Avro 1.8 and code clean up&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Verifying this change&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Reworked AvroRowDeSerializationTest&lt;/li&gt;
	&lt;li&gt;Added AvroSchemaConverterTest&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Does this pull request potentially affect one of the following parts:&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Dependencies (does it add or upgrade a dependency): yes&lt;/li&gt;
	&lt;li&gt;The public API, i.e., is any changed class annotated with `@Public(Evolving)`: no&lt;/li&gt;
	&lt;li&gt;The serializers: no&lt;/li&gt;
	&lt;li&gt;The runtime per-record code paths (performance sensitive): no&lt;/li&gt;
	&lt;li&gt;Anything that affects deployment or recovery: JobManager (and its components), Checkpointing, Yarn/Mesos, ZooKeeper: no&lt;/li&gt;
	&lt;li&gt;The S3 file system connector: no&lt;/li&gt;
&lt;/ul&gt;


&lt;ol&gt;
	&lt;li&gt;
	&lt;ol&gt;
		&lt;li&gt;Documentation&lt;/li&gt;
	&lt;/ol&gt;
	&lt;/li&gt;
&lt;/ol&gt;


&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;Does this pull request introduce a new feature? yes&lt;/li&gt;
	&lt;li&gt;If yes, how is the feature documented? docs&lt;/li&gt;
&lt;/ul&gt;



&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/twalthr/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/twalthr/flink&lt;/a&gt; &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9444&quot; title=&quot;Add full SQL support for Avro formats&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9444&quot;&gt;&lt;del&gt;FLINK-9444&lt;/del&gt;&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #6218&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit 3a4c5e6b6313648e532307d59082f1671b0695d5&lt;br/&gt;
Author: Timo Walther &amp;lt;twalthr@...&amp;gt;&lt;br/&gt;
Date:   2018-06-26T09:46:06Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9444&quot; title=&quot;Add full SQL support for Avro formats&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9444&quot;&gt;&lt;del&gt;FLINK-9444&lt;/del&gt;&lt;/a&gt; &lt;span class=&quot;error&quot;&gt;&amp;#91;formats&amp;#93;&lt;/span&gt; Add full SQL support for Avro formats&lt;/p&gt;

&lt;p&gt;    This PR adds full support of Apache Avro records for the Table API &amp;amp; SQL. It adds (de)serialization schemas to the row type for both specific and generic records. It converts all Avro types to Flink types and vice versa. It supports both physical and logical Avro types. Both an Avro class or a Avro schema string can be used for format initialization.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="16531044" author="githubbot" created="Tue, 3 Jul 2018 08:53:56 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199731327&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199731327&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-core/src/test/java/org/apache/flink/api/java/typeutils/RowTypeInfoTest.java &amp;#8212;&lt;br/&gt;
    @@ -123,4 +125,24 @@ public void testNestedRowTypeInfo() &lt;/p&gt;
{
     		assertEquals(&quot;Short&quot;, typeInfo.getTypeAt(&quot;f1.f0&quot;).toString());
     	}

&lt;p&gt;    +	@Test&lt;br/&gt;
    +	public void testSchemaEquals() {&lt;br/&gt;
    +		final RowTypeInfo row1 = new RowTypeInfo(&lt;br/&gt;
    +			new TypeInformation[]&lt;/p&gt;
{BasicTypeInfo.INT_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO},&lt;br/&gt;
    +			new String[] {&quot;field1&quot;, &quot;field2&quot;});&lt;br/&gt;
    +		final RowTypeInfo row2 = new RowTypeInfo(&lt;br/&gt;
    +			new TypeInformation[]{BasicTypeInfo.INT_TYPE_INFO, BasicTypeInfo.STRING_TYPE_INFO}
&lt;p&gt;,&lt;br/&gt;
    +			new String[] &lt;/p&gt;
{&quot;field1&quot;, &quot;field2&quot;}
&lt;p&gt;);&lt;br/&gt;
    +		assertTrue(row1.schemaEquals(row2));&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    This is covered by the test base. But I added another test data entry with different field names.&lt;/p&gt;</comment>
                            <comment id="16531052" author="githubbot" created="Tue, 3 Jul 2018 09:02:00 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199733710&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199733710&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -51,6 +51,17 @@ under the License.&lt;br/&gt;
     			&amp;lt;!-- managed version --&amp;gt;&lt;br/&gt;
     		&amp;lt;/dependency&amp;gt;&lt;/p&gt;

&lt;p&gt;    +		&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +			&amp;lt;groupId&amp;gt;joda-time&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +			&amp;lt;artifactId&amp;gt;joda-time&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +			&amp;lt;!-- managed version --&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Yes, we assume that the user provides a Joda-Time that matches the specific record. We only call 4 methods. I think changes are unlikely there. I went for the Flink-version the Avro version would be `2.9` but we would always have to keep this in sync.&lt;/p&gt;</comment>
                            <comment id="16531056" author="githubbot" created="Tue, 3 Jul 2018 09:04:08 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199734399&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199734399&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-connectors/flink-connector-kafka-base/src/test/java/org/apache/flink/streaming/connectors/kafka/KafkaAvroTableSourceTestBase.java &amp;#8212;&lt;br/&gt;
    @@ -44,7 +41,7 @@&lt;br/&gt;
     	@Override&lt;br/&gt;
     	protected void configureBuilder(KafkaTableSource.Builder builder) {&lt;br/&gt;
     		super.configureBuilder(builder);&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;((KafkaAvroTableSource.Builder) builder).forAvroRecordClass(SameFieldsAvroClass.class);&lt;br/&gt;
    +		((KafkaAvroTableSource.Builder) builder).forAvroRecordClass(SchemaRecord.class);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    No, but it simplifies the code base and uses only real-world generated records for testing.&lt;/p&gt;</comment>
                            <comment id="16531061" author="githubbot" created="Tue, 3 Jul 2018 09:09:52 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199736070&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199736070&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/BackwardsCompatibleAvroSerializerTest.java &amp;#8212;&lt;br/&gt;
    @@ -26,7 +26,7 @@&lt;br/&gt;
     import org.apache.flink.api.java.typeutils.runtime.PojoSerializer;&lt;br/&gt;
     import org.apache.flink.api.java.typeutils.runtime.PojoSerializer.PojoSerializerConfigSnapshot;&lt;br/&gt;
     import org.apache.flink.core.memory.DataInputViewStreamWrapper;&lt;br/&gt;
    -import org.apache.flink.formats.avro.generated.User;&lt;br/&gt;
    +import org.apache.flink.formats.avro.generated.SimpleUser;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    The problem is that &lt;tt&gt;BackwardsCompatibleAvroSerializer&lt;/tt&gt; does not support records with logical types. Logical types need a Kryo configuration that the serializer does not set correctly. This might be a bug or at least a missing feature. Given that this serializer only exists for backwards compatibility for 1.3 (which used Avro 1.7 without logical type), I added a simple user for this test. I will add a comment about this to the code.&lt;/p&gt;</comment>
                            <comment id="16531117" author="githubbot" created="Tue, 3 Jul 2018 09:43:32 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199746426&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199746426&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowSerializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -37,18 +43,42 @@&lt;br/&gt;
     import java.io.IOException;&lt;br/&gt;
     import java.io.ObjectInputStream;&lt;br/&gt;
     import java.io.ObjectOutputStream;&lt;br/&gt;
    +import java.math.BigDecimal;&lt;br/&gt;
    +import java.nio.ByteBuffer;&lt;br/&gt;
    +import java.sql.Date;&lt;br/&gt;
    +import java.sql.Time;&lt;br/&gt;
    +import java.sql.Timestamp;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
     import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.TimeZone;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Serialization schema that serializes 
{@link Row} over {@link SpecificRecord} into a Avro bytes.&lt;br/&gt;
    + * Serialization schema that serializes {@link Row}
&lt;p&gt; into Avro bytes.&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Serializes objects that are represented in (nested) Flink rows. It support types that&lt;br/&gt;
    + * are compatible with Flink&apos;s Table &amp;amp; SQL API.&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Note: Changes in this class need to be kept in sync with the corresponding runtime&lt;br/&gt;
    + * class &lt;/p&gt;
{@link AvroRowDeserializationSchema}
&lt;p&gt; and schema converter &lt;/p&gt;
{@link AvroSchemaConverter}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
     public class AvroRowSerializationSchema implements SerializationSchema&amp;lt;Row&amp;gt; {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Avro record class.&lt;br/&gt;
    +	 * Used for time conversions into SQL types.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private static final TimeZone LOCAL_TZ = TimeZone.getDefault();
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    We are using this pattern at different places. E.g. `org.apache.flink.orc.OrcBatchReader`. The problem is that Java&apos;s SQL time/date/timestamp are a complete design fail. They are timezone specific. This adds/removes the local timezone from the timestamp. Such that the string representation of the produced `Timestamp` object is always correct.&lt;/p&gt;</comment>
                            <comment id="16531120" author="githubbot" created="Tue, 3 Jul 2018 09:44:07 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199746600&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199746600&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -17,154 +17,338 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.formats.avro;&lt;/p&gt;

&lt;p&gt;    +import org.apache.flink.annotation.PublicEvolving;&lt;br/&gt;
     import org.apache.flink.api.common.serialization.AbstractDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;&lt;br/&gt;
     import org.apache.flink.api.common.typeinfo.TypeInformation;&lt;br/&gt;
    -import org.apache.flink.formats.avro.typeutils.AvroRecordClassConverter;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.Types;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.MapTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.RowTypeInfo;&lt;br/&gt;
    +import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;&lt;br/&gt;
     import org.apache.flink.formats.avro.utils.MutableByteArrayInputStream;&lt;br/&gt;
     import org.apache.flink.types.Row;&lt;br/&gt;
     import org.apache.flink.util.Preconditions;&lt;/p&gt;

&lt;p&gt;    +import org.apache.avro.LogicalTypes;&lt;br/&gt;
     import org.apache.avro.Schema;&lt;br/&gt;
    -import org.apache.avro.generic.GenericRecord;&lt;br/&gt;
    +import org.apache.avro.generic.GenericData;&lt;br/&gt;
    +import org.apache.avro.generic.GenericDatumReader;&lt;br/&gt;
    +import org.apache.avro.generic.GenericFixed;&lt;br/&gt;
    +import org.apache.avro.generic.IndexedRecord;&lt;br/&gt;
     import org.apache.avro.io.DatumReader;&lt;br/&gt;
     import org.apache.avro.io.Decoder;&lt;br/&gt;
     import org.apache.avro.io.DecoderFactory;&lt;br/&gt;
     import org.apache.avro.specific.SpecificData;&lt;br/&gt;
     import org.apache.avro.specific.SpecificDatumReader;&lt;br/&gt;
     import org.apache.avro.specific.SpecificRecord;&lt;br/&gt;
    -import org.apache.avro.specific.SpecificRecordBase;&lt;br/&gt;
    -import org.apache.avro.util.Utf8;&lt;br/&gt;
    +import org.joda.time.DateTime;&lt;br/&gt;
    +import org.joda.time.DateTimeFieldType;&lt;br/&gt;
    +import org.joda.time.LocalDate;&lt;br/&gt;
    +import org.joda.time.LocalTime;&lt;/p&gt;

&lt;p&gt;     import java.io.IOException;&lt;br/&gt;
     import java.io.ObjectInputStream;&lt;br/&gt;
     import java.io.ObjectOutputStream;&lt;br/&gt;
    +import java.lang.reflect.Array;&lt;br/&gt;
    +import java.math.BigDecimal;&lt;br/&gt;
    +import java.math.BigInteger;&lt;br/&gt;
    +import java.nio.ByteBuffer;&lt;br/&gt;
    +import java.sql.Date;&lt;br/&gt;
    +import java.sql.Time;&lt;br/&gt;
    +import java.sql.Timestamp;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
     import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.TimeZone;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Deserialization schema from Avro bytes over 
{@link SpecificRecord}
&lt;p&gt; to &lt;/p&gt;
{@link Row}.&lt;br/&gt;
    + * Deserialization schema from Avro bytes to {@link Row}
&lt;p&gt;.&lt;br/&gt;
      *&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink Rows.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink rows. It converts Avro types&lt;br/&gt;
    + * into types that are compatible with Flink&apos;s Table &amp;amp; SQL API.&lt;br/&gt;
      *&lt;/li&gt;
	&lt;li&gt;* 
{@link Utf8}
&lt;p&gt; is converted to regular Java Strings.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Projects with Avro records containing logical date/time types need to add a JodaTime&lt;br/&gt;
    + * dependency.&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Note: Changes in this class need to be kept in sync with the corresponding runtime&lt;br/&gt;
    + * class &lt;/p&gt;
{@link AvroRowSerializationSchema}
&lt;p&gt; and schema converter &lt;/p&gt;
{@link AvroSchemaConverter}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
    +@PublicEvolving&lt;br/&gt;
     public class AvroRowDeserializationSchema extends AbstractDeserializationSchema&amp;lt;Row&amp;gt; {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Avro record class.&lt;br/&gt;
    +	 * Used for time conversions into SQL types.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private static final TimeZone LOCAL_TZ = TimeZone.getDefault();
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    See comment above.&lt;/p&gt;</comment>
                            <comment id="16531123" author="githubbot" created="Tue, 3 Jul 2018 09:46:15 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199747223&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199747223&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -17,154 +17,338 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.formats.avro;&lt;/p&gt;

&lt;p&gt;    +import org.apache.flink.annotation.PublicEvolving;&lt;br/&gt;
     import org.apache.flink.api.common.serialization.AbstractDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;&lt;br/&gt;
     import org.apache.flink.api.common.typeinfo.TypeInformation;&lt;br/&gt;
    -import org.apache.flink.formats.avro.typeutils.AvroRecordClassConverter;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.Types;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.MapTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.RowTypeInfo;&lt;br/&gt;
    +import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;&lt;br/&gt;
     import org.apache.flink.formats.avro.utils.MutableByteArrayInputStream;&lt;br/&gt;
     import org.apache.flink.types.Row;&lt;br/&gt;
     import org.apache.flink.util.Preconditions;&lt;/p&gt;

&lt;p&gt;    +import org.apache.avro.LogicalTypes;&lt;br/&gt;
     import org.apache.avro.Schema;&lt;br/&gt;
    -import org.apache.avro.generic.GenericRecord;&lt;br/&gt;
    +import org.apache.avro.generic.GenericData;&lt;br/&gt;
    +import org.apache.avro.generic.GenericDatumReader;&lt;br/&gt;
    +import org.apache.avro.generic.GenericFixed;&lt;br/&gt;
    +import org.apache.avro.generic.IndexedRecord;&lt;br/&gt;
     import org.apache.avro.io.DatumReader;&lt;br/&gt;
     import org.apache.avro.io.Decoder;&lt;br/&gt;
     import org.apache.avro.io.DecoderFactory;&lt;br/&gt;
     import org.apache.avro.specific.SpecificData;&lt;br/&gt;
     import org.apache.avro.specific.SpecificDatumReader;&lt;br/&gt;
     import org.apache.avro.specific.SpecificRecord;&lt;br/&gt;
    -import org.apache.avro.specific.SpecificRecordBase;&lt;br/&gt;
    -import org.apache.avro.util.Utf8;&lt;br/&gt;
    +import org.joda.time.DateTime;&lt;br/&gt;
    +import org.joda.time.DateTimeFieldType;&lt;br/&gt;
    +import org.joda.time.LocalDate;&lt;br/&gt;
    +import org.joda.time.LocalTime;&lt;/p&gt;

&lt;p&gt;     import java.io.IOException;&lt;br/&gt;
     import java.io.ObjectInputStream;&lt;br/&gt;
     import java.io.ObjectOutputStream;&lt;br/&gt;
    +import java.lang.reflect.Array;&lt;br/&gt;
    +import java.math.BigDecimal;&lt;br/&gt;
    +import java.math.BigInteger;&lt;br/&gt;
    +import java.nio.ByteBuffer;&lt;br/&gt;
    +import java.sql.Date;&lt;br/&gt;
    +import java.sql.Time;&lt;br/&gt;
    +import java.sql.Timestamp;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
     import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.TimeZone;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Deserialization schema from Avro bytes over 
{@link SpecificRecord}
&lt;p&gt; to &lt;/p&gt;
{@link Row}.&lt;br/&gt;
    + * Deserialization schema from Avro bytes to {@link Row}
&lt;p&gt;.&lt;br/&gt;
      *&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink Rows.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink rows. It converts Avro types&lt;br/&gt;
    + * into types that are compatible with Flink&apos;s Table &amp;amp; SQL API.&lt;br/&gt;
      *&lt;/li&gt;
	&lt;li&gt;* 
{@link Utf8}
&lt;p&gt; is converted to regular Java Strings.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Projects with Avro records containing logical date/time types need to add a JodaTime&lt;br/&gt;
    + * dependency.&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Note: Changes in this class need to be kept in sync with the corresponding runtime&lt;br/&gt;
    + * class &lt;/p&gt;
{@link AvroRowSerializationSchema}
&lt;p&gt; and schema converter &lt;/p&gt;
{@link AvroSchemaConverter}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
    +@PublicEvolving&lt;br/&gt;
     public class AvroRowDeserializationSchema extends AbstractDeserializationSchema&amp;lt;Row&amp;gt; {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Avro record class.&lt;br/&gt;
    +	 * Used for time conversions into SQL types.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private static final TimeZone LOCAL_TZ = TimeZone.getDefault();&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro record class for deserialization. Might be null if record class is not available.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Schema for deterministic field order.&lt;br/&gt;
    +	 * Schema string for deserialization.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private String schemaString;&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro serialization schema.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private transient Schema schema;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Reader that deserializes byte array into a record.&lt;br/&gt;
    +	 * Type information describing the result type.
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Sorry about that. I actually rewrote the entire class. It might make sense to review it entirely instead of the diff.&lt;/p&gt;</comment>
                            <comment id="16531128" author="githubbot" created="Tue, 3 Jul 2018 09:49:45 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199748214&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199748214&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -17,154 +17,338 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.formats.avro;&lt;/p&gt;

&lt;p&gt;    +import org.apache.flink.annotation.PublicEvolving;&lt;br/&gt;
     import org.apache.flink.api.common.serialization.AbstractDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;&lt;br/&gt;
     import org.apache.flink.api.common.typeinfo.TypeInformation;&lt;br/&gt;
    -import org.apache.flink.formats.avro.typeutils.AvroRecordClassConverter;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.Types;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.MapTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.RowTypeInfo;&lt;br/&gt;
    +import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;&lt;br/&gt;
     import org.apache.flink.formats.avro.utils.MutableByteArrayInputStream;&lt;br/&gt;
     import org.apache.flink.types.Row;&lt;br/&gt;
     import org.apache.flink.util.Preconditions;&lt;/p&gt;

&lt;p&gt;    +import org.apache.avro.LogicalTypes;&lt;br/&gt;
     import org.apache.avro.Schema;&lt;br/&gt;
    -import org.apache.avro.generic.GenericRecord;&lt;br/&gt;
    +import org.apache.avro.generic.GenericData;&lt;br/&gt;
    +import org.apache.avro.generic.GenericDatumReader;&lt;br/&gt;
    +import org.apache.avro.generic.GenericFixed;&lt;br/&gt;
    +import org.apache.avro.generic.IndexedRecord;&lt;br/&gt;
     import org.apache.avro.io.DatumReader;&lt;br/&gt;
     import org.apache.avro.io.Decoder;&lt;br/&gt;
     import org.apache.avro.io.DecoderFactory;&lt;br/&gt;
     import org.apache.avro.specific.SpecificData;&lt;br/&gt;
     import org.apache.avro.specific.SpecificDatumReader;&lt;br/&gt;
     import org.apache.avro.specific.SpecificRecord;&lt;br/&gt;
    -import org.apache.avro.specific.SpecificRecordBase;&lt;br/&gt;
    -import org.apache.avro.util.Utf8;&lt;br/&gt;
    +import org.joda.time.DateTime;&lt;br/&gt;
    +import org.joda.time.DateTimeFieldType;&lt;br/&gt;
    +import org.joda.time.LocalDate;&lt;br/&gt;
    +import org.joda.time.LocalTime;&lt;/p&gt;

&lt;p&gt;     import java.io.IOException;&lt;br/&gt;
     import java.io.ObjectInputStream;&lt;br/&gt;
     import java.io.ObjectOutputStream;&lt;br/&gt;
    +import java.lang.reflect.Array;&lt;br/&gt;
    +import java.math.BigDecimal;&lt;br/&gt;
    +import java.math.BigInteger;&lt;br/&gt;
    +import java.nio.ByteBuffer;&lt;br/&gt;
    +import java.sql.Date;&lt;br/&gt;
    +import java.sql.Time;&lt;br/&gt;
    +import java.sql.Timestamp;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
     import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.TimeZone;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Deserialization schema from Avro bytes over 
{@link SpecificRecord}
&lt;p&gt; to &lt;/p&gt;
{@link Row}.&lt;br/&gt;
    + * Deserialization schema from Avro bytes to {@link Row}
&lt;p&gt;.&lt;br/&gt;
      *&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink Rows.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink rows. It converts Avro types&lt;br/&gt;
    + * into types that are compatible with Flink&apos;s Table &amp;amp; SQL API.&lt;br/&gt;
      *&lt;/li&gt;
	&lt;li&gt;* 
{@link Utf8}
&lt;p&gt; is converted to regular Java Strings.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Projects with Avro records containing logical date/time types need to add a JodaTime&lt;br/&gt;
    + * dependency.&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Note: Changes in this class need to be kept in sync with the corresponding runtime&lt;br/&gt;
    + * class &lt;/p&gt;
{@link AvroRowSerializationSchema}
&lt;p&gt; and schema converter &lt;/p&gt;
{@link AvroSchemaConverter}
&lt;p&gt;.&lt;br/&gt;
      */&lt;br/&gt;
    +@PublicEvolving&lt;br/&gt;
     public class AvroRowDeserializationSchema extends AbstractDeserializationSchema&amp;lt;Row&amp;gt; {&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Avro record class.&lt;br/&gt;
    +	 * Used for time conversions into SQL types.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private static final TimeZone LOCAL_TZ = TimeZone.getDefault();&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro record class for deserialization. Might be null if record class is not available.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Schema for deterministic field order.&lt;br/&gt;
    +	 * Schema string for deserialization.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private String schemaString;&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro serialization schema.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private transient Schema schema;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Reader that deserializes byte array into a record.&lt;br/&gt;
    +	 * Type information describing the result type.&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;private transient DatumReader&amp;lt;SpecificRecord&amp;gt; datumReader;&lt;br/&gt;
    +	private transient TypeInformation&amp;lt;Row&amp;gt; typeInfo;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Input stream to read message from.&lt;br/&gt;
    +	 * Record to deserialize byte array.&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;private transient MutableByteArrayInputStream inputStream;&lt;br/&gt;
    +	private transient IndexedRecord record;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Avro decoder that decodes binary data.&lt;br/&gt;
    +	 * Reader that deserializes byte array into a record.&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;private transient Decoder decoder;&lt;br/&gt;
    +	private transient DatumReader&amp;lt;IndexedRecord&amp;gt; datumReader;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Record to deserialize byte array to.&lt;br/&gt;
    +	 * Input stream to read message from.&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;private SpecificRecord record;&lt;br/&gt;
    +	private transient MutableByteArrayInputStream inputStream;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Type information describing the result type.&lt;br/&gt;
    +	 * Avro decoder that decodes binary data.&lt;br/&gt;
     	 */&lt;/li&gt;
	&lt;li&gt;private transient TypeInformation&amp;lt;Row&amp;gt; typeInfo;&lt;br/&gt;
    +	private transient Decoder decoder;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	/**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Creates a Avro deserialization schema for the given record.&lt;br/&gt;
    +	 * Creates a Avro deserialization schema for the given specific record class. Having the&lt;br/&gt;
    +	 * concrete Avro record class might improve performance.&lt;br/&gt;
     	 *&lt;/li&gt;
&lt;/ul&gt;
&lt;ul&gt;
	&lt;li&gt;@param recordClazz Avro record class used to deserialize Avro&apos;s record to Flink&apos;s row&lt;br/&gt;
     	 */&lt;/li&gt;
&lt;/ul&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecordBase&amp;gt; recordClazz) {&lt;br/&gt;
    +	public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz) 
{
     		Preconditions.checkNotNull(recordClazz, &quot;Avro record class must not be null.&quot;);
     		this.recordClazz = recordClazz;
    -		this.schema = SpecificData.get().getSchema(recordClazz);
    -		this.datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    -		this.record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    -		this.inputStream = new MutableByteArrayInputStream();
    -		this.decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    -		this.typeInfo = AvroRecordClassConverter.convert(recordClazz);
    +		schema = SpecificData.get().getSchema(recordClazz);
    +		typeInfo = AvroSchemaConverter.convert(recordClazz);
    +		schemaString = schema.toString();
    +		record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    +		datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    +		inputStream = new MutableByteArrayInputStream();
    +		decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a Avro deserialization schema for the given Avro schema string.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param avroSchemaString Avro schema string to deserialize Avro&apos;s record to Flink&apos;s row&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public AvroRowDeserializationSchema(String avroSchemaString) &lt;/p&gt;
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +		Preconditions.checkNotNull(avroSchemaString, &amp;quot;Avro schema must not be null.&amp;quot;);    +		recordClazz = null;    +		typeInfo = AvroSchemaConverter.convert(avroSchemaString);    +		schemaString = avroSchemaString;    +		schema = new Schema.Parser().parse(avroSchemaString);    +		record = new GenericData.Record(schema);    +		datumReader = new GenericDatumReader&amp;lt;&amp;gt;(schema);    +		inputStream = new MutableByteArrayInputStream();    +		decoder = DecoderFactory.get().binaryDecoder(inputStream, null);    +		// check for a schema that describes a record    +		if (!(typeInfo instanceof RowTypeInfo)) {
    +			throw new IllegalArgumentException(&quot;Row type information expected.&quot;);
    +		}     	}&lt;/span&gt; &lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;     	@Override&lt;br/&gt;
     	public Row deserialize(byte[] message) throws IOException {&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;// read record&lt;br/&gt;
     		try 
{
     			inputStream.setBuffer(message);
    -			this.record = datumReader.read(record, decoder);
    -		}
&lt;p&gt; catch (IOException e) {&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;throw new RuntimeException(&quot;Failed to deserialize Row.&quot;, e);&lt;br/&gt;
    +			final IndexedRecord read = datumReader.read(record, decoder);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Good point. I had a specific record before and changed it to `IndexedRecord` later. Will correct that.&lt;/p&gt;</comment>
                            <comment id="16531176" author="githubbot" created="Tue, 3 Jul 2018 10:28:07 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199758405&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199758405&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -17,154 +17,338 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.formats.avro;&lt;/p&gt;

&lt;p&gt;    +import org.apache.flink.annotation.PublicEvolving;&lt;br/&gt;
     import org.apache.flink.api.common.serialization.AbstractDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;&lt;br/&gt;
     import org.apache.flink.api.common.typeinfo.TypeInformation;&lt;br/&gt;
    -import org.apache.flink.formats.avro.typeutils.AvroRecordClassConverter;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.Types;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.MapTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.RowTypeInfo;&lt;br/&gt;
    +import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;&lt;br/&gt;
     import org.apache.flink.formats.avro.utils.MutableByteArrayInputStream;&lt;br/&gt;
     import org.apache.flink.types.Row;&lt;br/&gt;
     import org.apache.flink.util.Preconditions;&lt;/p&gt;

&lt;p&gt;    +import org.apache.avro.LogicalTypes;&lt;br/&gt;
     import org.apache.avro.Schema;&lt;br/&gt;
    -import org.apache.avro.generic.GenericRecord;&lt;br/&gt;
    +import org.apache.avro.generic.GenericData;&lt;br/&gt;
    +import org.apache.avro.generic.GenericDatumReader;&lt;br/&gt;
    +import org.apache.avro.generic.GenericFixed;&lt;br/&gt;
    +import org.apache.avro.generic.IndexedRecord;&lt;br/&gt;
     import org.apache.avro.io.DatumReader;&lt;br/&gt;
     import org.apache.avro.io.Decoder;&lt;br/&gt;
     import org.apache.avro.io.DecoderFactory;&lt;br/&gt;
     import org.apache.avro.specific.SpecificData;&lt;br/&gt;
     import org.apache.avro.specific.SpecificDatumReader;&lt;br/&gt;
     import org.apache.avro.specific.SpecificRecord;&lt;br/&gt;
    -import org.apache.avro.specific.SpecificRecordBase;&lt;br/&gt;
    -import org.apache.avro.util.Utf8;&lt;br/&gt;
    +import org.joda.time.DateTime;&lt;br/&gt;
    +import org.joda.time.DateTimeFieldType;&lt;br/&gt;
    +import org.joda.time.LocalDate;&lt;br/&gt;
    +import org.joda.time.LocalTime;&lt;/p&gt;

&lt;p&gt;     import java.io.IOException;&lt;br/&gt;
     import java.io.ObjectInputStream;&lt;br/&gt;
     import java.io.ObjectOutputStream;&lt;br/&gt;
    +import java.lang.reflect.Array;&lt;br/&gt;
    +import java.math.BigDecimal;&lt;br/&gt;
    +import java.math.BigInteger;&lt;br/&gt;
    +import java.nio.ByteBuffer;&lt;br/&gt;
    +import java.sql.Date;&lt;br/&gt;
    +import java.sql.Time;&lt;br/&gt;
    +import java.sql.Timestamp;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
     import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.TimeZone;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Deserialization schema from Avro bytes over 
{@link SpecificRecord} to {@link Row}.&lt;br/&gt;
    + * Deserialization schema from Avro bytes to {@link Row}.&lt;br/&gt;
      *&lt;br/&gt;
    - * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink Rows.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink rows. It converts Avro types&lt;br/&gt;
    + * into types that are compatible with Flink&apos;s Table &amp;amp; SQL API.&lt;br/&gt;
      *&lt;br/&gt;
    - * {@link Utf8} is converted to regular Java Strings.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Projects with Avro records containing logical date/time types need to add a JodaTime&lt;br/&gt;
    + * dependency.&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Note: Changes in this class need to be kept in sync with the corresponding runtime&lt;br/&gt;
    + * class {@link AvroRowSerializationSchema} and schema converter {@link AvroSchemaConverter}.&lt;br/&gt;
      */&lt;br/&gt;
    +@PublicEvolving&lt;br/&gt;
     public class AvroRowDeserializationSchema extends AbstractDeserializationSchema&amp;lt;Row&amp;gt; {&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Avro record class.&lt;br/&gt;
    +	 * Used for time conversions into SQL types.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private static final TimeZone LOCAL_TZ = TimeZone.getDefault();&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro record class for deserialization. Might be null if record class is not available.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Schema for deterministic field order.&lt;br/&gt;
    +	 * Schema string for deserialization.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private String schemaString;&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro serialization schema.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private transient Schema schema;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Reader that deserializes byte array into a record.&lt;br/&gt;
    +	 * Type information describing the result type.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient DatumReader&amp;lt;SpecificRecord&amp;gt; datumReader;&lt;br/&gt;
    +	private transient TypeInformation&amp;lt;Row&amp;gt; typeInfo;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Input stream to read message from.&lt;br/&gt;
    +	 * Record to deserialize byte array.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient MutableByteArrayInputStream inputStream;&lt;br/&gt;
    +	private transient IndexedRecord record;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Avro decoder that decodes binary data.&lt;br/&gt;
    +	 * Reader that deserializes byte array into a record.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient Decoder decoder;&lt;br/&gt;
    +	private transient DatumReader&amp;lt;IndexedRecord&amp;gt; datumReader;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Record to deserialize byte array to.&lt;br/&gt;
    +	 * Input stream to read message from.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private SpecificRecord record;&lt;br/&gt;
    +	private transient MutableByteArrayInputStream inputStream;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Type information describing the result type.&lt;br/&gt;
    +	 * Avro decoder that decodes binary data.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient TypeInformation&amp;lt;Row&amp;gt; typeInfo;&lt;br/&gt;
    +	private transient Decoder decoder;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Creates a Avro deserialization schema for the given record.&lt;br/&gt;
    +	 * Creates a Avro deserialization schema for the given specific record class. Having the&lt;br/&gt;
    +	 * concrete Avro record class might improve performance.&lt;br/&gt;
     	 *&lt;br/&gt;
     	 * @param recordClazz Avro record class used to deserialize Avro&apos;s record to Flink&apos;s row&lt;br/&gt;
     	 */&lt;br/&gt;
    -	public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecordBase&amp;gt; recordClazz) {&lt;br/&gt;
    +	public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz) {
     		Preconditions.checkNotNull(recordClazz, &quot;Avro record class must not be null.&quot;);
     		this.recordClazz = recordClazz;
    -		this.schema = SpecificData.get().getSchema(recordClazz);
    -		this.datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    -		this.record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    -		this.inputStream = new MutableByteArrayInputStream();
    -		this.decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    -		this.typeInfo = AvroRecordClassConverter.convert(recordClazz);
    +		schema = SpecificData.get().getSchema(recordClazz);
    +		typeInfo = AvroSchemaConverter.convert(recordClazz);
    +		schemaString = schema.toString();
    +		record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    +		datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    +		inputStream = new MutableByteArrayInputStream();
    +		decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a Avro deserialization schema for the given Avro schema string.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param avroSchemaString Avro schema string to deserialize Avro&apos;s record to Flink&apos;s row&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public AvroRowDeserializationSchema(String avroSchemaString) {&lt;br/&gt;
    +		Preconditions.checkNotNull(avroSchemaString, &quot;Avro schema must not be null.&quot;);&lt;br/&gt;
    +		recordClazz = null;&lt;br/&gt;
    +		typeInfo = AvroSchemaConverter.convert(avroSchemaString);&lt;br/&gt;
    +		schemaString = avroSchemaString;&lt;br/&gt;
    +		schema = new Schema.Parser().parse(avroSchemaString);&lt;br/&gt;
    +		record = new GenericData.Record(schema);&lt;br/&gt;
    +		datumReader = new GenericDatumReader&amp;lt;&amp;gt;(schema);&lt;br/&gt;
    +		inputStream = new MutableByteArrayInputStream();&lt;br/&gt;
    +		decoder = DecoderFactory.get().binaryDecoder(inputStream, null);&lt;br/&gt;
    +		// check for a schema that describes a record&lt;br/&gt;
    +		if (!(typeInfo instanceof RowTypeInfo)) {
    +			throw new IllegalArgumentException(&quot;Row type information expected.&quot;);
    +		}&lt;br/&gt;
     	}&lt;br/&gt;
     &lt;br/&gt;
     	@Override&lt;br/&gt;
     	public Row deserialize(byte[] message) throws IOException {&lt;br/&gt;
    -		// read record&lt;br/&gt;
     		try {
     			inputStream.setBuffer(message);
    -			this.record = datumReader.read(record, decoder);
    -		} catch (IOException e) {
    -			throw new RuntimeException(&quot;Failed to deserialize Row.&quot;, e);
    +			final IndexedRecord read = datumReader.read(record, decoder);
    +			return convertRecord(schema, (RowTypeInfo) typeInfo, read);
    +		} catch (Exception e) {
    +			throw new IOException(&quot;Failed to deserialize Avro record.&quot;, e);
     		}&lt;br/&gt;
    -&lt;br/&gt;
    -		// convert to row&lt;br/&gt;
    -		final Object row = convertToRow(schema, record);&lt;br/&gt;
    -		return (Row) row;&lt;br/&gt;
    -	}&lt;br/&gt;
    -&lt;br/&gt;
    -	private void writeObject(ObjectOutputStream oos) throws IOException {
    -		oos.writeObject(recordClazz);
    -	}&lt;br/&gt;
    -&lt;br/&gt;
    -	@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    -	private void readObject(ObjectInputStream ois) throws ClassNotFoundException, IOException {
    -		this.recordClazz = (Class&amp;lt;? extends SpecificRecord&amp;gt;) ois.readObject();
    -		this.schema = SpecificData.get().getSchema(recordClazz);
    -		this.datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    -		this.record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    -		this.inputStream = new MutableByteArrayInputStream();
    -		this.decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
     	}&lt;br/&gt;
     &lt;br/&gt;
     	@Override&lt;br/&gt;
     	public TypeInformation&amp;lt;Row&amp;gt; getProducedType() {
     		return typeInfo;
     	}&lt;br/&gt;
     &lt;br/&gt;
    -	/**&lt;br/&gt;
    -	 * Converts a (nested) Avro {@link SpecificRecord}
&lt;p&gt; into Flink&apos;s Row type.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* Avro&apos;s 
{@link Utf8}
&lt;p&gt; fields are converted into regular Java strings.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private static Object convertToRow(Schema schema, Object recordObj) {&lt;/li&gt;
	&lt;li&gt;if (recordObj instanceof GenericRecord) {&lt;/li&gt;
	&lt;li&gt;// records can be wrapped in a union&lt;/li&gt;
	&lt;li&gt;if (schema.getType() == Schema.Type.UNION) {&lt;br/&gt;
    +	// --------------------------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	private Row convertRecord(Schema schema, RowTypeInfo typeInfo, IndexedRecord record) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +		final List&amp;lt;Schema.Field&amp;gt; fields = schema.getFields();    +		final TypeInformation&amp;lt;?&amp;gt;[] fieldInfo = typeInfo.getFieldTypes();    +		final int length = fields.size();    +		final Row row = new Row(length);    +		for (int i = 0; i &amp;lt; length; i++) {
    +			final Schema.Field field = fields.get(i);
    +			row.setField(i, convert(field.schema(), fieldInfo[i], record.get(i)));
    +		}    +		return row;    +	}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;    +&lt;br/&gt;
    +	private Object convert(Schema schema, TypeInformation&amp;lt;?&amp;gt; info, Object object) {&lt;br/&gt;
    +		// we perform the conversion based on schema information but enriched with pre-computed&lt;br/&gt;
    +		// type information where useful (i.e., for arrays)&lt;br/&gt;
    +&lt;br/&gt;
    +		if (object == null) &lt;/p&gt;
{
    +			return null;
    +		}
&lt;p&gt;    +		switch (schema.getType()) {&lt;br/&gt;
    +			case RECORD:&lt;br/&gt;
    +				if (object instanceof IndexedRecord) {&lt;br/&gt;
    +					return convertRecord(schema, (RowTypeInfo) info, (IndexedRecord) object);&lt;/p&gt;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    The cast is not unchecked. The type information is derived from the schema. We can assume it is correct.&lt;/p&gt;</comment>
                            <comment id="16531177" author="githubbot" created="Tue, 3 Jul 2018 10:31:01 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199759148&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199759148&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -17,154 +17,338 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.formats.avro;&lt;/p&gt;

&lt;p&gt;    +import org.apache.flink.annotation.PublicEvolving;&lt;br/&gt;
     import org.apache.flink.api.common.serialization.AbstractDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;&lt;br/&gt;
     import org.apache.flink.api.common.typeinfo.TypeInformation;&lt;br/&gt;
    -import org.apache.flink.formats.avro.typeutils.AvroRecordClassConverter;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.Types;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.MapTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.RowTypeInfo;&lt;br/&gt;
    +import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;&lt;br/&gt;
     import org.apache.flink.formats.avro.utils.MutableByteArrayInputStream;&lt;br/&gt;
     import org.apache.flink.types.Row;&lt;br/&gt;
     import org.apache.flink.util.Preconditions;&lt;/p&gt;

&lt;p&gt;    +import org.apache.avro.LogicalTypes;&lt;br/&gt;
     import org.apache.avro.Schema;&lt;br/&gt;
    -import org.apache.avro.generic.GenericRecord;&lt;br/&gt;
    +import org.apache.avro.generic.GenericData;&lt;br/&gt;
    +import org.apache.avro.generic.GenericDatumReader;&lt;br/&gt;
    +import org.apache.avro.generic.GenericFixed;&lt;br/&gt;
    +import org.apache.avro.generic.IndexedRecord;&lt;br/&gt;
     import org.apache.avro.io.DatumReader;&lt;br/&gt;
     import org.apache.avro.io.Decoder;&lt;br/&gt;
     import org.apache.avro.io.DecoderFactory;&lt;br/&gt;
     import org.apache.avro.specific.SpecificData;&lt;br/&gt;
     import org.apache.avro.specific.SpecificDatumReader;&lt;br/&gt;
     import org.apache.avro.specific.SpecificRecord;&lt;br/&gt;
    -import org.apache.avro.specific.SpecificRecordBase;&lt;br/&gt;
    -import org.apache.avro.util.Utf8;&lt;br/&gt;
    +import org.joda.time.DateTime;&lt;br/&gt;
    +import org.joda.time.DateTimeFieldType;&lt;br/&gt;
    +import org.joda.time.LocalDate;&lt;br/&gt;
    +import org.joda.time.LocalTime;&lt;/p&gt;

&lt;p&gt;     import java.io.IOException;&lt;br/&gt;
     import java.io.ObjectInputStream;&lt;br/&gt;
     import java.io.ObjectOutputStream;&lt;br/&gt;
    +import java.lang.reflect.Array;&lt;br/&gt;
    +import java.math.BigDecimal;&lt;br/&gt;
    +import java.math.BigInteger;&lt;br/&gt;
    +import java.nio.ByteBuffer;&lt;br/&gt;
    +import java.sql.Date;&lt;br/&gt;
    +import java.sql.Time;&lt;br/&gt;
    +import java.sql.Timestamp;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
     import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.TimeZone;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Deserialization schema from Avro bytes over 
{@link SpecificRecord} to {@link Row}.&lt;br/&gt;
    + * Deserialization schema from Avro bytes to {@link Row}.&lt;br/&gt;
      *&lt;br/&gt;
    - * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink Rows.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink rows. It converts Avro types&lt;br/&gt;
    + * into types that are compatible with Flink&apos;s Table &amp;amp; SQL API.&lt;br/&gt;
      *&lt;br/&gt;
    - * {@link Utf8} is converted to regular Java Strings.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Projects with Avro records containing logical date/time types need to add a JodaTime&lt;br/&gt;
    + * dependency.&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Note: Changes in this class need to be kept in sync with the corresponding runtime&lt;br/&gt;
    + * class {@link AvroRowSerializationSchema} and schema converter {@link AvroSchemaConverter}.&lt;br/&gt;
      */&lt;br/&gt;
    +@PublicEvolving&lt;br/&gt;
     public class AvroRowDeserializationSchema extends AbstractDeserializationSchema&amp;lt;Row&amp;gt; {&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Avro record class.&lt;br/&gt;
    +	 * Used for time conversions into SQL types.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private static final TimeZone LOCAL_TZ = TimeZone.getDefault();&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro record class for deserialization. Might be null if record class is not available.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Schema for deterministic field order.&lt;br/&gt;
    +	 * Schema string for deserialization.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private String schemaString;&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro serialization schema.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private transient Schema schema;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Reader that deserializes byte array into a record.&lt;br/&gt;
    +	 * Type information describing the result type.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient DatumReader&amp;lt;SpecificRecord&amp;gt; datumReader;&lt;br/&gt;
    +	private transient TypeInformation&amp;lt;Row&amp;gt; typeInfo;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Input stream to read message from.&lt;br/&gt;
    +	 * Record to deserialize byte array.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient MutableByteArrayInputStream inputStream;&lt;br/&gt;
    +	private transient IndexedRecord record;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Avro decoder that decodes binary data.&lt;br/&gt;
    +	 * Reader that deserializes byte array into a record.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient Decoder decoder;&lt;br/&gt;
    +	private transient DatumReader&amp;lt;IndexedRecord&amp;gt; datumReader;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Record to deserialize byte array to.&lt;br/&gt;
    +	 * Input stream to read message from.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private SpecificRecord record;&lt;br/&gt;
    +	private transient MutableByteArrayInputStream inputStream;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Type information describing the result type.&lt;br/&gt;
    +	 * Avro decoder that decodes binary data.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient TypeInformation&amp;lt;Row&amp;gt; typeInfo;&lt;br/&gt;
    +	private transient Decoder decoder;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Creates a Avro deserialization schema for the given record.&lt;br/&gt;
    +	 * Creates a Avro deserialization schema for the given specific record class. Having the&lt;br/&gt;
    +	 * concrete Avro record class might improve performance.&lt;br/&gt;
     	 *&lt;br/&gt;
     	 * @param recordClazz Avro record class used to deserialize Avro&apos;s record to Flink&apos;s row&lt;br/&gt;
     	 */&lt;br/&gt;
    -	public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecordBase&amp;gt; recordClazz) {&lt;br/&gt;
    +	public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz) {
     		Preconditions.checkNotNull(recordClazz, &quot;Avro record class must not be null.&quot;);
     		this.recordClazz = recordClazz;
    -		this.schema = SpecificData.get().getSchema(recordClazz);
    -		this.datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    -		this.record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    -		this.inputStream = new MutableByteArrayInputStream();
    -		this.decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    -		this.typeInfo = AvroRecordClassConverter.convert(recordClazz);
    +		schema = SpecificData.get().getSchema(recordClazz);
    +		typeInfo = AvroSchemaConverter.convert(recordClazz);
    +		schemaString = schema.toString();
    +		record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    +		datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    +		inputStream = new MutableByteArrayInputStream();
    +		decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a Avro deserialization schema for the given Avro schema string.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param avroSchemaString Avro schema string to deserialize Avro&apos;s record to Flink&apos;s row&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public AvroRowDeserializationSchema(String avroSchemaString) {&lt;br/&gt;
    +		Preconditions.checkNotNull(avroSchemaString, &quot;Avro schema must not be null.&quot;);&lt;br/&gt;
    +		recordClazz = null;&lt;br/&gt;
    +		typeInfo = AvroSchemaConverter.convert(avroSchemaString);&lt;br/&gt;
    +		schemaString = avroSchemaString;&lt;br/&gt;
    +		schema = new Schema.Parser().parse(avroSchemaString);&lt;br/&gt;
    +		record = new GenericData.Record(schema);&lt;br/&gt;
    +		datumReader = new GenericDatumReader&amp;lt;&amp;gt;(schema);&lt;br/&gt;
    +		inputStream = new MutableByteArrayInputStream();&lt;br/&gt;
    +		decoder = DecoderFactory.get().binaryDecoder(inputStream, null);&lt;br/&gt;
    +		// check for a schema that describes a record&lt;br/&gt;
    +		if (!(typeInfo instanceof RowTypeInfo)) {
    +			throw new IllegalArgumentException(&quot;Row type information expected.&quot;);
    +		}&lt;br/&gt;
     	}&lt;br/&gt;
     &lt;br/&gt;
     	@Override&lt;br/&gt;
     	public Row deserialize(byte[] message) throws IOException {&lt;br/&gt;
    -		// read record&lt;br/&gt;
     		try {
     			inputStream.setBuffer(message);
    -			this.record = datumReader.read(record, decoder);
    -		} catch (IOException e) {
    -			throw new RuntimeException(&quot;Failed to deserialize Row.&quot;, e);
    +			final IndexedRecord read = datumReader.read(record, decoder);
    +			return convertRecord(schema, (RowTypeInfo) typeInfo, read);
    +		} catch (Exception e) {
    +			throw new IOException(&quot;Failed to deserialize Avro record.&quot;, e);
     		}&lt;br/&gt;
    -&lt;br/&gt;
    -		// convert to row&lt;br/&gt;
    -		final Object row = convertToRow(schema, record);&lt;br/&gt;
    -		return (Row) row;&lt;br/&gt;
    -	}&lt;br/&gt;
    -&lt;br/&gt;
    -	private void writeObject(ObjectOutputStream oos) throws IOException {
    -		oos.writeObject(recordClazz);
    -	}&lt;br/&gt;
    -&lt;br/&gt;
    -	@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    -	private void readObject(ObjectInputStream ois) throws ClassNotFoundException, IOException {
    -		this.recordClazz = (Class&amp;lt;? extends SpecificRecord&amp;gt;) ois.readObject();
    -		this.schema = SpecificData.get().getSchema(recordClazz);
    -		this.datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    -		this.record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    -		this.inputStream = new MutableByteArrayInputStream();
    -		this.decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
     	}&lt;br/&gt;
     &lt;br/&gt;
     	@Override&lt;br/&gt;
     	public TypeInformation&amp;lt;Row&amp;gt; getProducedType() {
     		return typeInfo;
     	}&lt;br/&gt;
     &lt;br/&gt;
    -	/**&lt;br/&gt;
    -	 * Converts a (nested) Avro {@link SpecificRecord}
&lt;p&gt; into Flink&apos;s Row type.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* Avro&apos;s 
{@link Utf8}
&lt;p&gt; fields are converted into regular Java strings.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private static Object convertToRow(Schema schema, Object recordObj) {&lt;/li&gt;
	&lt;li&gt;if (recordObj instanceof GenericRecord) {&lt;/li&gt;
	&lt;li&gt;// records can be wrapped in a union&lt;/li&gt;
	&lt;li&gt;if (schema.getType() == Schema.Type.UNION) {&lt;br/&gt;
    +	// --------------------------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	private Row convertRecord(Schema schema, RowTypeInfo typeInfo, IndexedRecord record) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +		final List&amp;lt;Schema.Field&amp;gt; fields = schema.getFields();    +		final TypeInformation&amp;lt;?&amp;gt;[] fieldInfo = typeInfo.getFieldTypes();    +		final int length = fields.size();    +		final Row row = new Row(length);    +		for (int i = 0; i &amp;lt; length; i++) {
    +			final Schema.Field field = fields.get(i);
    +			row.setField(i, convert(field.schema(), fieldInfo[i], record.get(i)));
    +		}    +		return row;    +	}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;    +&lt;br/&gt;
    +	private Object convert(Schema schema, TypeInformation&amp;lt;?&amp;gt; info, Object object) {&lt;br/&gt;
    +		// we perform the conversion based on schema information but enriched with pre-computed&lt;br/&gt;
    +		// type information where useful (i.e., for arrays)&lt;br/&gt;
    +&lt;br/&gt;
    +		if (object == null) &lt;/p&gt;
{
    +			return null;
    +		}
&lt;p&gt;    +		switch (schema.getType()) {&lt;br/&gt;
    +			case RECORD:&lt;br/&gt;
    +				if (object instanceof IndexedRecord) &lt;/p&gt;
{
    +					return convertRecord(schema, (RowTypeInfo) info, (IndexedRecord) object);
    +				}
&lt;p&gt;    +				throw new IllegalStateException(&quot;IndexedRecord expected but was: &quot; + object.getClass());&lt;br/&gt;
    +			case ENUM:&lt;br/&gt;
    +			case STRING:&lt;br/&gt;
    +				return object.toString();&lt;br/&gt;
    +			case ARRAY:&lt;br/&gt;
    +				if (info instanceof BasicArrayTypeInfo) &lt;/p&gt;
{
    +					final BasicArrayTypeInfo&amp;lt;?, ?&amp;gt; bati = (BasicArrayTypeInfo&amp;lt;?, ?&amp;gt;) info;
    +					final TypeInformation&amp;lt;?&amp;gt; elementInfo = bati.getComponentInfo();
    +					return convertObjectArray(schema.getElementType(), elementInfo, object);
    +				}
&lt;p&gt; else &lt;/p&gt;
{
    +					final ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt; oati = (ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt;) info;
    +					final TypeInformation&amp;lt;?&amp;gt; elementInfo = oati.getComponentInfo();
    +					return convertObjectArray(schema.getElementType(), elementInfo, object);
    +				}
&lt;p&gt;    +			case MAP:&lt;br/&gt;
    +				final MapTypeInfo&amp;lt;?, ?&amp;gt; mti = (MapTypeInfo&amp;lt;?, ?&amp;gt;) info;&lt;br/&gt;
    +				final Map&amp;lt;String, Object&amp;gt; convertedMap = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +				final Map&amp;lt;?, ?&amp;gt; map = (Map&amp;lt;?, ?&amp;gt;) object;&lt;br/&gt;
    +				for (Map.Entry&amp;lt;?, ?&amp;gt; entry : map.entrySet()) &lt;/p&gt;
{
    +					convertedMap.put(
    +						entry.getKey().toString(),
    +						convert(schema.getValueType(), mti.getValueTypeInfo(), entry.getValue()));
    +				}
&lt;p&gt;    +				return convertedMap;&lt;br/&gt;
    +			case UNION:&lt;br/&gt;
     				final List&amp;lt;Schema&amp;gt; types = schema.getTypes();&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (types.size() == 2 &amp;amp;&amp;amp; types.get(0).getType() == Schema.Type.NULL &amp;amp;&amp;amp; types.get(1).getType() == Schema.Type.RECORD) {&lt;/li&gt;
	&lt;li&gt;schema = types.get(1);&lt;br/&gt;
    +				final int size = types.size();&lt;br/&gt;
    +				final Schema actualSchema;&lt;br/&gt;
    +				if (size == 2 &amp;amp;&amp;amp; types.get(0).getType() == Schema.Type.NULL) 
{
    +					return convert(types.get(1), info, object);
    +				}
&lt;p&gt; else if (size == 2 &amp;amp;&amp;amp; types.get(1).getType() == Schema.Type.NULL) &lt;/p&gt;
{
    +					return convert(types.get(0), info, object);
    +				} else if (size == 1) {    +					return convert(types.get(0), info, object);    +				}
&lt;p&gt; else &lt;/p&gt;
{
    +					// generic type
    +					return object;
    +				}
&lt;p&gt;    +			case FIXED:&lt;br/&gt;
    +				final byte[] fixedBytes = ((GenericFixed) object).bytes();&lt;br/&gt;
    +				if (info == Types.BIG_DEC) &lt;/p&gt;
{
    +					return convertDecimal(schema, fixedBytes);
     				}&lt;/li&gt;
	&lt;li&gt;else {&lt;/li&gt;
	&lt;li&gt;throw new RuntimeException(&quot;Currently we only support schemas of the following form: UNION&lt;span class=&quot;error&quot;&gt;&amp;#91;null, RECORD&amp;#93;&lt;/span&gt;. Given: &quot; + schema);&lt;br/&gt;
    +				return fixedBytes;&lt;br/&gt;
    +			case BYTES:&lt;br/&gt;
    +				final ByteBuffer bb = (ByteBuffer) object;
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    We should only add checks in runtime code if really necessary. IMHO it does not matter if a cast exception or a illegal state exception is thrown.&lt;/p&gt;</comment>
                            <comment id="16531178" author="githubbot" created="Tue, 3 Jul 2018 10:33:46 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199759847&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199759847&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -17,154 +17,338 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.formats.avro;&lt;/p&gt;

&lt;p&gt;    +import org.apache.flink.annotation.PublicEvolving;&lt;br/&gt;
     import org.apache.flink.api.common.serialization.AbstractDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;&lt;br/&gt;
     import org.apache.flink.api.common.typeinfo.TypeInformation;&lt;br/&gt;
    -import org.apache.flink.formats.avro.typeutils.AvroRecordClassConverter;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.Types;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.MapTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.RowTypeInfo;&lt;br/&gt;
    +import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;&lt;br/&gt;
     import org.apache.flink.formats.avro.utils.MutableByteArrayInputStream;&lt;br/&gt;
     import org.apache.flink.types.Row;&lt;br/&gt;
     import org.apache.flink.util.Preconditions;&lt;/p&gt;

&lt;p&gt;    +import org.apache.avro.LogicalTypes;&lt;br/&gt;
     import org.apache.avro.Schema;&lt;br/&gt;
    -import org.apache.avro.generic.GenericRecord;&lt;br/&gt;
    +import org.apache.avro.generic.GenericData;&lt;br/&gt;
    +import org.apache.avro.generic.GenericDatumReader;&lt;br/&gt;
    +import org.apache.avro.generic.GenericFixed;&lt;br/&gt;
    +import org.apache.avro.generic.IndexedRecord;&lt;br/&gt;
     import org.apache.avro.io.DatumReader;&lt;br/&gt;
     import org.apache.avro.io.Decoder;&lt;br/&gt;
     import org.apache.avro.io.DecoderFactory;&lt;br/&gt;
     import org.apache.avro.specific.SpecificData;&lt;br/&gt;
     import org.apache.avro.specific.SpecificDatumReader;&lt;br/&gt;
     import org.apache.avro.specific.SpecificRecord;&lt;br/&gt;
    -import org.apache.avro.specific.SpecificRecordBase;&lt;br/&gt;
    -import org.apache.avro.util.Utf8;&lt;br/&gt;
    +import org.joda.time.DateTime;&lt;br/&gt;
    +import org.joda.time.DateTimeFieldType;&lt;br/&gt;
    +import org.joda.time.LocalDate;&lt;br/&gt;
    +import org.joda.time.LocalTime;&lt;/p&gt;

&lt;p&gt;     import java.io.IOException;&lt;br/&gt;
     import java.io.ObjectInputStream;&lt;br/&gt;
     import java.io.ObjectOutputStream;&lt;br/&gt;
    +import java.lang.reflect.Array;&lt;br/&gt;
    +import java.math.BigDecimal;&lt;br/&gt;
    +import java.math.BigInteger;&lt;br/&gt;
    +import java.nio.ByteBuffer;&lt;br/&gt;
    +import java.sql.Date;&lt;br/&gt;
    +import java.sql.Time;&lt;br/&gt;
    +import java.sql.Timestamp;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
     import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.TimeZone;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Deserialization schema from Avro bytes over 
{@link SpecificRecord} to {@link Row}.&lt;br/&gt;
    + * Deserialization schema from Avro bytes to {@link Row}.&lt;br/&gt;
      *&lt;br/&gt;
    - * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink Rows.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink rows. It converts Avro types&lt;br/&gt;
    + * into types that are compatible with Flink&apos;s Table &amp;amp; SQL API.&lt;br/&gt;
      *&lt;br/&gt;
    - * {@link Utf8} is converted to regular Java Strings.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Projects with Avro records containing logical date/time types need to add a JodaTime&lt;br/&gt;
    + * dependency.&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Note: Changes in this class need to be kept in sync with the corresponding runtime&lt;br/&gt;
    + * class {@link AvroRowSerializationSchema} and schema converter {@link AvroSchemaConverter}.&lt;br/&gt;
      */&lt;br/&gt;
    +@PublicEvolving&lt;br/&gt;
     public class AvroRowDeserializationSchema extends AbstractDeserializationSchema&amp;lt;Row&amp;gt; {&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Avro record class.&lt;br/&gt;
    +	 * Used for time conversions into SQL types.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private static final TimeZone LOCAL_TZ = TimeZone.getDefault();&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro record class for deserialization. Might be null if record class is not available.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Schema for deterministic field order.&lt;br/&gt;
    +	 * Schema string for deserialization.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private String schemaString;&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro serialization schema.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private transient Schema schema;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Reader that deserializes byte array into a record.&lt;br/&gt;
    +	 * Type information describing the result type.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient DatumReader&amp;lt;SpecificRecord&amp;gt; datumReader;&lt;br/&gt;
    +	private transient TypeInformation&amp;lt;Row&amp;gt; typeInfo;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Input stream to read message from.&lt;br/&gt;
    +	 * Record to deserialize byte array.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient MutableByteArrayInputStream inputStream;&lt;br/&gt;
    +	private transient IndexedRecord record;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Avro decoder that decodes binary data.&lt;br/&gt;
    +	 * Reader that deserializes byte array into a record.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient Decoder decoder;&lt;br/&gt;
    +	private transient DatumReader&amp;lt;IndexedRecord&amp;gt; datumReader;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Record to deserialize byte array to.&lt;br/&gt;
    +	 * Input stream to read message from.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private SpecificRecord record;&lt;br/&gt;
    +	private transient MutableByteArrayInputStream inputStream;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Type information describing the result type.&lt;br/&gt;
    +	 * Avro decoder that decodes binary data.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient TypeInformation&amp;lt;Row&amp;gt; typeInfo;&lt;br/&gt;
    +	private transient Decoder decoder;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Creates a Avro deserialization schema for the given record.&lt;br/&gt;
    +	 * Creates a Avro deserialization schema for the given specific record class. Having the&lt;br/&gt;
    +	 * concrete Avro record class might improve performance.&lt;br/&gt;
     	 *&lt;br/&gt;
     	 * @param recordClazz Avro record class used to deserialize Avro&apos;s record to Flink&apos;s row&lt;br/&gt;
     	 */&lt;br/&gt;
    -	public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecordBase&amp;gt; recordClazz) {&lt;br/&gt;
    +	public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz) {
     		Preconditions.checkNotNull(recordClazz, &quot;Avro record class must not be null.&quot;);
     		this.recordClazz = recordClazz;
    -		this.schema = SpecificData.get().getSchema(recordClazz);
    -		this.datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    -		this.record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    -		this.inputStream = new MutableByteArrayInputStream();
    -		this.decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    -		this.typeInfo = AvroRecordClassConverter.convert(recordClazz);
    +		schema = SpecificData.get().getSchema(recordClazz);
    +		typeInfo = AvroSchemaConverter.convert(recordClazz);
    +		schemaString = schema.toString();
    +		record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    +		datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    +		inputStream = new MutableByteArrayInputStream();
    +		decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a Avro deserialization schema for the given Avro schema string.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param avroSchemaString Avro schema string to deserialize Avro&apos;s record to Flink&apos;s row&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public AvroRowDeserializationSchema(String avroSchemaString) {&lt;br/&gt;
    +		Preconditions.checkNotNull(avroSchemaString, &quot;Avro schema must not be null.&quot;);&lt;br/&gt;
    +		recordClazz = null;&lt;br/&gt;
    +		typeInfo = AvroSchemaConverter.convert(avroSchemaString);&lt;br/&gt;
    +		schemaString = avroSchemaString;&lt;br/&gt;
    +		schema = new Schema.Parser().parse(avroSchemaString);&lt;br/&gt;
    +		record = new GenericData.Record(schema);&lt;br/&gt;
    +		datumReader = new GenericDatumReader&amp;lt;&amp;gt;(schema);&lt;br/&gt;
    +		inputStream = new MutableByteArrayInputStream();&lt;br/&gt;
    +		decoder = DecoderFactory.get().binaryDecoder(inputStream, null);&lt;br/&gt;
    +		// check for a schema that describes a record&lt;br/&gt;
    +		if (!(typeInfo instanceof RowTypeInfo)) {
    +			throw new IllegalArgumentException(&quot;Row type information expected.&quot;);
    +		}&lt;br/&gt;
     	}&lt;br/&gt;
     &lt;br/&gt;
     	@Override&lt;br/&gt;
     	public Row deserialize(byte[] message) throws IOException {&lt;br/&gt;
    -		// read record&lt;br/&gt;
     		try {
     			inputStream.setBuffer(message);
    -			this.record = datumReader.read(record, decoder);
    -		} catch (IOException e) {
    -			throw new RuntimeException(&quot;Failed to deserialize Row.&quot;, e);
    +			final IndexedRecord read = datumReader.read(record, decoder);
    +			return convertRecord(schema, (RowTypeInfo) typeInfo, read);
    +		} catch (Exception e) {
    +			throw new IOException(&quot;Failed to deserialize Avro record.&quot;, e);
     		}&lt;br/&gt;
    -&lt;br/&gt;
    -		// convert to row&lt;br/&gt;
    -		final Object row = convertToRow(schema, record);&lt;br/&gt;
    -		return (Row) row;&lt;br/&gt;
    -	}&lt;br/&gt;
    -&lt;br/&gt;
    -	private void writeObject(ObjectOutputStream oos) throws IOException {
    -		oos.writeObject(recordClazz);
    -	}&lt;br/&gt;
    -&lt;br/&gt;
    -	@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    -	private void readObject(ObjectInputStream ois) throws ClassNotFoundException, IOException {
    -		this.recordClazz = (Class&amp;lt;? extends SpecificRecord&amp;gt;) ois.readObject();
    -		this.schema = SpecificData.get().getSchema(recordClazz);
    -		this.datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    -		this.record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    -		this.inputStream = new MutableByteArrayInputStream();
    -		this.decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
     	}&lt;br/&gt;
     &lt;br/&gt;
     	@Override&lt;br/&gt;
     	public TypeInformation&amp;lt;Row&amp;gt; getProducedType() {
     		return typeInfo;
     	}&lt;br/&gt;
     &lt;br/&gt;
    -	/**&lt;br/&gt;
    -	 * Converts a (nested) Avro {@link SpecificRecord}
&lt;p&gt; into Flink&apos;s Row type.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* Avro&apos;s 
{@link Utf8}
&lt;p&gt; fields are converted into regular Java strings.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private static Object convertToRow(Schema schema, Object recordObj) {&lt;/li&gt;
	&lt;li&gt;if (recordObj instanceof GenericRecord) {&lt;/li&gt;
	&lt;li&gt;// records can be wrapped in a union&lt;/li&gt;
	&lt;li&gt;if (schema.getType() == Schema.Type.UNION) {&lt;br/&gt;
    +	// --------------------------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	private Row convertRecord(Schema schema, RowTypeInfo typeInfo, IndexedRecord record) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +		final List&amp;lt;Schema.Field&amp;gt; fields = schema.getFields();    +		final TypeInformation&amp;lt;?&amp;gt;[] fieldInfo = typeInfo.getFieldTypes();    +		final int length = fields.size();    +		final Row row = new Row(length);    +		for (int i = 0; i &amp;lt; length; i++) {
    +			final Schema.Field field = fields.get(i);
    +			row.setField(i, convert(field.schema(), fieldInfo[i], record.get(i)));
    +		}    +		return row;    +	}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;    +&lt;br/&gt;
    +	private Object convert(Schema schema, TypeInformation&amp;lt;?&amp;gt; info, Object object) {&lt;br/&gt;
    +		// we perform the conversion based on schema information but enriched with pre-computed&lt;br/&gt;
    +		// type information where useful (i.e., for arrays)&lt;br/&gt;
    +&lt;br/&gt;
    +		if (object == null) &lt;/p&gt;
{
    +			return null;
    +		}
&lt;p&gt;    +		switch (schema.getType()) {&lt;br/&gt;
    +			case RECORD:&lt;br/&gt;
    +				if (object instanceof IndexedRecord) &lt;/p&gt;
{
    +					return convertRecord(schema, (RowTypeInfo) info, (IndexedRecord) object);
    +				}
&lt;p&gt;    +				throw new IllegalStateException(&quot;IndexedRecord expected but was: &quot; + object.getClass());&lt;br/&gt;
    +			case ENUM:&lt;br/&gt;
    +			case STRING:&lt;br/&gt;
    +				return object.toString();&lt;br/&gt;
    +			case ARRAY:&lt;br/&gt;
    +				if (info instanceof BasicArrayTypeInfo) &lt;/p&gt;
{
    +					final BasicArrayTypeInfo&amp;lt;?, ?&amp;gt; bati = (BasicArrayTypeInfo&amp;lt;?, ?&amp;gt;) info;
    +					final TypeInformation&amp;lt;?&amp;gt; elementInfo = bati.getComponentInfo();
    +					return convertObjectArray(schema.getElementType(), elementInfo, object);
    +				}
&lt;p&gt; else &lt;/p&gt;
{
    +					final ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt; oati = (ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt;) info;
    +					final TypeInformation&amp;lt;?&amp;gt; elementInfo = oati.getComponentInfo();
    +					return convertObjectArray(schema.getElementType(), elementInfo, object);
    +				}
&lt;p&gt;    +			case MAP:&lt;br/&gt;
    +				final MapTypeInfo&amp;lt;?, ?&amp;gt; mti = (MapTypeInfo&amp;lt;?, ?&amp;gt;) info;&lt;br/&gt;
    +				final Map&amp;lt;String, Object&amp;gt; convertedMap = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +				final Map&amp;lt;?, ?&amp;gt; map = (Map&amp;lt;?, ?&amp;gt;) object;&lt;br/&gt;
    +				for (Map.Entry&amp;lt;?, ?&amp;gt; entry : map.entrySet()) &lt;/p&gt;
{
    +					convertedMap.put(
    +						entry.getKey().toString(),
    +						convert(schema.getValueType(), mti.getValueTypeInfo(), entry.getValue()));
    +				}
&lt;p&gt;    +				return convertedMap;&lt;br/&gt;
    +			case UNION:&lt;br/&gt;
     				final List&amp;lt;Schema&amp;gt; types = schema.getTypes();&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (types.size() == 2 &amp;amp;&amp;amp; types.get(0).getType() == Schema.Type.NULL &amp;amp;&amp;amp; types.get(1).getType() == Schema.Type.RECORD) {&lt;/li&gt;
	&lt;li&gt;schema = types.get(1);&lt;br/&gt;
    +				final int size = types.size();&lt;br/&gt;
    +				final Schema actualSchema;&lt;br/&gt;
    +				if (size == 2 &amp;amp;&amp;amp; types.get(0).getType() == Schema.Type.NULL) 
{
    +					return convert(types.get(1), info, object);
    +				}
&lt;p&gt; else if (size == 2 &amp;amp;&amp;amp; types.get(1).getType() == Schema.Type.NULL) &lt;/p&gt;
{
    +					return convert(types.get(0), info, object);
    +				} else if (size == 1) {    +					return convert(types.get(0), info, object);    +				}
&lt;p&gt; else &lt;/p&gt;
{
    +					// generic type
    +					return object;
    +				}
&lt;p&gt;    +			case FIXED:&lt;br/&gt;
    +				final byte[] fixedBytes = ((GenericFixed) object).bytes();&lt;br/&gt;
    +				if (info == Types.BIG_DEC) &lt;/p&gt;
{
    +					return convertDecimal(schema, fixedBytes);
     				}&lt;/li&gt;
	&lt;li&gt;else {&lt;/li&gt;
	&lt;li&gt;throw new RuntimeException(&quot;Currently we only support schemas of the following form: UNION&lt;span class=&quot;error&quot;&gt;&amp;#91;null, RECORD&amp;#93;&lt;/span&gt;. Given: &quot; + schema);&lt;br/&gt;
    +				return fixedBytes;&lt;br/&gt;
    +			case BYTES:&lt;br/&gt;
    +				final ByteBuffer bb = (ByteBuffer) object;&lt;br/&gt;
    +				bb.position(0);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    We cannot be sure about that. It is safer to reset the position.&lt;/p&gt;</comment>
                            <comment id="16531189" author="githubbot" created="Tue, 3 Jul 2018 10:41:47 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199761646&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199761646&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,71 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.formats.avro.typeutils;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.TypeInformation;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.Types;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.RowTypeInfo;&lt;br/&gt;
    +import org.apache.flink.formats.avro.generated.User;&lt;br/&gt;
    +import org.apache.flink.types.Row;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertEquals;&lt;br/&gt;
    +import static org.junit.Assert.assertTrue;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Tests for &lt;/p&gt;
{@link AvroSchemaConverter}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class AvroSchemaConverterTest {&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testAvroClassConversion() &lt;/p&gt;
{
    +		validateUserSchema(AvroSchemaConverter.convert(User.class));
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testAvroSchemaConversion() &lt;/p&gt;
{
    +		final String schema = User.getClassSchema().toString(true);
    +		validateUserSchema(AvroSchemaConverter.convert(schema));
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	private void validateUserSchema(TypeInformation&amp;lt;?&amp;gt; actual) {&lt;br/&gt;
    +		final TypeInformation&amp;lt;Row&amp;gt; address = Types.ROW_NAMED(&lt;br/&gt;
    +			new String[]&lt;/p&gt;
{&quot;num&quot;, &quot;street&quot;, &quot;city&quot;, &quot;state&quot;, &quot;zip&quot;}
&lt;p&gt;,&lt;br/&gt;
    +			Types.INT, Types.STRING, Types.STRING, Types.STRING, Types.STRING);&lt;br/&gt;
    +&lt;br/&gt;
    +		final TypeInformation&amp;lt;Row&amp;gt; user = Types.ROW_NAMED(&lt;br/&gt;
    +			new String[] {&quot;name&quot;, &quot;favorite_number&quot;, &quot;favorite_color&quot;, &quot;type_long_test&quot;,&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    Actually, I&apos;m a big fan of per line fields but it also blows up the code.&lt;/p&gt;</comment>
                            <comment id="16531251" author="githubbot" created="Tue, 3 Jul 2018 12:00:56 +0000"  >&lt;p&gt;Github user pnowojski commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199778602&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199778602&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -148,25 +148,26 @@ public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz)&lt;br/&gt;
     	public AvroRowDeserializationSchema(String avroSchemaString) {&lt;br/&gt;
     		Preconditions.checkNotNull(avroSchemaString, &quot;Avro schema must not be null.&quot;);&lt;br/&gt;
     		recordClazz = null;&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;typeInfo = AvroSchemaConverter.convert(avroSchemaString);&lt;br/&gt;
    +		final TypeInformation&amp;lt;?&amp;gt; typeInfo = AvroSchemaConverter.convertToTypeInfo(avroSchemaString);&lt;br/&gt;
    +		// check for a schema that describes a record&lt;br/&gt;
    +		if (!(typeInfo instanceof RowTypeInfo)) {&lt;br/&gt;
    +			throw new IllegalArgumentException(&quot;Row type information expected.&quot;);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    `Preconditions.checkArgument`?&lt;/p&gt;</comment>
                            <comment id="16531252" author="githubbot" created="Tue, 3 Jul 2018 12:00:56 +0000"  >&lt;p&gt;Github user pnowojski commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199780249&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199780249&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -17,154 +17,338 @@&lt;/p&gt;

&lt;p&gt;     package org.apache.flink.formats.avro;&lt;/p&gt;

&lt;p&gt;    +import org.apache.flink.annotation.PublicEvolving;&lt;br/&gt;
     import org.apache.flink.api.common.serialization.AbstractDeserializationSchema;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.BasicArrayTypeInfo;&lt;br/&gt;
     import org.apache.flink.api.common.typeinfo.TypeInformation;&lt;br/&gt;
    -import org.apache.flink.formats.avro.typeutils.AvroRecordClassConverter;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.Types;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.MapTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.ObjectArrayTypeInfo;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.RowTypeInfo;&lt;br/&gt;
    +import org.apache.flink.formats.avro.typeutils.AvroSchemaConverter;&lt;br/&gt;
     import org.apache.flink.formats.avro.utils.MutableByteArrayInputStream;&lt;br/&gt;
     import org.apache.flink.types.Row;&lt;br/&gt;
     import org.apache.flink.util.Preconditions;&lt;/p&gt;

&lt;p&gt;    +import org.apache.avro.LogicalTypes;&lt;br/&gt;
     import org.apache.avro.Schema;&lt;br/&gt;
    -import org.apache.avro.generic.GenericRecord;&lt;br/&gt;
    +import org.apache.avro.generic.GenericData;&lt;br/&gt;
    +import org.apache.avro.generic.GenericDatumReader;&lt;br/&gt;
    +import org.apache.avro.generic.GenericFixed;&lt;br/&gt;
    +import org.apache.avro.generic.IndexedRecord;&lt;br/&gt;
     import org.apache.avro.io.DatumReader;&lt;br/&gt;
     import org.apache.avro.io.Decoder;&lt;br/&gt;
     import org.apache.avro.io.DecoderFactory;&lt;br/&gt;
     import org.apache.avro.specific.SpecificData;&lt;br/&gt;
     import org.apache.avro.specific.SpecificDatumReader;&lt;br/&gt;
     import org.apache.avro.specific.SpecificRecord;&lt;br/&gt;
    -import org.apache.avro.specific.SpecificRecordBase;&lt;br/&gt;
    -import org.apache.avro.util.Utf8;&lt;br/&gt;
    +import org.joda.time.DateTime;&lt;br/&gt;
    +import org.joda.time.DateTimeFieldType;&lt;br/&gt;
    +import org.joda.time.LocalDate;&lt;br/&gt;
    +import org.joda.time.LocalTime;&lt;/p&gt;

&lt;p&gt;     import java.io.IOException;&lt;br/&gt;
     import java.io.ObjectInputStream;&lt;br/&gt;
     import java.io.ObjectOutputStream;&lt;br/&gt;
    +import java.lang.reflect.Array;&lt;br/&gt;
    +import java.math.BigDecimal;&lt;br/&gt;
    +import java.math.BigInteger;&lt;br/&gt;
    +import java.nio.ByteBuffer;&lt;br/&gt;
    +import java.sql.Date;&lt;br/&gt;
    +import java.sql.Time;&lt;br/&gt;
    +import java.sql.Timestamp;&lt;br/&gt;
    +import java.util.HashMap;&lt;br/&gt;
     import java.util.List;&lt;br/&gt;
    +import java.util.Map;&lt;br/&gt;
    +import java.util.TimeZone;&lt;/p&gt;

&lt;p&gt;     /**&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;* Deserialization schema from Avro bytes over 
{@link SpecificRecord} to {@link Row}.&lt;br/&gt;
    + * Deserialization schema from Avro bytes to {@link Row}.&lt;br/&gt;
      *&lt;br/&gt;
    - * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink Rows.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Deserializes the &amp;lt;code&amp;gt;byte[]&amp;lt;/code&amp;gt; messages into (nested) Flink rows. It converts Avro types&lt;br/&gt;
    + * into types that are compatible with Flink&apos;s Table &amp;amp; SQL API.&lt;br/&gt;
      *&lt;br/&gt;
    - * {@link Utf8} is converted to regular Java Strings.&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Projects with Avro records containing logical date/time types need to add a JodaTime&lt;br/&gt;
    + * dependency.&lt;br/&gt;
    + *&lt;br/&gt;
    + * &amp;lt;p&amp;gt;Note: Changes in this class need to be kept in sync with the corresponding runtime&lt;br/&gt;
    + * class {@link AvroRowSerializationSchema} and schema converter {@link AvroSchemaConverter}.&lt;br/&gt;
      */&lt;br/&gt;
    +@PublicEvolving&lt;br/&gt;
     public class AvroRowDeserializationSchema extends AbstractDeserializationSchema&amp;lt;Row&amp;gt; {&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Avro record class.&lt;br/&gt;
    +	 * Used for time conversions into SQL types.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private static final TimeZone LOCAL_TZ = TimeZone.getDefault();&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro record class for deserialization. Might be null if record class is not available.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Schema for deterministic field order.&lt;br/&gt;
    +	 * Schema string for deserialization.&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	private String schemaString;&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Avro serialization schema.&lt;br/&gt;
     	 */&lt;br/&gt;
     	private transient Schema schema;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Reader that deserializes byte array into a record.&lt;br/&gt;
    +	 * Type information describing the result type.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient DatumReader&amp;lt;SpecificRecord&amp;gt; datumReader;&lt;br/&gt;
    +	private transient TypeInformation&amp;lt;Row&amp;gt; typeInfo;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Input stream to read message from.&lt;br/&gt;
    +	 * Record to deserialize byte array.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient MutableByteArrayInputStream inputStream;&lt;br/&gt;
    +	private transient IndexedRecord record;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Avro decoder that decodes binary data.&lt;br/&gt;
    +	 * Reader that deserializes byte array into a record.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient Decoder decoder;&lt;br/&gt;
    +	private transient DatumReader&amp;lt;IndexedRecord&amp;gt; datumReader;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Record to deserialize byte array to.&lt;br/&gt;
    +	 * Input stream to read message from.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private SpecificRecord record;&lt;br/&gt;
    +	private transient MutableByteArrayInputStream inputStream;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Type information describing the result type.&lt;br/&gt;
    +	 * Avro decoder that decodes binary data.&lt;br/&gt;
     	 */&lt;br/&gt;
    -	private transient TypeInformation&amp;lt;Row&amp;gt; typeInfo;&lt;br/&gt;
    +	private transient Decoder decoder;&lt;br/&gt;
     &lt;br/&gt;
     	/**&lt;br/&gt;
    -	 * Creates a Avro deserialization schema for the given record.&lt;br/&gt;
    +	 * Creates a Avro deserialization schema for the given specific record class. Having the&lt;br/&gt;
    +	 * concrete Avro record class might improve performance.&lt;br/&gt;
     	 *&lt;br/&gt;
     	 * @param recordClazz Avro record class used to deserialize Avro&apos;s record to Flink&apos;s row&lt;br/&gt;
     	 */&lt;br/&gt;
    -	public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecordBase&amp;gt; recordClazz) {&lt;br/&gt;
    +	public AvroRowDeserializationSchema(Class&amp;lt;? extends SpecificRecord&amp;gt; recordClazz) {
     		Preconditions.checkNotNull(recordClazz, &quot;Avro record class must not be null.&quot;);
     		this.recordClazz = recordClazz;
    -		this.schema = SpecificData.get().getSchema(recordClazz);
    -		this.datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    -		this.record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    -		this.inputStream = new MutableByteArrayInputStream();
    -		this.decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    -		this.typeInfo = AvroRecordClassConverter.convert(recordClazz);
    +		schema = SpecificData.get().getSchema(recordClazz);
    +		typeInfo = AvroSchemaConverter.convert(recordClazz);
    +		schemaString = schema.toString();
    +		record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    +		datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    +		inputStream = new MutableByteArrayInputStream();
    +		decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
    +	}&lt;br/&gt;
    +&lt;br/&gt;
    +	/**&lt;br/&gt;
    +	 * Creates a Avro deserialization schema for the given Avro schema string.&lt;br/&gt;
    +	 *&lt;br/&gt;
    +	 * @param avroSchemaString Avro schema string to deserialize Avro&apos;s record to Flink&apos;s row&lt;br/&gt;
    +	 */&lt;br/&gt;
    +	public AvroRowDeserializationSchema(String avroSchemaString) {&lt;br/&gt;
    +		Preconditions.checkNotNull(avroSchemaString, &quot;Avro schema must not be null.&quot;);&lt;br/&gt;
    +		recordClazz = null;&lt;br/&gt;
    +		typeInfo = AvroSchemaConverter.convert(avroSchemaString);&lt;br/&gt;
    +		schemaString = avroSchemaString;&lt;br/&gt;
    +		schema = new Schema.Parser().parse(avroSchemaString);&lt;br/&gt;
    +		record = new GenericData.Record(schema);&lt;br/&gt;
    +		datumReader = new GenericDatumReader&amp;lt;&amp;gt;(schema);&lt;br/&gt;
    +		inputStream = new MutableByteArrayInputStream();&lt;br/&gt;
    +		decoder = DecoderFactory.get().binaryDecoder(inputStream, null);&lt;br/&gt;
    +		// check for a schema that describes a record&lt;br/&gt;
    +		if (!(typeInfo instanceof RowTypeInfo)) {
    +			throw new IllegalArgumentException(&quot;Row type information expected.&quot;);
    +		}&lt;br/&gt;
     	}&lt;br/&gt;
     &lt;br/&gt;
     	@Override&lt;br/&gt;
     	public Row deserialize(byte[] message) throws IOException {&lt;br/&gt;
    -		// read record&lt;br/&gt;
     		try {
     			inputStream.setBuffer(message);
    -			this.record = datumReader.read(record, decoder);
    -		} catch (IOException e) {
    -			throw new RuntimeException(&quot;Failed to deserialize Row.&quot;, e);
    +			final IndexedRecord read = datumReader.read(record, decoder);
    +			return convertRecord(schema, (RowTypeInfo) typeInfo, read);
    +		} catch (Exception e) {
    +			throw new IOException(&quot;Failed to deserialize Avro record.&quot;, e);
     		}&lt;br/&gt;
    -&lt;br/&gt;
    -		// convert to row&lt;br/&gt;
    -		final Object row = convertToRow(schema, record);&lt;br/&gt;
    -		return (Row) row;&lt;br/&gt;
    -	}&lt;br/&gt;
    -&lt;br/&gt;
    -	private void writeObject(ObjectOutputStream oos) throws IOException {
    -		oos.writeObject(recordClazz);
    -	}&lt;br/&gt;
    -&lt;br/&gt;
    -	@SuppressWarnings(&quot;unchecked&quot;)&lt;br/&gt;
    -	private void readObject(ObjectInputStream ois) throws ClassNotFoundException, IOException {
    -		this.recordClazz = (Class&amp;lt;? extends SpecificRecord&amp;gt;) ois.readObject();
    -		this.schema = SpecificData.get().getSchema(recordClazz);
    -		this.datumReader = new SpecificDatumReader&amp;lt;&amp;gt;(schema);
    -		this.record = (SpecificRecord) SpecificData.newInstance(recordClazz, schema);
    -		this.inputStream = new MutableByteArrayInputStream();
    -		this.decoder = DecoderFactory.get().binaryDecoder(inputStream, null);
     	}&lt;br/&gt;
     &lt;br/&gt;
     	@Override&lt;br/&gt;
     	public TypeInformation&amp;lt;Row&amp;gt; getProducedType() {
     		return typeInfo;
     	}&lt;br/&gt;
     &lt;br/&gt;
    -	/**&lt;br/&gt;
    -	 * Converts a (nested) Avro {@link SpecificRecord}
&lt;p&gt; into Flink&apos;s Row type.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;* Avro&apos;s 
{@link Utf8}
&lt;p&gt; fields are converted into regular Java strings.&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;*/&lt;/li&gt;
	&lt;li&gt;private static Object convertToRow(Schema schema, Object recordObj) {&lt;/li&gt;
	&lt;li&gt;if (recordObj instanceof GenericRecord) {&lt;/li&gt;
	&lt;li&gt;// records can be wrapped in a union&lt;/li&gt;
	&lt;li&gt;if (schema.getType() == Schema.Type.UNION) {&lt;br/&gt;
    +	// --------------------------------------------------------------------------------------------&lt;br/&gt;
    +&lt;br/&gt;
    +	private Row convertRecord(Schema schema, RowTypeInfo typeInfo, IndexedRecord record) 
&lt;div class=&quot;error&quot;&gt;&lt;span class=&quot;error&quot;&gt;Unknown macro: {    +		final List&amp;lt;Schema.Field&amp;gt; fields = schema.getFields();    +		final TypeInformation&amp;lt;?&amp;gt;[] fieldInfo = typeInfo.getFieldTypes();    +		final int length = fields.size();    +		final Row row = new Row(length);    +		for (int i = 0; i &amp;lt; length; i++) {
    +			final Schema.Field field = fields.get(i);
    +			row.setField(i, convert(field.schema(), fieldInfo[i], record.get(i)));
    +		}    +		return row;    +	}&lt;/span&gt; &lt;/div&gt;
&lt;p&gt;    +&lt;br/&gt;
    +	private Object convert(Schema schema, TypeInformation&amp;lt;?&amp;gt; info, Object object) {&lt;br/&gt;
    +		// we perform the conversion based on schema information but enriched with pre-computed&lt;br/&gt;
    +		// type information where useful (i.e., for arrays)&lt;br/&gt;
    +&lt;br/&gt;
    +		if (object == null) &lt;/p&gt;
{
    +			return null;
    +		}
&lt;p&gt;    +		switch (schema.getType()) {&lt;br/&gt;
    +			case RECORD:&lt;br/&gt;
    +				if (object instanceof IndexedRecord) &lt;/p&gt;
{
    +					return convertRecord(schema, (RowTypeInfo) info, (IndexedRecord) object);
    +				}
&lt;p&gt;    +				throw new IllegalStateException(&quot;IndexedRecord expected but was: &quot; + object.getClass());&lt;br/&gt;
    +			case ENUM:&lt;br/&gt;
    +			case STRING:&lt;br/&gt;
    +				return object.toString();&lt;br/&gt;
    +			case ARRAY:&lt;br/&gt;
    +				if (info instanceof BasicArrayTypeInfo) &lt;/p&gt;
{
    +					final BasicArrayTypeInfo&amp;lt;?, ?&amp;gt; bati = (BasicArrayTypeInfo&amp;lt;?, ?&amp;gt;) info;
    +					final TypeInformation&amp;lt;?&amp;gt; elementInfo = bati.getComponentInfo();
    +					return convertObjectArray(schema.getElementType(), elementInfo, object);
    +				}
&lt;p&gt; else &lt;/p&gt;
{
    +					final ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt; oati = (ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt;) info;
    +					final TypeInformation&amp;lt;?&amp;gt; elementInfo = oati.getComponentInfo();
    +					return convertObjectArray(schema.getElementType(), elementInfo, object);
    +				}
&lt;p&gt;    +			case MAP:&lt;br/&gt;
    +				final MapTypeInfo&amp;lt;?, ?&amp;gt; mti = (MapTypeInfo&amp;lt;?, ?&amp;gt;) info;&lt;br/&gt;
    +				final Map&amp;lt;String, Object&amp;gt; convertedMap = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
    +				final Map&amp;lt;?, ?&amp;gt; map = (Map&amp;lt;?, ?&amp;gt;) object;&lt;br/&gt;
    +				for (Map.Entry&amp;lt;?, ?&amp;gt; entry : map.entrySet()) &lt;/p&gt;
{
    +					convertedMap.put(
    +						entry.getKey().toString(),
    +						convert(schema.getValueType(), mti.getValueTypeInfo(), entry.getValue()));
    +				}
&lt;p&gt;    +				return convertedMap;&lt;br/&gt;
    +			case UNION:&lt;br/&gt;
     				final List&amp;lt;Schema&amp;gt; types = schema.getTypes();&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;if (types.size() == 2 &amp;amp;&amp;amp; types.get(0).getType() == Schema.Type.NULL &amp;amp;&amp;amp; types.get(1).getType() == Schema.Type.RECORD) {&lt;/li&gt;
	&lt;li&gt;schema = types.get(1);&lt;br/&gt;
    +				final int size = types.size();&lt;br/&gt;
    +				final Schema actualSchema;&lt;br/&gt;
    +				if (size == 2 &amp;amp;&amp;amp; types.get(0).getType() == Schema.Type.NULL) 
{
    +					return convert(types.get(1), info, object);
    +				}
&lt;p&gt; else if (size == 2 &amp;amp;&amp;amp; types.get(1).getType() == Schema.Type.NULL) &lt;/p&gt;
{
    +					return convert(types.get(0), info, object);
    +				} else if (size == 1) {    +					return convert(types.get(0), info, object);    +				}
&lt;p&gt; else &lt;/p&gt;
{
    +					// generic type
    +					return object;
    +				}
&lt;p&gt;    +			case FIXED:&lt;br/&gt;
    +				final byte[] fixedBytes = ((GenericFixed) object).bytes();&lt;br/&gt;
    +				if (info == Types.BIG_DEC) &lt;/p&gt;
{
    +					return convertDecimal(schema, fixedBytes);
     				}&lt;/li&gt;
	&lt;li&gt;else {&lt;/li&gt;
	&lt;li&gt;throw new RuntimeException(&quot;Currently we only support schemas of the following form: UNION&lt;span class=&quot;error&quot;&gt;&amp;#91;null, RECORD&amp;#93;&lt;/span&gt;. Given: &quot; + schema);&lt;br/&gt;
    +				return fixedBytes;&lt;br/&gt;
    +			case BYTES:&lt;br/&gt;
    +				final ByteBuffer bb = (ByteBuffer) object;&lt;br/&gt;
    +				bb.position(0);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    Why is it safer? I think opposite is true. This code as it is will only work if Avro passes a fresh ByteBuffer every time. Which probably is the case, or even they might be slicing `ByteBuffer` for safety/data visibility reasons, but I doubt it&apos;s guaranteed.&lt;/p&gt;

&lt;p&gt;    Usually if someone passes you a `ByteBuffer` to use, you shouldn&apos;t play around with initial offsets/limits but to use them as they are - component that creates and passes `ByteBuffer` to you is usually responsible for setting correct position (which might be `0` or not) and limit.&lt;/p&gt;</comment>
                            <comment id="16531263" author="githubbot" created="Tue, 3 Jul 2018 12:08:35 +0000"  >&lt;p&gt;Github user pnowojski commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199780422&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199780422&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -201,71 +202,69 @@ private Object convert(Schema schema, TypeInformation&amp;lt;?&amp;gt; info, Object object) {&lt;br/&gt;
     		switch (schema.getType()) {&lt;br/&gt;
     			case RECORD:&lt;br/&gt;
     				if (object instanceof IndexedRecord) &lt;/p&gt;
{
    -					return convertRecord(schema, (RowTypeInfo) info, (IndexedRecord) object);
    +					return convertAvroRecordToRow(schema, (RowTypeInfo) info, (IndexedRecord) object);
     				}
&lt;p&gt;     				throw new IllegalStateException(&quot;IndexedRecord expected but was: &quot; + object.getClass());&lt;br/&gt;
     			case ENUM:&lt;br/&gt;
     			case STRING:&lt;br/&gt;
     				return object.toString();&lt;br/&gt;
     			case ARRAY:&lt;br/&gt;
     				if (info instanceof BasicArrayTypeInfo) &lt;/p&gt;
{
    -					final BasicArrayTypeInfo&amp;lt;?, ?&amp;gt; bati = (BasicArrayTypeInfo&amp;lt;?, ?&amp;gt;) info;
    -					final TypeInformation&amp;lt;?&amp;gt; elementInfo = bati.getComponentInfo();
    -					return convertObjectArray(schema.getElementType(), elementInfo, object);
    +					final TypeInformation&amp;lt;?&amp;gt; elementInfo = ((BasicArrayTypeInfo&amp;lt;?, ?&amp;gt;) info).getComponentInfo();
    +					return convertToObjectArray(schema.getElementType(), elementInfo, object);
     				}
&lt;p&gt; else &lt;/p&gt;
{
    -					final ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt; oati = (ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt;) info;
    -					final TypeInformation&amp;lt;?&amp;gt; elementInfo = oati.getComponentInfo();
    -					return convertObjectArray(schema.getElementType(), elementInfo, object);
    +					final TypeInformation&amp;lt;?&amp;gt; elementInfo = ((ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt;) info).getComponentInfo();
    +					return convertToObjectArray(schema.getElementType(), elementInfo, object);
     				}
&lt;p&gt;     			case MAP:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final MapTypeInfo&amp;lt;?, ?&amp;gt; mti = (MapTypeInfo&amp;lt;?, ?&amp;gt;) info;&lt;br/&gt;
    +				final MapTypeInfo&amp;lt;?, ?&amp;gt; mapTypeInfo = (MapTypeInfo&amp;lt;?, ?&amp;gt;) info;&lt;br/&gt;
     				final Map&amp;lt;String, Object&amp;gt; convertedMap = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     				final Map&amp;lt;?, ?&amp;gt; map = (Map&amp;lt;?, ?&amp;gt;) object;&lt;br/&gt;
     				for (Map.Entry&amp;lt;?, ?&amp;gt; entry : map.entrySet()) 
{
     					convertedMap.put(
     						entry.getKey().toString(),
    -						convert(schema.getValueType(), mti.getValueTypeInfo(), entry.getValue()));
    +						convertAvroType(schema.getValueType(), mapTypeInfo.getValueTypeInfo(), entry.getValue()));
     				}
&lt;p&gt;     				return convertedMap;&lt;br/&gt;
     			case UNION:&lt;br/&gt;
     				final List&amp;lt;Schema&amp;gt; types = schema.getTypes();&lt;br/&gt;
     				final int size = types.size();&lt;br/&gt;
     				final Schema actualSchema;&lt;br/&gt;
     				if (size == 2 &amp;amp;&amp;amp; types.get(0).getType() == Schema.Type.NULL) &lt;/p&gt;
{
    -					return convert(types.get(1), info, object);
    +					return convertAvroType(types.get(1), info, object);
     				}
&lt;p&gt; else if (size == 2 &amp;amp;&amp;amp; types.get(1).getType() == Schema.Type.NULL) &lt;/p&gt;
{
    -					return convert(types.get(0), info, object);
    +					return convertAvroType(types.get(0), info, object);
     				} else if (size == 1) {    -					return convert(types.get(0), info, object);    +					return convertAvroType(types.get(0), info, object);     				}
&lt;p&gt; else &lt;/p&gt;
{
     					// generic type
     					return object;
     				}
&lt;p&gt;     			case FIXED:&lt;br/&gt;
     				final byte[] fixedBytes = ((GenericFixed) object).bytes();&lt;br/&gt;
     				if (info == Types.BIG_DEC) &lt;/p&gt;
{
    -					return convertDecimal(schema, fixedBytes);
    +					return convertToDecimal(schema, fixedBytes);
     				}
&lt;p&gt;     				return fixedBytes;&lt;br/&gt;
     			case BYTES:&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;final ByteBuffer bb = (ByteBuffer) object;&lt;/li&gt;
	&lt;li&gt;bb.position(0);&lt;/li&gt;
	&lt;li&gt;final byte[] bytes = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;bb.remaining()&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;bb.get(bytes);&lt;br/&gt;
    +				final ByteBuffer byteBuffer = (ByteBuffer) object;&lt;br/&gt;
    +				byteBuffer.position(0);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I think this is still wrong (unless it&apos;s specified somewhere in the avro documentation) - check my comment in previous thread&lt;/p&gt;</comment>
                            <comment id="16531264" author="githubbot" created="Tue, 3 Jul 2018 12:08:36 +0000"  >&lt;p&gt;Github user pnowojski commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199781443&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199781443&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/test/java/org/apache/flink/formats/avro/typeutils/AvroSchemaConverterTest.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,71 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +&lt;br/&gt;
    +package org.apache.flink.formats.avro.typeutils;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.TypeInformation;&lt;br/&gt;
    +import org.apache.flink.api.common.typeinfo.Types;&lt;br/&gt;
    +import org.apache.flink.api.java.typeutils.RowTypeInfo;&lt;br/&gt;
    +import org.apache.flink.formats.avro.generated.User;&lt;br/&gt;
    +import org.apache.flink.types.Row;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.junit.Test;&lt;br/&gt;
    +&lt;br/&gt;
    +import static org.junit.Assert.assertEquals;&lt;br/&gt;
    +import static org.junit.Assert.assertTrue;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * Tests for &lt;/p&gt;
{@link AvroSchemaConverter}
&lt;p&gt;.&lt;br/&gt;
    + */&lt;br/&gt;
    +public class AvroSchemaConverterTest {&lt;br/&gt;
    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testAvroClassConversion() &lt;/p&gt;
{
    +		validateUserSchema(AvroSchemaConverter.convert(User.class));
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	@Test&lt;br/&gt;
    +	public void testAvroSchemaConversion() &lt;/p&gt;
{
    +		final String schema = User.getClassSchema().toString(true);
    +		validateUserSchema(AvroSchemaConverter.convert(schema));
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	private void validateUserSchema(TypeInformation&amp;lt;?&amp;gt; actual) {&lt;br/&gt;
    +		final TypeInformation&amp;lt;Row&amp;gt; address = Types.ROW_NAMED(&lt;br/&gt;
    +			new String[]&lt;/p&gt;
{&quot;num&quot;, &quot;street&quot;, &quot;city&quot;, &quot;state&quot;, &quot;zip&quot;}
&lt;p&gt;,&lt;br/&gt;
    +			Types.INT, Types.STRING, Types.STRING, Types.STRING, Types.STRING);&lt;br/&gt;
    +&lt;br/&gt;
    +		final TypeInformation&amp;lt;Row&amp;gt; user = Types.ROW_NAMED(&lt;br/&gt;
    +			new String[] {&quot;name&quot;, &quot;favorite_number&quot;, &quot;favorite_color&quot;, &quot;type_long_test&quot;,&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I would argue that in that case one entry per line is more readable. The problem with such lines is that whenever someone modifies one entry or add an entry in the middle, diffs are unreadable. Also any conflicts (if two commits added an entry) with multiple entries per line are nasty, while with one entry per line usually there are no conflicts - or they are easy to solve.&lt;/p&gt;</comment>
                            <comment id="16531269" author="githubbot" created="Tue, 3 Jul 2018 12:10:29 +0000"  >&lt;p&gt;Github user pnowojski commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199782571&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199782571&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -51,6 +51,17 @@ under the License.&lt;br/&gt;
     			&amp;lt;!-- managed version --&amp;gt;&lt;br/&gt;
     		&amp;lt;/dependency&amp;gt;&lt;/p&gt;

&lt;p&gt;    +		&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +			&amp;lt;groupId&amp;gt;joda-time&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +			&amp;lt;artifactId&amp;gt;joda-time&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +			&amp;lt;!-- managed version --&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I do not feel competent enough here to make final call. Maybe you could ask @zentol (or someone else) about it?&lt;/p&gt;</comment>
                            <comment id="16531307" author="githubbot" created="Tue, 3 Jul 2018 12:42:57 +0000"  >&lt;p&gt;Github user twalthr commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199791671&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199791671&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/src/main/java/org/apache/flink/formats/avro/AvroRowDeserializationSchema.java &amp;#8212;&lt;br/&gt;
    @@ -201,71 +202,69 @@ private Object convert(Schema schema, TypeInformation&amp;lt;?&amp;gt; info, Object object) {&lt;br/&gt;
     		switch (schema.getType()) {&lt;br/&gt;
     			case RECORD:&lt;br/&gt;
     				if (object instanceof IndexedRecord) &lt;/p&gt;
{
    -					return convertRecord(schema, (RowTypeInfo) info, (IndexedRecord) object);
    +					return convertAvroRecordToRow(schema, (RowTypeInfo) info, (IndexedRecord) object);
     				}
&lt;p&gt;     				throw new IllegalStateException(&quot;IndexedRecord expected but was: &quot; + object.getClass());&lt;br/&gt;
     			case ENUM:&lt;br/&gt;
     			case STRING:&lt;br/&gt;
     				return object.toString();&lt;br/&gt;
     			case ARRAY:&lt;br/&gt;
     				if (info instanceof BasicArrayTypeInfo) &lt;/p&gt;
{
    -					final BasicArrayTypeInfo&amp;lt;?, ?&amp;gt; bati = (BasicArrayTypeInfo&amp;lt;?, ?&amp;gt;) info;
    -					final TypeInformation&amp;lt;?&amp;gt; elementInfo = bati.getComponentInfo();
    -					return convertObjectArray(schema.getElementType(), elementInfo, object);
    +					final TypeInformation&amp;lt;?&amp;gt; elementInfo = ((BasicArrayTypeInfo&amp;lt;?, ?&amp;gt;) info).getComponentInfo();
    +					return convertToObjectArray(schema.getElementType(), elementInfo, object);
     				}
&lt;p&gt; else &lt;/p&gt;
{
    -					final ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt; oati = (ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt;) info;
    -					final TypeInformation&amp;lt;?&amp;gt; elementInfo = oati.getComponentInfo();
    -					return convertObjectArray(schema.getElementType(), elementInfo, object);
    +					final TypeInformation&amp;lt;?&amp;gt; elementInfo = ((ObjectArrayTypeInfo&amp;lt;?, ?&amp;gt;) info).getComponentInfo();
    +					return convertToObjectArray(schema.getElementType(), elementInfo, object);
     				}
&lt;p&gt;     			case MAP:&lt;/p&gt;
&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;final MapTypeInfo&amp;lt;?, ?&amp;gt; mti = (MapTypeInfo&amp;lt;?, ?&amp;gt;) info;&lt;br/&gt;
    +				final MapTypeInfo&amp;lt;?, ?&amp;gt; mapTypeInfo = (MapTypeInfo&amp;lt;?, ?&amp;gt;) info;&lt;br/&gt;
     				final Map&amp;lt;String, Object&amp;gt; convertedMap = new HashMap&amp;lt;&amp;gt;();&lt;br/&gt;
     				final Map&amp;lt;?, ?&amp;gt; map = (Map&amp;lt;?, ?&amp;gt;) object;&lt;br/&gt;
     				for (Map.Entry&amp;lt;?, ?&amp;gt; entry : map.entrySet()) 
{
     					convertedMap.put(
     						entry.getKey().toString(),
    -						convert(schema.getValueType(), mti.getValueTypeInfo(), entry.getValue()));
    +						convertAvroType(schema.getValueType(), mapTypeInfo.getValueTypeInfo(), entry.getValue()));
     				}
&lt;p&gt;     				return convertedMap;&lt;br/&gt;
     			case UNION:&lt;br/&gt;
     				final List&amp;lt;Schema&amp;gt; types = schema.getTypes();&lt;br/&gt;
     				final int size = types.size();&lt;br/&gt;
     				final Schema actualSchema;&lt;br/&gt;
     				if (size == 2 &amp;amp;&amp;amp; types.get(0).getType() == Schema.Type.NULL) &lt;/p&gt;
{
    -					return convert(types.get(1), info, object);
    +					return convertAvroType(types.get(1), info, object);
     				}
&lt;p&gt; else if (size == 2 &amp;amp;&amp;amp; types.get(1).getType() == Schema.Type.NULL) &lt;/p&gt;
{
    -					return convert(types.get(0), info, object);
    +					return convertAvroType(types.get(0), info, object);
     				} else if (size == 1) {    -					return convert(types.get(0), info, object);    +					return convertAvroType(types.get(0), info, object);     				}
&lt;p&gt; else &lt;/p&gt;
{
     					// generic type
     					return object;
     				}
&lt;p&gt;     			case FIXED:&lt;br/&gt;
     				final byte[] fixedBytes = ((GenericFixed) object).bytes();&lt;br/&gt;
     				if (info == Types.BIG_DEC) &lt;/p&gt;
{
    -					return convertDecimal(schema, fixedBytes);
    +					return convertToDecimal(schema, fixedBytes);
     				}
&lt;p&gt;     				return fixedBytes;&lt;br/&gt;
     			case BYTES:&lt;/p&gt;&lt;/li&gt;
	&lt;li&gt;final ByteBuffer bb = (ByteBuffer) object;&lt;/li&gt;
	&lt;li&gt;bb.position(0);&lt;/li&gt;
	&lt;li&gt;final byte[] bytes = new byte&lt;span class=&quot;error&quot;&gt;&amp;#91;bb.remaining()&amp;#93;&lt;/span&gt;;&lt;/li&gt;
	&lt;li&gt;bb.get(bytes);&lt;br/&gt;
    +				final ByteBuffer byteBuffer = (ByteBuffer) object;&lt;br/&gt;
    +				byteBuffer.position(0);
	&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
		&lt;li&gt;
		&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
			&lt;li&gt;End diff &amp;#8211;&lt;/li&gt;
		&lt;/ul&gt;
		&lt;/li&gt;
	&lt;/ul&gt;
	&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    I will remove it. The tests succeed in both cases.&lt;/p&gt;</comment>
                            <comment id="16531310" author="githubbot" created="Tue, 3 Jul 2018 12:44:01 +0000"  >&lt;p&gt;Github user zentol commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218#discussion_r199792051&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218#discussion_r199792051&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-formats/flink-avro/pom.xml &amp;#8212;&lt;br/&gt;
    @@ -51,6 +51,17 @@ under the License.&lt;br/&gt;
     			&amp;lt;!-- managed version --&amp;gt;&lt;br/&gt;
     		&amp;lt;/dependency&amp;gt;&lt;/p&gt;

&lt;p&gt;    +		&amp;lt;dependency&amp;gt;&lt;br/&gt;
    +			&amp;lt;groupId&amp;gt;joda-time&amp;lt;/groupId&amp;gt;&lt;br/&gt;
    +			&amp;lt;artifactId&amp;gt;joda-time&amp;lt;/artifactId&amp;gt;&lt;br/&gt;
    +			&amp;lt;!-- managed version --&amp;gt;&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    I went through the upgrade docs for joda-time from 2.5 til 2.9 and they are all marked as binary compatible, so at least at the moment we should be fine with compiling against 2.5.&lt;/p&gt;</comment>
                            <comment id="16531314" author="githubbot" created="Tue, 3 Jul 2018 12:47:36 +0000"  >&lt;p&gt;Github user twalthr commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks for the feedback @pnowojski and @zentol. I will merge this...&lt;/p&gt;</comment>
                            <comment id="16531538" author="twalthr" created="Tue, 3 Jul 2018 15:18:37 +0000"  >&lt;p&gt;Fixed in 1.6.0: c34c7e4127c8947d68e2b960cd84206e59d479b3&lt;/p&gt;</comment>
                            <comment id="16531539" author="githubbot" created="Tue, 3 Jul 2018 15:19:28 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6218&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6218&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16531540" author="githubbot" created="Tue, 3 Jul 2018 15:19:33 +0000"  >&lt;p&gt;Github user asfgit closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="16531556" author="githubbot" created="Tue, 3 Jul 2018 15:30:12 +0000"  >&lt;p&gt;Github user twalthr commented on the issue:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/6082&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    In the end it seems that I had to rewrite this whole Avro logic in order to finally support all types and both specific and generic Avro records. I hope it is ok that I could not include your contribution. Please let me know if this big change also solved your issues.&lt;/p&gt;</comment>
                            <comment id="16567919" author="githubbot" created="Fri, 3 Aug 2018 07:55:15 +0000"  >&lt;p&gt;tragicjun commented on issue #6082: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-9444&quot; title=&quot;Add full SQL support for Avro formats&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-9444&quot;&gt;&lt;del&gt;FLINK-9444&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;table&amp;#93;&lt;/span&gt; KafkaAvroTableSource failed to work for map and array fields&lt;br/&gt;
URL: &lt;a href=&quot;https://github.com/apache/flink/pull/6082#issuecomment-410175862&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/6082#issuecomment-410175862&lt;/a&gt;&lt;/p&gt;


&lt;p&gt;   @twalthr it solves my issue as well, good job!&lt;/p&gt;

&lt;p&gt;----------------------------------------------------------------&lt;br/&gt;
This is an automated message from the Apache Git Service.&lt;br/&gt;
To respond to the message, please log on GitHub and use the&lt;br/&gt;
URL above to go to the specific comment.&lt;/p&gt;

&lt;p&gt;For queries about this service, please contact Infrastructure at:&lt;br/&gt;
users@infra.apache.org&lt;/p&gt;</comment>
                    </comments>
                    <attachments>
                            <attachment id="12925257" name="flink-9444.patch" size="13784" author="junz" created="Sat, 26 May 2018 17:24:48 +0000"/>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>1.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            7 years, 15 weeks, 4 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i3u74v:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>