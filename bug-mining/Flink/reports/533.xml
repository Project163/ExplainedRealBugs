<!-- 
RSS generated by JIRA (8.20.10#820010-sha1:ace47f9899e9ee25d7157d59aa17ab06aee30d3d) at Tue Nov 11 20:20:40 UTC 2025

It is possible to restrict the fields that are returned in this document by specifying the 'field' parameter in your request.
For example, to request only the issue key and summary append 'field=key&field=summary' to the URL of your request.
-->
<rss version="0.92" >
<channel>
    <title>ASF JIRA</title>
    <link>https://issues.apache.org/jira</link>
    <description>This file is an XML representation of an issue</description>
    <language>en-uk</language>    <build-info>
        <version>8.20.10</version>
        <build-number>820010</build-number>
        <build-date>22-06-2022</build-date>
    </build-info>


<item>
            <title>[FLINK-2555] Hadoop Input/Output Formats are unable to access secured HDFS clusters</title>
                <link>https://issues.apache.org/jira/browse/FLINK-2555</link>
                <project id="12315522" key="FLINK">Flink</project>
                    <description>&lt;p&gt;It seems that authentication tokens are not passed correctly to the input format when accessing secured HDFS clusters.&lt;/p&gt;

&lt;p&gt;Exception&lt;/p&gt;
&lt;div class=&quot;code panel&quot; style=&quot;border-width: 1px;&quot;&gt;&lt;div class=&quot;codeContent panelContent&quot;&gt;
&lt;pre class=&quot;code-java&quot;&gt;org.apache.flink.client.program.ProgramInvocationException: The program execution failed: Failed to submit job b319a28f62855917901cfb67c5457142 (Flink Java Job at Thu Aug 20 10:46:41 PDT 2015)
	at org.apache.flink.client.program.Client.run(Client.java:413)
	at org.apache.flink.client.program.Client.run(Client.java:356)
	at org.apache.flink.client.program.Client.run(Client.java:349)
	at org.apache.flink.client.program.ContextEnvironment.execute(ContextEnvironment.java:63)
	at org.apache.flink.api.java.ExecutionEnvironment.execute(ExecutionEnvironment.java:789)
	at org.apache.flink.api.java.DataSet.collect(DataSet.java:408)
	at org.apache.flink.api.java.DataSet.print(DataSet.java:1346)
	at de.robertmetzger.WordCount.main(WordCount.java:73)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.flink.client.program.PackagedProgram.callMainMethod(PackagedProgram.java:437)
	at org.apache.flink.client.program.PackagedProgram.invokeInteractiveModeForExecution(PackagedProgram.java:353)
	at org.apache.flink.client.program.Client.run(Client.java:315)
	at org.apache.flink.client.CliFrontend.executeProgram(CliFrontend.java:584)
	at org.apache.flink.client.CliFrontend.run(CliFrontend.java:290)
	at org.apache.flink.client.CliFrontend$2.run(CliFrontend.java:873)
	at org.apache.flink.client.CliFrontend$2.run(CliFrontend.java:870)
	at org.apache.flink.runtime.security.SecurityUtils$1.run(SecurityUtils.java:50)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1556)
	at org.apache.flink.runtime.security.SecurityUtils.runSecured(SecurityUtils.java:47)
	at org.apache.flink.client.CliFrontend.parseParameters(CliFrontend.java:870)
	at org.apache.flink.client.CliFrontend.main(CliFrontend.java:922)
Caused by: org.apache.flink.runtime.client.JobExecutionException: Failed to submit job b319a28f62855917901cfb67c5457142 (Flink Java Job at Thu Aug 20 10:46:41 PDT 2015)
	at org.apache.flink.runtime.jobmanager.JobManager.org$apache$flink$runtime$jobmanager$JobManager$$submitJob(JobManager.scala:594)
	at org.apache.flink.runtime.jobmanager.JobManager$$anonfun$receiveWithLogMessages$1.applyOrElse(JobManager.scala:190)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply$mcVL$sp(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:33)
	at scala.runtime.AbstractPartialFunction$mcVL$sp.apply(AbstractPartialFunction.scala:25)
	at org.apache.flink.yarn.ApplicationMasterActor$$anonfun$receiveYarnMessages$1.applyOrElse(ApplicationMasterActor.scala:100)
	at scala.PartialFunction$OrElse.apply(PartialFunction.scala:162)
	at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:36)
	at org.apache.flink.runtime.ActorLogMessages$$anon$1.apply(ActorLogMessages.scala:29)
	at scala.PartialFunction$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;applyOrElse(PartialFunction.scala:118)
	at org.apache.flink.runtime.ActorLogMessages$$anon$1.applyOrElse(ActorLogMessages.scala:29)
	at akka.actor.Actor$&lt;span class=&quot;code-keyword&quot;&gt;class.&lt;/span&gt;aroundReceive(Actor.scala:465)
	at org.apache.flink.runtime.jobmanager.JobManager.aroundReceive(JobManager.scala:92)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:254)
	at akka.dispatch.Mailbox.run(Mailbox.scala:221)
	at akka.dispatch.Mailbox.exec(Mailbox.scala:231)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
Caused by: org.apache.flink.runtime.JobException: Creating the input splits caused an error: Delegation Token can be issued only with kerberos or web authentication
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:7086)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:506)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getDelegationToken(AuthorizationProviderProxyClientProtocol.java:637)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:957)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.&amp;lt;init&amp;gt;(ExecutionJobVertex.java:162)
	at org.apache.flink.runtime.executiongraph.ExecutionGraph.attachJobGraph(ExecutionGraph.java:469)
	at org.apache.flink.runtime.jobmanager.JobManager.org$apache$flink$runtime$jobmanager$JobManager$$submitJob(JobManager.scala:534)
	... 21 more
Caused by: org.apache.hadoop.ipc.RemoteException(java.io.IOException): Delegation Token can be issued only with kerberos or web authentication
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:7086)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:506)
	at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.getDelegationToken(AuthorizationProviderProxyClientProtocol.java:637)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getDelegationToken(ClientNamenodeProtocolServerSideTranslatorPB.java:957)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:587)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1026)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2013)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2009)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1642)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2007)

	at org.apache.hadoop.ipc.Client.call(Client.java:1410)
	at org.apache.hadoop.ipc.Client.call(Client.java:1363)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:206)
	at com.sun.proxy.$Proxy14.getDelegationToken(Unknown Source)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:190)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:103)
	at com.sun.proxy.$Proxy14.getDelegationToken(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getDelegationToken(ClientNamenodeProtocolTranslatorPB.java:854)
	at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:924)
	at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:1336)
	at org.apache.hadoop.fs.FileSystem.collectDelegationTokens(FileSystem.java:527)
	at org.apache.hadoop.fs.FileSystem.addDelegationTokens(FileSystem.java:505)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:121)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:100)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:80)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:241)
	at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:375)
	at org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormatBase.createInputSplits(HadoopInputFormatBase.java:140)
	at org.apache.flink.api.java.hadoop.mapreduce.HadoopInputFormatBase.createInputSplits(HadoopInputFormatBase.java:51)
	at org.apache.flink.runtime.executiongraph.ExecutionJobVertex.&amp;lt;init&amp;gt;(ExecutionJobVertex.java:146)
	... 23 more

The exception above occurred &lt;span class=&quot;code-keyword&quot;&gt;while&lt;/span&gt; trying to run your command.
&lt;/pre&gt;
&lt;/div&gt;&lt;/div&gt;


&lt;p&gt;This issue has been reported by a user: &lt;a href=&quot;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Using-HadoopInputFormat-files-from-Flink-Yarn-in-a-secure-cluster-gives-an-error-td2472.html&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://apache-flink-user-mailing-list-archive.2336050.n4.nabble.com/Using-HadoopInputFormat-files-from-Flink-Yarn-in-a-secure-cluster-gives-an-error-td2472.html&lt;/a&gt;&lt;/p&gt;
</description>
                <environment></environment>
        <key id="12857604">FLINK-2555</key>
            <summary>Hadoop Input/Output Formats are unable to access secured HDFS clusters</summary>
                <type id="1" iconUrl="https://issues.apache.org/jira/secure/viewavatar?size=xsmall&amp;avatarId=21133&amp;avatarType=issuetype">Bug</type>
                                            <priority id="2" iconUrl="https://issues.apache.org/jira/images/icons/priorities/critical.svg">Critical</priority>
                        <status id="5" iconUrl="https://issues.apache.org/jira/images/icons/statuses/resolved.png" description="A resolution has been taken, and it is awaiting verification by reporter. From here issues are either reopened, or are closed.">Resolved</status>
                    <statusCategory id="3" key="done" colorName="green"/>
                                    <resolution id="1">Fixed</resolution>
                                        <assignee username="rmetzger">Robert Metzger</assignee>
                                    <reporter username="rmetzger">Robert Metzger</reporter>
                        <labels>
                    </labels>
                <created>Thu, 20 Aug 2015 17:49:53 +0000</created>
                <updated>Thu, 10 Sep 2015 09:15:05 +0000</updated>
                            <resolved>Wed, 26 Aug 2015 14:31:52 +0000</resolved>
                                    <version>0.9</version>
                    <version>0.10.0</version>
                                    <fixVersion>0.9.1</fixVersion>
                    <fixVersion>0.10.0</fixVersion>
                                        <due></due>
                            <votes>0</votes>
                                    <watches>6</watches>
                                                                                                                <comments>
                            <comment id="14706528" author="githubbot" created="Fri, 21 Aug 2015 10:39:19 +0000"  >&lt;p&gt;GitHub user rmetzger opened a pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-2555&quot; title=&quot;Hadoop Input/Output Formats are unable to access secured HDFS clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-2555&quot;&gt;&lt;del&gt;FLINK-2555&lt;/del&gt;&lt;/a&gt; Properly pass security credentials in the Hadoop Input/Output format wrappers&lt;/p&gt;

&lt;p&gt;    This is needed because the Hadoop IF/OF&apos;s are using Hadoop&apos;s FileSystem stack, which is using the security credentials passed in the JobConf / Job class in the getSplits() method.&lt;/p&gt;

&lt;p&gt;    Note that access to secured Hadoop 1.x using Hadoop IF/OF&apos;s is not possible with this change. This limitation is due to missing methods in the old APIs.&lt;/p&gt;

&lt;p&gt;    I&apos;ve also updated the version of the &quot;de.javakaffee.kryo-serializers&quot; from 0.27 to 0.36 because a user on the ML recently needed a specific Kryo serializer which was not available in the old dependency.&lt;/p&gt;

&lt;p&gt;    For the Java and Scala API, I renamed the first argument&apos;s name: `readHadoopFile(org.apache.hadoop.mapreduce.lib.input.FileInputFormat&amp;lt;K,V&amp;gt; mapreduceInputFormat, Class&amp;lt;K&amp;gt; key, Class&amp;lt;V&amp;gt; value, String inputPath, Job job)`&lt;/p&gt;

&lt;p&gt;    This makes it easier in IDE completions to distinguish between the mapreduce and the mapred variant. (before the argument was always called `mapredInputFormat` now, we have the `mapreduceInputFormat` variant where applicable)&lt;/p&gt;

&lt;p&gt;You can merge this pull request into a Git repository by running:&lt;/p&gt;

&lt;p&gt;    $ git pull &lt;a href=&quot;https://github.com/rmetzger/flink&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/rmetzger/flink&lt;/a&gt; flink2555&lt;/p&gt;

&lt;p&gt;Alternatively you can review and apply these changes as the patch at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038.patch&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038.patch&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;To close this pull request, make a commit to your master/trunk branch&lt;br/&gt;
with (at least) the following in the commit message:&lt;/p&gt;

&lt;p&gt;    This closes #1038&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;commit bac21bf5d77c8e15c608ecbf006d29e7af1dd68a&lt;br/&gt;
Author: Aljoscha Krettek &amp;lt;aljoscha.krettek@gmail.com&amp;gt;&lt;br/&gt;
Date:   2015-07-23T13:12:38Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-2398&quot; title=&quot;Decouple StreamGraph Building from the API&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-2398&quot;&gt;&lt;del&gt;FLINK-2398&lt;/del&gt;&lt;/a&gt;&lt;span class=&quot;error&quot;&gt;&amp;#91;api-breaking&amp;#93;&lt;/span&gt; Introduce StreamGraphGenerator&lt;/p&gt;

&lt;p&gt;    This decouples the building of the StreamGraph from the API methods.&lt;br/&gt;
    Before the methods would build the StreamGraph as they go. Now the API&lt;br/&gt;
    methods build a hierachy of StreamTransformation nodes. From these a&lt;br/&gt;
    StreamGraph is generated upon execution.&lt;/p&gt;

&lt;p&gt;    This also introduces some API breaking changes:&lt;/p&gt;

&lt;ul class=&quot;alternate&quot; type=&quot;square&quot;&gt;
	&lt;li&gt;The result of methods that create sinks is now DataStreamSink instead&lt;br/&gt;
       of DataStream&lt;/li&gt;
	&lt;li&gt;Iterations cannot have feedback edges with differing parallelism&lt;/li&gt;
	&lt;li&gt;&quot;Preserve partitioning&quot; is not the default for feedback edges. The&lt;br/&gt;
       previous option for this is removed.&lt;/li&gt;
	&lt;li&gt;You can close an iteration several times, no need for a union.&lt;/li&gt;
	&lt;li&gt;Strict checking of whether partitioning and parallelism work&lt;br/&gt;
       together. I.e. if upstream and downstream parallelism don&apos;t match it&lt;br/&gt;
       is not legal to have Forward partitioning anymore. This was not very&lt;br/&gt;
       transparent: When you went from low parallelism to high dop some&lt;br/&gt;
       downstream  operators would never get any input. When you went from high&lt;br/&gt;
       parallelism to low dop you would get skew in the downstream operators&lt;br/&gt;
       because all elements that would be forwarded to an operator that is not&lt;br/&gt;
       &quot;there&quot; go to another operator. This requires insertion of global()&lt;br/&gt;
       or rebalance() in some places. For example with most sources which&lt;br/&gt;
       have parallelism one.&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;    This also makes StreamExecutionEnvironment.execute() behave consistently&lt;br/&gt;
    across different execution environments (local, remote ...): The list of&lt;br/&gt;
    operators to be executed are cleared after execute is called.&lt;/p&gt;

&lt;p&gt;commit e4b72e6d0148d071a97d2dab5c3bd97b81ee97a5&lt;br/&gt;
Author: Robert Metzger &amp;lt;rmetzger@apache.org&amp;gt;&lt;br/&gt;
Date:   2015-08-20T16:43:04Z&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-2555&quot; title=&quot;Hadoop Input/Output Formats are unable to access secured HDFS clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-2555&quot;&gt;&lt;del&gt;FLINK-2555&lt;/del&gt;&lt;/a&gt; Properly pass security credentials in the Hadoop Input/Output format wrappers&lt;/p&gt;

&lt;p&gt;    This is needed because the Hadoop IF/OF&apos;s are using Hadoop&apos;s FileSystem stack, which is using&lt;br/&gt;
    the security credentials passed in the JobConf / Job class in the getSplits() method.&lt;/p&gt;

&lt;p&gt;    Note that access to secured Hadoop 1.x using Hadoop IF/OF&apos;s is not possible with this change.&lt;br/&gt;
    This limitation is due to missing methods in the old APIs.&lt;/p&gt;

&lt;hr /&gt;</comment>
                            <comment id="14706934" author="githubbot" created="Fri, 21 Aug 2015 16:13:43 +0000"  >&lt;p&gt;Github user StephanEwen commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-133478724&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-133478724&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Looks good. The HadoopFormatBase and similar classes could use a line or two more in comments, but otherwise, this seems well.&lt;/p&gt;

&lt;p&gt;    Any way to test this? There does not seem to be any test for the format wrappers, yet...&lt;/p&gt;</comment>
                            <comment id="14706946" author="githubbot" created="Fri, 21 Aug 2015 16:17:02 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-133479533&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-133479533&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I actually think that there is no need for the `HadoopInputFormatBase`s to exist. &lt;br/&gt;
    There are two implementations and two bases for mapred and mapreduce, but they have nothing in common.&lt;/p&gt;

&lt;p&gt;    There are some tests for the non secure case in `org.apache.flink.test.hadoop`.&lt;/p&gt;</comment>
                            <comment id="14706972" author="githubbot" created="Fri, 21 Aug 2015 16:32:18 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-133483445&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-133483445&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    There might actually be a way of testing against a secured cluster: &lt;a href=&quot;https://issues.apache.org/jira/browse/HADOOP-9848&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/HADOOP-9848&lt;/a&gt; / &lt;a href=&quot;https://github.com/apache/hadoop/blob/master/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/hadoop/blob/master/hadoop-common-project/hadoop-minikdc/src/main/java/org/apache/hadoop/minikdc/MiniKdc.java&lt;/a&gt;&lt;br/&gt;
    This seems to be available since Hadoop 2.3.0&lt;/p&gt;
</comment>
                            <comment id="14707094" author="githubbot" created="Fri, 21 Aug 2015 17:33:12 +0000"  >&lt;p&gt;Github user aljoscha commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-133505584&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-133505584&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    The Bases exist because there is a java-specific and a scals-specific version of each HadoopInputFormat. &lt;/p&gt;</comment>
                            <comment id="14709019" author="githubbot" created="Mon, 24 Aug 2015 09:39:23 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-134112685&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-134112685&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Okay, that makes sense.&lt;br/&gt;
    I&apos;ll add some comments to the classes.&lt;/p&gt;</comment>
                            <comment id="14710932" author="githubbot" created="Tue, 25 Aug 2015 09:00:18 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-134531698&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-134531698&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @mxm: I removed the comment.&lt;/p&gt;</comment>
                            <comment id="14711127" author="githubbot" created="Tue, 25 Aug 2015 11:55:40 +0000"  >&lt;p&gt;Github user mxm commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-134563009&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-134563009&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    It would be great if we implemented a test case against the MiniKDC server.&lt;/p&gt;</comment>
                            <comment id="14711251" author="githubbot" created="Tue, 25 Aug 2015 13:25:32 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-134584589&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-134584589&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I agree. Lets file a JIRA and do it separately, as this is probably a bigger task.&lt;/p&gt;</comment>
                            <comment id="14711458" author="githubbot" created="Tue, 25 Aug 2015 15:38:38 +0000"  >&lt;p&gt;Github user mxm commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-134632899&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-134632899&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I&apos;ve opened another issue for that: &lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-2573&quot; class=&quot;external-link&quot; rel=&quot;nofollow&quot;&gt;https://issues.apache.org/jira/browse/FLINK-2573&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14713043" author="githubbot" created="Wed, 26 Aug 2015 12:41:11 +0000"  >&lt;p&gt;Github user uce commented on a diff in the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#discussion_r37976251&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#discussion_r37976251&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    &amp;#8212; Diff: flink-java/src/main/java/org/apache/flink/api/java/hadoop/common/HadoopInputFormatCommonBase.java &amp;#8212;&lt;br/&gt;
    @@ -0,0 +1,79 @@&lt;br/&gt;
    +/*&lt;br/&gt;
    + * Licensed to the Apache Software Foundation (ASF) under one&lt;br/&gt;
    + * or more contributor license agreements.  See the NOTICE file&lt;br/&gt;
    + * distributed with this work for additional information&lt;br/&gt;
    + * regarding copyright ownership.  The ASF licenses this file&lt;br/&gt;
    + * to you under the Apache License, Version 2.0 (the&lt;br/&gt;
    + * &quot;License&quot;); you may not use this file except in compliance&lt;br/&gt;
    + * with the License.  You may obtain a copy of the License at&lt;br/&gt;
    + *&lt;br/&gt;
    + *     &lt;a href=&quot;http://www.apache.org/licenses/LICENSE-2.0&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;http://www.apache.org/licenses/LICENSE-2.0&lt;/a&gt;&lt;br/&gt;
    + *&lt;br/&gt;
    + * Unless required by applicable law or agreed to in writing, software&lt;br/&gt;
    + * distributed under the License is distributed on an &quot;AS IS&quot; BASIS,&lt;br/&gt;
    + * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.&lt;br/&gt;
    + * See the License for the specific language governing permissions and&lt;br/&gt;
    + * limitations under the License.&lt;br/&gt;
    + */&lt;br/&gt;
    +package org.apache.flink.api.java.hadoop.common;&lt;br/&gt;
    +&lt;br/&gt;
    +import org.apache.flink.api.common.io.RichInputFormat;&lt;br/&gt;
    +import org.apache.flink.core.io.InputSplit;&lt;br/&gt;
    +import org.apache.hadoop.security.Credentials;&lt;br/&gt;
    +import org.apache.hadoop.security.UserGroupInformation;&lt;br/&gt;
    +&lt;br/&gt;
    +import java.io.IOException;&lt;br/&gt;
    +import java.io.ObjectInputStream;&lt;br/&gt;
    +import java.io.ObjectOutputStream;&lt;br/&gt;
    +import java.lang.reflect.InvocationTargetException;&lt;br/&gt;
    +import java.lang.reflect.Method;&lt;br/&gt;
    +&lt;br/&gt;
    +/**&lt;br/&gt;
    + * A common base for both &quot;mapred&quot; and &quot;mapreduce&quot; Hadoop input formats.&lt;br/&gt;
    + */&lt;br/&gt;
    +public abstract class HadoopInputFormatCommonBase&amp;lt;T, SPITTYPE extends InputSplit&amp;gt; extends RichInputFormat&amp;lt;T, SPITTYPE&amp;gt; {&lt;br/&gt;
    +	protected transient Credentials credentials;&lt;br/&gt;
    +&lt;br/&gt;
    +	protected HadoopInputFormatCommonBase(Credentials creds) &lt;/p&gt;
{
    +		this.credentials = creds;
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	protected void write(ObjectOutputStream out) throws IOException &lt;/p&gt;
{
    +		this.credentials.write(out);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    +	public void read(ObjectInputStream in) throws IOException &lt;/p&gt;
{
    +		this.credentials = new Credentials();
    +		credentials.readFields(in);
    +	}
&lt;p&gt;    +&lt;br/&gt;
    &amp;#8212; End diff &amp;#8211;&lt;/p&gt;

&lt;p&gt;    whitespace&lt;/p&gt;</comment>
                            <comment id="14713047" author="githubbot" created="Wed, 26 Aug 2015 12:42:51 +0000"  >&lt;p&gt;Github user uce commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-134991397&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-134991397&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I&apos;ll address my trivial comment and merge this. Thanks!&lt;/p&gt;</comment>
                            <comment id="14713397" author="githubbot" created="Wed, 26 Aug 2015 13:18:29 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-135013779&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-135013779&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    Thanks alot!&lt;/p&gt;</comment>
                            <comment id="14713526" author="uce" created="Wed, 26 Aug 2015 14:31:52 +0000"  >&lt;p&gt;Fixed in 5869bf9 (release-0.9), b264b01 (master).&lt;/p&gt;</comment>
                            <comment id="14716297" author="githubbot" created="Thu, 27 Aug 2015 08:48:30 +0000"  >&lt;p&gt;Github user rmetzger commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038#issuecomment-135343326&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038#issuecomment-135343326&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    I&apos;m manually closing this pull request. It has been merged by @uce.&lt;/p&gt;</comment>
                            <comment id="14716298" author="githubbot" created="Thu, 27 Aug 2015 08:48:31 +0000"  >&lt;p&gt;Github user rmetzger closed the pull request at:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1038&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1038&lt;/a&gt;&lt;/p&gt;</comment>
                            <comment id="14738464" author="githubbot" created="Thu, 10 Sep 2015 09:15:05 +0000"  >&lt;p&gt;Github user fhueske commented on the pull request:&lt;/p&gt;

&lt;p&gt;    &lt;a href=&quot;https://github.com/apache/flink/pull/1079#issuecomment-139178373&quot; class=&quot;external-link&quot; target=&quot;_blank&quot; rel=&quot;nofollow noopener&quot;&gt;https://github.com/apache/flink/pull/1079#issuecomment-139178373&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;    @jamescao good observation! The fix of #1111 should be added to the HCatOutputFormatBase as well. &lt;br/&gt;
    There was another fix for HadoopIOFormats (&lt;a href=&quot;https://issues.apache.org/jira/browse/FLINK-2555&quot; title=&quot;Hadoop Input/Output Formats are unable to access secured HDFS clusters&quot; class=&quot;issue-link&quot; data-issue-key=&quot;FLINK-2555&quot;&gt;&lt;del&gt;FLINK-2555&lt;/del&gt;&lt;/a&gt;, PR #1038) that is also relevant for this PR.&lt;/p&gt;</comment>
                    </comments>
                <issuelinks>
                            <issuelinktype id="10030">
                    <name>Reference</name>
                                                                <inwardlinks description="is related to">
                                        <issuelink>
            <issuekey id="12858660">FLINK-2573</issuekey>
        </issuelink>
                            </inwardlinks>
                                    </issuelinktype>
                    </issuelinks>
                <attachments>
                    </attachments>
                <subtasks>
                    </subtasks>
                <customfields>
                                                                            <customfield id="customfield_12310310" key="com.atlassian.jira.toolkit:attachments">
                        <customfieldname>Attachment count</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0.0</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        <customfield id="customfield_12314020" key="com.atlassian.jira.plugins.jira-development-integration-plugin:devsummary">
                        <customfieldname>Development</customfieldname>
                        <customfieldvalues>
                            
                        </customfieldvalues>
                    </customfield>
                                                                                                                        <customfield id="customfield_12313422" key="com.atlassian.jirafisheyeplugin:jobcheckbox">
                        <customfieldname>Enable Automatic Patch Review</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue><![CDATA[false]]></customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                <customfield id="customfield_12310420" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Global Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                        <customfield id="customfield_12312521" key="com.atlassian.jira.toolkit:LastCommentDate">
                        <customfieldname>Last public comment date</customfieldname>
                        <customfieldvalues>
                            10 years, 10 weeks, 5 days ago
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                <customfield id="customfield_12311820" key="com.pyxis.greenhopper.jira:gh-lexo-rank">
                        <customfieldname>Rank</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>0|i2j64f:</customfieldvalue>

                        </customfieldvalues>
                    </customfield>
                                                                <customfield id="customfield_12310920" key="com.pyxis.greenhopper.jira:gh-global-rank">
                        <customfieldname>Rank (Obsolete)</customfieldname>
                        <customfieldvalues>
                            <customfieldvalue>9223372036854775807</customfieldvalue>
                        </customfieldvalues>
                    </customfield>
                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        </customfields>
    </item>
</channel>
</rss>